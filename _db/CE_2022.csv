Featured,Track,Name1,JobTitle1,Company1,Name2,JobTitle2,Company2,Title,Abstract,LinkedIn1,Twitter1,LinkedIn2,Twitter2,Slides,Picture,YouTube,Keywords,Duration
Yes,,Nick Hodges,Developer Advocate,Rollbar,,,,Self Healing Systems,"SaaS systems enable all kinds of cool new ways of designing and building software applications. 
One way this is manifested through the notion of self-healing systems, that is, systems that monitor and fix themselves.

Through the use of intelligent error monitoring and feature flags, we can start to see applications that can do just that.",https://www.linkedin.com/in/nickhodges,@NickHodges,,,,ce_nick.png,,self healing systems,30
Yes,,Zach Wasserman,CTO,Fleet,,,,"Glitches in the Matrix, or Taming Agent Chaos","This talk is accessible to all levels, and will include specific examples from [Fleet's codebase](https://github.com/fleetdm/fleet/) that will engage even advanced practitioners. We will also touch on the capabilities in [osquery](https://github.com/osquery/osquery) that help to improve reliability.

Questions posed and answered from Fleet's experience include:

* Where to focus efforts when considering performance and chaos?
* How does Fleet engineer for resiliency when our users self-host the software and have hundreds of thousands of (potentially misbehaving) agents checking in for * coordination?
* What Go tooling can we use to improve monitoring of the Fleet server both during internal performance testing and while running in production?
* How do we collect information about the infrastructure dependencies (MySQL & Redis) to understand the entire system beyond the code we control?
* How can we ensure osquery agents don't bring down the endpoints they are deployed to protect?",https://www.linkedin.com/in/zacharywasserman,@thezachw,,,,ce_zach.png,,"glitches in the matrix, or taming agent chaos",30
Yes,,Ana Margarita Medina,Senior Chaos Engineer,Gremlin,,,,Continuous Reliability. How?,"As engineers we expect our systems and applications to be reliable. And we often test to ensure that at a small scale or in development. But when you scale up and your infrastructure footprint increases, the assumption that conditions will remain stable is wrong. Reliability at scale does not mean eliminating failure; failure is inevitable. How can we get ahead of these failures and ensure we do it in a continuous way?

One of the ways we can go about this is by implementing solutions like CNCF's sandbox project Keptn. Keptn allows us to leverage the tooling we already use and implement pipelines where we execute chaos engineering experiments and performance testing while implementing SLOs. Ana will share how you can start simplifying cloud-native application delivery and operations with Keptn to ensure you deploy reliable applications to production.",https://www.linkedin.com/in/anammedina,@ana_m_medina,,,,ce_ana.png,,continuous reliability,30
Yes,,Sakshyam Shah,Developer Relations Engineer ,Teleport,,,,Defining Steady States and Developing Hypotheses for Security Chaos Engineering,This talk explores how teams can use existing security frameworks and benchmarks to define a Steady State and develop hypotheses for Security Chaos Engineering.,https://www.linkedin.com/in/sshahconnects,@sshahtweets,,,,ce_sakshyam.png,,steady states developing hypotheses,30
Yes,,Gunnar Grosch,Senior Developer Advocate,AWS,,,,Chaos engineering for serverless with AWS Fault Injection Simulator,"Companies of all sizes and industries perform chaos experiments on instance- and container-based workloads. However, serverless functions and managed services present different failure modes and levels of abstraction. 

This session looks at forming hypotheses to fit serverless, what the experiments can achieve, and how to perform them safely using AWS Fault Injection Simulator and open-source libraries.",https://www.linkedin.com/in/gunnargrosch/,@GunnarGrosch,,,,ce_gunnar.png,,serverless aws fault injection simulator,30
No,getting started,Narmatha Bala,Senior Software Engineering Manager,Microsoft CSE,,,,Confidence in Chaos - How properly applied observability practices can take the ‘chaos’ out of chaos testing,"Everyone wants to use observability because it can reduce time between a problem and a solution, but there is a difference between applying traditional observability practices and how they are applied for each custom project and scenario. Narmatha Bala is a Senior Software Engineering Manager in the Commercial Software Engineering (CSE) team and runs the Observability chapter of the CSE Engineering Playbook, a best practices guide put together by development teams to accelerate the sharing of how best to work together, to quickly transfer learnings across teams with customers, particularly with new technologies and projects as a way to enable customers to carry the work forward and continue the development. CSE is a global engineering organization that works directly with the largest companies and not-for-profits in the world to tackle their most significant technical challenges. 

In this talk, Narmatha will share her experiences in instilling Observability best practices with engineers who use best utilize observability in applications versus engineers that think they do. She will cover why actionable failures are a good thing in system design, how to best define Service Level Agreements and Objectives (SLAs, SLOs), and where monitoring fits into the process. This talk is best suited for intermediate practitioners who want to improve their chaos testing by improving their observability practices. 
",,,,,,ce_namatha.png,,observability confidence observability practices,
No,getting started,Ayelet Sachto,Strategic Cloud Engineer,Google,,,,Maintaining Reliable systems: How to minimize Incident's impact?,"Incidents are expensive to the business, especially if customers leave us if we are perceived as unreliable. But failures will happen, it's not an issue of IF, but a question of when. So how can we reduce the impact on our users?
In this talk, I will review the production incident cycle, the time that we are not reliable and our users are not happy which includes the time to detect, time to repair and time between failures.  I'll share a few methods to tackle each one of those parts in order to minimize incident impact both from technical and people aspects, expending on incident response and postmortems to know what is the most important thing for us, and we want to be data driven in those decisions.

",https://www.linkedin.com/in/ayelet-sachto,@ayeletsachto,,,,ce_ayelet.png,,reliable systems minimize incident's impact,
No,getting started,Paul Marsicovetere,Senior Cloud Infrastructure Engineer,Formidable,,,,Optimizing incident response thanks to Chaos Engineering,"When some groups think of chaos engineering, they may think of how the principles and experiments can add resiliency, security and performance improvements to a system. I too was of that mindset until performing a focussed chaos engineering experiment, which lead to some helpful conclusions that were then utilized during a production incident.

This talk will demonstrate how these experiments eventually led to key discoveries in a system and how during a production incident, the conclusions artifact was utilized to assist with incident response. Had the chaos experiment not been performed, incident response would have taken much longer and would have been more painful for end-users.

My goal is to help provide another answer to the question ""why chaos engineering?"". Incident response is always in need of constant improvement and refinement, and chaos engineering is a tool that can most certainly help us.",https://www.linkedin.com/in/paulmarsicovetere,@paulmarsicloud,,,,ce_paul.png,,optimizing incident response,
No,getting started,Kyle Shelton,Chaos & Reliability Engineering & Builder,AWS,,,,Disaster Recovery preparedness using Chaos Engineering,"In the context of disaster recovery, we use Chaos Engineering to recreate or simulate the actual event. This gives us the opportunity to test our Disaster Recovery Plan and our response procedures in a controlled scenario, as opposed to recreating disaster-like conditions manually or waiting for a real disaster.  By solidifying your disaster recovery plan through chaos engineering, you can be confident that the next big AWS service event will have little to no impact on your customers. 

The agenda of this talk is:
What is DR and Why do I need it 
How to use chaos engineering to help with DR Planning
How to identify critical asset and create RPO/RTO
Fire drills and DR plan Confidence

 ",https://www.linkedin.com/in/kyleshelton5/,@chaoskyle55,,,,ce_kyle.png,,disaster recovery preparedness,
No,security,Feross Aboukhadijeh,Founder,Socket,,,,What's Really Going on Inside Your Node_Modules Folder,Do you know what’s really going on in your node_modules folder? Software supply chain attacks have exploded over the past year and they’re only accelerating in 2022 and beyond. We’ll dive into examples of recent supply chain attacks and what concrete steps you can take to protect your team from this emerging threat.,https://www.linkedin.com/in/feross,@feross,,,,ce_feross.png,,your node_modules folder,
No,security,Kim Carter,Lead Security Engineer,BinaryMist,,,,Purple Teaming with OWASP PurpleTeam,"#### What is OWASP PurpleTeam?

PurpleTeam is a Developer focussed security regression testing CLI and SaaS targeting Web applications and APIs.
The CLI is specifically targeted at sitting within your build pipelines but can also be run manually.
The SaaS that does the security testing of your applications and APIs can be deployed anywhere.

Kim will briefly discuss the four year journey that has brought PurpleTeam from a proof of concept (PoC) to a production ready Developer first security regression testing CLI and SaaS.

An overview of the NodeJS micro-services with many features allowing a Build User (DevSecOps practitioner) to customise their Test Runs without having to write any tests by simply configuring a Job file.
Allowing multiple options to deal with false/true positives.
Setting alert thresholds in multiple places and for multiple testers (app-tester, tls-tester, server-tester) allowing the Build User to define what constitutes a successful or failed Test Run.

#### Why would I want it in my build pipelines?

In this section Kim will discus the problems that PurpleTeam solves, such as training the Build User with advice and tips on security defects as you fix the defects that PurpleTeam highlights.
As well as the huge cost savings of finding and fixing your application and infrastructure security defects early (as you're introducing them) as opposed to late (weeks or months later with external penetration testing) or not at all.

#### OK, I want it, how do we/I set it up?

Kim will walk you through all of the components and how to get them set-up and configured

### Great, but what do the work flows look like and how do I use it?

Let's walk through the different ways PurpleTeam can be run and utilised, such as:

* Running purpleteam stand-alone (with UI)
* Running purpleteam from within your pipelines as a spawned sub process (headless: without UI)
* Running all of the PurpleTeam components, including debugging each and every one of them if and when the need arises",https://www.linkedin.com/in/carterkim,@binarymist,,,,ce_kim.png,,owasp purpleteam,
No,security,Kennedy Torkura,Co-Founder & CTO,Mitigant,,,,Defeating Ransomware Attacks with Security Chaos Engineering,"Due to the rapid increase of ransomware attacks in the last year, 2021 was tagged the  ""Golden Era of Ransomware"".  Most ransomware countermeasures recommend backups and runbooks. However, these techniques are seldom verified to ascertain the level of technical efficiency they provide. Furthermore, the human operators who use these ransomware countermeasures rarely have the opportunity to understand how to react to ransomware scenarios. A more effective way is by leveraging security chaos engineering to overcome the aforementioned shortcomings. By conducting planned experiments, ransomware countermeasures can be crafted as a hypothesis and proven. This approach enables security incident response teams to gain confidence in their technical and organizational skills as well as practice how to operate the ransomware countermeasures.",https://www.linkedin.com/in/aondona/,@run2obtain,,,,ce_kennedy.png,,defeating ransomware attacks,
No,tools,Lerna Ekmekcioglu,Senio Solutions Architect,AWS,Jack Iu,Solutions Architect,AWS,Multi Region Terraform Deployments with Terraform Built CI/CD on AWS,"Infrastructure as Code tools like Terraform enable organizations achieve repeatability of their deployments at scale in the cloud.  Organizations deploy into multiple AWS regions whether it’s to enhance user experience, satisfy data residency requirements or ensure business continuity.  

In this session, we do a deep dive into how to deploy infrastructure using Terraform in multiple regions in AWS and CI/CD pipelines built with Terraform.  We cover the overall architecture for the CI/CD pipeline and target workload accounts, walk through how to structure Terraform code for multi region deployments and go over best practice design considerations for the CI/CD pipeline.

The session is targeted for cloud teams who provision resources using Terraform in AWS including those in regulated industries where security and reliability are critical such as Financial Services and Healthcare.",https://www.linkedin.com/in/lerna,@lerna_,https://www.linkedin.com/in/jackiu/,,,ce_lerna_jack.png,,multi region terraform deployments with terraform built ci/cd on aws,
No,tools,Akram Riahi,LitmusChaos Leader,WeScale,,,,Chaos Engineering alongside Litmus and Jenkins,"Today, Chaos Engineering is becoming more and more prevalent, aiming for stronger resilience in information systems.
The questions about its implementation, integration and automation are numerous and arouse the interest of all!
In this conference, i am  going to show you how to integrate chaos engineering using Litmus 2 within jenkins pipeline and test process in order to promote a resilient  built application image to production and get notified by its chaos results via slack.",https://www.linkedin.com/in/akram-riahi-323892123,,,,,ce_akram.png,,chaos engineering alongside litmus and jenkins,
No,tools,Rain Leander,Developer Advocate,Cockroach Labs,,,,When Gremlins Play with Cockroaches: A Chaos Experiment,"What happens when Gremlins play with resilient cockroaches? Using Chaos Engineering to improve system resilience, Gremlin’s “Failure as a Service” makes it easy to find weaknesses in a system while CockroachDB is the SQL database for building global, scalable cloud services that survive disasters. While chaos engineering is about collecting, designing, implementing, orchestrating and scaling the faults in systems, CockroachDB is famously known to “survive anywhere”. This talk explores a series of Gremlin experiments to help CockroachDB evolve.",https://www.linkedin.com/in/rainleander,@rainleander,,,,ce_rain.png,,when gremlins play with cockroaches: a chaos experiment,
No,ai,Michele Dodic,SRE DevOps Engineer,Accenture,Souha Gallala,SRE Devops Senior Analyst,Accenture,Chaos Experiments under the lens of AIOps,"Imagine this: you’re a Site Reliability Engineer (SRE) at a major tech giant and you are responsible for the overall system health, which is running in prod. Numerous alerts, server crashes, Jira tickets, incidents and an avalanche of responsibilities, which sometimes simply feel like a ticking time bomb. These are just some of the daily struggles an average SRE needs to go through. But why should it be like that? Well, it shouldn't - thanks to a term coined by Gartner in 2016. AIOps, meet audience. Audience, meet AIOps.

Let's extend this scenario. On top of all of the above mentioned issues, our poor SRE needs to watch out for potential security breaches and make sure nothing ever gets in through the cracks. However, by conducting proactive experimenting, continuos verification and improvement, he makes sure that the system is able to withstand these turbulent and malicious times that we're living in. Do these notions ring any bells? They sure do! Chaos Engineering, meet audience. Audience, meet Chaos Engineering.

What's our angle, you're wondering? AIOps and CE are two concepts, which are often kept separate. In this talk, we will discuss (and show you!) how both practices combined can significantly increase cyber resiliency, while at the same time maintain full E2E transparency and observability of your entire system. 

For this session, we have prepared and analyzed several use cases, followed main principles, summarized best practices and prepared a live demo through a combination of CE and AIOps tools.

Above all, we are SRE Engineers. As such, during this session, we will stay close to the SRE principles and best practices that we used to achieve our goals, e.g. reduce organizational silos, measure everything, learn from failures, analyze changes holistically, etc... As we proceed with our talk, the audience will be able to identify how these are related to AIOps, as well as CE, and finally, how it all ties together.",https://www.linkedin.com/in/michele-dodic/,,https://www.linkedin.com/in/souha-gallala-805741152,,,ce_michele_souha.png,,chaos experiments under the lens of aiops,
No,ai,Soumen Chatterjee,Partner Solution Architecture,AWS,,,,Chaos Engineering: At the age of AI and ML,"AI is omnipresent in our every day; Embedded AI is embraced in almost all types of the business fabric. At this juncture, we are setting our course on the value frontier by reimagining how we operate and putting AI at the heart of everything we do. At the same time, the Adversarial Machine Learning evasions and poison attacks are introducing the complexities and new challenges in Model testing strategies. As we all see how chaos engineering has become a no-brainer testing approach, Adversarial ML techniques raises the unique question if ML Models Testing still benefits from Chaos Engineering; how do we adapt the chaos engineering strategies.
This session will guide you through building an approach of Chaos Engineering at the age of AI and ML.
",https://www.linkedin.com/in/soumenc/,,,,,ce_soumen.png,,chaos engineering: at the age of ai and ml,
No,lessons learned,Julie Gunderson,Reliability Advocate,Gremlin,,,,The Road to Reliability,"Over the years a lot of research has been conducted and many books have been written on how to improve the resilience of our software. This talk will dive deep into the three keep practices identified by the authors of Accelerate to improve reliability: Chaos Engineering, GameDays, and Disaster Recovery. We will discuss the key measures of tempo and stability, and how practicing Chaos Engineering will increase both. 

We will be walking through the Google Cloud open source Bank of Anthos application to illustrate why teams should focus on the customer experience and how to test for failures. 

Attendees will learn practical tips that you can put into action focused on resource consumption, capacity planning, region failover, decoupling services and deployment pain.
",https://www.linkedin.com/in/juliegunderson,@julie_gund,,,,ce_julie.png,,the road to reliability,
No,lessons learned,Alparslan Avci,Software Developer,Zapata Computing,,,,Distributed Transactions in Service Mesh,"As we go deeper into cloud-native applications, microservices are becoming a part of any developer’s life. Together with Kubernetes and service meshes, they became the de facto standard in the industry. However, one question arises with microservices: How to implement distributed transactions in such an environment?

In this talk, we will discuss distributed transaction methodologies, talk about real-life scenarios, and provide a hands-on resolution in the Istio service mesh using the Hazelcast application platform. The attendees will easily understand the distributed saga pattern, backend architecture, and the topology of the solutions with live demonstrations.",https://www.linkedin.com/in/alparslan-avci,,,,,ce_alparsian.png,,distributed transactions in service mesh,
No,lessons learned,Ignas Galvelis,Software Developer,Sky,,,,Application for Blockchain in Crowdsourcing Data,"Dealing with a crowd is never easy. We need to make our systems resilient against a number of attacks and they way we do it is utilise the crowd itself. We will talk about:  *   Chainlink oracles, 
*   distributed open source systems, 
*   incentivisation, 
*   application of machine learning 
*   and more.",https://www.linkedin.com/in/ignas-galvelis,@IGalvelis,,,,ce_ignas.png,,application for blockchain in crowdsourcing data,
No,lessons learned,Zach Wasserman,CTO,Fleet,,,,"Glitches in the Matrix, or Taming Agent Chaos","This talk is accessible to all levels, and will include specific examples from [Fleet's codebase](https://github.com/fleetdm/fleet/) that will engage even advanced practitioners. We will also touch on the capabilities in [osquery](https://github.com/osquery/osquery) that help to improve reliability.

Questions posed and answered from Fleet's experience include:

* Where to focus efforts when considering performance and chaos?
* How does Fleet engineer for resiliency when our users self-host the software and have hundreds of thousands of (potentially misbehaving) agents checking in for * coordination?
* What Go tooling can we use to improve monitoring of the Fleet server both during internal performance testing and while running in production?
* How do we collect information about the infrastructure dependencies (MySQL & Redis) to understand the entire system beyond the code we control?
* How can we ensure osquery agents don't bring down the endpoints they are deployed to protect?",https://www.linkedin.com/in/zacharywasserman,@thezachw,,,,ce_zach.png,,"glitches in the matrix, or taming agent chaos",
No,lessons learned,Amir Shaked,Senior VP R&D,PerimeterX,,,,Taking control over cloud costs,"The modern SaaS approach requires engineering to address a lot more of the financial element, a critical part of delivery and efficiency which was less common for engineering leadership pre-cloud and the PaaS era.
In high scale growth companies, the need to manage cloud costs properly is even greater, since an oversight can create an exponential decline in COGS and run-rate.
In the talk, we will cover how to optimize the FinOps approach over time, focusing on culture, process, and technology.
We will describe the processes and framework that can be applied to every major cloud vendor, and give specific examples and tools we’ve built on GCP to address the needs we had at PerimeterX",https://www.linkedin.com/in/amirshaked,@amirshaked,,,,ce_amir.png,,taking control over cloud costs,
No,lessons learned,Vishnu Vardhan Chikoti,Senior SRE Manager ,Fanatics,,,,Learnings from Chaos experiments,"During a Chaos experiments, the system might withstand the injected failure condition or a weakness might be exposed. This talk will cover what I have learnt after conducting chaos experiments on different types of applications and infrastructure.
- Commonly identified scenarios
- Best practices to handle the identified weakness
- Knowledge sharing with the results from the Chaos experiments",https://www.linkedin.com/in/vishnu-vardhan-chikoti-3763262,@vishnuvchikoti,,,,ce_vishnu.png,,utilising results from chaos experiments,
No,culture,Lesley Cordero,Senior Software Engineer,Teachers Pay Teachers,,,,Running GameDays at Different Scales,"#### Introduction

I'll start off with a shared definition of Chaos Engineering (""the discipline of experimenting on a system in order to build confidence in the system’s capability to withstand turbulent conditions in production.), provide brief context on why it's important. and expand on how this talk is about operating GameDays at different levels of a technical organization. 

#### Designing & Running an Experiment

This section will cover the general methodology of running an experiment. 

#### Step 1: Lay out assumptions

- We are constantly observing our systems. Every time we contribute to our codebases, debug a production failure, use our observability tools, etc. we’re building up our mental model for how our systems work. Over time, assumptions get built into that mental model. Sometimes these assumptions are accurate, but when they’re not, this leads to production incidents. 

- The best way to form a chaos engineering experiment is to ask the question, “what would happen if…?” and answer that question(s) with explanations as to why. The answers to these questions are your assumptions. 

#### Step 2: Define your priorities

- Given the laid out assumptions, I'll talk about how prioritization of which assumption to test first is important. I'll reference this Gremlin article [here](https://www.gremlin.com/community/tutorials/chaos-engineering-the-history-principles-and-practice/#which-chaos-engineering-experiments-do-you-perform-first). 

#### Step 3: Define experiment scenarios & tools

- I'll talk about how once an assumption is identified as highest priority, an accompanying plan will need to be tested, including details like hat scenario(s) would reproduce the questions you asked to define assumptions and which tool(s) will enable you to do that in a safe way. 

#### Step 4: Define your steady-state hypothesis

- I'll talk about the prereq of being able to define a steady state for a system so that you can measure impact of the experiment. 

#### Step 5: Finalize Experiment details

- By the end of this step, I'll review an example chaos engineering experiment template and highlight which final details needed to be completed, including: 

* Timeline/Agenda: When are you running the experiment? If there are multiple scenarios to be executed, what’s the ordering? When will the post-mortem be conducted? 
- Experiment roles - who will be doing what during the actual experiment? (List of roles can be found here)

#### Step 6: Run the experiment! 

I'll talk about what the actual experiment time should look like (including how it should resemble an actual production incident)

#### Step 7: Post-Mortem

- I'll talk about how even if the experiment runs perfectly smoothly, you should take the time to evaluate the outcome of the experiment and answer where and how we were right or wrong with our steady-state hypotheses. Much like typical incident reviews, there should also be a focus on determining root cause. This includes answering questions like, 

* How did the experiment scenarios impact our steady-state criteria? Did they impact them in the way I thought?
* Was there impact on our systems we didn’t foresee? Why was that?

#### Step 8: Action Items

Again, similar to incident reviews, action items should be stated clearly and assigned owners.

### Different GameDay Flavors

#### Team-specific GameDays

Starting off with a GameDay can be daunting, especially if the desire is to host one that is org-wide. While org-wide GameDays are a good long-term goal, there's no shame in starting smaller. I'll talk about how team GameDays can be used to test the process you've formulated and good recommend starting points for teams conducting their first GameDay. 

#### Org-wide GameDays

I'll cover my approach to org-wide GameDays, which happens in three parts over the course of 1 day:

1. Pre-Mortem: Pre-mortems are when you hypothesize what will happen when you run the experiment. This is done to test your existing mental models, which is why it should take place in a group setting. During the pre-mortem, the group should be answering questions like, how do you expect your services to perform? What impact on UX are you expecting? What impact on downstream or upstream services do you expect? etc. 
2. Pre-mortems can be done during the actual GameDay, or separately, but they should be done by the same folks executing the experiment. 
3. Actual Experiment: The actual execution of the experiment. 
4. Post-Mortem: Similar to normal post-mortems for incidents, we want to make the experiment actionable by talking about what vulnerabilities we might have uncovered. We want to learn about where our mental models went wrong and how can we use these learnings to redefine what we once thought we knew.

I'll also talk about effective ownership & collaboration models here. 
",https://www.linkedin.com/in/lesleycordero,@lesleyclovesyou,,,,ce_lesley.png,,running gamedays at different scales,
No,culture,Russ Miles,Engineering Manager,Segovia Technology,,,,The Importance of Being ... Empathetic,"The most important skill that you can learn to be successful in chaos engineering is not what faults to inject, nor is it even how to select the best experiments to learn from. It is... empathy, and the good news is that it's a skill you can learn today in this talk!

Empathy is rapidly being recognised as _the_ superpower of Successful Engineering Teams.

In this talk Russ Miles will share how you can unlock the skills of empathy to make yourself a better teammate, a better engineer, and how it underpins the developing of a culture of resilience through chaos engineering in your team and across your organisation. Through concrete examples you will discover the frameworks, tips and tricks to making empathy one of your strongest skills.

Regardless of your background, cognitive biases and differences, neural diversity or even political leanings… Empathy is a super power that you can embrace today that will help you in every aspect of your life, and this talk will show you how.",https://www.linkedin.com/in/russmiles,@russmiles,,,,ce_russ.png,,the importance of being ... empathetic,
No,culture,Fabricio Buzeto,CTO,bxblue,,,,Learning to overcome the choice paradox,"The focus of this talk resides in helping to diminish the anxiety of those working with software development. Here I focus on two aspects. The first, founded on learning and consolidating new ways of doing things. And the second on exploring and amplifying the options and contexts and how to apply them. These two aspects will be developed according to three levels of decision-making.

  

1.  Daily: Covering the small decisions that accumulate over time. From bad variable naming, small refactorings. It consists of those 30 minutes to one-hour detours that can make a difference (or not) ahead.
2.  Short-term: This consists of choosing the best tool for the job, learning how to calibrate and change the route midway, and avoiding investing more time than needed where you don't need to.
3.  Long-term: This topic is about the challenging art of prioritizing and choosing. Here I'll cover different ways your brain works and the main frameworks on making choices and feeling optimistic about it (even in the chance of making a bad one).

  

The most crucial point here is to allow the audience to have a healthy discussion about the uncertainty of results. I'll cover how our brain works, lay down the basic frameworks that enable us to not freeze in front of the choices ahead of us. And so, move forward, adapting and learning with the outcomes we face.",https://www.linkedin.com/in/fabriciobuzeto,@nukdf,,,,ce_fabricio.png,,learning to overcome the choice paradox,
No,culture,Arlemi Turpault,Senior Developer Advocate,Postman,,,,How to Win Friends and Influence People with API First,"Ever got into a (virtual) fist fight with your development team? Unclear requirements, repeated rework, and other miscommunications can really tax your working relationships. Learn how an API-first development approach can salvage your working relationships and increase the velocity of your development process. API-first design and development is an industry standard, and in this presentation, we will review tactics for using mock servers in designing, integrating, and sharing your applications.",https://www.linkedin.com/in/arlemi,@arlemi,,,,ce_arlemi.png,,how to win friends and influence people with api first,
No,culture,Bram Vogelaar,DevOps Cloud Engineer,The Factory,,,,Gamification of Chaos Testing,"Things like Infrastructure as Code and Service Discovery and Config Management can and have helped us to quickly build and rebuild infrastructure but we haven't nearly spend enough time to train our self to review, monitor and respond to outages. Does our platform degrade in a graceful way or what does a high cpu load really mean. 

In this talk we ll discuss the need for and the options of creating a game day culture. Where we as engineers not only write, maintain and operate our software platforms but actively pursue ways to learn and predict its (non-functional) behavior. We ll look tools like toxiproxy and the simian army for ways to prepare teams to tweak their testing and monitoring setup and work instructions to quickly observe, react and resolve problems.",https://www.linkedin.com/in/bram-vogelaar-2316578,@attachmentgenie,,,,ce_bram.png,,gamification of  chaos testing,