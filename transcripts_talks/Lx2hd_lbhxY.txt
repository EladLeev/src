Hello everyone. Welcome to this session. Deploy any applications to end clusters using our Go City application set. My name is Devon Ahmed. I'm a developer advocate at Red Hat. I'm from Molten New Brunswick, which is in the beautiful Atlantic region of Canada. I love everything Cloud Native with a focus on DevOps and GitHub tools. Besides work, I love to play pool and ping pong. I'm also a freelance carrier coach, where I help students and new grades to start a carrier in tech. In terms of today's agenda, we'll start with the idea of GitOps, especially GitOps on Kubernetes, and then we'll cover the popular open source tool, Argo City. The main focus of today's session is a sub-project of Argo City called application set. For both Argo City and application set, we'll have demos, and at the end, we'll have some time for Q&A. On the bottom right corner of your screen, you can see a link to a Git repository. In that repository, I have added detail instructions on how you can do demo on Kubernetes platform. The demo I'll be showing will be based on OpenShift platform. If you have a mobile device, you can also scan that QR code, which will take you right to that Git repository. Luis Vasira summarized GitOps in one slide. So GitOps in one slide means there are three main principles we use Git as the single source of truth. We use Git as the single place where we operate. That means not on trick clusters, but we do that on Git, and the change is reflected on the clusters. All the changes we're making will also be observable and verifiable. From the fun fact of GitOps, Alexis Richards and first came up with the term GitOps. And when he ran it by his friend, his friend thought it's the ugliest word he has ever heard. And now he can't unhaired it. Trying to come up with a term that is easy to pronounce, but also difficult to forget. Alexis knew that he came up with a perfect term. Now let's understand GitOps, BOM, or Bill of Material on Kubernetes. First, we need Git repo, where our application manifests files with your site. Then we need Kubernetes cluster or OpenShift cluster, where we're deploying our application. You might also need a manifest generation tool like customize if you have different environments, like Dev, Pre-Prod, and Prod. Don't use a CI server to orchestrate direct updates to Kubernetes as a set of CI jobs. There's a blog post mentioned in the resources section that covers why this is an anti-patternal. And finally, you'll need your Kubernetes manifest files, such as deployments and services. What is Argos CD? Argos CD is declarative. GitOps continues delivery tool for Kubernetes. It is part of an open source project. The project started at end-to-it, but then companies like IBM and Red Hat and various individual contributors have contributed to this project. From this diagram, the concept is pretty simple. Argos CD watches over your Git repository. And whatever change you have or whatever configuration you have in your Git repository, Argos CD implements that change declaratively on your clusters. Now, why Argos CD? The first few points have been covered in previous slides. Why we're using that in Argos CD. You use Git push for managing application, delivering including changes to your cluster. This is the floor. Your developers are familiar and comfortable with. In terms of security, Argos CD offers ESO's integration based on OIDC or OAuth2. And it offers a single sign-on for various social profiles as well, including GitHub, GitLab, LinkedIn, etc. In terms of authorization, Argos CD offers multi-tenancy and Arabic policies. Argos CD also offers audit trails for application events and API calls. In terms of rollbacks or roll-in, it's a matter of your application configuration committed in a Git repository. So you can go back to any specific commit that's in your Git repository, either using the UI or the CLI. If you do Argos CD active from your CLI, Argos CD performs a diff between a target and a large state of the cluster. And this can also be seen on the UI. So you can observe the drift of your application defined in Git and the state in the target cluster. Now considering all this, your application team will have a higher velocity for faster application delivery. Now, how Argos CD works? Argos CD is implemented as a Kubernetes controller, which continuously monitors running applications and compares the current live state against the desired target state, as specified in the Git repo. So on the left hand side, you can see our developer persona, Kutas Agate push, or after a PR merges, Argos CD reacts to that change. Our deployed application, whose live state deviates from the target state, is considered out of sync. The Argos CD API server is a GRPC and a REST server, which exposes the API consumed by the Web UI, CLI, and various CI-CD systems that we can see on the left hand side of our screen. On the blue square, you can see there is a repository server, which is an internal service that maintains a local cache of the Git repository holding the application manifests. Synchronization can be configured using resource hooks, so that what we see at the bottom of our screen hooks are ways to run scripts before, during, and after a sync operation. One example or use case of such hooks is if you want to orchestrate a complex deployment that requires more assertive sophistication than the Kubernetes rolling upgrade strategy, you can have a script run before, during or after the sync operation. All right, so that's enough theory. Let's dive into Argos CD demo. Hello everyone. The first demo is on Argos CD. This is based on OpenShift, but if you'd like to follow a Kubernetes based demo, you can go to this GitHub repository and follow the instructions. This is based on a vanilla Kubernetes. Come back to OpenShift. We are installing an operator called Red Hat OpenShift KITOPs operator. This operator out of the box gives you an Argos CD instance and also installs applications at controller under OpenShift.js KITOPs name space. To begin with, I'll connect to the Argos CD UI. For that, I'll need the default Argos CD admin password, and we can find that under workload secrets under OpenShift KITOPs project. OpenShift KITOPs cluster is the name of that secret. I'll copy the secret and open Argos CD UI from here. The username is admin, and password is what I just copied. Argos CD has a concept of projects. Now, this is not OpenShift project. This is Argos CD projects where you can group your deployment applications. You can also add repositories. If it's a private repository, you can connect using your password or token the repository, which we're using for this demo is a public repository, so we don't need to connect it in advance. So, let me copy the URL of the repository and try to create an application. You can create application from the Argos CD UI or using Argos CD CLI, but I'll create the application using the OpenShift console. So, under the operator, I'm going into application, create application. Let me give it a name or go sample app, and then under destination, I have to give the server location since it's the same server where Argos CD is installed. It's the default co-ordinate service, and the name space, I'll use OpenShift KITOPs. You can choose to deploy in a different name space, and if you add a label that says managed by OpenShift KITOPs, but for now, I'll use OpenShift KITOPs name space. Project is the Argos CD project, which is default. That's the only project that's on Argos CD server now, so only default projects. So, that's what we are using now. Under source, I'm adding the repository URL, which I just copied, and under path, customize ngnext OpenShift, so this is a path where my manifest files are. For sync options, or sync policy, which basically says that if you'd like to automatically sync the application when there is a change in Git repository, or you'd like to manually sync either clicking the sync button or using the CLI, I won't go with the automated route now, so it's manual sync option, and I'll create the application. Once I create on the Argos CD console, I can see the app was created. If I go inside, I can see that my three resources, the service deployment, and the route were created. If you look at the customization file, you can see that there's a name prefix prod dash, which you can see in all three resources as well. Now, once I sync the application, you'll see that the applications will sync, and it turns green, sync, sync, okay, and you see the single pod that is being created. Now, if I'd like to change the number of rectifiers from one to three, so let's see if I make a change, so going the GitHub route, making change to the Git repository, and seeing the change reflect on the cluster, I'm not going to go through the pull request route, I'll push directly to the main, so once I do that, if I hit refresh, I'll hover over here, see the sync options should turn yellow out of sync, so it's out of sync now, and I can click on app diff to see the difference what changed. So when I scroll down, you can see on line 110, so the number of replicas in the cluster is one, but in Git repository expected or desired is three, and hence the application is out of sync. All we need to do is hit sync, sync rise, and the application will go in sync. All right, so we can delete the application either from here, RGoCD UI, or we can delete the application from OpenShift Console as well, let's delete the application, I can create the same application, but let's use auto sync now, so I'll say this is RGoCD, sample app, let's say this is auto sync, destination, the same cluster and name space, same RGoCD project, under source, I'm using the same source that we used, customize ntnex OpenShift, and then under sync policy, I'll choose automated, and then hit the prone and self-heal, what prone does is if you delete a resource from Git, then RGoCD will delete that on the cluster as well, and if you check self-heal, then even if you delete a resource, direct down the cluster, RGoCD will not let your app go out of sync, and it will prevent your app from even seeing that change, so let's keep those two options and create the app again, and if we go back to application, so the new application was created, and this time if you notice, we didn't have to click sync, because it's auto sync, the app was automatically synced, so let's try to delete one of the resource to see the self-heal feature in action, so for that, login to the cluster, so login, and I'll switch to OpenShift GitHub, I'm already OpenShift GitHub's project, so let me check the service, OC, get service, what are the service I'm getting? Okay, I see the prod engine X, so let me delete that service, OC delete service, prod engine X, and then observe what happens on RGoCD UI, so if you observe here, you don't see the app going out of sync, and that's because even if we try to delete that resource, which you can see the service was in fact deleted, but it got recreated, even if you refresh, you see the service is right there, so RGoCD prevented that operation from happening, so that's the pretty powerful feature with auto sync and self-heal. We created the app using OpenShift Console, you can very much delete the app from RGoCD UI, it's using the same set of APIs, so it will be the same operation, and that's the demo with RGoCD, deploying a single application from single-get repository to a same cluster. In the next demo, we'll see RGoCD application set where we deploy application to multiple clusters. That was a demo of RGoCD where we deployed one application from a single-get repository to a single cluster, but what if you'd like to deploy RGoCD application to multiple Kubernetes clusters all at once, or what if you had multiple RGoCD applications in the same repository more like a monorepo. Introducing applications that controller for RGoCD, unlike with an RGoCD application resource, which deploys resources from a single-get repository to a single destination cluster or namespace, applications that uses templated automation to create, modify and manage multiple RGoCD applications simultaneously, which targets multiple destination clusters as namespaces. The applications that controller is installed alongside RGoCD within the same namespace, and it automatically generates RGoCD applications based on the contents of a new application set custom resource. On the right-hand side of the YAML, on line number two, you can see the kind being application set. Now generators are responsible for generating parameters, which are then rendered into the template field of the application set resource. We'll be seeing two of the generators in the demo today. In this example, the list generator passes the URL and the cluster fields into the application template as parameters, which are then rendered into the template as three corresponding RGoCD applications, one for the each defined cluster. Targeting new clusters is simply a matter of adding new elements to the application set resource and the corresponding RGoCD applications will be automatically created. The sole responsibility of the application set controller is to create, update and delete applications resources within the RGoCD namespace. The controls only job to ensure that the application resources remain consistent with the defined declarative applications set resource. Nothing more. Does the applications set controller does not create modifier delete Kubernetes resources other than the application custom resource. Does not connect to cluster. You have to already have the clusters defined in RGoCD. Applics as set controller does not interact with namespaces other than the one where RGoCD is deployed. It is RGoCD itself that is responsible for the actual deployment of the generated child application resources, such as deployment services and config maps. The applications that controller can thus be thought of as an application factory taking an application set resource as input and outputting one or more RGoCD application resources that correspond to the parameters of that set. Here are some of the generators for application set. You can find more details of each of the generator from application set read the docs page which are in the resources section of the GitHub repo which I showed earlier. All right, again enough theory and let's dive into application set demo. In the second part of the demo we'll use RGoCD application set to deploy applications to multiple clusters. Let's look at the example. In these examples we have some generators but specifically we'll use a list generator and get a generator directory example. Let's start with list generator. Now in the list example we have a custom resource with kind application set and we're trying to deploy this particular application, RGoCD application into two clusters entering depth and entering fraud. I'll be commenting out this entering fraud resource pretty soon. Now one thing to notice is the template part is pretty similar to an RGoCD application and here some values are dynamically injected. For example cluster these values would be coming from here and here and URL value is coming from the URL fields. One other thing to notice is if you don't have a namespace you can use this flag create namespace equals true for the remote clusters. The first thing to do when you're deploying to multiple clusters is adding the remote cluster to RGoCD server. First, I'll log into RGoCD server. Let me go to RGoCD server by clicking this link which will open up the RGoCD server UI and we'll use the default admin account with the admin password. To find the admin password if you're using OpenShift go to Workloads secrets from OpenShift GitHub's namespace and you can get the OpenShift GitHub's cluster secret and that will get us logged in to the cluster. From here if I see clusters I can see there's only in cluster that means where RGoCD is installed. There's no remote cluster added. In order to add the remote cluster we have to add it to RGoCD CLI. You cannot do that from RGoCD UI. So let me log in to RGoCD server from the command line. I'll do RGoCD login and RGoCD is pretty picky about the URL so I have to remove the HTTPS part. The username admin and the default admin password. All right so once you're logged in we have to log into the remote cluster. So this is the remote cluster. Let me see if I'm already logged into the remote cluster. Yes I'm already logged in to the remote cluster as you can see. So we have to add this context RGoCD cluster add and then context. So once we do that a service account cluster role and cluster role binding will be created in the remote cluster that will let RGoCD manage resources in the remote clusters. I already executed this step beforehand that's why it says it already exists. Let's start with the list generator example. In this email we have an application set custom resource called guestbook and will be deploying this application in two different cluster. One is the cluster where RGoCD is installed and the other is a remote server along command these two lines shortly. So let's copy this email. You can create this application set custom resource from the CLI but if you're using OpenShifted Apps operator you can create this application set from the console. So we create create application set the button and we paste the custom resource here. Now one thing to notice is because you're creating this application set in a different namespace than OpenShifted Apps guestbook.ns. You'll have to create this guestbook.ns namespace beforehand with a specific flag called managed by OpenShifted Apps. So here's that namespace. You have to create this beforehand. Notice this specific label feed. So I have already done that. I'll uncomment these two lines and also update the URL feed. So let me go back to my RGoCD web UI and from cluster I can see the remote cluster and I'll update this value which is engineering prod and let's create this application set. So once I create that under application I can see these two RGoCD applications are being created. One in the cluster where RGoCD is deployed and one is in the remote cluster. Both apps are synced and healthy. If I try to delete one of these applications you'll see that the applications will be out of sync shortly but applications that controller will recreate this application because these applications were created using applications set. In contrast if you created these two applications separately using RGoCD you would be able to individually delete these applications. So that was the list generator example demo. I'll delete these two applications from here using application set and now I'll be able to delete this application. Now let's look at the kit generator directory examples. For the gate generator directory I have three different applications in the same repository. A help, a gson, and a customized application and similarly I have to create three namespaces beforehand because these three applications are being deployed in three different namespace. So this specific label allows open-sheet kit apps or RGoCD to be able to create resources in those namespaces. I have already created these three namespaces. Now let's look at the example for kit generator directory. I'll create application set from the m view and here you can see that path.base name which is all the folders in this specific path which are the three applications guestbook, Helm, guestbook, json, and guestbook, customize. These will be the three RGoCD applications that will be created. Let's create this application set and we should see three applications will be created. I did not have auto sync enabled for this re-application so I'll have to manually sync this application. Once I click on sync, these three applications should be synced. Similarly to the previous example if I try to manually delete any of this application RGoCD applications that will create this application again. So that was the demo for kit generator directory and you can practice on your own on these generators or if you want to test out matrix generator, that example is in this repo or a RGoCD application set demo and if you scroll down to the bottom of the page, you can see that I have an example for matrix generator as well and this repository is based on vanilla Kubernetes. To sum up in this talk, we learned about the concept of kidops. More specifically, kidops on Kubernetes. We talked about at the clarity of kidops tool for Kubernetes, RGoCD. We deployed an application from a single repository to a single cluster and then mentioned on the challenges of deploying multiple applications to multiple clusters. We talked about RGoCD application set which is a subproject of RGoCD and how we can use various generators within application set to deploy a number of applications to a number of clusters. Thank you for your time today to listen to my talk and I welcome any questions you have.