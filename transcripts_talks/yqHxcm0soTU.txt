Hello everybody. My name is Mike Haberman. I am the co-founder of the City of the Spector. I am here to talk with you about distributed tracing for Node.js using open telemetry. Even if you don't know distributed tracing or open telemetry, this is exactly what I'm going to focus, whether they are working. To be honest, you don't have to be super-experienced with specifically with Node.js but we'll focus mostly on the distributed tracing and open telemetry part. So why did I choose to speak about this topic? So I've been working with distributed applications for the past five years. And as you can assume, distributed tracing are related to distributed systems. So distributed systems and microservices, something I've done a lot. And for the past two years, I've been doing almost only open telemetry. So I kind of know this space and I wanted to share my experience with you. So let's get started. So we will start by understanding why do we need traces. I'll give you like a good, real use case, real example of when you need traces. We have all sort of solution that we're working with today, things like logs, like metrics. Why do we need another one? Once we will understand why do we need it? We will learn what they are, how they work and how to actually implement them. So let me give you an example. So you're working and it is distributed environment and you have a scenario where service A cannot write to a database. And let's assume that you know that from having logs sending you an exception and learned saying, hey, service A can write to DB1. So let's you know think together what can you think to do it that point, right? You have a service that is not able to write to a database. This is probably a high priority thing. We may have this the loss, most likely the user. I don't know to sing it. How are you going to understand what's happening there? So if you look at logs, they most likely are going to point you to a specific location in your code. So you'll have a line of code trying the exception and then you'll go to that code base and you would find out which other lines led to this specific line. So you kind of play this game where you're trying to go through the different files, different components of your code and try to figure out what could lead to this specific exception. Or you are more of a metric guy and you would go and say, okay, let's see what's currently happening in in DB1. Maybe maybe you're high CPU, maybe degrees. I don't know an IOPS issue. Maybe you're going to have some metric telling you about it. And maybe it's just an increasing traffic, right? Maybe I just have way more requests to service it. And then I do ask myself, well, what endpoints in service A are actually causing a query to DB1. And then I may ask myself, maybe it's not an HTTP call. Maybe it's, I don't know, a Kafka message that being sent. So this is kind of the thought process we are going through when you have issues in micro services in distributed environment. Let's try to illustrate that a bit. So we have service A and we have DB1. We know that. But we don't know that maybe we have two services producing messages just to service A. And then the question is, does only the communication between service B and service A are causing this issue? Or maybe it's service C or maybe it's it's both. So in some way, when we looked at logs went to our code base and started to go through the path that the code execution took, it's very efficient to do it within one process within one service. Talking about multiple services, it's hard to do it. It's hard to jump between services and understand how they interact with one another. This is basically traces. So log told us, hey, this is the situation of specific process, a specific service. And this service is unable to do a specific action in a line of code. The metric told us kind of the overall situation of the system. It told us that maybe DB1 had high CPU. The trace is telling us the story between the services is telling us what is the path that this specific API called took. Maybe it was service B service A DB1 and it kind of gave us the context between the service within the entire system. So we're probably going to say that we need all three. We need logs, metrics and traces in order to understand how an incident occurs. So let me give you a quick look how a trace could look like. So here you can see a system that present traces and you can see right here that we are all this trace is starting from an API call to slasper just older in order service. And the next thing that is happening is that we are calling slasperify new service. Then we are able to have this API call to an external to external API. Then we have some save interaction followed by another service that writes to the database and eventually a Kafka message is being produced. So I have this view kind of telling me the map and this view kind of telling me the timeline what happened in pilot what happened in sequence. And you know as expected you could click something and then get the overall data. What was the case? What was the response? And if we're talking particularly about Kafka messages as I mentioned before. It may not be an HTTP. It may be some messaging protocol such as Kafka. Then you want to be able to kind of correlate between both the produced and the consume. So basically for me it trace it's mostly this view. It's this tree view a child parent relation that kind of tell you your request started at this point. Then it got to service a or older service and then the user service and basically this is going to tell me what were the interaction between the different services. So that's raising for me. That's the ability to see a particular API being propagated throughout the different services. And this is kind of magic thing and the way that it works and that's I think very interesting from you know development point of view. So open telemetry. This is the standard way to collect traces open telemetry can collect other stuff like metrics and logs but mostly it most mature in traces. And this is an SDK. Maybe so before I'm starting to explain what it is open telemetry is an open source project. Of course under the CNCF the cloud native compute foundation. This is the foundation that is also responsible for Kubernetes for instance. So it's in good hands. So the process goes that you implement an SDK within the code and within your process within your microservice. And then this open telemetry is going to collect the data and collect the traces and then ship them somewhere so that you'll be able to have this review. And this is going to be a parent child relation between all the different hops. And as you can see here we have service a service being a database. You can see that both services have opened telemetry installed in them. And by this I mean we took the open telemetry SDK and we actually installed it within the service. And what happens is that when service a service be are communicating. So it's very easy like logs for service a to just report what happened for service be to just report what happened. But we don't want just the report of the event that hey I got an API call and want something a bit more sophisticated. I want to know that when service be is being invoked it was invoked by service a for that what open telemetry is doing is when you send an API call between service a to service be open telemetry is going to inject the open telemetry context. What that means it means that when service a sending an API call to service be is going to live like a bread comp that is going to say hey I was the one that sent you this message. So when you're reporting whatever happened with the in service be please report it as a child of what happened in service a. So all of those are going to be shipped into a backend and that's called tracing bacon bacon for simplicity purposes and let's see what is being reported. So service a is going to say hey I sent an API call to service be and it's going to say this is trace ID number one so every trace has its own ID and every interaction every. So we're going to pop between services any action taken within the spend within the trace we are going to refer as a spend so here we're just reporting this is that spend ID one a 55 and we don't have any parent because this is the root. So the five spend ID is going to be injected into the headers sent to service be and service be is going to say hey I got an API call service from service a it is still the trace ID one I'm spending 66 but I have a parent and unlike it written in the presentation data mistake I do have the parent and the parent is 55. And then when spend this spend reported by service be that it's writing to the DBO it's querying the DB then again we have the same ID where's reporting what trace ID and who is our parent and by reporting this parent child structure eventually we're able to render in the UI how this trace looks and this nice review that we saw earlier. Okay so this is how it works so when do you use it we use it mostly when we have production issues we can use it in other places as well we can we can use it while developing we can use it in our staging environment but mostly production I actually wrote a cool open source to how to use traces in your testing like how to utilize traces in your testing. So you can do a lot of stuff but the most common use case would be how am I understanding what fail how do I improve something that works slowly how do I understand whether the system is going is working is expected or not so that would be the common use case but if I try to give it like a bigger name I would say that we're trying to improve our empty T.R. empty T.R. being mean time to resolve or recover or something starting with R admitting that the problem no no no no log in exists so that's what we're trying to do we're trying to solve things faster and by having this cool image telling us how service A called B is called C and we have this specific indication where the L will happen and what led to that that's what going to help us do. Solving very fast. So I'm I'm smoking quite a lot and I really want to show you how it looks in real code like what do I need to do in order to have open to let me be implemented in my code tomorrow. So let me give you a quick look. So here I have two services I have my user service and the user service is doing something really really simple but let's start with the item service the item service is has a slash data endpoint. And what it basically is doing it's calling the slash user service that we saw in one second ago and we're responding that data so the slash item is calling slash user and then response the data if something doesn't work in slash data so for instance if I'll put in my query string fail what will happen is I will respond with an error and you can see here that. I did two like specific things around open telemetry which I'll explain in in a second the user service is also doing a very simple thing it's gets an API call it communicate with some mocking solution randomize a number according to the length of the array that we got. We are reporting to open telemetry this number and then we're just responding that and that's all good both services are importing a file called tracer and just provide the name user service and also the item services doing the same so basically that's all you need to do when it comes to open open telemetry you just need to have a single file you will see. The installation of it is quite simple the code within the tracer and that is it all the other interaction that I show you they're not mandatory but you can definitely go ahead and add them if you would like to. So that tracer the tracer is actually very very simple so basically open telemetry collect what's happening your service and then going to send it somewhere so the eager exporter it's going to export data to the year we will see you in a second it's an open source tool that can visualize traces so it's got either it can be something you spun up locally or some production and point that you're using all you're choosing to use a fan door and then you'll get a bit more feature than the eager and you don't need to operate eager by yourself. So we're telling open telemetry where you're going to send the traces then we're going to tell open telemetry when you send those traces your those spends to be more accurate please indicate that this is the service name so we will be able to distinguish between services. So this is a few kind of generic setups here you're specifying what kind of libraries you want to to be able to instrument to collect data from so here I went with a simple list of HTTP express but you can have a lot of a lot of other types of instrumentation like Kafka, Mongo, ready, CWS SDK. You name it most likely there is an instrumentation for your new basic instrumentation means please collect data from this library so here's specifically we're talking about the HTTP library then the node native HTTP and the express one. This is it this is all there is to it everything you would see is going to work based on that I'm running two services I did yarn users to start the user service and I did yarn items to start the item service. So let's go and have a quick look what happens when I'm sending an API call to slash data so I'll go to the other this is the other and let me fetch the latest traces so this is happened right now and when I'm clicking on it you can see that we actually called slash data and you can see this is under the data. So we have the item service and then we communicated with with the user service so you can see here that we sent an API call to our mocking service and you can see everything that you would like to see that is going to tell you what really happened now this trace is quite it's not that interesting because everything is local. But this is how you'll be able to debug whatever is working or not working you do remember I like make me even show you that they can so in slush user we're getting an array and we are randomizing a number so if I got someone from a breed let me refresh that so now I got somebody from long done and I want to understand why this thing happened. Why this data was randomized so what they did here I got the current span the active span actually handling this code and I just wrote the note hey a number was randomized and this was the number that I randomized and you can actually see it right here and this is kind of a log right this is kind of allow me to send logs within my traces. So it kind of putting them together I can see what happened between services but that you can also attach to those spans what happened within the service. So this is a cool trick that you can use if you want to use like an event it's very very useful now let's do something else let's make it fail. So when I am running a twisting fail let's look at the code when I have a query string fail and throwing a really bad role and I am doing a very interesting thing so what I'm doing here is I'm fetching the current span and in my logger in my console I'm actually writing what is the current trace ID. So assume that you have in your production environment you probably have some log solution something like kibana or so and you have an exception now that's cool but now I want to visualize this specific exception not only loads but also in traces. So you can see here and I'm pretty critical yellow and here I have my critical yellow and I have my trace ID so I'm going to copy that go back to the egg and just paste it right here and now I can see the specific of this yellow and I can see all the different things that related to this specific action. So we are kind of tying together the logs with the traces so we'll be able to jump with one with one another so you can add to your logs the current trace ID or spin ID and you can also add to your span something similar to logs. So this is all you need to do this is everything there is to know and I would urge you to kind of go and try because it's a really you know simple line of code that you can start and get started and see what you're getting from that from it. If you are interested the code examples are available right here so in github, a spectroio open telemetry bootcamp you can grab the first episode of the bootcamp that almost exactly the show that they showed you and that will get you started with open telemetry quite fast. So I really hope you enjoyed this talk and if you have any questions feel free to reach out and best of luck with the having traces.