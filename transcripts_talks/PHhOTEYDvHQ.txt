Okay, hi, I'm Matt Harrison and we're going to be talking about idiomatic pandas. I'm excited to be here today. I run a company called Metastake that is corporate training, consulting and Python and data science. Some of the reasons you might want to listen to me talking about pandas is because I've written a couple books on pandas. I wrote the one point x cookbook. I also wrote learning the pandas library and I have this machine learning pocket reference that also uses a lot of pandas as well. In addition, I've taught pandas thousands of people over the years and used it for various consulting projects. So I've seen it basically since it was released and used it from about that time as well. I've seen a lot of good pandas code and bad pandas code. So what I want to do today is talk about some ways that you can make your pandas code better. Let me just share my notebook here and here's the outline of what we're going to do. We're going to be looking at loading our data really quickly then we'll talk about using the correct types for your data. We'll talk about chaining, mutation, application or the apply method and finally aggregation. Okay, I'm going to do that. My imports here at the top. We're going to be using numpy and pandas. We are using pandas version 1.2.3. I'm going to set some options here just so I can see a few more rows in the default rows and reload our data set. This data set is from fueleconomy.gov. It's got information about cars that were sold in the US. I believe from 1984 to 2018 or so information about them. So it looks something like this and it's got quite a bit of data in it. So we've got 40,000 rows and 83 columns of data about this. So this is a nice little data set to let us explore. Various features has different types in these data sets as well. So if you look at the columns in here, here's some of the columns. We have like how many barrels of gas does it car consume in a year? What's the city mileage highway mileage? Engine description, the drive. So a bunch of good information about cars. Okay, so that's the data. Let's dive in. So the first part here is using the right types. So getting the right types and pandas can be a challenge. But if you can do that, it's going to save your memory. It's going to make your code easier to work with and it can save you computation time as well. I'm just going to limit what we're going to do here to a few columns. So again, we had 83 columns in that data set. I don't want all 83 columns here. So what I'm going to do is pull off the columns on and look at and look at the D types attribute. This is going to give me a series that has the columns in the index and the values for those attributes on the right hand side there. As you can see, I've got a bunch of n64, some float 64, some objects, and those are the types. One thing you might want to do is you might want to look at how much memory this is using. So here's our memory usage for each column. If we sum that up, we can see that this is the amount of memory we're using. Just look at that number at around 19 blah blah blah. So I'll just say it's 19 right now. 19 megabytes and we're going to basically try and lower that if we can. So the first thing is considering what are integer columns. So again, I can look at some of the integer columns here. So I'm going to use the select D types pass an int into it and then chain that with the describe method here. And here are the integer types that are currently on this data frame that I have. We've got city mileage combined mileage, highway mileage, fuel cost, the range, and the year. So fuel cost is how much you would spend on fuel. Range is how far an electric car will go. And city combined and highway, that's miles per gallon. How far these cars go? How many miles they go with a gallon of gas or petroleum? And then year, that's the year the car was released. Okay, so first of all, I probably wouldn't write this code like this. I would write this in this chain style right here. So you'll see throughout the course today that I do a lot of this chaining and I just think it makes it easier to read. So you can see that first I'm going to take the autos, then I'm going to pull off the columns and I'm going to select the types and then finally I'm going to describe that. It's a few more lines we're displaying into a few more lines and we're also wrapping it with princes, which allows us to put it on multiple lines. Now one thing you want to do when you do this describe is you want to look at the man and max values because that can indicate what types you can use for integers by default, pandas are going to use this lowercase in 64 type. And you might want to use a different type. So there are other types in here. Let's just show some of them. For example, this one down here and int 8, if you use that type, you can go from negative 128 to 127. So if we look at our values here, for example, highway mileage, we might get away with using this int 8. There's also an int 16 here. An int 16 goes up to 32,000. So it looks like all of our values there would support being changed to int 16 instead. So what we can do is we can stick in this as type call here into our chain and then in select detives, we'll just select int and int 8. And you can see that this is now showing us what is either an int or an int 8. It's not showing us the int 16 there, but we did. Okay, so if you're not familiar with this as type, what this is doing, here's the documentation. Basically, you can pass in this detype here or you can pass in a dictionary of type. So what we're doing above with this as type is we're passing an dictionary with the columns and the types that we want those columns to be so you can see that highway, we're casting that to an int 8 city and combined 08, we're passing, we're casting those or changing those to in 16. That's why city and city and combined don't show up in this because when you said select detives, we did not select the in 16 D type. Okay, so here I'm just flushing this out a little bit more. I'm filling in range and year as well, not that I'm saying now select detives with the string integer. And if we do that, this is going to pull off anything that's into your S and so it will get all those types in there. This is looking okay. We haven't lost any precision by casting our integer columns to these other columns. We still have the same values in them, but now we should be using a lot less memory to do that. Let's just look at the memory here and now we're using 18 instead of 19. So we've got a little bit of savings in that. Let's look at floating point numbers. So I'm going to say let's describe what's a float and we've got cylinders and displacement in there. So I'm going to jump into cylinders. One thing that strikes out at me is I look at this is that cylinders actually looks into your like. If you look at the values that came up there, those look very integer-esque. What I'm going to do is just stick a describe on here to look at cylinders. And this is confirming my suspicion that we have the min and the quartiles there that are all home numbers. So why is that being represented as a float instead of an integer? That might be the question that pops up in your mind. Hopefully it is. Well, one thing you can do is you can use this value counts call. And if I do that and say drop NA is equal to faults, we can see why this is represented as a float. And the reason why is because in pandas, the lower case in 64 type does not have support for missing numbers. And you can see right here there is 206 entries here where the cylinders are missing. These are cars that do not have cylinders in them. So you might want to dive into that and figure out why those cylinders are missing. So this code right here will just say let's query at and I want to query where cylinders is NA. And so these are the cars where cylinders are missing. And it looks like these are mostly if you look in this model column right here. It looks like these are mostly electric cars, which makes sense. I'm not a super car buff, but apparently electric motors do not have cylinders in them. So at this point, we need to figure out what we want to do with that cylinders column. So what I'm going to do is I'm going to make an executive decision rather than having missing numbers in there. I'm just going to say fill in A with this little operation here. And then with that, I should be able to cast it to an int 8. Also with this displacement here, I'm going to do a similar thing. I'm going to cast that or not. I'm going to cast it. I'm going to fill that in with empty values here. Let's just look at the integer types after we do that. And we should see that cylinders pops up here as an integer type. And the lowest value for a cylinder is zero. That's right here. And the highest value is 16. Okay, so we've converted cylinder successfully from a floating point to an integer. Let's see what else we have. We can also if we want to dive into these float types, the default float type is float 64. If we wanted to look at like a float 16, we can do this numpy call here to see what the minimum and maximum values for like a float 16 can be. And if we can support that in here. So this looks like the displacement. We should be able to use a float 16 for that. I'm going to stick that in there and we'll have a float 16 for that. Now, let's look at our memory usage. Now after we've done that, our memory usage is now 17.5. So again, we saved a little bit more memory. We started at 19 and we've gone down a little bit. The next type that I want to look at is object types. Pandas supports these native float and integer types. And basically it's allocating a block of memory in the specific type. So if it's an int, it's going to use eight bits of memory for each integer rather than a whole Python object. But pandas also allows you to stick Python objects into each cell there. Now when you're doing that, you're going to use a lot more memory because pandas objects take up more memory. You're also going to lose speed. And so you're sort of executing things at the Python level rather than pandas optimized, basically see speed if you stay at those low levels here. So let's look at what is object types and these columns we care about. We've got drive engine description, make model transmission and created on. So generally when you read a CSV file, an object represents string data, though it could also represent numeric data that had string-esque values in it. You can also see that created on looks like a date, but by default read CSV does not convert that into a date. Now, I like to generally treat string types as basically a few types. One is categorical. And what do I mean by categorical? Well, if you look at like make, there's only a few different makes in there. So that could be categorical model. On the other hand, is more freeform. There's going to be basically for every make. There'll be a couple models for every year. So it's kind of a gray area. It's a little freeform. It could be categorical S, but it's going to have a higher cardinality, a higher number of unique values. Transmission looks categorical created on. I mean, you could say it's categorical, but it's actually a date type. I mean, these entries all look the same. So because there's so many entries that look the same, you might think that's categorical. But I'm going to convert that to a date because then I can leverage pandas date operations on that. We also have this engine DSCR, which is the engine description, which looks like it's got parentheses and strings inside of it. We'll dig into that a little bit more. And then we have drive, which looks categorical as well. So we kind of need to figure out what we want to do with these. If we want to keep them as strings, if we convert them to categorical types, we'll save memory there. If we convert them to dates, we can be able to do date operations on them. Let's just start off with drive. So what I like to do when I come across a string column is stick it into this value counts call. That's going to give me the frequency. It let me evaluate whether it is indeed categorical. And again, I put in that drop NA is equal to faults to see if there are missing values in there. So these are the entries, the unique entries for drive. And I would call that this is categorical. Now it's interesting that again, there are some missing values in there. NAN is used to represent missing values. So small aside, an object type in pandas can mean string, but it can also mean a mixed type. So in this case, drive is actually a mixed type. It's actually strings with floating points. It's using NANs. Floating points are representing those NANs there for those missing values. So let's just dive into that. Like where is the drive missing? I'm going to do this little call here. Query drive is an A. Looks like a lot of these are electric, but some of them are not. Nissan 300. I'm not sure why the drive is missing from that. There's some core vets where the drive is missing. So is this front wheel drive or rear wheel drive? Why is this missing? Again, I'm not a car expert per se, but if you talk to a subject matter expert, they could probably give you some insight into figuring out the appropriate label for those various types. What I'm going to do, given that I'm not really subject matter expert here, I'm going to say, let's just fill in everything else with the string empty or not empty other. And we're going to convert it to a category type. So I'm going to say as type category, let's just look at the memory after you do that. And look at that. Just by changing that type there, well, we also did change the make type down here. So you can see that we changed the make types. We changed basically two types drive and make we went from, I believe we're around 17. We've now dropped into 12. So categories are a way to save a lot of memory in pandas. Why do we care about saving memory in pandas? Again, I talked about that before. It might make things run faster, but the other thing is that pandas is in memory tool. So in order to use pandas, you need to be able to load your data in memory. So if you're sort of pushing that edge where you're running out of memory, using the right types can allow you to process more data. Okay, let's jump into Tranny. Again, value counts is my go to here. Here's the call for value counts. And it looks like there are a bunch of entries in there. Does it say at the bottom here? Sometimes it will say how many rows there are, but you know, I'm going to estimate there's around 30 different entries in here. So is this categorical? Maybe. I mean, it looks a little freeform too. There's these parent-thedicals in there. I'm not quite sure what like the S4 versus the S7 versus the AMS7 means per say. But to me as sort of a novice, one thing that sort of sticks out is this is whether it's automatic or manual. The other thing is there's this number in there, right? So those are both things that might stick out that I might want to keep track of in this data set. So what I'm going to do is I'm going to put a new column here called automatic. And that's just whether it contains automatic in there. And then I'm going to put another new column here called speed. And that's just using this stirr extract, which is passing in a regular expression here and pulling out that value. If they're missing values, I'm sticking in the string 20 instead. It looks like the ones that are missing if you dive into that are probably like variable speed. So I'm just putting 20 in as a large number for speeds. And then I'm casting that as an int 8. So if we do that, we look at our memory. Our memory now has gone down from 19 to 10 weeks. So we've almost halfed our memory by doing these little operations here. Note that I also now have a few more columns that might have more interesting information. Is there more that you could do with this column? The tranny column? Yeah, there might be, right? So again, talking to a subject matter expert might reveal more insight other features that you can create for your data. Let's look at the date columns here. So I do have this created on column. So what I'm going to do is I'm going to run this two date time call. This is a function in pandas. And then I'm going to call this tz localize down here because it looked like it was in the eastern time zone. So let's just run that. And I get a warning here. It complains about that. But if you look at my memory usage, again, I'm going down quite a bit. I'm now at 7,000 here. So this warning here is due to a Python issue where Python doesn't understand this EDT or EST in there. So with this little change here, replacing EDT and EST by these numbers. When I rerun this here, I don't have any issues. Now in this case, I'm pulling off the value counts here and just looking at the engine description value counts here. But let's just drop the engine description. So that's what I'll do down here because this looks like freeform. But before I drop it, I'm going to add this little thing here. Does it contain FFS? Again, I'm not at pro at these engine things, but looks like there's a bunch of FFS in there. So that might be something that I want to stick in there. So this is an example of just making a new column whether it contains FFS or not. So let's run that. We're now down to 8. So we're almost, we're not quite a third of our original data size, but we're getting close to that. Okay, so this is kind of nice. Now one thing you may or may not have noticed is that I've just sort of built this up as we've been going along and Jupiter makes it really nice to do that. What I like to do now is I like to make a function that will take my raw data that I've read directly out of a CSV file. And I generally just call it tweak, whatever the name is. I pass in my raw data here and I just apply all these chains right here. And this gives me my cleaned up data. So this is really nice to have a function like this in one place in one cell. Generally, I'll put it right after the load at the top. Then when I want to revisit my data, I know that I can load the raw data and I can immediately get my cleaned up data. This also allows for what's called data provenance where I can track through and see what happened where every row is coming from. Generally, your pointy head boss after you do an analysis or calculation is going to ask you to explain it. And if you don't have this province where you can come back to the original data, that might be problematic and bite you. So this is what I like to do. And this should read pretty clearly here. I'm taking autos. I'm pulling out these columns. I'm signing a cylinders column, displacement column, drive column, automatic column, speeds column, created on column and an FSS. I'm changing the types for various ones of these and I'm dropping these columns. So this sort of reads like a recipe. Do these steps and then you'll have your cleaned up data. Okay. So that is my first thing to do with pandas is you probably want to look at your data and make sure that your types are correct. The next section here is to talk about chains. So chains are something that I actually just showed you. Sometimes this is called flow programming. But what we're doing is we're leveraging the fact that in pandas, pandas generally does not mutate data, rather it returns new data frames. And so we're just going to take the new data frame and do operations on that. So on that note, because I said pandas in memory tool, you want to have three to 10 times the amount of memory as the data frame you're working with because you need to have some overhead for doing these intermediate operations. I know some of you who probably use pandas are like no, Matt, that's not the way to do it. There's an in place parameter that you can use on various methods and pandas will do things in place. Yeah. There is an in place parameter. However, the in place parameter generally doesn't do things in place as you might think it does. So I do have a hint here. If you find some code that you can't change, sometimes I like to stick in a pipe call. Pipe is a nice way to inline any arbitrary code. Basically, you can stick in a function that takes a data frame and returns a data frame for you. So this is my chain here. Now, I think this actually reads very cleanly. I just walked through it in the last section here. Now compare this with this is what I generally see in pandas code in the wild. So basically, I've taken just these few lines here. And I split them out into something that I'll see like this. I'll see something like this. Making all these intermediate variables, changing them, and then sticking them back in. And I'm not even doing that for the rest of the code. So the rest of the code, I'm just sort of doing a not even making the intermediate variables, but just assigning these directly. And generally, again, I wouldn't see that. But just for the sake of like typing this in as an example, you could imagine that generally, what I see when I'm working with students or clients is that this code here is going to be like three times as long as it is here. So contrast that with this, which I think is really easy to read, it's easy to step through and is going to make your code in life a lot easier. Now let's just run this here. And actually, when you run it, you'll notice all these little errors here that's sort of the bane of pandas people everywhere, the setting with warning copy. You'll note that I didn't run into that at all because I used a sign here. So if you use a sign and you're changing, you're never going to run into this setting with warning copy error that's kind of confusing. And you go on to read the descriptions and the stack overflows and you're still kind of confused on what to do. This is also easy to debug as well. So people sometimes say, Matt, it's not easy to debug back because I don't have the intermediate objects. I say it is easy to debug. Let me just show you some things you can do. One thing I can do is I can just comment out everything and walk through it as I'm going through it. So here I've got to call to tweak autos here. Let me just show you. So here is tweak autos. If I want to, I can just come in here and say, well, what did it do before this? This is what it did, right? And I can look at that intermediate and I can walk through from the start to the end. So that's one way to debug it, which I think is actually very useful more so than having all these arbitrary variables sitting around. Another thing I can do if I want an arbitrary variable sticking around is I can make a function like this get far. It's going to take a data frame in a variable name and look what it does. This is sort of a dirty hack. It just basically injects that variable name pointing to the current data frame into my globals. So guess what? Right down here, I've got this here pipe getVard DF3. So this is right after getting the columns. And if I come down here below this and look at DF3, this is the intermediate state. So I can get that intermediate state very easily by piping in this little hack function here that updates that if I need it. So again, I claim that this is a lot cleaner. It's going to allow you to very easily look at your code, but pull out stuff that you need to. One more thing that I have in here is this pipe and this is a piped display a data frame here. So I can display one as well. And I didn't really show this, but if you look at the output here, there's actually two data frames here. So here's the first one. And if I scroll down a little bit more, let me rerun this. This was commented out. So I'm going to rerun this. Okay. So here's the first one. I commented that out when I ran it. And there's the first one. And there's actually the second one here. So I put a little function in here that I piped in that just as a lambda function says display. And then it returns that data frame after it displays it. So the pipe takes a data frame and returns a data frame. So we're just saying shim in a print along the way so we can inspect it. So this is the way that it's going to make it very easy for you to debug these chain operations. Okay. The next step that I have is do not mutate. And if you look at my chain operations because I'm using chains, chains don't mutate. And so I'm obeying this. And I got a quote here from one of the core developers in pandas who I got the link to the bug as well that's talking about getting rid of the in place option in pandas. And the quote is Mr. Reback says you're missing the point in place rarely actually does something in place. You're thinking you're saving memory, but you're not. And I come across this all the time when I'm teaching pandas or looking at code people will use in place. And they think, Oh, it says in place. So it's just updating the memory in place. Generally, it's not doing that. Generally, when you call in place, it's making the new variable that you'd get anyway and then shimming it back in. So it's not saving you any memory that you would get by doing chaining. But basically by forcing yourself to chain, you're going to make that clean code. And you're also going to get rid of the vein of Python developers everywhere, that annoying error that's like, what does this mean by using chains and assigns you get rid of that? This is a short section, but I've sort of demoed it up to this point. In general, there are no performance benefits by doing in place. So if you think you're doing that, you're just fooling yourself. Pandas is not really doing an in place operation, even though it says it is. It also prohibits this chaining, which I think is going to really mess up your code, make it uglier and harder to read. And I think that's one of the most important things is to have code that's easy to read. And then again, you will get rid of that sitting with warning error that is the vein of pandas developers everywhere. Okay, the next section is don't apply if you can. So let's just make our data frame here. I'm going to call autos to here. And I might, you know, this is sort of US centric, but maybe I want to move to something that's more like world centric. A lot of people, instead of doing miles per gallon, they don't even know what a mile let alone a gallon is. They will use leaders per 100 kilometers, right? As a US person, I don't really know what that is, but Americans are kind of weird that way. But I've got this function here that will convert miles per gallons to leaders per 100 kilometers in the conversion, there's you multiply it by 235. So what I can do is I can take like the city mileage and I call this apply method on it. And I can do that. And there's our conversions there. This looks like that works. Note that I can also do this instead of doing a apply here, I can just multiply that right here. And this gives me something that is actually the same value. Now the output is the same, but what's going on under the covers is actually very different. Because I'm calling apply, what applies doing is going to take each individual entry in that column and run each individual entry into this Python operation. And this sell down here, because I'm multiplying the column, what this is going to do is a vectorized or broadcast operation. Remember pandas stores us as a block of data. It's going to take that block of data, multiply it by this number and give me a new block of data without going back into Python. We're saving sort of the serialization, de-serialization step. In fact, we can just time this here and run this little time operator. And we'll see how long this takes with both of these options here. So it looks like the apply takes about six milliseconds. Let's look at the vectorized version here. It takes about 143. So sometimes it's changes, but by my calculations, this is from a previous one, the numbers here, but that it's pretty similar. It's around a 50x difference in doing an apply versus doing the vectorized operation. So my point here is anytime you find yourself doing apply, check whether that is what you want to do. Now there are some cases where apply sort of makes sense. Let's just sort of talk about them. Let's just say, you know, I want to check whether make is American or not. And I'm going to apply this little function here that just says, you know, is the value for that American. And I'll also do this other one. There's also is in on make. And so if I do apply, that's 1.5 milliseconds. You can notice that I do is in that's about half the amount of time. So even with this string operation here by doing it is I am saving some operations because pandas can do it a little bit faster. Now if we do something a little bit more complicated here, we might do something like, you know, I want to make a country column in here. And it's either going to US or other. Again, I'm sort of US centric. I'll apologize for that if that offends you here. The timing for that is like 2.6 milliseconds. Generally, if I was doing a calculation and it had some conditional like this, I would use a wear statement here. So this is using the wear down here. This is how I can onically would do this with a calculation. However, in this case, because it's working with strings instead of numbers, it's going to be slower. So this is almost two times as slow to do this with the wear. However, if you were to do a wear on a numeric column rather than a string, it would actually see an improvement in data. Let's just do one more operation here. There's also this NP select, which is a nice little tool and there's NP wear, which again are very useful for basically building up conditional statements. They're a little bit more powerful than the wear in pandas. However, in this case, for strings, they are slower. But if you're doing numeric calculations, all of these will be faster than the apply operation there. My takeaway here with this apply, I see a lot of people using apply. Don't use it unless you're in a string situation. Otherwise, you're going to want to use one of these NP selects, or pandas wear, NP wear, and your code is going to run a lot faster. I get it that wear might be a little bit confusing. Your code might be a little bit harder to read. But if speed is of the essence, try and avoid apply if possible. And really, with string operations, because strings are stored as Python objects, you're going back to that Python layer anyway. So in that case, I'll say, yeah, apply is okay there, but generally I try to avoid apply for numeric operations. Okay, the last section here is aggregation. Let's talk about aggregation. Let's say that I want to compare mileage by country by year. And so when I started hearing buy this or buy that, basically it means to me that I want to do these group by aggregation. Pandas kind of has two ways to do this one is with a group by the other one is with a pivot table. I'm a software engineer by training. And so group by makes sense to me, I'm not really an Excel master. People who are very adept with Excel tend to favor the pivot table way of doing this, but both of them will get you to the same place. I'm just going to show the group by way here. So what I'm going to say is group by and then I'm going to take the mean here. So if you're not familiar with this, this is really powerful. What is doing is it's going to take and put in the index of the year you can see in the left hand side, the bolded thing is the index. And then it's going to say, okay, for every row that had the year of 1984, get all the numeric columns and take the mean of each numeric column, put that in the column. So this is actually pretty cool. This is, I mean, I've written it out as three lines of code. It's based, you know, you could write it as a single line, a single operation to do this aggregation here, which is pretty cool. Now, this guy sort of the limit what you can do with these. So instead of just doing all the columns, I can say, I want to pull off combined and speed and just get the mean for those if I want to. So I stick in this little operation here. Note that I am chaining these. I'm writing these out each on their own line. And I can sort of walk through and debug that if I need to. Now, one thing I like to do is just stick on a plot. Humans aren't optimized for looking at tables of data. So I can look at this table and I can see it looks like, you know, speeds is going up, but it's kind of hard for me to get a feel for what's going on. But if I just stick a plot on the end of this, what this is going to do, it's going to take the index by default and each column will plot as a line in there. So look at this. There's a visualization. This visualization tells me a lot more to me because I can visually see the representation what's going on there that our speeds are going up over time, are combined mileage with sort of static until 2010. Then it started taking up. Now, again, I'm not a cargoer by any means, but this kind of interesting just from looking at the visualization. So what happened? Why was there that in flutching point around 2007, 2008, the main thing start taking up. And this could also be the data we're also taking the mean so there could be outliers in that. I'm imagining what this is is electric cars and electric cars tend to push off the mean a lot. So maybe we'd look at the median instead. And the median's still going up a little bit, but it doesn't look like it's going up quite as much as the mean there. And so in this case, what I'm going to do down here is I'm going to we said by country, right? And I've just been doing this by year, but now I'm going to do something to change some operations in here. I'm going to make a country column. So I'm going to do that. Apply and get the country column in there. And then I'm going to group by look, I'm going to group by year and country. So let's look at what this gives me. This gives me this data frame with the index. You can see that there's both the year and the country in the index. So this is a hierarchical or a multi index, which is a little bit confusing to use. But hopefully you can see that there's power there. We can see for 1984 here's the summary for US columns. And there's a summary for other non-US as well. We can do a little bit deeper as well. So we go down this rabbit hole here. I'm going to say instead of just doing the mean here, I'm going to call AGG, which allows me to call multiple aggregations. So I'm going to call the minimum the mean. And I'm even going to pass in my own defined function here that's going to take a series and aggregate that series collapse to a value. So in this case, my aggregation is a little bit weird. It takes a second to last value. Now this is just showing that you can pass in whatever function you want to. It just has to take a series and collapse it to a single value. And so here we go. Now you can see that we have in addition to hierarchical index, we have hierarchical columns. So for every numeric column here, we have the minimum value, the mean value, and the second to last value as well. So the sky's sort of the limit with what you want to do with these things. So what I'm going to do is I'm just going to go back to what I had year and country take the mean of that and then plot that. And we get this plot here that's a little bit hard to rock here. I'm not quite sure what's going on. Issue here is that remember pandas is going to plot the index along the x-axis. So now it's actually plotting these tuples of year and country like US or other in there, which makes a little bit hard. So what you might want to do is one of these more advanced operations here, where I'm going to use this unstack here. So if you're not familiar with unstack, let me just comment it out here and show you here is grouping by year and country and then we're going to take the mean. If we come in here and say unstack, what that's going to do is it's going to take the innermost index, which is that country, and it's going to pop that up into the columns. And so you can see that we now have country in the columns. Now we only have a single thing in the index. We've got year. And if we wanted to look at, for example, just the city mileage for a year, we could from that we could pull off the city column, which is going to give us the sub columns there, because that's hierarchical. Let me just comment that out and show you what that's giving us. It gives us that again, we're going from this unstack version, we're going to say city and you can see I'm using this debug style to pull that out. Now we're going to plot that and apply the index and I'm going to plot each of those columns as a value and we get something that looks like this. So I sort of use this construct of grouping by unstacking to illustrate some of the power. And this allows me to quickly visualize that and see what's going on with the US. We can see that the US city mileage is going up maybe faster than the non US mileage, at least in this data set that I have here. Okay, so in summary, pandas is super powerful. You can do a lot of things with it with very few lines of code. In my experience, I've seen good pandas code and I've seen some bad pandas code. If you follow these steps, I guarantee that your code will be better. It will have less bugs. It will probably run faster and your colleagues will probably be more happy with you because they're going to be able to understand your code. So correct types, again, those save time and make things fast and also give you functionality like date and stream manipulation, chaining operations, that's going to make your code more readable. It's going to remove bugs. It's going to be easier to debug as well. I showed some examples of that. Don't mutate. If you find yourself mutating, you should take a step back and think about how to do it without mutating. Again, apply is generally your enemy. So if you're trying to do math operations with a fly, you're probably doing it the wrong way. Apply is okay for string operations, but generally should be avoided. And then aggregations are very powerful. I get that they can be confusing. But if you use this chain style and start building them up, it's going to help you understand what's going on and you're going to be very empowered. So thanks for watching this. I hope you enjoyed this. If you want to learn more about pandas, you can follow me on Twitter, underscore underscore, underscore m Harrison underscore underscore. I also have a discount code to my pandas course. It's going to be 50% off for people who are watching this. So the code is learn pandas, and that will be active for the next week. Good luck with your pandas. It's been a pleasure being with you. Bye.