Hi, I'm Oliver. Welcome to my talk about DevOps automation with Go. I've been a software engineer for more than 20 years, and I discovered Go back in 2017, and I immediately found love with it. It's a great language to write, but especially to read when you have a large code base to get into. I've been the lead developer at RestorePoint since 2019. RestorePoint is the name of our company, but also of our main product, which is a network automation device backup and restore solution. It's all written as a Go monolith, so we have a single binary, which is highly concurrent. We have our own scheduler, HTTP server, FTP server, TFTP server, a Lua environment, etc. etc. And all this runs inside the Linux environment, which we tightly control. Most of our customers run it on premise or in their own cloud, and it's updated either manual or automatic by an update server. So we currently have around 120,000 lines of Go, not counting comments, plus roughly 2.8 million from external libraries. And we use skidlab for our whole development lifecycle. So how does our DevOps look like? So we have three different release versions. We have two target environments. We do weekly production releases. We could actually release every day if we wanted to, but most of our customers prefer a weekly release. So we release in the middle of the week. But we do our development releases. Internally, they are released whenever there's a change. So that's continuous. And we have multiple internal tools that make our lives easier. And as you can see down here in that image, that is how our pipeline looks at the moment. So one of our internal tools is the release API, which avoids us having to copy the build artifact from our build server to the update server. So it's tightly controlled solution. And it's used by multiple of our products. And it's a single binary service as well. And so it has two sides. So the build server sends a call to like a biopost, of course. And it sends the final build artifact as a TGZ. It amb5 sums the TGZ. And then sends additional metadata. So down here, I've copied the call that we actually sent to our server. As you can see, there's a lot of additional metadata. Doesn't apply for all products for most of them. And then there's shared secret between the build server and the release API. So that's the release API will only react to calls that contain that shared secret. And then on the receiving side, so the release API receives that post request that I mentioned, checks that all required metadata fields for a product have been passed, checks the shared secret, of course. And then it writes the file that's been passed and calculates the amb5 sum at the same time, which is quite a nice trick you can do in Go by using a T reader. And if the calculated amb5 sum is not the same as the one that has been sent in the request, then the release is also aborted. And once all the checks are done, then the metadata is written to an end file, as well as the TGZ. And then it's passed to an individual release script based on the product. And this is a single binary service, as I mentioned. And it's also, yeah, it's maybe a hundred lines of code. And it's a really nice, like it's one of the powers of Go, in my opinion, that you can actually write a web server and very few lines. Another tool that we have is the fresh desk GitLab bridge. So for our first line support, we use fresh desks. And as developers, we only deal with issues in GitLab. And our support engineers decide when to escalate issues to us as developers. And we've written a temper monkey script around that, which injects a button into the fresh desk UI. So it's quite easy to trigger that escalation process. And it will copy all comments from fresh desk and all attachments into an issue in GitLab. And it avoids creating duplicates as well. And also make sure that both sides have a link. So you know which ones are have been escalated, which ones are not. I can show that real quick. So this is a video that I took. Just you can see that button over here. This is the injected button. And it will ask you if you really want to do this. And then it will copy the files from a fresh desk. And will create a GitLab issue out of the fresh desk issue. And that's quite a neat way for us to deal with customer support without having to expose the whole team to all custom issues. Not all of them are related to development. And also this is a single binary service as well. And then we have another tool, which we call the automatic version check. It wants us because we have more than one production release. We have three actually. It wants us if we are trying to merge mismatched versions. So if I want to say, as you can see here in the screenshot, we have a 531 version and a 540 version. When trying to merge that, then it will get I get this warning as a comment. And the way it works with merge requests internally, you cannot merge a merge request unless you have resolved all issues, like all discussions on a merge request. So this will keep the merge request from being or accidentally merged. This works by a webhook. So this is also a service that's running on a server. And GitLab basically sends all merge requests or signals all merge requests via webhook to this endpoint. And then we use the GitLab API to check the version of the source and target branch. And then we have an additional thing for automating our development workflow. So we use GitLab has these things called boards. And you can use different statuses, which are labels in GitLab. And these labels, they are for... I mean, we use them for everything, for the area of the product it applies to, if it's a UI or an API issue, if it's a fresh test ticket, for example, but also for process. So our GitLab issues always go through that state from open to to do, to in development, to in-review, to test, to testing. And then eventually they get closed. And we just make sure that we automatically transition issues when a merge request is opened. So the only thing a developer has to do is to actually mention the number of a... and GitLab issue in their merge request. And then the ticket will automatically be set to be in-review. And when the merge request is merged, then it's changed to test. And this really reduces the amount of manual updates that we have to do. Because as developers, we tend to always forget these things. And but it's nice to have our issues in the right state. So it's clear where we are, what the progress is, etc. And then another thing that, because we have a highly concurrent computer software with a lot of lines of code, so we sometimes have data races. And Go has this nice way of allowing you to detect race conditions. So it will see if a variable is read and written to at the same time. And therefore, all of our internal development builds have race condition detection enabled, which is a bit of a performance or has an performance impact. So it I think it increases CPU usage by... I can't remember, but it definitely takes more CPU cycles, but especially memory. I think it doubles the memory usage. So we only do this for development builds internally. And the reason why we have to do this is because most of our race conditions, they happen whenever a certain code paths sit. And we have, of course, fixed all the low-hanging fruit, but there's always something left somewhere. And also sometimes it's library code. We have discovered quite a lot of race conditions in external libraries and then reported that as well. And so we have a lot of internal boxes that replicate all the common usage scenarios that we have. And they run 24-7. And then they write a race condition error messages into their log files. And then we run a this race condition check tool once every day on these individual machines. And then they automatically, if a race condition is found in logs, then it will automatically create a g-tlap issue for each entry. And if an entry already exists, then it will add a comment instead to keep the issue fresh. So I copied here an example of how that looks like in a log. So it starts with warning colon data race. That's the start marker. And then it usually goes like right at blah, blah memory address and go routine number number something. And then the code, the function, where this occurs. This is what we use as the title. Then everything below. So between the start and the end marker, we put into the issue. And this ends up looking like this. So I had to blur, of course, the details for obvious reasons. But it will basically show this. It shows where the current, where the right was, where the previous right was, where the read was. And it will automatically label it with the race conditions tag, which is important. So we can actually see if that this was an actual race condition problem. Yeah. And so that's a really nice solution for that. And then we have another tool, which is for automatic library version. So we have roughly 20 internal libraries that are being used by different products. And these are consumed via go modules, of course. And go like semantic version tags. So when you do a go get, and then you say the name of the library, or the URL of the library, and then add an end diversion tag. And we built a tool around that, which is a job. That's run on the individual libraries at CI, CD pipeline. It's a tag job. And it will basically whenever the master branch of the libraries updated, it will tag the library automatically using the last commit message, as the whole description of the tag, and increases the patch level of the previous tag. And they will create a new version, which then can be used in the product that is using the library. And it will make sure that it will either increment the any existing tags, or if no tags exist, then it will just create a new one. Yeah, and this is it. So this is how we automate our own DevOps at restore point. And I have to do a shameless plug at the end, of course. So we're hiring in either remote UK or EU. And Hoppich is, of course, the, if you're tired of the same old Go microservice on Kubernetes pitch, then maybe have a chat with us. As I explained, we ship an on-premise Go monolith, wrapped in a Linux box every week, and our customers love it. And yeah, we're looking for driven and analytical software engineers. Ideally, we'll go experience, but we can also consider you, if you are really experienced in another language, and you want to cross-strain because it goes relatively easy to pick up. Yeah, so please come and talk to either me or it's our careers page. Thank you very much.