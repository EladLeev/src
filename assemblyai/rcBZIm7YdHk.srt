1
00:00:23,210 --> 00:00:26,386
So, hello and welcome to everyone who's decided to join

2
00:00:26,418 --> 00:00:30,994
me for this talk today. I'm Irisini Chami, and today I'll be discussing

3
00:00:31,122 --> 00:00:34,402
how we can utilize the power of machine learning in healthcare.

4
00:00:34,546 --> 00:00:38,226
So, to start off, I think the best way of fully

5
00:00:38,258 --> 00:00:41,526
showing how useful machine learning is in healthcare is

6
00:00:41,548 --> 00:00:45,282
to try to find a specific problem we can apply to. For this presentation,

7
00:00:45,346 --> 00:00:48,760
I chose heart failure just because of how impactful it was.

8
00:00:49,210 --> 00:00:52,974
I'll talk about heart failure in general for the first part.

9
00:00:53,092 --> 00:00:56,478
Then I'll talk about neural networks and the algorithms behind it.

10
00:00:56,644 --> 00:01:00,618
Then I'll talk about how we can apply the algorithm of neural

11
00:01:00,634 --> 00:01:04,194
networks to the problem of heart failure, and then how

12
00:01:04,232 --> 00:01:07,550
we can try to make that neural network

13
00:01:07,630 --> 00:01:11,266
that predicts heart failure into like, an actual deployable model.

14
00:01:11,448 --> 00:01:14,546
So just some background on heart failure. Basically,

15
00:01:14,648 --> 00:01:18,322
heart failure has been proven to be a major public health concern

16
00:01:18,386 --> 00:01:21,670
that's negatively affected millions of people worldwide.

17
00:01:22,170 --> 00:01:25,714
It's basically when the heart is unable to pump blood

18
00:01:25,762 --> 00:01:29,642
effectively, which can cause a buildup of fluid in the lungs as well as

19
00:01:29,696 --> 00:01:34,214
other organs. As you can see by these various metrics

20
00:01:34,262 --> 00:01:37,386
here, one out of every five deaths in the United States are

21
00:01:37,408 --> 00:01:41,066
caused by heart disease. The average cost in the United States for treating

22
00:01:41,098 --> 00:01:44,926
heart failure is $30,000. And there's limited accuracy for

23
00:01:45,028 --> 00:01:49,950
traditional methods of predicting heart failure. When I say traditional methods,

24
00:01:50,450 --> 00:01:53,810
I mean like clinical assessments, which are

25
00:01:53,880 --> 00:01:57,218
the long standing method of predicting heart failure, which are basically

26
00:01:57,304 --> 00:02:00,558
like a doctor looking at a patient's symptoms,

27
00:02:00,734 --> 00:02:04,770
looking at their medical history, and conducting a physical examination.

28
00:02:05,210 --> 00:02:08,882
And also imaging techniques like ct scans,

29
00:02:08,946 --> 00:02:12,680
mris, and also biomarkers, which are basically

30
00:02:13,210 --> 00:02:17,560
measurable substances in the body that can indicate a biological process.

31
00:02:18,270 --> 00:02:21,914
All these have limitations in various forms with

32
00:02:21,952 --> 00:02:25,450
clinical assessments and biomarkers. They're not specific

33
00:02:25,600 --> 00:02:29,366
enough where it could lead to delayed diagnosis

34
00:02:29,398 --> 00:02:32,906
in the treatment of heart failure. And then there's modern imaging

35
00:02:32,938 --> 00:02:36,398
techniques, which are basically too expensive, and there hasn't been a lot of research

36
00:02:36,484 --> 00:02:39,854
surrounding it. So just

37
00:02:39,892 --> 00:02:43,850
some more background on the current limitations of heart failure.

38
00:02:44,010 --> 00:02:47,326
It's the leading cause of death. As I said earlier, it accounts

39
00:02:47,358 --> 00:02:50,674
for approximately 655,000 United States each

40
00:02:50,712 --> 00:02:54,462
year. Traditional methods, like the clinical assessment,

41
00:02:54,606 --> 00:02:57,970
can lead to inconsistencies and errors in diagnosis,

42
00:02:58,130 --> 00:03:01,746
and it's costly and time consuming. This time consuming

43
00:03:01,778 --> 00:03:05,526
process can create delays in diagnosis, which could result in

44
00:03:05,548 --> 00:03:09,190
a negative patient outcome. And this is where machine learning

45
00:03:09,260 --> 00:03:13,530
comes in. Specifically, neural networks is what I'm going to talk about in this presentation.

46
00:03:14,030 --> 00:03:17,638
But as you guys, I'm sure all familiar with. It's a field that's

47
00:03:17,654 --> 00:03:20,926
been growing in popularity recently and has potential to

48
00:03:20,948 --> 00:03:24,874
be really useful in problems like these. Machine learning's ability

49
00:03:24,922 --> 00:03:29,006
to analyze large volumes of data makes them

50
00:03:29,188 --> 00:03:33,354
very useful because it can look at trends

51
00:03:33,402 --> 00:03:37,086
that aren't apparent with the traditional methods that I'm explaining

52
00:03:37,118 --> 00:03:40,526
earlier. There's already been a lot of literature in the machine

53
00:03:40,558 --> 00:03:44,226
learning field that examines the use of machine learning algorithms for

54
00:03:44,248 --> 00:03:47,814
heart failure prediction. And they found that machine learning is

55
00:03:47,852 --> 00:03:51,682
more accurate compared to traditional models. So in this presentation,

56
00:03:51,746 --> 00:03:55,542
I just chose to do neural networks because that's what seems to be really

57
00:03:55,596 --> 00:03:59,098
popular right now. So basically,

58
00:03:59,184 --> 00:04:02,534
neural networks are in interconnected layers

59
00:04:02,582 --> 00:04:06,634
of neurons. It attempts to mimic the brain in that

60
00:04:06,832 --> 00:04:10,686
it has neurons, which are each of these dots here that

61
00:04:10,708 --> 00:04:14,350
are connected with each other. And basically, when one

62
00:04:14,420 --> 00:04:18,094
fires, it triggers a sense in

63
00:04:18,132 --> 00:04:21,406
the layers following it. So it

64
00:04:21,428 --> 00:04:25,486
typically consists of an input layer and an output layer with multiple

65
00:04:25,518 --> 00:04:29,090
hidden layers. And in each of these input layers,

66
00:04:30,470 --> 00:04:33,774
in our case, the input layer would be various

67
00:04:33,822 --> 00:04:37,874
characteristics of the patient, so it would be like their age, their maximum

68
00:04:37,922 --> 00:04:41,286
heart rate, et cetera. Hidden layers are just

69
00:04:41,308 --> 00:04:45,794
here for complexity in that it allows

70
00:04:45,842 --> 00:04:50,070
the neural networks to basically learn using two main algorithms,

71
00:04:50,230 --> 00:04:53,706
forward propagation and backward propagation. And the

72
00:04:53,728 --> 00:04:57,770
output layer in our scenario is just going to be one neuron

73
00:04:59,070 --> 00:05:02,406
from zero to one, where zero would basically mean that

74
00:05:02,448 --> 00:05:07,022
the patient does not have that high of an outcome, of that

75
00:05:07,076 --> 00:05:10,478
high of a likelihood of getting heart failure where one is,

76
00:05:10,564 --> 00:05:13,726
the model is confident that the patient will get

77
00:05:13,748 --> 00:05:16,960
heart failure. So hang on.

78
00:05:17,750 --> 00:05:21,122
For any task like this, we need

79
00:05:21,176 --> 00:05:24,546
data. This is from Kaggle. If you guys wanted to

80
00:05:24,568 --> 00:05:28,038
find any other data sets you wanted to use, I think Kaggle would be

81
00:05:28,044 --> 00:05:30,600
a great resource here.

82
00:05:31,850 --> 00:05:36,786
This guy's compiled from different hospitals,

83
00:05:36,978 --> 00:05:40,646
from the UCI machine learning repository, and combined the

84
00:05:40,668 --> 00:05:43,994
common features into one big one. And that makes this better

85
00:05:44,032 --> 00:05:48,038
than other hard failure prediction data sets that are out there, because this allows

86
00:05:48,054 --> 00:05:51,498
me to have more rows and more data to train on.

87
00:05:51,664 --> 00:05:54,686
So I have the link here, and you guys should be able to find the

88
00:05:54,708 --> 00:05:58,318
slide deck if you guys want to follow along. Just some information

89
00:05:58,404 --> 00:06:02,454
about the data set. It has various numerical

90
00:06:02,522 --> 00:06:06,206
characteristics, like age, resting blood pressure

91
00:06:06,238 --> 00:06:10,206
and cholesterol, and it also has categorical columns

92
00:06:10,238 --> 00:06:14,014
like the chest train type and the gender

93
00:06:14,142 --> 00:06:18,146
and the resting electrocardiogram

94
00:06:18,178 --> 00:06:22,006
results. So these are all just various attributes that

95
00:06:22,028 --> 00:06:25,590
will allow us to help in the predictions of heart failure.

96
00:06:27,050 --> 00:06:29,180
Moving on. Sorry.

97
00:06:30,430 --> 00:06:34,170
Yeah, in this demo, I'm just going to use

98
00:06:34,320 --> 00:06:37,514
Google Colab because it's pretty easy to follow

99
00:06:37,552 --> 00:06:41,306
along with. You don't really need a lot of gpus materials

100
00:06:41,418 --> 00:06:45,310
as well as it already has tensorflow installed

101
00:06:45,650 --> 00:06:46,560
in it.

102
00:06:48,850 --> 00:06:52,538
So to start off, go to the Kaggle

103
00:06:52,554 --> 00:06:55,490
data set that I mentioned earlier and download the csv.

104
00:06:56,150 --> 00:06:59,506
And then in a new Google Colab notebook we

105
00:06:59,528 --> 00:07:03,086
can import pandas. And that basically allows

106
00:07:03,118 --> 00:07:06,834
us to manipulate CSVs and read them in. So we

107
00:07:06,952 --> 00:07:10,398
set a variable and set it equal to reading in the csV.

108
00:07:10,574 --> 00:07:13,606
And then I call the dot head function on it, which allows me to see

109
00:07:13,628 --> 00:07:16,950
the first five rows. And we can see that

110
00:07:17,100 --> 00:07:20,458
these are basically different patient records. So each of

111
00:07:20,464 --> 00:07:23,994
these rows represents a different patient. And we can see like

112
00:07:24,032 --> 00:07:28,086
this guy was 40, a male, had a resting blood pressure

113
00:07:28,118 --> 00:07:31,686
of 140, and he had a heart disease column

114
00:07:31,718 --> 00:07:35,134
of zero. So this means that the patient did not actually end up getting heart

115
00:07:35,172 --> 00:07:38,080
failure. So it's just pretty interesting to see.

116
00:07:39,330 --> 00:07:42,766
Normally you would do a lot of analyzing the

117
00:07:42,788 --> 00:07:46,046
data, but I think with the scope of this presentation,

118
00:07:46,078 --> 00:07:49,060
I'm just going to hop straight into the pre processing the data.

119
00:07:49,670 --> 00:07:53,810
So neural networks take in basically numbers exclusively.

120
00:07:54,150 --> 00:07:58,226
So we need to convert the categorical columns into numerical columns.

121
00:07:58,418 --> 00:08:01,778
And we can do this through something called one hot encoding,

122
00:08:01,954 --> 00:08:05,080
which is basically like, you take in,

123
00:08:05,770 --> 00:08:09,634
I'll go back to this part, you take in each of these columns

124
00:08:09,682 --> 00:08:13,466
and then set it equal to, sorry, take in each one of

125
00:08:13,488 --> 00:08:17,446
these outputs that are possible from one of these categorical

126
00:08:17,638 --> 00:08:21,434
columns and then set each of these equal to a new column.

127
00:08:21,562 --> 00:08:25,482
Then we would go through each value inside of this categorical

128
00:08:25,546 --> 00:08:28,800
column and set it equal to

129
00:08:29,330 --> 00:08:32,802
one for the thing that's actually matching on the value

130
00:08:32,856 --> 00:08:36,274
for it and set it equal to zero for everything else, which is just

131
00:08:36,312 --> 00:08:39,966
a way of converting the categorical columns

132
00:08:39,998 --> 00:08:41,490
into numerical columns.

133
00:08:43,750 --> 00:08:47,880
So we do that for each of the categorical columns in the data set.

134
00:08:48,250 --> 00:08:51,654
And then we remove the categorical columns and

135
00:08:51,692 --> 00:08:55,462
add these numerical columns that we just added. And then now we can call

136
00:08:55,516 --> 00:08:59,160
head. So we can see before that our

137
00:09:00,490 --> 00:09:04,614
40 year old patient had an ascent at eight chest

138
00:09:04,662 --> 00:09:08,138
pain type. And now if we

139
00:09:08,144 --> 00:09:11,286
go here, there's no column for chest pain

140
00:09:11,328 --> 00:09:15,310
type. Instead there's just these different columns.

141
00:09:16,210 --> 00:09:19,694
So we can see here that for ATA, the guy has one and

142
00:09:19,732 --> 00:09:23,378
zero for everything else, which just allows us, the neural network, to have that kind

143
00:09:23,384 --> 00:09:27,010
of information that's preserved from the categories.

144
00:09:29,750 --> 00:09:33,662
So then we get our input and our output arrays.

145
00:09:33,806 --> 00:09:36,978
So our x or our input is just everything

146
00:09:37,064 --> 00:09:40,386
except the heart disease column. And our output, which is what we're

147
00:09:40,418 --> 00:09:42,870
predicting, is just the heart disease column.

148
00:09:43,610 --> 00:09:47,266
And then we can use sklearn functions to further preprocess

149
00:09:47,298 --> 00:09:50,810
the data. We can import their train test

150
00:09:50,880 --> 00:09:54,426
split to split up the input and

151
00:09:54,448 --> 00:09:57,820
output into a training and testing data set.

152
00:09:59,070 --> 00:10:02,206
I put the test size as 0.2, so we can use 20% of the

153
00:10:02,228 --> 00:10:05,982
data set for testing accuracy. I think there's around

154
00:10:06,036 --> 00:10:09,726
1000 rows, so there should be around 800

155
00:10:09,908 --> 00:10:13,586
records for the model to learn on and around 200 for

156
00:10:13,608 --> 00:10:17,474
us to do testing on. So then

157
00:10:17,512 --> 00:10:20,882
from there we can do feature scaling on it.

158
00:10:20,936 --> 00:10:24,206
Because neural network essentially likes

159
00:10:24,238 --> 00:10:27,702
to have small numbers, and doing a standard

160
00:10:27,756 --> 00:10:31,622
scalar on it allows us to achieve that. So then we fit

161
00:10:31,676 --> 00:10:35,990
the training and testing on the standard scalar.

162
00:10:36,330 --> 00:10:39,820
So now we have an x train and x test,

163
00:10:41,390 --> 00:10:45,450
and then from here we can create a model or the neural network.

164
00:10:46,510 --> 00:10:49,914
This is kind of like the architecture I chose for it, to be honest,

165
00:10:49,952 --> 00:10:52,970
a lot of the choices are arbitrary.

166
00:10:53,870 --> 00:10:57,614
In the real world, I think you would mess around a lot more with these

167
00:10:57,652 --> 00:11:00,990
values and to try to see what could give you the best accuracy.

168
00:11:01,490 --> 00:11:05,406
But for the purposes of this demonstration, we have our input

169
00:11:05,438 --> 00:11:09,410
layer, and that connects to a hidden layer containing 16 neurons

170
00:11:09,830 --> 00:11:13,774
connecting to another hidden layer containing 16 neurons,

171
00:11:13,902 --> 00:11:18,390
and that connects to another output layer, which contains just one neuron.

172
00:11:20,330 --> 00:11:24,200
All these connected together makes up our sequential model.

173
00:11:27,050 --> 00:11:31,014
I use keras to create the neural network, which is just like an extension

174
00:11:31,062 --> 00:11:34,060
of tensorflow. To make it easier to create them,

175
00:11:34,670 --> 00:11:38,234
all I had to do was just import the sequential model from

176
00:11:38,352 --> 00:11:41,610
Keras models and the dense layer from keras layers,

177
00:11:42,110 --> 00:11:46,014
and then I could set my model equal to sequential and then add

178
00:11:46,052 --> 00:11:49,440
my different layers. So this line of code

179
00:11:50,050 --> 00:11:53,018
basically takes in my input layer.

180
00:11:53,194 --> 00:11:56,850
So I set the dimensions of the input layer equal to whatever

181
00:11:56,920 --> 00:12:00,242
the shape of my first value in

182
00:12:00,296 --> 00:12:03,934
my training data set was. So it would like scale

183
00:12:03,982 --> 00:12:07,682
with it. And then I set 16

184
00:12:07,746 --> 00:12:11,618
neurons for the next layer. So it just matches up just like this. That connects

185
00:12:11,634 --> 00:12:15,110
to another layer containing 16 neurons, and then

186
00:12:15,180 --> 00:12:18,538
our output layer contains one neuron. One thing I do want

187
00:12:18,544 --> 00:12:22,362
to note is like the activation functions, Relu means

188
00:12:22,416 --> 00:12:25,994
rectified linear unit. And that's basically a graph that

189
00:12:26,032 --> 00:12:29,420
looks just like zero for everything up to zero,

190
00:12:29,870 --> 00:12:33,038
and then the value from zero

191
00:12:33,124 --> 00:12:36,320
onwards. So it's basically like a piecewise function

192
00:12:36,770 --> 00:12:40,298
where it's like it's zero for everything that's negative,

193
00:12:40,474 --> 00:12:43,870
and then it's just a positive value for anything that's positive.

194
00:12:44,370 --> 00:12:48,674
And that adds like a nonlinear complexity that

195
00:12:48,712 --> 00:12:50,500
allows the neural network to learn.

196
00:12:51,270 --> 00:12:54,642
Sigmoid just squishes all values between zero

197
00:12:54,696 --> 00:12:57,906
and one, which is essentially what we need in our output layer

198
00:12:57,938 --> 00:13:02,226
to create a prediction value for a patient.

199
00:13:02,418 --> 00:13:04,520
So that allows us to achieve that.

200
00:13:05,850 --> 00:13:09,478
We then compile our model using an optimizer.

201
00:13:09,654 --> 00:13:13,500
I chose Adam, a loss function, which basically

202
00:13:14,510 --> 00:13:18,666
is what the model uses to see how bad it is when learning,

203
00:13:18,768 --> 00:13:22,606
so it can adjust its weights and

204
00:13:22,628 --> 00:13:26,350
biases in these different arrows to try to create a better output.

205
00:13:30,930 --> 00:13:34,830
Um, from there, I chose a metric of accuracy.

206
00:13:36,210 --> 00:13:39,854
So then you fit the model on our training data with a validation

207
00:13:39,902 --> 00:13:43,266
split of 33%, which just gives us information as the

208
00:13:43,288 --> 00:13:45,880
model is training about how well it's doing.

209
00:13:46,650 --> 00:13:49,462
And then epochs equals 100,

210
00:13:49,596 --> 00:13:53,590
so that's how many times it forward propagates and backward propagates.

211
00:13:54,330 --> 00:13:58,026
Generally, more is better accuracy up to a certain point where

212
00:13:58,048 --> 00:13:59,370
there's minimal train.

213
00:14:01,870 --> 00:14:05,340
So then I trained the model on it,

214
00:14:05,950 --> 00:14:09,654
and then I tested the accuracy

215
00:14:09,702 --> 00:14:13,238
of it on the actual testing data set. So, like data it's never

216
00:14:13,264 --> 00:14:16,702
seen before, this is like the best indicator of what the data

217
00:14:16,756 --> 00:14:20,826
looks like in the real world. So I set a variable of predictions equal

218
00:14:20,858 --> 00:14:25,066
to the model predicting on every single value from the x underscore

219
00:14:25,098 --> 00:14:28,658
test. And then what I did was,

220
00:14:28,744 --> 00:14:32,226
for each of those predictions, if it was greater than 0.5,

221
00:14:32,408 --> 00:14:36,450
I made that a one. And if it was less than 0.5 or

222
00:14:36,520 --> 00:14:39,878
equal to it, I made it e zero. So if it was less than or

223
00:14:39,884 --> 00:14:43,186
equal to 0.5, the model basically predictions

224
00:14:43,218 --> 00:14:47,186
that the patient doesn't have heart failure in the future, and if it was greater

225
00:14:47,218 --> 00:14:50,386
than 0.5, then the model predictions that the patient

226
00:14:50,418 --> 00:14:53,866
does have heart failure. And when we

227
00:14:53,888 --> 00:14:58,086
ended up doing that and use the accuracy score metric from Sklearn,

228
00:14:58,278 --> 00:15:02,620
we got an accuracy of around 79%,

229
00:15:02,930 --> 00:15:06,334
which isn't too bad considering how few lines of code this

230
00:15:06,372 --> 00:15:09,774
really is. And I feel like if I tweaked it around more,

231
00:15:09,892 --> 00:15:14,002
we could get a much better accuracy. But other

232
00:15:14,056 --> 00:15:17,986
than that, that's basically it for the model creation part of

233
00:15:18,008 --> 00:15:21,522
it. What you could do next is try to take this model

234
00:15:21,576 --> 00:15:24,690
variable and host it somewhere.

235
00:15:25,190 --> 00:15:28,614
So, this is something that when I was trying

236
00:15:28,652 --> 00:15:32,166
to choose machine learning models to use on

237
00:15:32,188 --> 00:15:36,786
the data set, I made like an input form where you could input

238
00:15:36,898 --> 00:15:40,406
your age values, your resting blood pressure, and it would give you

239
00:15:40,428 --> 00:15:43,640
a heart failure prediction. So it's just something that I thought was cool,

240
00:15:44,250 --> 00:15:46,920
and you could also learn more about heart failure and stuff.

241
00:15:49,210 --> 00:15:52,826
But other than that, that's it for this talk. Thank you for joining

242
00:15:52,858 --> 00:15:55,806
me. If you're curious and you want to play around with the code a little

243
00:15:55,828 --> 00:15:59,774
bit more. I've left the colab link, which I think you can see in

244
00:15:59,972 --> 00:16:03,406
the slides here, but other than that, thank you for

245
00:16:03,428 --> 00:16:03,660
your time.

