1
00:00:23,290 --> 00:00:26,854
Hi, thank you for your interest in this session. So in this

2
00:00:26,892 --> 00:00:30,630
session unlocking reasoning and planning abilities in large

3
00:00:30,700 --> 00:00:33,974
language models, I would like to take you through the different

4
00:00:34,092 --> 00:00:38,418
methodologies and techniques to elicit reasoning abilities

5
00:00:38,514 --> 00:00:42,198
from llms. So I'll be taking you through the

6
00:00:42,284 --> 00:00:45,430
recent research works related to this.

7
00:00:45,580 --> 00:00:49,170
So, about myself, I'm logesh Kumar Umapathi,

8
00:00:49,250 --> 00:00:52,974
a lead measure learning research engineer at Sama. So my

9
00:00:53,012 --> 00:00:56,602
research interests includes biomedical,

10
00:00:56,666 --> 00:01:00,634
NLP, large language models and code generation.

11
00:01:00,762 --> 00:01:04,850
So you can reach me through these social

12
00:01:04,920 --> 00:01:09,310
media channels. And I'm also involved

13
00:01:09,390 --> 00:01:13,150
in maintaining can open source package called mutate

14
00:01:13,230 --> 00:01:17,122
which is about synthesizing data from large language models.

15
00:01:17,266 --> 00:01:21,062
So the agenda for this session is we'll start

16
00:01:21,116 --> 00:01:24,774
with understanding what is reasoning and how

17
00:01:24,812 --> 00:01:27,966
the reasoning can be measured and is measured

18
00:01:28,018 --> 00:01:31,660
in the research literature. And also

19
00:01:32,590 --> 00:01:36,314
in the bulk of the session we will be discussing about how

20
00:01:36,352 --> 00:01:40,098
to elicit reasoning. We'll be discussing different techniques

21
00:01:40,134 --> 00:01:43,962
like direct prompting, direct one shot generation

22
00:01:44,026 --> 00:01:48,046
of the solution, and then recursive and iterative prompting in

23
00:01:48,068 --> 00:01:51,834
which we will be discussing techniques

24
00:01:51,882 --> 00:01:56,434
to recursively and iteratively let the LLM generate the

25
00:01:56,472 --> 00:02:00,930
solution. And then we will be discussing about tool usage,

26
00:02:02,630 --> 00:02:06,710
which is the most popular one. Now with

27
00:02:06,780 --> 00:02:10,840
the advent of hugging GPT and

28
00:02:11,210 --> 00:02:14,646
hugging face agents and so on. So what

29
00:02:14,668 --> 00:02:18,534
is reasoning? So, reasoning can be defined

30
00:02:18,582 --> 00:02:22,058
as an ability to make inference from the given

31
00:02:22,224 --> 00:02:25,530
evidences and logic. So there are different

32
00:02:25,600 --> 00:02:29,322
types of reasoning like common sense mathematical and

33
00:02:29,376 --> 00:02:33,360
symbolic reasoning. And reasoning can also be

34
00:02:33,730 --> 00:02:37,902
defined as an ability to break

35
00:02:37,956 --> 00:02:41,642
down a bigger problem into a smaller

36
00:02:41,706 --> 00:02:44,958
solvable problems, and these recursively solve

37
00:02:44,974 --> 00:02:49,010
the sub problems to solve these bigger problem.

38
00:02:49,080 --> 00:02:52,766
Finally, so this can be considered

39
00:02:52,798 --> 00:02:56,182
as a broad definition of

40
00:02:56,236 --> 00:03:00,038
reasoning. So now that we know what is reasoning or

41
00:03:00,124 --> 00:03:04,066
a broader definition of what we are trying with reasoning,

42
00:03:04,178 --> 00:03:07,110
let's see how it's measured in the literature.

43
00:03:07,470 --> 00:03:10,874
So in literature the reasoning is

44
00:03:10,912 --> 00:03:15,222
usually measured as these separate categories,

45
00:03:15,366 --> 00:03:19,206
mathematical reasoning, common sense reasoning and symbolic reasoning.

46
00:03:19,318 --> 00:03:23,546
In mathematical reasoning, it's usually measured with math

47
00:03:23,578 --> 00:03:27,806
world problems, usually math world problems that are

48
00:03:27,988 --> 00:03:31,726
available online. So GSM eight k is about

49
00:03:31,828 --> 00:03:36,086
grade school math world problems. This is from Opena.

50
00:03:36,218 --> 00:03:40,046
And then the other data sets are benchmarks

51
00:03:40,078 --> 00:03:43,906
also related to that. And then for common sense reasoning we

52
00:03:43,928 --> 00:03:47,706
have arc a two reasoning challenge from allen

53
00:03:47,758 --> 00:03:54,082
AI. So there we have science

54
00:03:54,146 --> 00:03:57,474
question answering to measure the common sense

55
00:03:57,532 --> 00:04:01,366
reasoning of the models. Then we have CsQA,

56
00:04:01,398 --> 00:04:05,414
which is common sense question answering. And then we have strategy Qa

57
00:04:05,462 --> 00:04:09,686
from Malin Aa. So these data sets

58
00:04:09,718 --> 00:04:13,006
or benchmarks help in measuring the common sense ability of

59
00:04:13,028 --> 00:04:16,174
these models. So to give an example, one of the questions

60
00:04:16,292 --> 00:04:17,360
could be like,

61
00:04:22,790 --> 00:04:26,914
would Aristotle have used a

62
00:04:26,952 --> 00:04:30,530
keyboard? So this could be a question in that data

63
00:04:30,600 --> 00:04:34,498
set, the model has to reduce that when

64
00:04:34,584 --> 00:04:38,034
the keyboard was invented

65
00:04:38,162 --> 00:04:41,746
and when Aristotle existed, and then it should deduce

66
00:04:41,778 --> 00:04:45,746
whether it's possible or not. So these type of reasoning is covered

67
00:04:45,778 --> 00:04:49,046
in common sense reasoning. Then we have symbolic reasoning,

68
00:04:49,078 --> 00:04:52,586
which was mostly introduced by Jason V in

69
00:04:52,608 --> 00:04:57,142
his chain of thoughts paper. So we have last letter concatenation

70
00:04:57,206 --> 00:05:00,814
and coin flip type problems. These. So this

71
00:05:00,852 --> 00:05:04,126
is how mostly the reasoning is measured in the

72
00:05:04,148 --> 00:05:07,674
literature. To give an example of sample benchmark,

73
00:05:07,722 --> 00:05:11,134
we have taken a snapshot of what is

74
00:05:11,172 --> 00:05:14,594
available in GPT four technical review.

75
00:05:14,712 --> 00:05:18,814
So this gives us an overview of how the reasoning

76
00:05:18,862 --> 00:05:22,674
is measured and what the current state of things. So we can

77
00:05:22,712 --> 00:05:26,434
see that the reasoning tasks like GSM eight k

78
00:05:26,472 --> 00:05:29,766
and a two reasoning, the sort of for that

79
00:05:29,868 --> 00:05:33,430
currently is 96 percentage and 92 percentage.

80
00:05:34,090 --> 00:05:38,158
So now that we know what is reasoning and how is it measured,

81
00:05:38,354 --> 00:05:41,814
let's see how and what are the methodology to elicit

82
00:05:41,862 --> 00:05:45,260
reasoning. And before even going to that, let's see

83
00:05:46,830 --> 00:05:50,226
why there is a need to elicit reasoning.

84
00:05:50,358 --> 00:05:53,754
So given the size of these models,

85
00:05:53,802 --> 00:05:57,710
this huge 175,000,000,000, 540,000,000,000 models,

86
00:05:59,090 --> 00:06:03,258
one can think that why wouldn't reasoning be

87
00:06:03,364 --> 00:06:06,434
come or generated by default by these

88
00:06:06,472 --> 00:06:10,114
models? Why there is a need to elicit it? We'll first

89
00:06:10,152 --> 00:06:13,650
try to address that and then come to the different

90
00:06:13,720 --> 00:06:16,594
methodologies of eliciting reasoning.

91
00:06:16,722 --> 00:06:20,722
So yesterday I tried this prompt in Chat GPT

92
00:06:20,866 --> 00:06:25,186
for this session. So if you can see, I've tried to ask Chat

93
00:06:25,218 --> 00:06:28,962
GPT to take the last letters of the word Augusta

94
00:06:29,026 --> 00:06:33,114
ducking and concatenate them using a space.

95
00:06:33,312 --> 00:06:36,714
So can see that the model has detected the

96
00:06:36,752 --> 00:06:40,526
last words incorrectly. It has detected last letters incorrectly as

97
00:06:40,548 --> 00:06:44,622
a G and G. And the final answer because

98
00:06:44,676 --> 00:06:49,214
of that is also wrong. But when

99
00:06:49,252 --> 00:06:52,782
we break down that, the same problems into

100
00:06:52,836 --> 00:06:56,158
three different problems. So what are the words in

101
00:06:56,244 --> 00:07:00,340
Augusta adaking? These model is able to come up with the words,

102
00:07:00,710 --> 00:07:04,034
and then what are the last letters of these words? It's able to come up

103
00:07:04,072 --> 00:07:07,826
with the last letters, AAG correctly. And when

104
00:07:07,848 --> 00:07:11,574
I ask it to concatenate it, it's able to concatenate it. So this is

105
00:07:11,612 --> 00:07:16,354
why we would need eliciting techniques

106
00:07:16,402 --> 00:07:19,690
to elicit reasoning. So these models,

107
00:07:21,230 --> 00:07:24,922
as the objective of its training, are not trained to

108
00:07:25,056 --> 00:07:28,474
do reasoning, or at least from

109
00:07:28,512 --> 00:07:31,710
my understanding, it's not trained to do reasoning.

110
00:07:33,250 --> 00:07:36,670
It still has the tendency to do text completion

111
00:07:38,370 --> 00:07:41,886
so that's why we would need the methodologies that we are

112
00:07:41,908 --> 00:07:46,458
going to discuss in the further slides for eliciting

113
00:07:46,554 --> 00:07:50,318
these reasoning. So let's start with probably

114
00:07:50,404 --> 00:07:54,014
the most popular and also from my

115
00:07:54,052 --> 00:07:57,958
understanding, these methodology which kick started

116
00:07:58,044 --> 00:08:01,522
all the different prompting

117
00:08:01,666 --> 00:08:04,806
techniques to elicit reasoning, the chain of

118
00:08:04,828 --> 00:08:09,286
thought prompting. So in chain of thought prompting, the authors

119
00:08:09,398 --> 00:08:13,066
JSon V had tried to do so what

120
00:08:13,088 --> 00:08:17,100
they have tried to do is for all the mathematical and

121
00:08:18,830 --> 00:08:21,200
other reasoning related questions,

122
00:08:22,050 --> 00:08:25,246
instead of asking the direct answers to the model.

123
00:08:25,428 --> 00:08:28,702
If we ask the model to generate step by step

124
00:08:28,756 --> 00:08:32,506
reason and then generate the final answers,

125
00:08:32,698 --> 00:08:36,574
answers. Finally, they found that the model tend to

126
00:08:36,612 --> 00:08:40,190
do tend to generate answers

127
00:08:40,270 --> 00:08:43,890
better. The accuracy of generation of answers was better.

128
00:08:44,040 --> 00:08:47,622
So here, if you see the question, Roger has five

129
00:08:47,676 --> 00:08:51,234
tennis balls, he buys two more cans of tennis balls,

130
00:08:51,282 --> 00:08:55,026
each can has three tennis balls. How many tennis

131
00:08:55,058 --> 00:08:58,166
balls does he have now? So there in the answer,

132
00:08:58,268 --> 00:09:02,134
as a one shot example, they have explained the different steps

133
00:09:02,182 --> 00:09:06,122
that can be deduced from this question and then the final answer.

134
00:09:06,256 --> 00:09:09,258
So for a new question that the model sees here,

135
00:09:09,344 --> 00:09:12,734
the cafeteria example, the model would come up

136
00:09:12,772 --> 00:09:16,874
with a similar chain

137
00:09:16,922 --> 00:09:20,830
of steps and then it would generate an answer

138
00:09:20,900 --> 00:09:24,446
from this. So these methodology

139
00:09:24,558 --> 00:09:28,402
has resulted or has shown to give

140
00:09:28,456 --> 00:09:32,434
better results. You can see across all

141
00:09:32,472 --> 00:09:35,926
the reasoning data sets and across all the

142
00:09:35,948 --> 00:09:40,034
different models, this approach seems to give better results.

143
00:09:40,162 --> 00:09:44,226
And there is another variation of it from Wang

144
00:09:44,258 --> 00:09:47,982
et al. It's called as self consistency.

145
00:09:48,146 --> 00:09:52,326
So in the chain of thought prompting, initial chain of thought prompting, the answers

146
00:09:52,358 --> 00:09:56,758
are generated using greedy decoding. So there is only one generation

147
00:09:56,854 --> 00:10:00,634
for a given prompt. So in the self consistencies,

148
00:10:00,682 --> 00:10:04,426
the authors have tried to do sampling

149
00:10:04,458 --> 00:10:08,538
based decoding to generate multiple generations

150
00:10:08,634 --> 00:10:12,190
for a given prompt. And then they consider the most,

151
00:10:12,260 --> 00:10:15,778
or the majority voted answers which are similar

152
00:10:15,864 --> 00:10:19,314
or which are same, and then they evaluate that

153
00:10:19,352 --> 00:10:22,420
particular answers against the evaluation set.

154
00:10:24,970 --> 00:10:29,174
So that way the model performs better even than

155
00:10:29,292 --> 00:10:33,186
chain of thought prompting. So their intuition

156
00:10:33,378 --> 00:10:36,806
is that if a

157
00:10:36,828 --> 00:10:40,922
model comes up with majority of solutions, a majority of

158
00:10:40,976 --> 00:10:43,370
methods to come up with the same solution,

159
00:10:44,110 --> 00:10:47,786
then they consider that it's most likely that that is a

160
00:10:47,808 --> 00:10:51,206
proper and correct answer. So that's the intuition

161
00:10:51,318 --> 00:10:55,054
for that and can extension to

162
00:10:55,092 --> 00:10:58,746
chain of thought. Is that so? The challenge

163
00:10:58,778 --> 00:11:02,534
with chain of thought is we are leaving

164
00:11:02,602 --> 00:11:06,254
the arithmetic operations when it comes to mathematical

165
00:11:06,302 --> 00:11:09,970
reasoning. We are leaving that to the

166
00:11:10,040 --> 00:11:14,698
llms. So we all know that llms

167
00:11:14,894 --> 00:11:19,138
lack even simple arithmetic abilities.

168
00:11:19,314 --> 00:11:22,706
So in this paper, program aided

169
00:11:22,818 --> 00:11:26,600
language models. So the authors have tried a

170
00:11:27,050 --> 00:11:31,610
clever methodology where they have offloaded

171
00:11:32,670 --> 00:11:36,422
the arithmetic calculations to the Python interpreter.

172
00:11:36,566 --> 00:11:40,334
So the way they have done is they have created few

173
00:11:40,372 --> 00:11:44,026
short prompts. So each With a question like we saw in China

174
00:11:44,058 --> 00:11:48,526
of thought prompting. And they divided the

175
00:11:48,548 --> 00:11:51,966
solutions, they have represented it

176
00:11:52,068 --> 00:11:55,634
as a Python problem. So here you can see these

177
00:11:55,672 --> 00:11:59,262
tennis ball, they have deduced the tennis ball

178
00:11:59,326 --> 00:12:02,942
value from the question, and then what are the balls

179
00:12:03,006 --> 00:12:06,294
that are bought? And then these answer. So this way

180
00:12:06,332 --> 00:12:10,706
they have converted that to a pythonic solution.

181
00:12:10,898 --> 00:12:14,646
So this way, for a new problem based on

182
00:12:14,668 --> 00:12:17,438
the examples that are there in Fusot,

183
00:12:17,634 --> 00:12:21,050
the model generates a similar Python

184
00:12:21,950 --> 00:12:25,626
problem, reducing the question and then coming up

185
00:12:25,648 --> 00:12:28,682
with the final answer. So to get the final answer,

186
00:12:28,816 --> 00:12:32,874
these generated solution is executed

187
00:12:32,922 --> 00:12:37,130
in the Python interpreter and that is considered as the final solution.

188
00:12:37,290 --> 00:12:41,326
So this performed better than Chain of thought as you

189
00:12:41,348 --> 00:12:45,166
can see in the results across all the mathematical reasoning

190
00:12:45,198 --> 00:12:48,946
benchmarks. The main reason is we are using

191
00:12:49,048 --> 00:12:52,130
llms for its advantages,

192
00:12:53,430 --> 00:12:56,866
for its strengths, and these we

193
00:12:56,888 --> 00:13:00,866
are offloading the weakness of llms to the Python

194
00:13:00,898 --> 00:13:04,614
interpreter. And another variation of this

195
00:13:04,652 --> 00:13:08,626
prompting, or this type of prompting, is plan and solve prompting.

196
00:13:08,818 --> 00:13:12,520
So this is mostly to address the

197
00:13:13,130 --> 00:13:16,614
performance of zero short chain of thought prompting.

198
00:13:16,742 --> 00:13:20,714
So zero short chain of thought prompting is usually done by this

199
00:13:20,912 --> 00:13:24,634
prompt where we ask, given a question, we ask let's

200
00:13:24,682 --> 00:13:29,582
think step by step, and the model will come up with step by step thought

201
00:13:29,636 --> 00:13:33,602
process and the final answer. But this wasn't working

202
00:13:33,736 --> 00:13:37,234
that well, mainly due to different reasons. One of that

203
00:13:37,272 --> 00:13:43,106
is the arithmetic ability, as we saw before. And then there

204
00:13:43,128 --> 00:13:47,014
was few inference steps that are missed by the model

205
00:13:47,132 --> 00:13:51,074
and few inference steps that are not converted

206
00:13:51,122 --> 00:13:54,550
to solutions. In these zero shot can of thought

207
00:13:54,620 --> 00:13:58,178
prompting. So this is rectified by a methodology

208
00:13:58,274 --> 00:14:00,470
called as plan and solve prompting.

209
00:14:02,090 --> 00:14:05,382
In these, the authors authors

210
00:14:05,446 --> 00:14:08,982
try a different style of prompting called let's prompting.

211
00:14:09,046 --> 00:14:12,602
That is, let's first understand the problem and devise a plan

212
00:14:12,656 --> 00:14:16,206
to solve the problem. Then let's carry out the plan and solve these

213
00:14:16,228 --> 00:14:19,614
problem step by step. So they ask

214
00:14:19,652 --> 00:14:22,862
the model to come up with the plan first and then the solution

215
00:14:22,926 --> 00:14:26,580
based on the plan that it has derived. So this way, these model,

216
00:14:27,270 --> 00:14:31,860
they have show that the model performs better than

217
00:14:32,950 --> 00:14:36,754
zero short chain of thoughts prompting,

218
00:14:36,882 --> 00:14:40,950
and even they have shown that it performs better than few

219
00:14:41,020 --> 00:14:44,070
short chain of thought prompting in some cases.

220
00:14:44,410 --> 00:14:47,866
So until now we have seen methodologies of how to

221
00:14:47,888 --> 00:14:51,754
inference or how to do in context, learning to

222
00:14:51,872 --> 00:14:56,010
incontext, learning to elicit

223
00:14:56,350 --> 00:14:58,810
reasoning abilities from llms.

224
00:15:00,370 --> 00:15:04,320
But there are techniques that can be used to

225
00:15:07,090 --> 00:15:10,714
fine tune our large language models

226
00:15:10,842 --> 00:15:14,254
to elicit or improve the reasoning abilities

227
00:15:14,382 --> 00:15:17,666
so we'll be seeing that in this section of

228
00:15:17,688 --> 00:15:22,130
the talk. So one paper that does that is learning

229
00:15:22,200 --> 00:15:26,550
math reasoning from cell sample, the correct and partially correct solution.

230
00:15:27,210 --> 00:15:30,886
Here the authors have authors use

231
00:15:31,068 --> 00:15:35,974
LLM. I think in these case they have used GPT Neo 2.7

232
00:15:36,012 --> 00:15:39,298
billion model. So for

233
00:15:39,324 --> 00:15:42,714
a given set of question, they ask the model to generate a

234
00:15:42,752 --> 00:15:46,554
solution, a pythonic solution, and then they

235
00:15:46,592 --> 00:15:50,374
evaluate the answer. When the answer matches with the ground truth or gold

236
00:15:50,422 --> 00:15:53,982
answers that they have, they use these solution, they use

237
00:15:54,036 --> 00:15:58,218
that in the fine tuning data set. Similarly they generate

238
00:15:58,394 --> 00:16:02,250
solutions for whatever generations

239
00:16:02,330 --> 00:16:05,742
that had got correct answers.

240
00:16:05,886 --> 00:16:09,250
They filter those generation and then they

241
00:16:09,320 --> 00:16:12,610
iteratively fine tune the same model on that,

242
00:16:12,760 --> 00:16:16,262
same model on that. So they not only use

243
00:16:16,316 --> 00:16:20,146
the fully correct solution, they also have introduced

244
00:16:20,178 --> 00:16:23,858
a methodology to utilize partially correct solutions.

245
00:16:23,954 --> 00:16:28,060
So the way they do partially correct solution is

246
00:16:28,830 --> 00:16:34,010
they have a gold

247
00:16:34,590 --> 00:16:39,782
solutions where they have outputs for individual

248
00:16:39,936 --> 00:16:43,454
steps, as you see here and these similarly we

249
00:16:43,492 --> 00:16:47,226
have a generated solution with individual outputs

250
00:16:47,258 --> 00:16:50,880
from each of these steps. So whenever there is a match between

251
00:16:52,130 --> 00:16:55,662
these individual steps in gold and the generated

252
00:16:55,726 --> 00:16:59,794
one, they consider that as a partially correct solution and

253
00:16:59,832 --> 00:17:03,026
then they use that to further fine tune the model.

254
00:17:03,208 --> 00:17:06,742
So they have shown in their paper

255
00:17:06,796 --> 00:17:10,854
that this type of iterative fine tuning on the

256
00:17:10,892 --> 00:17:13,590
model generated solutions,

257
00:17:14,490 --> 00:17:18,386
there is an improvement in mathematical reasoning

258
00:17:18,418 --> 00:17:21,690
abilities of the model. So if you can see, the green

259
00:17:21,760 --> 00:17:25,526
ones are the one which are fine tuned only on fully

260
00:17:25,718 --> 00:17:29,642
correct solution and the orange ones are the one that are self

261
00:17:29,696 --> 00:17:33,710
sampled with fully correct and partially correct solutions.

262
00:17:34,050 --> 00:17:37,806
And we can also observe that the pass at one rate is

263
00:17:37,828 --> 00:17:41,934
not improved. So the authors, authors comment that

264
00:17:42,052 --> 00:17:46,260
this is mainly because the nature of

265
00:17:46,710 --> 00:17:49,986
these training facilitates the

266
00:17:50,008 --> 00:17:54,914
model to generate diverse set of solutions and it

267
00:17:54,952 --> 00:17:59,134
does not make the model to favor any one particular solution.

268
00:17:59,182 --> 00:18:02,774
That's why the pass at one is not improved much, but you can

269
00:18:02,812 --> 00:18:07,730
see other improvements in other passet k values.

270
00:18:07,890 --> 00:18:11,734
So another paper that does something similar is

271
00:18:11,852 --> 00:18:15,482
self taught reasoner bootstrapping, reasoning with reasoning or

272
00:18:15,536 --> 00:18:18,410
star. So here in this methodology,

273
00:18:19,390 --> 00:18:23,834
the authors generate rational

274
00:18:23,882 --> 00:18:27,694
and answers from an existing large language model.

275
00:18:27,892 --> 00:18:31,422
And then whenever the answer is correct for that

276
00:18:31,476 --> 00:18:34,846
particular gold standards, when they compare it with

277
00:18:34,868 --> 00:18:38,562
the gold ground root data set that they have, they take

278
00:18:38,616 --> 00:18:42,562
that, they put that into a fine tuning coppers along with

279
00:18:42,696 --> 00:18:46,462
as a triplet, as question, rational and answer. So whenever

280
00:18:46,526 --> 00:18:49,762
the answer is wrong, they hint the model

281
00:18:49,816 --> 00:18:53,714
to generate a correct rationale by giving the correct answer from the ground

282
00:18:53,762 --> 00:18:57,222
truth. So they ask the model to generate a rational and then they

283
00:18:57,276 --> 00:19:00,470
put that back into the fine tuning mixture.

284
00:19:00,630 --> 00:19:04,570
So that way they fine tune the model again so

285
00:19:04,720 --> 00:19:08,374
that fine tuned model has a better ability to generate rational.

286
00:19:08,422 --> 00:19:12,014
So these do this iteratively and then they have

287
00:19:12,052 --> 00:19:15,294
a final model. So this has

288
00:19:15,332 --> 00:19:19,066
proved to improve the mathematical reasoning

289
00:19:19,098 --> 00:19:23,070
abilities even by using only the partial

290
00:19:23,490 --> 00:19:26,914
training data set. So if you can see, the few short and

291
00:19:26,952 --> 00:19:30,914
then fine tuned abilities of the GPTJ model has

292
00:19:30,952 --> 00:19:34,770
improved from 5.8 to 10.7 here. So another

293
00:19:34,920 --> 00:19:38,614
variation to this approach is this is more of distilling from

294
00:19:38,652 --> 00:19:42,002
a large language model, a paper called specializing

295
00:19:42,066 --> 00:19:45,606
smaller language models towards multistep reasoning by

296
00:19:45,708 --> 00:19:49,530
few et al. Here the authors have tried to distill

297
00:19:50,910 --> 00:19:54,742
the reasoning steps as well as the solution

298
00:19:54,806 --> 00:19:59,642
from a large language model, a bigger model like from GPT-3

299
00:19:59,776 --> 00:20:03,786
and then they fine tuned a smaller model like different t five versions,

300
00:20:03,978 --> 00:20:08,494
different t five versions 250,000,000 760,000,003

301
00:20:08,532 --> 00:20:12,030
billion. So they tried two different

302
00:20:12,100 --> 00:20:15,570
variations. One is fine tuning only

303
00:20:15,640 --> 00:20:19,378
on answers and then fine tuning on both answers and

304
00:20:19,464 --> 00:20:22,786
chain of thought steps. So they found that

305
00:20:22,888 --> 00:20:26,790
fine tuning on chain of thought and answers

306
00:20:27,130 --> 00:20:30,742
are giving better accuracy as the model also

307
00:20:30,796 --> 00:20:34,486
tries to understand the rationale of the answers. And we

308
00:20:34,508 --> 00:20:37,666
could see the improvement here. Similarly,

309
00:20:37,698 --> 00:20:41,546
they have tried that not only to vanilla Tfi but also to

310
00:20:41,568 --> 00:20:45,990
flan t five. So flan t five shows a better improvement

311
00:20:46,070 --> 00:20:50,380
compared to the vanilla t five models. So another

312
00:20:50,690 --> 00:20:54,526
recent approach in distilling is

313
00:20:54,548 --> 00:20:58,586
these distilling step by step paper from Hashe et

314
00:20:58,618 --> 00:21:02,202
al. So here for an unlabeled

315
00:21:02,266 --> 00:21:06,178
set of data set, they use a large language model like

316
00:21:06,264 --> 00:21:10,226
a palm or a palm or a gpt-3

317
00:21:10,328 --> 00:21:13,934
models to generate labels. Not only labels,

318
00:21:13,982 --> 00:21:17,178
they also ask the model to generate the rational or chain

319
00:21:17,214 --> 00:21:20,726
of starts for this particular answer,

320
00:21:20,908 --> 00:21:24,034
particular answer. And then when they distill

321
00:21:24,082 --> 00:21:27,538
it and train a smaller model, they train it on an objective

322
00:21:27,714 --> 00:21:31,370
similar to a multitask planning. So they ask the model

323
00:21:31,440 --> 00:21:34,726
to predict a label as well as the rational

324
00:21:34,758 --> 00:21:38,026
for it instead of concatenating the rational and

325
00:21:38,048 --> 00:21:41,194
label as one chunk. They had approached

326
00:21:41,242 --> 00:21:44,894
this as a multitask planning. And then they have shown that these gives

327
00:21:44,932 --> 00:21:49,002
a better improvement in improving

328
00:21:49,066 --> 00:21:54,174
the smaller models reasoning

329
00:21:54,222 --> 00:21:57,954
abilities. So the loss they have done is

330
00:21:58,072 --> 00:22:01,534
they had taken a weighted loss of label

331
00:22:01,582 --> 00:22:04,706
loss as well as the rational loss generation of

332
00:22:04,728 --> 00:22:08,082
rational. So they show that these models,

333
00:22:08,226 --> 00:22:12,386
the fine tuned model in this distilling step by step model performs

334
00:22:12,418 --> 00:22:16,438
even better than a 540,000,000,000 model in

335
00:22:16,524 --> 00:22:20,150
540,000,000,000 models, a few short generations.

336
00:22:20,310 --> 00:22:23,722
So you could see that t five to 20 million sound,

337
00:22:23,776 --> 00:22:26,986
70 million and 11 billion doing a better job

338
00:22:27,088 --> 00:22:31,310
in the mathematical reasoning and common

339
00:22:31,380 --> 00:22:34,794
sense Qa common sense Qa

340
00:22:34,842 --> 00:22:38,782
task. So until now we saw how

341
00:22:38,916 --> 00:22:42,478
a generation is made at one shot, that is,

342
00:22:42,644 --> 00:22:47,006
given a set of prompt with few shot or zero shot examples.

343
00:22:47,198 --> 00:22:51,678
The model generates in one shot the reasoning

344
00:22:51,774 --> 00:22:54,500
as well as the answer for the given problem.

345
00:22:55,690 --> 00:22:58,934
But as a human like how,

346
00:22:59,052 --> 00:23:02,658
if we had, if we approach a problem iteratively,

347
00:23:02,754 --> 00:23:06,118
we have a better chance to solve the problem more

348
00:23:06,284 --> 00:23:09,862
accurately. So, similar intuition has been tried

349
00:23:09,916 --> 00:23:13,820
with these llms. So we will be seeing those

350
00:23:14,350 --> 00:23:17,270
methodologies in the papers in this section.

351
00:23:17,430 --> 00:23:20,862
So one paper that implements is least

352
00:23:20,916 --> 00:23:24,862
to most prompting. So the

353
00:23:24,916 --> 00:23:28,926
idea of this paper is that they have broken down the

354
00:23:28,948 --> 00:23:33,698
approach into two stages. The first stage they

355
00:23:33,784 --> 00:23:37,570
prompt these model to come up with sub questions. So given

356
00:23:37,640 --> 00:23:41,426
a broader question, they prompt the model to come up with

357
00:23:41,528 --> 00:23:45,090
sub questions. And in the stage two

358
00:23:45,160 --> 00:23:49,206
they sequentially ask the model to solve the questions one

359
00:23:49,228 --> 00:23:53,078
by one. So for example, for the question given here,

360
00:23:53,244 --> 00:23:56,838
the first stage, the model will come up with sub questions,

361
00:23:57,004 --> 00:24:00,954
and in the second stage the original question is appended with the

362
00:24:00,992 --> 00:24:04,634
sub question and the model has to answer that sub

363
00:24:04,672 --> 00:24:08,394
question. And then the second sub question will be appended to

364
00:24:08,432 --> 00:24:12,366
the first one and the overall one. And then the model has to

365
00:24:12,388 --> 00:24:16,126
answer that and then until it comes up with the final answer.

366
00:24:16,308 --> 00:24:20,158
So this way the author has shown that the model does

367
00:24:20,244 --> 00:24:24,046
better compared to the vanilla

368
00:24:24,078 --> 00:24:27,986
chain of thought prompting that we saw before. So this

369
00:24:28,008 --> 00:24:32,926
is an example prompts that's been implemented

370
00:24:32,958 --> 00:24:37,266
as part of this paper here. These have for the decomposition

371
00:24:37,378 --> 00:24:41,190
stage where they ask the model to decompose the question into

372
00:24:41,260 --> 00:24:44,374
different questions.

373
00:24:44,572 --> 00:24:48,038
These were these few short examples that was given. And then

374
00:24:48,124 --> 00:24:51,366
for the new set of example following this few shot,

375
00:24:51,398 --> 00:24:55,500
the model has to come up with sub questions.

376
00:24:57,150 --> 00:25:00,102
And the second part,

377
00:25:00,176 --> 00:25:03,726
second stage where for problem solving, these are all the

378
00:25:03,748 --> 00:25:07,758
few short examples that were given for the model to

379
00:25:07,844 --> 00:25:10,800
solve the sub problem.

380
00:25:11,970 --> 00:25:15,954
So another paper that implemented the recursive or

381
00:25:15,992 --> 00:25:20,242
iterative prompting is

382
00:25:20,296 --> 00:25:24,430
plan, eliminate and track. So they have done this in an interesting setting

383
00:25:24,590 --> 00:25:28,134
where they have tried to evaluate and embodied the agent on

384
00:25:28,172 --> 00:25:31,702
a data set called as half world

385
00:25:31,836 --> 00:25:35,762
data set, which is about evaluating

386
00:25:35,826 --> 00:25:39,206
the abilities of the agent to follow a given task

387
00:25:39,318 --> 00:25:43,114
given the text word environment and a visual equivalent of

388
00:25:43,152 --> 00:25:47,260
it. So they have

389
00:25:49,470 --> 00:25:52,686
broken down their approach into different modules. One is a

390
00:25:52,708 --> 00:25:56,286
planner module which is also can LLM. So it takes in the

391
00:25:56,308 --> 00:26:00,014
instruction, it tries to convert that into a plan

392
00:26:00,212 --> 00:26:03,950
that the agent needs to follow. And then there is an eliminator.

393
00:26:04,030 --> 00:26:07,234
So eliminator based on the visual input and

394
00:26:07,272 --> 00:26:10,514
also on the visual input of what is

395
00:26:10,552 --> 00:26:14,382
there in the environment. The eliminator

396
00:26:14,526 --> 00:26:17,798
tries to eliminate whatever that

397
00:26:17,884 --> 00:26:20,998
the agent sees and what it needs to focus

398
00:26:21,084 --> 00:26:25,122
on. And these the actor does the action and the tracker

399
00:26:25,186 --> 00:26:28,966
tracks whether a given task is finished or not. Once it's finished,

400
00:26:28,998 --> 00:26:33,100
it is updating the progress. So this overall approach is

401
00:26:33,870 --> 00:26:37,398
in a way it's similar to what has been followed in auto

402
00:26:37,414 --> 00:26:41,690
GPT and Baby Aga and all those applications.

403
00:26:42,430 --> 00:26:45,854
So if you see here first for the task, heat some apple and

404
00:26:45,892 --> 00:26:49,566
put it in the fridge. The LLMs first comes up with a plan. Take an

405
00:26:49,588 --> 00:26:52,350
apple, heat the apple, place the apple in fridge,

406
00:26:54,050 --> 00:26:58,766
and then an eliminator eliminates what

407
00:26:58,788 --> 00:27:02,510
are the things that are not important for this particular

408
00:27:02,660 --> 00:27:05,926
task to be completed. And an actor picks up

409
00:27:05,948 --> 00:27:09,206
the, excuse me,

410
00:27:09,308 --> 00:27:13,826
the actor picks up the action

411
00:27:13,858 --> 00:27:16,978
that is more suitable for that particular task.

412
00:27:17,074 --> 00:27:20,620
And then a tracker tracks the progress of it.

413
00:27:24,290 --> 00:27:28,286
Another interesting paper that does this

414
00:27:28,388 --> 00:27:32,146
iterative prompting is describe, explain,

415
00:27:32,248 --> 00:27:36,110
plan and select. So in this paper, the authors

416
00:27:36,190 --> 00:27:40,258
have tried to use an LLM to solve or

417
00:27:40,424 --> 00:27:43,774
to play Minecraft. So minecraft,

418
00:27:43,822 --> 00:27:47,922
as you'd know, it's an open ended game. So they have used llms

419
00:27:47,986 --> 00:27:52,374
to play the minecraft. So here again,

420
00:27:52,492 --> 00:27:54,280
similar to what we saw before,

421
00:27:55,610 --> 00:27:58,854
they have split the approach

422
00:27:58,902 --> 00:28:02,534
into different modules. One is planner module,

423
00:28:02,662 --> 00:28:06,554
selector module and explainer module, and then a

424
00:28:06,592 --> 00:28:10,874
describer module. So the first, for example

425
00:28:11,072 --> 00:28:14,510
for these task how to mine one diamond from scratch.

426
00:28:15,170 --> 00:28:18,766
The planner module comes up with a set of plans of what are

427
00:28:18,788 --> 00:28:22,638
the tasks that needs to be done by the agent.

428
00:28:22,804 --> 00:28:26,126
So here from the different set of ushot

429
00:28:26,158 --> 00:28:29,474
examples in the ground truth plan, the planner would

430
00:28:29,512 --> 00:28:32,814
come up with an actual plan that needs to be executed.

431
00:28:32,942 --> 00:28:36,694
And then the selector, based on its knowledge of the

432
00:28:36,732 --> 00:28:40,406
environment, it selects in that given step what

433
00:28:40,588 --> 00:28:43,782
a goal that needs to be achieved. First based

434
00:28:43,836 --> 00:28:47,610
on prioritizing

435
00:28:48,270 --> 00:28:50,570
the different tasks involved.

436
00:28:52,270 --> 00:28:55,946
And then that particular task is executed by

437
00:28:55,968 --> 00:28:59,514
an executor. And that result of the executor is

438
00:28:59,552 --> 00:29:02,622
given as a description by the descriptor. So it says,

439
00:29:02,676 --> 00:29:06,270
if it finishes a goal, it says I finished goal g zero,

440
00:29:06,420 --> 00:29:10,350
and then the selector goes to the next

441
00:29:10,500 --> 00:29:13,666
task or next goal and so on. So this

442
00:29:13,688 --> 00:29:15,650
is done recursively,

443
00:29:17,670 --> 00:29:20,686
this is done by an LLM, it is prone to failures.

444
00:29:20,798 --> 00:29:23,790
So when a particular plan has failed,

445
00:29:23,870 --> 00:29:27,810
so these descriptor says I fail on this particular goal.

446
00:29:27,890 --> 00:29:31,298
And then it also gives the details of the environment.

447
00:29:31,474 --> 00:29:35,254
So based on that, an explainer explains what could

448
00:29:35,292 --> 00:29:39,180
have actually gone wrong and it explains what needs to be done.

449
00:29:39,630 --> 00:29:43,434
And then that goes to the planner. So planner then does

450
00:29:43,472 --> 00:29:47,066
the replanning again. Replanning again.

451
00:29:47,168 --> 00:29:51,022
And then this process continues until the final objective is

452
00:29:51,076 --> 00:29:54,366
met. So this is again a very interesting

453
00:29:54,468 --> 00:29:58,762
paper. I would urge the audience

454
00:29:58,826 --> 00:30:02,786
to go to the GitHub repo and go through there. They have done a

455
00:30:02,808 --> 00:30:05,570
wonderful implementation of their approach.

456
00:30:10,150 --> 00:30:14,186
So until now we have seen how recursive

457
00:30:14,238 --> 00:30:18,274
and iterative prompting can be used to elicit

458
00:30:18,322 --> 00:30:22,246
reason. So now the

459
00:30:22,268 --> 00:30:26,654
recent advancements have been enabled these LLMs

460
00:30:26,722 --> 00:30:30,570
to use tools. So that comes in handy to

461
00:30:30,720 --> 00:30:34,774
make the model even more accurate when it comes to reasoning

462
00:30:34,822 --> 00:30:38,202
and planning. So you'll see a few examples of

463
00:30:38,256 --> 00:30:41,434
how the tool usage is implemented

464
00:30:41,482 --> 00:30:43,840
in some of the literature work.

465
00:30:45,090 --> 00:30:48,794
So here the paper is react, reason and hacked.

466
00:30:48,922 --> 00:30:52,334
So in these paper, the authors,

467
00:30:52,462 --> 00:30:58,082
Yoert hall had broken down a

468
00:30:58,136 --> 00:31:03,714
reasoning question to

469
00:31:03,752 --> 00:31:07,814
use a tool like searching Wikipedia or

470
00:31:07,932 --> 00:31:11,702
looking up Wikipedia, and then do and

471
00:31:11,756 --> 00:31:15,350
come up with an answer based on that. So if you see here

472
00:31:15,500 --> 00:31:18,762
for this reasoning question, when the model

473
00:31:18,816 --> 00:31:22,186
tries to come up with a direct answer, it gets it

474
00:31:22,208 --> 00:31:26,282
wrong. Even with cot, this is getting it wrong because

475
00:31:26,336 --> 00:31:29,994
for this particular question, aside from Apple remote,

476
00:31:30,042 --> 00:31:34,014
what other devices can control the program Apple remote was

477
00:31:34,052 --> 00:31:38,154
originally designed to interact with? It needs an external

478
00:31:38,202 --> 00:31:41,360
information for the model to be relied on.

479
00:31:41,830 --> 00:31:44,994
So both these approaches are failing there.

480
00:31:45,112 --> 00:31:49,154
So in act approach, what they

481
00:31:49,192 --> 00:31:52,242
do is they come up with different set

482
00:31:52,296 --> 00:31:56,530
of, so they have two variations. One is act only approach.

483
00:31:56,610 --> 00:32:00,566
So in act only approach they come up with different actions that needs to

484
00:32:00,588 --> 00:32:05,238
be taken and then they come up with an answer.

485
00:32:05,404 --> 00:32:09,126
Then we have react which is based on reason and act which

486
00:32:09,148 --> 00:32:12,970
is the actual paper. So first they come up with a different thought

487
00:32:13,120 --> 00:32:16,874
and what act needs to be done, action needs to be done. And what

488
00:32:16,912 --> 00:32:20,614
is the observation that is done from this action.

489
00:32:20,742 --> 00:32:24,398
So based on that, that gets passed on to the next thought and

490
00:32:24,484 --> 00:32:28,286
that is used to do the second act, and then observation and so

491
00:32:28,308 --> 00:32:31,054
on. Finally they come up with these can answer.

492
00:32:31,252 --> 00:32:34,962
So this way they iteratively prompt it. First thought

493
00:32:35,016 --> 00:32:39,150
one is done and these act one action is generated

494
00:32:39,230 --> 00:32:42,750
based on that. Once we have this, once the model generates

495
00:32:42,830 --> 00:32:46,326
search the apple remote, the keyword is used to

496
00:32:46,348 --> 00:32:50,054
search the Wikipedia and then an observation is appended to

497
00:32:50,092 --> 00:32:53,382
the generation. So based on that a thought

498
00:32:53,436 --> 00:32:57,386
two is done until the model generates finish

499
00:32:57,488 --> 00:33:01,078
as one of the actions. So this is react,

500
00:33:01,174 --> 00:33:02,460
reason and act.

501
00:33:06,750 --> 00:33:10,186
Another paper that uses tools is

502
00:33:10,288 --> 00:33:13,694
camelion plug and play compositional reasoning with

503
00:33:13,732 --> 00:33:17,934
large language models. So here the authors have used

504
00:33:18,132 --> 00:33:22,646
tool based reasoning and compulsion

505
00:33:22,778 --> 00:33:26,050
for answering science question

506
00:33:26,120 --> 00:33:30,942
answers as well as to answer table

507
00:33:31,006 --> 00:33:34,654
based word problems. So here

508
00:33:34,712 --> 00:33:38,678
in this science based question answering or

509
00:33:38,764 --> 00:33:42,422
common sense question answering, here the image is given

510
00:33:42,556 --> 00:33:46,402
and the question is what is the direction of this push? And then an

511
00:33:46,476 --> 00:33:49,980
image is given and there is a question related to this.

512
00:33:51,070 --> 00:33:54,954
So the LLM first tries to come up

513
00:33:54,992 --> 00:33:59,334
with or decomposes this problem into set of

514
00:33:59,472 --> 00:34:02,794
tools that it needs to call. And then there is a separate

515
00:34:02,842 --> 00:34:06,174
set of prompts that are available for each of these

516
00:34:06,212 --> 00:34:09,818
tools, which gets executed sequentially

517
00:34:09,994 --> 00:34:13,474
to invoke that particular tool

518
00:34:13,592 --> 00:34:16,786
and get these answers that get appended to the

519
00:34:16,888 --> 00:34:20,386
original prompt. And these the process continues to get the

520
00:34:20,408 --> 00:34:24,100
final answer. For example, for this question where we have

521
00:34:25,050 --> 00:34:29,186
image and then a set of options, which is the main persuasive apple

522
00:34:29,218 --> 00:34:33,186
used in this ad. So this particular image

523
00:34:33,218 --> 00:34:36,758
is of an ADC. Paper plates now carry

524
00:34:36,774 --> 00:34:40,282
the Sierra Club seal of approval. So it's an

525
00:34:40,336 --> 00:34:44,554
ad. And then we have different options

526
00:34:44,672 --> 00:34:48,940
whether this ad conveys petals, ethos or

527
00:34:50,050 --> 00:34:53,694
logos, and these has

528
00:34:53,732 --> 00:34:57,214
different options. So what the Cameleon does is it

529
00:34:57,252 --> 00:35:00,846
first tries to call the text deductor as a

530
00:35:01,028 --> 00:35:04,900
tool, and the text deductor deducts the text

531
00:35:05,430 --> 00:35:09,250
and then it calls the knowledge retrieval. So knowledge retrieval

532
00:35:10,310 --> 00:35:13,326
based on the input that is there, the knowledge retrieval

533
00:35:13,358 --> 00:35:17,334
tries to come up with its inference of

534
00:35:17,532 --> 00:35:21,206
the overall perspective of question and the information

535
00:35:21,308 --> 00:35:26,070
that is available at that point. This is from

536
00:35:26,140 --> 00:35:29,290
call to an opena API. And then

537
00:35:29,360 --> 00:35:33,254
there is a solution generator which creates a descriptive

538
00:35:33,302 --> 00:35:36,490
solution of what needs to be done, and then the answer

539
00:35:36,560 --> 00:35:40,186
generator, which could be a rule based approach to generate the answers

540
00:35:40,218 --> 00:35:43,854
from the solution that was generated by the model.

541
00:35:43,972 --> 00:35:47,610
So these way this paper uses different tools,

542
00:35:47,690 --> 00:35:50,110
right from hugging face models,

543
00:35:51,090 --> 00:35:55,022
and then open a GPT models iteratively,

544
00:35:55,086 --> 00:35:58,466
and also the other models like text deductor to

545
00:35:58,488 --> 00:36:00,290
come up with a final solution.

546
00:36:04,810 --> 00:36:08,760
Another example is the tab math world

547
00:36:09,290 --> 00:36:13,110
problem solving data set, wherein for a given tabular

548
00:36:13,850 --> 00:36:17,122
example, for a question that was asked,

549
00:36:17,196 --> 00:36:21,066
the model has to come up with set of answers. So here the

550
00:36:21,088 --> 00:36:24,746
model again uses different tools like knowledge retriever to

551
00:36:24,768 --> 00:36:28,742
retrieve the knowledge that it has related

552
00:36:28,806 --> 00:36:32,602
to the question that was asked. And then it goes to the table

553
00:36:32,666 --> 00:36:35,482
verbalizer to verbalize what is there in the table,

554
00:36:35,626 --> 00:36:38,666
and then for all the calculation that is offloaded

555
00:36:38,698 --> 00:36:42,434
to a Python program and interpreter. And then

556
00:36:42,552 --> 00:36:46,786
we have a program verifier which verifies whether the program is correct,

557
00:36:46,888 --> 00:36:50,622
and then the program is executed and the answer is generated

558
00:36:50,686 --> 00:36:54,514
from it. So let's see how a prompt

559
00:36:54,562 --> 00:36:56,040
looks like for this.

560
00:36:59,130 --> 00:37:02,598
So here as we can see,

561
00:37:02,764 --> 00:37:06,838
the instruction or the prompt has different

562
00:37:06,924 --> 00:37:10,826
tools that the model can use and then it also has the

563
00:37:10,848 --> 00:37:14,442
context of what is the question and what are all the options

564
00:37:14,496 --> 00:37:17,290
that are there in these question and metadata of the image.

565
00:37:18,190 --> 00:37:21,786
And the model has to generate the set of modules

566
00:37:21,818 --> 00:37:25,614
that it has to call, set of steps that it has

567
00:37:25,652 --> 00:37:28,990
to execute, whether it has to execute text reductor first,

568
00:37:29,060 --> 00:37:32,606
knowledge retrieval solution generator and answer generator.

569
00:37:32,718 --> 00:37:36,446
So this way these model comes up with steps

570
00:37:36,478 --> 00:37:40,274
and then each of these separate tools are

571
00:37:40,392 --> 00:37:44,046
prompted to get the output. And then finally all

572
00:37:44,088 --> 00:37:47,906
these outputs from these individual tools are concatenated

573
00:37:47,938 --> 00:37:51,110
into one sequentially to generate the final

574
00:37:51,180 --> 00:37:54,294
answer. So yeah,

575
00:37:54,492 --> 00:37:57,994
that's pretty much I had for today. So there is

576
00:37:58,032 --> 00:38:00,570
a lot that has come out recently.

577
00:38:01,710 --> 00:38:04,858
Probably I might not have had a

578
00:38:04,864 --> 00:38:08,794
chance to include it here, like tool formers, hugging GPT and so

579
00:38:08,832 --> 00:38:14,238
on, but I

580
00:38:14,244 --> 00:38:17,882
would like to acknowledge the sources

581
00:38:17,946 --> 00:38:21,354
for this presentation. One is augment language

582
00:38:21,402 --> 00:38:25,514
models survey from Milan

583
00:38:25,562 --> 00:38:29,134
et al. And towards reasoning language models survey from Huang

584
00:38:29,182 --> 00:38:32,830
et al. And then these blog posts. I will also urge

585
00:38:32,910 --> 00:38:36,642
the audience to go through these

586
00:38:36,776 --> 00:38:40,766
papers and these blog posts if you'd

587
00:38:40,798 --> 00:38:44,546
like to learn further on this topic. So thank

588
00:38:44,568 --> 00:38:48,060
you very much for your attention and

589
00:38:48,830 --> 00:38:52,582
looking forward to hearing your feedback on these session

590
00:38:52,726 --> 00:38:56,426
and also to have discussions on this topic can

591
00:38:56,448 --> 00:38:57,960
be discussed today. Thank you.

