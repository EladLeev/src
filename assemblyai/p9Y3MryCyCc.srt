1
00:00:36,508 --> 00:00:39,734
My talk I'm very happy to be here at

2
00:00:39,772 --> 00:00:43,110
the 2023 edition of Conf

3
00:00:43,180 --> 00:00:46,838
42 cloud native track. Thank you

4
00:00:47,004 --> 00:00:51,050
to everyone for listening to me talk. My topic

5
00:00:51,130 --> 00:00:54,634
is about event driven change data capture

6
00:00:54,682 --> 00:00:58,046
pattern using Apache Pulsar. You might be wondering, well,

7
00:00:58,068 --> 00:01:01,454
what does it have to do with cloud native kind

8
00:01:01,492 --> 00:01:05,458
of this track? And in fact I wanted to

9
00:01:05,544 --> 00:01:09,410
point out to you that this particular pattern

10
00:01:09,750 --> 00:01:13,570
in fact is ideal if you are running things

11
00:01:13,640 --> 00:01:17,446
in a cloud, a cloud native kind of environment. And I'll explain

12
00:01:17,548 --> 00:01:21,526
shortly. So this particular pattern, let's first take a look and

13
00:01:21,548 --> 00:01:25,400
see what it is. But before we do that,

14
00:01:25,770 --> 00:01:28,746
let me have a quick introduction of myself.

15
00:01:28,848 --> 00:01:32,060
If you have not heard me talk before,

16
00:01:33,070 --> 00:01:36,842
my name is Mary Grygleski. I'm a streaming developer advocate at

17
00:01:36,896 --> 00:01:40,474
Datastax. Datastax is a company that's based in

18
00:01:40,512 --> 00:01:45,170
California, and Datastax

19
00:01:45,190 --> 00:01:47,790
specializes in data management,

20
00:01:48,450 --> 00:01:52,062
type of software and the flagship product if you really

21
00:01:52,116 --> 00:01:55,166
is based on Apache Cassandra,

22
00:01:55,278 --> 00:01:59,122
if you're familiar with NoSQL, I'm sure you will be very

23
00:01:59,176 --> 00:02:02,946
familiar with Apache Cassandra as well. So we are

24
00:02:02,968 --> 00:02:06,214
the company,

25
00:02:06,332 --> 00:02:09,474
commercial company behind this Apache Cassandra,

26
00:02:09,602 --> 00:02:13,682
and also essentially too, it is a NoSQL

27
00:02:13,746 --> 00:02:17,462
database supporting big data, very fast and

28
00:02:17,516 --> 00:02:21,242
very efficient. So much like that too.

29
00:02:21,296 --> 00:02:25,206
Our streaming platform, which is newer, about two years old, and that's

30
00:02:25,238 --> 00:02:28,362
where I'm working for, streaming platform essentially is also open

31
00:02:28,416 --> 00:02:31,726
source based, and that's on Apache Pulsar. And that's what I'm going to

32
00:02:31,748 --> 00:02:35,786
spend some time today talking about is Apache Pulsar.

33
00:02:35,978 --> 00:02:39,482
It's an event streaming platform really built ideally

34
00:02:39,546 --> 00:02:43,126
for the cloud native platform. Also one note

35
00:02:43,178 --> 00:02:46,610
to point out is that Datastax also has recently acquired another

36
00:02:46,680 --> 00:02:49,886
company called Cascada, and that's

37
00:02:49,918 --> 00:02:53,554
a real time AI software. So company is

38
00:02:53,592 --> 00:02:57,622
also moving into that direction. I mean, given the fact that we already have data

39
00:02:57,676 --> 00:03:01,554
in motion very strong with Apache Cassandra, then we're also supporting

40
00:03:01,602 --> 00:03:05,526
data in motion, this event streaming platform. Then it comes time

41
00:03:05,548 --> 00:03:08,722
to really apply it to higher forms of specialty,

42
00:03:08,786 --> 00:03:12,106
which is the real time AI. So that's something new. I just want to

43
00:03:12,128 --> 00:03:15,546
point out to you, that's the company I'm working for right now, have been

44
00:03:15,568 --> 00:03:18,842
here for a year and then previously too I worked

45
00:03:18,896 --> 00:03:22,086
for IBM as a Java developer advocate.

46
00:03:22,198 --> 00:03:26,506
My areas back then included reactive

47
00:03:26,538 --> 00:03:30,554
systems and taken also the open source websphere

48
00:03:30,602 --> 00:03:34,094
side of things, which is the open Liberty microprofile

49
00:03:34,142 --> 00:03:37,326
Jakarta Enterprise system, the Jakarta ee

50
00:03:37,358 --> 00:03:41,134
two. So those are kind of my area. I'm based in Chicago,

51
00:03:41,182 --> 00:03:45,650
I'm also a Java champion I'm also a community builder.

52
00:03:46,010 --> 00:03:50,118
I currently lead the Chicago Java users group. I'm the president

53
00:03:50,204 --> 00:03:54,546
of that since beginning of pandemic and I'm

54
00:03:54,578 --> 00:03:58,022
still keeping it strong. And I

55
00:03:58,076 --> 00:04:02,330
also actually, well, I should give you a bit of background, is that I previously

56
00:04:03,390 --> 00:04:07,660
was an engineer myself, so I have hands on experience too,

57
00:04:08,030 --> 00:04:11,742
but have become an advocate since about five years ago.

58
00:04:11,876 --> 00:04:15,454
And so my area includes a lot of software engineering work,

59
00:04:15,572 --> 00:04:18,958
design development, as well as some

60
00:04:19,044 --> 00:04:22,542
technical architecture as well. So my area too,

61
00:04:22,596 --> 00:04:25,838
that I came from primarily Java open source,

62
00:04:26,014 --> 00:04:30,420
then leading into cloud systems too, and DevOps and all those things.

63
00:04:30,790 --> 00:04:34,754
So that's briefly about me. And now let's then show

64
00:04:34,792 --> 00:04:38,786
you the agenda. We're going to talk about change data capture

65
00:04:38,978 --> 00:04:42,882
CDC now, as some of you based in the US CDC,

66
00:04:42,946 --> 00:04:46,758
do not mistake that with our center

67
00:04:46,844 --> 00:04:50,874
for Disease Control, not that change data capture here is

68
00:04:50,912 --> 00:04:54,314
what we're talking about. And the purpose of such is to

69
00:04:54,352 --> 00:04:57,690
serve databases, data sources such as

70
00:04:57,760 --> 00:05:00,826
databases, data lakes, data warehouses.

71
00:05:00,938 --> 00:05:04,974
And I also want to point out a bit of a history of

72
00:05:05,012 --> 00:05:08,446
CDC, how it came about, basically pointing out

73
00:05:08,468 --> 00:05:11,642
ETL, extract, transform, load,

74
00:05:11,786 --> 00:05:15,746
and then I want to introduce to you the components of

75
00:05:15,768 --> 00:05:19,474
a CDC system, then also requirements of

76
00:05:19,592 --> 00:05:23,294
a truly modern day cloud native supported

77
00:05:23,342 --> 00:05:26,598
CDC system, what you should have

78
00:05:26,684 --> 00:05:30,594
the requirements for those. And then I'll introduce to you Apache Pulsar

79
00:05:30,642 --> 00:05:34,674
and why it's a good choice to be used in combination

80
00:05:34,722 --> 00:05:38,018
with, for example, Apache Cassandra

81
00:05:38,194 --> 00:05:42,214
that my company Datastax has too. So a CDC

82
00:05:42,262 --> 00:05:46,506
support for Astradb for the managed cloud platform,

83
00:05:46,688 --> 00:05:50,878
and also showing you a quick demo after that. So first,

84
00:05:50,964 --> 00:05:53,834
let's start, why change data capture?

85
00:05:53,962 --> 00:05:57,594
What is CDC for? Basically, I already pointed

86
00:05:57,642 --> 00:06:00,974
out earlier, is that it's to serve data sources such as

87
00:06:01,012 --> 00:06:04,634
databases or data lakes, or even like data

88
00:06:04,692 --> 00:06:08,482
warehouses too. Essentially, too, we're treating the data

89
00:06:08,536 --> 00:06:12,226
sources as the source of truth, of the data. That's what

90
00:06:12,248 --> 00:06:14,500
CDC has to serve. But the thing is,

91
00:06:15,430 --> 00:06:19,206
how does it kind of serve? Right? It serves it because data,

92
00:06:19,308 --> 00:06:23,318
as we know in today's world, is that the state of data

93
00:06:23,484 --> 00:06:27,122
can keep changing too. And we need to be able to capture

94
00:06:27,186 --> 00:06:31,202
those changes. Any kind of times that something happens

95
00:06:31,276 --> 00:06:35,414
to the data, we want to be able to make sure we don't lose

96
00:06:35,462 --> 00:06:39,290
track of it. We basically have to keep track of certain types of changes.

97
00:06:39,440 --> 00:06:43,142
And then with those data changes, we want to transform

98
00:06:43,206 --> 00:06:47,230
them too at certain times, right when we need to filter them out

99
00:06:47,380 --> 00:06:50,654
or enhance it, or do whatever is

100
00:06:50,692 --> 00:06:54,734
needed to the data. And then after the data has been transformed,

101
00:06:54,862 --> 00:06:58,382
then it will be propagated. These changes be propagated

102
00:06:58,446 --> 00:07:01,298
to downstream data systems too.

103
00:07:01,384 --> 00:07:05,910
So what we're looking at is basically CDC is capture changes

104
00:07:06,060 --> 00:07:10,274
in source of truth of data, and then do all sorts of manipulation

105
00:07:10,322 --> 00:07:13,858
that's needed and then propagate that downstream,

106
00:07:14,034 --> 00:07:17,486
and also then essentially sends it downstream

107
00:07:17,538 --> 00:07:20,602
and taken, that becomes what is called derived data

108
00:07:20,656 --> 00:07:24,826
source. And this is a very

109
00:07:24,928 --> 00:07:28,780
simplified illustration of CDC. I mean, in case

110
00:07:29,150 --> 00:07:32,398
just talking about it is not clear. So let's first take a

111
00:07:32,404 --> 00:07:35,726
look. Data sources, right? Basically there can be

112
00:07:35,748 --> 00:07:39,022
changes. So over here, up there too is looking

113
00:07:39,076 --> 00:07:42,766
at could be we are kind of using a soldier in some sort

114
00:07:42,788 --> 00:07:45,794
of monitoring type of process in there.

115
00:07:45,912 --> 00:07:49,922
Its job is to detect changes. So in the old ways too,

116
00:07:49,976 --> 00:07:54,274
very often we may be using polling to do things right. So essentially every

117
00:07:54,312 --> 00:07:58,214
so often you kind of go out and check your data source to

118
00:07:58,252 --> 00:08:00,630
see if there is any changes like that.

119
00:08:00,780 --> 00:08:03,522
But it's not just polling.

120
00:08:03,586 --> 00:08:06,838
Polling can become basically not as fast,

121
00:08:06,924 --> 00:08:10,722
because if something happens, we basically too, in today's world,

122
00:08:10,796 --> 00:08:14,506
we want things to happen in real time as it happens, we want

123
00:08:14,528 --> 00:08:17,626
to capture it. So doing polling may not

124
00:08:17,648 --> 00:08:21,406
be the way to do it, but it's just illustrating right this detection of

125
00:08:21,428 --> 00:08:24,734
changes can be. Polling can also be some

126
00:08:24,772 --> 00:08:28,474
form of database trigger, for example. But database

127
00:08:28,522 --> 00:08:32,334
trigger has, we know too, may not be an ideal kind of situation

128
00:08:32,452 --> 00:08:37,086
too, because the trigger can only be trigger

129
00:08:37,118 --> 00:08:40,786
it from maybe one database and map it to the database. So there may

130
00:08:40,808 --> 00:08:44,110
be subjected to some database type of limitations

131
00:08:44,190 --> 00:08:47,926
and the dependencies on the database itself. But the thing is,

132
00:08:47,948 --> 00:08:51,894
too, is that the idea is to detect changes and then

133
00:08:52,012 --> 00:08:55,174
essentially to then capture the changes too.

134
00:08:55,292 --> 00:08:58,986
Hold on a second. Capture the changes and taken,

135
00:08:59,088 --> 00:09:02,858
capturing it too, and then sends it down to a CDC system,

136
00:09:02,944 --> 00:09:06,730
which is to kind of transform.

137
00:09:07,150 --> 00:09:09,900
Oops, sorry. Okay,

138
00:09:10,670 --> 00:09:13,902
here we go. Okay, so we capture the

139
00:09:13,956 --> 00:09:17,242
changes, and then we capture the changes, and taken, we transform

140
00:09:17,306 --> 00:09:20,634
it. Sometimes we may want to do filter,

141
00:09:20,762 --> 00:09:24,750
do mapping, or do some sort of enhancement or even

142
00:09:24,820 --> 00:09:28,514
some sort of remediation maybe. We know that there are some

143
00:09:28,632 --> 00:09:32,030
data that comes through and we need to actually detect

144
00:09:32,110 --> 00:09:35,822
and also notice that if there are some sort of discrepancies,

145
00:09:35,886 --> 00:09:39,654
we need to adjust those data and all that. But in other

146
00:09:39,692 --> 00:09:43,782
words too, some sort of transformation that needs to happen to the data

147
00:09:43,836 --> 00:09:47,474
that's been captured and then from after it has been

148
00:09:47,532 --> 00:09:51,690
captured and basically to what we are doing then next will be

149
00:09:51,840 --> 00:09:55,402
to propagate it, to send that

150
00:09:55,456 --> 00:09:58,726
down to downstream data system. So that's

151
00:09:58,758 --> 00:10:02,766
what essentially this

152
00:10:02,788 --> 00:10:07,022
whole CDC process is regardless of how we want to implement it.

153
00:10:07,156 --> 00:10:10,414
But now then, let's take a look at what

154
00:10:10,452 --> 00:10:14,654
was there before CDC. Okay, so CDC is not new.

155
00:10:14,772 --> 00:10:18,942
In fact, it can be, CDC can be a slow way. But then today's

156
00:10:19,006 --> 00:10:22,018
talk is really about a new way of doing things in real time.

157
00:10:22,104 --> 00:10:25,666
Very responsive. Okay, so before CDCs,

158
00:10:25,698 --> 00:10:29,042
basically there's also a process called ETL,

159
00:10:29,106 --> 00:10:32,754
right? Short form describing it, extract,

160
00:10:32,802 --> 00:10:36,626
transform, load. As such, it describes the process of extracting

161
00:10:36,658 --> 00:10:40,394
the data from the sources and then transform it and then

162
00:10:40,432 --> 00:10:44,454
load it back into a target database

163
00:10:44,502 --> 00:10:47,786
or data source, some kind. So as we all know,

164
00:10:47,808 --> 00:10:51,790
right, ETL, it still has its place in the it

165
00:10:51,860 --> 00:10:55,294
world, don't get me wrong. But you have to bear in mind too,

166
00:10:55,412 --> 00:10:59,502
ETL traditionally is a very slow type of processing because

167
00:10:59,556 --> 00:11:03,354
it is synchronous in nature. Processing will occur

168
00:11:03,402 --> 00:11:06,994
in batches. So maybe it's collecting some data and

169
00:11:07,032 --> 00:11:10,686
extract a set of data. They are all being done. It's like bundled

170
00:11:10,798 --> 00:11:14,530
them up. It's ready. Now, maybe a certain size of this set of data

171
00:11:14,600 --> 00:11:18,278
are captured within a certain time period. From there,

172
00:11:18,364 --> 00:11:22,070
we kind of take that batch of data and then solely

173
00:11:23,210 --> 00:11:26,710
send it through another processing of transformation of some kind.

174
00:11:26,860 --> 00:11:30,860
As you can see, it's a very synchronous type of process, meaning that

175
00:11:32,830 --> 00:11:36,634
things are not decoupled, so to speak, too. So it

176
00:11:36,672 --> 00:11:40,154
kind of sends through the transformation, and then when the transformation is

177
00:11:40,192 --> 00:11:43,722
done, then it will be loaded back into some sort of data sources,

178
00:11:43,786 --> 00:11:47,646
right, so to speak. So as you can see, it can

179
00:11:47,668 --> 00:11:51,390
also kind of incur or use a

180
00:11:51,540 --> 00:11:54,562
wide, big network bandwidth too,

181
00:11:54,616 --> 00:11:58,786
because now we're dealing with data in bulk. And basically

182
00:11:58,888 --> 00:12:02,642
it requires a large set of data. Everything is being done okay,

183
00:12:02,696 --> 00:12:06,322
nicely, methodically, but synchronously too.

184
00:12:06,376 --> 00:12:09,734
So very slow. And the process set up too tends to be also

185
00:12:09,772 --> 00:12:13,222
very heavy if you kind of look around in today's world,

186
00:12:13,276 --> 00:12:15,846
right, there are ETL tools that are out there,

187
00:12:15,948 --> 00:12:19,414
produced by sometimes companies that are more catered

188
00:12:19,462 --> 00:12:23,574
for the older, traditional enterprise type of environments.

189
00:12:23,702 --> 00:12:27,078
These tools too, the licensing can be quite expensive,

190
00:12:27,174 --> 00:12:30,762
as you know, and it also very often make use of a lot of different

191
00:12:30,816 --> 00:12:34,138
database or replication behind the scenes, all of these things. And they tend

192
00:12:34,154 --> 00:12:38,266
to be expensive and less of an agility that's

193
00:12:38,298 --> 00:12:41,566
associated with it, essentially not able to keep up with

194
00:12:41,588 --> 00:12:45,550
the times. But then again, going back, right, it's not like it is bad

195
00:12:45,620 --> 00:12:49,006
or anything, but I think the key, the trick is that it depends

196
00:12:49,038 --> 00:12:52,210
on your usage circumstances. And in today's world,

197
00:12:52,280 --> 00:12:56,130
we're talking about. We want systems to respond in real time.

198
00:12:56,280 --> 00:13:00,034
We want data to capture data in bulk

199
00:13:00,082 --> 00:13:03,942
and process them as they come. Not wait for it, not like

200
00:13:03,996 --> 00:13:08,166
synchronously. We are kind of looking for ways of doing things in a more

201
00:13:08,268 --> 00:13:11,754
asynchronous time of fashion. And the advantage of such

202
00:13:11,792 --> 00:13:14,794
would be the speed that they can come through.

203
00:13:14,832 --> 00:13:18,758
Right. We want data to be ingested in high speed,

204
00:13:18,934 --> 00:13:22,766
high frequency of ingestion as well as processing them

205
00:13:22,868 --> 00:13:26,286
with very low latency. So now let's take

206
00:13:26,308 --> 00:13:29,262
a look know understanding this kind of needs.

207
00:13:29,316 --> 00:13:32,606
Then next comes to the components of a

208
00:13:32,628 --> 00:13:36,420
CDC system. What make up a CDC system.

209
00:13:37,350 --> 00:13:40,482
I already show you that picture. But let's kind of take a look

210
00:13:40,536 --> 00:13:45,070
a bit more like the components, right? So one is the change detection

211
00:13:45,150 --> 00:13:48,338
piece of it. This again we're looking at abstract,

212
00:13:48,434 --> 00:13:51,782
right? This change detection again can be a process,

213
00:13:51,916 --> 00:13:55,494
can also be, if you are on a Unix system it can also be

214
00:13:55,532 --> 00:13:59,814
maybe some sort of scripting language that write this change.

215
00:13:59,932 --> 00:14:03,206
Capturing changes to. I mean theoretically

216
00:14:03,318 --> 00:14:07,302
you can do that or doing a program that would go into the database

217
00:14:07,366 --> 00:14:11,258
and kind of sense the kind of changes that are occurring in

218
00:14:11,264 --> 00:14:14,666
the database. And of course in some cases it could be a file

219
00:14:14,698 --> 00:14:18,254
system too. It isn't limited to just a database. So some

220
00:14:18,292 --> 00:14:21,614
form of change detection processing process is needed.

221
00:14:21,732 --> 00:14:25,826
And then basically too from there you send it to any kind

222
00:14:25,848 --> 00:14:29,854
of change that happens. You capture it and sends it through this capturing

223
00:14:29,982 --> 00:14:34,194
engine process. And then from there it

224
00:14:34,232 --> 00:14:37,574
may or may not need transformation. But I think a lot of times

225
00:14:37,612 --> 00:14:41,254
too we probably want some sort of transformation. And then

226
00:14:41,292 --> 00:14:44,726
from there basically the changes will get propagated. It kind

227
00:14:44,748 --> 00:14:48,440
of sends it down to some target kind of system.

228
00:14:49,290 --> 00:14:52,714
So let's also take a look too, right? Traditionally too

229
00:14:52,752 --> 00:14:56,378
we might be doing capture data using

230
00:14:56,464 --> 00:14:59,750
SQL, all of these but yet doing polling.

231
00:14:59,830 --> 00:15:03,422
For example the SQL kind of you can go through like select

232
00:15:03,556 --> 00:15:07,390
some field and see if it has change. Maybe since a certain

233
00:15:07,460 --> 00:15:10,766
time you set some criteria to kind of test it.

234
00:15:10,948 --> 00:15:14,318
So that would be more of a SQL database kind of approach.

235
00:15:14,414 --> 00:15:17,806
But yet there's a way that actually it's

236
00:15:17,838 --> 00:15:21,662
probably more efficient. In fact it's more desirable

237
00:15:21,726 --> 00:15:24,750
would be using a log based type of approach for CDC.

238
00:15:24,830 --> 00:15:28,674
So let's take a look in here. So let's say we have two

239
00:15:28,872 --> 00:15:32,994
clients kind of writing to database. So let's say client

240
00:15:33,042 --> 00:15:36,598
one on here it set a variable, we kind

241
00:15:36,604 --> 00:15:40,202
of for example x equals to one and another client comes along

242
00:15:40,256 --> 00:15:43,418
and set variable x to two. So as

243
00:15:43,424 --> 00:15:46,486
you can see then right now there are some changes now to the column.

244
00:15:46,518 --> 00:15:49,754
We are monitoring the column changes x.

245
00:15:49,872 --> 00:15:53,726
So immediately then the data gets captured. But the thing is too is

246
00:15:53,748 --> 00:15:57,914
that what is happening now is that at the same time there are also transaction

247
00:15:57,962 --> 00:16:01,022
log messages that are sensing the changes

248
00:16:01,076 --> 00:16:04,698
and writing them. This is kind of recording

249
00:16:04,794 --> 00:16:08,386
those changes in a transaction log and they must also occur in

250
00:16:08,408 --> 00:16:12,162
the order as they happen too. That part is very important.

251
00:16:12,296 --> 00:16:16,146
So the transaction log will capture all of the changes. And then

252
00:16:16,328 --> 00:16:20,322
the next thing to do is basically then propagate these change to downstream

253
00:16:20,386 --> 00:16:23,606
system. Now we apply the changes. We first kind

254
00:16:23,628 --> 00:16:27,346
of take the x equals one writing to the database,

255
00:16:27,378 --> 00:16:30,666
to the target data system. And then the next thing we know is that it

256
00:16:30,688 --> 00:16:33,578
gets overwritten because there's set x equals two.

257
00:16:33,664 --> 00:16:37,866
So using this lock based system, when we do any query at

258
00:16:37,888 --> 00:16:43,066
the target system data system level is always the

259
00:16:43,088 --> 00:16:46,734
most recent change, that the state change from

260
00:16:46,772 --> 00:16:50,126
this particular column will get returned to you. So that's more

261
00:16:50,148 --> 00:16:53,454
of a log based type of system. It doesn't make use

262
00:16:53,492 --> 00:16:57,346
of any kind of polling involved. It's only occur basically the

263
00:16:57,368 --> 00:17:00,020
logs will get written when there is some kind of changes.

264
00:17:01,510 --> 00:17:04,914
Now let's take a look know understanding a little bit more

265
00:17:04,952 --> 00:17:08,818
about CDC. What are the requirements for a modern CDC

266
00:17:08,914 --> 00:17:12,246
system? Definition of modern. Let's kind

267
00:17:12,268 --> 00:17:15,446
of take a look. What does it mean by modern? It can mean many things

268
00:17:15,468 --> 00:17:18,714
to many people, but in today's kind of

269
00:17:18,752 --> 00:17:22,362
cloud native cloud, so many things are about cloud.

270
00:17:22,416 --> 00:17:25,914
Now it can be either on prem type

271
00:17:25,952 --> 00:17:29,722
of or private cloud, right? Or public cloud or hybrid cloud.

272
00:17:29,776 --> 00:17:33,418
All of these things. These are really like about cloud native systems.

273
00:17:33,514 --> 00:17:37,278
We want systems to be very efficient. We don't want system,

274
00:17:37,444 --> 00:17:41,454
especially if you're cloud native, we don't want it to take

275
00:17:41,492 --> 00:17:44,702
up extra resources. Why? Because especially in a public

276
00:17:44,756 --> 00:17:48,194
cloud environment, you get charged by how much resources you are

277
00:17:48,232 --> 00:17:52,066
using. So you want to be very careful in how you

278
00:17:52,088 --> 00:17:56,238
are utilizing the resources. It needs to be absolutely lean

279
00:17:56,334 --> 00:17:59,430
and also be cost the least for your business.

280
00:17:59,580 --> 00:18:02,662
So cloud native, we want that. And then also too,

281
00:18:02,716 --> 00:18:06,246
the system has to be very responsive. If there are requests coming

282
00:18:06,268 --> 00:18:09,846
in, you want it to be immediately being able to respond back

283
00:18:09,948 --> 00:18:13,206
with the right thing. Very responsive type of kind of flow

284
00:18:13,238 --> 00:18:16,922
of data through the system. And also not only that,

285
00:18:17,056 --> 00:18:21,274
and at certain times too, you could have tons of data flowing through the system

286
00:18:21,392 --> 00:18:25,166
and at other times it could be very little. So the system itself needs

287
00:18:25,188 --> 00:18:28,954
to be very scalable, very elastic,

288
00:18:29,002 --> 00:18:32,398
so to speak, right? So you have different components, different nodes in a

289
00:18:32,404 --> 00:18:35,746
cloud based system, you want that the

290
00:18:35,768 --> 00:18:39,346
node to be able to shrink and grow as the

291
00:18:39,368 --> 00:18:42,786
needs arises, right? So very scalable type of system.

292
00:18:42,888 --> 00:18:46,402
And another thing is that the resiliency aspect too is basically

293
00:18:46,536 --> 00:18:50,422
if a node goes down, right, it shouldn't slow down

294
00:18:50,476 --> 00:18:53,894
everything. Another node should kind of pop up

295
00:18:53,932 --> 00:18:57,414
to kind of overtake whatever is down. So in other words, it's a very

296
00:18:57,452 --> 00:19:01,450
dynamic type of environment that we're talking about for CDC

297
00:19:01,790 --> 00:19:04,858
now that's kind of like the environment taken. Let's take a

298
00:19:04,864 --> 00:19:08,074
look too at the data itself. Right now we are talking about

299
00:19:08,192 --> 00:19:11,646
data being transmitted, data in motion, and they

300
00:19:11,668 --> 00:19:14,958
are being sent as messages. So let's kind of take a

301
00:19:14,964 --> 00:19:19,146
look what kind of techniques

302
00:19:19,178 --> 00:19:22,702
that we're using. Okay, let's take a look again, going back

303
00:19:22,836 --> 00:19:25,498
some of these requirements, reliability,

304
00:19:25,674 --> 00:19:30,882
resiliency, right. So there should be, this system should

305
00:19:30,936 --> 00:19:34,222
implement like a quality of service type of scenario.

306
00:19:34,286 --> 00:19:37,814
Basically messages must be delivered at least once or some

307
00:19:37,852 --> 00:19:41,960
cases, or actually I should say exactly once or

308
00:19:42,330 --> 00:19:45,910
at least one time too. All these kind of mechanisms probably

309
00:19:45,980 --> 00:19:49,866
needs to be properly defined for data to guarantee to

310
00:19:49,888 --> 00:19:54,374
be delivered, to be able to get transmitted to the destination,

311
00:19:54,502 --> 00:19:58,154
delivery needs to be guaranteed. In other words, something goes down,

312
00:19:58,192 --> 00:20:01,934
should not affect delivery. The data, once your messages get

313
00:20:01,972 --> 00:20:05,578
sent, it must be delivered accordingly to destination.

314
00:20:05,754 --> 00:20:10,894
And then another kind of key

315
00:20:10,932 --> 00:20:14,506
point of a modern

316
00:20:14,618 --> 00:20:18,146
cloud native system will be the responsiveness, right? So how is it

317
00:20:18,168 --> 00:20:21,838
being achieved is basically your system needs to be lightweight,

318
00:20:21,934 --> 00:20:25,586
very loosely coupled too. You can have multiple kind of components of these

319
00:20:25,608 --> 00:20:29,506
things. They send messages in a synchronous manner, or asynchronous

320
00:20:29,538 --> 00:20:33,458
manner, I should say. So basically sender send messages, it doesn't

321
00:20:33,474 --> 00:20:37,126
need to wait for the receiver. It's basically I'm going to send the messages and

322
00:20:37,148 --> 00:20:40,774
somebody will take care of it. It's essentially in a pop sub published subscribe

323
00:20:40,822 --> 00:20:45,114
type of system, would be the broker that handle the messages, kind of

324
00:20:45,232 --> 00:20:48,922
labeling the messages accordingly. And then basically the receiving side,

325
00:20:48,976 --> 00:20:52,426
the subscriber side would say I need the messages and they will subscribe to

326
00:20:52,448 --> 00:20:56,254
the messages. And basically you can be going about doing

327
00:20:56,292 --> 00:20:59,898
your own thing. But on the producer side, oh, I have messages

328
00:20:59,914 --> 00:21:03,754
to send. I send it to the broker to actually through call a topic

329
00:21:03,802 --> 00:21:07,762
and then subscriber will then subscribe to the topic when the data comes,

330
00:21:07,816 --> 00:21:10,866
only when the data comes. And other times it could be doing something else and

331
00:21:10,888 --> 00:21:14,622
not holding up the whole system, so to speak. So the responsiveness,

332
00:21:14,686 --> 00:21:18,498
right like that. And then there's also scalability aspect.

333
00:21:18,594 --> 00:21:22,534
Now I just brought up the pub sub type of approach. So that

334
00:21:22,572 --> 00:21:26,674
would be really a good approach because then in a published subscribe

335
00:21:26,722 --> 00:21:29,826
type of system, again, it deals with loosely coupled

336
00:21:29,858 --> 00:21:33,334
type of systems. The publisher and the subscriber, they don't tightly

337
00:21:33,382 --> 00:21:37,110
get coupled to each other. My job is to publish. I will just keep publishing

338
00:21:37,190 --> 00:21:40,870
and then subscriber. I need data, I'll just subscribe to the topics.

339
00:21:40,950 --> 00:21:44,922
So in that sense, if you kind of think about it, the scalability aspect

340
00:21:44,986 --> 00:21:49,130
of a system can be guaranteed too. So basically you can be producing

341
00:21:49,290 --> 00:21:52,446
many messages and then let the broker takes care of all of these

342
00:21:52,468 --> 00:21:56,130
messages and kind of deal with maybe sometimes they are message

343
00:21:56,200 --> 00:22:00,014
retry, all of these things are reduced duplication

344
00:22:00,062 --> 00:22:03,474
of messages, things like that. So the scalability aspect is

345
00:22:03,512 --> 00:22:07,254
absolutely crucial too. And then also too, one last thing

346
00:22:07,292 --> 00:22:10,866
to kind of point out in here, when it comes to data transmission,

347
00:22:10,978 --> 00:22:16,934
the order of the messaging is very important, especially in

348
00:22:16,972 --> 00:22:20,490
some systems, for example, right. And how the message is coming,

349
00:22:20,560 --> 00:22:24,586
you can be like showing the step of maybe some

350
00:22:24,688 --> 00:22:28,474
money gets deducted from an account and

351
00:22:28,512 --> 00:22:32,042
then maybe adding back. All of these needs to be

352
00:22:32,176 --> 00:22:35,822
kept in the order as it happens too. So we

353
00:22:35,876 --> 00:22:39,386
kind of have to make sure a CDC system too will preserve the ordering

354
00:22:39,418 --> 00:22:42,960
of the messages as well. So all of these, right,

355
00:22:43,590 --> 00:22:47,474
just now we just talk about requirements, but how

356
00:22:47,512 --> 00:22:50,722
can it happen, right, that there are so many type of different

357
00:22:50,776 --> 00:22:54,754
systems and basically there's no one single system

358
00:22:54,872 --> 00:22:58,306
that can handle that type of requirement

359
00:22:58,418 --> 00:23:02,118
for kind of modern day kind of processing, so to speak.

360
00:23:02,204 --> 00:23:05,622
So in today's world, though, good news is that we have

361
00:23:05,676 --> 00:23:08,614
solutions and these all require all of us,

362
00:23:08,652 --> 00:23:12,326
right? If we're implementing the systems or designing

363
00:23:12,358 --> 00:23:16,186
the systems, we need to think in a different way. It's basically

364
00:23:16,288 --> 00:23:19,706
requiring us to have a paradigm shift in what we are

365
00:23:19,728 --> 00:23:23,498
used to. So how about taken taking an event driven approach

366
00:23:23,594 --> 00:23:27,354
that would be actually perfect for a CDC type of implementation

367
00:23:27,482 --> 00:23:30,782
now, right. Now let me then give you a very quick

368
00:23:30,836 --> 00:23:34,394
kind of overview of Apache Pulsar. Okay, so pulsar,

369
00:23:34,442 --> 00:23:38,206
right? And folks probably have heard more about Apache Kafka, but that's

370
00:23:38,238 --> 00:23:41,874
what Apache Pulsar is capable of and also has the same kind

371
00:23:41,912 --> 00:23:46,054
of approach, is making use of the pub sub approach of message

372
00:23:46,252 --> 00:23:49,586
delivery, so to speak, right. Message receiving from producer

373
00:23:49,618 --> 00:23:53,234
and delivering it. Pulsar itself is an open source created

374
00:23:53,282 --> 00:23:57,554
by Yahoo Project back in early 2013

375
00:23:57,602 --> 00:24:01,074
or so and taken, basically Yahoo decided to taken donate

376
00:24:01,122 --> 00:24:05,130
it to the Apache Software foundation in 2016 and

377
00:24:05,200 --> 00:24:08,342
quickly it became a top level project in 2018.

378
00:24:08,486 --> 00:24:12,074
It is designed with the cloud native in mind and basically very

379
00:24:12,112 --> 00:24:15,726
good design. And it's cluster based already kind of built in like

380
00:24:15,748 --> 00:24:18,602
that. Also too, it has what is called multitenancy.

381
00:24:18,666 --> 00:24:22,806
It's basically a way of allowing you to organize your data according

382
00:24:22,858 --> 00:24:26,546
to how you want to kind

383
00:24:26,568 --> 00:24:30,386
of segregate or organize essentially, right? You can have a

384
00:24:30,408 --> 00:24:34,174
company of different departments, you want to have different tenants handle

385
00:24:34,222 --> 00:24:37,890
all these messages accordingly. So it's already built into Pulsar

386
00:24:37,970 --> 00:24:40,642
also too. If you want to interact working with Pulsar,

387
00:24:40,786 --> 00:24:44,326
the client binding the APIs can

388
00:24:44,348 --> 00:24:47,974
be in different languages, including Java, C sharp too,

389
00:24:48,012 --> 00:24:51,386
if you're a net person, python and go, and even community

390
00:24:51,488 --> 00:24:55,322
contribution like Scala, Ruby, all of these other things. And also

391
00:24:55,376 --> 00:24:58,582
pulsar too. It separates out the compute and storage.

392
00:24:58,726 --> 00:25:02,134
We talk about CDC, we're dealing with tons of messages

393
00:25:02,182 --> 00:25:06,190
coming through. Well, I should say they are like detecting changes

394
00:25:06,260 --> 00:25:10,430
in the database and it could be a lot too. And basically you want to

395
00:25:10,580 --> 00:25:14,622
have Pulsar worry about delivering all these messages to the target,

396
00:25:14,766 --> 00:25:18,082
kind of propagating them downstream faster, and not

397
00:25:18,136 --> 00:25:21,474
worry about the storage part as such.

398
00:25:21,512 --> 00:25:25,266
Right. Messages it comes, we don't just throw them away, we need to have

399
00:25:25,288 --> 00:25:28,966
some ways of logging them. Think of how I talk about you can

400
00:25:28,988 --> 00:25:32,674
use a log based type of system. So in some ways we are leveraging pulsar

401
00:25:32,722 --> 00:25:36,038
as the logging mechanism for kind of storing all of

402
00:25:36,044 --> 00:25:39,962
these messages. So pulsar is a very efficient way. It makes

403
00:25:40,016 --> 00:25:43,514
use of the Apache bookkeeper to help it to

404
00:25:43,632 --> 00:25:47,338
kind of with storing all of the messages. And then you

405
00:25:47,344 --> 00:25:50,362
can also do message replay afterwards, all of these things,

406
00:25:50,416 --> 00:25:53,934
which I won't go into all the details now. And also too,

407
00:25:53,972 --> 00:25:57,518
basically pulsar, the requirement is about message

408
00:25:57,604 --> 00:26:01,274
delivery, the ordering very important, and also not just ordering,

409
00:26:01,322 --> 00:26:04,420
basically it's the delivery part is absolutely important.

410
00:26:05,110 --> 00:26:08,382
You send the messages out that should never disappear.

411
00:26:08,446 --> 00:26:10,820
So Pulsar has that built in already.

412
00:26:11,670 --> 00:26:15,406
Basically pulsar is the broker, serverless runtime,

413
00:26:15,518 --> 00:26:19,414
and it guarantees that any messages that comes through

414
00:26:19,452 --> 00:26:22,674
to pulsar will be delivered to the intended target.

415
00:26:22,802 --> 00:26:26,166
And there's also a pulsar function kind of framework that allows you

416
00:26:26,188 --> 00:26:29,994
to transform data as well, which comes in very handy when I talk

417
00:26:30,032 --> 00:26:33,434
about transform. That's what you can use in terms

418
00:26:33,472 --> 00:26:37,174
of, without using any external dependencies, you can immediately

419
00:26:37,222 --> 00:26:41,194
leverage on pulsar functions to help you transform the data as they

420
00:26:41,232 --> 00:26:44,842
come in and before you propagate them down to a derived

421
00:26:44,906 --> 00:26:48,666
data source target system. And then there's also another note

422
00:26:48,698 --> 00:26:52,622
to point out is that pulsar is also very smart in recognizing if

423
00:26:52,676 --> 00:26:56,446
your data becomes cold, meaning that you are staying

424
00:26:56,478 --> 00:27:00,354
in warm storage. Why it costs more money. So if you are not

425
00:27:00,392 --> 00:27:04,274
using those data, it will move them to offline storage such as S

426
00:27:04,312 --> 00:27:07,902
three buckets, hdfs, the Hadoop file system.

427
00:27:07,976 --> 00:27:11,606
These are much less expensive kind of storage space.

428
00:27:11,708 --> 00:27:15,000
So pulsar takes care of things like that for you as well.

429
00:27:15,450 --> 00:27:18,658
Okay, so here just want to kind of quickly bring to you

430
00:27:18,684 --> 00:27:21,994
what is pulsar, Apache Pulsar over there on the right hand side

431
00:27:22,032 --> 00:27:25,530
is basically you can see increasing number of committees,

432
00:27:25,870 --> 00:27:29,626
GitHub stars and the activities. So it's kind

433
00:27:29,648 --> 00:27:32,862
of definitely like increasing in

434
00:27:32,916 --> 00:27:36,254
popularity. And in 2021 it was actually one of the

435
00:27:36,292 --> 00:27:39,946
top five Apache projects too in the foundation.

436
00:27:40,138 --> 00:27:43,278
And again, this is a brief history. I won't go into all the

437
00:27:43,284 --> 00:27:46,674
details. And who else is using pulsars here? As you can see these are big

438
00:27:46,712 --> 00:27:49,918
companies such as Yahoo, Overstock,

439
00:27:50,014 --> 00:27:53,618
Splunk, Verizon, all these GM, these are like big companies that

440
00:27:53,624 --> 00:27:57,414
are already using it. And also too it has what is called like connectors and

441
00:27:57,452 --> 00:28:02,162
clients too, as you can see this is over here, all the symbols,

442
00:28:02,306 --> 00:28:05,320
logos from different companies and as you can see.

443
00:28:05,770 --> 00:28:09,162
What are the language bindings, language clients on the right hand side,

444
00:28:09,216 --> 00:28:12,646
what kind of messaging systems, including Kafka as well, it interacts

445
00:28:12,678 --> 00:28:16,860
well, it's absolutely a very flexible type of system.

446
00:28:17,950 --> 00:28:21,774
And also this here I won't go into all the details we already taken

447
00:28:21,812 --> 00:28:25,438
about producer consumer is what you have to write that you

448
00:28:25,444 --> 00:28:29,386
can write code about. Right. And there are also actually new UI that we're

449
00:28:29,418 --> 00:28:33,246
going to be coming out to that helps you in writing your producer

450
00:28:33,278 --> 00:28:36,274
and consumer code. And the broker serverless process,

451
00:28:36,392 --> 00:28:39,278
very efficient managing all of the message delivery.

452
00:28:39,374 --> 00:28:43,010
Communicate with the bookkeeper which manages all the

453
00:28:43,080 --> 00:28:47,154
backend storage will be bookkeeper but broker communicates

454
00:28:47,202 --> 00:28:51,190
with it as well as Broker also communicates with Zookeeper is another

455
00:28:51,260 --> 00:28:54,966
Apache project that helps with managing your

456
00:28:54,988 --> 00:28:59,186
cluster metadata handles all the coordination tasks between the pulsar

457
00:28:59,218 --> 00:29:02,406
clusters and so on and so forth. And here too I

458
00:29:02,428 --> 00:29:05,402
won't get into all of the details, I just thought I'd provide them here.

459
00:29:05,456 --> 00:29:08,806
Just so you know, that's what it is. We want to get back to CDC

460
00:29:08,918 --> 00:29:12,620
enabling CDC for AstraDB, right. I work for a company

461
00:29:13,070 --> 00:29:16,350
data stack, so we know Apache perfect,

462
00:29:16,420 --> 00:29:19,962
right. Data and storage. We have Apache Cassandra.

463
00:29:20,026 --> 00:29:23,342
So as you can see, database table, we want to kind of

464
00:29:23,476 --> 00:29:26,974
basically look into it and create a serverless

465
00:29:27,022 --> 00:29:30,242
database and then create some tables, one for data

466
00:29:30,296 --> 00:29:33,742
and one for CDC messages. And taken create the streaming tenant.

467
00:29:33,806 --> 00:29:37,174
Right. That's kind of the procedure, how you can build

468
00:29:37,212 --> 00:29:40,562
out the change data capture and then enable the CDC

469
00:29:40,626 --> 00:29:43,462
for Astra to connect everything together.

470
00:29:43,596 --> 00:29:47,174
So that's kind of essentially the steps you do. And then basically too,

471
00:29:47,212 --> 00:29:50,874
over here, downstream targets will be you building up a

472
00:29:50,912 --> 00:29:54,460
streaming sync to kind of receive the changes

473
00:29:54,830 --> 00:29:58,598
that has been kind of transformed and send it back down to the CDC

474
00:29:58,694 --> 00:30:02,798
messaging table. Right. So you can then add

475
00:30:02,884 --> 00:30:06,894
these data back to the data table. So in other words, you can actually go

476
00:30:06,932 --> 00:30:10,666
do detecting changes in an Astra database,

477
00:30:10,778 --> 00:30:14,202
Cassandra database, and then having the streaming

478
00:30:14,266 --> 00:30:18,114
target in there and basically help you create

479
00:30:18,152 --> 00:30:22,222
the tenant and then enable CDC. Make sure you do that. And taken essentially

480
00:30:22,286 --> 00:30:25,986
any kind of transformation, you can use pulsar function, as I mentioned before,

481
00:30:26,088 --> 00:30:29,526
and then you write all the transform data back down to the

482
00:30:29,548 --> 00:30:32,934
downstream targets. And then you can also kind of essentially over

483
00:30:32,972 --> 00:30:36,374
here, send it back into the Cassandra database, but with

484
00:30:36,412 --> 00:30:39,942
the transform too. So even

485
00:30:39,996 --> 00:30:43,686
over here. Oh, I should have done that. So CDC for Astra streaming sync.

486
00:30:43,718 --> 00:30:47,722
And these are what our company can do for you. And also to

487
00:30:47,856 --> 00:30:51,994
wanting to point out to you, pulsar meets you where you are here too.

488
00:30:52,032 --> 00:30:55,778
We have managed cloud platform called Astra streaming, and that's our managed

489
00:30:55,814 --> 00:30:59,870
pulsar. And then if you want to manage

490
00:30:59,940 --> 00:31:03,278
your own cloud, you can use our Luna streaming. So in

491
00:31:03,284 --> 00:31:06,958
other words, you can use that and write your helm chart and work with deploying

492
00:31:06,974 --> 00:31:10,626
it to your Kubernetes cluster, everything. Or you can

493
00:31:10,808 --> 00:31:13,874
use essentially the open source version, completely open

494
00:31:13,912 --> 00:31:16,898
source, and run it on your desktop as well. You can do that too.

495
00:31:16,984 --> 00:31:20,470
So really, really flexible data stacks too, also has different

496
00:31:20,540 --> 00:31:24,210
streaming related products too. There's also Luna streaming.

497
00:31:24,370 --> 00:31:28,198
I talk about if, you know, you use our customer

498
00:31:28,284 --> 00:31:31,610
manage kind of platform, Luna streaming, you manage yourself.

499
00:31:31,680 --> 00:31:34,986
And there's also CDC for Cassandra too. And then if you

500
00:31:35,008 --> 00:31:38,746
want to use our managed cloud, you can use Astra streaming. And then

501
00:31:38,768 --> 00:31:42,394
there's also CDC for Astra database within the GUI,

502
00:31:42,442 --> 00:31:45,374
which I'll show you shortly. Okay,

503
00:31:45,572 --> 00:31:48,702
so let me do that. Or actually,

504
00:31:48,756 --> 00:31:52,206
you know what, I probably don't have time now. I do have to say right

505
00:31:52,228 --> 00:31:55,898
now, given like 30 minutes or so. So what I'll do is

506
00:31:55,924 --> 00:31:59,650
that I'll give you the link and then you can then take a look into

507
00:31:59,800 --> 00:32:02,962
that. Right. So, okay, maybe let me still do that.

508
00:32:03,016 --> 00:32:06,502
Okay, let me do that. Sorry about

509
00:32:06,556 --> 00:32:10,038
that. Okay, here we go. So this actually is the link

510
00:32:10,124 --> 00:32:14,002
to our documentation. It describes how you can do CDC

511
00:32:14,066 --> 00:32:17,542
for estradb so the link

512
00:32:17,596 --> 00:32:21,226
again is towards the end of our document. So if you

513
00:32:21,248 --> 00:32:24,474
want to reference that and then go to this page,

514
00:32:24,592 --> 00:32:28,294
it will guide you through how you can actually set up CDC

515
00:32:28,342 --> 00:32:31,802
change data capture. And this particular example here is basically

516
00:32:31,856 --> 00:32:35,482
help you create a tenant, create a topic using our pulsar

517
00:32:35,626 --> 00:32:39,342
or actually the Astra streaming do that and basically you can

518
00:32:39,396 --> 00:32:42,702
also then create a table or use any table you want

519
00:32:42,756 --> 00:32:46,800
that you want to capture changes from a column. You can do that

520
00:32:47,570 --> 00:32:51,314
and then you have to then enables the CDC feature for the

521
00:32:51,352 --> 00:32:55,250
Astradb. Once it's enables, then you are all set. And basically

522
00:32:55,400 --> 00:32:59,634
from that point on you can then connect your streaming tenant

523
00:32:59,682 --> 00:33:03,218
to an elastic, for example, this example is using elasticsearch,

524
00:33:03,314 --> 00:33:06,566
but you can actually replace it with astradb as a

525
00:33:06,588 --> 00:33:10,314
sync is the place where you receive the changes that has

526
00:33:10,352 --> 00:33:14,390
happened after capture it. So this particular example is assume

527
00:33:14,470 --> 00:33:18,074
the use of an elastic search and taken

528
00:33:18,112 --> 00:33:22,174
basically too, it will guide you through how you can do too.

529
00:33:22,372 --> 00:33:25,982
Okay, so has such, I don't have very much enough

530
00:33:26,036 --> 00:33:29,294
time so I won't get into all the details. Again, this particular link

531
00:33:29,332 --> 00:33:34,366
in here has been shared in

532
00:33:34,388 --> 00:33:37,666
the slide itself. So let me get back to the slide, but I also

533
00:33:37,688 --> 00:33:41,662
want to really quickly point out to you, over here is our estra database.

534
00:33:41,726 --> 00:33:45,186
We offer like $25 credit to per month you can

535
00:33:45,208 --> 00:33:48,966
use for free. And every month it will refresh itself for you to do personal

536
00:33:49,068 --> 00:33:53,062
experimentation and projects too. So in order to sign in,

537
00:33:53,116 --> 00:33:56,406
you can example, oops, in my case is you sign it

538
00:33:56,428 --> 00:33:59,898
with my account. And somehow I think there's a

539
00:33:59,904 --> 00:34:03,258
bit of a slow thing today.

540
00:34:03,344 --> 00:34:06,714
But let's see. Okay, so this is just

541
00:34:06,752 --> 00:34:09,740
as easy as when you sign up. You can just give your,

542
00:34:10,750 --> 00:34:14,226
you don't need your credit card for the $25 credit free tier.

543
00:34:14,278 --> 00:34:17,950
Give your name and also your email address. And that's what's needed.

544
00:34:18,020 --> 00:34:21,406
So over here, if you want to create database, it's over here.

545
00:34:21,508 --> 00:34:25,742
And you can create your database or you can do create your tenant over

546
00:34:25,796 --> 00:34:29,630
here like that. So on the left side it shows you streaming and database.

547
00:34:29,710 --> 00:34:33,362
So this is how you create it. And then also as you step through,

548
00:34:33,416 --> 00:34:36,854
as I mentioned earlier with the example

549
00:34:36,972 --> 00:34:40,774
CDC, you can step through this example and see how it is being done

550
00:34:40,812 --> 00:34:41,990
at your own pace.

551
00:34:44,090 --> 00:34:48,850
Okay, and let me then go back now we finish this

552
00:34:48,940 --> 00:34:53,126
presentation. Where to go from here? Please do keep in touch. These are resources

553
00:34:53,158 --> 00:34:57,126
for our Apache Pulsar and Astra from Datastax.

554
00:34:57,238 --> 00:35:00,966
Also the pulsar bookkeeper and zookeeper projects from Apache.

555
00:35:01,078 --> 00:35:05,470
Also, we have here in the middle right here is our Astra

556
00:35:05,810 --> 00:35:09,198
database link. And you can. Oops. And then you

557
00:35:09,204 --> 00:35:12,814
can kind of take a look in here. Actually, I'm sorry, I should have gone

558
00:35:12,852 --> 00:35:16,490
back to a little bit here. Okay, so all these are Astra streaming,

559
00:35:16,570 --> 00:35:20,706
Luna streaming, and also CDC for Astra. So that's where that page

560
00:35:20,808 --> 00:35:24,034
comes from. So please do take a look into that. And here

561
00:35:24,072 --> 00:35:27,330
is additional credit if you're interested. You're a small business.

562
00:35:27,400 --> 00:35:31,030
You need more credits. Our company is very supportive of you too.

563
00:35:31,100 --> 00:35:34,754
So please visit this link and use this particular open source

564
00:35:34,802 --> 00:35:37,782
200 code to get additional $200 credits.

565
00:35:37,916 --> 00:35:41,658
Or if you like to stay in touch with me and I'll help you too.

566
00:35:41,824 --> 00:35:45,546
Community resources if you want to stay in touch with the community side of

567
00:35:45,568 --> 00:35:49,610
the open source Pulsar Apache project, here are the links and also select

568
00:35:49,680 --> 00:35:53,174
channel and I myself also has a twitch stream

569
00:35:53,222 --> 00:35:56,826
every Wednesday at 02:00 p.m. When I'm not traveling or just follow me when it's

570
00:35:56,858 --> 00:36:00,206
up. When you are available you can watch it or there's also recording being

571
00:36:00,228 --> 00:36:03,486
kept for 60 days too. Thank you. And with

572
00:36:03,508 --> 00:36:07,422
that, I really want to thank you for having sat through here and watch

573
00:36:07,476 --> 00:36:11,470
me this today's presentation. Yes, please stay in touch.

574
00:36:11,620 --> 00:36:15,238
Follow me on Twitter on LinkedIn, connect with me and

575
00:36:15,244 --> 00:36:18,866
also on my discord channel. I'll be open to any kind of conversation

576
00:36:18,978 --> 00:36:22,486
at any time. Everybody, good luck to

577
00:36:22,508 --> 00:36:25,154
you for all your projects and happy coding.

