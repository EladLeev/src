1
00:02:14,840 --> 00:02:19,296
Hi, welcome to this session. The freedom of Kubernetes

2
00:02:19,408 --> 00:02:23,120
requires chaos engineering to shine in production.

3
00:02:23,280 --> 00:02:26,516
So obviously if you're connected to this session, there is a big chance that you

4
00:02:26,538 --> 00:02:30,404
are interested in Kubernetes and also in chaos engineering. So take

5
00:02:30,442 --> 00:02:33,312
a glass of water, a glass of coffee,

6
00:02:33,456 --> 00:02:37,208
enjoy yourself, relax and let's

7
00:02:37,304 --> 00:02:40,572
spend half an hour together. So before you start actually

8
00:02:40,626 --> 00:02:43,976
to the main content, I would like to briefly introduce

9
00:02:44,008 --> 00:02:47,336
myself. So, my name is Henrik rexed.

10
00:02:47,448 --> 00:02:51,120
I'm a cloud native advocate at Dynatrace.

11
00:02:51,460 --> 00:02:55,184
But prior to Dynatrace I've been involved in the

12
00:02:55,222 --> 00:02:57,970
performance engineering market more than 15 years.

13
00:02:58,500 --> 00:03:02,112
And as a result of that I have become

14
00:03:02,166 --> 00:03:05,664
one of the producers of one of YouTube channel dedicated for performance

15
00:03:05,712 --> 00:03:09,904
engineers called Perfytes. Check it out if you're looking for content for performance

16
00:03:09,952 --> 00:03:12,950
engineers. On the other hand,

17
00:03:13,800 --> 00:03:18,056
last year, in July 2021, I started a new

18
00:03:18,158 --> 00:03:21,348
fresh YouTube channel called is it observable?

19
00:03:21,524 --> 00:03:25,300
It's a dedicated YouTube channel related to observability

20
00:03:25,380 --> 00:03:28,700
in general. So if you're looking for content tutorials,

21
00:03:29,200 --> 00:03:32,988
content that will explain a given framework or technology, check it out.

22
00:03:33,074 --> 00:03:37,132
It will really helpful. And I also looking for feedback. So please

23
00:03:37,186 --> 00:03:41,068
connect and send me your feedback. So what are

24
00:03:41,074 --> 00:03:44,288
we going to learn if you stay with me on the next 30 minutes?

25
00:03:44,374 --> 00:03:47,488
So a couple of things. Because we're going to talk a lot about Kubernetes and

26
00:03:47,494 --> 00:03:51,344
the challenges and the problems that we could probably face in production, it makes

27
00:03:51,382 --> 00:03:55,008
sense that we do some couple of reminding related to Kubernetes.

28
00:03:55,104 --> 00:03:58,884
We will of course present the challenges itself. And then to

29
00:03:58,922 --> 00:04:02,544
validate that we will use chaos engineering. So we'll introduce what is chaos

30
00:04:02,592 --> 00:04:06,104
engineering and then see what will be the experiments that

31
00:04:06,142 --> 00:04:10,100
we will need to design to be able to validate our community settings.

32
00:04:10,260 --> 00:04:13,844
Last, because we do some testing,

33
00:04:13,892 --> 00:04:17,804
we also need to have observability in place. So we will see what type

34
00:04:17,842 --> 00:04:21,372
of metrics and events that we will need

35
00:04:21,506 --> 00:04:25,128
to collect to be able to validate our experiments.

36
00:04:25,224 --> 00:04:28,364
And last, we will briefly explain how we could

37
00:04:28,402 --> 00:04:32,032
automate that process. So the dark side

38
00:04:32,086 --> 00:04:35,552
of kubernetes. So Kubernetes is

39
00:04:35,606 --> 00:04:39,372
an orchestration framework. Everyone knows it, no surprises.

40
00:04:39,516 --> 00:04:43,524
And in kubernetes there are two type of

41
00:04:43,562 --> 00:04:47,172
nodes. In fact, Kubernetes relies on nodes and nodes at the end are

42
00:04:47,226 --> 00:04:50,900
physical servers and virtual servers or virtual servers.

43
00:04:51,320 --> 00:04:55,028
The master node is one of the node that is in the top.

44
00:04:55,114 --> 00:04:58,376
In this slide, as you can see, there are various icons. You can see the

45
00:04:58,398 --> 00:05:01,716
scheduler, etCD control manager and the API

46
00:05:01,748 --> 00:05:05,480
server. So the master node, if you're using on any

47
00:05:05,550 --> 00:05:09,464
managed Kubernetes environment provided by any of the iposcaler

48
00:05:09,512 --> 00:05:12,844
AVs or Azure or GCP, then you

49
00:05:12,882 --> 00:05:16,216
probably don't see that master node. If you do manage fully

50
00:05:16,248 --> 00:05:19,620
the cluster, you will have to manage as well the master node.

51
00:05:19,720 --> 00:05:23,392
On the bottom you have the workload nodes. The worker node are there

52
00:05:23,446 --> 00:05:27,484
to basically host our workload. So when we deploy

53
00:05:27,532 --> 00:05:31,452
any workload within our cluster, Kubernetes will basically

54
00:05:31,606 --> 00:05:34,644
move our workload in different state.

55
00:05:34,842 --> 00:05:38,128
And behind that there is a lot of events,

56
00:05:38,224 --> 00:05:42,148
and I need to remind those projects because it's very important

57
00:05:42,314 --> 00:05:45,912
to understand the various challenges that we're going to talk to in a few minutes.

58
00:05:46,046 --> 00:05:49,240
So when we deploy using Kucatl

59
00:05:49,980 --> 00:05:53,220
or anything else or maybe other systems,

60
00:05:53,380 --> 00:05:56,696
the first step of deploying our workload will

61
00:05:56,718 --> 00:06:00,396
be in pending state. So pending state means

62
00:06:00,578 --> 00:06:04,236
kubernetes know it has to deploy a new workload. So he

63
00:06:04,258 --> 00:06:07,516
will try to identify a node that is able to

64
00:06:07,538 --> 00:06:11,068
host that new workload. So based on resources, based on

65
00:06:11,154 --> 00:06:14,316
tension tolerance and various policies, then once he has

66
00:06:14,338 --> 00:06:18,096
identified in the right node, then our workload moves to

67
00:06:18,118 --> 00:06:22,020
the creating state. Creating state means I know which node going to host

68
00:06:22,840 --> 00:06:26,004
my workload. And because our workload relies on

69
00:06:26,042 --> 00:06:29,972
containers at that state, Kubernetes will basically

70
00:06:30,026 --> 00:06:34,400
interact with our docker registry to pull out the images

71
00:06:34,480 --> 00:06:37,480
and validate that there is all the requires that is needed.

72
00:06:37,550 --> 00:06:41,156
If we have any volumes, if you have any config map, any secrets,

73
00:06:41,188 --> 00:06:44,584
it will check that those exist to be able to deploy it.

74
00:06:44,782 --> 00:06:48,040
Then once we have all the requirements out there,

75
00:06:48,110 --> 00:06:51,276
then our workload is in a running state. So it doesn't mean

76
00:06:51,298 --> 00:06:55,404
that it's officially running, it's just that Kubernetes has started

77
00:06:55,602 --> 00:06:59,068
the pod with the various containers in it. And if you want to

78
00:06:59,074 --> 00:07:02,576
check if the app is obviously running, you need to check all the

79
00:07:02,678 --> 00:07:06,640
readiness probes or health probes that Kubernetes provides.

80
00:07:07,060 --> 00:07:11,020
All right, so now you know all the various states of our workload.

81
00:07:11,180 --> 00:07:14,784
Once upon a time, Kubernetes killed my workload. All right, so here,

82
00:07:14,822 --> 00:07:18,992
let's say we have a cluster, I have two applications, so I've decided

83
00:07:19,056 --> 00:07:22,784
to have two pods. Let's pretend that it's not two pods, but two namespaces

84
00:07:22,832 --> 00:07:25,668
with a lot of various pods inside of them.

85
00:07:25,834 --> 00:07:29,316
And I have more than one node. So here we see only one worker node.

86
00:07:29,428 --> 00:07:33,240
And during that time frame one of the app was basically

87
00:07:33,310 --> 00:07:37,212
consuming more resources and as a consequence there were

88
00:07:37,266 --> 00:07:41,820
almost no resources left for the other workload. And as a consequence

89
00:07:42,320 --> 00:07:46,024
to be able to avoid any node pressure events

90
00:07:46,072 --> 00:07:49,884
or infrastructure issues, Kubernetes will try to resolve that

91
00:07:49,922 --> 00:07:53,356
problem and for that it will start an eviction

92
00:07:53,388 --> 00:07:56,640
process. Eviction process means it's going to select one of the

93
00:07:56,710 --> 00:07:59,504
existing pod running in that node and it will evict it.

94
00:07:59,542 --> 00:08:03,152
Invicted means first I kill the pod

95
00:08:03,216 --> 00:08:07,044
from that node and then I reschedule it on an

96
00:08:07,082 --> 00:08:11,232
available node that can take that new pod or that new workload.

97
00:08:11,376 --> 00:08:15,684
So here in few minutes we were able to resolve

98
00:08:15,732 --> 00:08:19,572
our pressure situation and our user

99
00:08:19,636 --> 00:08:22,120
were almost not impacted.

100
00:08:22,620 --> 00:08:26,320
But it could be worse, it could be very very worse.

101
00:08:26,420 --> 00:08:29,960
Imagine that all your nodes are pretty much saturated,

102
00:08:30,040 --> 00:08:33,944
or you have designed some tension tolerance policies

103
00:08:34,072 --> 00:08:37,240
and there is no nodes for your workload anymore.

104
00:08:37,400 --> 00:08:40,588
So you have killed the workload from these

105
00:08:40,674 --> 00:08:44,284
previous node and that new workload

106
00:08:44,332 --> 00:08:48,508
is not able to be scheduled elsewhere. So basically no app, nothing responding

107
00:08:48,524 --> 00:08:52,368
to our user. So here it's pretty critical for us.

108
00:08:52,454 --> 00:08:56,550
So we need to figure out how we can avoid that type of critical situation.

109
00:08:57,080 --> 00:09:00,484
So how can we do that? Well, for that they are,

110
00:09:00,602 --> 00:09:04,164
they say the recommended Kubernetes approach.

111
00:09:04,212 --> 00:09:07,690
So first, eviction works

112
00:09:08,220 --> 00:09:11,384
by quality of services.

113
00:09:11,502 --> 00:09:15,252
So we need to define requires and limits in our pods.

114
00:09:15,396 --> 00:09:18,716
So by doing this, Kubernetes will

115
00:09:18,738 --> 00:09:22,860
basically determine a specific quality of services related to our

116
00:09:22,930 --> 00:09:26,520
request and limits. So if our requests

117
00:09:26,600 --> 00:09:30,168
equal our limits, then our workload will be considered

118
00:09:30,184 --> 00:09:33,664
to be guaranteed. If the request is under limits then

119
00:09:33,702 --> 00:09:37,388
we are burstable and asked. There's nothing defined. It's best effort

120
00:09:37,484 --> 00:09:40,992
when the eviction happened, the eviction will happen in that order.

121
00:09:41,046 --> 00:09:43,964
So first it will try to delete the best effort,

122
00:09:44,012 --> 00:09:47,264
then burstable and ask guaranteed. The second recommendation

123
00:09:47,312 --> 00:09:50,964
of course is to put resource quotas in our namespaces. Remember our

124
00:09:51,002 --> 00:09:54,388
situation where the app we're eating the resources of

125
00:09:54,394 --> 00:09:58,436
the second app. If I want to avoid that, because we usually silo

126
00:09:58,468 --> 00:10:01,716
our apps based on namespaces. If I define resource quotas

127
00:10:01,748 --> 00:10:05,464
on my namespace, then I'm pretty sure that I won't be able to

128
00:10:05,502 --> 00:10:09,316
have that situation because my app will basically have dedicated resources

129
00:10:09,348 --> 00:10:11,150
and it won't be able to eat more.

130
00:10:12,640 --> 00:10:15,788
So requires and limits is a hot topic in

131
00:10:15,794 --> 00:10:19,240
kubernetes. So let's have a look at the request and the limits.

132
00:10:19,320 --> 00:10:22,592
What is the value of that and how you can express it. So first

133
00:10:22,646 --> 00:10:26,064
request requires is

134
00:10:26,102 --> 00:10:29,712
a bit like Tetris games. So kubernetes behaves like a Tetris game.

135
00:10:29,766 --> 00:10:33,184
Remember you have shapes going down from the screen we move

136
00:10:33,222 --> 00:10:36,628
them and we place them on available spots. And the idea we want

137
00:10:36,634 --> 00:10:40,228
to make lines. It's the same thing with Kubernetes. Kubernetes we deploy, we have

138
00:10:40,234 --> 00:10:43,396
a new workload. He sees the workload coming in and

139
00:10:43,418 --> 00:10:46,984
then based on the size of the workload, so the request, he will

140
00:10:47,022 --> 00:10:50,644
try to place it on the right node. So basically request

141
00:10:50,692 --> 00:10:54,696
is there to tell to kubernetes. Okay, I need at least

142
00:10:54,878 --> 00:10:58,556
100 megs to run my workload. So he

143
00:10:58,578 --> 00:11:01,384
knows basically the shape of your pod.

144
00:11:01,512 --> 00:11:04,876
So we will be able to play a tetris if you

145
00:11:04,898 --> 00:11:08,636
don't precise any request at the end kubernetes doesn't know

146
00:11:08,738 --> 00:11:12,608
the shape of your workload. So he sees a small square,

147
00:11:12,774 --> 00:11:16,048
he place it somewhere and then suddenly that square becomes a huge

148
00:11:16,134 --> 00:11:19,712
shape. So impossible to play properly the game.

149
00:11:19,846 --> 00:11:23,420
We can also imagine that Kubernetes is a. But like a box of chocolate.

150
00:11:23,500 --> 00:11:27,504
Remember when you're a kid you receive a box of chocolate.

151
00:11:27,552 --> 00:11:30,628
Of course you don't read the manual, I mean who reads the manual today?

152
00:11:30,794 --> 00:11:34,564
But we pick the chocolates and these we discover, we eat it and suddenly we

153
00:11:34,602 --> 00:11:38,292
discover there's liquors. And remember we were making those faces.

154
00:11:38,436 --> 00:11:42,360
I don't hate liquors. Whatever. Maybe today is different but

155
00:11:42,430 --> 00:11:46,008
it's the same thing. If you dont precise the request and limits the

156
00:11:46,014 --> 00:11:50,024
same thing, kubernetes will take the chocolate, it will imagine that it's a chocolate

157
00:11:50,072 --> 00:11:54,044
without liquor. And then he starts to deployed it and he realized there's liquors inside

158
00:11:54,082 --> 00:11:58,056
of that. So settings requests is very helpful

159
00:11:58,168 --> 00:12:01,576
because it helps kubernetes to properly orchestrate your workload.

160
00:12:01,768 --> 00:12:04,980
So how do you express it? Cpu of course in mini

161
00:12:05,000 --> 00:12:08,736
cores and memory invites, nothing complicated. If you do the

162
00:12:08,758 --> 00:12:14,336
right test we know what will be the minimal resources

163
00:12:14,368 --> 00:12:17,636
that we need to run properly. Our applications, of course you can.

164
00:12:17,658 --> 00:12:21,156
But very high values because if

165
00:12:21,178 --> 00:12:24,310
you don't want to test and you want to basically guess,

166
00:12:25,000 --> 00:12:28,708
yes you can do. But keep in mind that those resources will be allocated

167
00:12:28,804 --> 00:12:32,276
by kubernetes and they will never be used. So at the end you're

168
00:12:32,308 --> 00:12:35,080
not optimizing the usage of your node properly.

169
00:12:35,580 --> 00:12:38,996
The second concept is limits. So now we

170
00:12:39,038 --> 00:12:43,052
have defined that we need a three room bed apartment. And now

171
00:12:43,106 --> 00:12:46,392
we have a contract with kubernetes saying okay so you have the three bedroom

172
00:12:46,456 --> 00:12:49,964
apartment, it's fine, but you won't be able to

173
00:12:50,002 --> 00:12:53,936
consume more power or more water during some period of the

174
00:12:53,958 --> 00:12:57,200
day. So that's basically the limit. So we have a contract with the Kubernetes saying

175
00:12:57,270 --> 00:13:00,784
how much resources can I officially utilize in that

176
00:13:00,822 --> 00:13:04,560
cluster in maximum what will be tolerated by kubernetes?

177
00:13:04,720 --> 00:13:07,796
So for that we can express it for

178
00:13:07,818 --> 00:13:10,870
the memory in byte. So this is very useful, very easy,

179
00:13:11,800 --> 00:13:16,756
I do a low test, I can see what's the maximum value that

180
00:13:16,778 --> 00:13:20,264
I need for memory, so that I can basically guess

181
00:13:20,302 --> 00:13:23,816
it. On the cpu side it's more difficult. And this

182
00:13:23,838 --> 00:13:27,950
is due to the fact that we have an heritage from the Docker world.

183
00:13:28,400 --> 00:13:32,668
So the way docker works to share resources within our host it users

184
00:13:32,834 --> 00:13:33,900
CFS.

185
00:13:35,680 --> 00:13:39,232
And for this the cpu basically is

186
00:13:39,286 --> 00:13:42,736
split it in function timing. So let's say that

187
00:13:42,838 --> 00:13:47,020
owned core has periods

188
00:13:47,100 --> 00:13:50,652
of work of 100 milliseconds. And we're going to determine

189
00:13:50,716 --> 00:13:54,036
the quota that we can use within that period. So we

190
00:13:54,058 --> 00:13:58,100
have 100 milliseconds of cpu

191
00:13:58,520 --> 00:14:02,180
periods. And if I defined, let's say 20,

192
00:14:02,330 --> 00:14:05,976
I will be able to consume only 20 milliseconds. So basically I

193
00:14:05,998 --> 00:14:10,312
do some work, I consume 20 milliseconds, then the

194
00:14:10,366 --> 00:14:14,664
node or docker will basically pose my work and

195
00:14:14,702 --> 00:14:18,328
wait until the next cpu cycle. And then I

196
00:14:18,334 --> 00:14:22,536
will be able to resume my work at least for 20 milliseconds

197
00:14:22,568 --> 00:14:26,380
and so on and so forth. And that mechanism to be posed during our work

198
00:14:26,450 --> 00:14:29,752
means throttle. So cpu throttled.

199
00:14:29,896 --> 00:14:32,972
So if I define my value too low, as a consequence

200
00:14:33,036 --> 00:14:36,844
I will get a lot of cpu throttling. And in these memory,

201
00:14:36,972 --> 00:14:40,240
if my value too low, then kubernetes will kill

202
00:14:40,390 --> 00:14:43,396
my workload and send an event called om kill.

203
00:14:43,578 --> 00:14:46,788
And if we don't input the value too high at the end, we are

204
00:14:46,794 --> 00:14:50,580
not optimizing properly our resource in our cluster.

205
00:14:52,200 --> 00:14:56,204
So that's why it's really important to define it properly

206
00:14:56,272 --> 00:15:00,564
those limits at least because in the memory we can see that kubernetes

207
00:15:00,612 --> 00:15:04,084
can kill our workload. And if these in the cpu

208
00:15:04,132 --> 00:15:08,200
side, basically we are throttled. And as a consequence our workload will be very

209
00:15:08,270 --> 00:15:11,756
slow to be able to work properly. In fact

210
00:15:11,778 --> 00:15:15,084
it's funny because when I started to work with those

211
00:15:15,122 --> 00:15:18,216
requested limits, I said let's do a test. So I said let's

212
00:15:18,248 --> 00:15:21,564
remove all the limits from our workload, run it and

213
00:15:21,602 --> 00:15:24,848
measure on the response times in one end and on

214
00:15:24,854 --> 00:15:27,868
the resources. So you can see on the graph on the top, it's a cpu

215
00:15:27,884 --> 00:15:32,100
usage. You can see that without I was able to consume almost

216
00:15:32,250 --> 00:15:36,420
650 millicores. And when

217
00:15:36,490 --> 00:15:40,164
applied limits, you can see that my resources, I'm consuming way,

218
00:15:40,202 --> 00:15:44,048
way less resources. It's normal because I have a limit defined.

219
00:15:44,224 --> 00:15:47,520
But now let's have a look at the actual experience

220
00:15:47,610 --> 00:15:51,316
that we're giving to our users. In the bottom you can see response

221
00:15:51,348 --> 00:15:54,984
times. As you can see in the bottom we have these without

222
00:15:55,102 --> 00:15:58,684
we have a response time that is about 100 milliseconds or

223
00:15:58,722 --> 00:16:02,472
less. Pretty good. But then when I apply the limits,

224
00:16:02,616 --> 00:16:06,716
boom. You can see that we almost have 3.6 seconds of

225
00:16:06,738 --> 00:16:10,524
response times, which is very very high. And this is

226
00:16:10,562 --> 00:16:14,636
only due to CP throttling. So what

227
00:16:14,658 --> 00:16:18,236
do I need to put resource cpu limits in my cluster?

228
00:16:18,428 --> 00:16:21,616
Because at the end it seems that it works better without it.

229
00:16:21,718 --> 00:16:25,236
Well, because it's a best practice for the industry, at least

230
00:16:25,258 --> 00:16:28,576
for the memory side you need to define requires and limits,

231
00:16:28,688 --> 00:16:32,340
otherwise you dont utilize properly your resources and your nodes.

232
00:16:33,160 --> 00:16:36,792
And for this topic request and limits there is

233
00:16:36,846 --> 00:16:43,450
tons of horror stories available over

234
00:16:43,980 --> 00:16:46,612
various presentations done in Kubecon.

235
00:16:46,756 --> 00:16:50,348
So here you can scan the QR code. There comes to a website

236
00:16:50,434 --> 00:16:53,996
listing all those horror stories. So I will definitely recommend to

237
00:16:54,018 --> 00:16:58,024
watch it. There is plenty of interesting stories performance issues related

238
00:16:58,072 --> 00:17:01,916
to cpu throttling, RBnB Zorando that talks about

239
00:17:01,938 --> 00:17:05,244
it, stability issues due to OMKL. Same thing,

240
00:17:05,282 --> 00:17:08,576
RBnB Zorando. So check it out. You see that you can learn a lot of

241
00:17:08,598 --> 00:17:12,356
things and this topic is very important because at the end we know

242
00:17:12,378 --> 00:17:15,968
that we can have a major impact on the stability

243
00:17:16,064 --> 00:17:19,396
and the behavior of users. So how do

244
00:17:19,418 --> 00:17:22,816
we validate and avoid

245
00:17:22,848 --> 00:17:26,416
that type of situation? Well, obviously chaos engineering.

246
00:17:26,528 --> 00:17:30,404
So what is chaos engineering? Chaos engineering. If I took the definition for in Wikipedia

247
00:17:30,452 --> 00:17:34,420
you say chaos engineering is a process to discover vulnerability

248
00:17:34,580 --> 00:17:38,356
by injecting failures and errors. And it even says in production.

249
00:17:38,388 --> 00:17:42,056
So first of all, don't do it directly in production. You don't improvise

250
00:17:42,088 --> 00:17:46,024
in production. So first you do it on a non production environment.

251
00:17:46,072 --> 00:17:49,840
And then once you're mature enough, you move to closer to a production environment.

252
00:17:50,420 --> 00:17:55,276
So let's have a look at the various workflow. How do you define

253
00:17:55,308 --> 00:17:59,104
those errors and vulnerabilities in our environments? So first,

254
00:17:59,302 --> 00:18:02,708
the process is in our three steps. The first step is we need to

255
00:18:02,714 --> 00:18:06,164
define hypotheses so we can say, okay, I've designed my

256
00:18:06,202 --> 00:18:09,748
app, I know the architecture, I know the system,

257
00:18:09,914 --> 00:18:12,820
what could go wrong? What could fail?

258
00:18:13,240 --> 00:18:17,152
So basically, for example, I have a connection to database, I may assume

259
00:18:17,216 --> 00:18:20,776
that I can basically have network connectivity issues between my system and

260
00:18:20,798 --> 00:18:24,632
database. That could be a problem. Then I need to predict how my system

261
00:18:24,686 --> 00:18:28,484
react. Either it will basically handle

262
00:18:28,532 --> 00:18:31,948
properly because I've designed an awesome architecture, an awesome design

263
00:18:32,034 --> 00:18:35,324
of a software. Or I can also predict that, okay,

264
00:18:35,362 --> 00:18:38,440
my system is going to fail or have some problems to write to database.

265
00:18:38,520 --> 00:18:42,770
So we need to list basically what we expect from that situation.

266
00:18:43,380 --> 00:18:46,444
Then we need to define what are the metrics and events,

267
00:18:46,492 --> 00:18:50,400
whatever that we need to collect to be able to validate our experiments.

268
00:18:51,640 --> 00:18:54,832
Then we define our experiments. Usually an experiment

269
00:18:54,896 --> 00:18:58,564
is a workflow of tasks. So first I want

270
00:18:58,602 --> 00:19:01,924
to inject latency between my app

271
00:19:01,962 --> 00:19:05,060
and my database. Or I want to inject,

272
00:19:06,200 --> 00:19:10,244
let's say packet loss or whatever. Basically to simulate

273
00:19:10,292 --> 00:19:14,308
network problems I can restart my app. Basically you define

274
00:19:14,324 --> 00:19:17,576
the workflow of this given specific situation that you want to

275
00:19:17,598 --> 00:19:21,616
test. Then we need to define how to rollback.

276
00:19:21,668 --> 00:19:24,760
Why? Because keep in mind that we're going to run that probably in production.

277
00:19:24,840 --> 00:19:28,136
So if in case there is a problem, we need to have the process described

278
00:19:28,168 --> 00:19:32,130
and automated to be able to come back to a normal state,

279
00:19:32,660 --> 00:19:36,396
then we also need to figure out how we're going to collect the various KPI

280
00:19:36,428 --> 00:19:40,450
that is required to validate our experiment. And last

281
00:19:41,060 --> 00:19:45,592
is basically we can our test. All right, so what are the hypotheses

282
00:19:45,676 --> 00:19:49,396
related to Kubernetes? Remember we talked about it, request and limits. So what

283
00:19:49,418 --> 00:19:51,220
are the various hypotheses?

284
00:19:53,240 --> 00:19:56,832
Well, I got some ideas. So first we have Kubernetes settings.

285
00:19:56,896 --> 00:20:00,056
We know that if I change requires and limits, I have an impact probably on

286
00:20:00,078 --> 00:20:04,196
my users. So I want to basically validate

287
00:20:04,228 --> 00:20:08,656
that those settings are working fine. So my expectation is that kubernetes

288
00:20:08,708 --> 00:20:12,232
is good. I have already defined the right request and limits.

289
00:20:12,296 --> 00:20:16,296
So I expect that my app is stable performance, no impact

290
00:20:16,328 --> 00:20:19,310
and no error rates. Okay, fine, fair enough.

291
00:20:19,680 --> 00:20:22,896
Then you have the maintenance scenario. What are we referring to

292
00:20:22,918 --> 00:20:26,352
maintenance by the way? Yeah. Remember when

293
00:20:26,406 --> 00:20:30,256
you have to upgrade your nodes because nodes at the end are a

294
00:20:30,278 --> 00:20:33,596
physical or virtual service like I mentioned. So if you want to upgrade the version

295
00:20:33,628 --> 00:20:37,284
of kubernetes because you're upgrading your master nodes, then to do that

296
00:20:37,322 --> 00:20:40,676
you will have to node drain. So you remove that node from

297
00:20:40,698 --> 00:20:44,404
the actual work, you do your maintenance task and then you

298
00:20:44,522 --> 00:20:47,860
reattach it to the cluster back. So this is a maintenance task.

299
00:20:47,940 --> 00:20:51,880
These will clearly happen during your production hours or

300
00:20:51,950 --> 00:20:55,480
night hours. But as an expectation, I say if I

301
00:20:55,550 --> 00:20:58,824
have designed well my cluster, I should have no impact,

302
00:20:58,872 --> 00:21:02,456
stable, no user impacted, everything works smoothly.

303
00:21:02,648 --> 00:21:05,816
And last, it's a store that we had before eviction.

304
00:21:05,928 --> 00:21:10,140
So we're turning into nodes pressure or a situation where we come

305
00:21:10,290 --> 00:21:13,536
into these situation where there is an eviction. So my expectation is

306
00:21:13,558 --> 00:21:17,360
that because I have defined the right priority on my pods, no downtime,

307
00:21:17,940 --> 00:21:20,560
stable and performance, no impact on my users.

308
00:21:21,700 --> 00:21:25,196
What are the observerity pillars that we need to collect? Of course in

309
00:21:25,238 --> 00:21:29,140
observability there is plenty of pillars. You have logs, you have traces,

310
00:21:29,480 --> 00:21:33,076
you have metrics, you have Kubernetes events is

311
00:21:33,178 --> 00:21:36,312
also one of the pillar. And then you can add profiling and others.

312
00:21:36,366 --> 00:21:42,228
But here in our particular case, because it's pretty much related to infrastructure

313
00:21:42,404 --> 00:21:45,960
topic, we will focus on metrics and Kubernetes events.

314
00:21:47,420 --> 00:21:51,052
So let's have a look at those metrics and events that make sense for us.

315
00:21:51,186 --> 00:21:54,684
So first let's have a look at the metrics. Keep in mind that

316
00:21:54,802 --> 00:21:58,204
Kubernetes is like applications are a bit like an

317
00:21:58,242 --> 00:22:02,048
onion. There's different layers. So first you have the layer outside

318
00:22:02,134 --> 00:22:05,788
which is the user. So the user is interacting

319
00:22:05,804 --> 00:22:08,976
with our app. So I will probably collect some metrics about

320
00:22:08,998 --> 00:22:11,844
these user. So response times, failure rate,

321
00:22:11,882 --> 00:22:15,156
basically user experience. Then I will go

322
00:22:15,178 --> 00:22:18,896
to the next layer. Because Kubernetes relies

323
00:22:18,928 --> 00:22:22,992
on nodes I need to figure out how my nodes are behaving.

324
00:22:23,056 --> 00:22:26,744
So in terms of resources, cpu memory, number of

325
00:22:26,782 --> 00:22:30,840
pods running on that specific node, maybe also

326
00:22:30,990 --> 00:22:32,890
number of ports, ports available,

327
00:22:33,580 --> 00:22:37,108
ip address and so on a pod.

328
00:22:37,204 --> 00:22:40,796
So within that node I have pods running in that.

329
00:22:40,898 --> 00:22:43,964
So here I will keep track on what I have defined in teams

330
00:22:44,002 --> 00:22:47,596
of requires and limits for my cpu and memory, what is the

331
00:22:47,618 --> 00:22:50,648
limits and what is the actual usage. So then I can figure but if

332
00:22:50,674 --> 00:22:54,652
I'm far from the limits or not so I can optimize that settings.

333
00:22:54,796 --> 00:22:58,268
And last in the pod I have some containers. And remember cpu

334
00:22:58,284 --> 00:23:02,096
throttling is a docker concept so we need to

335
00:23:02,198 --> 00:23:05,776
measure that from the container perspective. So we'll look at the cpu throttling,

336
00:23:05,888 --> 00:23:09,796
the memory usage and cpu usage of course on

337
00:23:09,818 --> 00:23:12,880
the events side, as we saw at the beginning,

338
00:23:13,040 --> 00:23:16,612
there are various states in Kubernetes and in those states

339
00:23:16,666 --> 00:23:20,628
there are different types of events that is sent by Kubernetes.

340
00:23:20,724 --> 00:23:24,968
So of course the user won't send any major events except maybe tweets or

341
00:23:25,134 --> 00:23:28,808
support case. But we are focused mainly on the events

342
00:23:28,824 --> 00:23:32,860
coming from the cluster. So the node nodes pressure, that is

343
00:23:32,930 --> 00:23:37,000
a sign of a problem on the pods, I could say fade scheduling.

344
00:23:37,160 --> 00:23:40,752
That seems that we are not able to place

345
00:23:40,806 --> 00:23:44,048
that workload anymore on any of the nodes. That could be a really important

346
00:23:44,134 --> 00:23:49,100
sign. Eviction of course om kill unhealthy

347
00:23:49,260 --> 00:23:52,432
that type of events. So what are these

348
00:23:52,486 --> 00:23:55,764
kind of experiments that we need for Kubernetes so first we're going to test

349
00:23:55,802 --> 00:23:58,964
the evictions and the maintenance scenario in the same one. Why?

350
00:23:59,082 --> 00:24:03,044
Because I probably have a lot of nodes. So to be able to come

351
00:24:03,082 --> 00:24:06,608
to these situation where I'm putting high pressure on my node,

352
00:24:06,704 --> 00:24:10,228
I want to remove some nodes. So I'm going to have less nodes than expected

353
00:24:10,324 --> 00:24:13,828
because I want to come to the situation where I'm having a node pressure.

354
00:24:14,004 --> 00:24:18,860
So our first nodes drain and then I will then simulate

355
00:24:19,200 --> 00:24:22,348
cpu stress on these node memory, stress on

356
00:24:22,354 --> 00:24:26,220
the nodes and I will generate some

357
00:24:26,290 --> 00:24:29,516
load test because I'm not going to run it in production for

358
00:24:29,538 --> 00:24:32,508
this type of experiments. I want to run in a non production environment and I

359
00:24:32,514 --> 00:24:35,456
need to measure the user, the impact on the user. So I will run a

360
00:24:35,478 --> 00:24:38,748
constant cloud, so no spike test, nothing fancy,

361
00:24:38,844 --> 00:24:42,084
just the load test will only be there to report

362
00:24:42,202 --> 00:24:45,364
how is the actual response time

363
00:24:45,402 --> 00:24:49,232
from the user perspective and what is the error rates, what's the stability

364
00:24:49,296 --> 00:24:52,580
of the applications on the community

365
00:24:52,650 --> 00:24:55,984
settings. And I may not need necessarily

366
00:24:56,032 --> 00:24:59,416
some chaos experiments. If I want, I can do it to be able

367
00:24:59,438 --> 00:25:02,584
to come to the right situation. But usually just a standard load test

368
00:25:02,622 --> 00:25:06,632
is fair enough. You run the test stable load again

369
00:25:06,766 --> 00:25:10,990
and you measure the response time, the failure rates, and you compare it between

370
00:25:11,600 --> 00:25:15,164
with requires and limits without you measure the

371
00:25:15,202 --> 00:25:18,588
cpu throttling and you tweak those settings to

372
00:25:18,674 --> 00:25:22,124
at least get the right settings that will provide the highest response

373
00:25:22,172 --> 00:25:25,900
times and with the right stability

374
00:25:25,980 --> 00:25:29,456
of our apps. So what do we need

375
00:25:29,478 --> 00:25:32,864
for this? So first I

376
00:25:32,902 --> 00:25:36,268
will need a Kubernetes cluster and there is various tools

377
00:25:36,284 --> 00:25:39,444
that going are to be deployed here. You can see that I have two

378
00:25:39,482 --> 00:25:43,076
colors here to the nodes I've separated. I want

379
00:25:43,098 --> 00:25:46,904
to make sure if I run the experiments on the same cluster as

380
00:25:46,942 --> 00:25:51,124
my app, I want to make sure that the experiments are not impacting

381
00:25:51,252 --> 00:25:55,028
my tooling. So I need an observability back end solution,

382
00:25:55,124 --> 00:25:59,128
prometheus or data trace. I need a chaos engineering

383
00:25:59,144 --> 00:26:02,796
tool. So in my case I use litmus and a load testing products. I use

384
00:26:02,818 --> 00:26:07,800
k six in my case. So for these I have labeled

385
00:26:07,960 --> 00:26:11,712
the nodes to make sure that all the toolings that I need for my

386
00:26:11,766 --> 00:26:15,132
experiments are placed on dedicated nodes.

387
00:26:15,196 --> 00:26:18,656
And then I have other nodes that are dedicated for my app. And I know

388
00:26:18,678 --> 00:26:21,952
that my experience will only impact the nodes for my app

389
00:26:22,006 --> 00:26:23,940
and not my testing tools.

390
00:26:25,000 --> 00:26:28,736
Then like I said, I need some tools. So I need a chaos engineering

391
00:26:28,768 --> 00:26:32,340
product. I use litmus chaos, which is part of CNCF.

392
00:26:32,920 --> 00:26:36,772
Really good product by the way. There's a web UI called the Chaos center.

393
00:26:36,906 --> 00:26:40,152
So litmus chaos can be installed either

394
00:26:40,206 --> 00:26:43,348
in the same cluster as your app or it could be installed on dedicated cluster

395
00:26:43,444 --> 00:26:47,368
and it will interact with your cluster where your app is running. For this.

396
00:26:47,534 --> 00:26:51,176
In any case, you will need to deploy an agent either on these same cluster

397
00:26:51,208 --> 00:26:54,424
as litmus chaos, or if you have a remote cluster,

398
00:26:54,472 --> 00:26:58,056
you will have to install the agent that comes with the Chaos exporter.

399
00:26:58,088 --> 00:27:01,704
The chaos exporter exposed couple of metrics to our experiments in a Prometheus

400
00:27:01,752 --> 00:27:04,976
format and then there is a lot of chaos workflow and so on. I'm not

401
00:27:04,998 --> 00:27:08,496
going to go in details on the architecture, but at least keep in mind that

402
00:27:08,518 --> 00:27:12,470
those components, the one I'm showing here on the screen, are the main ones.

403
00:27:12,840 --> 00:27:16,436
What is great with chaos litmus chaos is that there

404
00:27:16,458 --> 00:27:20,132
is a chaos hub. That chaos hub provided 50

405
00:27:20,186 --> 00:27:23,844
plus experiments in kubernetes and all the right

406
00:27:23,882 --> 00:27:27,252
experiments for our case, which is node drain,

407
00:27:27,396 --> 00:27:31,332
I need node cpu hog, node memory

408
00:27:31,396 --> 00:27:35,044
hog, all are there. They are already there in the chaos

409
00:27:35,092 --> 00:27:38,792
hub. So it's perfect. So I don't need to reinvent the wheel, I can simply

410
00:27:38,856 --> 00:27:42,860
use it. The other advantage of litmus chaos is that it's rely on

411
00:27:42,930 --> 00:27:46,856
argo workflow. So I can define a really specific workflow

412
00:27:46,888 --> 00:27:50,656
combining pure chaos experiments and lotus with

413
00:27:50,678 --> 00:27:53,884
k six in parallel. I'm picking

414
00:27:53,932 --> 00:27:57,852
k six. Why? Because k six has an output extensions

415
00:27:57,916 --> 00:28:01,692
that I personally like. It's their Prometheus integration.

416
00:28:01,756 --> 00:28:05,236
So k six provides results that

417
00:28:05,258 --> 00:28:09,184
is sent in the command line, in the std out or in JSoN

418
00:28:09,232 --> 00:28:12,772
or other format. But in my case, I want k six to

419
00:28:12,906 --> 00:28:16,790
write into Prometheus the statistics. So these I will have all

420
00:28:17,320 --> 00:28:20,988
the observatory in Prometheus or dynatrace because nitroce will be able to scrap

421
00:28:21,024 --> 00:28:24,356
the data from Prometheus. But at least I need the response times, the requires,

422
00:28:24,388 --> 00:28:28,124
the failure rates. And I also want to have also all these

423
00:28:28,162 --> 00:28:31,996
data related to the health of the cluster itself.

424
00:28:32,178 --> 00:28:36,136
For this I will also have a Prometheus in my cluster.

425
00:28:36,328 --> 00:28:39,708
If you install the Prometheus operator, it comes with

426
00:28:39,714 --> 00:28:43,504
several components. So of course the Prometheus stack, it's the stack, but also a couple

427
00:28:43,542 --> 00:28:47,084
of exporters. These exporters is the component producing

428
00:28:47,132 --> 00:28:50,176
metrics. So we'll have the Kubernetes metrics to see how the

429
00:28:50,198 --> 00:28:54,368
various objects of kubernetes, the node exporter, anything related to how

430
00:28:54,454 --> 00:28:57,904
healthy are my nodes CA advisor to collect

431
00:28:57,952 --> 00:29:01,684
cpu throttling metrics on the container level. And then I

432
00:29:01,722 --> 00:29:05,456
have an exporter for litmus to be able to collect metrics from these nitrous

433
00:29:05,488 --> 00:29:08,888
perspective. And I have an exporter for K six to be able to collect the

434
00:29:08,894 --> 00:29:12,804
metrics from k six perspective and those with data trace

435
00:29:12,932 --> 00:29:16,596
and our components, we will be able to collect the right metrics and push

436
00:29:16,628 --> 00:29:20,188
it back to data trace. So now you know all

437
00:29:20,194 --> 00:29:23,644
the toolings and let's have a look at how we could automate this

438
00:29:23,682 --> 00:29:27,516
process. To automate this, we can define obviously a

439
00:29:27,538 --> 00:29:30,844
pipeline in Jenkins or any CI CD system that we have.

440
00:29:30,882 --> 00:29:34,352
So we're going to build, we're going to deploy, we're going to deploy our

441
00:29:34,406 --> 00:29:37,824
exporters, we're going to configure data trace or

442
00:29:37,862 --> 00:29:41,088
whatever is we can do it because all those tools could be configured with the

443
00:29:41,094 --> 00:29:44,196
API or with command lines. And then I'm going to run my test.

444
00:29:44,378 --> 00:29:47,396
Fair enough. But then after these test,

445
00:29:47,578 --> 00:29:50,724
someone needs to approve, someone needs

446
00:29:50,762 --> 00:29:54,788
to look at the results. So it's not automation anymore because

447
00:29:54,954 --> 00:29:58,744
actually we have a pose here in this automation. So how can I remove that

448
00:29:58,782 --> 00:30:02,010
pose? Well, for this I'm going to use another CNCF project

449
00:30:02,380 --> 00:30:05,576
called captain. It's an open source solution provided by

450
00:30:05,598 --> 00:30:08,684
Dynatrace. And captain provides several use

451
00:30:08,722 --> 00:30:12,012
cases. So first you have the progress delivery. So you can

452
00:30:12,066 --> 00:30:15,884
basically give the power to captain to

453
00:30:16,002 --> 00:30:19,724
deploy manage test

454
00:30:19,922 --> 00:30:23,500
similar to a CI CD process, and also

455
00:30:23,650 --> 00:30:27,104
manage production use cases like automotions and so

456
00:30:27,142 --> 00:30:30,508
on. Or I don't want to use everything. I just want to rely

457
00:30:30,524 --> 00:30:34,176
on my traditional CI CD system. I just want to use captain

458
00:30:34,208 --> 00:30:37,830
for coitigate and this is the use case I'm going to use.

459
00:30:38,200 --> 00:30:41,524
Or last, I can use pure SRE and production use

460
00:30:41,562 --> 00:30:43,780
cases, which is autoremediation.

461
00:30:44,440 --> 00:30:47,812
Captain is very easy to configure. It's based on files,

462
00:30:47,876 --> 00:30:51,688
on Yaml files. So you have a ship, your file SLo. So we need to

463
00:30:51,854 --> 00:30:55,224
define the SLI. Define slos in

464
00:30:55,262 --> 00:30:58,584
captain and those will be used, you see, for these Kuwait gate perspective.

465
00:30:58,632 --> 00:31:01,340
And then we can connect our tools.

466
00:31:02,080 --> 00:31:05,656
Captain is in a framework case on cloud events.

467
00:31:05,768 --> 00:31:08,696
So basically it's an event driven framework.

468
00:31:08,728 --> 00:31:11,960
So it's very easy. So I can easily connect, disconnect tools.

469
00:31:12,040 --> 00:31:14,976
And the beauty is that all the tools I'm going to use is part of

470
00:31:14,998 --> 00:31:18,956
captain. So if we look at the pipeline

471
00:31:18,988 --> 00:31:22,100
that we had a few minutes ago, so I'm deploy,

472
00:31:22,520 --> 00:31:26,276
I run my test, and then just

473
00:31:26,298 --> 00:31:29,236
when I run the test, after the test itself,

474
00:31:29,338 --> 00:31:33,072
I will basically send to captain, there's an API

475
00:31:33,136 --> 00:31:36,520
to say, an event to say, hey captain,

476
00:31:36,860 --> 00:31:42,584
I just finished the test. Could you evaluate the

477
00:31:42,622 --> 00:31:45,620
environment during that time frame?

478
00:31:45,780 --> 00:31:49,044
So I've already expressed a couple of SLI and SLO.

479
00:31:49,092 --> 00:31:52,316
So captain will reach out to the SLI and SLO that we've defined for that

480
00:31:52,338 --> 00:31:55,516
particular services. And so you say, okay, so pod failure needs

481
00:31:55,538 --> 00:31:59,176
to be owned, hundred percent node pressure under 1% and so on. So you define

482
00:31:59,208 --> 00:32:02,864
basically what you expect. Once he looks at all

483
00:32:02,902 --> 00:32:06,588
the SLI, he will reach out to the observed backend

484
00:32:06,684 --> 00:32:09,724
that has the value. So either Prometheus standard race,

485
00:32:09,852 --> 00:32:13,216
and then he will look at the values and match it with the objective that

486
00:32:13,238 --> 00:32:17,572
we have defined. And then he will basically present the results of each individual

487
00:32:17,706 --> 00:32:20,836
SLO that we've defined into a heat map like you can

488
00:32:20,858 --> 00:32:24,084
see here. And the great thing is, at the end it

489
00:32:24,122 --> 00:32:27,840
provides a score, and at the end I will have one SLA

490
00:32:27,920 --> 00:32:31,288
based on the score. So you say my experiments were fine. If we have a

491
00:32:31,294 --> 00:32:34,360
score of 90%, for example, and that will be

492
00:32:34,430 --> 00:32:37,880
basically in 1 minute, unless my test

493
00:32:37,950 --> 00:32:42,088
ends, Captain trigger all that workflow I just described,

494
00:32:42,184 --> 00:32:46,028
gets back the scoring, and from the scoring would say, okay, everything is green,

495
00:32:46,114 --> 00:32:48,430
or in the other way, everything is red.

496
00:32:50,100 --> 00:32:53,264
So let's have a few quick takeaway here. So, a couple

497
00:32:53,302 --> 00:32:56,320
of things. So, first, in Kubernetes, define quotas,

498
00:32:56,980 --> 00:33:00,528
resource quotas to separate non namespaces. That is a

499
00:33:00,534 --> 00:33:03,410
really high recommendation from the community's world.

500
00:33:04,020 --> 00:33:07,364
Define qos. So request and limits we talked about during

501
00:33:07,402 --> 00:33:11,984
a long time, observability. Of course, we need to collect metrics,

502
00:33:12,112 --> 00:33:15,496
logs, traces. We need to understand what's going on our

503
00:33:15,518 --> 00:33:19,476
environment. So make sure to have everything sres.

504
00:33:19,668 --> 00:33:23,130
So of course, define SLI and SLO. I mean,

505
00:33:23,500 --> 00:33:27,316
we want to be smart, we want to automate. So without SLI SLO,

506
00:33:27,348 --> 00:33:31,164
it will be very difficult to be efficient. And last, we know

507
00:33:31,202 --> 00:33:34,972
that we have problem related to Kubernetes. We could face problems

508
00:33:35,026 --> 00:33:38,428
related to Kubernetes. So let's test them utilizing low test and

509
00:33:38,434 --> 00:33:41,576
chaos engineering to validate that our cluster is stable,

510
00:33:41,688 --> 00:33:45,020
our user are happy, and there is no surprise in production.

511
00:33:45,920 --> 00:33:49,276
So before we actually finish this presentation, I will do another

512
00:33:49,378 --> 00:33:52,992
quick promo to my YouTube channel. Is it observable?

513
00:33:53,056 --> 00:33:56,736
So check out the various episodes there. There is one dedicated

514
00:33:56,768 --> 00:34:00,180
to Litmus chaos, chaos engineering, performance testing and so on.

515
00:34:00,250 --> 00:34:03,828
So check it out. And yeah, I'm trying to improve the content, so if you

516
00:34:03,834 --> 00:34:08,244
can send some feedback, that will be great. So then I can continue production content

517
00:34:08,362 --> 00:34:11,636
and help you guys in your project. All right, so thank

518
00:34:11,658 --> 00:34:14,320
you for your time and enjoy the conference.

