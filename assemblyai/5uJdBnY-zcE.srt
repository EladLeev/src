1
00:00:22,090 --> 00:00:26,098
Hi, everyone, and welcome to this session. History meets AI,

2
00:00:26,194 --> 00:00:29,686
unveiling the secrets of ancient coins. My name is

3
00:00:29,708 --> 00:00:33,286
Nico. I work with the EMEA public sector at Amazon Web

4
00:00:33,308 --> 00:00:36,982
Services. So today we're going to split this session into

5
00:00:37,036 --> 00:00:42,098
two parts. The first part is going to be talking about this unveiling

6
00:00:42,114 --> 00:00:45,990
of the secret of ancient coins. We care going to explore the challenge

7
00:00:46,330 --> 00:00:49,446
that we had at hand, and we are also going to explore the

8
00:00:49,468 --> 00:00:53,406
solution that we came up with. Then the second part of this session is

9
00:00:53,428 --> 00:00:57,418
going to be dedicated to hands on examples where I'm

10
00:00:57,434 --> 00:01:01,086
going to show you some ways that you can build your own application to

11
00:01:01,108 --> 00:01:04,606
solve challenge similar to this. So we're going to be focusing on

12
00:01:04,628 --> 00:01:07,710
three things. Number one is going to be image classification.

13
00:01:07,870 --> 00:01:11,634
Number two is going to be background removal and image segmentation. And number

14
00:01:11,672 --> 00:01:15,826
three is going to be how to build a visual search engine.

15
00:01:16,008 --> 00:01:19,286
So with that, let's dive right into the challenge that

16
00:01:19,308 --> 00:01:22,326
we had at hand. So first, let's start with these.

17
00:01:22,428 --> 00:01:26,342
Why? So the University of Oxford houses 21 million

18
00:01:26,396 --> 00:01:30,646
objects in the collections of its gardens, libraries and museums.

19
00:01:30,758 --> 00:01:34,506
Glam for short. One aspect of their mission is

20
00:01:34,528 --> 00:01:37,738
that they want to preserve these assets and make them accessible to

21
00:01:37,744 --> 00:01:41,098
the world for education and research. But of course,

22
00:01:41,264 --> 00:01:44,942
there are only so many space that you can have for these.

23
00:01:44,996 --> 00:01:48,906
So the organization only has enough space to display

24
00:01:48,938 --> 00:01:51,978
about 10% of its holding at a single time. And there's

25
00:01:51,994 --> 00:01:55,710
an enormous backlog of artifacts still waiting to be cataloged.

26
00:01:55,790 --> 00:01:59,374
So to optimize the access to these collections for digital

27
00:01:59,422 --> 00:02:03,074
teaching and research, Glam asked the question,

28
00:02:03,192 --> 00:02:06,454
can we maybe use machine learning to help us?

29
00:02:06,652 --> 00:02:10,066
If we are successful, that will reduce

30
00:02:10,178 --> 00:02:13,734
the time that a research department needs to identify and

31
00:02:13,772 --> 00:02:17,142
catalog an object. But before we even

32
00:02:17,196 --> 00:02:20,806
think of that, the first thing that we had to identify is a

33
00:02:20,828 --> 00:02:24,298
suitable, well cataloged collection that will become

34
00:02:24,464 --> 00:02:28,234
the prototype candidate. So that candidate was

35
00:02:28,272 --> 00:02:31,578
the roman provincial coinage. Digital collection. This is a

36
00:02:31,584 --> 00:02:34,570
world renowned research project in numinastics.

37
00:02:34,930 --> 00:02:38,686
The team included a curator with previous experience in

38
00:02:38,708 --> 00:02:42,334
developing digital collections from the ground up. This person is

39
00:02:42,372 --> 00:02:46,162
Sharon Meirat. He's the curator for the Herberdin coin room

40
00:02:46,216 --> 00:02:49,746
in the Ashmolian Museum. So the first step in

41
00:02:49,768 --> 00:02:52,994
any machine learning project is to decide what you

42
00:02:53,032 --> 00:02:57,122
want to predict. In this case, Anshanesh Babu, who is

43
00:02:57,176 --> 00:03:00,766
the system architect and network manager

44
00:03:00,798 --> 00:03:04,310
from clan, wanted to predict a very simple outcome.

45
00:03:04,730 --> 00:03:08,226
Heads or tails. That is, is the specimen

46
00:03:08,258 --> 00:03:11,946
that I have in front of me that I'm looking at. This photograph is that

47
00:03:11,968 --> 00:03:14,890
of the overs or the reverse of a coin,

48
00:03:15,230 --> 00:03:19,020
which is another way of saying that given a known training data,

49
00:03:19,550 --> 00:03:22,874
can we have a machine learning solution? Predict the right side of a coin

50
00:03:22,922 --> 00:03:26,238
with a high degree of a crest. So now

51
00:03:26,324 --> 00:03:29,726
that we have the why we want to do this,

52
00:03:29,828 --> 00:03:33,578
let's move into what are the actual things that

53
00:03:33,604 --> 00:03:37,294
we want to solve for. So this is the moment when the Ashmore

54
00:03:37,342 --> 00:03:41,086
Museum came to AWS and together we started discussing

55
00:03:41,198 --> 00:03:44,738
what is a normal day for the

56
00:03:44,744 --> 00:03:48,146
people who are working at this museums, what care, the challenges that they are facing?

57
00:03:48,258 --> 00:03:51,606
What are the limitations and constraints that they have.

58
00:03:51,788 --> 00:03:55,282
So we knew from before, the Asmonian Museum has built the world's largest

59
00:03:55,346 --> 00:03:59,066
digital collection of roman provincial coinage that is open to

60
00:03:59,088 --> 00:04:01,834
anyone to browse online for free. Now,

61
00:04:01,952 --> 00:04:06,006
getting an item into this collection requires expert input

62
00:04:06,118 --> 00:04:09,686
from curators, but these people are highly skilled

63
00:04:09,718 --> 00:04:13,514
and very care, making this task very difficult to

64
00:04:13,552 --> 00:04:16,846
escape. So the way that this works is that, for example,

65
00:04:17,028 --> 00:04:20,526
you may have a multitude of physical specimens and you want

66
00:04:20,548 --> 00:04:23,966
to catalog them. AWS items. Maybe the item that you

67
00:04:23,988 --> 00:04:27,586
want to catalog this for already exists in the collections and

68
00:04:27,608 --> 00:04:31,058
this is just another specimen to that item. Or maybe

69
00:04:31,224 --> 00:04:35,326
the item is completely new to these digital collection. Some cases

70
00:04:35,438 --> 00:04:38,546
you may have all the information available for these specimen,

71
00:04:38,658 --> 00:04:42,278
or maybe in some other cases, you may lack some other

72
00:04:42,364 --> 00:04:45,910
information. Also, something that might happen is that

73
00:04:45,980 --> 00:04:49,826
other research institutions, or maybe even individuals might reach

74
00:04:49,868 --> 00:04:53,434
out to these ashmolian museum with the simple question, look,

75
00:04:53,552 --> 00:04:57,066
we have this item. Do you know what it is? And the

76
00:04:57,088 --> 00:05:00,830
answer to this question is at times very complex

77
00:05:01,650 --> 00:05:05,166
because of the sheer volume of items that need to

78
00:05:05,188 --> 00:05:08,686
be processed. Oftentimes, groups of

79
00:05:08,708 --> 00:05:13,134
people who want to help out. The mission of the university volunteers

80
00:05:13,182 --> 00:05:16,974
to help out with this task. But normally,

81
00:05:17,102 --> 00:05:20,734
because of the way that this is established, in some cases,

82
00:05:20,782 --> 00:05:24,450
even the most simple task cannot be accomplished

83
00:05:25,270 --> 00:05:28,754
by a single person or a small group of individuals.

84
00:05:28,802 --> 00:05:32,498
Right? When I mean task, I mean getting a specimen

85
00:05:32,674 --> 00:05:36,758
and identifying the right item that this specimen belongs to.

86
00:05:36,924 --> 00:05:40,954
So what we wanted to do is not automate this

87
00:05:40,992 --> 00:05:44,250
task, but augment the humans behind it.

88
00:05:44,320 --> 00:05:47,994
Build tools that can support these people who

89
00:05:48,032 --> 00:05:51,694
are working with this every day in a way that they can focus on

90
00:05:51,732 --> 00:05:54,558
more relevant tasks, avoiding, for example,

91
00:05:54,724 --> 00:05:58,606
spending hours and hours rotating photos so that

92
00:05:58,628 --> 00:06:02,270
they are aligned perfectly before they move to the next task.

93
00:06:02,610 --> 00:06:06,226
In this case, these customer objective is to reduce the

94
00:06:06,248 --> 00:06:10,302
time that it takes for the correct appraisal of a single specimen.

95
00:06:10,446 --> 00:06:14,290
Currently, these is estimated between 10 minutes and several hours

96
00:06:14,360 --> 00:06:17,922
for each item. You can imagine that you get an item

97
00:06:18,066 --> 00:06:22,290
and you want to spend some time corroborating

98
00:06:22,370 --> 00:06:26,166
that the information that you have at hand that is available matches the

99
00:06:26,188 --> 00:06:29,114
one that you have in the collection. And if it doesn't match,

100
00:06:29,232 --> 00:06:32,566
then you need to figure out what is that missing

101
00:06:32,598 --> 00:06:36,554
information. And for this, you can have a

102
00:06:36,592 --> 00:06:40,306
multitude of combinations, making this exponentially

103
00:06:40,358 --> 00:06:43,790
difficult when you have items that are not

104
00:06:43,860 --> 00:06:47,802
the standard ones. Right? So the difficult items will require

105
00:06:47,866 --> 00:06:51,726
an enormous amount of time, and normally they

106
00:06:51,748 --> 00:06:54,962
will require an expert who is very

107
00:06:55,016 --> 00:06:59,394
scars. These sense. So let's take deep into what

108
00:06:59,432 --> 00:07:02,834
we are talking about. So, these are two screenshots from

109
00:07:03,032 --> 00:07:06,418
the digital collection. The image that you see on the left are

110
00:07:06,504 --> 00:07:09,750
three items. So three coins.

111
00:07:10,410 --> 00:07:13,718
And you can see that we have the overs and the rivers on the left.

112
00:07:13,804 --> 00:07:17,206
And then we have information on the right, where you

113
00:07:17,228 --> 00:07:20,678
can see, for example, the inscription that is written on the

114
00:07:20,684 --> 00:07:24,054
overs and the rivers, the city that belongs to the region,

115
00:07:24,102 --> 00:07:27,434
these province, even the person that is in the

116
00:07:27,472 --> 00:07:30,630
image. Now, the item that you see on the left, on the right.

117
00:07:30,800 --> 00:07:34,538
Care different specimens to this same item,

118
00:07:34,634 --> 00:07:38,830
right. So you can see how the quality of the specimen varies

119
00:07:39,170 --> 00:07:42,510
in a big way. These, what you can see, care four

120
00:07:42,580 --> 00:07:45,826
photos of the same coins. So you can see

121
00:07:45,848 --> 00:07:49,620
that we have a very high quality on the left, where we can

122
00:07:50,470 --> 00:07:54,114
figure out the text that is written. We can very

123
00:07:54,152 --> 00:07:58,174
easily figure out the person. But then when we look at examples

124
00:07:58,222 --> 00:08:01,880
like the ones that are in the middle, right, in the top and the bottom,

125
00:08:02,330 --> 00:08:06,130
we are having a pretty difficult time discerning what is what in this picture.

126
00:08:06,210 --> 00:08:09,590
So when you're presented with an item like this and you have to match it

127
00:08:09,660 --> 00:08:13,350
to the image on the left, this quickly becomes a very difficult task.

128
00:08:13,510 --> 00:08:17,146
So this comes back to the question, can we use machine learning to

129
00:08:17,168 --> 00:08:20,526
solve this? Why? So now we know the why. We know

130
00:08:20,548 --> 00:08:24,206
the what. Now let's move at how did we

131
00:08:24,228 --> 00:08:28,206
solve this? So these

132
00:08:28,228 --> 00:08:32,570
first thing that you can see is that these images,

133
00:08:32,650 --> 00:08:36,062
right? So let's take the image on the right doesn't

134
00:08:36,126 --> 00:08:39,262
quite look the same as this image.

135
00:08:39,406 --> 00:08:43,486
So this one on the left, for example, this has been taken with a professional

136
00:08:43,518 --> 00:08:46,690
equipment, has been taken without a background.

137
00:08:46,850 --> 00:08:50,150
So you can see that the illumination is very constant. You can see that

138
00:08:50,220 --> 00:08:53,750
this is very high resolution, and there's also

139
00:08:53,900 --> 00:08:57,270
no blurriness, and everything is on focus.

140
00:08:57,420 --> 00:09:00,890
So this is not always the case, especially when we get

141
00:09:00,960 --> 00:09:04,554
images. The Ashmore museum gets images that belong to

142
00:09:04,592 --> 00:09:08,602
individuals or other research institutions who might not have the same

143
00:09:08,656 --> 00:09:11,966
researchers for capturing this information. So some

144
00:09:11,988 --> 00:09:15,838
of the technical challenges that

145
00:09:15,924 --> 00:09:19,374
we can face is that first, the image will be very low

146
00:09:19,412 --> 00:09:22,590
resolution. For example, let's say you take it with a smartphone,

147
00:09:23,270 --> 00:09:26,786
maybe it's blurry or noisy. There is also a

148
00:09:26,808 --> 00:09:30,514
very inconsistent illumination across the image, so some

149
00:09:30,552 --> 00:09:33,794
areas might be darker than others. Also,

150
00:09:33,832 --> 00:09:38,194
the physical condition of the coin might be that the coin might be highly deteriorated,

151
00:09:38,322 --> 00:09:42,166
right? So this will play against actually

152
00:09:42,268 --> 00:09:46,086
finding out similar items. And also, the problem itself is

153
00:09:46,108 --> 00:09:49,826
very hard because we are talking about coins or objects that

154
00:09:49,868 --> 00:09:53,226
are more than 2000 years old in some cases. So in

155
00:09:53,248 --> 00:09:56,634
short, photos that are taken by non museum personnel look

156
00:09:56,672 --> 00:10:00,010
very different than images within the digital collection,

157
00:10:00,430 --> 00:10:03,614
making visual search very challenged. So the way that we thought about

158
00:10:03,652 --> 00:10:07,838
this is that we should first split the task into two.

159
00:10:07,924 --> 00:10:11,674
First, one is let's improve the base image quality and let's

160
00:10:11,722 --> 00:10:16,206
make this coin look as much as possible to

161
00:10:16,308 --> 00:10:19,746
look as similar as possible to the one that we have in these

162
00:10:19,768 --> 00:10:23,298
digital collection. Right? So we want to. For example, in this case, we have

163
00:10:23,304 --> 00:10:26,574
a blurry background, we have a rotation, we have low resolution.

164
00:10:26,622 --> 00:10:29,894
So we want to account for all of those things and create

165
00:10:29,932 --> 00:10:33,286
an image that is very similar to the ones on the right. Once we have

166
00:10:33,308 --> 00:10:36,758
this image, we can extract features out of it

167
00:10:36,844 --> 00:10:40,086
and search in the collection to bring back the most similarly

168
00:10:40,118 --> 00:10:43,642
looking items. So this is an example of

169
00:10:43,776 --> 00:10:47,158
what we're doing here. You can see that we are detecting

170
00:10:47,174 --> 00:10:50,506
the shape of the coin and then we

171
00:10:50,528 --> 00:10:53,422
care coins, all of these activities at the same time,

172
00:10:53,476 --> 00:10:57,178
right? So we are removing the background, we are rotating

173
00:10:57,274 --> 00:11:00,746
these image, and then we are also increased the resolution

174
00:11:00,778 --> 00:11:04,398
of this image. So that way we have the item on the right,

175
00:11:04,484 --> 00:11:07,746
which is more similar than the image that we had on the left.

176
00:11:07,848 --> 00:11:10,580
Once we had this image on the right,

177
00:11:11,990 --> 00:11:15,406
we come back to this metadata that we have also extracted

178
00:11:15,438 --> 00:11:19,762
from the image, right? So we know if the image is heads on tails

179
00:11:19,906 --> 00:11:23,654
or tails with a 95% aggressive. So we know that

180
00:11:23,692 --> 00:11:27,350
in this case, we are looking at the overs of a coin and we can

181
00:11:27,420 --> 00:11:30,770
scan through all the images in the collection,

182
00:11:30,850 --> 00:11:34,650
but only at the overs. We don't need to look at the back aws well,

183
00:11:34,800 --> 00:11:37,994
and we can also use this other

184
00:11:38,032 --> 00:11:41,758
information, like, for example, the material, the region, the city,

185
00:11:41,844 --> 00:11:45,934
the province, and the person who is at the coin to make the

186
00:11:45,972 --> 00:11:49,950
task of identifying this item easier. So with this,

187
00:11:50,020 --> 00:11:53,838
let's move to a very quick demo of what this

188
00:11:54,004 --> 00:11:57,522
proof of concept was. And just

189
00:11:57,576 --> 00:12:00,900
have to say that this demo has been produced more than a year ago.

190
00:12:02,470 --> 00:12:05,140
There is a new open source solution that is in the works.

191
00:12:05,910 --> 00:12:09,842
It's not going to be restricted only to coins, but rather any collections

192
00:12:09,906 --> 00:12:12,626
object that you will have either physical or digital,

193
00:12:12,738 --> 00:12:16,070
say gems or fossils. Any object

194
00:12:16,220 --> 00:12:20,474
with the idea, the care concept that you want to visually search for

195
00:12:20,512 --> 00:12:24,362
similar items inside a collection. If you're interested in something

196
00:12:24,416 --> 00:12:28,522
like this, keep posted to this video. We're going to add

197
00:12:28,656 --> 00:12:32,446
any news that come out. Anything that

198
00:12:32,468 --> 00:12:35,646
is released will be added in the comments below this

199
00:12:35,668 --> 00:12:39,754
video. So with this, this is a web application created

200
00:12:39,802 --> 00:12:43,118
using streamlip and show you.

201
00:12:43,204 --> 00:12:47,278
So the idea for this is that we can interact with these models

202
00:12:47,454 --> 00:12:51,314
that we have created in a way that we can,

203
00:12:51,352 --> 00:12:54,626
for example, upload a picture. This case, the first thing that we

204
00:12:54,648 --> 00:12:58,610
want to do is either choose an example from a library or

205
00:12:58,680 --> 00:13:02,214
upload a picture. In this case, we choose the image that we saw before.

206
00:13:02,332 --> 00:13:05,858
Blurry background, image rotated, low resolution.

207
00:13:05,954 --> 00:13:09,606
So what we want to do is first find a region of interest

208
00:13:09,708 --> 00:13:12,970
out of this image, remove the background, auto rotate it,

209
00:13:13,040 --> 00:13:16,746
and these finally apply some deep blur and upscaling to

210
00:13:16,768 --> 00:13:20,246
the image. So once we have finished this and this is all

211
00:13:20,288 --> 00:13:24,270
happening in real time, you will have this image.

212
00:13:24,610 --> 00:13:27,822
That is the output of this process.

213
00:13:27,956 --> 00:13:31,950
Once we have this, we want to extract also metadata

214
00:13:32,610 --> 00:13:34,820
out of this image, right? So for example,

215
00:13:37,510 --> 00:13:40,530
is the overs, who is the person

216
00:13:40,600 --> 00:13:44,114
that is in the image? What is the material that this coin is made

217
00:13:44,152 --> 00:13:47,400
of? What is the reason that this belongs to? And so on.

218
00:13:48,010 --> 00:13:51,894
Once we have this metadata, we care going to use the features that

219
00:13:51,932 --> 00:13:55,640
we have extracted out of this image. So this is,

220
00:13:57,210 --> 00:14:00,614
for example, the faces, the eyes,

221
00:14:00,732 --> 00:14:03,946
the way that these are placed in the image. We care going to use this

222
00:14:04,048 --> 00:14:08,106
to look inside our collection. And you can see that we are

223
00:14:08,128 --> 00:14:12,654
going to come back with eight results that

224
00:14:12,692 --> 00:14:15,966
are similar to the image that we are looking for. So in

225
00:14:15,988 --> 00:14:18,240
this case, volunteers, for example,

226
00:14:19,170 --> 00:14:22,366
doesn't have to go through thousands of images. They only

227
00:14:22,388 --> 00:14:25,914
have to focus on eight images. And they also have information

228
00:14:26,052 --> 00:14:29,666
that can point them in the right direction. Right. So they have

229
00:14:29,688 --> 00:14:32,946
the region, the person who is in the picture, and also they

230
00:14:32,968 --> 00:14:36,994
have similar items. So maybe when they see an item that

231
00:14:37,032 --> 00:14:40,360
already exists, this is just another specimen, they can quickly

232
00:14:40,970 --> 00:14:44,790
attach this to that one. So what are the benefits of using

233
00:14:44,860 --> 00:14:48,406
aws for dash model? So the first one is that this is very quick and

234
00:14:48,428 --> 00:14:52,666
easy experimentation. They built and deploy eleven machine learning models in about ten weeks.

235
00:14:52,848 --> 00:14:56,730
There's a smaller workload, right. So you can imagine that saving

236
00:14:56,800 --> 00:14:59,978
up minutes of every task in a

237
00:14:59,984 --> 00:15:03,520
pipeline. In the end, when you have a large volume of

238
00:15:04,450 --> 00:15:07,614
items that you have to digitalize they adapt to a lot of time.

239
00:15:07,652 --> 00:15:11,230
In this case, it's estimated that they will save up to three years

240
00:15:11,300 --> 00:15:15,146
of work cataloging a collection of 300,000 coins.

241
00:15:15,338 --> 00:15:18,514
Less time. The coin analysis is expected to take just

242
00:15:18,552 --> 00:15:21,794
a few minutes versus times that are ranging from 10

243
00:15:21,832 --> 00:15:25,106
minutes to maybe hours. And also more value, right?

244
00:15:25,128 --> 00:15:28,406
So this is complementing the work that is already being carried out

245
00:15:28,428 --> 00:15:32,230
by volunteers. This is not automating anything, this is augmenting

246
00:15:32,970 --> 00:15:36,134
the people, the humans who are behind this.

247
00:15:36,252 --> 00:15:39,634
So these are some quotes of this. I thought this project

248
00:15:39,692 --> 00:15:43,260
would be complex and time consuming, but using Aws made it easy.

249
00:15:43,870 --> 00:15:46,938
Another one, this comes from Jerome. Now we

250
00:15:46,944 --> 00:15:50,890
can focus our volunteers on other steps that add value machine learning process

251
00:15:50,960 --> 00:15:54,926
improves the workflow and productivity and adds value for the public.

252
00:15:55,108 --> 00:15:58,762
With this, let's have a look at this very small task

253
00:15:58,906 --> 00:16:01,902
as an example. We want to remove the background of this, right?

254
00:16:01,956 --> 00:16:04,386
And we are going to see in one of the examples how we can do

255
00:16:04,408 --> 00:16:06,290
this actually technically.

256
00:16:07,910 --> 00:16:11,714
And doing this doesn't have to be all

257
00:16:11,752 --> 00:16:15,858
done by yourself. For example, there are some solutions available

258
00:16:15,944 --> 00:16:19,510
in the marketplace that you can use an out of the box,

259
00:16:19,580 --> 00:16:22,886
right, for background removal. And in this case, this one,

260
00:16:22,908 --> 00:16:26,806
for example, at this time you have a price for every API call and you

261
00:16:26,828 --> 00:16:30,026
can just subscribe to this one. So if you have images that you

262
00:16:30,048 --> 00:16:33,254
want to remove this background for, then you will just subscribe

263
00:16:33,302 --> 00:16:38,090
to this API and then just run them through this

264
00:16:38,240 --> 00:16:41,834
service right through this endpoint. Another way that you can do it is of course

265
00:16:41,872 --> 00:16:45,310
you can build your own algorithm. And we're going to see an example

266
00:16:45,460 --> 00:16:48,320
actually out of this, where you pick up, for example,

267
00:16:48,850 --> 00:16:52,174
this data set that is an image segmentation data

268
00:16:52,212 --> 00:16:55,838
set, open images, and you have more than 600 classes

269
00:16:55,934 --> 00:16:59,506
where these segmentation masks are available. Then you

270
00:16:59,528 --> 00:17:02,606
use an algorithm. In this case you're

271
00:17:02,638 --> 00:17:05,830
using mask or CNN. And we can use different machine learning

272
00:17:05,900 --> 00:17:10,134
frameworks, Pytorch, Tensorflow, Mxnet, together with

273
00:17:10,252 --> 00:17:13,800
Sagemaker and different ways of doing training.

274
00:17:14,330 --> 00:17:17,846
So with this you can build also very

275
00:17:17,868 --> 00:17:22,194
easily, you can build your own custom pipeline. These shows us some resources.

276
00:17:22,242 --> 00:17:25,498
I'm going to add these to the description of the video anyway,

277
00:17:25,664 --> 00:17:29,180
but just to give you an idea of other things that you can do,

278
00:17:29,970 --> 00:17:33,946
there is a recent collaboration between hiringface and Sagemaker

279
00:17:33,978 --> 00:17:36,954
that is very useful. It's very robust,

280
00:17:37,002 --> 00:17:40,814
secure, and also I

281
00:17:40,852 --> 00:17:44,500
added some documents and repositories for

282
00:17:45,030 --> 00:17:49,086
deploying your very own web application for machine

283
00:17:49,118 --> 00:17:52,706
learning. So with that, I'm coins to actually change the

284
00:17:52,728 --> 00:17:56,466
focus to the second bit of the presentation.

285
00:17:56,578 --> 00:18:00,120
And I'm going to move to this one. Okay,

286
00:18:01,290 --> 00:18:04,838
so now we want to explore these idea of

287
00:18:04,924 --> 00:18:07,618
building your own machine learning solution,

288
00:18:07,714 --> 00:18:10,860
right? So we want to build the same thing. How can we do,

289
00:18:12,190 --> 00:18:15,690
we want to create these heads versus tails model

290
00:18:15,760 --> 00:18:19,530
the classifier. We want to remove the background and also we want to visually search

291
00:18:19,600 --> 00:18:23,494
these images in a collection. So how can we do it? Okay, so let's

292
00:18:23,542 --> 00:18:27,070
focus first on the first one. Okay, so image classification.

293
00:18:27,410 --> 00:18:29,680
So for that one, I'm going to show you now,

294
00:18:30,050 --> 00:18:34,170
Sagemaker Studio. This is an end to end platform

295
00:18:34,260 --> 00:18:37,666
for machine learning from AWS. In this case, I'm not

296
00:18:37,688 --> 00:18:41,042
going to go into a lot of detail about what thing

297
00:18:41,096 --> 00:18:43,780
does what or anything like that,

298
00:18:44,310 --> 00:18:47,990
but I'm going to show you that there is something called

299
00:18:48,060 --> 00:18:51,302
Shamstat. And what is that? Basically when you click

300
00:18:51,356 --> 00:18:54,866
here, let's take the first one. Right? So model popular image

301
00:18:54,898 --> 00:18:58,058
classification based on. Okay, so that's exactly what we want to do. We want

302
00:18:58,064 --> 00:19:01,878
to build an image classifier. Let's do some more exploration.

303
00:19:01,974 --> 00:19:05,818
Okay, so when we explore it, you see that,

304
00:19:05,984 --> 00:19:09,670
for example, I particularly like this architecture.

305
00:19:09,750 --> 00:19:13,658
Efficient. Net has a very good performance.

306
00:19:13,834 --> 00:19:17,358
And you can see how you have different versions of this

307
00:19:17,444 --> 00:19:21,322
available out of the box. So these one are feature

308
00:19:21,386 --> 00:19:25,026
vector extractor. Right. And we'll get to

309
00:19:25,048 --> 00:19:28,802
why this is important in a second. But just

310
00:19:28,856 --> 00:19:32,674
keep them in mind for now. What we want to choose this is we

311
00:19:32,712 --> 00:19:36,118
want to choose the biggest variation, the b seven,

312
00:19:36,284 --> 00:19:40,246
these most performant, and we want to use these for our

313
00:19:40,348 --> 00:19:43,954
model. So once you click these and you have selected

314
00:19:44,002 --> 00:19:46,280
these model or these,

315
00:19:47,710 --> 00:19:51,302
then we can either deploy

316
00:19:51,366 --> 00:19:54,780
the version that is available without any changes.

317
00:19:55,390 --> 00:19:58,486
This model has been trained with imagenet.

318
00:19:58,518 --> 00:20:02,430
So let's go back for a second. So we have this.

319
00:20:02,580 --> 00:20:06,270
What is this? So this is jumpstart is a repository of

320
00:20:06,340 --> 00:20:10,222
solutions and models that you can quickly

321
00:20:10,356 --> 00:20:13,760
deploy with one click. In this case,

322
00:20:14,450 --> 00:20:17,874
we want to look at vision models and we want to look at

323
00:20:17,912 --> 00:20:21,666
solving the task image classification, right. So we

324
00:20:21,688 --> 00:20:25,634
also have the data set that this model has been trained on

325
00:20:25,752 --> 00:20:29,126
and we know if the model is fine tunable or

326
00:20:29,148 --> 00:20:33,046
not. This case it is. Right? So the same as this

327
00:20:33,068 --> 00:20:36,486
model that we have here. So how can you fine

328
00:20:36,508 --> 00:20:39,666
tune it? Well, you just go here to fine tune

329
00:20:39,698 --> 00:20:43,306
model. You choose the data source and you find your s

330
00:20:43,328 --> 00:20:47,146
three buck. You choose it, choose the directory name where you have it and

331
00:20:47,168 --> 00:20:50,506
then you can choose the instance that you want to use to

332
00:20:50,528 --> 00:20:54,526
train and then the parameters that you want to use and

333
00:20:54,548 --> 00:20:57,966
you will train it. And once this is trained, you can deploy it

334
00:20:57,988 --> 00:21:01,230
as an endpoint and use this model for inference.

335
00:21:01,730 --> 00:21:04,880
So how should you position your data?

336
00:21:05,330 --> 00:21:08,350
So you would have your input directory.

337
00:21:08,430 --> 00:21:11,346
This is the s three bucket that we were talking about before. And these you

338
00:21:11,368 --> 00:21:14,978
will have two folders. First one will be the overs and then

339
00:21:15,064 --> 00:21:18,754
you will have your examples and then you will have the reverse.

340
00:21:18,802 --> 00:21:22,358
Right, an example. And with that you don't have to

341
00:21:22,364 --> 00:21:26,502
do anything else. You can directly train it from this

342
00:21:26,556 --> 00:21:30,050
screen. Once this is trained and deployed,

343
00:21:30,130 --> 00:21:33,562
you can deploy it. And what you're going to see is something like this.

344
00:21:33,696 --> 00:21:37,546
So you can see that this takes around 10 minutes

345
00:21:37,648 --> 00:21:41,290
maybe to deploy or even less than that. This is using

346
00:21:41,360 --> 00:21:45,054
a CPU instance in this case. So you don't have to worry about

347
00:21:45,172 --> 00:21:47,920
GPU or CPU. You can use both.

348
00:21:48,690 --> 00:21:52,570
And you have an endpoint, you have a notebook that will

349
00:21:52,660 --> 00:21:56,066
show you how you can

350
00:21:56,248 --> 00:21:57,780
use this,

351
00:21:59,430 --> 00:22:03,006
how you can use this endpoint,

352
00:22:03,118 --> 00:22:06,814
right? So this one you see that we have two pictures.

353
00:22:06,862 --> 00:22:10,546
In this case we are using the original

354
00:22:10,578 --> 00:22:13,734
model. So the only thing that it has to do is pick up

355
00:22:13,772 --> 00:22:17,430
that this is a cat and a dog. And then you can see here

356
00:22:17,580 --> 00:22:20,874
top five model predictions or tabby, et cetera and so on.

357
00:22:20,992 --> 00:22:24,426
Top five model, et cetera. If you were using your

358
00:22:24,448 --> 00:22:27,654
own model, these classes

359
00:22:27,782 --> 00:22:31,510
will have been drivers and overs,

360
00:22:31,590 --> 00:22:34,942
right. So with that, let's actually move to

361
00:22:34,996 --> 00:22:38,414
the second model that we want to do. So we finish

362
00:22:38,452 --> 00:22:42,490
an image classifier and now we want to move to a segmentation model.

363
00:22:42,580 --> 00:22:47,346
We want to remove the background. So you

364
00:22:47,368 --> 00:22:51,614
can use your own segmentation

365
00:22:51,662 --> 00:22:55,842
model or you can just check other solutions

366
00:22:55,906 --> 00:22:59,174
that are open and available. So this is a website

367
00:22:59,292 --> 00:23:03,250
that I really like, papers with code. The task

368
00:23:03,330 --> 00:23:07,110
that we want to solve for is saliency detection.

369
00:23:07,710 --> 00:23:10,954
And you can see that you also have here

370
00:23:10,992 --> 00:23:15,046
available things like YouTube

371
00:23:15,078 --> 00:23:19,638
net. This one is very successful at detecting background

372
00:23:19,734 --> 00:23:23,006
and removing it. So choosing the most important object in the

373
00:23:23,028 --> 00:23:26,606
image and these removing the background. So this

374
00:23:26,628 --> 00:23:30,266
is also something that you can use with Sagemaker and then deploy

375
00:23:30,298 --> 00:23:33,714
it as an endpoint. Because we want to build something

376
00:23:33,752 --> 00:23:34,900
that is very custom.

377
00:23:37,990 --> 00:23:41,700
We care going to go through a different route and we care going to use

378
00:23:43,430 --> 00:23:46,766
an open data set, in this case these open image

379
00:23:46,798 --> 00:23:50,134
data set. We're going to look for coin, but it can be other things

380
00:23:50,172 --> 00:23:54,006
as well. And you can see that we have here

381
00:23:54,108 --> 00:23:57,670
the segmentation mask and they are available. So we are going to use this

382
00:23:57,740 --> 00:24:01,674
segmentation mask to train our model. So for

383
00:24:01,712 --> 00:24:05,306
this we're going to use this repo that we

384
00:24:05,328 --> 00:24:08,890
have here. I'm going to put these, this is in the links that are available

385
00:24:08,960 --> 00:24:13,242
in the presentation and will be made available, the description of

386
00:24:13,376 --> 00:24:17,162
the video. And I'm just going to walk you through some of the steps

387
00:24:17,226 --> 00:24:20,666
that we want to do this, you want to do here. So we're

388
00:24:20,698 --> 00:24:24,562
going to use custom library that is called Ice vision. This is

389
00:24:24,616 --> 00:24:28,414
built on top of Pytorch and it's on top of Pytorch lightning

390
00:24:28,542 --> 00:24:31,662
and also fast AI.

391
00:24:31,726 --> 00:24:35,506
So it uses both things for training. And at

392
00:24:35,528 --> 00:24:38,022
the same time it has available many,

393
00:24:38,076 --> 00:24:41,670
many algorithms out of the box. So for example,

394
00:24:41,740 --> 00:24:45,218
factor CNN or Mascar CNN. So this is these thing that I'm

395
00:24:45,234 --> 00:24:49,030
going to be using for training in this session.

396
00:24:49,870 --> 00:24:53,066
So the first thing that we want to do is we want to

397
00:24:53,088 --> 00:24:56,906
download that data set and all the

398
00:24:56,928 --> 00:25:01,322
images, but only for the class coin,

399
00:25:01,466 --> 00:25:05,086
right? There are more than 600 classes available,

400
00:25:05,188 --> 00:25:07,870
but we only want to use this one coin.

401
00:25:08,370 --> 00:25:12,190
And 600 are these, these are the 600

402
00:25:12,260 --> 00:25:15,794
like person, piano, et cetera and so on. So we only want

403
00:25:15,832 --> 00:25:18,610
to train this model on coins.

404
00:25:19,190 --> 00:25:23,298
Okay, so the first thing that we do, we download the data and

405
00:25:23,384 --> 00:25:27,046
extract these images and the segmentation mask, we save them

406
00:25:27,068 --> 00:25:31,362
locally and then we convert these annotations because originally

407
00:25:31,506 --> 00:25:35,414
they use one vocabulary for this

408
00:25:35,452 --> 00:25:38,662
annotation and we want to move it to another one. So we move it from

409
00:25:38,716 --> 00:25:42,054
something that is called Pascal to another one that is called cocoa.

410
00:25:42,102 --> 00:25:45,306
Common objects in comma. So once we do

411
00:25:45,328 --> 00:25:49,194
this, we upload the data with this one

412
00:25:49,232 --> 00:25:53,162
line of code, right? We upload the data to a string,

413
00:25:53,226 --> 00:25:56,430
which is our object storage, storage.

414
00:25:57,650 --> 00:26:01,566
And we define what

415
00:26:01,588 --> 00:26:05,266
are the resources that we want to use for training. This case we want to

416
00:26:05,288 --> 00:26:08,162
use CPU instance. So we use this, these p, three,

417
00:26:08,216 --> 00:26:11,470
two, x large. And I'm not going to use a spot,

418
00:26:11,630 --> 00:26:16,614
but you can think about spot as a way for going

419
00:26:16,652 --> 00:26:19,670
into an auction for unused compute capacity.

420
00:26:20,250 --> 00:26:24,002
And you bid for this unused capacity.

421
00:26:24,146 --> 00:26:28,186
Normally the savings range from 60% to 90%.

422
00:26:28,288 --> 00:26:31,420
So this is whatever the on demand price is,

423
00:26:31,790 --> 00:26:35,930
60% to 90% less than on demand price.

424
00:26:36,080 --> 00:26:40,486
And the only caveat that you have is that these resources,

425
00:26:40,598 --> 00:26:44,954
because you are bidding for them, once someone wants

426
00:26:44,992 --> 00:26:48,398
to use on demand researchers, your capacity will be

427
00:26:48,404 --> 00:26:52,846
taken away and given to them. So effectively your training will stop.

428
00:26:53,028 --> 00:26:57,266
The good thing is that all of this is already taken care of on

429
00:26:57,288 --> 00:27:00,658
AWS and you are saving checkpoints as you are

430
00:27:00,664 --> 00:27:04,178
moving on with your training. So in this way, if your training

431
00:27:04,264 --> 00:27:08,086
suddenly stops, for example, once this

432
00:27:08,108 --> 00:27:11,862
compute capacity becomes available again, you can start using

433
00:27:11,916 --> 00:27:15,654
it one more time. So I would recommend you to use

434
00:27:15,692 --> 00:27:19,386
these things because with only three lines of code you

435
00:27:19,408 --> 00:27:23,210
can save maybe from 60% to 90% of the cost.

436
00:27:23,360 --> 00:27:26,906
So once we

437
00:27:26,928 --> 00:27:29,814
have set up that configuration,

438
00:27:29,942 --> 00:27:33,066
we go here and we create something that is

439
00:27:33,088 --> 00:27:36,942
called an estimator, right? And we take our

440
00:27:36,996 --> 00:27:39,040
train script which is this one,

441
00:27:39,970 --> 00:27:42,640
the source directory where everything is.

442
00:27:43,590 --> 00:27:46,894
Let me show you this case. It's only two files,

443
00:27:46,942 --> 00:27:50,866
requires TXT and train and we pass

444
00:27:51,048 --> 00:27:54,114
arguments parameters to these

445
00:27:54,152 --> 00:27:58,322
training shop. So what this is going to do effectively is create a container

446
00:27:58,386 --> 00:28:02,342
new, different from what you're seeing here. Another instance only

447
00:28:02,396 --> 00:28:06,214
for this task and you will only have to pay for the amount

448
00:28:06,252 --> 00:28:09,546
of time that you've been training, not more than

449
00:28:09,568 --> 00:28:15,514
that. So with that you can see that we

450
00:28:15,552 --> 00:28:18,906
create this estimator and then we fit to the data that

451
00:28:18,928 --> 00:28:22,746
we had. So inputs, this is the data that we downloaded and

452
00:28:22,768 --> 00:28:26,062
then uploaded to s three. And after some time

453
00:28:26,116 --> 00:28:29,614
this is going to finish and it's going to tell us that

454
00:28:29,652 --> 00:28:33,166
it was successful. Of course you can also track this if

455
00:28:33,188 --> 00:28:36,626
you go to the AWS console and you

456
00:28:36,648 --> 00:28:40,626
can see the shops here for example, you can see

457
00:28:40,808 --> 00:28:44,754
how much this training took. This is around 22 minutes and we were

458
00:28:44,792 --> 00:28:49,030
charged for 22 minutes. If we were using spot instances

459
00:28:50,570 --> 00:28:54,054
we would have had reduction of

460
00:28:54,092 --> 00:28:58,262
around 70% of the cost in this case. So once this

461
00:28:58,316 --> 00:29:01,994
is finished training we want to deploy this model and

462
00:29:02,032 --> 00:29:05,738
run our predictions. So for that we can use this other example

463
00:29:05,904 --> 00:29:10,074
where what I'm actually doing here is I'm creating a

464
00:29:10,112 --> 00:29:13,406
container but I'm running this model,

465
00:29:13,508 --> 00:29:17,166
right. So you can see all the steps, just want to show you don't want

466
00:29:17,188 --> 00:29:21,198
to stay on the details too much. You can

467
00:29:21,364 --> 00:29:24,706
explore this at your own time. But I just want to

468
00:29:24,728 --> 00:29:28,866
show you the results of this. You can see that the

469
00:29:28,888 --> 00:29:31,970
actual time that it takes for a prediction is quite quick,

470
00:29:32,120 --> 00:29:35,554
right. And the quality is quite

471
00:29:35,592 --> 00:29:38,598
good right. So we have the image on the left and we only want to

472
00:29:38,604 --> 00:29:41,926
pick up one coins. So we pick up the one on the right and you

473
00:29:41,948 --> 00:29:45,800
can see how the background has been removed completely

474
00:29:46,730 --> 00:29:50,086
and the image is clean. So with

475
00:29:50,108 --> 00:29:54,282
that and conscious of time, going to move to the last item today

476
00:29:54,416 --> 00:29:58,522
and that is how can we build a visual search engine.

477
00:29:58,656 --> 00:30:02,110
And for that we care going to follow this blog post

478
00:30:02,260 --> 00:30:04,666
building a visual search application with Amazon,

479
00:30:04,698 --> 00:30:07,390
sagemaker and elasticsearch.

480
00:30:08,050 --> 00:30:11,886
So basically what we want to do is you have, and this is using

481
00:30:11,988 --> 00:30:15,294
an open source data set from clothes fashion.

482
00:30:15,422 --> 00:30:19,074
But of course you can think that you can change these

483
00:30:19,112 --> 00:30:22,686
things, these images, to the images

484
00:30:22,718 --> 00:30:26,354
that you have, for example, coins, right. So what this is going to do is

485
00:30:26,392 --> 00:30:29,750
it's going to run a convolutional neural network against

486
00:30:29,820 --> 00:30:33,366
these images. It's going to extract these feature vectors. And this is

487
00:30:33,388 --> 00:30:36,454
going back to that model, right, that we were talking about,

488
00:30:36,572 --> 00:30:40,774
the Shamstad model. Right. So we

489
00:30:40,812 --> 00:30:44,546
have this feature vector structure.

490
00:30:44,658 --> 00:30:48,314
Right. So we can actually deploy this and

491
00:30:48,352 --> 00:30:52,142
we don't have to do any type of custom modeling. We have the model right

492
00:30:52,196 --> 00:30:56,106
here. So once we have these vectors,

493
00:30:56,218 --> 00:30:59,546
we input all of these vectors into elasticsearch,

494
00:30:59,658 --> 00:31:03,558
and then we do something called k nearest

495
00:31:03,594 --> 00:31:07,102
neighbors search. So we look at the images

496
00:31:07,166 --> 00:31:11,518
that have the lowest distance between them, between the feature vectors

497
00:31:11,534 --> 00:31:14,766
from these images and the reference image

498
00:31:14,798 --> 00:31:18,150
that we have, right. So if you go through these steps,

499
00:31:18,730 --> 00:31:21,542
you will see that clicking here,

500
00:31:21,596 --> 00:31:24,982
launch a stack. This is going to open up this

501
00:31:25,036 --> 00:31:28,866
screen where basically we just create the resources

502
00:31:28,898 --> 00:31:32,874
that you need to run this. So it will create an

503
00:31:32,912 --> 00:31:36,314
S three bucket, it will create a sage maker notebook. And then the only

504
00:31:36,352 --> 00:31:39,420
thing that you need to do is actually, let me show you,

505
00:31:39,790 --> 00:31:43,042
open your notebook that was recently increased,

506
00:31:43,126 --> 00:31:46,800
increased. And you care going to be presented with

507
00:31:48,050 --> 00:31:52,000
this repo, right. And this repo is this one.

508
00:31:52,550 --> 00:31:55,970
Again, the link to this is,

509
00:31:56,120 --> 00:31:59,300
you can find it in the description of the video.

510
00:32:00,070 --> 00:32:03,742
Let's dive right into it. Right. So we have this image,

511
00:32:03,806 --> 00:32:07,046
visual image search. The first thing that we want to do

512
00:32:07,068 --> 00:32:11,430
is get these trend data, right. So this is almost 10,000

513
00:32:11,500 --> 00:32:15,734
high resolution images. In this case. In your use case,

514
00:32:15,932 --> 00:32:19,626
this will be your images. It wouldn't be these 10,000 images, it will

515
00:32:19,648 --> 00:32:23,530
be yours. And you can see

516
00:32:23,600 --> 00:32:27,082
that the first step that we do is we get this

517
00:32:27,136 --> 00:32:30,586
data and then we do some transformations, and then

518
00:32:30,608 --> 00:32:34,190
we upload this data to a string where we will have it.

519
00:32:34,260 --> 00:32:37,758
That will be the location that we are going to read from once we want

520
00:32:37,764 --> 00:32:41,630
to train our model. So once we have these images,

521
00:32:42,690 --> 00:32:46,610
we are going to be using a pretrained model

522
00:32:46,680 --> 00:32:49,570
that comes included in the Keras libraries.

523
00:32:49,910 --> 00:32:53,266
This case, it will be Resnet 50. But like we

524
00:32:53,288 --> 00:32:56,786
were seeing before, you can actually, instead of doing all of these steps, you can

525
00:32:56,808 --> 00:33:00,198
just use the model that we saw before.

526
00:33:00,284 --> 00:33:04,018
Right. Let me go again. So this model, you can just, once you deploy

527
00:33:04,034 --> 00:33:07,634
it, you click deployment, you will be presented with

528
00:33:07,772 --> 00:33:11,546
an endpoint URL. And that is the one that you can use to do

529
00:33:11,568 --> 00:33:16,122
this task. Otherwise, let's continue with this custom

530
00:33:16,176 --> 00:33:19,660
implementation. So we take this Resnet 50

531
00:33:20,190 --> 00:33:23,722
and we want to deploy it as

532
00:33:23,776 --> 00:33:27,390
an endpoint, right. So that's what we do now

533
00:33:27,460 --> 00:33:30,686
we use this piece of script. It's the one

534
00:33:30,708 --> 00:33:34,322
that we are going to be using to pick up the model, load it into

535
00:33:34,376 --> 00:33:38,046
memory, and then run all of these images

536
00:33:38,238 --> 00:33:41,374
through this and only return the feature vectors,

537
00:33:41,422 --> 00:33:45,410
not the actual label, out of it, just the feature vectors.

538
00:33:45,910 --> 00:33:49,142
So that is what we do here.

539
00:33:49,276 --> 00:33:53,030
So in this place we are going to deploy the model

540
00:33:53,180 --> 00:33:56,246
as a sage maker endpoint. This normally takes around 10 minutes.

541
00:33:56,348 --> 00:33:59,730
You can see that I'm using CPU instance.

542
00:33:59,810 --> 00:34:03,354
This case I'm going to deploy only one, but you can

543
00:34:03,392 --> 00:34:07,050
change it if you want this to be quicker, for example. So all

544
00:34:07,200 --> 00:34:10,010
the requests will be routed to one instance.

545
00:34:10,370 --> 00:34:14,426
In this case we are going to be using an example image

546
00:34:14,458 --> 00:34:17,822
and this is the result that comes

547
00:34:17,876 --> 00:34:22,746
back from these input. So these are the feature vectors.

548
00:34:22,938 --> 00:34:26,354
Once we tested that, this actually works. We want to build

549
00:34:26,392 --> 00:34:29,826
this index, right? So we want to first get all

550
00:34:29,848 --> 00:34:33,154
the images, all the keys of these files on s

551
00:34:33,192 --> 00:34:36,414
three. And then we want to basically

552
00:34:36,552 --> 00:34:40,166
process all of those images. We want to get the feature vectors out of all

553
00:34:40,188 --> 00:34:44,166
of those images and we want to upload them or

554
00:34:44,348 --> 00:34:48,326
get them into this elasticsearch index.

555
00:34:48,518 --> 00:34:52,426
So once we have done that, you can see

556
00:34:52,448 --> 00:34:56,730
that that is what we are coins here, we care, importing these features

557
00:34:58,430 --> 00:35:01,950
into elasticsearch. And the next thing that we can do is

558
00:35:02,020 --> 00:35:05,854
now we can do a test. So you see that

559
00:35:05,972 --> 00:35:09,280
we have the first image, the query image here,

560
00:35:10,610 --> 00:35:14,226
and now we say, okay, so bring me back

561
00:35:14,328 --> 00:35:17,794
examples out of your index, bring me back

562
00:35:17,832 --> 00:35:21,474
the most similarly looking images, right? So you can

563
00:35:21,512 --> 00:35:25,310
see that you're only returning outfits

564
00:35:25,390 --> 00:35:29,160
that have all of these patterns. So these are very similar

565
00:35:29,610 --> 00:35:33,094
between each other and the same

566
00:35:33,132 --> 00:35:36,214
thing. We use a different method, but these is the same

567
00:35:36,252 --> 00:35:40,042
result and you can see what this looks

568
00:35:40,096 --> 00:35:43,194
like, right? So in your case, using your own data,

569
00:35:43,312 --> 00:35:46,806
this will be presenting one coin as the reference image,

570
00:35:46,838 --> 00:35:50,250
the input image, and then returning

571
00:35:51,250 --> 00:35:55,514
all of these most similarly looking images

572
00:35:55,562 --> 00:35:58,846
in the collection, right? So the good thing about this application is

573
00:35:58,868 --> 00:36:02,778
that it actually also involves this implementation, that it

574
00:36:02,804 --> 00:36:06,594
also involves deploying a full stack visual search application. So this is great

575
00:36:06,632 --> 00:36:09,906
if you're doing a demo. So what

576
00:36:09,928 --> 00:36:13,810
you will do is these are several steps

577
00:36:14,150 --> 00:36:17,182
for creating the architecture.

578
00:36:17,246 --> 00:36:20,758
But basically once you run all of these steps, you're going to

579
00:36:20,764 --> 00:36:23,622
be presented with an application that looks like this.

580
00:36:23,756 --> 00:36:27,286
And I actually have it running locally here. So you

581
00:36:27,308 --> 00:36:31,194
see that, for example, you will choose how many

582
00:36:31,232 --> 00:36:34,490
items you want to return out of this. And these

583
00:36:34,560 --> 00:36:37,370
you can choose an image,

584
00:36:38,030 --> 00:36:41,774
and you will just submit your shop and then

585
00:36:41,812 --> 00:36:45,440
get the results back. Let me show you, for example,

586
00:36:46,050 --> 00:36:49,582
in here. The way that this will look is something

587
00:36:49,636 --> 00:36:53,214
like this. So, at the end of your experimentation, if you want,

588
00:36:53,252 --> 00:36:57,166
you can delete all of the resources that we created, and then you

589
00:36:57,188 --> 00:37:01,022
will just finish with your

590
00:37:01,076 --> 00:37:04,720
experimentation. You wouldn't have any extra cost out of this.

591
00:37:06,570 --> 00:37:10,386
So, with that, I actually wanted to come back to original

592
00:37:10,418 --> 00:37:14,534
presentation, and I wanted to thank you for staying with us so

593
00:37:14,572 --> 00:37:18,246
long, and I hope you find this presentation useful. Thank you very

594
00:37:18,268 --> 00:37:18,340
much.

