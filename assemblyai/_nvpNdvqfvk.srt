1
00:00:23,930 --> 00:00:27,462
Hello, everyone, and welcome to this talk on chaos validation made

2
00:00:27,516 --> 00:00:30,802
easy plug and play with resilience probes.

3
00:00:30,946 --> 00:00:34,930
My name is Silanjan, and I'm a software engineer working at harness,

4
00:00:35,090 --> 00:00:37,762
and I'm also a litmus chaos maintainer.

5
00:00:37,906 --> 00:00:40,950
Joining me today as my co speaker is Shyan.

6
00:00:41,370 --> 00:00:44,962
Hey guys, what's up? My name is Cheyenne. I'm a senior software engineer at Harness,

7
00:00:45,026 --> 00:00:48,086
and I've also been maintaining litmus chaos, the open source project.

8
00:00:48,268 --> 00:00:52,126
Thanks to Confortu for having us and really looking forward to you

9
00:00:52,148 --> 00:00:55,662
guys enjoying the talk. All right, with that out of the way,

10
00:00:55,716 --> 00:00:59,166
let's get going with chaos. Let's start the

11
00:00:59,188 --> 00:01:03,070
talk with a very important question. What causes downtime?

12
00:01:03,510 --> 00:01:06,514
Sure, we all have been there and experienced it,

13
00:01:06,552 --> 00:01:10,020
perhaps multiple times. However, it never gets easy.

14
00:01:10,390 --> 00:01:13,890
A downtime has many adverse effects for an organization.

15
00:01:14,390 --> 00:01:17,886
Take instance of Slack, whose SLA violations led

16
00:01:17,918 --> 00:01:22,070
to an 8 million payout and gravelly impacted the company's revenue.

17
00:01:22,570 --> 00:01:24,962
Wells Fargo, the financial giant,

18
00:01:25,106 --> 00:01:28,918
suffered a power shutdown in a data center due

19
00:01:28,934 --> 00:01:32,102
to smoke, which caused loss of transactions,

20
00:01:32,166 --> 00:01:36,490
and some direct deposit checks were not reflected in its accounts.

21
00:01:36,910 --> 00:01:40,906
In this instance, a single hour of downtime costed

22
00:01:40,938 --> 00:01:43,440
them over $100,000.

23
00:01:44,130 --> 00:01:48,202
Lastly, British Airways had to cancel 400 flights,

24
00:01:48,266 --> 00:01:51,342
which led to 75,000 passengers getting

25
00:01:51,396 --> 00:01:54,770
stranded, and costed them over 100 million

26
00:01:54,840 --> 00:01:58,366
in losses. In this case, it was a debugging

27
00:01:58,398 --> 00:02:01,950
issue in one server that cascaded to other servers,

28
00:02:02,030 --> 00:02:03,810
impacting the billing systems.

29
00:02:04,390 --> 00:02:08,374
Therefore, a downtime is often a result of a combination of

30
00:02:08,412 --> 00:02:12,674
issues in a system. With the ever increasing complexity

31
00:02:12,722 --> 00:02:16,214
of cloud native microservice applications, the question remains that

32
00:02:16,252 --> 00:02:20,326
how can we ensure that our distributed systems always withstand

33
00:02:20,438 --> 00:02:23,850
the adverse and potentially unforeseen situations?

34
00:02:24,350 --> 00:02:28,502
So why are we not better prepared at managing downtimes?

35
00:02:28,646 --> 00:02:32,018
First of all, microservices are prone to downtimes.

36
00:02:32,134 --> 00:02:35,642
While one can prepare for the apparent causes that need attention,

37
00:02:35,786 --> 00:02:39,710
no one can fully anticipate an overwhelming downtime before

38
00:02:39,780 --> 00:02:43,986
it takes place, as there are plethora of ways in which things can go wrong.

39
00:02:44,168 --> 00:02:48,206
And that's where chaos engineering can help in uncovering the weaknesses

40
00:02:48,238 --> 00:02:51,922
in a system and becoming better prepared at

41
00:02:51,976 --> 00:02:55,518
managing the various downtime scenarios.

42
00:02:55,694 --> 00:02:59,542
Our chaos failure scenarios can be difficult to run while

43
00:02:59,596 --> 00:03:02,966
ensuring the safety of the target resources, and often there

44
00:03:02,988 --> 00:03:06,534
isn't a good culture around it which makes it difficult to

45
00:03:06,572 --> 00:03:10,442
implement and scale. Lastly, as more

46
00:03:10,496 --> 00:03:14,650
volume of code gets pushed over the time in any organization,

47
00:03:14,990 --> 00:03:18,890
it becomes difficult to assess the system against

48
00:03:18,960 --> 00:03:22,334
the weaknesses at scale due to the lack of a good

49
00:03:22,372 --> 00:03:26,794
CI CD pipeline of chaos integration in the development

50
00:03:26,842 --> 00:03:30,094
stage and also failing to effectively measure the

51
00:03:30,132 --> 00:03:33,278
impact of the faults automatically at scale,

52
00:03:33,374 --> 00:03:37,122
it becomes difficult to assess the resilience of any

53
00:03:37,176 --> 00:03:40,260
application. To better understand it,

54
00:03:40,710 --> 00:03:44,366
consider this your applications, now being cloud native,

55
00:03:44,478 --> 00:03:47,814
stand atop a plethora of other services that determine its

56
00:03:47,852 --> 00:03:51,110
functioning and resiliency. You have your

57
00:03:51,180 --> 00:03:54,214
application dependencies, then the other cloud

58
00:03:54,252 --> 00:03:57,618
native services provisioning the underlying infrastructure,

59
00:03:57,794 --> 00:04:01,658
the Kubernetes services itself, and lastly the platform

60
00:04:01,744 --> 00:04:05,434
on which your application is deployed. Failure in

61
00:04:05,472 --> 00:04:09,274
any one of these services can cause your entire application to

62
00:04:09,312 --> 00:04:12,414
not be able to cope up. The problem is only

63
00:04:12,452 --> 00:04:16,122
accentuated as more amount of code is now shipped

64
00:04:16,186 --> 00:04:19,770
more frequently at a weekly or even shorter cadence

65
00:04:19,930 --> 00:04:23,970
which is expected to run in a multiple different environments.

66
00:04:24,630 --> 00:04:28,514
This unpredictability of the application behavior is

67
00:04:28,552 --> 00:04:31,454
the prime cause for service outages,

68
00:04:31,582 --> 00:04:34,894
since there is no reliable way to know that how our

69
00:04:34,952 --> 00:04:39,266
application will behave when subjugated to an unanticipated

70
00:04:39,378 --> 00:04:43,382
situation. Therefore, chaos engineering is the way to go

71
00:04:43,436 --> 00:04:47,138
for all those enterprises who want to prioritize

72
00:04:47,234 --> 00:04:50,620
resiliency and reduce downtimes for their customers.

73
00:04:51,150 --> 00:04:54,506
The key to successfully practicing chaos engineering is

74
00:04:54,528 --> 00:04:57,946
to understand the complexity involved in your system

75
00:04:58,048 --> 00:05:01,866
through realistic experiments and hypothesis conditions,

76
00:05:01,978 --> 00:05:06,382
and then slowly scaling them up so that all

77
00:05:06,436 --> 00:05:09,146
parts of your application can be assessed.

78
00:05:09,338 --> 00:05:12,990
However, what does a good chaos engineering practice look like

79
00:05:13,060 --> 00:05:16,642
and how can you implement one? As far as the general

80
00:05:16,696 --> 00:05:19,966
best practices are, Chaos engineering is a culture oriented

81
00:05:19,998 --> 00:05:23,982
approach which finds its place as part of the DevOps practices

82
00:05:24,126 --> 00:05:27,190
and hence developers and SRE should work together

83
00:05:27,340 --> 00:05:31,362
for the best efforts. While developers should run chaos

84
00:05:31,426 --> 00:05:34,754
experiments from an early stage in the development and slowly

85
00:05:34,802 --> 00:05:39,262
scale up their test to cover all the different kind of chaos scenarios.

86
00:05:39,426 --> 00:05:43,302
SRE should focus on how to make chaos engineering practices

87
00:05:43,366 --> 00:05:46,454
scalable enough to run into their CI CD pipelines,

88
00:05:46,582 --> 00:05:50,134
as well as execute the set tests within the staging and

89
00:05:50,192 --> 00:05:52,270
eventually in the production environment.

90
00:05:52,930 --> 00:05:56,318
Also, it is paramount to have a robust set of

91
00:05:56,324 --> 00:06:00,266
chaos experiments that can cover all the different types of failures

92
00:06:00,378 --> 00:06:03,200
that might potentially affect the application.

93
00:06:03,890 --> 00:06:07,646
Lastly, you need a good observability to assess the impact

94
00:06:07,678 --> 00:06:11,246
of the chaos throughout the system, and hence your chaos engineering

95
00:06:11,278 --> 00:06:14,594
tool should provide with enough insights that can help

96
00:06:14,632 --> 00:06:17,990
you understand if the application is deviating from its steady state

97
00:06:18,060 --> 00:06:20,550
in an unanticipated manner.

98
00:06:21,050 --> 00:06:24,578
So how do you implement a great chaos engineering

99
00:06:24,594 --> 00:06:28,406
practice within your organization? Well, harness Chaos engineering

100
00:06:28,438 --> 00:06:32,730
can help you get there. Harness Chaos Engineering tackles the

101
00:06:32,800 --> 00:06:36,154
problem of providing a streamlined platform and

102
00:06:36,272 --> 00:06:39,562
provides powerful features which helps you get started

103
00:06:39,616 --> 00:06:43,962
with harness Chaos engineering. It provides simplified experiment

104
00:06:44,026 --> 00:06:47,706
creation, so instead of writing complex scripts. Honest Chaos

105
00:06:47,738 --> 00:06:51,626
engineering offers a declarative approach, allowing you to define experiments

106
00:06:51,658 --> 00:06:54,882
in code version, control them, and easily integrate them

107
00:06:54,936 --> 00:06:58,654
into your workflow. It probes you an extensive fault

108
00:06:58,702 --> 00:07:01,390
library. So whether you are targeting kubernetes,

109
00:07:01,470 --> 00:07:04,830
AWS, Azure, GCP, VMware, Linux,

110
00:07:04,910 --> 00:07:08,610
or even your custom services, harness Chaos engineering provides

111
00:07:08,690 --> 00:07:12,518
with a rich library of prebuilt faults, on top of which you

112
00:07:12,524 --> 00:07:15,974
can create your own chaos experiments. You can

113
00:07:16,012 --> 00:07:20,726
select, customize and compliance these faults to create realistic scenarios

114
00:07:20,838 --> 00:07:23,834
that can stress your system in various ways.

115
00:07:24,032 --> 00:07:27,370
Also, you can leverage the chaos hubs to store and provide

116
00:07:27,440 --> 00:07:31,370
access to these faults and experiments throughout your organization.

117
00:07:32,050 --> 00:07:35,914
It also provides you with real time monitoring and metrics, wherein harness

118
00:07:35,962 --> 00:07:40,154
chaos engineering leverages Prometheus to provide real time insights

119
00:07:40,202 --> 00:07:43,826
into your system's behavior. During the experiment runs. You can

120
00:07:43,848 --> 00:07:47,374
visualize the metrics, correlate the impacts, and gain deeper

121
00:07:47,422 --> 00:07:50,370
understanding of your chaos effectiveness.

122
00:07:50,950 --> 00:07:54,414
Automate your experimentation with harness chaos engineering

123
00:07:54,462 --> 00:07:57,858
so once you have defined your experiments,

124
00:07:57,954 --> 00:08:01,762
it's time to automate them. You can schedule regular runs,

125
00:08:01,826 --> 00:08:05,570
integrate them with your CI CD pipelines, and continuously assess

126
00:08:05,650 --> 00:08:09,778
your system's resilience. The proactive approach helps

127
00:08:09,794 --> 00:08:13,318
you identify the weaknesses before they can cost downtimes.

128
00:08:13,494 --> 00:08:17,654
Lastly, harness Chaos Engineering also probes you with advanced

129
00:08:17,702 --> 00:08:21,710
features that provide you with

130
00:08:21,860 --> 00:08:25,246
resilience. Scoring private Chaos Hub security

131
00:08:25,348 --> 00:08:29,674
chaos faults, tailor your chaos experiments and gain deeper understanding

132
00:08:29,722 --> 00:08:32,850
within your system's vulnerability. In short,

133
00:08:32,920 --> 00:08:36,514
harness chaos engineering provides you a plethora of benefits,

134
00:08:36,632 --> 00:08:40,094
including reduced downtimes. That is, by proactively

135
00:08:40,142 --> 00:08:43,570
identifying the weaknesses, you can fix them before

136
00:08:43,640 --> 00:08:47,334
they can cause outages, which can lead to

137
00:08:47,452 --> 00:08:49,880
improved uptime and user experience.

138
00:08:50,730 --> 00:08:54,534
It also helps you with faster recovery in the sense

139
00:08:54,572 --> 00:08:58,186
that harness chaos engineering helps you build systems that

140
00:08:58,208 --> 00:09:02,154
can automatically recover from failures, and therefore it

141
00:09:02,192 --> 00:09:06,218
can help you minimize the downtime and impact on your business.

142
00:09:06,384 --> 00:09:10,146
It can also aid you with the validation and optimization

143
00:09:10,278 --> 00:09:13,546
of your disaster recovery setup. Lastly,

144
00:09:13,658 --> 00:09:18,362
it helps you reduce the cost by avoiding unplanned downtimes,

145
00:09:18,506 --> 00:09:21,758
which helps you in cost savings as

146
00:09:21,844 --> 00:09:25,166
any of your resources aren't waste on the recovery

147
00:09:25,198 --> 00:09:28,242
efforts. So it's as

148
00:09:28,296 --> 00:09:31,966
simple as getting started with harness chaos engineering as choosing

149
00:09:31,998 --> 00:09:35,774
your platform of choice. That is the SaaS or on premise. The SaaS

150
00:09:35,822 --> 00:09:39,414
platform is deployed on cloud, while on premise platform can be

151
00:09:39,452 --> 00:09:43,154
deployed into your environment. Once you have selected

152
00:09:43,202 --> 00:09:47,094
your environment, your platform of your choice, you can pick

153
00:09:47,132 --> 00:09:49,910
an experiment and depending on that experiment,

154
00:09:49,990 --> 00:09:53,194
be it a Kubernetes, AWS, GCP, or any

155
00:09:53,232 --> 00:09:56,838
other type of chaos engineering fault,

156
00:09:57,014 --> 00:10:00,086
you can select the blast radius to which you want to

157
00:10:00,128 --> 00:10:03,162
affect your target application and your target environment,

158
00:10:03,306 --> 00:10:07,498
and then choose to execute the chaos experiments.

159
00:10:07,674 --> 00:10:11,374
Upon executing any chaos experiments, you'll be able to see

160
00:10:11,492 --> 00:10:15,730
and observe the chaos impact, as well as measure the critical metrics

161
00:10:16,710 --> 00:10:20,754
which give you an insight into what is happening throughout your system

162
00:10:20,872 --> 00:10:24,386
when an chaos experiment is run. Finally, when you

163
00:10:24,408 --> 00:10:28,002
have found enough confidence with your chaos

164
00:10:28,066 --> 00:10:32,194
experiment runs, you can automate with the CI CD

165
00:10:32,242 --> 00:10:35,682
tooling of your choice. Harness Chaos engineering

166
00:10:35,746 --> 00:10:39,654
experiments integrate out of the box with Harness CI

167
00:10:39,702 --> 00:10:43,798
and CD. However, you can also leverage the APIs

168
00:10:43,974 --> 00:10:48,954
provided by harness chaos Engineering to integrate it with any

169
00:10:48,992 --> 00:10:51,150
CI CD tooling of your choice.

170
00:10:51,490 --> 00:10:55,454
So observing impact of the chaos at

171
00:10:55,492 --> 00:10:59,066
scale can be difficult, especially if you are performing

172
00:10:59,098 --> 00:11:02,350
chaos experiments in CI CD pipelines.

173
00:11:02,950 --> 00:11:06,062
To overcome. This harness provides resilience

174
00:11:06,126 --> 00:11:09,940
probes. Let's hear from Cheyenne how they work.

175
00:11:10,630 --> 00:11:13,986
Thanks Lanjan, for talking about chaos engineering, its practices, and how

176
00:11:14,008 --> 00:11:17,838
slas are important in this practice. As mentioned, I'm Cheyenne.

177
00:11:17,854 --> 00:11:21,574
I'll be talking about resilience probe and giving you guys a hands on demo as

178
00:11:21,612 --> 00:11:25,046
well on how you can practically approach probes and how to use them in your

179
00:11:25,068 --> 00:11:28,534
regular day to day applications. So before jumping right

180
00:11:28,572 --> 00:11:32,306
into the actual hands on approach, let's first understand what

181
00:11:32,348 --> 00:11:35,782
probes are. What is a resilience probe? What is this term that we are coining?

182
00:11:35,926 --> 00:11:39,126
So, resilience probes are nothing but reusable pluggable checks

183
00:11:39,158 --> 00:11:43,774
that could be used in your experiment. So let's say you have an

184
00:11:43,812 --> 00:11:48,254
application where you want to do some kind of a query or some

185
00:11:48,292 --> 00:11:52,442
monitoring parameters that you want to check or assert based on certain criteria,

186
00:11:52,506 --> 00:11:55,726
so you can put in a probe in that specific fault. So what that

187
00:11:55,748 --> 00:11:59,534
will do is go and query or go and check certain aspect

188
00:11:59,582 --> 00:12:03,646
that you have configured the probes on and then return some values

189
00:12:03,678 --> 00:12:07,730
based on which you can do your chaos validation. So to understand this in

190
00:12:07,800 --> 00:12:10,966
further depth, we'll of course take a deeper look into it. But yeah, that's the

191
00:12:10,988 --> 00:12:14,194
general gist of it. So it's basically a write once, use anywhere

192
00:12:14,242 --> 00:12:17,718
kind of a paradigm, which means you just create the probe once and then

193
00:12:17,804 --> 00:12:21,626
you are free to use it in as many chaos experiments or faults you

194
00:12:21,648 --> 00:12:25,670
want to attach it to. That's in brief,

195
00:12:25,830 --> 00:12:29,594
what is a resilience probe? Now, how to use this probe? So you basically

196
00:12:29,632 --> 00:12:32,474
have to configure a resilience probe globally. For example,

197
00:12:32,512 --> 00:12:36,382
let's configure a health check probe which checks the health of my application,

198
00:12:36,516 --> 00:12:40,206
of my pod, of my container or anything. And once I do kind

199
00:12:40,228 --> 00:12:43,594
of generalize and create this health check probe, I have to add the necessary

200
00:12:43,642 --> 00:12:47,026
probes to my specific faults and then observe the impact that

201
00:12:47,048 --> 00:12:50,466
it is causing to my specific experiment. So what

202
00:12:50,488 --> 00:12:54,654
are the different types of probes we have? Right now we have two different infrastructure

203
00:12:54,702 --> 00:12:58,574
based probes. So for Kubernetes infrastructure we have HTTP CMD kubernetes

204
00:12:58,622 --> 00:13:02,086
from ACS, Datadoc, Dynatrace and SLO as of today. And for

205
00:13:02,108 --> 00:13:05,830
the Linux one we have support for HTTP CMD datadog and dynatrace.

206
00:13:06,490 --> 00:13:10,594
So what are the typical use cases that you would normally see for probes?

207
00:13:10,642 --> 00:13:13,546
And this is not definitely an exhaustive list, but yeah, this is just something we

208
00:13:13,568 --> 00:13:17,402
came up with. So some of the use cases would be to query health

209
00:13:17,456 --> 00:13:20,666
or downstream uris to execute user defined health check functions or

210
00:13:20,688 --> 00:13:24,462
user defined any functions for that matter. You can perform crud operations in

211
00:13:24,516 --> 00:13:28,586
your custom Kubernetes sources definitions. You can execute promql

212
00:13:28,618 --> 00:13:32,378
queries using Prometheus probes, or you can validate

213
00:13:32,474 --> 00:13:36,334
your error budget using the SLO probes. You can also do

214
00:13:36,372 --> 00:13:40,178
exit and entry criteria check with dynamics probes. So there are multiple ways

215
00:13:40,264 --> 00:13:44,370
you can configure and use a probe in your specific application. Yeah, this is just

216
00:13:44,440 --> 00:13:48,326
as I mentioned, not an exhaustive list. So there are different modes to how you

217
00:13:48,348 --> 00:13:51,446
might want to execute these probes. And this

218
00:13:51,468 --> 00:13:54,642
is dependent on what behavior you are trying to achieve.

219
00:13:54,706 --> 00:13:57,782
So for example, SOT is a start of test,

220
00:13:57,836 --> 00:14:01,334
EOT is the end of test. So if you want to execute your probes just

221
00:14:01,372 --> 00:14:04,522
when the chaos execution hasn't started or is about to start,

222
00:14:04,576 --> 00:14:07,910
you want to execute before that. So you can use the SOT mode for EOT

223
00:14:07,990 --> 00:14:11,306
after your chaos finishes it will basically do the assertion on chaos is

224
00:14:11,328 --> 00:14:14,874
when the chaos is happening, and continuous is throughout the entire chaos execution

225
00:14:14,922 --> 00:14:18,494
flow and edge is actually before and after your chaos is about

226
00:14:18,532 --> 00:14:22,378
to happen. So before chaos happens it runs the assertion,

227
00:14:22,394 --> 00:14:25,906
and after chaos finishes it runs another assertion. So yeah, these are different

228
00:14:26,008 --> 00:14:28,900
modes that are available for probes as of today.

229
00:14:29,430 --> 00:14:33,346
Now let's jump right into the hands on them. So now

230
00:14:33,368 --> 00:14:36,610
I'm in the harness platform. As you can see in the URL, it's actually app

231
00:14:36,680 --> 00:14:40,398
harness IO. So what it would look like normally is something like

232
00:14:40,424 --> 00:14:43,574
this. So you might have to sign in, or if you're new you can click

233
00:14:43,612 --> 00:14:47,574
on sign up and you can create an account. You can use social sign in

234
00:14:47,612 --> 00:14:51,126
as well. Depends on your choice. And once you are logged in you would

235
00:14:51,148 --> 00:14:54,230
definitely get a free trial as well as some free applications,

236
00:14:54,310 --> 00:14:57,466
free to use modules which you can give a try. And of course you have

237
00:14:57,488 --> 00:15:00,490
the free trial, so definitely go ahead and check it out.

238
00:15:00,640 --> 00:15:04,154
So once you're inside, you would see all these different modules.

239
00:15:04,202 --> 00:15:07,886
You can quickly navigate to the chaos module and then you can create a project

240
00:15:08,068 --> 00:15:11,806
up just for testing or just to explore. So I already have a

241
00:15:11,828 --> 00:15:15,294
project in here selected, and in this I've gone

242
00:15:15,332 --> 00:15:18,766
to the resilience probe tabs. This is where I can see all my different resilience

243
00:15:18,798 --> 00:15:21,874
probes. Currently I've filtered it via the conf 42 tag. That's why you're only

244
00:15:21,912 --> 00:15:25,954
seeing the four probes that I've pre created already 2

245
00:15:25,992 --> 00:15:29,666
hours one 2 hours ago. So these are how the probes

246
00:15:29,698 --> 00:15:33,286
would look like and irrespective of which platform you're in. So let's say you're not

247
00:15:33,308 --> 00:15:36,822
trying this on harness. These features, functionalities are also available

248
00:15:36,876 --> 00:15:40,866
in the open source litmus, so it does not matter, it's a platform agnostic.

249
00:15:40,898 --> 00:15:44,298
You can also take this, you'll also get the same level of features in

250
00:15:44,304 --> 00:15:47,770
the open source version as well. So moving forward.

251
00:15:47,840 --> 00:15:51,162
So these are some of the probes I've pre configured. So there's a

252
00:15:51,296 --> 00:15:54,410
Prometheus probe, there's an HTTP probe, a CMD probe,

253
00:15:54,570 --> 00:15:57,406
and one kts probe that I've also configured. But yeah,

254
00:15:57,428 --> 00:16:00,506
for the demo we'll be mostly using the three probes,

255
00:16:00,538 --> 00:16:04,114
and we'll be trying to assert certain criteria and

256
00:16:04,152 --> 00:16:07,842
validate our microservice application, which is called

257
00:16:07,896 --> 00:16:11,714
bootycap. And we'll be doing some probe validations on top

258
00:16:11,752 --> 00:16:15,614
of that application. So just to give you a brief setup

259
00:16:15,662 --> 00:16:19,618
tour of what I have, I have a GKE cluster

260
00:16:19,714 --> 00:16:23,922
running in which I have monitoring setup with Prometheus and grafana.

261
00:16:23,986 --> 00:16:27,958
I have my boutique application set up. This is the microservice demo application that

262
00:16:27,964 --> 00:16:31,606
I'm going to use and do chaos on. And this is the infrastructure setup

263
00:16:31,638 --> 00:16:34,778
that I have for harness. So harness requires you

264
00:16:34,784 --> 00:16:38,134
to have an environment in where you can deploy your chaos

265
00:16:38,182 --> 00:16:41,734
infrastructure. So this is that infrastructure that I've connected, which is nothing but the GKE

266
00:16:41,782 --> 00:16:44,906
cluster. Cool. All right, now let's

267
00:16:44,938 --> 00:16:48,190
move on and actually see the application. So this is the online booty application.

268
00:16:48,260 --> 00:16:51,614
As you can see, there are multiple items which I can select.

269
00:16:51,732 --> 00:16:54,730
I can add things to the cart. Let's say I want to add some sunglasses,

270
00:16:54,810 --> 00:16:57,918
I can add two of them to the cart. Once I do that, this is

271
00:16:57,924 --> 00:16:59,918
my cart, so I can go to my cart and I can see that the

272
00:16:59,924 --> 00:17:03,566
cart is functional. So if you go to the microservice list, actually you would

273
00:17:03,588 --> 00:17:07,240
see that there's a service called cart service. This is what's responsible for

274
00:17:07,850 --> 00:17:11,286
handling all the cartilage activity. So what

275
00:17:11,308 --> 00:17:14,546
we'll do is we'll actually try to break this service. We'll do a simple pod

276
00:17:14,578 --> 00:17:19,686
delete, but on this specific service and we'll turn

277
00:17:19,708 --> 00:17:23,226
it down, we'll kind of disrupt this service. But this

278
00:17:23,248 --> 00:17:26,186
is just a very simple application. But what we want to do is we want

279
00:17:26,208 --> 00:17:29,370
to do all these different kind of validations on top of that disruption.

280
00:17:30,190 --> 00:17:33,130
For example, let's take the HTTP probes for now.

281
00:17:33,200 --> 00:17:36,314
So if we go over to the HTTP probe and see the probe configuration,

282
00:17:36,442 --> 00:17:40,074
we can see that we have certain set of timeouts, attempt interval

283
00:17:40,122 --> 00:17:43,434
and initial delay. So we want it to be start after a certain delay,

284
00:17:43,482 --> 00:17:47,054
you want it to have certain interval if it fails, like how many

285
00:17:47,092 --> 00:17:50,110
times you want it to attempt again and again if the first one doesn't succeed.

286
00:17:50,190 --> 00:17:52,962
So these kind of things and what we are doing in the probe is actually

287
00:17:53,016 --> 00:17:56,546
the probe details where you'll get all the information, which is

288
00:17:56,568 --> 00:18:00,242
basically we are trying to check or connect to this specific URI,

289
00:18:00,306 --> 00:18:04,214
UrI which is the FQDM link for the specific front

290
00:18:04,252 --> 00:18:07,554
end booty cap. And we are checking if this is accessible.

291
00:18:07,602 --> 00:18:10,970
So if it's actually returning the response code of 200 or not,

292
00:18:11,040 --> 00:18:14,074
we are checking if it's actually live, if this FQDN link

293
00:18:14,112 --> 00:18:17,930
is actually visible and we can navigate to that specific port

294
00:18:18,000 --> 00:18:21,100
or not. Now coming back to the probe screen again.

295
00:18:22,030 --> 00:18:24,926
Now a lot of the other probes would also be listed because we just got

296
00:18:24,948 --> 00:18:28,046
rid of the filter, but yeah, so if we check

297
00:18:28,068 --> 00:18:31,562
the CMD probe, what it's actually doing is if we go to the configuration,

298
00:18:31,626 --> 00:18:35,770
we'll see that it's trying to do a kubectl get pod in the boutique namespace.

299
00:18:35,850 --> 00:18:39,982
So if you see over here, this is in the boutique namespace,

300
00:18:40,046 --> 00:18:43,362
it's trying to check if in the boutique namespace we want to grab the card

301
00:18:43,416 --> 00:18:46,974
service, which is the microservice we want to target and drop

302
00:18:47,102 --> 00:18:50,486
if it's actually in the running state, and if so, what's the count of it.

303
00:18:50,508 --> 00:18:54,358
So we want at least one card service to always be present,

304
00:18:54,444 --> 00:18:57,286
that is in the running state, in the healthy state. So we are kind of

305
00:18:57,308 --> 00:19:01,226
asserting in the comparator as the integer criteria should be greater than

306
00:19:01,328 --> 00:19:05,082
zero. So there should always be at least one card service.

307
00:19:05,216 --> 00:19:08,506
So that is what this specific probe is doing.

308
00:19:08,608 --> 00:19:12,110
And the third is the Prometheus probe which is

309
00:19:12,260 --> 00:19:16,046
asserting, yeah, it is asserting on

310
00:19:16,068 --> 00:19:19,406
the specific Prometheus endpoint. It's checking the

311
00:19:19,428 --> 00:19:23,386
average over time. It's checking the prom query, the promql query actually of the probe

312
00:19:23,418 --> 00:19:26,734
success percent. And it's also checking that

313
00:19:26,852 --> 00:19:30,078
according to our evaluation criteria, this should be greater

314
00:19:30,094 --> 00:19:33,554
than equal to 90. So we are saying that if it's the probe success percent

315
00:19:33,592 --> 00:19:39,202
is greater than equal to 90 only, then consider this as a resilient fault.

316
00:19:39,346 --> 00:19:43,074
So that's our assertion, that's our hypothesis

317
00:19:43,122 --> 00:19:46,518
of what we want to do to configure any new resilience probe. You can go

318
00:19:46,524 --> 00:19:50,226
over to the plus new probe and you can choose between which infra

319
00:19:50,258 --> 00:19:53,578
type you want, Kubernetes or Linux. So for Kubernetes you can

320
00:19:53,584 --> 00:19:56,950
go ahead and select any of the probes types. Let's say HTTP,

321
00:19:57,110 --> 00:20:00,394
you can give it a name. So this is the unique name. So once you

322
00:20:00,432 --> 00:20:03,642
assign it you can't really get rid of it. So be mindful about that.

323
00:20:03,696 --> 00:20:06,766
You can force delete it of course. But yeah, so let me just

324
00:20:06,788 --> 00:20:10,538
do HTTP probe one, one one or something you can configure,

325
00:20:10,634 --> 00:20:13,822
you can do next, you can set up the timeout for this one,

326
00:20:13,956 --> 00:20:17,998
for example, something like this. And if I

327
00:20:18,004 --> 00:20:21,106
go next, this is where you can give your probe details so similar things.

328
00:20:21,128 --> 00:20:23,746
So you can choose the get or post method. If I do post, you can

329
00:20:23,768 --> 00:20:27,506
choose the HTTP criteria if you want to compare the response code or

330
00:20:27,528 --> 00:20:30,598
response body. So yeah, this is just an example of how you can go ahead

331
00:20:30,604 --> 00:20:34,466
and configure whatever probes you want. And once you do that they'll be shown

332
00:20:34,498 --> 00:20:38,034
up like this. Now let's come to the scheduling

333
00:20:38,082 --> 00:20:41,382
part and actually let's try and run an experiment and see the observation,

334
00:20:41,446 --> 00:20:44,598
see how it's going. So let's create a new experiment.

335
00:20:44,694 --> 00:20:49,034
I'll call it boutique app conf

336
00:20:49,152 --> 00:20:52,442
42 and I'll select

337
00:20:52,496 --> 00:20:56,266
in Kubernetes infrastructure type. So in here I would select

338
00:20:56,448 --> 00:20:59,566
the conf 42 intra that I created and I'll just

339
00:20:59,588 --> 00:21:03,214
apply. So you have few options. You can upload your own AML, you can

340
00:21:03,252 --> 00:21:06,366
create your template or you can just start from a blank canvas. If I start

341
00:21:06,388 --> 00:21:10,174
from a blank canvas I would just filter from the chaos hub

342
00:21:10,222 --> 00:21:12,206
what I want to do. So in this case I want to do a pod

343
00:21:12,238 --> 00:21:15,714
delete. If I do select pod delete, I would be given

344
00:21:15,752 --> 00:21:19,154
certain choices of where you want to do the pod delete. So in this case

345
00:21:19,192 --> 00:21:22,566
I want to select the app namespace, which is the boutique namespace where my app

346
00:21:22,588 --> 00:21:26,050
is currently present, I'll select the kind. The kind is nothing but a deployment

347
00:21:26,130 --> 00:21:29,090
and it is nothing but the card service. This is the label,

348
00:21:29,170 --> 00:21:32,538
these are all the labels that are present in my specific boutique namespace, but I

349
00:21:32,544 --> 00:21:35,500
just want to target the card namespace, the card service.

350
00:21:36,430 --> 00:21:40,102
So now that I've selected this, I can go ahead and tune

351
00:21:40,166 --> 00:21:43,066
the faults if I want to. I don't really have a use case for now,

352
00:21:43,088 --> 00:21:46,206
so I'll just leave it, let it be. And this is the section where

353
00:21:46,228 --> 00:21:49,918
you actually add the resilience probes. So in the probe section, currently I

354
00:21:49,924 --> 00:21:53,518
don't have any probes added to my specific fault, but they are configured in the

355
00:21:53,524 --> 00:21:58,034
resilience probe section. So what I'll do is select and all

356
00:21:58,072 --> 00:22:01,618
these different probes will which are eligible to be added for your specific

357
00:22:01,704 --> 00:22:05,042
experiment. For your specific faults I would just select

358
00:22:05,096 --> 00:22:08,030
the HTTP one add this to fault.

359
00:22:08,190 --> 00:22:11,974
So for this one I would kind of want sot check in the sense like

360
00:22:12,012 --> 00:22:15,606
whenever the chaos starts. Before that I want to do an assertion and

361
00:22:15,628 --> 00:22:19,446
check if this is actually live or not.

362
00:22:19,548 --> 00:22:22,918
So I want to apply that. Secondly, I want to add one more probe,

363
00:22:22,934 --> 00:22:26,426
which is the CMD probe. So the CMD probe is

364
00:22:26,448 --> 00:22:30,186
doing nothing but checking if the card service is in running state. So we want

365
00:22:30,208 --> 00:22:33,594
to kind of assert this before the start

366
00:22:33,632 --> 00:22:36,766
of the chaos and after the start of the chaos. So what it means is

367
00:22:36,788 --> 00:22:40,174
before start it was already running and then chaos happened. So it might have gone

368
00:22:40,212 --> 00:22:43,374
down, but after the chaos finished, whether it came up back again or not.

369
00:22:43,412 --> 00:22:45,658
So that kind of a check I can do with the CMD one. So I'll

370
00:22:45,674 --> 00:22:48,882
just run it in the edge mode, I'll apply that and

371
00:22:48,936 --> 00:22:52,018
next I'll do the Prometheus one. So for the

372
00:22:52,024 --> 00:22:55,586
Prometheus we are again checking for the probes success percentage. So for this I would

373
00:22:55,608 --> 00:22:59,662
want it to go in a continuous mode. So keep checking forever,

374
00:22:59,726 --> 00:23:03,362
like within the certain interval, polling interval that you specify.

375
00:23:03,426 --> 00:23:06,790
But yeah, just keep doing that so that I get a constant verification.

376
00:23:07,450 --> 00:23:10,786
So now I apply the changes. Now if you look at the yaml,

377
00:23:10,898 --> 00:23:14,426
this might scare you, but it's a big yaml. Now where are

378
00:23:14,448 --> 00:23:17,546
the probes? So for the resilience probes we kind of add it here in the

379
00:23:17,568 --> 00:23:21,114
annotation. Now since we have certain

380
00:23:21,152 --> 00:23:24,598
probes configured in the hub, so things like the health check might pop up,

381
00:23:24,704 --> 00:23:27,866
which is another probe right here. But this is not considered a resilience probes.

382
00:23:27,898 --> 00:23:31,374
This is something we do for backward compatibility. But yeah,

383
00:23:31,492 --> 00:23:35,006
you can also go ahead and remove it, it should not affect your application or

384
00:23:35,028 --> 00:23:38,466
your fault. But yeah, so these are the three probes we

385
00:23:38,488 --> 00:23:41,714
have added and if you want a little more information on where you can add

386
00:23:41,752 --> 00:23:45,006
probes. So in the documentation for developer

387
00:23:45,038 --> 00:23:48,078
harness IO, you can go to any of the probes, let's say CMD probe,

388
00:23:48,174 --> 00:23:51,766
you can see that this is the exact place where you

389
00:23:51,788 --> 00:23:55,446
have to define your probes. This is for the old legacy way. So if you

390
00:23:55,468 --> 00:23:58,786
want more information that is. But yeah, this is not something you're

391
00:23:58,818 --> 00:24:01,974
doing by hand. This is already pre created if you're using the UI using Chaos

392
00:24:02,022 --> 00:24:05,354
studio, so you don't have to worry too much about it, I think so,

393
00:24:05,392 --> 00:24:08,700
yeah. Cool. So that's that. Now let's save,

394
00:24:09,630 --> 00:24:13,434
give it a minute. Yeah, and let's just run it. So once we run

395
00:24:13,472 --> 00:24:16,718
it, we are checking if certain rules are

396
00:24:16,724 --> 00:24:19,898
met or not, then we are actually installing the chaos faults and then we'll

397
00:24:19,914 --> 00:24:23,822
actually do the pod delete. So if I go back to my application,

398
00:24:23,956 --> 00:24:27,860
to my mic service, you can see that this guy card service

399
00:24:28,390 --> 00:24:32,082
age is 70 minutes. It was running for 70 minutes

400
00:24:32,136 --> 00:24:36,094
as of now. Now when the chaos happens this will actually terminate.

401
00:24:36,142 --> 00:24:39,766
So the age will be much, much in seconds, I think.

402
00:24:39,868 --> 00:24:43,286
So we'll see that as well. But yeah, as of now

403
00:24:43,308 --> 00:24:46,758
you can see that some things would pop up here, like this guy running

404
00:24:46,844 --> 00:24:50,360
for 2 seconds, the boutique app just started

405
00:24:51,150 --> 00:24:54,620
and if I go back to my application currently everything works

406
00:24:55,390 --> 00:24:58,538
well and good, but once the chaos actually happens and I click on

407
00:24:58,544 --> 00:25:02,842
the card, things should start breaking. So for the monitoring

408
00:25:02,906 --> 00:25:05,834
I actually have set up the Grafana and Prometheus integration.

409
00:25:05,962 --> 00:25:08,400
So you can see that.

410
00:25:11,410 --> 00:25:15,662
So you can see that the chaos injecting is actually starting based

411
00:25:15,716 --> 00:25:18,962
on the annotations, as you can see in the bottom and this is the card

412
00:25:19,016 --> 00:25:22,178
Qps that's going to be affected because the card service is the one we are

413
00:25:22,184 --> 00:25:25,442
targeting. So you can see the QPS go down, which is card service is actually

414
00:25:25,496 --> 00:25:28,994
starting to get affected. And if you come over

415
00:25:29,032 --> 00:25:32,646
and check the logs actually of this, you would see the probe logs as well.

416
00:25:32,828 --> 00:25:35,714
So if I go down you can see the health check probe has been passed.

417
00:25:35,762 --> 00:25:39,506
This is the default legacy one. You can see the Conf 42 HTTP probe

418
00:25:39,538 --> 00:25:43,318
has also been passed. Maybe I can zoom in. Yeah, the Conf 42

419
00:25:43,324 --> 00:25:46,758
HTTP probe has also been passed, which is just doing the assertion. So this

420
00:25:46,764 --> 00:25:50,074
is an sot thing. So before the chaos started, it did this assertion

421
00:25:50,162 --> 00:25:53,760
now the chaos is going on, and you can see certain things like the

422
00:25:54,610 --> 00:25:58,170
CMD, which would be before and after the chaos

423
00:25:58,250 --> 00:26:01,406
pop up. So in here you would see the CMD thing. So the

424
00:26:01,428 --> 00:26:04,980
confidant to CMD probe has been passed because its expected value.

425
00:26:05,510 --> 00:26:08,706
Where is it? Yeah, so the

426
00:26:08,728 --> 00:26:11,986
expected value is zero, which is greater than zero, but its

427
00:26:12,008 --> 00:26:15,378
actual value is one. So that means it did receive something. It was in the

428
00:26:15,384 --> 00:26:18,546
running state. So the count is one. And now you can see

429
00:26:18,568 --> 00:26:21,942
the prompt probe, it's actually failing because prom probe's actual value

430
00:26:21,996 --> 00:26:25,346
is 88.33%, whereas it should be equal

431
00:26:25,378 --> 00:26:28,774
to, greater than or equal to 90. So that is the one thing which is

432
00:26:28,812 --> 00:26:32,154
in this case, our specific application is not really our

433
00:26:32,192 --> 00:26:35,558
specific fault. That application is not really that resilient,

434
00:26:35,654 --> 00:26:39,066
but according to our criteria should have been greater than equal to

435
00:26:39,088 --> 00:26:42,986
90. So if your application is resilient, you should

436
00:26:43,008 --> 00:26:45,918
have to configure your application in such a way that it's actually greater than or

437
00:26:45,924 --> 00:26:49,738
equal to 90, so that you can term it as resilient.

438
00:26:49,914 --> 00:26:53,390
Now if we go back to the boutique and click on the cart,

439
00:26:53,890 --> 00:26:56,974
I think it's actually restarted.

440
00:26:57,022 --> 00:27:00,706
Yeah, as you can see. So the cart service is restarted and it's 89

441
00:27:00,808 --> 00:27:04,622
seconds. So that's why you did not see the chaos

442
00:27:04,686 --> 00:27:08,306
in the UI, because it just restarted that quickly. But yeah, so you can

443
00:27:08,328 --> 00:27:11,638
see that this guy did terminate, and because it

444
00:27:11,644 --> 00:27:14,120
terminated and came back up, you can see the age difference.

445
00:27:14,570 --> 00:27:18,920
But yeah, if I go back to my booty gap now, you can see the

446
00:27:19,450 --> 00:27:23,366
fault injection. You can see that the chaos injection is finished

447
00:27:23,478 --> 00:27:26,922
and the annotation has stopped going further. So yeah,

448
00:27:26,976 --> 00:27:30,362
that is just a brief assertion of what I wanted to show you.

449
00:27:30,416 --> 00:27:32,714
And if I come back to the probes section, you can see all the different

450
00:27:32,752 --> 00:27:36,558
probes mentioned here as well. So the HTTP probes passed because

451
00:27:36,644 --> 00:27:39,786
its expected code was 200 and we received 200. The CMD

452
00:27:39,818 --> 00:27:43,134
one passed because we had a value greater than equal to zero, greater than zero,

453
00:27:43,172 --> 00:27:46,666
and its actual value was one. But unfortunately the prom one failed because it received

454
00:27:46,698 --> 00:27:50,078
88, but we wanted greater than 90. So yeah, that's how

455
00:27:50,084 --> 00:27:53,386
we can coin like determine the resilience percentage

456
00:27:53,418 --> 00:27:56,486
of our application and come over, come back to the

457
00:27:56,508 --> 00:27:59,554
experiment. I think this one also finished, so I think we will get the resilience

458
00:27:59,602 --> 00:28:02,214
score for this. Yeah, so it's 75. So you can still make it better,

459
00:28:02,252 --> 00:28:06,038
but it's actually okay. But you should definitely look into

460
00:28:06,124 --> 00:28:09,814
what's wrong with your application and change it. So yeah, that's all about us

461
00:28:09,852 --> 00:28:13,106
from me and Ilanjan on resilience probe and chaos engineering.

462
00:28:13,138 --> 00:28:16,246
So if you have any questions, you can use the social handles to chat with

463
00:28:16,268 --> 00:28:19,460
us. Yeah, I hope you guys enjoyed. Thanks for watching.

