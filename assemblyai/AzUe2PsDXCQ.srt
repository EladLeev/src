1
00:02:15,080 --> 00:02:19,428
Hi. I'm super excited to be here at Conf 42,

2
00:02:19,514 --> 00:02:23,056
talking about four key metrics to measuring your team's

3
00:02:23,088 --> 00:02:26,580
performance. But before I get started, let me

4
00:02:26,650 --> 00:02:30,048
just introduce myself. So my name is Christina.

5
00:02:30,144 --> 00:02:33,812
I'm currently a founding engineer at Cortex. I've been at Cortex for

6
00:02:33,866 --> 00:02:37,096
over a year now. We're a series startup backed

7
00:02:37,128 --> 00:02:40,984
by Sequoia. And basically we're giving organizations

8
00:02:41,032 --> 00:02:45,272
visibility into the status and quality of their microservices

9
00:02:45,416 --> 00:02:48,956
and helping teams drive adoption of best practices so that they

10
00:02:48,978 --> 00:02:52,284
can deliver higher quality software. Before joining Cortex,

11
00:02:52,332 --> 00:02:55,360
I was a front end team lead at Bridgewater Associates

12
00:02:56,020 --> 00:02:59,872
for four years, and I also previously interned at

13
00:02:59,926 --> 00:03:03,472
Microsoft. I studied at the University of Pennsylvania,

14
00:03:03,616 --> 00:03:06,340
and I'm originally from Columbia.

15
00:03:07,880 --> 00:03:12,064
So what I'll be talking about today actually is dorometrics.

16
00:03:12,112 --> 00:03:15,684
So dorometrics are used by DevOps teams to measure their

17
00:03:15,722 --> 00:03:19,352
performance. And the origin of dorometrics is pretty cool.

18
00:03:19,486 --> 00:03:22,996
So it actually comes from an organization called DevOps

19
00:03:23,028 --> 00:03:25,720
research and Assessment, hence Dora.

20
00:03:26,220 --> 00:03:29,896
And it was a team put together by Google to survey thousands

21
00:03:29,928 --> 00:03:33,304
of development teams across many different industries

22
00:03:33,432 --> 00:03:37,276
to understand what a high performing team is, what a

23
00:03:37,298 --> 00:03:40,572
low performing team is, and the key differences between those and

24
00:03:40,626 --> 00:03:43,620
after this, basically, dorametrics,

25
00:03:43,720 --> 00:03:46,770
these four metrics that I'll be talking about came to life.

26
00:03:47,380 --> 00:03:51,164
You might have heard of them before. If any of you are using Circle CI,

27
00:03:51,212 --> 00:03:54,452
I actually just got an email from them two days ago asking

28
00:03:54,506 --> 00:03:58,612
me to fill out a survey because they're partnering with DevOps research

29
00:03:58,666 --> 00:04:01,270
and assessment to put out the next report.

30
00:04:01,960 --> 00:04:06,244
And so the four key metrics are actually lead time for changes deployment

31
00:04:06,292 --> 00:04:09,684
frequency, meantime to recovery and change failure

32
00:04:09,732 --> 00:04:13,108
rate. So the first one I'll

33
00:04:13,124 --> 00:04:16,968
be talking about and digging into is lead time for changes. This is basically

34
00:04:17,054 --> 00:04:20,140
the amount of time between a commit and production.

35
00:04:20,800 --> 00:04:24,172
Different teams choose to measure this differently. You could also choose

36
00:04:24,226 --> 00:04:27,976
to have it be from the time that a ticket gets added

37
00:04:28,008 --> 00:04:31,944
to a sprint and is actually in progress to getting to production,

38
00:04:32,072 --> 00:04:35,776
from a time it's merged to getting to production. That's up to you

39
00:04:35,798 --> 00:04:39,392
and your team to actually decide and figure out what works best for you.

40
00:04:39,526 --> 00:04:43,344
But this is a good indicator of how agile and responsive your team

41
00:04:43,382 --> 00:04:47,812
is. And so from that DevOps report that was put out in 2021,

42
00:04:47,946 --> 00:04:51,204
what they found is that elite performers can have less than can

43
00:04:51,242 --> 00:04:54,872
hour in lead time for changes, and low performers have six

44
00:04:54,926 --> 00:04:58,376
plus months. And so how do you

45
00:04:58,398 --> 00:05:01,640
actually measure and improve lead time for changes?

46
00:05:01,710 --> 00:05:05,416
Right. We've all been in this situation of opening up

47
00:05:05,438 --> 00:05:08,760
a pull request, being ready to review it and seeing that

48
00:05:08,830 --> 00:05:12,696
it has hundreds of file changed and many commits and lines

49
00:05:12,728 --> 00:05:15,356
of code and you just close it back up, you're like not going to do

50
00:05:15,378 --> 00:05:18,732
this right now. And that's something that's actually going to

51
00:05:18,866 --> 00:05:22,236
increase your lead time for change and actually

52
00:05:22,338 --> 00:05:24,830
mean that you're not doing so great.

53
00:05:25,440 --> 00:05:29,084
So this image is actually a perfect example of what can lead

54
00:05:29,122 --> 00:05:32,308
to long lead time for changes. If your team is trying

55
00:05:32,314 --> 00:05:35,716
to make huge changes in one go, it's going to take a lot longer to

56
00:05:35,738 --> 00:05:38,436
review it. It's going to take a lot longer to test it. It's going to

57
00:05:38,458 --> 00:05:42,224
take a lot longer to be confident before you get it to production.

58
00:05:42,352 --> 00:05:45,830
It also means that your code reviews might sit around for too long

59
00:05:46,940 --> 00:05:50,330
and so you want to avoid making huge changes like this.

60
00:05:50,780 --> 00:05:54,804
Another thing that can lead to long lead time for change is potentially changing

61
00:05:54,852 --> 00:05:58,184
requirements. So once you open up a pull request and you're testing it out,

62
00:05:58,222 --> 00:06:01,608
say, with your design team or your product manager or your users,

63
00:06:01,624 --> 00:06:04,844
and they're like, oh, but can you just add this one extra thing

64
00:06:04,882 --> 00:06:08,056
or this other thing? That's where it's on you as an engineer

65
00:06:08,088 --> 00:06:11,644
to say, no, these were my requirements going in and so

66
00:06:11,682 --> 00:06:14,896
this is what my pr is going to do and just

67
00:06:14,918 --> 00:06:18,880
make follow up tickets for those additional tasks. And then

68
00:06:18,950 --> 00:06:22,240
the fourth thing that can lead to long lead time for changes is

69
00:06:22,310 --> 00:06:26,116
insufficient CI CD pipeline. So you could be

70
00:06:26,138 --> 00:06:29,556
merging to production often, but if releasing is

71
00:06:29,578 --> 00:06:33,204
actually really long and painful process, you're probably not releasing that

72
00:06:33,242 --> 00:06:36,712
often. And I'll talk more about that later when I talk about

73
00:06:36,766 --> 00:06:40,104
deployment frequency. So what does short

74
00:06:40,142 --> 00:06:43,448
lead time for changes look like? You want to make sure that everyone

75
00:06:43,534 --> 00:06:46,824
on your team can review prs and that there's not

76
00:06:46,862 --> 00:06:50,204
bottlenecks waiting on that one person to do the review.

77
00:06:50,402 --> 00:06:53,644
You also want to reject tickets that aren't fully fleshed out.

78
00:06:53,682 --> 00:06:57,068
So if something doesn't make sense,

79
00:06:57,154 --> 00:07:00,396
they're like, oh, we'll get the design box to you later. That's going to mean

80
00:07:00,418 --> 00:07:02,920
that your ticket is going to be open for a long time. You're going to

81
00:07:02,930 --> 00:07:06,524
have merge conflicts. You're going to be going back and forth.

82
00:07:06,652 --> 00:07:10,332
It's not worth starting development on it if it's not fully

83
00:07:10,396 --> 00:07:14,276
fleshed out and the requirements aren't clear, and then

84
00:07:14,298 --> 00:07:17,764
you want to again escalate changing requirements. So if

85
00:07:17,802 --> 00:07:22,324
you see that something is taking super long and

86
00:07:22,362 --> 00:07:25,844
people keep adding things to it, you probably want to say, let's pause on

87
00:07:25,882 --> 00:07:29,108
development and make sure that we go back and flesh

88
00:07:29,124 --> 00:07:32,424
out the tickets before starting. And so how

89
00:07:32,462 --> 00:07:35,736
you can actually improve this lead time for changes is by breaking it up

90
00:07:35,758 --> 00:07:39,556
into buckets. So you can see the time that a developer

91
00:07:39,588 --> 00:07:42,956
is taking to work on the change and see if that's what's taking the

92
00:07:42,978 --> 00:07:46,264
longest, or if it's the time that these pull request

93
00:07:46,312 --> 00:07:49,804
is open and these review is done and taking it to test it,

94
00:07:49,842 --> 00:07:53,676
or if it's actually after it's merged and getting to production.

95
00:07:53,788 --> 00:07:57,664
And you can identify which of those three buckets is taking the longest and

96
00:07:57,702 --> 00:08:00,850
focus on increasing that amount of time.

97
00:08:02,420 --> 00:08:06,268
And so a way to measure this, again, is just using Jira.

98
00:08:06,364 --> 00:08:09,616
Look at your tickets, look at how long they've been open, look at the status

99
00:08:09,648 --> 00:08:13,008
of your tickets and how long it's taking to go from column to column.

100
00:08:13,104 --> 00:08:16,472
And you can see sprint over sprint if these numbers are

101
00:08:16,606 --> 00:08:19,656
getting longer, decreasing, or just staying the same.

102
00:08:19,758 --> 00:08:23,352
And again, spot where those bottlenecks are and figure out how your team

103
00:08:23,406 --> 00:08:26,876
can improve on it. So,

104
00:08:26,978 --> 00:08:30,776
moving on to my second metric, which is I touched

105
00:08:30,808 --> 00:08:35,208
briefly upon in lead time for changes, is deployment frequency.

106
00:08:35,304 --> 00:08:39,292
So deployment frequency is how often you ship changes and

107
00:08:39,346 --> 00:08:43,104
how consistent your software delivery is. So you

108
00:08:43,142 --> 00:08:46,930
want to ship as few changes to production as you can.

109
00:08:47,460 --> 00:08:50,844
And so a common misconception is that by shipping to production

110
00:08:50,892 --> 00:08:53,956
more often, you're creating more risk and that you

111
00:08:53,978 --> 00:08:57,700
might have more incidents. But actually it's the opposite

112
00:08:58,360 --> 00:09:01,684
because it's going to be easier to figure out what

113
00:09:01,722 --> 00:09:04,490
caused those incidents by having small changes.

114
00:09:05,420 --> 00:09:08,756
And so you'll be able to actually pinpoint incidents

115
00:09:08,788 --> 00:09:12,344
faster and get your meantime to resolve another metric I'll be

116
00:09:12,382 --> 00:09:15,720
talking about later to decrease.

117
00:09:16,060 --> 00:09:19,688
And so basically, the idea is that if you ship

118
00:09:19,704 --> 00:09:23,148
to production often, you deeply understand the small changes

119
00:09:23,234 --> 00:09:27,004
going into it, and you'll be able to basically

120
00:09:27,122 --> 00:09:30,892
improve upon that. A high deployment frequency

121
00:09:30,956 --> 00:09:34,172
will end up actually reducing your overall risk,

122
00:09:34,236 --> 00:09:38,252
even though you are deploying more often. And it's a useful

123
00:09:38,396 --> 00:09:42,124
to determine when your team is meeting goals for continuous

124
00:09:42,172 --> 00:09:46,240
delivery and that you're actually continuously improving customer experience.

125
00:09:46,390 --> 00:09:50,356
So deployment frequency has an impact on

126
00:09:50,378 --> 00:09:54,724
the end user, right. You're just getting stuff out to them way

127
00:09:54,762 --> 00:09:58,072
more quickly than if you're waiting on

128
00:09:58,126 --> 00:10:01,704
many releases to deploy. And then again, you don't know

129
00:10:01,742 --> 00:10:05,610
how those changes can play with each other, and potentially it can create problems.

130
00:10:06,540 --> 00:10:09,996
So going back to the report put out

131
00:10:10,018 --> 00:10:13,676
in 2021, they basically found that

132
00:10:13,698 --> 00:10:17,864
elite performers deploy multiple times a day and low performers

133
00:10:17,912 --> 00:10:21,310
do it every six months. And so basically,

134
00:10:22,100 --> 00:10:26,032
you want to encourage your engineers and your QA team

135
00:10:26,086 --> 00:10:29,724
once again to work closely together to make sure that you can deploy

136
00:10:29,772 --> 00:10:34,004
often and you want to build out good automated tests so that

137
00:10:34,122 --> 00:10:37,910
you are confident in your releases as you're going through.

138
00:10:38,680 --> 00:10:41,924
And so again, another image that we've all seen

139
00:10:41,962 --> 00:10:45,216
before, it kind of looks like that cascading waterfall.

140
00:10:45,328 --> 00:10:49,224
It kind of reminds me of back when Microsoft Office used

141
00:10:49,262 --> 00:10:52,360
to sell the cds and put out releases every

142
00:10:52,430 --> 00:10:55,976
like three years or so and we would all go and buy the software and

143
00:10:55,998 --> 00:10:59,468
upgrade. We're not in that world anymore. And so we're in a

144
00:10:59,474 --> 00:11:03,660
world where you can be deploying and releasing changes to customers often.

145
00:11:03,810 --> 00:11:07,372
And so you don't really want a waterfall looking thing

146
00:11:07,426 --> 00:11:11,196
like this. So again, low deployment

147
00:11:11,228 --> 00:11:15,244
frequency can be the result of having insufficient CICD

148
00:11:15,292 --> 00:11:19,024
pipelines. It can

149
00:11:19,062 --> 00:11:23,490
be that people areas bottlenecked. So if you only have

150
00:11:24,020 --> 00:11:27,024
say, three engineers who know how to deploy to production,

151
00:11:27,072 --> 00:11:30,516
you're taking up their time, they might not be around, they might be

152
00:11:30,538 --> 00:11:34,468
on vacation. That can mean that you're deploying less often. And then if

153
00:11:34,474 --> 00:11:38,116
you have a lengthy manual testing process, that's also going to mean that you deploy

154
00:11:38,148 --> 00:11:41,972
less frequently because it's going to be taking up your engineers

155
00:11:42,036 --> 00:11:45,492
time. Whereas a high deployment

156
00:11:45,556 --> 00:11:48,892
frequency comes from making it super easy

157
00:11:48,946 --> 00:11:52,716
to release. You want to be shipping each pr to production in

158
00:11:52,738 --> 00:11:56,636
can ideal world on its own so that basically you know exactly

159
00:11:56,738 --> 00:12:00,364
what the change is. And I totally get that. This might not

160
00:12:00,402 --> 00:12:03,936
work for big teams with monolith, but in this case you can

161
00:12:03,958 --> 00:12:07,504
use a technique called release trains where basically you

162
00:12:07,542 --> 00:12:11,536
ship to production in fixed intervals throughout the day. And that can

163
00:12:11,558 --> 00:12:15,636
help also increase your deployment frequency. You want

164
00:12:15,658 --> 00:12:19,296
to make sure that you're setting up good integrated and end time teams

165
00:12:19,328 --> 00:12:23,012
so that you're confident in your deployments and aren't spending a long

166
00:12:23,066 --> 00:12:26,720
time on manually testing each use case and

167
00:12:26,730 --> 00:12:30,036
each application. And you want to make sure that you have good testing

168
00:12:30,068 --> 00:12:33,368
environments with accurate data once again, just so that

169
00:12:33,374 --> 00:12:36,716
you're more confident in these releases. And you really want

170
00:12:36,738 --> 00:12:40,316
to drive a DevOps ethos across your whole team so

171
00:12:40,338 --> 00:12:43,550
that everyone knows that this is how things work.

172
00:12:45,440 --> 00:12:49,144
And so again, just ways to actually measure deployment

173
00:12:49,192 --> 00:12:52,884
frequency. You can look at the number of releases in a sprint.

174
00:12:53,032 --> 00:12:56,432
Everyone has different sprints. I've seen one week, I've seen two weeks,

175
00:12:56,486 --> 00:13:00,332
I've seen three weeks. Whatever it is that your team is doing, just measure

176
00:13:00,396 --> 00:13:03,408
how often are you actually releasing every sprint.

177
00:13:03,504 --> 00:13:06,916
Is your average number once a day or

178
00:13:06,938 --> 00:13:10,244
is your average number once a week and see how

179
00:13:10,282 --> 00:13:12,900
you can actually get that to be more frequent.

180
00:13:14,300 --> 00:13:18,136
And so you can do this by looking at GitHub, you can do it

181
00:13:18,158 --> 00:13:21,944
by looking at your deployments and

182
00:13:21,982 --> 00:13:25,412
seeing your pods. There's various

183
00:13:25,476 --> 00:13:28,840
ways to measure deployment frequency using whatever tools you're

184
00:13:28,920 --> 00:13:32,860
using today. Moving on to our

185
00:13:32,930 --> 00:13:36,376
third metric, meantime, to recovery.

186
00:13:36,488 --> 00:13:39,896
So this is the average amount of time that it takes

187
00:13:39,938 --> 00:13:43,788
your team to restore service when there's a service disruption,

188
00:13:43,964 --> 00:13:48,236
like an outage. And so this one actually offers

189
00:13:48,268 --> 00:13:51,764
a look into the stability of your software and the agility of your

190
00:13:51,802 --> 00:13:53,700
team in the face of a challenge.

191
00:13:55,080 --> 00:13:59,424
So again, the DevOps report found that elite performers

192
00:13:59,552 --> 00:14:02,512
have less than an hour meantime to recovery,

193
00:14:02,576 --> 00:14:06,484
and low performers can be anywhere from over

194
00:14:06,522 --> 00:14:09,304
six months to actually get that app. And by that point,

195
00:14:09,342 --> 00:14:12,920
you've probably lost all of your customers and should really

196
00:14:12,990 --> 00:14:16,570
evaluate why it took you that long to get back up and running.

197
00:14:17,920 --> 00:14:21,484
But to dive into why this metric is

198
00:14:21,522 --> 00:14:24,990
important a little bit more than I've done for the other two,

199
00:14:25,600 --> 00:14:29,532
I'll just use a concrete example, which is Meta's

200
00:14:29,596 --> 00:14:33,440
outage from October 4 that

201
00:14:33,510 --> 00:14:36,704
lasted five and a half hours. So whether you

202
00:14:36,742 --> 00:14:39,548
use Facebook Messenger, Instagram,

203
00:14:39,644 --> 00:14:43,284
whatsApp, you were probably impacted by this outage. I know I

204
00:14:43,322 --> 00:14:47,104
was. I have all of my family in Colombia and couldn't

205
00:14:47,152 --> 00:14:51,188
talk to them during that day because WhatsApp was down. But a

206
00:14:51,194 --> 00:14:54,732
lot of businesses actually run on WhatsApp.

207
00:14:54,896 --> 00:14:58,664
And so a lot of businesses were impacted by this outage as

208
00:14:58,702 --> 00:15:02,616
well. And so the outage was actually triggered by a

209
00:15:02,638 --> 00:15:06,600
system that manages the global backbone network capacity

210
00:15:07,360 --> 00:15:11,004
for Facebook. And basically it's built to

211
00:15:11,042 --> 00:15:14,428
connect all the computing facilities together.

212
00:15:14,594 --> 00:15:18,220
And they consist of tens of thousands of miles of

213
00:15:18,370 --> 00:15:22,112
fiber optic cables crossing the globe and link

214
00:15:22,166 --> 00:15:25,264
to all their data centers. And basically the

215
00:15:25,382 --> 00:15:28,560
problem was that during a routine maintenance job

216
00:15:28,710 --> 00:15:32,384
for these routers, there was a command issued

217
00:15:32,512 --> 00:15:36,404
with the intention to assess the availability of that

218
00:15:36,442 --> 00:15:40,320
global backbone capacity, which unintentionally

219
00:15:40,480 --> 00:15:45,208
took down all of the connections and

220
00:15:45,294 --> 00:15:49,320
effectively disconnected Facebook, all of Facebook's data centers.

221
00:15:49,900 --> 00:15:53,812
And so their commands are designed to audit

222
00:15:53,956 --> 00:15:57,384
things like this and prevent mistakes from happening.

223
00:15:57,502 --> 00:16:00,744
But there was a bug in the audit tool that actually did not catch

224
00:16:00,792 --> 00:16:04,232
this one. And as the engineers

225
00:16:04,296 --> 00:16:07,644
were working to basically figure out what was

226
00:16:07,682 --> 00:16:11,100
going wrong and how to get it back up, they faced two main obstacles.

227
00:16:11,260 --> 00:16:15,280
The first is that it was not possible to physically access

228
00:16:15,350 --> 00:16:19,200
the data centers because they were protected.

229
00:16:20,420 --> 00:16:24,324
And then also the total loss of DNS ended up breaking many

230
00:16:24,362 --> 00:16:28,070
of the internal tools that would help them diagnose these problems.

231
00:16:29,800 --> 00:16:33,616
And so Facebook actually put out a long post mortem

232
00:16:33,648 --> 00:16:37,124
on this and a long article about what they're going to do

233
00:16:37,162 --> 00:16:40,168
to prevent this from happening in the past. And I encourage you to take a

234
00:16:40,174 --> 00:16:44,052
look at it if you're interested. But at the end of the day, this outage

235
00:16:44,116 --> 00:16:47,630
cost Facebook over $60 million

236
00:16:48,320 --> 00:16:52,044
and again lasted five and a half hours. It's the longest outage they've ever

237
00:16:52,082 --> 00:16:55,660
had. Another popular tool

238
00:16:55,810 --> 00:16:59,664
that had a similar issue also in October of last

239
00:16:59,702 --> 00:17:03,840
year is Roblox. I was at a party recently with

240
00:17:03,910 --> 00:17:07,776
a bunch of kids and the seven year olds were talking my ear off

241
00:17:07,798 --> 00:17:11,296
about Roblox. And actually

242
00:17:11,478 --> 00:17:15,364
what happened was that they had an outage that lasted over three

243
00:17:15,402 --> 00:17:19,488
days. And you may be saying, yeah, it's just a kids game that's impacted,

244
00:17:19,584 --> 00:17:23,124
but it actually cost them about $25 million. So once

245
00:17:23,162 --> 00:17:26,724
again, huge cost associated with

246
00:17:26,762 --> 00:17:29,930
this outage. But what happened was

247
00:17:30,780 --> 00:17:34,616
two issues and once again, they put out a long post cortex on this and

248
00:17:34,638 --> 00:17:37,356
what they're going to do to fix it. So encourage you to take a look

249
00:17:37,378 --> 00:17:40,732
at it. But they were enabling a new

250
00:17:40,786 --> 00:17:44,332
feature that created unusually high read and

251
00:17:44,386 --> 00:17:47,544
write load and led to excessive contention and poor

252
00:17:47,592 --> 00:17:51,196
performance. And these load conditions actually triggered

253
00:17:51,228 --> 00:17:54,752
a pathological performance issue in an open source system

254
00:17:54,886 --> 00:17:58,700
that is used to manage the write ahead logs.

255
00:17:58,860 --> 00:18:02,132
And what this did is it actually brought down

256
00:18:02,186 --> 00:18:06,420
critical monitoring systems that provide visibility into these tools.

257
00:18:06,760 --> 00:18:10,660
And so this circular dependency on the thing being out,

258
00:18:10,730 --> 00:18:14,496
being the thing to help you diagnose is exactly what Roblox

259
00:18:14,528 --> 00:18:18,024
had said they're going to fix going forward. And it's something that

260
00:18:18,062 --> 00:18:21,384
you need to be thinking about as your team thinks about the meantime to

261
00:18:21,422 --> 00:18:25,332
resolve. You don't want your observability stack to be tied

262
00:18:25,396 --> 00:18:28,788
to everything that your tool is, because at the end of the day,

263
00:18:28,894 --> 00:18:31,644
it's just going to make it harder for you to bring it back up when

264
00:18:31,682 --> 00:18:33,340
these outages do occur.

265
00:18:34,880 --> 00:18:38,412
So again, if we look at what could cause long

266
00:18:38,466 --> 00:18:40,060
meantime to recovery,

267
00:18:40,880 --> 00:18:44,924
risky infrastructure, poor ability to actually roll

268
00:18:44,972 --> 00:18:48,432
back these changes, right. You want to make sure you always have a plan in

269
00:18:48,486 --> 00:18:52,336
place so that if there is an outage, you can roll back while you

270
00:18:52,358 --> 00:18:56,036
figure out what's wrong with that latest release. Having a

271
00:18:56,058 --> 00:18:59,684
bad incident management process where potentially you don't know who's on

272
00:18:59,722 --> 00:19:03,476
call or who the owner is or who to call, and then

273
00:19:03,578 --> 00:19:07,144
having tribal knowledge or insufficient documentation. You want to make sure

274
00:19:07,182 --> 00:19:10,776
that you have clear documentation for all the services that

275
00:19:10,798 --> 00:19:14,824
you have run, books that you have, logs that are accessible to everyone.

276
00:19:15,022 --> 00:19:18,564
Basically anything that could be needed to actually debug

277
00:19:18,612 --> 00:19:22,110
what's going wrong. You want to make sure your team is trained to do.

278
00:19:22,480 --> 00:19:25,710
And this is actually something that Cortex helps with.

279
00:19:26,080 --> 00:19:29,488
We have a service catalog feature where you can see all this information

280
00:19:29,574 --> 00:19:33,650
about your services and basically have one spot to go

281
00:19:34,420 --> 00:19:38,450
as you areas dealing with an incident and looking for this information

282
00:19:40,100 --> 00:19:43,540
so short meantime to recovery. The big difference

283
00:19:43,610 --> 00:19:46,676
here is having a tight incident management process.

284
00:19:46,778 --> 00:19:50,276
Again, knowing who to call when, having the ability to

285
00:19:50,298 --> 00:19:53,984
roll back quickly, having the tools needed to diagnose what's

286
00:19:54,032 --> 00:19:57,560
wrong and having those clear runbooks easily accessible.

287
00:19:58,300 --> 00:20:01,604
And again, a thing that personally I learned

288
00:20:01,652 --> 00:20:04,744
from hearing about these two outages I went through is that

289
00:20:04,782 --> 00:20:08,490
you probably don't want the DNS for your status page

290
00:20:08,940 --> 00:20:12,092
to be the same as these DNS for your website. If your website's down,

291
00:20:12,146 --> 00:20:15,724
so is your status page and you want to make sure that you're thinking about

292
00:20:15,762 --> 00:20:19,100
those things and keeping them separate. And so

293
00:20:19,170 --> 00:20:22,656
ways to actually measure meantime to recovery is using whatever on

294
00:20:22,678 --> 00:20:26,428
call provider. So for example, pagerduty Victor Ops,

295
00:20:26,604 --> 00:20:30,496
Ops Genie, you want to measure how long that outage was,

296
00:20:30,598 --> 00:20:34,224
how much time between the fix was discovered and how much time between it

297
00:20:34,262 --> 00:20:38,192
being released. Again, if you have insufficient CI CD pipelines,

298
00:20:38,256 --> 00:20:41,332
it might take longer to get that out, even if you know what the fix

299
00:20:41,386 --> 00:20:44,836
is. And then also you want to look at how long it took

300
00:20:44,858 --> 00:20:48,744
you to discover the outage. Like do you have the proper alerting so that

301
00:20:48,782 --> 00:20:52,392
when an outage happens, you know immediately, or is it taking

302
00:20:52,446 --> 00:20:55,816
a few hours and taking a customer calling it out for

303
00:20:55,838 --> 00:20:59,020
you to see the outage? And so

304
00:20:59,090 --> 00:21:02,744
you can use whatever tools you're using to measure

305
00:21:02,792 --> 00:21:06,300
this and see where those gaps are in order to

306
00:21:06,370 --> 00:21:09,952
improve your incident machine process and see how you can improve this going

307
00:21:10,006 --> 00:21:13,964
forward. And that brings

308
00:21:14,012 --> 00:21:17,676
me to my fourth metric, which is change failure

309
00:21:17,708 --> 00:21:21,448
rate. So this is these percentage of failures.

310
00:21:21,644 --> 00:21:25,412
This can include bugs that affect customers or

311
00:21:25,466 --> 00:21:28,896
releases that result in downtime or degraded

312
00:21:28,928 --> 00:21:32,516
service or rollbacks. This is again up to your

313
00:21:32,538 --> 00:21:35,656
team to define what you want to include in that

314
00:21:35,678 --> 00:21:39,432
change failure rate and basically figure

315
00:21:39,486 --> 00:21:43,240
out which parts of it you want to measure. A common

316
00:21:43,310 --> 00:21:46,808
mistake that teams make when measuring this is to just look at

317
00:21:46,814 --> 00:21:50,444
the total number of failures rather than the rate. But the

318
00:21:50,482 --> 00:21:54,156
rate is actually pretty important because the goal is

319
00:21:54,178 --> 00:21:57,912
to ship quickly. So if you look at the number of failures

320
00:21:57,976 --> 00:22:01,008
and you're shipping super often, that number might be higher,

321
00:22:01,174 --> 00:22:05,296
right? But actually you want to make sure that

322
00:22:05,398 --> 00:22:09,104
you have more deployments so that it's easier to

323
00:22:09,142 --> 00:22:12,470
again have that meantime to resolve be lower.

324
00:22:13,800 --> 00:22:16,884
And so you want to look at the rate,

325
00:22:17,002 --> 00:22:21,072
not just the number of failures. This can also be a good indicator

326
00:22:21,136 --> 00:22:25,284
for how much time your team is spending fixing

327
00:22:25,332 --> 00:22:27,960
processes rather than working on new features.

328
00:22:29,980 --> 00:22:32,490
And so again, looking at this report,

329
00:22:33,260 --> 00:22:36,804
the state of DevOps 2021 report found that elite

330
00:22:36,852 --> 00:22:40,460
performers have anywhere from zero to 15% change failure rate.

331
00:22:40,610 --> 00:22:43,390
Anywhere 15 or higher isn't great.

332
00:22:45,040 --> 00:22:48,136
And so we've all seen these memes, we've all kind of laughed

333
00:22:48,168 --> 00:22:51,688
at these. But you know, that moment when you're looking at your code and

334
00:22:51,714 --> 00:22:55,376
you're just patching up bug after bug after bug, that's something that

335
00:22:55,398 --> 00:22:59,024
you want to evaluate because it is increasing the number

336
00:22:59,062 --> 00:23:02,336
of bugs that your customers see and creating a poor customer

337
00:23:02,438 --> 00:23:05,716
experience. So a high change failure rate can

338
00:23:05,738 --> 00:23:09,380
be the cause of sloppy code reviews, maybe people

339
00:23:09,450 --> 00:23:12,768
areas just looking at the code, but not thinking about all the use cases,

340
00:23:12,864 --> 00:23:16,272
actually testing it out. Again, insufficient testing,

341
00:23:16,336 --> 00:23:19,792
whether it's unit teams integration

342
00:23:19,856 --> 00:23:23,604
tests, n ten tests, and then having staging environments

343
00:23:23,652 --> 00:23:27,988
with insufficient test data. So if your staging environment

344
00:23:28,164 --> 00:23:31,388
doesn't reflect the data that customers are using, at the end of

345
00:23:31,394 --> 00:23:34,776
the day, it may not be a good representation of testing

346
00:23:34,808 --> 00:23:37,550
your changes before actually rolling them out.

347
00:23:38,800 --> 00:23:42,770
And so a low change failure rate can actually,

348
00:23:43,700 --> 00:23:47,084
the way you get to that is by promoting an those

349
00:23:47,212 --> 00:23:50,716
that is focused on DevOps. And so basically creating that culture

350
00:23:50,748 --> 00:23:54,832
of quality, making sure that you have representative deployment,

351
00:23:54,896 --> 00:23:58,356
development and staging environments so that you can test this before it

352
00:23:58,378 --> 00:24:02,144
gets to production, and having a strong partnership

353
00:24:02,192 --> 00:24:05,576
between product and engineering so that you deeply understand

354
00:24:05,678 --> 00:24:09,672
the use cases and actually know how to test before

355
00:24:09,726 --> 00:24:12,884
going forward. And if these handled all the potential edge cases

356
00:24:12,932 --> 00:24:15,240
that you write tests for, those edge cases,

357
00:24:16,460 --> 00:24:20,376
anything basically to make you more confident that the features you're

358
00:24:20,408 --> 00:24:22,830
releasing work in the way that they're meant to.

359
00:24:23,680 --> 00:24:27,256
And so ways that you can measure this change failure

360
00:24:27,288 --> 00:24:31,308
rate is you can look at how many releases have caused downtime,

361
00:24:31,484 --> 00:24:35,420
you can look at how many tickets have actually resulted

362
00:24:35,580 --> 00:24:39,164
in incidents, you can look at how many tickets

363
00:24:39,212 --> 00:24:42,176
have follow up bug tickets to them. Because again,

364
00:24:42,278 --> 00:24:45,380
ideally you would catch those bugs before they go out.

365
00:24:45,530 --> 00:24:48,804
Even if they don't necessarily cause an outage, it still causes bad

366
00:24:48,842 --> 00:24:52,452
customer experience. And then you can

367
00:24:52,506 --> 00:24:55,992
honestly dig a step further and you can see

368
00:24:56,046 --> 00:24:59,636
how many of these issues areas a result of not having unit

369
00:24:59,668 --> 00:25:02,916
tests in place. Like would a unit test have caught

370
00:25:02,948 --> 00:25:06,504
the issue? Would an end

371
00:25:06,542 --> 00:25:09,996
to end test have caught the issue? Or was it having bad data and

372
00:25:10,018 --> 00:25:13,784
then making sure that you actually update that data in your staging environment

373
00:25:13,832 --> 00:25:17,388
so that going forward you can catch issues similar to

374
00:25:17,554 --> 00:25:19,490
whatever it is that caused these problem.

375
00:25:21,300 --> 00:25:24,784
And so that was a broad overview of all

376
00:25:24,822 --> 00:25:28,176
these four metrics, but now if we put them together, so again,

377
00:25:28,278 --> 00:25:31,100
they're lead time for changes deployment frequency,

378
00:25:31,180 --> 00:25:34,852
meantime to recovery and change failure rate. What they're really

379
00:25:34,906 --> 00:25:38,164
looking at is speed versus stability. So lead

380
00:25:38,202 --> 00:25:41,632
time for changes and deployment frequency are really looking at speed.

381
00:25:41,776 --> 00:25:45,552
How fast are you getting these changes out to your users?

382
00:25:45,696 --> 00:25:49,796
And stability is meantime to recovery and change failure

383
00:25:49,828 --> 00:25:53,656
rate. So how often is your app unstable due to changes that

384
00:25:53,678 --> 00:25:57,748
you have gotten out? And so the key is actually to empower

385
00:25:57,924 --> 00:26:01,480
your developers and give them the tools that they need to succeed.

386
00:26:01,640 --> 00:26:04,936
At the end of the day, it's not literally about these metrics,

387
00:26:05,048 --> 00:26:08,744
it's just about your team and figuring out using these metrics

388
00:26:08,792 --> 00:26:12,156
to improve their performance. And so your

389
00:26:12,178 --> 00:26:15,344
developers are the ones who are able to make the changes to help

390
00:26:15,382 --> 00:26:19,072
your team reach its goals. And you want to make sure that they

391
00:26:19,126 --> 00:26:22,484
understand these metrics, why they're important, and are using them to

392
00:26:22,522 --> 00:26:24,950
improve their processes day to day.

393
00:26:26,440 --> 00:26:30,112
And to give a more concrete example on literally measuring

394
00:26:30,176 --> 00:26:33,456
this, as I mentioned earlier, I'm an engineer

395
00:26:33,488 --> 00:26:37,364
at Cortex, and we help teams define standards

396
00:26:37,412 --> 00:26:40,936
and measure how they're doing. And so what you

397
00:26:40,958 --> 00:26:45,524
see on the screen right now is one of our features, which is called scorecards.

398
00:26:45,652 --> 00:26:49,900
It allows you to create rules for your team and

399
00:26:49,970 --> 00:26:53,932
actually will measure all your services and how they're doing and

400
00:26:53,986 --> 00:26:57,564
give you the scores based on the rules that you

401
00:26:57,602 --> 00:27:00,968
created, based on your integrations for your services.

402
00:27:01,154 --> 00:27:04,796
And then from this, you can create initiatives to help improve

403
00:27:04,828 --> 00:27:08,208
those things going forward. So you can say by Q three,

404
00:27:08,294 --> 00:27:11,564
I really want to improve my deployment frequency.

405
00:27:11,612 --> 00:27:15,236
I want to make sure that the CI CD pipelines are sufficient that

406
00:27:15,258 --> 00:27:18,660
we have better testing. So you can measure things like test coverage,

407
00:27:19,160 --> 00:27:23,188
and you can use scorecards to actually make best.

408
00:27:23,274 --> 00:27:26,480
You can use scorecards to make this a moving target

409
00:27:26,560 --> 00:27:30,436
across your teams. And so this is exactly what

410
00:27:30,458 --> 00:27:34,212
we do. Thank you very much. I hope

411
00:27:34,266 --> 00:27:38,028
you enjoyed learning about dorometrics, and feel free to

412
00:27:38,034 --> 00:27:39,480
put any questions in the chat.

