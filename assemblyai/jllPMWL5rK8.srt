1
00:00:00,730 --> 00:00:04,366
What if you could work with some of the world's most innovative companies,

2
00:00:04,548 --> 00:00:07,338
all from the comfort of a remote workplace?

3
00:00:07,514 --> 00:00:11,294
Andela has matched thousands of technologists across the globe to their

4
00:00:11,332 --> 00:00:15,818
next career adventure. We're empowering new talent worldwide,

5
00:00:15,914 --> 00:00:19,802
from Sao Paulo to Egypt and Lagos to Warsaw.

6
00:00:19,946 --> 00:00:23,038
Now the future of work is yours to create.

7
00:00:23,204 --> 00:00:27,030
Anytime, anywhere. The world is at your fingertips.

8
00:00:27,370 --> 00:00:29,190
This is Andela.

9
00:01:12,870 --> 00:01:16,454
Hey, friends. My name is Joe Carlson, and this is

10
00:01:16,492 --> 00:01:20,418
a gentle introduction to building dataintensive applications.

11
00:01:20,594 --> 00:01:24,390
So, first of all, do you know how much data is created every day

12
00:01:24,460 --> 00:01:28,374
in 2022? Well, it looks

13
00:01:28,412 --> 00:01:32,186
like about 80 zetabytes of data. And by 2025, we're looking

14
00:01:32,208 --> 00:01:35,590
at about 175 zettabytes. That's over double

15
00:01:35,670 --> 00:01:38,778
the amount of current data we're processing today.

16
00:01:38,864 --> 00:01:42,142
In just, was it three years? Three years from now?

17
00:01:42,276 --> 00:01:44,160
That's coming up fast. Right?

18
00:01:45,010 --> 00:01:48,062
So I want to talk to you about what it

19
00:01:48,116 --> 00:01:51,118
takes to build a data intensive application,

20
00:01:51,284 --> 00:01:55,230
because knowing how to do this is going to be critical

21
00:01:55,310 --> 00:01:58,834
for building applications that can scale in the future. And it's hard,

22
00:01:58,952 --> 00:02:03,380
right? But wet your pal a little bit.

23
00:02:05,530 --> 00:02:09,430
I want to show you today how building

24
00:02:09,500 --> 00:02:14,946
data intensive applications is a little bit like SpongeBob

25
00:02:14,978 --> 00:02:18,486
squarepants. And yes, I'm serious.

26
00:02:18,598 --> 00:02:21,786
We're about to dig in, right? SpongeBob is a

27
00:02:21,808 --> 00:02:25,962
reliable friend. He can absorb things

28
00:02:26,096 --> 00:02:30,330
like a sponge and scale to huge sizes.

29
00:02:30,690 --> 00:02:34,574
He's able to withstand nautical nonsense even

30
00:02:34,612 --> 00:02:37,630
as our servers and users flop like a fish.

31
00:02:38,050 --> 00:02:41,726
And most importantly, he can be simple. Right? Let's dig

32
00:02:41,748 --> 00:02:44,878
in and see how SpongeBob can help us learn how to

33
00:02:44,884 --> 00:02:48,754
build data intensive applications. So my

34
00:02:48,792 --> 00:02:51,746
name is Joe Carlson. I work for a company called Single Store,

35
00:02:51,848 --> 00:02:54,978
and I make a lot of TikToks and Twitter. So if you like what you

36
00:02:54,984 --> 00:02:59,150
see today, be sure to check that out. You can go to JoeCarlson dev links

37
00:02:59,230 --> 00:03:02,418
for everything discussed here in this talk today, as well as all my socials.

38
00:03:02,434 --> 00:03:05,222
So go check that out. Okay, so before we begin,

39
00:03:05,356 --> 00:03:09,106
I'll be checking out the chat if anyone has any questions. And credit to Martin

40
00:03:09,138 --> 00:03:12,362
Kletman for pioneering a lot of this content in his book,

41
00:03:12,416 --> 00:03:15,898
designing data intensive applications, which you should totally go check out. A lot

42
00:03:15,904 --> 00:03:19,398
of this is from his content. Today we're

43
00:03:19,414 --> 00:03:23,258
going to be introducing concepts about data intensive applications

44
00:03:23,274 --> 00:03:27,114
and how you can get started building them. And this is for developers

45
00:03:27,162 --> 00:03:30,766
who maybe have some comfortability with building some

46
00:03:30,788 --> 00:03:33,380
simple applications and are looking to scale up.

47
00:03:35,430 --> 00:03:38,974
Any knowledge about SQL or rdbms is also really useful,

48
00:03:39,022 --> 00:03:43,042
too. Okay, so we're going to be discussing what data

49
00:03:43,096 --> 00:03:46,294
intensive applications are, then we'll be going into

50
00:03:46,332 --> 00:03:50,440
the fundamentals of how to design and build a scalable data intensive application.

51
00:03:52,090 --> 00:03:56,050
Three key tenants are reliability, scalability and maintainability.

52
00:03:56,210 --> 00:03:59,894
And then I know this isn't live, so we're not going to do A-Q-A today,

53
00:03:59,932 --> 00:04:01,980
but let's get started with our intro content.

54
00:04:02,590 --> 00:04:06,326
Clearly we live in a world of dataintensive applications, and if we're

55
00:04:06,358 --> 00:04:09,786
not currently, it's going to be taking up more

56
00:04:09,808 --> 00:04:13,502
and more sectors very soon. So you may be asking

57
00:04:13,556 --> 00:04:17,086
yourself, what exactly is a data intensive application? And in

58
00:04:17,108 --> 00:04:20,414
my humble opinion, it is an application that has one of these

59
00:04:20,452 --> 00:04:23,794
five core tenets. It has an application that has a mass

60
00:04:23,832 --> 00:04:29,262
amount of data, or where data is streaming

61
00:04:29,326 --> 00:04:33,278
in really quickly, where low latency

62
00:04:33,454 --> 00:04:36,150
queries on your database is critical,

63
00:04:36,490 --> 00:04:39,734
if you have a complex series of databases with

64
00:04:39,772 --> 00:04:43,714
joins or whatever, or if you have a massive parallelism

65
00:04:43,762 --> 00:04:46,950
or concurrency in your application. The bottom line

66
00:04:47,020 --> 00:04:50,890
is though that data intensive applications is where data

67
00:04:50,960 --> 00:04:54,442
is the main constraint or bottleneck. I think

68
00:04:54,496 --> 00:04:57,862
previously cpu cycles was one of the main constraints,

69
00:04:57,926 --> 00:05:01,100
but in my humble opinion, with the advent of the cloud,

70
00:05:01,470 --> 00:05:05,194
that made that really less of an issue. Right? With a Kubernetes

71
00:05:05,242 --> 00:05:08,574
cluster or whatever in the cloud, we can just start scaling up servers and new

72
00:05:08,612 --> 00:05:12,442
nodes to start handling our applications. And in my opinion,

73
00:05:12,506 --> 00:05:15,822
as developers, one of the things that we're going to be struggling with the most

74
00:05:15,956 --> 00:05:19,822
is designing applications to scale up with their data intensity,

75
00:05:19,886 --> 00:05:22,786
not with the compute cycles, which is why this is important.

76
00:05:22,888 --> 00:05:25,720
So anyway, glad to be here.

77
00:05:26,490 --> 00:05:30,214
So obviously, businesses are becoming more and more data

78
00:05:30,252 --> 00:05:33,810
intensive. We're seeing massively rising complexity

79
00:05:33,890 --> 00:05:37,266
and data analysis and data science, machine learning models consuming

80
00:05:37,298 --> 00:05:41,078
mass amounts of data in real time, and users

81
00:05:41,094 --> 00:05:45,926
who are becoming more requiring

82
00:05:45,958 --> 00:05:48,490
that as their application scale.

83
00:05:49,230 --> 00:05:52,234
We want the apps we use be fast. We want them to be in real

84
00:05:52,272 --> 00:05:56,126
time. We want those real time alerts. Being able to wait 24 hours for

85
00:05:56,148 --> 00:05:58,958
a batch processing job no longer cuts it.

86
00:05:59,124 --> 00:06:02,078
And our applications are struggling to keep up,

87
00:06:02,244 --> 00:06:05,490
and for good reason. Like building these apps is

88
00:06:05,560 --> 00:06:09,310
hard, it takes time and it takes experimentation,

89
00:06:09,390 --> 00:06:12,786
and it usually involves hybrid approaches in order to scale our

90
00:06:12,808 --> 00:06:14,980
applications up. That's okay.

91
00:06:16,790 --> 00:06:20,882
I like giving some more concrete examples here, and I've broken these up by industries

92
00:06:20,946 --> 00:06:24,774
and verticals and some use cases for each of them. This is just

93
00:06:24,812 --> 00:06:28,854
a small example of what's available for data intensive applications

94
00:06:28,902 --> 00:06:33,690
here. Dashboards real time ML streaming,

95
00:06:34,110 --> 00:06:38,038
ads optimization real time what's

96
00:06:38,054 --> 00:06:41,622
the word? Recommendations. IoT devices

97
00:06:41,686 --> 00:06:45,760
streaming in mass amounts of data into dataset, real time data

98
00:06:46,130 --> 00:06:49,438
dashboards, whatever, there's so many things you can possibly do.

99
00:06:49,524 --> 00:06:52,138
And again, this is just a small spattering.

100
00:06:52,314 --> 00:06:56,410
Okay, so I'm obviously biased here, but single stored

101
00:06:56,500 --> 00:07:00,094
is the single database for data intensive applications,

102
00:07:00,142 --> 00:07:03,166
and I'm going to tell you why today. Single stored handles

103
00:07:03,198 --> 00:07:06,594
this and makes it easier to build these applications because they can get

104
00:07:06,632 --> 00:07:09,958
incredibly difficult. When you need a database that's going

105
00:07:09,964 --> 00:07:13,302
to help you solve those problems as easily as possible.

106
00:07:13,356 --> 00:07:16,662
We're going to try to keep our architecture as simple as possible,

107
00:07:16,716 --> 00:07:19,274
and single stored can help you do that. So if you're going to take anything

108
00:07:19,312 --> 00:07:22,854
home today, just know single store is the best database for handling

109
00:07:22,902 --> 00:07:27,254
data intensive applications. We have some comparisons

110
00:07:27,302 --> 00:07:31,830
here too. We scale better, we're faster, we have lowest latencies,

111
00:07:31,990 --> 00:07:35,294
and we handle the most data models and most parallelism than

112
00:07:35,412 --> 00:07:38,846
any other database. So go check those out. We're great for data analytics, real time

113
00:07:38,868 --> 00:07:42,446
machine learning, real time recommendations, whatever. We handle all

114
00:07:42,468 --> 00:07:46,226
those amazingly well. Okay, let's jump into it. What does

115
00:07:46,248 --> 00:07:49,794
an architecture for a data intensive application look like?

116
00:07:49,992 --> 00:07:52,770
Again, great question, dear listener.

117
00:07:53,270 --> 00:07:56,934
They tend to be pretty complicated. You have lots of different

118
00:07:56,972 --> 00:07:59,842
dataset pieces kind of fitting together, shuffling,

119
00:07:59,906 --> 00:08:03,366
transforming and saving data in lots of places as

120
00:08:03,388 --> 00:08:06,886
they flow through. We're seeing a lot of microservices kind of

121
00:08:06,908 --> 00:08:10,474
decoupling databases, which is fine, but it does makes our

122
00:08:10,512 --> 00:08:13,050
applications more complicated,

123
00:08:15,630 --> 00:08:19,974
which I don't know about you, I think developers have a tendency to overcomplicate

124
00:08:20,022 --> 00:08:23,526
things, to appear smarter. We are handling harder

125
00:08:23,558 --> 00:08:26,894
problems and I think we're kind of in this age of trying to understand and

126
00:08:26,932 --> 00:08:30,238
simplify these things as we scale. I think kubernetes is an

127
00:08:30,244 --> 00:08:33,346
example of something that's going through that right now and getting easier and

128
00:08:33,368 --> 00:08:37,330
better to use as it matures. But data intensive

129
00:08:37,910 --> 00:08:41,620
architectures and dataset just aren't quite there yet.

130
00:08:42,150 --> 00:08:45,374
But again, single store does make that a lot easier.

131
00:08:45,422 --> 00:08:48,950
We simplify that by handling more data types and better

132
00:08:49,020 --> 00:08:53,254
performant than alternatives. Lets of users are coming and using us

133
00:08:53,292 --> 00:08:57,174
to replace lots of different aspects of their data intensive application because

134
00:08:57,212 --> 00:09:01,238
it can handle that scale and simplifies their architectures.

135
00:09:01,414 --> 00:09:04,554
Okay, much, much better. Stripping all down. Right,

136
00:09:04,672 --> 00:09:08,278
thank you, SpongeBob. All right, so let's get into the nuts

137
00:09:08,294 --> 00:09:12,126
and bolts of designing a data intensive application. So the

138
00:09:12,148 --> 00:09:15,774
three big ideas here today are reliability, scalability and

139
00:09:15,812 --> 00:09:19,022
maintainability. These are the three things that make up

140
00:09:19,076 --> 00:09:23,086
a scalable, data intensive application. Let's dig into all three

141
00:09:23,108 --> 00:09:25,762
of them and kind of explore each one with some real world.

142
00:09:25,816 --> 00:09:28,734
Examples. First up, reliability.

143
00:09:28,862 --> 00:09:32,322
We got a SpongeBob with this good old reliable friend,

144
00:09:32,456 --> 00:09:35,558
Patrick Starr, right? Best buddies there to

145
00:09:35,564 --> 00:09:38,934
the end through thick and thin, just like our

146
00:09:38,972 --> 00:09:41,714
applications and our databases supporting those applications,

147
00:09:41,762 --> 00:09:45,334
right? So typically, as developers building a data

148
00:09:45,372 --> 00:09:48,850
intensive application, we're expecting them to perform as expected.

149
00:09:48,930 --> 00:09:52,166
Duh. We want them to be able to tolerate any errors

150
00:09:52,278 --> 00:09:56,362
that a user might make. Okay? Yes, good luck predicting those.

151
00:09:56,496 --> 00:10:00,134
We want it to be fast performance, and we want it to prevent any abuse

152
00:10:00,182 --> 00:10:03,630
or insecurities or any leaking of any data or whatever,

153
00:10:03,700 --> 00:10:07,466
right? We don't want any user secrets kind of getting out into the ether.

154
00:10:07,658 --> 00:10:10,830
This all seems pretty straightforward, right? But it gets

155
00:10:10,900 --> 00:10:14,706
complicated actually trying to make sure all these things actually happen that we

156
00:10:14,728 --> 00:10:18,622
want, right? So first thing with reliability

157
00:10:18,686 --> 00:10:22,946
is hardware and software faults. So that means

158
00:10:23,128 --> 00:10:26,760
a lot of times, especially for databases, redundancy was

159
00:10:27,770 --> 00:10:31,046
really difficult, even frowned upon in a lot of situations. You saw

160
00:10:31,068 --> 00:10:33,880
a lot of single node databases popping up.

161
00:10:34,650 --> 00:10:38,226
But we're moving towards a future of systems

162
00:10:38,258 --> 00:10:41,466
that tolerate more losses of the machines, right? And we need that

163
00:10:41,488 --> 00:10:45,210
to include our databases too. Netflix, for example,

164
00:10:45,280 --> 00:10:49,174
has a tool called Chaos monkey that randomly shuts down servers

165
00:10:49,222 --> 00:10:52,222
and databases to ensure that other systems are still working.

166
00:10:52,276 --> 00:10:56,014
And it's backing up, which is really helpful for their

167
00:10:56,052 --> 00:11:00,234
scalable architectures. But you also have software architectures

168
00:11:00,282 --> 00:11:04,122
too. It's unlikely, again, not impossible.

169
00:11:04,186 --> 00:11:07,326
You want to plan for it, but that large numbers of hardware components fail

170
00:11:07,358 --> 00:11:10,706
at the same time. Typically it's software or

171
00:11:10,728 --> 00:11:13,794
human errors related to software that is the main component of something.

172
00:11:13,832 --> 00:11:17,318
Shutting down and having redundancy helps us.

173
00:11:17,404 --> 00:11:20,614
And some other stuff we'll talk about in a little bit, like testing and

174
00:11:20,652 --> 00:11:23,480
good systems and safeguards does help that as well.

175
00:11:24,890 --> 00:11:28,390
Single store, for example, has data

176
00:11:28,460 --> 00:11:31,654
copied horizontally on leaf nodes in order to

177
00:11:31,692 --> 00:11:36,554
improve redundancy. So each leaf has replicated its data so

178
00:11:36,592 --> 00:11:39,478
that if one of the nodes goes down, you don't lose your data, you don't

179
00:11:39,494 --> 00:11:43,390
have any downtime. It reelects a new leaf node

180
00:11:43,730 --> 00:11:47,406
so that you don't lose any of that data. It stays online in

181
00:11:47,428 --> 00:11:50,990
case something goes down. All of the data is replicated across

182
00:11:51,060 --> 00:11:54,398
all of the leaf partitions, and your data is still there.

183
00:11:54,484 --> 00:11:58,046
You can control how many leaf nodes are it's replicated

184
00:11:58,078 --> 00:12:02,142
to and the amount of data, or like what kind of write or read consistency

185
00:12:02,206 --> 00:12:05,746
you want, so that you get the performance

186
00:12:05,778 --> 00:12:09,030
hits you want. So human errors,

187
00:12:10,410 --> 00:12:14,054
humans tend to be the least reliable component of any

188
00:12:14,092 --> 00:12:17,286
system. It's true, right?

189
00:12:17,388 --> 00:12:20,154
Computers go down, but humans go down way more often,

190
00:12:20,272 --> 00:12:22,854
configuration errors, outages,

191
00:12:22,982 --> 00:12:25,862
whatever. Humans are the leading cause of outages.

192
00:12:26,006 --> 00:12:29,494
That's just the way it is. Right? So as developers,

193
00:12:29,542 --> 00:12:33,146
we want to design systems that minimize the opportunities for human error

194
00:12:33,178 --> 00:12:36,906
in our systems. That means we want to have interfaces and admins

195
00:12:36,938 --> 00:12:41,002
that are admin interfaces that are designed well so you don't make mistakes

196
00:12:41,146 --> 00:12:44,290
or that you can't do things that you don't want

197
00:12:44,360 --> 00:12:47,380
other people shouldn't be doing on your system.

198
00:12:47,990 --> 00:12:50,594
Automated testing can be really helpful for this,

199
00:12:50,712 --> 00:12:54,414
and deploying fully featured sandbox

200
00:12:54,462 --> 00:12:57,958
environments locally or in the cloud to do testing on can be another way

201
00:12:58,044 --> 00:13:02,578
to help mitigate some of these issues. This is a minor sidebar, but databases

202
00:13:02,674 --> 00:13:05,682
frequently get left out of the DevOps discussion.

203
00:13:05,826 --> 00:13:09,738
And I get it, it's hard to do. But that doesn't mean like data

204
00:13:09,824 --> 00:13:13,494
tends to be the most important and critical part of our application. And leaving

205
00:13:13,542 --> 00:13:17,354
it down to a manual process is going to leave

206
00:13:17,392 --> 00:13:21,162
you vulnerable to human error

207
00:13:21,226 --> 00:13:24,080
being introduced to your data intensive application.

208
00:13:24,850 --> 00:13:28,526
Okay, great. TLDr just have

209
00:13:28,548 --> 00:13:32,910
good practices to protect your system from dust down humans and scalability.

210
00:13:33,970 --> 00:13:37,698
This is how we cope with increased load. So we need to

211
00:13:37,704 --> 00:13:41,330
be able to scale the system up in order to handle.

212
00:13:42,070 --> 00:13:45,986
This is the second tenant, right. Scalability to handle the increased load of

213
00:13:46,008 --> 00:13:49,746
our systems. Right. Whether that's increased throughput

214
00:13:49,778 --> 00:13:53,394
to our database, we're adding more paralyzed users kind of accessing

215
00:13:53,442 --> 00:13:57,030
data or whatever. Right. And I want to use an example

216
00:13:57,100 --> 00:14:00,578
here to illustrate what I mean by scalability. So let's talk

217
00:14:00,604 --> 00:14:04,106
about Twitter, right? The core tenets of Twitter. We want to post a tweet and

218
00:14:04,128 --> 00:14:07,914
we want each of our users to read tweets from a

219
00:14:07,952 --> 00:14:12,094
timeline. So there's two key methods you

220
00:14:12,132 --> 00:14:13,840
could use to implement this.

221
00:14:16,130 --> 00:14:19,306
We have the two posting tweets.

222
00:14:19,338 --> 00:14:23,326
We're seeing about 4000 2000 tweets posted per

223
00:14:23,348 --> 00:14:26,734
second, about 300,000 tweets per second. This might be different.

224
00:14:26,772 --> 00:14:29,106
This is a couple of years ago I got this data from. But you get

225
00:14:29,128 --> 00:14:32,754
the point, right? We want to design the system. So the first

226
00:14:32,792 --> 00:14:36,366
way you might be able to do that is by building a couple of tables.

227
00:14:36,478 --> 00:14:40,294
We have a user who has a follows table and

228
00:14:40,412 --> 00:14:43,686
that we have a users table that allows us to find all of the

229
00:14:43,708 --> 00:14:47,074
users that are tweeting and we can pull that into a feed,

230
00:14:47,122 --> 00:14:49,560
right? We could do a couple joins and pull that in.

231
00:14:50,570 --> 00:14:53,658
But Twitter actually did this, right? And if you

232
00:14:53,664 --> 00:14:56,506
were Golang to build this at home, like your little toy Twitter app at home,

233
00:14:56,528 --> 00:14:59,946
which by the way, that was my first web app I ever built just

234
00:14:59,968 --> 00:15:03,614
to learn web development. Little Twitter clone. But with

235
00:15:03,652 --> 00:15:06,814
this approach, the system struggled to keep up the load of the home

236
00:15:06,852 --> 00:15:10,682
timeline queries. That's because joins are both expensive with memory

237
00:15:10,746 --> 00:15:14,382
and time. Doing all those joins within the system was really,

238
00:15:14,436 --> 00:15:17,634
really expensive and slow. And as the users went

239
00:15:17,672 --> 00:15:20,354
up reposting more and more and more,

240
00:15:20,552 --> 00:15:24,158
and they're reading more and more and more, those joins became a major blocker

241
00:15:24,174 --> 00:15:27,480
for the system. Yes.

242
00:15:27,930 --> 00:15:31,714
So that didn't work. So what's the second method

243
00:15:31,762 --> 00:15:35,590
that Twitter use? This is a can out

244
00:15:35,660 --> 00:15:39,302
approach. So think of it as like a mailbox, right?

245
00:15:39,356 --> 00:15:42,618
So like a user posts a tweet and they put it

246
00:15:42,624 --> 00:15:45,882
in their mailbox, and then we do a fan outs, then something.

247
00:15:45,936 --> 00:15:49,546
A service reads that letter in your mailbox and

248
00:15:49,568 --> 00:15:53,278
it copies it and sends it out to a bunch of other people's mailboxes out.

249
00:15:53,364 --> 00:15:56,750
So every user that follows you gets it in their timeline,

250
00:15:57,410 --> 00:16:00,030
which allows you to do way less joins,

251
00:16:01,090 --> 00:16:04,990
and that you're just inserting them into each new timeline cache.

252
00:16:05,490 --> 00:16:09,102
There's a bunch of benefits of this, but there is one downside.

253
00:16:09,246 --> 00:16:12,718
The downside of this approach is that posting a tweet now requires

254
00:16:12,734 --> 00:16:17,214
a lot of extra work. Some users might have like 30 million followers,

255
00:16:17,262 --> 00:16:20,678
like a Justin Bieber or Kim Kardashian or something, right.

256
00:16:20,844 --> 00:16:24,870
That means that a single tweet from one of these power users would result

257
00:16:24,940 --> 00:16:28,630
in over 30 million rights to other mailboxes,

258
00:16:29,690 --> 00:16:33,514
which is a lot. Right. This approach does

259
00:16:33,552 --> 00:16:36,922
work really well, but there are some downsides to it. Right. And it's a lot

260
00:16:36,976 --> 00:16:40,354
more complicated, especially when you're talking Twitter scale.

261
00:16:40,502 --> 00:16:45,146
So what did Twitter do? They actually approached

262
00:16:45,178 --> 00:16:48,634
this using both. So they used a hybrid approach.

263
00:16:48,682 --> 00:16:52,442
Tweets continue to be fanned out along among most users,

264
00:16:52,506 --> 00:16:56,058
but for a small number of power users,

265
00:16:56,234 --> 00:16:59,394
they still use the first method to kind of send them

266
00:16:59,432 --> 00:17:03,058
out as read only. So people would go and read the message from

267
00:17:03,224 --> 00:17:06,530
the person they're following, which is wild.

268
00:17:07,430 --> 00:17:09,954
There's a little bit of both. I think that's kind of genius, though. And that

269
00:17:09,992 --> 00:17:13,158
tends to be the approach with a lot of these systems. Right. You kind of,

270
00:17:13,164 --> 00:17:15,766
like, see what happens. You try to approach it, and you have to try to

271
00:17:15,788 --> 00:17:20,220
do some interesting massaging to get it to fit your system.

272
00:17:24,750 --> 00:17:28,314
Okay, there we go. By the way, single stored also

273
00:17:28,352 --> 00:17:31,950
does this, too. We do a horizontally scalable

274
00:17:33,090 --> 00:17:37,370
dataset to try to increase parallelism and concurrency,

275
00:17:37,450 --> 00:17:41,466
which increases throughput for our databases. But there's

276
00:17:41,498 --> 00:17:44,462
lots of ways you can do it. You can scale up horizontally like single store

277
00:17:44,516 --> 00:17:48,306
does scale up vertically, which typical SQL dataset do? I know you

278
00:17:48,328 --> 00:17:52,046
can do it with a lot of other ones, but that's just buying a bigger

279
00:17:52,078 --> 00:17:54,530
server to handle the increased load.

280
00:17:55,430 --> 00:17:58,694
Yeah, those are kind of the load. The systems that do it,

281
00:17:58,892 --> 00:18:02,822
in my humble opinion, going the node based horizontal approach is

282
00:18:02,876 --> 00:18:06,166
one of the most scalable and effective ways moving forward.

283
00:18:06,348 --> 00:18:09,530
Okay, last core tenant here is maintainability.

284
00:18:10,110 --> 00:18:13,386
So the majority of our software cost is actually not from

285
00:18:13,408 --> 00:18:17,014
the development of the initial product. It's the ongoing maintenance. Us engineers

286
00:18:17,062 --> 00:18:20,566
are very expensive and it's important for us to

287
00:18:20,608 --> 00:18:23,978
build systems that are maintainable long term.

288
00:18:24,154 --> 00:18:27,438
So that means that there's three things we needed to keep in mind for this

289
00:18:27,524 --> 00:18:31,070
operability, simplicity and evolvability of our systems.

290
00:18:31,650 --> 00:18:36,014
So operability does not refer to performing

291
00:18:36,062 --> 00:18:39,394
operations or surgery operations, has to do with

292
00:18:39,512 --> 00:18:43,122
a team that's responsible for the ongoing operations of

293
00:18:43,176 --> 00:18:46,662
the code base that's running. So tracking down

294
00:18:46,716 --> 00:18:50,690
problems with the infrastructure or software,

295
00:18:50,850 --> 00:18:53,926
they're anticipating future problems or scalable problems.

296
00:18:54,108 --> 00:18:58,490
They're monitoring for anything that goes down or any sort of systems.

297
00:18:59,230 --> 00:19:03,066
They're performing complex maintenance tasks and security audits of

298
00:19:03,088 --> 00:19:07,366
the system, bunch of these things. Basically, good operability

299
00:19:07,478 --> 00:19:10,682
means you're making routine tasks easy for your scalable

300
00:19:10,746 --> 00:19:14,814
system. Okay, simplicity. This allows us

301
00:19:14,852 --> 00:19:17,840
to manage complexity and make it easier. Obviously,

302
00:19:18,370 --> 00:19:21,406
building complex systems is really, really hard. And the

303
00:19:21,428 --> 00:19:24,914
more moving parts you have in a system, the harder it is to troubleshoot and

304
00:19:24,952 --> 00:19:28,946
build. Thinking long term about your product,

305
00:19:29,128 --> 00:19:32,306
you want to make sure that it is easy enough for new people to

306
00:19:32,328 --> 00:19:35,506
come in and start using it. I've seen that a lot

307
00:19:35,528 --> 00:19:38,546
too. I feel like engineers, we get really into something, we go crazy about it

308
00:19:38,568 --> 00:19:42,502
and then we realize it's hard to maintain. I know personally for me,

309
00:19:42,636 --> 00:19:46,226
I've favored low maintenance scalable solutions

310
00:19:46,258 --> 00:19:49,570
because I would rather be building new stuff than maintaining stuff long term.

311
00:19:49,650 --> 00:19:53,494
And by focusing on simplifying our architecture, that allows

312
00:19:53,542 --> 00:19:56,538
us to do that. That allows us to do that.

313
00:19:56,624 --> 00:20:00,654
Again, I'm going to say this just last time here, but about single store people

314
00:20:00,692 --> 00:20:04,382
are replacing lots of different databases because it's much

315
00:20:04,436 --> 00:20:08,490
more simple and scalable to do that. If you have three different databases,

316
00:20:08,570 --> 00:20:13,178
like in memory database, a NoSQL database,

317
00:20:13,354 --> 00:20:16,886
and a SQL database like postgres or whatever, you can simplify

318
00:20:16,938 --> 00:20:20,386
those down into one data service that does all those things. That's going to make

319
00:20:20,408 --> 00:20:23,858
your ongoing maintenance so much more simple. Single store can

320
00:20:23,864 --> 00:20:26,994
do that, which is amazing. Check it out. And lastly,

321
00:20:27,042 --> 00:20:30,118
evolvability. The only thing that's certain in software is change.

322
00:20:30,284 --> 00:20:33,014
The only certainty is change. Something like that.

323
00:20:33,052 --> 00:20:36,294
Right? But we want to make sure your systems are

324
00:20:36,492 --> 00:20:39,846
built to evolve and change, which I will say,

325
00:20:39,948 --> 00:20:43,386
data makes it hard, data is sticky, and we want to make sure

326
00:20:43,408 --> 00:20:46,714
you're trying your best. You can't predict everything that's going to happen,

327
00:20:46,752 --> 00:20:49,818
but you can try your best to make it easy for you to change in

328
00:20:49,824 --> 00:20:52,160
the future as new requirements come up in your system.

329
00:20:52,770 --> 00:20:56,206
Okay, so quick recap here. If you're building out a

330
00:20:56,228 --> 00:20:59,758
scalable, data intensive application, you need

331
00:20:59,924 --> 00:21:03,370
a database and system that scales with your usage. As it grows,

332
00:21:03,450 --> 00:21:07,018
your data usage will grow. That's just how it goes. Data is sticky

333
00:21:07,114 --> 00:21:11,186
and unless you have a strong governance policy, it's probably

334
00:21:11,208 --> 00:21:13,442
Golang to be staying there and growing. So you want to make sure your system

335
00:21:13,496 --> 00:21:16,450
can grow for at best next five years. Honestly,

336
00:21:17,190 --> 00:21:20,674
you make sure you're securing your data and you can ensure privacy

337
00:21:20,722 --> 00:21:24,806
for your users. You need to make sure it can handle load today as

338
00:21:24,828 --> 00:21:28,166
well as your anticipated load again within five years, which is hard to

339
00:21:28,188 --> 00:21:31,766
predict. It needs to be capable of delivering

340
00:21:31,798 --> 00:21:35,434
analytics. I would recommend making

341
00:21:35,472 --> 00:21:38,714
sure it can handle real time analytics because that's going to be if it's not

342
00:21:38,752 --> 00:21:42,342
a current need, you should be anticipating it as a potential future

343
00:21:42,416 --> 00:21:45,914
need for your system. You want to make sure that there's no noticeable

344
00:21:46,042 --> 00:21:49,040
leg, especially for the end users of your system.

345
00:21:50,290 --> 00:21:53,754
This is all admittedly a tall order, but single stored

346
00:21:53,812 --> 00:21:57,646
can do all this and way, way more. It's a fast, unified database

347
00:21:57,678 --> 00:22:00,606
system that's acid compliant,

348
00:22:00,718 --> 00:22:04,226
all that good stuff. You should definitely go check that out. Okay,

349
00:22:04,328 --> 00:22:07,570
so, questions? I'm in the comments if anyone wants to chat.

350
00:22:07,730 --> 00:22:12,066
Otherwise, next step I would recommend you checking out designing

351
00:22:12,098 --> 00:22:15,606
data intensive applications by Martin Kletman I

352
00:22:15,628 --> 00:22:19,538
also recommend setting up your own

353
00:22:19,564 --> 00:22:23,500
project. I think the best way to learn something is to build it yourself.

354
00:22:24,430 --> 00:22:28,058
I have some examples that I can share in the chat here.

355
00:22:28,224 --> 00:22:32,570
Building your own stock ticker, stock scraper.

356
00:22:32,910 --> 00:22:36,314
You can do a real time Twitter data stream in and doing analysis

357
00:22:36,362 --> 00:22:40,014
on that with machine learning models. There's tons of huge data

358
00:22:40,052 --> 00:22:43,134
sets you can play around with. The best way to do that is to find

359
00:22:43,172 --> 00:22:46,826
someone that's a system that is free with like a developer

360
00:22:46,858 --> 00:22:50,370
tier and try to build something. And I think go

361
00:22:50,440 --> 00:22:53,806
is a great place to do that too. In fact, I've got a great shipping

362
00:22:53,838 --> 00:22:56,100
logistics demo I'm going to share in the comments here.

363
00:22:57,750 --> 00:23:00,738
Definitely learn by reading. There's a bunch of ways to do that here. And if

364
00:23:00,744 --> 00:23:02,918
you want to get started with single store and try that out, it's a great

365
00:23:03,004 --> 00:23:06,134
free, easy way to do that. We have a managed service. You get $500

366
00:23:06,172 --> 00:23:09,158
in free credits today. No credit card needed. Just you can go try it out.

367
00:23:09,164 --> 00:23:13,550
It's amazing. Go to singlestore.com managed servicetrial.

368
00:23:13,730 --> 00:23:17,354
Okay. And here is some additional reading if you want it as well

369
00:23:17,392 --> 00:23:20,362
too. I'm going to flash this up. Great.

370
00:23:20,496 --> 00:23:23,870
And thank you so much, everybody. This has been an absolute

371
00:23:23,940 --> 00:23:27,486
blast. I am so honored to be here.

372
00:23:27,588 --> 00:23:30,718
You all are amazing. Again, my name is Joe Karlsson. I'm a software engineer and

373
00:23:30,724 --> 00:23:33,806
I work at single store. If you want to follow me, check me out at

374
00:23:33,828 --> 00:23:37,358
Joe Karlsson one on Twitter. That's in the lower right hand corner. If you want

375
00:23:37,364 --> 00:23:42,650
to follow me on all the other links, you can check out Joecarlson dev links.

376
00:23:42,810 --> 00:23:45,894
All right, I'm ahead out. Thank, thank you so much.

377
00:23:46,012 --> 00:23:46,660
Talk to you later.

