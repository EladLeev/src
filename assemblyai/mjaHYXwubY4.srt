1
00:00:27,010 --> 00:00:31,202
Hi everyone. My name is Melisa, and I'm a developer advocate

2
00:00:31,266 --> 00:00:34,550
here at iterative AI. And today

3
00:00:34,620 --> 00:00:38,946
I'm going to talk to you about using reproducible experiments

4
00:00:39,058 --> 00:00:41,730
to create better machine learning models.

5
00:00:41,890 --> 00:00:45,442
So feel free to reach out to me personally

6
00:00:45,506 --> 00:00:48,966
on Twitter at flipped coding. If you have any questions,

7
00:00:49,148 --> 00:00:52,782
or if you want to get in touch with the whole DVC team,

8
00:00:52,916 --> 00:00:56,922
feel free to reach out to us on twitter@dvc.org

9
00:00:57,066 --> 00:01:00,606
but to get started, if you want

10
00:01:00,628 --> 00:01:04,734
to follow along or at some point go back and reference

11
00:01:04,862 --> 00:01:08,098
the project that I'm going to be using as an example.

12
00:01:08,184 --> 00:01:11,806
Throughout this talk, youll need a few things installed.

13
00:01:11,918 --> 00:01:15,758
So you'll need Python three. You don't need

14
00:01:15,864 --> 00:01:19,302
vs. Code, but vs. Code does make it a lot

15
00:01:19,356 --> 00:01:22,902
easier. And you'll need to fork this

16
00:01:22,956 --> 00:01:26,694
repo here, and it'll give you the exact project that you

17
00:01:26,812 --> 00:01:30,166
see me show in this presentation.

18
00:01:30,358 --> 00:01:34,378
So let's just jump into it. There are a few common

19
00:01:34,464 --> 00:01:38,170
issues when it comes to machine learning projects.

20
00:01:38,670 --> 00:01:42,926
First, we're tuning to find the best combination of

21
00:01:43,028 --> 00:01:45,786
hyperparameter values, algorithms,

22
00:01:45,898 --> 00:01:49,466
data sets, environment configurations.

23
00:01:49,658 --> 00:01:52,730
There's a lot that goes into each of these

24
00:01:52,820 --> 00:01:56,562
models we produce. And every time we get new

25
00:01:56,616 --> 00:02:00,530
requirements, or we get new data, or we get access to

26
00:02:00,600 --> 00:02:04,562
new resources, we still have to come back

27
00:02:04,616 --> 00:02:07,830
to this fundamental thing where we find

28
00:02:07,980 --> 00:02:11,494
this best combination of all of these

29
00:02:11,532 --> 00:02:13,560
different things that go into our model.

30
00:02:15,050 --> 00:02:17,746
So this is fine, right?

31
00:02:17,868 --> 00:02:21,020
We're just going to keep trying out different things.

32
00:02:21,390 --> 00:02:24,710
We'll do our hyperparameter tuning.

33
00:02:24,790 --> 00:02:28,246
We'll read through academic papers

34
00:02:28,278 --> 00:02:32,394
and find the most cutting edge algorithms to practice

35
00:02:32,442 --> 00:02:35,982
with or to try out. And we have to keep track

36
00:02:36,036 --> 00:02:39,502
of all of these changes, because eventually you will find

37
00:02:39,556 --> 00:02:42,702
a model that's really, really good.

38
00:02:42,836 --> 00:02:47,486
Like it'll give you some kind of incredible accuracy,

39
00:02:47,678 --> 00:02:51,586
or youll notice that the performance is

40
00:02:51,608 --> 00:02:54,942
a lot faster or something. And you need to keep track

41
00:02:55,016 --> 00:02:58,614
of all of the changes you made throughout all of these

42
00:02:58,652 --> 00:03:02,760
different experiments, so that when you get this incredible model,

43
00:03:03,130 --> 00:03:06,614
you can reproduce it. So as

44
00:03:06,652 --> 00:03:10,540
we go through all of these experiments, trying to find

45
00:03:10,990 --> 00:03:15,494
the best combination of hyperparams

46
00:03:15,542 --> 00:03:19,750
and data sets, we have to keep track of each

47
00:03:19,920 --> 00:03:23,294
experiment we run. And the problem with

48
00:03:23,332 --> 00:03:26,574
that is that over time, it gets really hard to

49
00:03:26,612 --> 00:03:30,270
follow those changes. So there's this thing

50
00:03:30,340 --> 00:03:34,894
where we might have hundreds of hyperparameter

51
00:03:34,942 --> 00:03:38,574
values to test out. How do youll manually

52
00:03:38,702 --> 00:03:41,954
keep track of that many different

53
00:03:42,072 --> 00:03:45,554
experiments? And in between times you're

54
00:03:45,602 --> 00:03:49,590
changing hyperparameters? You might take a look at your code and think,

55
00:03:49,660 --> 00:03:53,238
oh, maybe if I just change this one line

56
00:03:53,324 --> 00:03:57,314
here, that might do something different, and then

57
00:03:57,452 --> 00:04:00,746
you get more data from production or

58
00:04:00,768 --> 00:04:04,966
something. So you layer on to these experiments

59
00:04:05,078 --> 00:04:08,934
really fast and sometimes without even noticing

60
00:04:08,982 --> 00:04:12,302
that it's happening. That's why over time, it's hard

61
00:04:12,356 --> 00:04:16,030
to follow those changes that led you to your

62
00:04:16,100 --> 00:04:19,678
current best model that actually

63
00:04:19,764 --> 00:04:23,390
has that best combination of all of those factors

64
00:04:23,470 --> 00:04:26,100
that go into tracking this model.

65
00:04:26,550 --> 00:04:30,162
So we want to make sure we have some

66
00:04:30,216 --> 00:04:34,142
kind of way to keep track of all of these experiments,

67
00:04:34,286 --> 00:04:38,342
make sure that we know what code,

68
00:04:38,476 --> 00:04:41,814
data and configs were associated with our

69
00:04:41,852 --> 00:04:45,282
model. So when we get ready to deploy to production,

70
00:04:45,426 --> 00:04:48,762
we don't have that weird drop in

71
00:04:48,816 --> 00:04:53,078
accuracy or there's this strange

72
00:04:53,174 --> 00:04:56,682
environment difference that we just couldn't account

73
00:04:56,736 --> 00:05:00,174
for before. When we're able to follow these changes over

74
00:05:00,212 --> 00:05:03,690
time, when you're ready to deploy to production,

75
00:05:03,850 --> 00:05:06,000
it becomes a lot more,

76
00:05:09,090 --> 00:05:12,910
it just becomes a lot more consistent, a lot more reliable.

77
00:05:13,590 --> 00:05:16,820
So let's look at how we actually fix these issues.

78
00:05:17,590 --> 00:05:21,170
The first way is just by thinking of each

79
00:05:21,240 --> 00:05:24,094
experiment as its own little bundle.

80
00:05:24,222 --> 00:05:28,022
So can experiment consists of your data

81
00:05:28,076 --> 00:05:31,094
set, any hyperparameters you have,

82
00:05:31,292 --> 00:05:34,694
and maybe you have a model to start with, or you just have

83
00:05:34,732 --> 00:05:38,954
some algorithm you want to test out. But for each experiment you

84
00:05:38,992 --> 00:05:42,410
run, they all have these same

85
00:05:42,480 --> 00:05:46,150
things in common. So as you're adjusting

86
00:05:46,230 --> 00:05:49,642
your parameters, as you're updating your

87
00:05:49,696 --> 00:05:53,054
data set, you want to be able two track

88
00:05:53,172 --> 00:05:56,750
each of those experiments, kind of like you see on the screen here.

89
00:05:56,900 --> 00:05:59,934
So that's why

90
00:05:59,972 --> 00:06:04,046
we're going to track about a little background on hyperparameter tuning

91
00:06:04,158 --> 00:06:08,100
before we jump too deep into how we fix the problem.

92
00:06:08,630 --> 00:06:11,870
So with hyperparameter tuning,

93
00:06:11,950 --> 00:06:15,354
we know that hyperparameters are the values

94
00:06:15,422 --> 00:06:19,266
that define the model. If you're working with a neural

95
00:06:19,298 --> 00:06:22,710
net, that means the values like the

96
00:06:22,780 --> 00:06:26,326
number of layers in your neural net, or if

97
00:06:26,348 --> 00:06:29,962
you have a random forest classifier that will be

98
00:06:30,016 --> 00:06:34,518
like the max depth for that classifier.

99
00:06:34,694 --> 00:06:38,374
So these aren't the things that your model predicts

100
00:06:38,422 --> 00:06:41,918
for you. These are the values that actually build

101
00:06:42,004 --> 00:06:45,534
the model that does the prediction. And there

102
00:06:45,572 --> 00:06:49,834
are a couple of common ways. Two, approach hyperparameter

103
00:06:49,882 --> 00:06:53,618
tuning, and that's through grid search and random search.

104
00:06:53,784 --> 00:06:57,246
With grid search, you have sets

105
00:06:57,278 --> 00:07:00,158
of values for each of your hyperparameters,

106
00:07:00,334 --> 00:07:03,922
and you go through all of them. So if

107
00:07:03,976 --> 00:07:07,794
there is a best combination of hyperparameter

108
00:07:07,842 --> 00:07:11,174
values, grid search is definitely going to find that

109
00:07:11,212 --> 00:07:14,200
for you because it's testing everything.

110
00:07:14,810 --> 00:07:18,426
And random search is another method that we

111
00:07:18,448 --> 00:07:22,234
use for hyperparameter tuning. And it's similar to

112
00:07:22,272 --> 00:07:25,414
grid search in that you give it sets

113
00:07:25,462 --> 00:07:29,434
of values for each hyperparameter, but the difference is

114
00:07:29,472 --> 00:07:33,562
that it just jumps around random conditions

115
00:07:33,626 --> 00:07:37,054
of these values instead of going through each of

116
00:07:37,092 --> 00:07:40,414
them very systematically. A lot of

117
00:07:40,452 --> 00:07:43,826
times if you run a random search for about the

118
00:07:43,848 --> 00:07:47,922
same amount of time as a grid search, youll end up getting

119
00:07:48,056 --> 00:07:52,206
better conditions of hyperparameter values. And that's

120
00:07:52,238 --> 00:07:56,390
just because random search samples a wider

121
00:07:56,730 --> 00:07:59,590
variety of those values.

122
00:08:00,730 --> 00:08:04,422
So we know our problem is keeping track

123
00:08:04,476 --> 00:08:07,682
of all of these experiments. We know we want to

124
00:08:07,756 --> 00:08:11,082
solve it by making these little

125
00:08:11,136 --> 00:08:15,462
bundles for each experiment. And we know with hyperparameter tuning,

126
00:08:15,526 --> 00:08:19,226
we have a lot of values that we're going

127
00:08:19,248 --> 00:08:22,334
to experiment with. So let's take a look

128
00:08:22,372 --> 00:08:26,414
at DVC, which is a tool that helps us manage all

129
00:08:26,452 --> 00:08:30,442
of this experiment. Tracking DVC

130
00:08:30,506 --> 00:08:33,982
is an open source tool. You can go check out the GitHub

131
00:08:34,046 --> 00:08:37,278
repo. It works on top of git.

132
00:08:37,374 --> 00:08:40,930
So think of DVC as

133
00:08:41,080 --> 00:08:45,582
git for your data. So you're able to

134
00:08:45,736 --> 00:08:49,014
check in your code with git. You're able to check

135
00:08:49,052 --> 00:08:52,854
in your data with DVC, and it works on top

136
00:08:52,892 --> 00:08:56,470
of git so that you're able to bundle your

137
00:08:56,540 --> 00:09:00,150
code, your data, and any hyperparameters

138
00:09:00,230 --> 00:09:03,722
or other environment configs together for

139
00:09:03,776 --> 00:09:07,322
each experiment you run. And the best

140
00:09:07,376 --> 00:09:10,102
part is, it's not opinionated at all.

141
00:09:10,256 --> 00:09:14,238
So to use DVC, you don't actually

142
00:09:14,324 --> 00:09:17,722
need to install any particular libraries.

143
00:09:17,866 --> 00:09:21,630
You just need to initialize DVC in your

144
00:09:21,700 --> 00:09:25,054
git repo and use the commands.

145
00:09:25,182 --> 00:09:28,626
That's really it. There's no API calls to

146
00:09:28,648 --> 00:09:33,246
slow down your training. I know this comes up a little bit with Mlflow

147
00:09:33,278 --> 00:09:36,214
because it makes API calls to their service.

148
00:09:36,412 --> 00:09:39,602
But with DVC, there's no API calls.

149
00:09:39,666 --> 00:09:43,302
Everything is right there on your local machine. If youll

150
00:09:43,356 --> 00:09:47,662
decide to set up some kind of remote environment,

151
00:09:47,826 --> 00:09:52,090
you can use DVC there. It works with AWS,

152
00:09:52,830 --> 00:09:56,634
which is the one that I think most people work with when

153
00:09:56,672 --> 00:10:00,038
they're handling their file storage. But you can

154
00:10:00,064 --> 00:10:02,320
use GCP in Azure as well.

155
00:10:03,330 --> 00:10:07,306
The main thing that I really like about DVC,

156
00:10:07,418 --> 00:10:11,802
there's a lot of stuff to it, but my favorite thing is experiments.

157
00:10:11,946 --> 00:10:16,622
So every time you run this DVCep run command,

158
00:10:16,766 --> 00:10:20,274
it takes a snapshot of your code, your data,

159
00:10:20,392 --> 00:10:24,482
and your configurations, and it

160
00:10:24,616 --> 00:10:28,042
stores this as metadata in DVC.

161
00:10:28,206 --> 00:10:31,974
And all of that is attached to whatever model

162
00:10:32,092 --> 00:10:35,478
you produce from this experiments run.

163
00:10:35,644 --> 00:10:39,654
So let's say we've decided to update

164
00:10:39,702 --> 00:10:43,242
a hyperparameter, and we run an experiment with this

165
00:10:43,296 --> 00:10:46,362
command that will be

166
00:10:46,416 --> 00:10:49,990
bundled together with our data and everything else

167
00:10:50,080 --> 00:10:53,854
we already have in place to tie to

168
00:10:53,892 --> 00:10:57,630
the model that we're going two get from that experiment.

169
00:10:58,530 --> 00:11:02,730
So basically, what we're looking at is something like this.

170
00:11:02,900 --> 00:11:06,930
A single experiment has our current data

171
00:11:07,000 --> 00:11:10,642
set that we just used to tracking our model.

172
00:11:10,776 --> 00:11:14,354
It has the hyperparameters that we use

173
00:11:14,392 --> 00:11:18,726
to tracking the model. And again,

174
00:11:18,828 --> 00:11:22,406
it has a model. If maybe you're working with

175
00:11:22,428 --> 00:11:26,562
some from production and youll need to do some kind of comparison.

176
00:11:26,706 --> 00:11:31,206
It's all lumped together in this one experiment,

177
00:11:31,398 --> 00:11:36,470
so you don't have to keep some kind of ridiculous spreadsheet

178
00:11:36,630 --> 00:11:40,334
stashed off to the side where you have a

179
00:11:40,372 --> 00:11:44,602
link to your GitHub commit. For this one hyperparameter

180
00:11:44,666 --> 00:11:49,210
value you changed, and then another link to this zip file

181
00:11:49,290 --> 00:11:52,538
on Google Drive that you can't change because it

182
00:11:52,564 --> 00:11:56,562
was just for this particular experiment. And then you have

183
00:11:56,696 --> 00:12:00,430
another link to some other git repo

184
00:12:00,510 --> 00:12:03,694
that has all of your configurations,

185
00:12:03,822 --> 00:12:08,194
because of course, that's in a separate repo, you don't have two do that anymore.

186
00:12:08,322 --> 00:12:11,362
It's all right there in DVC,

187
00:12:11,506 --> 00:12:15,494
and you can look at your experiments as you

188
00:12:15,532 --> 00:12:19,990
run them. So when you run DVCEP

189
00:12:20,070 --> 00:12:23,014
run, it's going to go through your training script,

190
00:12:23,142 --> 00:12:26,506
it's going to look at whatever dependencies youll gave,

191
00:12:26,608 --> 00:12:30,294
and it's just going to run that experiment. And once it's

192
00:12:30,342 --> 00:12:34,090
finished, you can take a look at the results from that experiment

193
00:12:34,170 --> 00:12:37,214
and decide which way you want to go from there.

194
00:12:37,332 --> 00:12:41,594
So in this example, we have some experiment

195
00:12:41,642 --> 00:12:44,734
we run. It has an average precision, and this rock

196
00:12:44,782 --> 00:12:48,530
AUC value and a couple of hyperparameters.

197
00:12:48,870 --> 00:12:51,620
So the average precision looks really good,

198
00:12:52,310 --> 00:12:56,802
but we want to do some more hyperparameter tuning

199
00:12:56,866 --> 00:13:00,120
because we think we can get something better

200
00:13:01,050 --> 00:13:04,486
and what we'll do, which is pretty common, we're going

201
00:13:04,508 --> 00:13:08,234
to set up a queue of experiments. So we

202
00:13:08,272 --> 00:13:12,454
have a bunch of different hyperparameter values

203
00:13:12,582 --> 00:13:17,510
that you can see over here under our train nest

204
00:13:17,670 --> 00:13:20,938
and our train min split columns.

205
00:13:21,114 --> 00:13:24,378
These are all the different hyperparameter values

206
00:13:24,474 --> 00:13:28,910
that we want to test out for this

207
00:13:28,980 --> 00:13:32,618
particular project. So we've queued

208
00:13:32,634 --> 00:13:36,402
up these experiments in DVC, and you'll see they gave

209
00:13:36,456 --> 00:13:39,714
their own ids associated with them, and they

210
00:13:39,752 --> 00:13:43,282
are in the queued state. We don't have any results yet

211
00:13:43,336 --> 00:13:47,250
because these haven't run. One really big advantage

212
00:13:47,330 --> 00:13:51,206
of queuing experiments like this is not only can

213
00:13:51,228 --> 00:13:54,674
you see the values before you run the experiments,

214
00:13:54,802 --> 00:13:58,234
you're also able to push these

215
00:13:58,272 --> 00:14:01,834
experiments off to some cloud environment. If you

216
00:14:01,872 --> 00:14:05,514
want to run them on a different server, use a

217
00:14:05,552 --> 00:14:09,274
GPU or some other resources. So we have

218
00:14:09,312 --> 00:14:12,558
those experiments queued and now we're going

219
00:14:12,564 --> 00:14:17,018
to run them all. So we'll use this exp run command

220
00:14:17,114 --> 00:14:21,034
and then we'll use this DVCEP show command

221
00:14:21,162 --> 00:14:24,606
to take a look at the results from all of

222
00:14:24,628 --> 00:14:28,594
those queued experiments. Now you

223
00:14:28,632 --> 00:14:32,414
can see all of the different experiments

224
00:14:32,462 --> 00:14:36,802
that were run with all of the different hyperparameter

225
00:14:36,866 --> 00:14:41,346
values. And you can see all of the results.

226
00:14:41,538 --> 00:14:45,222
Now take a second and imagine if youll

227
00:14:45,276 --> 00:14:48,358
had two document all of this manually.

228
00:14:48,534 --> 00:14:52,902
So you have to go back in that spreadsheet

229
00:14:52,966 --> 00:14:57,126
or in some kind of document and manually

230
00:14:57,238 --> 00:15:01,066
say when I had an nest of

231
00:15:01,088 --> 00:15:04,526
26 and 355 and a min split of

232
00:15:04,548 --> 00:15:08,606
355, these were my outputs. Then you

233
00:15:08,628 --> 00:15:11,966
have to attach the data somewhere. You have to attach the

234
00:15:11,988 --> 00:15:15,458
code somewhere. Now you don't have to worry about

235
00:15:15,544 --> 00:15:18,606
how everything is being track.

236
00:15:18,798 --> 00:15:22,302
All you have to do is take a look at this table,

237
00:15:22,446 --> 00:15:26,326
decide which experiments you want to keep working with,

238
00:15:26,428 --> 00:15:29,160
which experiments you want to share with other people.

239
00:15:29,610 --> 00:15:33,190
And you already have these results here just

240
00:15:33,260 --> 00:15:36,054
to share and look at at any time.

241
00:15:36,252 --> 00:15:39,930
No more managing all of those things separately.

242
00:15:40,510 --> 00:15:43,866
So let's say that you want

243
00:15:43,888 --> 00:15:47,002
to make some kind of plot to compare a couple of

244
00:15:47,056 --> 00:15:50,282
experiments, because you see some that just have some

245
00:15:50,336 --> 00:15:53,402
interesting results. Maybe you want to get a second opinion

246
00:15:53,466 --> 00:15:56,606
from somebody else on the team. So we have

247
00:15:56,628 --> 00:16:00,890
this DVC plots command. You can take the experiment

248
00:16:00,970 --> 00:16:04,610
ids you want to compare and it generates this

249
00:16:04,680 --> 00:16:08,718
plot for you based on the parameters

250
00:16:08,814 --> 00:16:13,106
you define. So all of this data is coming

251
00:16:13,208 --> 00:16:16,494
from either some kind of JSON file

252
00:16:16,542 --> 00:16:20,594
or tsv is generated typically

253
00:16:20,722 --> 00:16:24,610
within your training script. So DVC

254
00:16:24,690 --> 00:16:28,162
isn't adding anything new, it's just using

255
00:16:28,236 --> 00:16:32,186
the information that you provided with. But you're able

256
00:16:32,208 --> 00:16:35,786
to quickly generate these plots. And again, I want you

257
00:16:35,808 --> 00:16:39,882
to think about if you had that many experiments you had

258
00:16:39,936 --> 00:16:44,000
run and you wanted to create a plot like this,

259
00:16:44,770 --> 00:16:48,750
how much effort would it take to actually do that?

260
00:16:48,900 --> 00:16:52,630
I'm pretty sure it would take a little bit more than just one command.

261
00:16:52,810 --> 00:16:56,370
And yeah, it's just easy

262
00:16:56,440 --> 00:16:59,886
to use some of the tools that are already built to handle

263
00:16:59,918 --> 00:17:03,026
this stuff for us, but we're not done.

264
00:17:03,128 --> 00:17:06,758
We have more hyperparameter values to try out, of course.

265
00:17:06,924 --> 00:17:11,222
So we're going to queue up a few more experiments this time.

266
00:17:11,356 --> 00:17:15,414
Let's just say we're doing a random search, but we

267
00:17:15,452 --> 00:17:19,110
see the values is jumping around. This hasn't been anything

268
00:17:19,180 --> 00:17:23,180
we've tried before. Let's just see what we get.

269
00:17:23,710 --> 00:17:26,874
So we'll go ahead and run all of the experiments we had

270
00:17:26,912 --> 00:17:30,150
queued up, and we'll take a look at our table again.

271
00:17:30,320 --> 00:17:33,470
And now you see these new values.

272
00:17:34,210 --> 00:17:37,834
Well, it still looks like one of our earlier experiments

273
00:17:37,882 --> 00:17:41,742
gave us better results. But we might

274
00:17:41,796 --> 00:17:45,474
not need to use numbers quite as big as we

275
00:17:45,512 --> 00:17:49,090
did in the first experiment. So just being

276
00:17:49,160 --> 00:17:52,834
able to quickly look at these metrics shows you

277
00:17:52,872 --> 00:17:56,258
which direction you should take your hyperparameter tuning.

278
00:17:56,354 --> 00:18:00,118
Or maybe it tells you that it's time to try

279
00:18:00,204 --> 00:18:03,394
a different algorithm, or maybe it's

280
00:18:03,442 --> 00:18:07,634
time to try a different data set. Or youll need to slice

281
00:18:07,682 --> 00:18:10,810
up your data set different, or youll need new data points.

282
00:18:10,960 --> 00:18:14,460
But whatever it is, your next step will be.

283
00:18:14,830 --> 00:18:18,938
This is a very quick and easy way to see

284
00:18:19,024 --> 00:18:23,040
how youll experiments should guide your model training.

285
00:18:23,650 --> 00:18:27,546
And of course, we have to do hyperparameter tuning

286
00:18:27,658 --> 00:18:31,134
one more time, because we have all

287
00:18:31,172 --> 00:18:34,546
of these different experiments to run. We have all of

288
00:18:34,568 --> 00:18:38,926
these different values to try, and it's

289
00:18:38,958 --> 00:18:43,218
not uncommon to run hundreds of experiments in a day for

290
00:18:43,384 --> 00:18:46,686
a machine learning engineer or a data scientist.

291
00:18:46,878 --> 00:18:50,726
So we're using to queue up a few more experiments to see

292
00:18:50,908 --> 00:18:54,422
maybe how low we can get those values. Or maybe

293
00:18:54,476 --> 00:18:57,746
we just have another theory we want to test out from showing our

294
00:18:57,788 --> 00:19:00,506
results to somebody else on the team.

295
00:19:00,688 --> 00:19:03,866
And again, we'll run our experiments and look at the

296
00:19:03,888 --> 00:19:07,446
table, and we see some promise.

297
00:19:07,558 --> 00:19:11,920
So this one looks a little bit better than the previous one,

298
00:19:12,370 --> 00:19:15,834
and these values are definitely a lot smaller.

299
00:19:15,962 --> 00:19:20,174
So maybe we're getting a better feel for the

300
00:19:20,212 --> 00:19:23,540
range of the hyperparameter values, or maybe

301
00:19:23,910 --> 00:19:27,300
which hyperparameter values are the most important.

302
00:19:27,750 --> 00:19:32,100
So with something like DVC, you're just able to

303
00:19:32,790 --> 00:19:36,306
do this kind of whimsical

304
00:19:36,498 --> 00:19:40,150
experimentation without worrying about taking

305
00:19:40,220 --> 00:19:43,894
notes every 2 seconds. You're able to focus

306
00:19:44,012 --> 00:19:48,186
on finding that good model instead of

307
00:19:48,368 --> 00:19:52,570
having this eureka moment and no idea how

308
00:19:52,640 --> 00:19:54,700
to get back to it.

309
00:19:55,870 --> 00:19:59,466
And just to make sure that we

310
00:19:59,568 --> 00:20:02,954
are not crazy and we're looking at our values

311
00:20:03,002 --> 00:20:07,790
correctly, we might take another look at some plots just

312
00:20:07,860 --> 00:20:11,566
to see if these experiments are going in the

313
00:20:11,588 --> 00:20:15,938
direction we think they should. So you might share these with somebody

314
00:20:16,024 --> 00:20:19,250
else on the team. They might just be for you to get

315
00:20:19,320 --> 00:20:23,346
a range of what you should be expecting or what you

316
00:20:23,368 --> 00:20:27,174
should do next. Either way, DVC just makes it

317
00:20:27,212 --> 00:20:30,710
easy to do that and play around with your metrics in whatever

318
00:20:30,780 --> 00:20:34,406
way you need to. So these are all

319
00:20:34,428 --> 00:20:37,942
of the experiments that we've run over the course of this

320
00:20:37,996 --> 00:20:41,386
talk. And actually, these aren't even all of

321
00:20:41,408 --> 00:20:45,158
them. These are a few of them just from this table.

322
00:20:45,334 --> 00:20:48,442
So, as you can see, there's a lot of

323
00:20:48,496 --> 00:20:51,440
experiments that we can really fast,

324
00:20:52,210 --> 00:20:55,982
and we didn't have to keep track of all of it. You can

325
00:20:56,036 --> 00:20:59,230
see right here in the table that DVC has

326
00:20:59,300 --> 00:21:02,874
tracked every hyperparameter combination we've

327
00:21:02,922 --> 00:21:05,714
run, and we don't have to worry about it.

328
00:21:05,832 --> 00:21:09,154
With each experiments, it's taken a snapshot of the

329
00:21:09,192 --> 00:21:12,494
code and the data set that we have associated

330
00:21:12,542 --> 00:21:17,538
with it, and it's created those little bundles with our hyperparameters,

331
00:21:17,714 --> 00:21:21,346
our data, and our code to associate

332
00:21:21,458 --> 00:21:25,426
with each model produced by each experiment.

333
00:21:25,618 --> 00:21:28,794
And basically what that means, if you wanted to

334
00:21:28,832 --> 00:21:32,982
go back and redo any of these experiments,

335
00:21:33,126 --> 00:21:36,810
all you need is the experiment id over here,

336
00:21:36,960 --> 00:21:40,414
a few DVC commands, and you

337
00:21:40,452 --> 00:21:43,822
have the exact reproduction of

338
00:21:43,876 --> 00:21:47,150
the conditions that led up to the model. That is

339
00:21:47,220 --> 00:21:51,034
just this awesome thing you need to get out to production

340
00:21:51,082 --> 00:21:54,450
right now. But I hope

341
00:21:54,520 --> 00:21:58,370
that you see just how we can solve some of those

342
00:21:58,440 --> 00:22:02,500
problems that are in the machine learning community,

343
00:22:02,870 --> 00:22:06,434
and how we can use tools that already exist

344
00:22:06,562 --> 00:22:10,214
to do this heavy lifting for us. Please don't use

345
00:22:10,252 --> 00:22:13,906
spreadsheets to keep up with your machine

346
00:22:13,938 --> 00:22:17,254
learning experiments when we have stuff that'll do it for

347
00:22:17,292 --> 00:22:21,386
you now. But there are a few key takeaways that

348
00:22:21,408 --> 00:22:24,186
I hope you get from this. First,

349
00:22:24,368 --> 00:22:28,010
adding reproducibility to your experiments is

350
00:22:28,080 --> 00:22:31,614
important when it's time to deploy your

351
00:22:31,652 --> 00:22:35,182
model two production, you want to make sure that

352
00:22:35,236 --> 00:22:38,618
you have the exact same accuracy

353
00:22:38,714 --> 00:22:42,110
and the exact same metrics that you

354
00:22:42,180 --> 00:22:45,714
had while you were testing in production, so that

355
00:22:45,752 --> 00:22:50,020
there isn't any weirdness happening. And you need to roll everything back.

356
00:22:50,950 --> 00:22:54,962
And DVC is just one of the tools that helps you track every part

357
00:22:55,016 --> 00:22:58,706
of your experiments. Of course, there's still Mlflow

358
00:22:58,738 --> 00:23:01,846
and some others in the ML ops area,

359
00:23:02,028 --> 00:23:05,366
but you always want to make sure you have some kind

360
00:23:05,388 --> 00:23:10,102
of tool that's tracking every part of your experiments.

361
00:23:10,246 --> 00:23:13,994
DVC is, at least from what I've seen

362
00:23:14,032 --> 00:23:18,282
around, it's probably the best one. Just because it

363
00:23:18,416 --> 00:23:22,026
tracks your data changes too. So when you're

364
00:23:22,058 --> 00:23:25,550
dealing with data drift in production,

365
00:23:25,970 --> 00:23:29,802
you still have the exact copies

366
00:23:29,866 --> 00:23:33,374
of those data sets before the drift happened.

367
00:23:33,492 --> 00:23:37,026
So if there's any research you need to do that you can check it out.

368
00:23:37,128 --> 00:23:40,274
If there's anything you want to go back to and refer

369
00:23:40,392 --> 00:23:43,986
in your model, you can check it out. DVC just does all

370
00:23:44,008 --> 00:23:47,814
of this for you, and then don't be

371
00:23:47,852 --> 00:23:51,494
afraid to try new tools. I know a lot of

372
00:23:51,532 --> 00:23:55,494
times those of us who write code feel a

373
00:23:55,532 --> 00:23:59,482
need to build our own tools for every issue that

374
00:23:59,536 --> 00:24:03,878
pops up. Youll don't gave to do that. It's not cheating

375
00:24:03,974 --> 00:24:07,434
to use tools that are already there for youll it

376
00:24:07,472 --> 00:24:11,002
makes you faster, it makes it easier for you

377
00:24:11,056 --> 00:24:14,654
to have an impact, and it takes a lot of

378
00:24:14,692 --> 00:24:18,398
stress off when there's already tools out there.

379
00:24:18,564 --> 00:24:22,174
Even if you spend an hour or two and it's not quite

380
00:24:22,212 --> 00:24:26,126
what you're looking for, it's at least good to know that they exist

381
00:24:26,238 --> 00:24:30,020
just in case something pops up and you need it later.

382
00:24:31,030 --> 00:24:34,834
And I want to leave you with a few resources, so if

383
00:24:34,872 --> 00:24:38,214
you're interested in DVC, make sure to check out our

384
00:24:38,252 --> 00:24:42,370
docs. We have a very active discord

385
00:24:42,450 --> 00:24:46,054
channel, so if you want to drop in, ask some

386
00:24:46,092 --> 00:24:49,566
questions, say what's up to the mlops

387
00:24:49,618 --> 00:24:53,178
community, feel free to do that. And if you want

388
00:24:53,184 --> 00:24:56,650
to see a more gui type

389
00:24:56,720 --> 00:25:00,570
version of DVC, head to DVC studio

390
00:25:00,650 --> 00:25:04,320
here at Studio iterative AI and

391
00:25:04,770 --> 00:25:08,462
check it out. Hook up your GitHub repo and start

392
00:25:08,516 --> 00:25:11,742
running experiments. And of course,

393
00:25:11,796 --> 00:25:15,594
if you want to see these slides, you can go to my speaker

394
00:25:15,642 --> 00:25:19,280
deck link here and download them and

395
00:25:19,650 --> 00:25:22,926
get whatever you need. So thank you and

396
00:25:22,948 --> 00:25:25,894
I hope that this talk was useful for for you.

