1
00:00:25,410 --> 00:00:28,594
You. Good morning, good afternoon,

2
00:00:28,642 --> 00:00:32,290
good evening everyone. Welcome to Conf fourty two or Java.

3
00:00:32,370 --> 00:00:35,362
Today I'm going to be sharing some really cool stuff about garbarge collection,

4
00:00:35,426 --> 00:00:38,690
so let's get on to it. These are some important disclaimers

5
00:00:38,770 --> 00:00:42,198
saying that this prestigious for information purposes. So this

6
00:00:42,284 --> 00:00:45,734
is the rough outline of what I'm going to be going through with you today.

7
00:00:45,772 --> 00:00:49,366
IBM going to start with the garbage collection overview. Going to start slowly and

8
00:00:49,388 --> 00:00:53,066
then go into them the more details with the OpenJ nine GC

9
00:00:53,098 --> 00:00:56,554
algorithms and I'm going to finish off with advanced structure

10
00:00:56,602 --> 00:01:00,122
called double mapping. A little bit about me I'm a software developer here at IBM.

11
00:01:00,186 --> 00:01:03,806
IBM almost for almost two years. I graduated from

12
00:01:03,908 --> 00:01:07,860
University of Waterloo. My master is in 2019 and

13
00:01:08,710 --> 00:01:11,646
one of my interests are systems, compilers,

14
00:01:11,678 --> 00:01:14,526
machine learning and AI. And a fun fact about me is that I'm a tennis

15
00:01:14,558 --> 00:01:18,390
aggregate. All right, if you want to get the builds for

16
00:01:18,460 --> 00:01:22,418
either hotspot OPG nine, I recommend going to adopt OpenJDK.

17
00:01:22,514 --> 00:01:25,602
There you can get both builds and terms.

18
00:01:25,746 --> 00:01:28,882
So you sure whats you have the best jvm

19
00:01:28,946 --> 00:01:32,554
and garbage collection which is the main part of the talk. Objects nine is

20
00:01:32,592 --> 00:01:35,578
a fairly mature project. It went to the open 2017.

21
00:01:35,584 --> 00:01:39,446
A fun fact about it is that it started as an embedded

22
00:01:39,478 --> 00:01:43,066
jvm. If familiar with Java Macro edition. That's how OPG

23
00:01:43,098 --> 00:01:46,874
nine started, which is why it has some really good memory performance

24
00:01:46,922 --> 00:01:50,078
when it comes to memory and it's open community.

25
00:01:50,164 --> 00:01:53,550
We welcome anyone who's interested and wants to contribute to the project.

26
00:01:53,620 --> 00:01:56,866
All right, garbage collection. So what is it? This is a

27
00:01:56,888 --> 00:02:00,494
more by the book definition. Garbarge collection is a form of automatic

28
00:02:00,542 --> 00:02:04,066
memory management. The garbage collect attempts to reclaim memory occupied by

29
00:02:04,088 --> 00:02:07,794
objects that are no longer used by the application. That's opposed to

30
00:02:07,912 --> 00:02:11,826
other programming languages like C Plus plus where you have to manually allocated

31
00:02:11,858 --> 00:02:15,526
memory with new and malloc and free those memory with free

32
00:02:15,548 --> 00:02:19,634
and delete. So what are the responsibilities of garbarge collection? They are allocate

33
00:02:19,682 --> 00:02:23,894
memory, identify the liveness of data and everything else is going to be considered garbage.

34
00:02:23,942 --> 00:02:27,530
Therefore the garbage collection is going to reclaim. So these are the most important

35
00:02:27,600 --> 00:02:31,302
responsibilities of the garbarge collection positives GCs.

36
00:02:31,366 --> 00:02:34,762
They are automatic managed automatic memory management.

37
00:02:34,826 --> 00:02:38,766
So it frees the burden of the developer to manually have to

38
00:02:38,788 --> 00:02:42,366
deal with memory. Also helps reduce some certain categories of

39
00:02:42,388 --> 00:02:46,018
bugs like dangling pointers and double freeze. On the other hand,

40
00:02:46,104 --> 00:02:49,762
there is the negative part, right? It requires some additional resources like

41
00:02:49,816 --> 00:02:54,158
additional cpu cycles. More memory can cause unpredictable pauses.

42
00:02:54,254 --> 00:02:57,342
You will see that they are known as stop the road pauses.

43
00:02:57,406 --> 00:03:00,758
It may introduce runtime costs and application. Whats little control

44
00:03:00,844 --> 00:03:04,386
of when memory is reclaimed. Here are some of the digital algorithms.

45
00:03:04,418 --> 00:03:07,494
I'm going to be explaining each of these as I go

46
00:03:07,532 --> 00:03:10,798
through the presentation. One, and that I won't be talking about, is reference counting,

47
00:03:10,834 --> 00:03:14,314
but in simple terms. Imagine I'm an object and I have

48
00:03:14,352 --> 00:03:17,482
two objects that's pointed to me. My reference count is going to be two.

49
00:03:17,536 --> 00:03:21,034
As soon as these objects deletes the references to me,

50
00:03:21,072 --> 00:03:24,670
my reference counts goes to zero. So pretty much I'm counting how many

51
00:03:24,740 --> 00:03:28,074
references I have, and once that count reaches zero, I'm considered garbage.

52
00:03:28,122 --> 00:03:32,426
Therefore I can be reclaimed. That's reference counting in 30 seconds.

53
00:03:32,618 --> 00:03:35,810
All right, this is what I'll be going through with you today.

54
00:03:35,960 --> 00:03:39,566
These are the different policies regarding open, unite first one's

55
00:03:39,598 --> 00:03:43,070
op throughput, the stop the world pilot collector, the second Gencon

56
00:03:43,150 --> 00:03:46,306
generational copy collector. Gencon Cs is an improvement of

57
00:03:46,328 --> 00:03:49,586
a Gencon's apostles collector. Metronome is an incremental,

58
00:03:49,618 --> 00:03:53,106
soft, real time collector and balance, the region based collector balance

59
00:03:53,138 --> 00:03:56,486
is really close to g one and John cone is really close to

60
00:03:56,588 --> 00:04:00,226
cs from hotspot optroput. So optroput is a parallel garbarge

61
00:04:00,258 --> 00:04:03,030
collection. GC operations are completely stop the world pauses,

62
00:04:03,110 --> 00:04:06,666
meaning all the application threads are paused. Marksweep and

63
00:04:06,688 --> 00:04:10,234
optional copat collector. I'll explain whats in more detail a little bit.

64
00:04:10,272 --> 00:04:14,522
And all GC operations are completed in parallel, meaning multiple threads. Garbarge collection

65
00:04:14,666 --> 00:04:17,694
the heat and GC native memory overhead for macmap and work

66
00:04:17,732 --> 00:04:20,970
packets. These are metadata needed by the GC

67
00:04:21,050 --> 00:04:24,990
to collect some garbage. So whenever you start an application,

68
00:04:25,140 --> 00:04:28,086
we allocate objects in what we call the heat.

69
00:04:28,138 --> 00:04:31,826
Normally is a contiguous block of memory, but it doesn't necessarily whats

70
00:04:31,848 --> 00:04:34,606
to be, but over 90% of the time it's going to be a contiguous block

71
00:04:34,638 --> 00:04:37,954
of memory. So we have a flat heap, but for optuput we further

72
00:04:38,002 --> 00:04:41,462
divide into two logical spaces, small object area and large

73
00:04:41,516 --> 00:04:45,318
object area. Most objects are allocated in the small object area,

74
00:04:45,404 --> 00:04:48,726
so this is how an application behaves under this policy.

75
00:04:48,828 --> 00:04:52,506
The white arrows represent the application thread and the red arrows represent the

76
00:04:52,528 --> 00:04:55,866
GC thread. So application starts right, and as soon as we

77
00:04:55,888 --> 00:04:59,066
hit an allocation failure, we deplete the memory of

78
00:04:59,088 --> 00:05:02,982
the heap. We start a GC cycle. That's the red arrows there. So GC

79
00:05:03,046 --> 00:05:06,574
collects the heap while all application threads stop. That's why

80
00:05:06,612 --> 00:05:09,966
it's called stop the road, and then the application threads can continue from

81
00:05:09,988 --> 00:05:13,514
there. So the GC is divided into three phases, marking, sweeping,

82
00:05:13,562 --> 00:05:16,954
collection, so marking where it's where we find all the live objects.

83
00:05:17,002 --> 00:05:20,414
So just go through the object graph, finding all the live objects.

84
00:05:20,462 --> 00:05:24,126
Then we sweep the dead objects, meaning all those objects

85
00:05:24,158 --> 00:05:27,874
that are not marked are dead objects, therefore we sweep them.

86
00:05:27,912 --> 00:05:31,446
And then there's a compaction phase because the heap might get fragmented. So what does

87
00:05:31,468 --> 00:05:35,078
it mean, a heap to get fragmented? So imagine through the lifetime of an application

88
00:05:35,164 --> 00:05:38,566
when we have many DC cycles, our heap might

89
00:05:38,588 --> 00:05:42,250
get like a swiss cheese, meaning it's going to have very little

90
00:05:42,320 --> 00:05:46,118
small free spaces, but maybe not enough contiguous free space to allocate

91
00:05:46,134 --> 00:05:50,486
an object. Therefore we need to compact that heap. This is what this slide

92
00:05:50,518 --> 00:05:54,654
is trying to explain, where we take all those live objects, put in some

93
00:05:54,692 --> 00:05:58,126
part of the heap, and now we have all that contiguous memory that

94
00:05:58,148 --> 00:06:02,314
we can work with. Jenco if IBM familiar with cs

95
00:06:02,442 --> 00:06:06,138
from hotspot. It's really similar to the generational copy collector,

96
00:06:06,234 --> 00:06:09,486
where objects have edges provides a significant reduction

97
00:06:09,518 --> 00:06:13,026
in the assist of the repulse times. Introduces right barrier for a member set and

98
00:06:13,048 --> 00:06:17,134
we have a cocurrent global marking phase where now we're going to be marking objects

99
00:06:17,182 --> 00:06:21,078
concurrently while the application track is running. So here, instead of dividing the

100
00:06:21,084 --> 00:06:24,486
heap into small object area and large object area, we divide the

101
00:06:24,508 --> 00:06:28,454
heap into nursery and tenure. In other words, nursery is a new space,

102
00:06:28,572 --> 00:06:32,002
tenure is old space. Furthermore, we divide nursery into allocate

103
00:06:32,066 --> 00:06:35,414
and survivor, and all objects are actually locating

104
00:06:35,462 --> 00:06:38,794
allocated space. This is how it behaves though. Don't panic. So again,

105
00:06:38,832 --> 00:06:41,962
the white arrows represent the application threads and red,

106
00:06:42,016 --> 00:06:45,722
the GC thread. Scavenge here represents a copy collector.

107
00:06:45,786 --> 00:06:49,534
So what is a cop collector? Cop collector means it's copying objects from

108
00:06:49,572 --> 00:06:53,066
one space to the next. So it's copying objects from allocate to survivor.

109
00:06:53,098 --> 00:06:56,446
So when we hit the limit of the allocate space,

110
00:06:56,628 --> 00:07:00,162
we start the scavenge, which is top the word copy collector. And now we start

111
00:07:00,216 --> 00:07:03,746
copying objects from scavenge to survivor. That's what scavenge is

112
00:07:03,768 --> 00:07:07,150
doing. Global mark face is concurrent

113
00:07:07,230 --> 00:07:10,418
mark phase, where it's actually marking the tenure space,

114
00:07:10,504 --> 00:07:13,846
because scavenge is only dealing with a new space. So how do we deal with

115
00:07:13,868 --> 00:07:17,078
the old space? That's what global mark phase is doing, marking the

116
00:07:17,084 --> 00:07:20,358
tenure space concurrently. And at the end, at the global end,

117
00:07:20,444 --> 00:07:24,266
we sweep the tenure space to collect those original tenure space.

118
00:07:24,368 --> 00:07:27,994
So because of this concurrency, we need what we call write

119
00:07:28,032 --> 00:07:32,538
barriers. So imagine we are trying to store a reference

120
00:07:32,634 --> 00:07:35,674
into an object field like that, and imagine

121
00:07:35,722 --> 00:07:39,706
we have the following object graph. We are in the middle of marking

122
00:07:39,738 --> 00:07:43,994
objects and applications running. So GC has already processed the roots.

123
00:07:44,042 --> 00:07:47,422
GC has already processed object a. As you see with the green tick,

124
00:07:47,486 --> 00:07:51,570
GC is in the middle processing object b and hasn't seen c yet.

125
00:07:51,640 --> 00:07:54,946
Then this happens. Mutated thread comes along and

126
00:07:54,968 --> 00:07:58,466
deletes pointer from b to c and adds a reference from

127
00:07:58,488 --> 00:08:01,938
a to c. And then the GC thread comes along and okay, b doesn't

128
00:08:01,954 --> 00:08:05,506
have any pointers, so I'm done marking. Therefore this is my resulting

129
00:08:05,538 --> 00:08:08,934
graph where I found roots. Object a, object b,

130
00:08:08,972 --> 00:08:12,646
object C is normal, therefore it's considered garbage. So IBM going to collect erroneously.

131
00:08:12,678 --> 00:08:15,994
So that's a problem. So how do we remember, how can we remember object c?

132
00:08:16,032 --> 00:08:19,706
We do that through write barriers. So how this is

133
00:08:19,728 --> 00:08:23,238
implemented. So we have a simply check there if it's concurrent

134
00:08:23,254 --> 00:08:26,734
disease is active. We remember that object. So we put that in a car

135
00:08:26,772 --> 00:08:30,602
table so that at the beginning of the GC cycle

136
00:08:30,666 --> 00:08:34,240
we actually go to the car table to look for these.

137
00:08:34,850 --> 00:08:37,746
Sorry, it's not the beginning. At the end we go through the car table to

138
00:08:37,768 --> 00:08:40,910
remember these references that we might have missed.

139
00:08:40,990 --> 00:08:44,242
But we do need an extra barrier because

140
00:08:44,376 --> 00:08:47,778
we remember we have new space and old space,

141
00:08:47,864 --> 00:08:51,078
but we might have references going from old space

142
00:08:51,164 --> 00:08:55,014
to new space. And scavenger is not aware of that. So in order to

143
00:08:55,052 --> 00:08:58,934
capture these references coming from old space to new space, we need another right

144
00:08:58,972 --> 00:09:02,486
barrier. And this right barrier looks like this. All we do is check if a

145
00:09:02,508 --> 00:09:06,186
is in old space, a is tenured and c is not. Meaning a is an

146
00:09:06,208 --> 00:09:09,754
old space, c is in a new space. Then we also remember that since

147
00:09:09,792 --> 00:09:12,862
we have two barriers here, we can put that in the same check to save

148
00:09:12,916 --> 00:09:16,366
some cpu cycles there. And that's the resulting right barrier for

149
00:09:16,388 --> 00:09:19,738
Gencon. Cool. Gencon concurrent scavenger,

150
00:09:19,834 --> 00:09:23,534
that's an improvement upon Gencon. This is a puzzleless collector. If you're familiar with

151
00:09:23,572 --> 00:09:26,926
Shenandoah and ZGC, those are also puzzles collection.

152
00:09:26,958 --> 00:09:31,022
They are some differences there, but concurrent scavenger is very similar to Gencon

153
00:09:31,086 --> 00:09:34,546
here. The generation cup collector introduced a readbearer because now

154
00:09:34,648 --> 00:09:38,390
we're going to be moving objects concurrently. Not just marking, but also

155
00:09:38,460 --> 00:09:41,766
moving objects. The heap structure is the same, right? We have

156
00:09:41,788 --> 00:09:44,998
the new space, old space, allocate, survivor, nothing new there. So the

157
00:09:45,004 --> 00:09:48,898
biggest difference here is the scavenger, which is instead of being stopped,

158
00:09:48,914 --> 00:09:52,662
the world now is going to be concurrent. The CS there stands for concurrent scavenger.

159
00:09:52,726 --> 00:09:56,778
So application starts instead of having a stop the world scavenger. We're going

160
00:09:56,784 --> 00:10:00,186
to have two small pauses, one at the beginning, one at the

161
00:10:00,208 --> 00:10:04,046
end for like synchronization, marking the roots. And the bulk of the

162
00:10:04,068 --> 00:10:07,326
work is going to be done concurrently with the application, which is where

163
00:10:07,348 --> 00:10:11,770
you see the yellow arrows there which represent application threads running concurrently

164
00:10:11,850 --> 00:10:15,518
with the application. And global marking phase here is still the

165
00:10:15,524 --> 00:10:18,834
same, right. The only thing is that the cop collector now is running

166
00:10:18,872 --> 00:10:22,850
concurrently with the application. Now we have a problem

167
00:10:22,920 --> 00:10:26,766
here, right, because the GC thread is trying to move objects while mutator

168
00:10:26,798 --> 00:10:30,294
is trying to access those same objects. So let's step

169
00:10:30,332 --> 00:10:33,590
back a little bit. Let's think about the parallel scavenger, right?

170
00:10:33,660 --> 00:10:37,442
Parallel scavenger is a stop toward scavenger. So I have multiple GC threads

171
00:10:37,506 --> 00:10:41,414
trying to move an object, right? So we can deal with that with an atomic

172
00:10:41,462 --> 00:10:44,954
operation, cas cop and swap, right? So let's see what happens in this

173
00:10:44,992 --> 00:10:48,410
case. So we have two DC threads trying to move an object,

174
00:10:48,480 --> 00:10:52,186
right? The green and brownish thread here. So what would

175
00:10:52,208 --> 00:10:55,518
happen in this case? So they both try to copy the object,

176
00:10:55,604 --> 00:10:58,718
right. So they race to cop the object. So in this case

177
00:10:58,804 --> 00:11:02,366
the brown one wins the race. Copy the object, install the

178
00:11:02,388 --> 00:11:05,422
foreign pointer, the green thread is going to be. Okay, I lost all good.

179
00:11:05,476 --> 00:11:08,754
I'll just continue do my work and cop some other objects. So far

180
00:11:08,792 --> 00:11:12,754
so good. Now what if we had a mutator in a DGC thread trying

181
00:11:12,792 --> 00:11:15,950
to do that? So let's keep this same Cas

182
00:11:16,030 --> 00:11:19,234
operation and see what happens. Right.

183
00:11:19,352 --> 00:11:22,486
So we have the same cas there, but instead of two GC threads we have

184
00:11:22,508 --> 00:11:25,558
a GC thread which is the green one, and a motel thread. Okay,

185
00:11:25,644 --> 00:11:28,600
so let's see what happens. So in this case,

186
00:11:29,290 --> 00:11:33,114
let's assume the GC thread won the race which copies the

187
00:11:33,152 --> 00:11:36,634
object. And then the motra thread is going to come along say

188
00:11:36,672 --> 00:11:40,026
okay, I lost the race, but I see a foreign pointer is going to

189
00:11:40,048 --> 00:11:43,438
follow that to access that object in the two space. But since

190
00:11:43,524 --> 00:11:46,414
GC thread's not done copying the object,

191
00:11:46,532 --> 00:11:49,594
the motel thread is going to access some garbage memory

192
00:11:49,642 --> 00:11:52,878
and that's a problem. So how do we fix that? That's how we fix it.

193
00:11:52,884 --> 00:11:56,382
So what we do is we unconditionally copy

194
00:11:56,446 --> 00:11:59,714
the object, right? So we copy object. So both

195
00:11:59,752 --> 00:12:03,794
is going to copy the object and what they do is they erase to

196
00:12:03,832 --> 00:12:07,186
install the foreign pointer. So how does this work actually?

197
00:12:07,288 --> 00:12:10,638
So both is going to copy the object, right? So that's going to be the

198
00:12:10,664 --> 00:12:13,478
resulting at the beginning. Both is going to have their own copy. And here at

199
00:12:13,484 --> 00:12:16,680
this point they're going to race to install the foreign pointer in the

200
00:12:17,370 --> 00:12:21,162
foreign space copy. So in this case, the GC thread won. The race to install

201
00:12:21,216 --> 00:12:24,090
the foreign porter material thread is going to be okay, I lost, but no matter,

202
00:12:24,160 --> 00:12:29,082
I'm just going to go to the foreign pointer to access that

203
00:12:29,136 --> 00:12:32,486
object. But since the G thread have already copied

204
00:12:32,518 --> 00:12:36,106
the object, the object, everything is fine because the

205
00:12:36,128 --> 00:12:39,626
object's there. The GC thread is done on copying the object, so everybody's

206
00:12:39,658 --> 00:12:43,278
happy. So that's what we call a read barrier. You're familiar with Shenandoa

207
00:12:43,294 --> 00:12:46,978
version two? They use the same barrier as here.

208
00:12:47,064 --> 00:12:50,210
Okay, metronome. Metronome is an

209
00:12:50,360 --> 00:12:54,126
incremental, soft real time collector. It provides an upper bound on GC

210
00:12:54,158 --> 00:12:57,662
post times. What I mean by upper bound is that imagine

211
00:12:57,726 --> 00:13:01,102
a window of ten milliseconds on that window.

212
00:13:01,166 --> 00:13:04,358
We guarantee that 7% of that time is going to

213
00:13:04,364 --> 00:13:07,782
be dedicated to the application, while the other 30% is going to be for

214
00:13:07,836 --> 00:13:11,526
the GC. So it's up to the GC to do with those 30%. Whatever he

215
00:13:11,548 --> 00:13:15,114
wants to do can do some GC work or give some time

216
00:13:15,152 --> 00:13:18,566
back to the application. Here uses a different write barrier called snapshot

217
00:13:18,598 --> 00:13:21,126
at the beginning. And because of this barrier,

218
00:13:21,238 --> 00:13:24,554
it has a lot, a high percentage of floating

219
00:13:24,602 --> 00:13:28,462
garbage, and you're going to see why. So metronome heap is not divided into new

220
00:13:28,516 --> 00:13:31,322
and old space. We actually divide the heap into regions.

221
00:13:31,466 --> 00:13:35,058
Here is a fixed size region 64,

222
00:13:35,064 --> 00:13:39,378
bigger the heap, the more regions we have. Regions are assign a cell size.

223
00:13:39,544 --> 00:13:42,894
So here it's a very nice characteristic

224
00:13:42,942 --> 00:13:46,454
is that objects on the same region have the same size. And if

225
00:13:46,492 --> 00:13:49,254
an object is larger than 2,

226
00:13:49,292 --> 00:13:52,882
can actually span multiple regions with the exception

227
00:13:52,946 --> 00:13:56,534
of arrays. So if we have an array that's bigger than

228
00:13:56,572 --> 00:13:59,862
a region, we actually is going to divide whats

229
00:13:59,916 --> 00:14:03,354
array into chunks and put a chunk here, another chunk here,

230
00:14:03,392 --> 00:14:06,454
and have the array header points to these different chunks.

231
00:14:06,502 --> 00:14:09,770
Because normally if we have an object that's logging region,

232
00:14:10,110 --> 00:14:13,258
we're going to trigger a stop the world GC, right?

233
00:14:13,344 --> 00:14:17,214
For arrays, we can just chunk that array and put this

234
00:14:17,252 --> 00:14:20,014
in different chunks. That's what we call arraylets. I'm going to explain a little bit

235
00:14:20,052 --> 00:14:23,418
more what arraylets are at the end of the presentation.

236
00:14:23,514 --> 00:14:26,858
This is how metronome behaves. Very, very simple, if we

237
00:14:26,884 --> 00:14:30,046
will remember opt throughput. From the beginning of the presentation,

238
00:14:30,158 --> 00:14:33,602
it had one huge top to roll, right?

239
00:14:33,656 --> 00:14:37,910
So imagine that huge top, the road pause divided into little

240
00:14:37,980 --> 00:14:41,766
increments. That's what metronome is, right? It does a little

241
00:14:41,788 --> 00:14:45,702
bit of work here, a little bit of work here, but never going

242
00:14:45,756 --> 00:14:49,794
over that 30% threshold, right. Remember those ten milliseconds

243
00:14:49,842 --> 00:14:53,354
window, only 30% is going to be dedicated to the GC. And that's what these

244
00:14:53,392 --> 00:14:56,762
increments are. A little bit of GC work here, here and here and here until

245
00:14:56,816 --> 00:15:01,020
it's done. So the right barrier that I mentioned to you was

246
00:15:01,390 --> 00:15:04,666
snapshot at the beginning, right? And it causes a lot of

247
00:15:04,688 --> 00:15:08,510
floating garbage because the barrier is actually performed before the store.

248
00:15:08,580 --> 00:15:12,154
So imagine we have the following object graph. We are marking object concurrently,

249
00:15:12,202 --> 00:15:15,338
right? And the GCA has already processed the

250
00:15:15,364 --> 00:15:18,882
roots, has already processed object a, is in the middle of processing object

251
00:15:18,936 --> 00:15:22,114
b, it hasn't visited object c, neither object D.

252
00:15:22,152 --> 00:15:26,190
The mutated thread comes along and does this, right. Deletes the reference

253
00:15:26,270 --> 00:15:29,814
from b to c and adds a reference from a to d. And then if

254
00:15:29,852 --> 00:15:33,462
we didn't have a barrier, this will be the resulting object graph and we

255
00:15:33,516 --> 00:15:36,950
would erroneously collect object D. So how does

256
00:15:37,020 --> 00:15:40,294
snapshot at the beginning actually work? It is implemented this way,

257
00:15:40,412 --> 00:15:44,106
meaning we remember the reference before we actually do the

258
00:15:44,128 --> 00:15:47,862
store. So we put it in a temporary field. Remember that if the berries

259
00:15:47,926 --> 00:15:51,654
active. So if you come back to the object graph here, the berry

260
00:15:51,702 --> 00:15:55,326
is actually triggered. Whenever we delete the reference from

261
00:15:55,348 --> 00:15:58,814
b to c, that's when the berry is triggered. So instead of

262
00:15:58,932 --> 00:16:02,510
having a resulting object graph like this, we would have something like this.

263
00:16:02,580 --> 00:16:06,366
In this case we would actually remember object C, which that is

264
00:16:06,388 --> 00:16:09,554
considered garbage because we don't really need object C.

265
00:16:09,592 --> 00:16:12,818
It just happens to be in the path of object D. That's why we have

266
00:16:12,824 --> 00:16:16,690
a lot of floating garbage with this type of barriers. Now, our last

267
00:16:16,760 --> 00:16:20,294
but not least, DC policy is balanced. If you're familiar with

268
00:16:20,332 --> 00:16:23,778
g one from hotspot, it's very similar. Both are generational,

269
00:16:23,874 --> 00:16:27,782
both are region based. One difference that I see is that the g

270
00:16:27,836 --> 00:16:31,014
one from hotspot has only three ages and we

271
00:16:31,052 --> 00:16:34,246
have 23 to 27 ages.

272
00:16:34,278 --> 00:16:38,058
Whats we have a lot more ages. Like I said, they are both region based.

273
00:16:38,144 --> 00:16:42,058
Balance provides significant rendition. Max stop the world

274
00:16:42,144 --> 00:16:45,962
poses. We introduce right barriers similar to the one from Gencon,

275
00:16:46,026 --> 00:16:49,454
because you might have reference from different regions coming to my

276
00:16:49,492 --> 00:16:53,802
region. And we have an incremental heap defragmentation. So similar to metronome,

277
00:16:53,866 --> 00:16:57,070
the heap is divided into regions. But here the bigger the heap,

278
00:16:57,150 --> 00:17:01,282
the larger the region, because we try to have between 1000 2000

279
00:17:01,336 --> 00:17:05,390
regions in this policy, objects are located in what we call addon regions,

280
00:17:05,470 --> 00:17:09,654
which is our new regions. And if an object is

281
00:17:09,692 --> 00:17:13,586
larger than a region, we just throw an out of memory error, with exceptions

282
00:17:13,618 --> 00:17:16,722
of arrays, same thing as metronome. We divide the arrays into chunks

283
00:17:16,786 --> 00:17:20,354
and put them into different regions. This is how a balanced

284
00:17:20,482 --> 00:17:24,422
application behaves. The balanced policy. PGC stands for partial

285
00:17:24,486 --> 00:17:28,838
garbage collection. It's a copy collector. So what it's doing is copying

286
00:17:28,934 --> 00:17:32,746
the live objects from one region to the other and tries to

287
00:17:32,768 --> 00:17:36,750
find those regions with the highest return of investment. So what does

288
00:17:36,820 --> 00:17:40,814
that mean? So imagine we have a region with only one live object.

289
00:17:40,932 --> 00:17:44,146
All we have to do is cop that object over. And there we go,

290
00:17:44,168 --> 00:17:46,900
we have an entire free region to work with.

291
00:17:47,350 --> 00:17:51,586
So PGC tries to find those regions which has the most

292
00:17:51,688 --> 00:17:55,206
garbage, which is going to do the least amount

293
00:17:55,228 --> 00:17:58,966
of work to free that region. And GMP here

294
00:17:59,148 --> 00:18:03,270
is a global marking phase. It doesn't collect

295
00:18:03,770 --> 00:18:06,866
objects, it actually helps PGC.

296
00:18:06,978 --> 00:18:10,362
So one problem about PGC is that it only has

297
00:18:10,416 --> 00:18:13,558
local knowledge about each region.

298
00:18:13,654 --> 00:18:17,110
As time goes on, it loses this local knowledge.

299
00:18:17,270 --> 00:18:21,066
So GMP is there to help update this local knowledge

300
00:18:21,098 --> 00:18:24,586
of PGC. And at the end of the GMP, it updates PGC

301
00:18:24,618 --> 00:18:28,190
information so that PGC can do its work more

302
00:18:28,260 --> 00:18:31,486
effectively. So GMP does not

303
00:18:31,508 --> 00:18:34,834
reclaim memory, right. It prefers a marking phase only.

304
00:18:34,952 --> 00:18:38,306
It's scheduled to run between pgcs and

305
00:18:38,488 --> 00:18:41,934
it gives an accurate mark map of the entire heap.

306
00:18:41,982 --> 00:18:45,682
And it's going to use this mark map to update the

307
00:18:45,736 --> 00:18:49,118
PGC. So this is more of a visual

308
00:18:49,134 --> 00:18:52,758
representation of how PGC actually looks. So on the X axis we

309
00:18:52,764 --> 00:18:56,514
have time and the Y axis we have free heap. So lines

310
00:18:56,562 --> 00:19:00,394
going up is the PGC doing its work and lines going down

311
00:19:00,432 --> 00:19:04,122
is the application consuming memory. So you can see this trend going

312
00:19:04,176 --> 00:19:07,258
down is the heap getting fragmented because of the

313
00:19:07,264 --> 00:19:11,002
PGC. And that happens because PGC loses this local

314
00:19:11,056 --> 00:19:14,766
information. So GMP is smart enough to trigger at some point here so

315
00:19:14,788 --> 00:19:18,606
that it finished in a timely manner to update PGC so

316
00:19:18,628 --> 00:19:21,950
that it can defragment the heat. So you can see around there where

317
00:19:22,020 --> 00:19:25,886
I have affected the fragmentation. That's where we just updated PGC

318
00:19:25,918 --> 00:19:29,586
information. And you see that trend going up. And that's when PGC has the most

319
00:19:29,688 --> 00:19:32,946
recent information of every region, so it

320
00:19:32,968 --> 00:19:36,526
can better collect and defragment. And with time that trend

321
00:19:36,558 --> 00:19:39,846
going down, it's going to start up again and GMP is going to kick in

322
00:19:39,868 --> 00:19:43,926
again. So you can update BGC. And that's how a balanced policy

323
00:19:44,028 --> 00:19:47,174
behaves. So in balance, we do have a right

324
00:19:47,212 --> 00:19:51,754
barrier. It's very similar to the Gencon barrier because we

325
00:19:51,792 --> 00:19:55,510
might have multiple references coming from different regions to this region,

326
00:19:55,590 --> 00:19:58,746
and PGC doesn't really know how to keep track

327
00:19:58,768 --> 00:20:02,714
of that. So we need a right barrier for that. So actually here we do,

328
00:20:02,912 --> 00:20:06,910
unconditional barrier if you think about it. So we just study the card

329
00:20:06,980 --> 00:20:10,238
whenever we set a field of an object and then at the beginning of the

330
00:20:10,244 --> 00:20:14,042
PGC we go through this card to see if there's an iter region reference.

331
00:20:14,106 --> 00:20:17,454
So you see here, whenever we think about, read and write barriers,

332
00:20:17,582 --> 00:20:21,602
it's always a matter of who should we put the burden on.

333
00:20:21,656 --> 00:20:25,262
Should we put the burden on the motel thread or should we put the burden

334
00:20:25,326 --> 00:20:28,306
on the GC thread. So here we chose to put the burden on the GC

335
00:20:28,338 --> 00:20:31,446
thread so that we don't penalize motel threads as much.

336
00:20:31,548 --> 00:20:35,158
Now, arraylets, so you probably heard me talking about

337
00:20:35,244 --> 00:20:38,454
arraylets a few times here. So arraylets are a

338
00:20:38,572 --> 00:20:42,086
cleverer way to store larger rates in

339
00:20:42,108 --> 00:20:45,462
a region based GC. So these arrays have a spine,

340
00:20:45,526 --> 00:20:49,142
right? And this spine is going to be pointing to the data allocated

341
00:20:49,206 --> 00:20:52,426
with this array. Each leaf of this array, which is

342
00:20:52,448 --> 00:20:55,854
are the different chunks of the array, consumes the entire region. So how does

343
00:20:55,892 --> 00:20:58,926
it look? Actually, if I were to graph forward to

344
00:20:58,948 --> 00:21:02,462
visualize this, so we have the heap here, and the bottom we have the

345
00:21:02,516 --> 00:21:06,194
array header. Arrayoids represent the pointers to

346
00:21:06,232 --> 00:21:09,554
the array data. So imagine if you want to

347
00:21:09,592 --> 00:21:13,422
access element one of this array. What we do is we follow reference,

348
00:21:13,486 --> 00:21:16,674
the first reference here, the first arrayoid, and you can see we go to the

349
00:21:16,712 --> 00:21:20,134
last blue region and access the element that way.

350
00:21:20,172 --> 00:21:23,718
So you can see here, it's really nice way to store large

351
00:21:23,804 --> 00:21:27,218
arrays in a heap as opposed to having this array

352
00:21:27,314 --> 00:21:30,266
span multiple regions. But there's a problem.

353
00:21:30,448 --> 00:21:33,862
This array that's divided into regions cannot

354
00:21:33,926 --> 00:21:37,366
work with APIs that require a contiguous representation

355
00:21:37,398 --> 00:21:41,258
of this array. So how do we deal with this before? So this

356
00:21:41,264 --> 00:21:44,566
is the case of Jni APIs, Java native interface

357
00:21:44,598 --> 00:21:48,142
critical APIs. They actually need a contiguous review

358
00:21:48,196 --> 00:21:51,006
of the array, which is not the case of array, lets. Right. So whats we

359
00:21:51,028 --> 00:21:54,718
do? So we actually make a copy of this array, right, and then

360
00:21:54,804 --> 00:21:58,894
copy element by element to this temporary array, pass that array,

361
00:21:58,942 --> 00:22:02,610
copy to the J nine API. Then the J API is going to do whatever

362
00:22:02,680 --> 00:22:06,190
it does with the array, maybe modify some of its elements,

363
00:22:06,270 --> 00:22:09,446
and then we need to copy everything back. So you can see there

364
00:22:09,468 --> 00:22:13,506
are like two, imagine we have large arrays like 1050 megabytes

365
00:22:13,538 --> 00:22:16,966
array. As you can see, that's really expensive. So how can

366
00:22:16,988 --> 00:22:20,790
we deal with that? That's where double mapping comes in. So we can make

367
00:22:20,860 --> 00:22:24,582
this large arrays discontinuous arrays look contiguously.

368
00:22:24,646 --> 00:22:27,702
So we take the idea that physical memory is limited,

369
00:22:27,766 --> 00:22:31,322
but virtual memory is not. On the contrary, virtual memory address space

370
00:22:31,376 --> 00:22:35,246
is very very large in 64 bit systems to the power 64

371
00:22:35,268 --> 00:22:38,878
in fact. So what we do is we map two virtual memory addresses to

372
00:22:38,884 --> 00:22:42,046
the same physical address. And the nice thing about this is that any

373
00:22:42,068 --> 00:22:45,246
modifications to any of the virtual address is reflected in the

374
00:22:45,268 --> 00:22:49,010
other virtual address. So ZGC does a similar trick to keep track

375
00:22:49,080 --> 00:22:52,754
of the different faces of the GC. So every object is

376
00:22:52,792 --> 00:22:56,514
going to have actually multiple copies in the vitro address space, but they actually point

377
00:22:56,552 --> 00:22:59,778
to the same physical address. Here we just do it for arrays.

378
00:22:59,874 --> 00:23:03,638
How would this look like if you were to visualize? So in the

379
00:23:03,644 --> 00:23:07,602
bottom part there we have the actual arraylets, right, the different regions of the arrays

380
00:23:07,666 --> 00:23:11,126
and then we map that to a second virtual address and we

381
00:23:11,148 --> 00:23:14,694
do in such a way that the second vitro memory is going to look contiguously

382
00:23:14,742 --> 00:23:18,026
in the vitro space. All we have to do now is pass the address of

383
00:23:18,048 --> 00:23:21,402
that second virtual memory to the gen nine API and every

384
00:23:21,456 --> 00:23:24,986
modification to that virtual memory is going to be reflected in the original

385
00:23:25,018 --> 00:23:28,334
virtual address. That way we don't have to copy everything to the temporary array and

386
00:23:28,372 --> 00:23:32,382
copy everything back. So there's a really nice trick that we do in our

387
00:23:32,436 --> 00:23:36,222
region based DC and with that we actually boosted array operations

388
00:23:36,286 --> 00:23:39,806
by 30 times, which is pretty cool. So what's next? Double map

389
00:23:39,838 --> 00:23:43,154
has some downsize rights, only available Linux and 64

390
00:23:43,192 --> 00:23:46,526
bit systems. But can we do better? Some platforms doesn't

391
00:23:46,558 --> 00:23:49,990
really support this double mapping trick. So what we're actually working

392
00:23:50,060 --> 00:23:53,686
right now is on a technology called off heap where

393
00:23:53,708 --> 00:23:57,634
we're actually going to store large objects at a secondary heap

394
00:23:57,682 --> 00:24:00,746
and that way we can do some more tricks and try to

395
00:24:00,768 --> 00:24:04,950
optimize this large object even further. So stay tuned.

396
00:24:05,030 --> 00:24:07,862
This is a quick GC policy guide.

397
00:24:07,926 --> 00:24:11,466
So on the left we have the GC policy and on the right we

398
00:24:11,488 --> 00:24:14,430
have some characteristics. For instance, some of them,

399
00:24:14,500 --> 00:24:17,882
some of these gcs are generational, right Gencon and Gencon

400
00:24:17,946 --> 00:24:21,322
cs and balanced. Some of them is going to have some concurrent faces.

401
00:24:21,386 --> 00:24:25,018
The only one that has no concurrent face at all is of throughput but of

402
00:24:25,044 --> 00:24:28,626
throughput has the highest throughput and that's because it

403
00:24:28,648 --> 00:24:32,494
doesn't have to deal with read or write barriers. And these different policies

404
00:24:32,542 --> 00:24:36,210
have different heap layouts, right? Gencon, Genco has the

405
00:24:36,280 --> 00:24:39,794
new and old separation right balance and metronoma region

406
00:24:39,842 --> 00:24:43,798
based and opt throughput has a small object area and large object area.

407
00:24:43,884 --> 00:24:47,542
This is another table where it shows for which domain is

408
00:24:47,596 --> 00:24:51,642
which policy is better. This is not, you shouldn't follow this by

409
00:24:51,696 --> 00:24:55,740
the foot, right? You should actually test the policy because

410
00:24:56,590 --> 00:25:00,518
even though you might have like a desktop application, depending on your workload,

411
00:25:00,614 --> 00:25:04,474
another policy might work best. So here for such a general tip.

412
00:25:04,602 --> 00:25:07,930
Gencon are good for web servers, desktop applications,

413
00:25:08,010 --> 00:25:11,534
balanced. RG one is good for very large heaps, and metronome is

414
00:25:11,572 --> 00:25:15,326
good for systems that have softer real time

415
00:25:15,508 --> 00:25:19,214
constraints. And optropod is very good for small heaps.

416
00:25:19,262 --> 00:25:22,658
Or if you don't care about GCP poses at all. And you can see here

417
00:25:22,744 --> 00:25:26,486
different policies are going to have different throughput and different gcpuzzles. You can

418
00:25:26,508 --> 00:25:30,002
see Genco Cs has not the best throughput,

419
00:25:30,066 --> 00:25:33,842
but whats the fewer GCPU on the other hand of throughput

420
00:25:33,906 --> 00:25:37,026
has the highest but the largest GCP.

421
00:25:37,138 --> 00:25:40,394
And that's because of concurrency and how we deal

422
00:25:40,432 --> 00:25:44,762
with that. That's because read barriers and write barriers has

423
00:25:44,896 --> 00:25:48,762
overhead on the GC. And you can see here all

424
00:25:48,816 --> 00:25:51,882
policies are going to have some type of additional memory.

425
00:25:52,026 --> 00:25:55,662
Maybe they have like a mark map or something to keep

426
00:25:55,716 --> 00:25:59,854
track of these objects, because we need a way to keep

427
00:25:59,972 --> 00:26:03,102
the DC needs these extra data structures

428
00:26:03,166 --> 00:26:06,354
to help collect the object. And that's what

429
00:26:06,392 --> 00:26:10,340
this middle column is telling you about. This is

430
00:26:10,710 --> 00:26:14,194
like a diagram showing the similarities between the different

431
00:26:14,312 --> 00:26:17,746
and the most common GCS policies out there. So on the top there

432
00:26:17,768 --> 00:26:20,966
in the yellow we see the most common gcs. So Gentcon is very

433
00:26:20,988 --> 00:26:24,882
similar to cms from hotspot. I think I've been saying cs,

434
00:26:24,946 --> 00:26:28,674
I apologize. So Gencons is similar to cms from hotspot.

435
00:26:28,802 --> 00:26:32,662
So both are gencon, both have neo and old space balanced,

436
00:26:32,726 --> 00:26:36,362
and G one very similar to each other. Right? Both are

437
00:26:36,416 --> 00:26:40,170
rigid based, both are generational, right? Then we have

438
00:26:40,320 --> 00:26:43,242
on the blue section there we have the puzzler gcs,

439
00:26:43,306 --> 00:26:47,422
which their objective is to have the least amount

440
00:26:47,556 --> 00:26:51,374
of poses and the shorter puzzles, right? So you

441
00:26:51,412 --> 00:26:54,826
can see the thickness of the lines there are thinner

442
00:26:54,858 --> 00:26:58,530
because even though they share some similarities, not as similar as the ones

443
00:26:58,600 --> 00:27:01,762
above, right? So ZGC, Chin and door, they're both region based,

444
00:27:01,816 --> 00:27:05,238
but they are not generational. Gen conference is not region based, but it

445
00:27:05,244 --> 00:27:08,466
is generational. Metronome is region based, but it's

446
00:27:08,498 --> 00:27:12,226
not generational. And remember, ZGC uses

447
00:27:12,258 --> 00:27:15,746
that multimapping kind of technology, right? Shindo version

448
00:27:15,778 --> 00:27:19,438
two uses the same read barrier as Gencon CS Shillando

449
00:27:19,474 --> 00:27:23,914
version one uses Brooks Pointer, which is kind of a different

450
00:27:23,952 --> 00:27:27,274
technology. And on the green side there we have

451
00:27:27,312 --> 00:27:30,614
Azul C four, which also puzzle legacy, but you require

452
00:27:30,662 --> 00:27:33,770
like a special hardware or some special software.

453
00:27:33,850 --> 00:27:37,262
Now this is the summary of the talk here.

454
00:27:37,316 --> 00:27:40,926
If there's one thing that I want you to get out of this is that

455
00:27:40,948 --> 00:27:44,926
there's no perfect fits, no perfect GC that fits everything,

456
00:27:45,028 --> 00:27:49,490
all workloads, right? And if you want a GC that

457
00:27:49,640 --> 00:27:53,186
has perfect throughput and you don't care about puzzles at all, you should go with

458
00:27:53,208 --> 00:27:56,642
the perfect stop, the wood GC, because no matter what, that's always going to have

459
00:27:56,696 --> 00:28:00,790
a higher throughput as opposed to a perfect puzzleless GC. And that's because the

460
00:28:00,860 --> 00:28:04,646
read and write barriers overhead, even though they're very important for

461
00:28:04,668 --> 00:28:08,262
the currency of these puzzles, gcs, there is an overhead of,

462
00:28:08,316 --> 00:28:11,786
because for every object read or write you

463
00:28:11,808 --> 00:28:15,366
need to make sure that the GC has that knowledge whenever

464
00:28:15,398 --> 00:28:18,634
we are changing those references along with the application,

465
00:28:18,752 --> 00:28:21,962
right? So if you don't care about the pauses, go for perfect

466
00:28:22,016 --> 00:28:25,514
stopwdc. If you do care about deposits, go for puzzles

467
00:28:25,562 --> 00:28:28,858
GC. But if you care about both, or you want to balance

468
00:28:28,954 --> 00:28:33,114
between the two, can go for the balanced GC or G one, right? Or Gencon

469
00:28:33,162 --> 00:28:37,294
Semas. So you have the options, right? Both extremes,

470
00:28:37,342 --> 00:28:40,754
perfect stop the road, perfect puzzlers and something somewhere in the middle.

471
00:28:40,872 --> 00:28:44,542
And these different policies deal with heap fragmentation

472
00:28:44,606 --> 00:28:48,198
differently, right? Depend they might have

473
00:28:48,364 --> 00:28:52,130
the heap configuration different, so therefore they're going to collect objects

474
00:28:52,290 --> 00:28:55,986
differently, right? Remember the barriers, right? Snapshot at the beginning,

475
00:28:56,018 --> 00:28:59,626
barrier against just a regular write barrier, right? So there

476
00:28:59,648 --> 00:29:02,762
are a lot of things here to playing when it comes

477
00:29:02,816 --> 00:29:06,730
to a perfect GC policy. But again,

478
00:29:06,800 --> 00:29:09,898
there's not going to be a perfect GC for every policy.

479
00:29:09,984 --> 00:29:13,422
You should, whatever workload you have, you should try three, four different

480
00:29:13,476 --> 00:29:17,354
policies and to see what's going to work best for that workload.

481
00:29:17,402 --> 00:29:20,814
So here are some links. OPG nine is a very cool

482
00:29:20,852 --> 00:29:24,670
project. We do recommend people to come check it out. That second link there,

483
00:29:24,740 --> 00:29:28,206
CMD line migration is for those people that are familiar

484
00:29:28,238 --> 00:29:31,906
with hotspot, right. And want to try out OPG. So I really recommend looking

485
00:29:32,008 --> 00:29:36,290
at that. Adopt opendicates, the website whats will get the binaries.

486
00:29:36,370 --> 00:29:39,830
And Omar is another project that has many different

487
00:29:39,900 --> 00:29:43,398
components to build a runtime. And Opgnan uses a lot of

488
00:29:43,404 --> 00:29:47,234
these components. For instance, uses GC component and compiler

489
00:29:47,282 --> 00:29:50,586
component. It's really cool. Also open source project and we welcome

490
00:29:50,688 --> 00:29:55,020
contributors there as well. So that's it for me.

491
00:29:55,710 --> 00:29:58,550
I'm very happy if you can connect with me LinkedIn,

492
00:29:58,630 --> 00:30:02,330
Twitter and before I open to

493
00:30:02,400 --> 00:30:05,886
questions, I want to show this book here. If you want

494
00:30:05,908 --> 00:30:09,486
to know everything there is about garbage collection, this is the book. Everything you want

495
00:30:09,508 --> 00:30:13,006
to know about is in there. And these are some

496
00:30:13,028 --> 00:30:16,074
of the common options that if you're

497
00:30:16,122 --> 00:30:19,986
interested to go through. If you want to start, like tweaking your

498
00:30:20,088 --> 00:30:23,138
application, you can start with these ones. They are most,

499
00:30:23,224 --> 00:30:26,402
let's say simple. The first one there is only available on

500
00:30:26,536 --> 00:30:29,762
Obajni. And the nice thing about verbose GC

501
00:30:29,826 --> 00:30:33,654
option is that you can actually visualize how your memory is

502
00:30:33,692 --> 00:30:37,282
behaving under an application with a tool called GcMV.

503
00:30:37,346 --> 00:30:40,678
And you can only visualize this if you're running an application with

504
00:30:40,684 --> 00:30:44,194
the Openj nine VM because with hotspot they have a different variables.

505
00:30:44,242 --> 00:30:47,814
So if you're running with Openj nine, you can use that verbals GC to

506
00:30:47,852 --> 00:30:51,326
capture the lock of the GC and visualize that with GcMB, which is really

507
00:30:51,348 --> 00:30:55,674
cool. If you remember that graph I showed you balanced, I use GCMB

508
00:30:55,722 --> 00:30:58,638
with that. I put some colors into it, but that's pretty much what you're going

509
00:30:58,644 --> 00:31:02,526
to get with GcMB, which is really cool. So I think that's it for

510
00:31:02,548 --> 00:31:06,190
me. I'll stop here and I'll check it out for any questions.

511
00:31:06,260 --> 00:31:06,860
Thank you so much.

