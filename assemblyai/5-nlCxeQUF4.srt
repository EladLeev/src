1
00:00:25,410 --> 00:00:29,110
You hello and become to

2
00:00:29,180 --> 00:00:32,166
Conf 40 director director of cyber Chaos engineering.

3
00:00:32,188 --> 00:00:35,746
Aaron Reinhardt, CTO and co founder of Verica. And I'm,

4
00:00:35,778 --> 00:00:39,186
I'm, my co presenter is David Lavezzo,

5
00:00:39,218 --> 00:00:42,358
director director of Cyber Chaos Engineers, Capital one.

6
00:00:42,444 --> 00:00:45,654
Today we're going to talk about cyber chaos engineering and

7
00:00:45,692 --> 00:00:49,446
security chaos engineering. Some of the things we'll be talking about are what is

8
00:00:49,468 --> 00:00:52,974
chaos engineers? Very brief introduction to it. We'll talk a bit about

9
00:00:53,012 --> 00:00:56,110
how it applies to cybersecurity,

10
00:00:57,090 --> 00:01:00,714
how you can get started. David's also going to share his journey.

11
00:01:00,762 --> 00:01:04,834
Director of Cyber Chaos Engineering, Capital one a little background about

12
00:01:04,872 --> 00:01:08,450
me. Like I said before, I'm the CTO and co founder of Verica.

13
00:01:09,510 --> 00:01:13,246
I'm also the former chief security architect

14
00:01:13,278 --> 00:01:17,810
of UnitedHealth Group. That's where I wrote the first open source tool that applied

15
00:01:17,890 --> 00:01:21,906
Netflix's chaos engineering to cybersecurity. I'm also the O'Reilly

16
00:01:21,938 --> 00:01:25,254
author on the Chaos engineering book,

17
00:01:25,372 --> 00:01:29,370
all for Security Chaos Engineering. And I'm also the author of

18
00:01:29,520 --> 00:01:32,380
the O'Reilly Security Chaos Engineering report.

19
00:01:32,830 --> 00:01:36,950
I am David, Capital one. I'm the director of security chaos engineering,

20
00:01:37,030 --> 00:01:40,566
formerly from Zappos, Amazon, and also a contributing

21
00:01:40,598 --> 00:01:44,570
O'Reilly author for security Chaos engineers. I've released multiple

22
00:01:44,650 --> 00:01:48,126
metal albums and surf albums, and the picture that you see right

23
00:01:48,148 --> 00:01:51,626
on there is actually my best photograph. And most notably,

24
00:01:51,738 --> 00:01:55,066
I did indeed start an indoor snowball fight in Las Vegas.

25
00:01:55,178 --> 00:01:58,478
You're much more interesting than I am, David. So let's

26
00:01:58,494 --> 00:02:01,326
get started. So what is the base of the problem that we're trying to solve?

27
00:02:01,438 --> 00:02:04,420
Well, in cybersecurity, the problem, I mean,

28
00:02:05,130 --> 00:02:08,374
these headlines are just examples of the

29
00:02:08,412 --> 00:02:12,486
problem is becoming more and more frequent in

30
00:02:12,508 --> 00:02:16,898
terms of breaches and outages, as well as they're getting more magnified.

31
00:02:17,074 --> 00:02:20,478
But why do they seem to be happening more often? That's part of the theme

32
00:02:20,514 --> 00:02:24,540
we're going to talk about is why we believe that's happening. So to start,

33
00:02:25,070 --> 00:02:28,842
part of the problem is our systems have fundamentally evolved beyond our

34
00:02:28,896 --> 00:02:32,266
human ability to mentally model their behavior. Every dot you see,

35
00:02:32,288 --> 00:02:36,010
so what you see in front of you is something called a Death Star diagram.

36
00:02:36,090 --> 00:02:39,246
Each dot represents a microservice. This used to be

37
00:02:39,348 --> 00:02:43,210
kind of a reflection of how fang, Facebook, Apple Alphabet,

38
00:02:43,290 --> 00:02:46,878
Netflix and Google, how they built their systems.

39
00:02:47,054 --> 00:02:50,866
And so imagine if l every dot is a microservice. That's very

40
00:02:50,888 --> 00:02:54,418
complex. That's a lot of things for you to keep track of in your head.

41
00:02:54,584 --> 00:02:58,654
Well, now it's not just Google, Facebook and Netflix

42
00:02:58,702 --> 00:03:02,022
building like this. It's everyone, everyone's not building this.

43
00:03:02,076 --> 00:03:06,262
The problem is the complexities and the scale of these systems is very

44
00:03:06,316 --> 00:03:10,386
difficult to keep track of in our heads. So here's an example from Netflix

45
00:03:10,418 --> 00:03:13,660
is actually edge streaming service.

46
00:03:14,350 --> 00:03:17,050
The point of this slide is to demonstrate that.

47
00:03:17,200 --> 00:03:20,746
So each large circle is actually a microservice, and each dot represents a

48
00:03:20,768 --> 00:03:23,854
request. It's very difficult at any given point in time to keep

49
00:03:23,892 --> 00:03:26,078
track of what the heck is even going on here.

50
00:03:26,244 --> 00:03:29,550
And this is part of the problem that we're trying to

51
00:03:29,700 --> 00:03:33,040
articulate. So where does this complex come from?

52
00:03:34,870 --> 00:03:38,706
So some of the areas that introduce complexity into how

53
00:03:38,728 --> 00:03:41,678
we build software today are things like continuous delivery,

54
00:03:41,774 --> 00:03:45,486
cloud computing, auto canaries, service meshes,

55
00:03:45,678 --> 00:03:49,750
microservices, distributed systems in general, mutable infrastructure.

56
00:03:50,090 --> 00:03:53,634
But these are the mechanisms that are enabling us CTO deliver

57
00:03:53,682 --> 00:03:56,854
value to customer faster than we've ever done before. The problem is

58
00:03:56,892 --> 00:04:00,326
they also add a layer of complexity that makes it difficult for us humans to

59
00:04:00,348 --> 00:04:03,450
keep track of what's going on. Where is the state of security? Well,

60
00:04:03,600 --> 00:04:06,598
in security, our solutions are still mostly monolithic.

61
00:04:06,694 --> 00:04:09,622
They're still expert in nature, meaning they require domain knowledge.

62
00:04:09,686 --> 00:04:13,226
Operative. It's hard to pick up Palo

63
00:04:13,258 --> 00:04:17,166
Alto firewall. A lot of our security is

64
00:04:17,188 --> 00:04:20,538
very tools rich, but it's very dependent upon commercial tool sets.

65
00:04:20,634 --> 00:04:24,030
And you can't just really go to GitHub and download a tool and

66
00:04:24,100 --> 00:04:27,586
get working with it. It's not exactly the same as it

67
00:04:27,608 --> 00:04:31,554
is in the world of software engineering. We're getting better at it by

68
00:04:31,592 --> 00:04:34,974
leaps and bounds. We are getting better at it, but still our solutions are mostly

69
00:04:35,022 --> 00:04:38,758
staple in nature. So what is the answer to the complexity problem?

70
00:04:38,844 --> 00:04:42,280
Well, the most logical result would be to simplify. Well,

71
00:04:42,730 --> 00:04:46,502
it's important to recognize that software has now officially taken

72
00:04:46,556 --> 00:04:49,594
over. What you see on the right hand side here is you see the new

73
00:04:49,632 --> 00:04:53,398
OSI model. It's now software, software, software, software, software.

74
00:04:53,494 --> 00:04:57,210
Software was taken over the entire stack. But also

75
00:04:57,360 --> 00:05:00,746
coming back to fundamentals, it's important to recognize because of the nature of the

76
00:05:00,768 --> 00:05:04,382
ease and flexibility of change, software only

77
00:05:04,436 --> 00:05:07,854
ever increases in complexity. It never decreases. Right.

78
00:05:07,892 --> 00:05:11,454
There's an old saying in software that there's no problem in software that another

79
00:05:11,492 --> 00:05:15,006
layer of abstraction can't solve. But when does those

80
00:05:15,028 --> 00:05:18,439
layers of abstraction start to become a problem? Well, we're starting to

81
00:05:18,939 --> 00:05:22,194
see that at speed and scale in terms of complex. So what does all this

82
00:05:22,232 --> 00:05:25,086
have to do with my systems? What does that have to do with the systems

83
00:05:25,118 --> 00:05:28,690
we build every day? We're getting to it. So how? Well, the question

84
00:05:28,760 --> 00:05:32,118
remains. How well, given all these things that we have to

85
00:05:32,124 --> 00:05:35,800
keep track of in our head and the changes, how well do you really

86
00:05:36,330 --> 00:05:39,842
understand how your system works? I mean, how well do you really understand

87
00:05:39,916 --> 00:05:43,162
it? Well, one thing, and this has been really a

88
00:05:43,216 --> 00:05:47,254
transformational thing for me. It sounds really simple, but we love to forget

89
00:05:47,302 --> 00:05:50,650
that systems engineering is very messy exercise.

90
00:05:51,150 --> 00:05:54,458
So in the beginning, we love CTO. Look at our systems like this, right?

91
00:05:54,544 --> 00:05:57,454
We have a nice plant, we have time, we have the resources, we've got our

92
00:05:57,492 --> 00:06:00,714
pipeline figured out, the code, the base image for our docker

93
00:06:00,762 --> 00:06:04,510
container, the configs. We know what services we need to deliver.

94
00:06:04,670 --> 00:06:07,854
We've got a nice, beautiful 3d diagram of the AWS

95
00:06:07,902 --> 00:06:11,794
instance that we're going to build in. It's clear what the

96
00:06:11,832 --> 00:06:15,534
system is, right? In reality, our system never

97
00:06:15,592 --> 00:06:19,174
looks like this. It's never this clear. It never really

98
00:06:19,212 --> 00:06:21,720
achieves anything that looks like this.

99
00:06:22,410 --> 00:06:26,182
What happens is, after a few weeks, a few months,

100
00:06:26,236 --> 00:06:30,006
our system slowly drifts into a state where we don't no

101
00:06:30,028 --> 00:06:34,138
longer recognize it and what sort of manifests that is.

102
00:06:34,224 --> 00:06:37,418
So after a week, what happens is there's outage on the payments API. You have

103
00:06:37,424 --> 00:06:41,146
to hard code a token or there's a DNS error that you have to

104
00:06:41,248 --> 00:06:44,958
get on the war room and fix. Marketing comes down and

105
00:06:44,964 --> 00:06:49,166
says you have to refactor the pricing module or service because they

106
00:06:49,188 --> 00:06:52,606
got it wrong, or Google hires your

107
00:06:52,628 --> 00:06:56,434
best or your elite software engineer. What happens is we solely learn about

108
00:06:56,472 --> 00:07:00,174
what we didn't really know about our systems through a series of surprises,

109
00:07:00,302 --> 00:07:03,826
right, unforeseen events. But these are also the

110
00:07:03,848 --> 00:07:08,162
mechanisms by which we learn about how our system really functions.

111
00:07:08,226 --> 00:07:12,290
We learn through these surprises. But as a result of these surprises

112
00:07:12,450 --> 00:07:16,306
comes pain. And the problem just magnifies. Over time, our system just slowly

113
00:07:16,338 --> 00:07:20,186
drifts into a state that we really don't recognize it anymore. Through these

114
00:07:20,208 --> 00:07:23,466
unforeseen events. These unforeseen events don't have to be

115
00:07:23,488 --> 00:07:26,922
unforeseen. We can surface these problems proactively using

116
00:07:26,976 --> 00:07:31,398
chaos engineering and chaos engineering, because it's proactive

117
00:07:31,574 --> 00:07:34,666
and managed. It's a managed way of doing that. We're not constantly

118
00:07:34,698 --> 00:07:38,606
reacting to an outages or incidents as a result of

119
00:07:38,628 --> 00:07:42,334
not understanding how our system functioned. And we're going to get more into

120
00:07:42,372 --> 00:07:46,080
how that works in the end.

121
00:07:46,630 --> 00:07:50,062
The summation of that is that our systems become more complex

122
00:07:50,126 --> 00:07:53,010
and messier than we initially remember them being.

123
00:07:53,160 --> 00:07:55,798
So what does all this stuff have to do with security? Well, we're getting to

124
00:07:55,804 --> 00:07:59,042
it. We're building towards it. So it's important to recognize

125
00:07:59,106 --> 00:08:02,966
that the normal function of

126
00:08:02,988 --> 00:08:06,466
your systems is to fail. It's to fail.

127
00:08:06,498 --> 00:08:10,402
It's humans to keep them from failure. So humans,

128
00:08:10,466 --> 00:08:13,610
we need failure. It's a fundamental component of

129
00:08:13,680 --> 00:08:16,906
growth. We needed to learn and to grow and to learn new things.

130
00:08:17,008 --> 00:08:19,786
We learned to learn how not to do it, to learn how to do it.

131
00:08:19,808 --> 00:08:23,390
Right. But the things we build are

132
00:08:23,460 --> 00:08:27,114
really no different. They require the same inputs.

133
00:08:27,242 --> 00:08:30,762
So how do we typically discover when our security measures fail?

134
00:08:30,906 --> 00:08:34,734
Well, usually it's not till some sort of incident is launched. There's some human reports,

135
00:08:34,782 --> 00:08:38,660
some tool throws an alert, tells us we're missing information,

136
00:08:40,790 --> 00:08:43,870
but by the time a security incident is launched,

137
00:08:44,030 --> 00:08:47,346
it's too late. The problem is already probably out

138
00:08:47,368 --> 00:08:50,546
of hand, and we need to be more proactive

139
00:08:50,578 --> 00:08:53,926
in identifying when things aren't working the way we intend them to

140
00:08:53,948 --> 00:08:57,286
work, so we don't incur the pain of incidents and

141
00:08:57,308 --> 00:09:00,106
breaches or outages, for that matter.

142
00:09:00,288 --> 00:09:04,118
So no system, like I said before, no system is inherently secure.

143
00:09:04,214 --> 00:09:07,562
By default, it is humans that

144
00:09:07,616 --> 00:09:10,342
make them that way. If so, security,

145
00:09:10,496 --> 00:09:12,526
reliability, quality,

146
00:09:12,708 --> 00:09:16,810
resilience, these things are human constructs.

147
00:09:16,890 --> 00:09:20,654
It requires a human to manifest it, to create it.

148
00:09:20,772 --> 00:09:24,018
So point a finger at a human for causing a security

149
00:09:24,104 --> 00:09:27,646
problem is kind of not productive, right? It requires

150
00:09:27,678 --> 00:09:31,394
us initially to even create this thing we call security.

151
00:09:31,592 --> 00:09:35,282
So, cognitively, we need to

152
00:09:35,336 --> 00:09:38,674
also remember that people operate fundamentally

153
00:09:38,722 --> 00:09:42,082
differently when they expect things to fail.

154
00:09:42,226 --> 00:09:45,794
What do I mean? So what I mean by this is when there's an incident

155
00:09:45,842 --> 00:09:49,482
or an outage, especially one related to security, people frig out,

156
00:09:49,536 --> 00:09:53,206
right? Because they're expecting, hey, this is the one they're worrying

157
00:09:53,238 --> 00:09:57,002
about being named, blamed and

158
00:09:57,056 --> 00:10:00,554
shamed, right? I shouldn't have made that change late at night,

159
00:10:00,592 --> 00:10:04,474
or I

160
00:10:04,512 --> 00:10:06,650
should have been more careful. Should have, could have, woulda.

161
00:10:08,370 --> 00:10:11,646
This environment. So what happens when there's an incident or outage is

162
00:10:11,668 --> 00:10:14,826
that within 15 minutes, some executive

163
00:10:14,858 --> 00:10:17,906
gets on the phone or gets involved and says, hey, I don't care what you

164
00:10:17,928 --> 00:10:21,422
have to do. Get that thing back up and running. The company is losing

165
00:10:21,486 --> 00:10:24,738
money, right? So it becomes more about, get that thing back up and

166
00:10:24,744 --> 00:10:28,310
running, not what really caused it, the problem to occur.

167
00:10:29,530 --> 00:10:32,902
And so this environment is

168
00:10:32,956 --> 00:10:36,166
not a good environment to learn. This is not how we should be learning about

169
00:10:36,188 --> 00:10:40,522
how our systems actually function. So chaos engineering we do here,

170
00:10:40,656 --> 00:10:44,314
we do it when there is no active war room. Nobody's freaking out.

171
00:10:44,352 --> 00:10:48,170
Nobody's worrying about being blamed, aimed, or shamed for causing an incident.

172
00:10:48,510 --> 00:10:51,990
We're able to proactively surface these inherent

173
00:10:52,070 --> 00:10:55,198
failures that are already in the system, the inherent chaos. We're trying to

174
00:10:55,204 --> 00:10:58,938
make order of the inherent chaos that's already in the system, and we're

175
00:10:58,954 --> 00:11:02,702
able to not be worried about the company

176
00:11:02,836 --> 00:11:06,766
having an outage, losing money, customers calling in. We're able to do this eyes wide

177
00:11:06,798 --> 00:11:10,754
open, understand the failure, proactively address it so we

178
00:11:10,792 --> 00:11:14,110
don't incur customer pain. So, chaos engineering,

179
00:11:14,190 --> 00:11:17,506
this is where we enter into chaos. So, chaos engineers.

180
00:11:17,538 --> 00:11:21,474
The definition that Netflix wrote is the discipline of experimentation

181
00:11:21,522 --> 00:11:25,058
on distributed systems in order to build confidence

182
00:11:25,154 --> 00:11:28,626
in the system's ability to withstand turbulent conditions. So it's

183
00:11:28,658 --> 00:11:31,914
about establishing, building confidence in how the systems actually

184
00:11:31,952 --> 00:11:35,606
function. Another way of understanding it is it's a technique

185
00:11:35,638 --> 00:11:38,762
of proactively introducing turbulent conditions into a system

186
00:11:38,816 --> 00:11:41,886
or service. Try CTO, determine the conditions by which the

187
00:11:41,908 --> 00:11:45,706
system or service will fail before it actually fails.

188
00:11:45,818 --> 00:11:49,802
That's the key aspect. So, cyber chaos engineers, about establishing

189
00:11:49,866 --> 00:11:53,578
order from chaos. It's not about just breaking things. If you're

190
00:11:53,594 --> 00:11:56,930
just breaking things, you're not doing chaos engineering. Things are already

191
00:11:57,000 --> 00:12:00,386
breaking. What we're trying to do is proactively understand why that

192
00:12:00,408 --> 00:12:04,622
is, fix it before it manifests and breaks and causes

193
00:12:04,686 --> 00:12:08,566
company problems. So, no story about chaos engineering would

194
00:12:08,588 --> 00:12:12,262
be complex without explaining the Chaos monkey story.

195
00:12:12,396 --> 00:12:16,226
So, in late 2008, Netflix was changing over from shipping

196
00:12:16,258 --> 00:12:19,302
dvds to building

197
00:12:19,356 --> 00:12:21,900
their streaming services and Amazon Web services.

198
00:12:23,870 --> 00:12:26,742
This is important to recognize. A lot of people say, oh, we can't do cyber

199
00:12:26,806 --> 00:12:30,442
chaos engineering. Can barely do the DevOps. We're barely in the cloud.

200
00:12:30,576 --> 00:12:34,542
Right? Well, Netflix had the need for

201
00:12:34,596 --> 00:12:38,078
chaos engineering during their cloud transformation. It was a

202
00:12:38,084 --> 00:12:41,182
key component of Netflix transforming to the cloud.

203
00:12:41,236 --> 00:12:44,386
So if you're just transforming the cloud, what it ends up being? It ends up

204
00:12:44,408 --> 00:12:47,940
being a feedback mechanism to help you understand.

205
00:12:48,470 --> 00:12:51,922
Are the things that I'm building in this new world actually

206
00:12:52,056 --> 00:12:55,746
functional? Actually, are they doing the things that I build them to do

207
00:12:55,848 --> 00:12:59,554
over time? Right. You can almost think of it as a post deployment

208
00:12:59,602 --> 00:13:03,366
regression test for the things you're build. And so

209
00:13:03,388 --> 00:13:06,120
that's important to recognize. So this isn't some crazy thing.

210
00:13:06,890 --> 00:13:10,346
This is that you have to be super advanced to do that is a

211
00:13:10,368 --> 00:13:13,820
false interpretation of chaos engineering. Also,

212
00:13:14,830 --> 00:13:18,234
what chaos engineering really represented. So what Chaos monkey did

213
00:13:18,272 --> 00:13:21,526
was because at the time, they were building out their services in AWS,

214
00:13:21,638 --> 00:13:24,606
what was happening was Amis. Amazon machine images were,

215
00:13:24,628 --> 00:13:28,590
poof, just disappearing. At the time, it was a feature of AWS.

216
00:13:29,490 --> 00:13:33,346
So service owners had a difficult time when that happened. So what

217
00:13:33,368 --> 00:13:37,442
they needed was they needed a way to

218
00:13:37,496 --> 00:13:41,902
test that if I built my service to be resilient to amis

219
00:13:42,046 --> 00:13:45,714
disappearing, that it would be resilient to that problem.

220
00:13:45,912 --> 00:13:49,190
So it put a well defined problem in front of engineers.

221
00:13:49,610 --> 00:13:53,586
And so basically, chaos monkey would, during business hours, it would pseudo

222
00:13:53,618 --> 00:13:57,206
randomly bring down an AMI on a random service. What it

223
00:13:57,228 --> 00:14:01,514
did was it put the problem of an AMI outage or

224
00:14:01,552 --> 00:14:05,740
disappearing in front of the engineers so they clearly knew what they had to

225
00:14:06,510 --> 00:14:10,006
build resilience into. And that's

226
00:14:10,038 --> 00:14:14,042
really what chaos engineering is about. It's about building with better context and better understanding

227
00:14:14,186 --> 00:14:17,998
who's doing chaos engineers. I've lost track at this point.

228
00:14:18,084 --> 00:14:21,566
There's roughly about 2000 companies or so in

229
00:14:21,588 --> 00:14:25,134
some. I mean, the CNCF has declared 2021 cyber

230
00:14:25,182 --> 00:14:28,702
chaos engineers being one of the top five crafts

231
00:14:28,766 --> 00:14:32,846
that are being adopted practices that's being adopted. But it's

232
00:14:32,878 --> 00:14:36,814
a little over. Just about every major company is now looking into it or adopting

233
00:14:36,862 --> 00:14:41,258
it in some level of maturity or fashion. This is no longer just a fat.

234
00:14:41,374 --> 00:14:45,046
So there's also three books out there, as David said. David and I were

235
00:14:45,068 --> 00:14:48,134
involved in writing the security chaos engineering report. Actually,

236
00:14:48,172 --> 00:14:51,642
if you stay tuned, toward the end of the presentation, we'll give a link to

237
00:14:51,696 --> 00:14:55,274
downloading that for free, as well as a link to

238
00:14:55,312 --> 00:14:59,366
download the Chaos engineering O'Reilly book written by my co founder,

239
00:14:59,398 --> 00:15:02,362
casey Rosenthal, the creator of chaos engineering at Netflix.

240
00:15:02,506 --> 00:15:05,978
And there's also the original report that Netflix wrote.

241
00:15:06,154 --> 00:15:09,466
So, instrumenting chaos, so loosely

242
00:15:09,498 --> 00:15:12,842
defined, where does chaos engineering fit in terms of instrumentation?

243
00:15:12,986 --> 00:15:16,082
So Casey always likes to frame it as

244
00:15:16,136 --> 00:15:20,066
testing versus experimentation. So testing is a verification or validation of something

245
00:15:20,088 --> 00:15:23,602
you know to be true or false. You know the information you're looking

246
00:15:23,656 --> 00:15:27,742
for before you go looking for it. Whereas in our world, it's a CVE

247
00:15:27,806 --> 00:15:28,580
and attack,

248
00:15:31,270 --> 00:15:34,022
that's we kind of know what we're looking for,

249
00:15:34,156 --> 00:15:38,038
whereas experimentation, we're trying to derive new understanding and new information

250
00:15:38,204 --> 00:15:42,022
that we did not have before. And we do that through the scientific method.

251
00:15:42,086 --> 00:15:45,402
We do that because the hypotheses are in the form

252
00:15:45,456 --> 00:15:48,810
of, I believe when x condition occurs on my system,

253
00:15:48,960 --> 00:15:52,566
y should be the response, right? And we actually introduce

254
00:15:52,598 --> 00:15:55,882
x in the system and observe whether or not y was a response,

255
00:15:55,946 --> 00:15:59,978
and we learn about what really happens. And it's quite interesting, because I've

256
00:15:59,994 --> 00:16:03,486
never seen a chaos engineering experiment succeed the first time

257
00:16:03,508 --> 00:16:06,898
we run it. What does that mean? What that means is our understanding of

258
00:16:06,904 --> 00:16:10,740
our systems is almost always wrong. And it's because

259
00:16:11,910 --> 00:16:15,422
speed and the scale and the complexity of software

260
00:16:15,566 --> 00:16:18,520
is hard for humans to keep track of.

261
00:16:19,050 --> 00:16:22,754
In the world of applied sciences, we refer to this as the complex

262
00:16:22,802 --> 00:16:27,030
adaptive systems. So Casey also likes to say, casey and Nora Jones

263
00:16:27,690 --> 00:16:31,366
like to talk about that. It's not about breaking things

264
00:16:31,468 --> 00:16:34,762
on purpose or breaking things in production. It's about fixing them

265
00:16:34,816 --> 00:16:38,762
proactively on purpose. Right. And Casey also likes to point out, if you're just

266
00:16:38,816 --> 00:16:42,282
like, I'm pretty sure if I went around breaking things, I wouldn't have a job

267
00:16:42,336 --> 00:16:45,550
very long. Right. And that's. That's important to recognize.

268
00:16:46,210 --> 00:16:49,694
So, security, chaos engineering. All right,

269
00:16:49,892 --> 00:16:53,680
what is the application of this to security? Well,

270
00:16:54,050 --> 00:16:57,166
what we're really trying to address is that hope is not an effective strategy.

271
00:16:57,198 --> 00:17:00,210
So, engineers, as an engineer, I've been a builder most of my career,

272
00:17:00,550 --> 00:17:03,954
and engineers, we don't believe in two things.

273
00:17:03,992 --> 00:17:07,026
We don't believe in hope. We don't believe in luck. Right.

274
00:17:07,128 --> 00:17:11,174
We believe in instrumentation and empirical data that tells us, is it working

275
00:17:11,212 --> 00:17:14,978
or not? Right. So we can have context to build from and improve what we're

276
00:17:14,994 --> 00:17:18,454
building. And so hope and luck, that worked

277
00:17:18,492 --> 00:17:22,166
in Star wars as an effective strategy, but it doesn't work in engineering.

278
00:17:22,358 --> 00:17:26,234
So what we're trying to do is understand our systems and its

279
00:17:26,272 --> 00:17:30,118
security gaps before an adversary does. So here's the problem that we're dealing

280
00:17:30,134 --> 00:17:33,338
with today, is that our systems need to be

281
00:17:33,424 --> 00:17:37,162
open and available to be able for a builder to have the flexibility

282
00:17:37,306 --> 00:17:40,862
to change, to change what they need to do to make the system function

283
00:17:40,916 --> 00:17:44,606
or to make the product operational for a customer. There's this need to change and

284
00:17:44,628 --> 00:17:47,890
change quickly, even if you're changing a small thing. But there's also

285
00:17:47,960 --> 00:17:51,474
the need for a security mechanism. CTO reflect that change.

286
00:17:51,592 --> 00:17:54,786
Right. What's happening is the speed of our

287
00:17:54,808 --> 00:17:57,846
ability to change and the flexibility. CTO change is coming at the cost of our

288
00:17:57,868 --> 00:18:01,174
ability. CTO understand and rapidly meet that change.

289
00:18:01,212 --> 00:18:04,882
From a security perspective, you magnify that problem with speed and scale.

290
00:18:05,026 --> 00:18:08,246
What's happening is things are slipping out

291
00:18:08,268 --> 00:18:11,318
of our ability. CTO understand what happened,

292
00:18:11,404 --> 00:18:15,098
right. So what's happening is adversaries are able to take advantage of

293
00:18:15,184 --> 00:18:18,618
our lack of visibility and understanding how effective our controls are or not.

294
00:18:18,704 --> 00:18:21,914
We have too much assumption and too much hope in how we build. We need

295
00:18:21,952 --> 00:18:24,880
more instrumentation post appointment to tell us, hey,

296
00:18:26,210 --> 00:18:29,166
when the misconfigured port happens, we have,

297
00:18:29,188 --> 00:18:33,802
like, ten controls. CTO catch that, or when we accidentally

298
00:18:33,866 --> 00:18:37,330
turn off encryption or downgrade encryption on an s three bucket,

299
00:18:37,750 --> 00:18:40,500
we should be able to detect that we have all these things in place,

300
00:18:41,110 --> 00:18:44,974
but what we're actually doing is we're introducing test conditions

301
00:18:45,022 --> 00:18:48,582
to ensure that that is constantly happening. So it's about

302
00:18:48,716 --> 00:18:52,626
continuously verifying that security works the way we think it does. We introduced

303
00:18:52,658 --> 00:18:56,322
the condition to ensure that when misconfiguration

304
00:18:56,386 --> 00:18:59,702
happens, that control catches it or stops it or blocks it,

305
00:18:59,756 --> 00:19:03,098
or reports log data or the alerts fire. I think David's going to talk

306
00:19:03,104 --> 00:19:06,940
a lot about how capital one does a lot of that, but that's really

307
00:19:07,630 --> 00:19:11,334
what we're trying to achieve here. So it's about reducing uncertainty

308
00:19:11,382 --> 00:19:15,066
through building confidence. So really, you're seeing a theme

309
00:19:15,098 --> 00:19:18,126
here, the application of chaos engineers. Security is no different.

310
00:19:18,228 --> 00:19:21,054
It's just we're looking at it more from the security problem set,

311
00:19:21,172 --> 00:19:24,754
but we're trying to correct how

312
00:19:24,792 --> 00:19:27,966
we believe the system works and how it actually works in reality.

313
00:19:27,998 --> 00:19:30,740
We're bringing that to meet.

314
00:19:31,350 --> 00:19:34,754
We build confidence as we

315
00:19:34,792 --> 00:19:38,526
build that that is actually true, that our understanding

316
00:19:38,718 --> 00:19:42,258
is in line with reality. So, use cases, I think David's

317
00:19:42,274 --> 00:19:44,466
going to expand on some of these in a minute, but some rough use cases

318
00:19:44,498 --> 00:19:48,730
you can apply chaos engineering to are instant response,

319
00:19:49,390 --> 00:19:52,890
security control validation, security observability,

320
00:19:54,350 --> 00:19:57,834
and also every chaos experiment has compliance value.

321
00:19:57,952 --> 00:20:01,354
So basically approving whether the technology worked the way you thought it did

322
00:20:01,392 --> 00:20:04,798
or not. Each one of these use cases you can find in the O'Reilly report

323
00:20:04,884 --> 00:20:08,670
expanded from people who have applied them in these particular

324
00:20:08,740 --> 00:20:12,394
areas. I'm going to talk a little bit about incident response in security

325
00:20:12,452 --> 00:20:15,540
cyber chaos engineers. The problem with instant response is

326
00:20:16,230 --> 00:20:18,260
response is actually the problem.

327
00:20:22,150 --> 00:20:25,282
So security incidents, by nature, are somewhat subjective. No matter

328
00:20:25,336 --> 00:20:29,198
how much money we have, how many people we hire, how many fancy controls

329
00:20:29,214 --> 00:20:32,566
we put in place, we still don't know. A lot. It's very subjective. We don't

330
00:20:32,588 --> 00:20:35,702
know where it's going to happen, why it's happening, who's trying to get in,

331
00:20:35,756 --> 00:20:38,714
how they're going to get in. And so all the things we put in place,

332
00:20:38,752 --> 00:20:42,326
we don't know if they're effective until the situation occurs.

333
00:20:42,438 --> 00:20:45,798
Right? With security chaos engineering,

334
00:20:45,814 --> 00:20:49,526
we're actually introducing that x condition, the condition that we prepped

335
00:20:49,558 --> 00:20:53,374
all that time, effort and money to

336
00:20:53,492 --> 00:20:57,534
occur or detect and block. And what we do is because

337
00:20:57,572 --> 00:21:01,262
we're no longer waiting for something to happen, to find out whether it

338
00:21:01,316 --> 00:21:04,686
was effective or not, we're introducing it purposefully, a signal into

339
00:21:04,708 --> 00:21:07,700
the system, trying to understand, do we have enough people on call?

340
00:21:09,670 --> 00:21:13,170
Did they have the right skills? Did the security

341
00:21:13,240 --> 00:21:16,806
controls throw good log data? Were we able to

342
00:21:16,828 --> 00:21:20,418
understand that log data? Did it throw an alert? Did the Soc

343
00:21:20,514 --> 00:21:23,814
process the security operations center,

344
00:21:23,852 --> 00:21:26,390
did they process the alert?

345
00:21:27,210 --> 00:21:30,938
All these things are difficult when you're to

346
00:21:31,024 --> 00:21:34,790
assess confidence when it's happening. We're doing this proactively,

347
00:21:34,870 --> 00:21:38,762
and we can now understand and measure and manage things we can no longer manage

348
00:21:38,816 --> 00:21:41,914
and measure before. So it's about flipping the model instead. The post

349
00:21:41,952 --> 00:21:45,406
mortem exercise being after the fact, in a way, you can

350
00:21:45,428 --> 00:21:49,210
think about it being the postmortem now being a preparation exercise,

351
00:21:49,290 --> 00:21:52,394
determining how effective we are at what we've been preparing

352
00:21:52,522 --> 00:21:56,274
for. So inner chaos slingers. So this is a tool I talked about

353
00:21:56,312 --> 00:21:58,820
earlier when I was at UnitedHealth group.

354
00:22:00,070 --> 00:22:03,442
We started experimenting with Netflix's chaos engineering. We started

355
00:22:03,496 --> 00:22:07,646
applying it to some security use cases around instant response, security control validation,

356
00:22:07,758 --> 00:22:11,418
and we got some amazing results. And that tool we wrote

357
00:22:11,454 --> 00:22:14,434
was called chaos Slinger. There's an r at the end of that slide.

358
00:22:14,482 --> 00:22:17,186
It's not showing up, but it's an open source tool. It's out there on GitHub.

359
00:22:17,218 --> 00:22:20,294
You can check it out. But really, chaos Slinger represents

360
00:22:20,342 --> 00:22:24,202
a framework on how to write experiments. There's three major

361
00:22:24,256 --> 00:22:28,486
functions. There's Slinger generator and tracker generator

362
00:22:28,598 --> 00:22:32,490
finds out the target to introduce the failure condition into

363
00:22:32,640 --> 00:22:35,966
Slinger actually makes the change, and tracker tracks the change and reports it into

364
00:22:35,988 --> 00:22:39,214
slack. So here's an example. So chaos Slinger had

365
00:22:39,252 --> 00:22:42,654
a primary example. We open sourced it. We needed something

366
00:22:42,692 --> 00:22:46,786
to share with the wider community so they would know kind of what

367
00:22:46,808 --> 00:22:50,318
we're trying to achieve. So we picked this example called Portslinger,

368
00:22:50,414 --> 00:22:54,926
because everyone kind of, we've been solving for misconfigured ports and firewalls

369
00:22:54,958 --> 00:22:58,594
for, like, 30 years, right? Whether you're a software engineer, a network engineer,

370
00:22:58,642 --> 00:23:02,598
a system engineer, executive, everybody kind of knows what a firewall does.

371
00:23:02,684 --> 00:23:06,386
Okay, so what port Slinger did, was it proactive?

372
00:23:06,418 --> 00:23:10,426
Would introduce a misconfigured or unauthorized port change into

373
00:23:10,528 --> 00:23:14,220
an AWS EC two security group. And so

374
00:23:14,830 --> 00:23:18,874
we started doing this throughout Unitedhealth group at the time. And so

375
00:23:18,912 --> 00:23:22,246
Unitedhealth group had a non commercial and a commercial software division.

376
00:23:22,358 --> 00:23:25,726
So what we started doing is started introducing it into these security gifts and

377
00:23:25,748 --> 00:23:28,746
started finding out that the firewall.

378
00:23:28,778 --> 00:23:32,074
So our expectation was that our firewalls would immediately detect

379
00:23:32,122 --> 00:23:35,054
and block it. It'll be a non issue. This is something we've been solving for,

380
00:23:35,092 --> 00:23:38,238
remember, for 30 years. We should be able to detect it. It'd be a non

381
00:23:38,254 --> 00:23:42,434
issue. That only happened about 60% of the time. The problem was,

382
00:23:42,472 --> 00:23:46,206
there's a drift issue between our commercial and our non commercial environments.

383
00:23:46,318 --> 00:23:49,534
Well, that was the first thing we learned, remember, there is no active incident.

384
00:23:49,582 --> 00:23:52,934
There was no active incident. There was no war room. We did this proactively to

385
00:23:52,972 --> 00:23:56,134
learn about was the system working the way it was supposed

386
00:23:56,172 --> 00:23:59,814
to. The second thing we learned was the cloud native configuration management tool.

387
00:23:59,852 --> 00:24:03,718
Caught it and blocked it almost every time. That was amazing. Something we're barely

388
00:24:03,734 --> 00:24:06,874
paying for is working better than we expected. A third

389
00:24:06,912 --> 00:24:10,842
thing we expected to happen was that both

390
00:24:10,896 --> 00:24:14,742
tools sent good log data to our security logging

391
00:24:14,806 --> 00:24:18,186
tool. We didn't really use a sim at the time. We used our own homegrown

392
00:24:18,218 --> 00:24:21,198
solution. So I wasn't sure if alert would be thrown because we're new to the

393
00:24:21,204 --> 00:24:25,118
cloud, we're very new to AWS at the time, so we're learning all these things

394
00:24:25,284 --> 00:24:28,850
and what happened is actually through alert. So that was awesome.

395
00:24:28,920 --> 00:24:32,514
That was the third thing I kind of didn't expect to happen happened. So the

396
00:24:32,552 --> 00:24:35,458
fourth thing is that the alert went to the Sock. But the SOC didn't know

397
00:24:35,464 --> 00:24:39,560
what to do with the alert because they couldn't tell what account it came from,

398
00:24:40,810 --> 00:24:43,814
what instance it came from. Now, as an engineers, you can say, well,

399
00:24:43,852 --> 00:24:48,182
Aaron, you can map back the IP address. Well, truth, that could take 515

400
00:24:48,316 --> 00:24:52,906
30 minutes. CTO do that if snat is in play, it could take 2 hours

401
00:24:53,088 --> 00:24:56,140
because snat hides the real IP address.

402
00:24:56,510 --> 00:24:59,610
But remember, if this were an incident or an outage,

403
00:25:00,670 --> 00:25:04,286
2 hours is very expensive when you're the largest healthcare company in

404
00:25:04,308 --> 00:25:07,594
the world. But there was no incident, there was no outage.

405
00:25:07,722 --> 00:25:10,110
Proactively figured out, oh crap,

406
00:25:11,890 --> 00:25:15,294
this would have been bad had this

407
00:25:15,332 --> 00:25:18,466
actually happened. So we're able to add metadata, pointers to

408
00:25:18,488 --> 00:25:21,538
the alert and then be able to fix it, right? There wasn't no problem.

409
00:25:21,624 --> 00:25:25,758
But that could have been very expensive had we waited for a surprise or

410
00:25:25,864 --> 00:25:29,126
CTO manifest on its own. But these are the kind of things you can

411
00:25:29,148 --> 00:25:35,750
uncover with security cyber chaos engineers.

412
00:25:35,900 --> 00:25:38,806
That's kind of a prime example. You can check out more examples. Like I said

413
00:25:38,828 --> 00:25:41,930
earlier in the book that David and I wrote,

414
00:25:42,750 --> 00:25:45,978
security enhanced engineering and why you should do so.

415
00:25:46,064 --> 00:25:49,354
As for why, I don't really see a choice. Looking back

416
00:25:49,392 --> 00:25:53,546
over CTO Aaron's slides, the relationship maps kind of look like coronavirus. The timely

417
00:25:53,578 --> 00:25:56,874
reference security is hard and it's getting harder.

418
00:25:56,922 --> 00:26:00,622
With all these vendors promising the world will be fixed by using their products,

419
00:26:00,756 --> 00:26:03,946
it's even more difficult to make good decisions

420
00:26:03,978 --> 00:26:08,082
on what the correct course of action is. Are the tools you have in working?

421
00:26:08,216 --> 00:26:11,506
Do you need new tooling? Is the problem that you need to invest time and

422
00:26:11,528 --> 00:26:15,154
effort. If it is, how do you know? People talk

423
00:26:15,192 --> 00:26:18,806
about needing to address the basics before they talk about doing the advanced things like

424
00:26:18,828 --> 00:26:22,882
security, chaos, engineers, they're kind of missing the point around SCE

425
00:26:22,946 --> 00:26:26,226
in general. If you build this foundation into your engineering

426
00:26:26,258 --> 00:26:29,330
teams and the engineering functions, it becomes a basic capability.

427
00:26:29,490 --> 00:26:33,114
It becomes business as usual, and it removes the lift needed

428
00:26:33,152 --> 00:26:37,066
to get it implemented later on. Many basic issues are

429
00:26:37,088 --> 00:26:40,358
the ones that you need to worry about before tackling an advanced adversary. But I'm

430
00:26:40,374 --> 00:26:43,766
not saying it's not important, because depending on your threat model, it can be,

431
00:26:43,888 --> 00:26:47,066
but you need to know what your capabilities are before it's taken advantage

432
00:26:47,098 --> 00:26:50,122
of by someone else. So I know it's going to be generalizing,

433
00:26:50,186 --> 00:26:53,726
but most things that we're seeing in security are kind

434
00:26:53,748 --> 00:26:57,230
of basic. Like we see password reuse credentials in GitHub,

435
00:26:57,310 --> 00:27:00,994
cross site scripting firewalls, the stuff that's been around for 30 years.

436
00:27:01,112 --> 00:27:04,626
They're all basic things everyone knows, and these issues can just pop up

437
00:27:04,648 --> 00:27:08,482
at any time. But it's not easy. Even though they're basic, they're definitely

438
00:27:08,536 --> 00:27:11,974
not easy. You can look back to some of the recent hacks like

439
00:27:12,012 --> 00:27:15,222
Solarwinds, and think, wow, how can they mess up something so easy?

440
00:27:15,276 --> 00:27:18,486
But if you're in a large environment, you kind of know the answer to that

441
00:27:18,508 --> 00:27:22,594
one. It's the complexity of things. Nothing is more permanent

442
00:27:22,642 --> 00:27:26,438
than doing something, a quick fix, something temporary.

443
00:27:26,614 --> 00:27:30,166
Nobody knows how all the systems interconnect, how one firewall

444
00:27:30,198 --> 00:27:35,546
rule can really affect another one, or even how service accounts can have undocumented

445
00:27:35,578 --> 00:27:38,734
services be affected. Not until you do it. And that's what needs

446
00:27:38,772 --> 00:27:42,206
to be observed and measured. You just do it like

447
00:27:42,228 --> 00:27:45,650
in a night's tale. That movie from like 100 years ago with Heath Ledger.

448
00:27:46,470 --> 00:27:49,714
It was a good movie, I think. So with

449
00:27:49,752 --> 00:27:52,260
baselining is where you want to get all this to.

450
00:27:52,630 --> 00:27:56,270
Trying to see what actually happens is how you establish that baseline.

451
00:27:56,430 --> 00:28:00,274
Do we really know that we can stop the malware or the vendors

452
00:28:00,322 --> 00:28:04,166
doing what it is they promise to do? Does a proxy really stop a

453
00:28:04,188 --> 00:28:07,286
specific category? Well, you don't really know unless you try.

454
00:28:07,388 --> 00:28:10,726
And when you try it, you document it and treat anything that you find

455
00:28:10,828 --> 00:28:14,154
as that baseline. So you know where you've been, so you know where you're going

456
00:28:14,192 --> 00:28:17,642
to and when you've got something to compare against. And why

457
00:28:17,696 --> 00:28:20,986
you want to do that is to understand when and how things change and to

458
00:28:21,008 --> 00:28:24,698
drive usable metrics that aren't just uptime, but it tells

459
00:28:24,714 --> 00:28:28,138
you how your tooling performs. So when you want to prove

460
00:28:28,154 --> 00:28:31,754
the way that things work, you want to prove that your tools

461
00:28:31,802 --> 00:28:35,018
function. You want to know that it's more than just a blinking light that you're

462
00:28:35,034 --> 00:28:38,306
going to be using to comply with a policy somewhere. You don't want to wait

463
00:28:38,328 --> 00:28:41,902
until the ghost is already in your ballroom before you check out your security capabilities

464
00:28:41,966 --> 00:28:46,046
function. How you think they do when you're asked, are we vulnerable

465
00:28:46,078 --> 00:28:49,142
to stuff? You'll have an answer. So instead of like loose waffling around

466
00:28:49,196 --> 00:28:52,774
with, well, we've got rules in place that will

467
00:28:52,812 --> 00:28:56,854
alert when it happens. You can prove that you have

468
00:28:56,892 --> 00:29:00,498
an answer by having the log, having the data that work that shows the

469
00:29:00,524 --> 00:29:04,346
results or triggered alerts. You can remove the guesswork and replace it

470
00:29:04,368 --> 00:29:07,574
with analysis and data. And as you tune your environment,

471
00:29:07,702 --> 00:29:11,126
you have a baseline to reference when detection or alerting failed.

472
00:29:11,158 --> 00:29:15,002
You'll know if you fine tune the rule too much because maybe a simple variable

473
00:29:15,066 --> 00:29:18,682
change has now bypassed your tooling. But in practice,

474
00:29:18,746 --> 00:29:22,126
it's not going to be that simple. With products,

475
00:29:22,228 --> 00:29:26,106
they're difficult. They're difficult to implement, they're difficult to configure

476
00:29:26,138 --> 00:29:29,138
and tune. And some of the things may not even do what they're supposed to

477
00:29:29,144 --> 00:29:32,338
be doing outside of a few test cases that the vendor prepared you

478
00:29:32,424 --> 00:29:35,906
so they knew that their tool would work. You can get

479
00:29:35,928 --> 00:29:39,986
poorly designed ux, which mandates vendor specific training and vendor time because

480
00:29:40,008 --> 00:29:43,286
you've got a deep investment in them, because that's the only way to know how

481
00:29:43,308 --> 00:29:46,886
their tools work, is to get deep training. When people move, they take that

482
00:29:46,908 --> 00:29:50,540
training knowledge with them and you've got to reinvest into new people.

483
00:29:51,310 --> 00:29:54,682
As your defenses move through this testing, you continue to

484
00:29:54,736 --> 00:29:58,298
update your expectations on what a system provides. You've got a

485
00:29:58,304 --> 00:30:01,594
way to actually show improvement that isn't just, we saw this many attacks last

486
00:30:01,632 --> 00:30:05,482
month, we blocked this many this month. You can show incremental improvement

487
00:30:05,546 --> 00:30:08,238
as you go from one spot to another to another.

488
00:30:08,404 --> 00:30:11,886
And as for why, maybe you're bored and you

489
00:30:11,908 --> 00:30:14,986
think, you know what, I want to jump in the deepest rabbit

490
00:30:15,018 --> 00:30:18,446
hole I can find. Let's see what I can find out. Because once you succeed

491
00:30:18,478 --> 00:30:22,126
at this, you're going to be really popular. You're going to be so popular,

492
00:30:22,318 --> 00:30:25,666
fetch might actually happen. I don't know.

493
00:30:25,688 --> 00:30:29,346
It hasn't happened for me yet. But what they say

494
00:30:29,368 --> 00:30:32,774
about when you do the same thing over and over again expecting a different result,

495
00:30:32,892 --> 00:30:36,034
but for reals, people get really excited. And when they get excited,

496
00:30:36,082 --> 00:30:39,842
they get ideas. And sometimes they're dangerous ideas. Dangerous meaning increased

497
00:30:39,906 --> 00:30:43,546
asks, expectations and wanting to get even more of the data that

498
00:30:43,568 --> 00:30:46,918
you're generating. And it's a good thing, but you don't want to be caught unprepared.

499
00:30:47,094 --> 00:30:50,822
So how do you get started with this? Like strong defenses,

500
00:30:50,886 --> 00:30:54,650
alerts going off. How are people and teams being rewarded?

501
00:30:54,810 --> 00:30:58,366
Uptime and visibility. When you're doing this, you want to

502
00:30:58,388 --> 00:31:01,934
give them support. And when you're supporting people, they don't want to feel

503
00:31:01,972 --> 00:31:05,274
like they're being attacked. When people feel attacked, they kind of buckle down and they

504
00:31:05,332 --> 00:31:08,674
kind of resist any of the efforts that you're doing. So you

505
00:31:08,712 --> 00:31:12,274
want to take the relationships you have, take what you know, and just

506
00:31:12,312 --> 00:31:16,226
sort of nurture them. So start with

507
00:31:16,248 --> 00:31:19,842
something that will show immediate impact, something quick, something easy.

508
00:31:19,976 --> 00:31:23,030
Your ultimate goal at this stage is to show the value of what you're doing

509
00:31:23,100 --> 00:31:25,846
and do it in a way that shows support. CTO, the team is tasked with

510
00:31:25,868 --> 00:31:29,542
owning and operating the platforms that you're experimenting on and with.

511
00:31:29,676 --> 00:31:32,806
Like, let's say you have a sock. Take some of the critical alerts they've got

512
00:31:32,828 --> 00:31:36,186
and test them. They haven't gone off in a while. It's even better because you

513
00:31:36,208 --> 00:31:39,546
don't really know what alerts work until they go off. So you do

514
00:31:39,568 --> 00:31:43,066
it and measure it if they work, awesome. We've got a success. Let's celebrate

515
00:31:43,098 --> 00:31:46,106
the success. Tell everyone in a way that get these teams rewarded,

516
00:31:46,138 --> 00:31:49,806
because too often people don't get rewarded for

517
00:31:49,828 --> 00:31:53,726
doing the things that they're doing well because they don't know.

518
00:31:53,908 --> 00:31:56,814
You will get that data driven metric to show that your tool is working,

519
00:31:56,852 --> 00:32:00,338
that your teams are working. So what I did is starting out, I used a

520
00:32:00,344 --> 00:32:03,934
combination of endpoint alerting and endpoint protection. I took an attack trophy.

521
00:32:03,982 --> 00:32:06,834
I documented an expected path that the attacker would take,

522
00:32:06,952 --> 00:32:10,882
what defenses are expected, what alerts would go off and just let loose

523
00:32:11,026 --> 00:32:14,386
within 15 minutes, roughly. I had my initial results

524
00:32:14,418 --> 00:32:17,734
and data to bring to my new best friends and I brought the data with

525
00:32:17,772 --> 00:32:21,330
questions like, is this what you expected to happen? Are the blogs what

526
00:32:21,340 --> 00:32:24,506
you expected? Did the alert go off when you expected it? If it

527
00:32:24,528 --> 00:32:28,726
did, awesome. So take that and repeat it every day and help generate

528
00:32:28,758 --> 00:32:31,966
metrics with that data. So I mentioned I

529
00:32:31,988 --> 00:32:35,358
like the to do. I really wasn't settling on

530
00:32:35,364 --> 00:32:37,680
this gift, but it worked for me.

531
00:32:39,890 --> 00:32:43,486
Getting started back then, it was still endpoints and it was something that

532
00:32:43,508 --> 00:32:46,666
I can test myself, something that I didn't need to get other people involved.

533
00:32:46,698 --> 00:32:51,086
I could say, all right, let me just do this. Look, because anytime

534
00:32:51,118 --> 00:32:54,018
you have to leverage other people, it's taking away from their day job, from the

535
00:32:54,024 --> 00:32:57,554
work that they needed to keep doing. So I picked something that we cared about,

536
00:32:57,592 --> 00:33:00,600
critical alerts. And then picked the ones that we haven't seen fire in a while.

537
00:33:01,130 --> 00:33:05,106
And for why that why it shows alerts. It's alerts

538
00:33:05,138 --> 00:33:08,374
go off when you're attacked. If you don't see alerts, is it because you're not

539
00:33:08,412 --> 00:33:12,130
being attacked? Maybe your alerts don't work. This thing logs.

540
00:33:12,210 --> 00:33:15,180
Maybe the tools don't actually stop it. They're supposed to stop.

541
00:33:15,790 --> 00:33:19,494
And for how starting it. I took something nondestructive.

542
00:33:19,542 --> 00:33:23,078
So lull bins, living off the land binaries powershell

543
00:33:23,174 --> 00:33:26,910
things that wouldn't actually affect the status of the machine. And I checked

544
00:33:27,250 --> 00:33:30,718
it didn't want to do anything destructive because if you leave the machine in

545
00:33:30,724 --> 00:33:33,870
a state worse than when you found it, that could be something

546
00:33:33,940 --> 00:33:37,666
very bad. You could put like a trademark on that, very bad in

547
00:33:37,688 --> 00:33:41,106
the future. So when I did that, did I see

548
00:33:41,128 --> 00:33:44,546
what I expected? Did it respond how I thought it would? If so, cool.

549
00:33:44,648 --> 00:33:48,034
Now figure out how to automate it. Run it every day. It's a small thing,

550
00:33:48,072 --> 00:33:51,526
but you're starting to establish that baseline that you can use to know

551
00:33:51,548 --> 00:33:55,154
your basic level of protections and have a spot to compare. It was your environment

552
00:33:55,202 --> 00:33:58,920
gets tuned, gets updated, gets fixed, and then

553
00:34:00,010 --> 00:34:03,386
when you're getting started, you need to find what your

554
00:34:03,408 --> 00:34:06,906
target cares about, what their goal is, changes the approach that

555
00:34:06,928 --> 00:34:10,214
should be taken, like identifying malware detection, isn't going to necessarily

556
00:34:10,262 --> 00:34:14,142
be the same approach that you would take for validating alerts, trigger or things

557
00:34:14,196 --> 00:34:17,386
like lateral movement, network based attacks.

558
00:34:17,578 --> 00:34:20,766
And again, report on what you're doing. You want to be

559
00:34:20,788 --> 00:34:24,306
like Malcolm up there. You want to just observe, report, show the

560
00:34:24,328 --> 00:34:27,714
changes, and show teamwork to make things better. So getting

561
00:34:27,752 --> 00:34:31,202
back into the use cases that Aaron was talking about.

562
00:34:31,336 --> 00:34:34,050
So do critical alerts work?

563
00:34:34,200 --> 00:34:37,602
It's always going to be a rhetorical question, do they work?

564
00:34:37,736 --> 00:34:40,998
We all know alerts work until they don't. And sometimes you find out at

565
00:34:41,004 --> 00:34:44,546
the worst possible time that something stops working. But there is a way to prevent

566
00:34:44,578 --> 00:34:47,746
this. You test them. Always be running the attacks.

567
00:34:47,778 --> 00:34:51,322
It's not as cool as ABC. Always be closing, but that was

568
00:34:51,376 --> 00:34:54,854
close enough. Like, always run attacks, you schedule

569
00:34:54,902 --> 00:34:58,326
them, just run them at consistent time so you can identify outliers.

570
00:34:58,518 --> 00:35:01,626
If you're running at 08:00 a.m. Every single day, if you suddenly see it at

571
00:35:01,648 --> 00:35:05,600
815 or 830, like, okay, well, what's going on that day? Was there

572
00:35:06,290 --> 00:35:09,918
an issue with logging that we didn't know about? When you

573
00:35:09,924 --> 00:35:13,258
do that, you're going to have historical trends of alert uptime, ensuring the alerts

574
00:35:13,274 --> 00:35:16,866
trigger as expected. Even though tuning processes in the future may

575
00:35:16,888 --> 00:35:20,178
inadvertently change how systems behave. You'll know what's working

576
00:35:20,264 --> 00:35:23,422
and what is not working. With the typical

577
00:35:23,486 --> 00:35:26,980
alerts become the negatives. Like, if you're like any

578
00:35:27,350 --> 00:35:30,866
company ever, you hate false positives. Unlimited resources

579
00:35:30,898 --> 00:35:33,830
aren't a thing, and teams have to prioritize what they spend their time on.

580
00:35:33,900 --> 00:35:37,174
What do you do with false positives? In many cases, you just

581
00:35:37,212 --> 00:35:40,666
spend the time and you tune out the excessive noise. But like anything, it can

582
00:35:40,688 --> 00:35:44,266
cause its own issues. Like has false positive production made your alerts so

583
00:35:44,288 --> 00:35:47,450
brittle that you're now missing attack that you used to catch?

584
00:35:47,870 --> 00:35:51,514
And our alerts hyper focus on a single technique that

585
00:35:51,552 --> 00:35:54,686
introduces false negatives. Have alerts been so finely tuned that

586
00:35:54,708 --> 00:35:57,994
a slight variable tweaking now no longer triggers

587
00:35:58,042 --> 00:36:01,630
any logging or alerts? If so, well, at least you know now,

588
00:36:01,700 --> 00:36:05,034
because then you can feed that into your other use cases, such as like engineering,

589
00:36:05,082 --> 00:36:08,610
hunting, security architecture. So when the alerts fire,

590
00:36:08,680 --> 00:36:11,698
how long does it take for the alert to fire? Does it fire when you

591
00:36:11,704 --> 00:36:15,486
think it's going to happen? Because sometimes, in reference to the tuning, when you tune

592
00:36:15,518 --> 00:36:19,046
something, you can still affect time to alert, which is going to also affect the

593
00:36:19,068 --> 00:36:22,338
time to respond. And do the results show what's expected?

594
00:36:22,434 --> 00:36:25,910
If you're expecting the result within ten minutes, do you get them within ten minutes?

595
00:36:25,980 --> 00:36:29,914
If maybe not. If so, cool. If not, that's when most

596
00:36:29,952 --> 00:36:34,186
of the work starts, because now you've got to start tracing back.

597
00:36:34,368 --> 00:36:37,802
Where is everything going? What does the

598
00:36:37,936 --> 00:36:41,520
ingestion path look like? What do we need to change?

599
00:36:42,770 --> 00:36:46,222
And finally, with there, it's helping to train

600
00:36:46,276 --> 00:36:49,994
the new people they're going through. Standard training exercises

601
00:36:50,042 --> 00:36:53,262
can be boring. So what we do is we try to spice it up.

602
00:36:53,316 --> 00:36:56,754
Since it's already in production, there's not really a better way to get people to

603
00:36:56,792 --> 00:37:00,478
practice investigations and doing real work. Like, how are they actually responding

604
00:37:00,494 --> 00:37:04,178
to security events? Do they adhere to playbooks? Are the playbooks right?

605
00:37:04,264 --> 00:37:07,526
Are there critical steps that people are keeping in their heads that never actually

606
00:37:07,548 --> 00:37:10,920
made it? CTo, the electronic paper that they're writing on.

607
00:37:11,370 --> 00:37:14,946
So testing removes guessing, and when you remove that guessing,

608
00:37:14,978 --> 00:37:18,726
you're removing mistakes. Like, everyone will make mistakes. It's part of everything that we

609
00:37:18,748 --> 00:37:22,274
do. We make mistakes all the time. It's just how we recover

610
00:37:22,322 --> 00:37:25,626
from it. How we respond to it. And when you're doing it, just make

611
00:37:25,648 --> 00:37:28,858
sure you have permission and you've got approval to do it, because otherwise you

612
00:37:28,864 --> 00:37:32,442
can also have a pretty bad day. So as we get to another

613
00:37:32,496 --> 00:37:35,786
use case of gap hunting, gap hunting, it relates

614
00:37:35,818 --> 00:37:39,502
to all of this because was you're finding out that your tools are running

615
00:37:39,556 --> 00:37:42,846
and you're missing things like, well, you are in a really

616
00:37:42,868 --> 00:37:46,354
good position to identify what you don't see. You're able to know what you

617
00:37:46,392 --> 00:37:50,194
don't detect. And there's a lot of overlap with this in something

618
00:37:50,232 --> 00:37:53,666
like detection engineering, but still there's a difference. We're not

619
00:37:53,688 --> 00:37:57,234
really worried about what we're detecting yet, because the primary goal is to find

620
00:37:57,272 --> 00:38:00,726
out how systems are responding, how they work, and establishing a baseline of

621
00:38:00,748 --> 00:38:04,374
behavior that we know can be detected and was well as understand what our

622
00:38:04,412 --> 00:38:06,920
security stack sees and what it doesn't see.

623
00:38:10,760 --> 00:38:14,324
And the behavior detection leads to

624
00:38:14,362 --> 00:38:18,068
discovering gaps in understanding like what was missed. Did we expect

625
00:38:18,154 --> 00:38:20,870
to actually see it? Is there a configuration issue?

626
00:38:21,960 --> 00:38:25,096
Has something changed that we never thought about when we did the

627
00:38:25,118 --> 00:38:28,232
initial deployment? Does deviating from an expected attack

628
00:38:28,286 --> 00:38:32,068
result, attack technique results

629
00:38:32,084 --> 00:38:36,316
in reduced visibility? Does changing a delivery technique from

630
00:38:36,418 --> 00:38:39,980
one method to another, let's say Powershell versus

631
00:38:40,480 --> 00:38:43,756
python, does it produce false negatives? Is it

632
00:38:43,778 --> 00:38:47,228
something that is detected only? Is it something that we can change in

633
00:38:47,234 --> 00:38:50,536
the future as we improve the environment? Can we go from detection

634
00:38:50,568 --> 00:38:54,210
only to prevention, too? If so, that's another huge win

635
00:38:56,580 --> 00:39:00,400
that can also lead into the next use case of engineering.

636
00:39:00,980 --> 00:39:04,228
I apologize. I ran out of gifts. I think the

637
00:39:04,314 --> 00:39:07,924
child was maybe four years old when he drew a cat, and it

638
00:39:07,962 --> 00:39:11,110
was really one of the most frightening things I've ever seen.

639
00:39:11,560 --> 00:39:13,450
Imagine if that came to life.

640
00:39:15,740 --> 00:39:20,744
I don't know. I don't know what to think about this. But all

641
00:39:20,782 --> 00:39:24,664
the other cases can lead back into something for engineering, like the return on an

642
00:39:24,702 --> 00:39:28,188
investment. Tooling and staffing isn't cheap. None of

643
00:39:28,194 --> 00:39:32,456
this is cheap. So it's great to know. Are the things that we're investing

644
00:39:32,488 --> 00:39:35,356
on in the tools, the people?

645
00:39:35,538 --> 00:39:38,716
Is it paying off? Is everything working the way that

646
00:39:38,738 --> 00:39:42,332
we want to? Are the tools optimized? Do we have duplicative tuning?

647
00:39:42,476 --> 00:39:46,368
Like maybe we've got five, six endpoint agents that

648
00:39:46,374 --> 00:39:50,144
all do the exact same thing? Do you need that many? Do they have

649
00:39:50,262 --> 00:39:53,524
any extra capabilities the other one doesn't have? If not,

650
00:39:53,642 --> 00:39:57,584
do you need them all? Can you reduce complexity in the environment without reducing

651
00:39:57,632 --> 00:40:01,280
your security capabilities? How is your defense in depth?

652
00:40:01,360 --> 00:40:05,336
Like how many layers can fail before you're in trouble. Let's say you've got ten

653
00:40:05,358 --> 00:40:08,984
different layers of security that you're expecting and you

654
00:40:09,022 --> 00:40:12,696
know that you can handle up to three of them failing. That's good to

655
00:40:12,718 --> 00:40:16,904
know. So you can determine, do we want to get 100% coverage

656
00:40:16,952 --> 00:40:20,492
in every single tool? Can we live without some? Can we be 80%

657
00:40:20,546 --> 00:40:24,104
effective like the bugs in starship troopers

658
00:40:24,152 --> 00:40:28,210
when Dookie Hauser was showing combat effectiveness? Can you do that?

659
00:40:32,450 --> 00:40:35,390
And that also leads into measuring your environment.

660
00:40:35,970 --> 00:40:39,706
Without measurement, you can't really show progress. It's like the training montage

661
00:40:39,738 --> 00:40:43,086
from footloose. The guy on the left, he goes from not

662
00:40:43,108 --> 00:40:46,474
being able to snap to getting just stand up applause from Kevin Bacon.

663
00:40:46,522 --> 00:40:49,746
I don't remember his character's name, but I guess just Kevin Bacon is

664
00:40:49,768 --> 00:40:52,946
going to be enough for me. Measurement is going to help. When you stand up

665
00:40:52,968 --> 00:40:55,986
and say, here's where we were last quarter, here's where we are now, here's where

666
00:40:56,008 --> 00:40:59,606
we've improved, here's what I think should be targeted, and this is where we

667
00:40:59,628 --> 00:41:02,966
hope to get so you can start planning out your roadmap, where you were,

668
00:41:02,988 --> 00:41:07,142
where you are, where you're going. Audits become a lot easier when

669
00:41:07,196 --> 00:41:10,342
metrics are constantly being generated. So instead of saying,

670
00:41:10,476 --> 00:41:13,626
we need to show this one point in time when something works, you could say,

671
00:41:13,648 --> 00:41:16,300
okay, well, here's how you get the data.

672
00:41:17,310 --> 00:41:22,426
We've been running this for this long. We have consistent

673
00:41:22,458 --> 00:41:26,622
reporting all the way through every week. So instead

674
00:41:26,676 --> 00:41:30,654
of being a snapshot in time, showing compliance that

675
00:41:30,692 --> 00:41:34,398
one time, way back when, you are always showing compliance because you

676
00:41:34,484 --> 00:41:38,194
are proving that you are secure and all the data can be used to support

677
00:41:38,232 --> 00:41:42,222
the people you work with for improving the entire security program. And it helps

678
00:41:42,286 --> 00:41:46,146
use data driven decisions on what to focus on what needs help and providing

679
00:41:46,178 --> 00:41:48,760
evidence that what used to work still works.

680
00:41:51,130 --> 00:41:54,630
There are some risks. Sometimes people

681
00:41:54,700 --> 00:41:58,118
don't care. It could be the way it's presented. But sometimes

682
00:41:58,204 --> 00:42:02,022
there are other problems. If you don't do it the right way,

683
00:42:02,076 --> 00:42:05,126
telling people about problems can make them feel bad, then they get the pushback.

684
00:42:05,158 --> 00:42:08,186
So what I use is more of a framework for the issues. Like here's what

685
00:42:08,208 --> 00:42:11,062
I see. I'm saying maybe it's incomplete metrics,

686
00:42:11,126 --> 00:42:13,962
maybe there's alerts without context.

687
00:42:14,106 --> 00:42:17,386
Here's my interpretation. And then ask, what am I missing?

688
00:42:17,498 --> 00:42:20,942
Here's what I think that means. What do you think? So it could be something

689
00:42:20,996 --> 00:42:24,382
that people know about it. It's something that's a work in progress.

690
00:42:24,446 --> 00:42:27,634
It's important not to go in there head

691
00:42:27,672 --> 00:42:30,818
on fire, pants on your head. Go in there

692
00:42:30,904 --> 00:42:33,966
with a collaborative effort, a collaborative spirit,

693
00:42:34,158 --> 00:42:37,554
and then deliver one message that you want them to walk away

694
00:42:37,592 --> 00:42:41,366
with. Why should they believe this message? What confidence do you have? Just provide all

695
00:42:41,388 --> 00:42:45,094
of that and it makes it a lot easier to have a good working relationship

696
00:42:45,212 --> 00:42:48,306
and move the needle,

697
00:42:48,498 --> 00:42:49,800
is what they say.

698
00:42:52,010 --> 00:42:55,894
And my final messages are, don't worry, because if

699
00:42:55,932 --> 00:42:59,320
I can do it, you can do it.

700
00:43:04,060 --> 00:43:07,508
Just like that. So here's

701
00:43:07,524 --> 00:43:10,728
the link to the security chaos engineering book. You can download it

702
00:43:10,734 --> 00:43:13,460
for free. Free copy, compliments of Verica.

703
00:43:13,620 --> 00:43:17,684
And if you're also interested in chaos

704
00:43:17,732 --> 00:43:20,520
engineering, here's a link to both books. Great.

705
00:43:20,590 --> 00:43:21,190
Thank you very much.

