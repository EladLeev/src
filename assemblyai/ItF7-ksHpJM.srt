1
00:01:42,590 --> 00:01:46,578
You. Hi everybody, my name is Robert Hodges and

2
00:01:46,584 --> 00:01:50,382
I'd like to welcome you to my talk on fast, cheap doityourself

3
00:01:50,446 --> 00:01:54,254
monitoring with Opensource analytics and visualization.

4
00:01:54,382 --> 00:01:58,530
I'm presenting today at Comp 42 devsecops 2023.

5
00:01:58,600 --> 00:02:02,002
I'd like to thank the organizers for inviting me to talk and for

6
00:02:02,056 --> 00:02:05,538
doing all the work to make this conference possible. Thanks a

7
00:02:05,544 --> 00:02:08,966
bunch. It's a pleasure to be here. Let's do a few intros.

8
00:02:09,078 --> 00:02:12,566
So, my name again, Robert Hodges. I've been working on databases

9
00:02:12,678 --> 00:02:15,578
for over 30 years. Actually this year it's 40.

10
00:02:15,744 --> 00:02:19,740
And I've been heavily involved with open source Kubernetes security.

11
00:02:21,010 --> 00:02:25,818
Other issues related to operational topics

12
00:02:25,914 --> 00:02:29,182
around managing data, particularly in cloud and cloud

13
00:02:29,236 --> 00:02:32,650
native environments. My day job, I run a company called

14
00:02:32,740 --> 00:02:36,174
Altinity. We are a service provider

15
00:02:36,222 --> 00:02:40,034
for Clickhouse. It's a very popular data warehouse. We'll be talking about

16
00:02:40,072 --> 00:02:42,980
this good chunk of this talk.

17
00:02:44,230 --> 00:02:47,314
We run a cloud, so we have hundreds of clusters that we run on

18
00:02:47,352 --> 00:02:51,286
behalf of people. We also help a very large number of people run it themselves.

19
00:02:51,468 --> 00:02:54,898
Among other things were the authors of the Kubernetes operator for Clickhouse.

20
00:02:54,914 --> 00:02:58,658
So if you run clickhouse and use cloud native approach,

21
00:02:58,754 --> 00:03:02,026
run it on Kubernetes, it's a good chance. Using our software already.

22
00:03:02,208 --> 00:03:05,900
And just a little bit about my colleagues who've helped put together

23
00:03:06,910 --> 00:03:09,966
the information behind this talk. We have about 45 people in

24
00:03:09,988 --> 00:03:13,230
the company, spread out over 16 countries. We are,

25
00:03:13,300 --> 00:03:17,546
by and large, database geeks, centuries of experience with database

26
00:03:17,578 --> 00:03:20,670
and applications, particularly analytic databases.

27
00:03:21,730 --> 00:03:24,930
So let's jump right in. Monitoring. Why do we do it?

28
00:03:25,000 --> 00:03:28,814
Well, it could be because we like looking at nice screens,

29
00:03:28,862 --> 00:03:32,578
but really it's to answer questions. So when something

30
00:03:32,664 --> 00:03:36,920
happens in your system, for example, users start to see performance problems.

31
00:03:37,450 --> 00:03:41,206
You want to know why. And as you dig deeper, like when

32
00:03:41,228 --> 00:03:45,618
do the performance problems start? How many users are affected?

33
00:03:45,714 --> 00:03:49,594
Which of the services is at fault? These are questions that

34
00:03:49,632 --> 00:03:53,814
require data, and moreover, they require a history of the systems

35
00:03:53,862 --> 00:03:57,546
in order to answer. Now, in the

36
00:03:57,568 --> 00:04:01,142
old days, we used to take a slightly different approach,

37
00:04:01,206 --> 00:04:04,862
which leads to a question, what's the best way to answer those

38
00:04:04,916 --> 00:04:08,320
questions I showed on the previous slide? Here's the old way.

39
00:04:09,010 --> 00:04:12,286
Go into the system, lay hands on it, run vm stat, kind of

40
00:04:12,308 --> 00:04:16,098
watch the numbers until they become blurry. So would you like to

41
00:04:16,104 --> 00:04:19,218
do it this way, or would you like to do

42
00:04:19,224 --> 00:04:22,594
it visually? So chances are if you're in this

43
00:04:22,632 --> 00:04:26,134
business, you already have monitoring like this. This is actually

44
00:04:26,172 --> 00:04:29,826
Grafana, which we'll be talking about. But this type of visual

45
00:04:29,858 --> 00:04:33,622
display is much easier to understand, interpret and

46
00:04:33,676 --> 00:04:37,238
use. So visual displays, well,

47
00:04:37,324 --> 00:04:41,034
there's a lot of systems that will actually do this that come

48
00:04:41,072 --> 00:04:43,980
right off the shelf. Now.

49
00:04:45,150 --> 00:04:48,938
There have been proprietary solutions developed in this space for years.

50
00:04:49,104 --> 00:04:52,826
And in fact, if anything, over the last few years we've seen a blossoming

51
00:04:52,858 --> 00:04:56,334
of systems to do observability in general and

52
00:04:56,372 --> 00:04:59,786
monitoring system monitoring in particular. But perhaps they're

53
00:04:59,818 --> 00:05:04,106
not for you. One simple reason is they can be very costly.

54
00:05:04,298 --> 00:05:07,650
But another one is that you may have specialized needs

55
00:05:07,800 --> 00:05:11,774
for monitoring that you need to cover. Perhaps your business is monitoring,

56
00:05:11,822 --> 00:05:15,314
so you don't want to use somebody else's system. You're developing your own, you may

57
00:05:15,352 --> 00:05:18,906
want to own the stack. There's a bunch of reasons

58
00:05:18,958 --> 00:05:21,766
you may want to control the data. There's a bunch of reasons why you might

59
00:05:21,788 --> 00:05:25,206
want to do it yourself. So let's look

60
00:05:25,228 --> 00:05:28,758
at how to do that. The basic system that we're going to build

61
00:05:28,844 --> 00:05:32,362
to do monitoring is we're going to have a system that

62
00:05:32,416 --> 00:05:35,974
consists of three parts. We have the source

63
00:05:36,022 --> 00:05:39,914
data, so we need something that can collect that data and

64
00:05:39,952 --> 00:05:43,706
ingest it. We need a place for it to live. And that's

65
00:05:43,738 --> 00:05:47,086
what we call an analytic database. This is a database that's designed to

66
00:05:47,108 --> 00:05:50,446
hold large amounts of data and answer questions on it

67
00:05:50,468 --> 00:05:53,742
very quickly. And then finally we need a mechanism to

68
00:05:53,796 --> 00:05:57,650
display it so that you end up with some nice graphical visualization

69
00:05:57,990 --> 00:06:01,300
like what I showed you a couple of slides ago.

70
00:06:01,750 --> 00:06:04,934
So let's look into how we would go about building that type

71
00:06:04,972 --> 00:06:08,614
of system. So the first thing we want to do is pick

72
00:06:08,652 --> 00:06:12,386
an open source analytic database. Open source

73
00:06:12,418 --> 00:06:16,230
databases tend to be problem specific,

74
00:06:16,380 --> 00:06:19,818
and as we're looking at them, there are several that you might consider.

75
00:06:19,904 --> 00:06:24,214
So you might consider open search, which is the open source version of elasticsearch

76
00:06:24,342 --> 00:06:28,422
that's great for full text searches on unstructured

77
00:06:28,486 --> 00:06:31,930
data. It can be used for log analytics.

78
00:06:32,090 --> 00:06:35,834
You could also use Presto, which is a very powerful

79
00:06:35,882 --> 00:06:39,930
database that can do federated query across many data sources

80
00:06:40,010 --> 00:06:43,166
and information and data lakes. But for this

81
00:06:43,188 --> 00:06:46,754
type of system, particularly observability, one of the best

82
00:06:46,792 --> 00:06:49,630
choices on the market is Clickhouse,

83
00:06:49,710 --> 00:06:53,794
which allows you to do real time analytics. By that I mean

84
00:06:53,992 --> 00:06:57,670
be able to run queries and get answers back almost instantly.

85
00:06:59,850 --> 00:07:04,402
And it can do this performantly

86
00:07:04,466 --> 00:07:07,430
and easily on very, very large quantities of data.

87
00:07:07,580 --> 00:07:11,414
It's in fact used for an enormous number of use cases

88
00:07:11,462 --> 00:07:14,326
ranging from web analytics to network flow logs.

89
00:07:14,358 --> 00:07:16,902
Observability, of course, financial markets,

90
00:07:16,966 --> 00:07:20,758
seim, so on and so forth. Super popular for this and

91
00:07:20,784 --> 00:07:24,346
a great choice. So here's

92
00:07:24,378 --> 00:07:27,454
a short list of reasons why Clickhouse has

93
00:07:27,492 --> 00:07:30,766
turned out to be such a good choice for so many people.

94
00:07:30,948 --> 00:07:34,498
So it is a SQL database. In fact, in many ways it has

95
00:07:34,664 --> 00:07:38,462
the simplicity and the accessibility of a system like MySQL.

96
00:07:38,606 --> 00:07:42,254
So it understands SQL, it runs practically anywhere.

97
00:07:42,302 --> 00:07:45,926
You can literally run it on a phone. There was actually a demo of that

98
00:07:45,948 --> 00:07:49,622
a few years back, all the way up to running

99
00:07:49,676 --> 00:07:52,966
it in huge clusters containing hundreds of servers in the

100
00:07:52,988 --> 00:07:56,774
cloud. It's also open source in this particular case,

101
00:07:56,892 --> 00:07:59,946
Apache 20, which is super flexible and gives you the

102
00:07:59,968 --> 00:08:03,830
ability to run it for any purpose. In addition,

103
00:08:03,910 --> 00:08:07,606
it has very powerful analytic capabilities. So it shares

104
00:08:07,638 --> 00:08:09,420
this with many,

105
00:08:11,630 --> 00:08:15,370
in fact a number of features that are standard for analytic databases.

106
00:08:15,450 --> 00:08:19,326
Overall, they include storing data in columns. We'll show an example of

107
00:08:19,348 --> 00:08:22,766
that in the following page about why

108
00:08:22,788 --> 00:08:26,834
that's such an important feature. It can also parallelize execution very

109
00:08:26,872 --> 00:08:30,226
well, so that the data is organized so it can be read quickly, and then

110
00:08:30,248 --> 00:08:34,206
it can read from many locations in parallel, and it scales

111
00:08:34,238 --> 00:08:37,734
to many petabytes. So these are all

112
00:08:37,772 --> 00:08:41,270
good reasons why Clickhouse has become a core engine

113
00:08:41,340 --> 00:08:45,080
for real time analytics. Across the use cases that I mentioned,

114
00:08:45,690 --> 00:08:49,590
let's look at some of the details that are relevant for observability

115
00:08:49,670 --> 00:08:53,402
and monitoring. So as I mentioned,

116
00:08:53,456 --> 00:08:57,114
clickhouse is optimized for very fast response on large data

117
00:08:57,152 --> 00:09:01,180
sets. So if you actually look at the data, you can see that

118
00:09:01,710 --> 00:09:05,214
you'll quickly see, particularly if you go in and look at its storage, that each

119
00:09:05,252 --> 00:09:09,162
column is stored separately, basically as an array.

120
00:09:09,306 --> 00:09:12,682
So when you go and look at the on disk representation,

121
00:09:12,746 --> 00:09:16,754
the columns, each of them has a couple of files that

122
00:09:16,792 --> 00:09:20,002
elements it. So within that column you have very

123
00:09:20,056 --> 00:09:23,918
highly compressed data. Putting things into an array

124
00:09:23,934 --> 00:09:27,862
like this makes it easier to compress. Moreover, it's sorted, which can result

125
00:09:27,916 --> 00:09:31,160
in and then compression is applied to it.

126
00:09:32,410 --> 00:09:35,858
Particularly in observability cases, we can often get compression levels

127
00:09:35,874 --> 00:09:40,138
of 90 or even 95% on data. Second thing

128
00:09:40,224 --> 00:09:43,734
is we have replication between nodes, so we can maintain

129
00:09:43,782 --> 00:09:47,434
multiple nodes and then query across them.

130
00:09:47,552 --> 00:09:51,306
And the third using is we have vectorized or parallelized query,

131
00:09:51,418 --> 00:09:54,526
which can run across all nodes. In fact,

132
00:09:54,708 --> 00:09:58,638
you can also divide data up into shards and

133
00:09:58,724 --> 00:10:01,822
run parallel query across them. This allows you to apply

134
00:10:01,876 --> 00:10:05,634
the power of multiple machines if you need a fast answer. And then

135
00:10:05,672 --> 00:10:09,006
within single machines, clickhouse is extremely efficient.

136
00:10:09,118 --> 00:10:13,410
It uses what's called vectorized query, where we basically treat these columns

137
00:10:14,070 --> 00:10:17,506
basically as array values, because that's how they're stored

138
00:10:17,538 --> 00:10:21,154
and can take advantage of things like SIMD instructions,

139
00:10:21,202 --> 00:10:25,042
a single instruction, multiple data. Also, we have great performance

140
00:10:25,106 --> 00:10:28,634
because this kind of data aligns well with the cache structure in

141
00:10:28,672 --> 00:10:32,442
modern cpus. So for all these reasons and

142
00:10:32,496 --> 00:10:35,180
more, clickhouse tends to be extremely fast.

143
00:10:35,870 --> 00:10:39,962
Another thing that clickhouse does that makes it very nice for monitoring data is

144
00:10:40,016 --> 00:10:44,094
it has a huge number of what we call input formats. So these are

145
00:10:44,132 --> 00:10:47,662
things like CSV, which is one of the most widely used

146
00:10:47,716 --> 00:10:51,726
formats in all of it, but also CSV with names where

147
00:10:51,748 --> 00:10:55,542
you have the name of each column in the first row.

148
00:10:55,626 --> 00:10:59,138
We can read Json, we can read what's called Json. Each row, which is

149
00:10:59,144 --> 00:11:02,318
a record. Each record is a separate JSON document, protobuff,

150
00:11:02,334 --> 00:11:05,666
parquet, tab separated, you name it, there's dozens of

151
00:11:05,688 --> 00:11:08,846
these. And what this means is that there's a

152
00:11:08,888 --> 00:11:12,230
pretty good chance that the data that's being emitted from your monitoring system,

153
00:11:12,300 --> 00:11:15,286
clickhouse, just knows what it is and can read it and stick it in a

154
00:11:15,308 --> 00:11:18,758
table. Finally, once you get in the table,

155
00:11:18,854 --> 00:11:22,220
clickhouse is extremely good support for time ordered data.

156
00:11:23,150 --> 00:11:27,082
And that's important because monitoring data

157
00:11:27,136 --> 00:11:30,990
is fundamentally time series. It is a series of measurements

158
00:11:31,490 --> 00:11:35,098
on things, for example like hosts, that have particular properties

159
00:11:35,194 --> 00:11:40,426
and then particular measurements associated

160
00:11:40,458 --> 00:11:44,226
with a point in time on that host. So there

161
00:11:44,248 --> 00:11:47,566
are three date types, regular dates,

162
00:11:47,598 --> 00:11:51,074
which are pretty useless for high granularity data,

163
00:11:51,192 --> 00:11:54,718
but date time, which is your typical Unix timestamp,

164
00:11:54,734 --> 00:11:58,394
and then date time 64, so you can get precision down to nanosecond.

165
00:11:58,542 --> 00:12:01,894
Bi tools tend to like date time, and then there's a whole

166
00:12:01,932 --> 00:12:05,606
raft of functions that allow you to process the data. So for

167
00:12:05,628 --> 00:12:09,382
example, to be able to normalize a date to the year, to the nearest hour,

168
00:12:09,516 --> 00:12:12,698
to the start of the year, so on and so forth, as well as a

169
00:12:12,704 --> 00:12:15,738
bunch of conversion functions to pop out to

170
00:12:15,904 --> 00:12:19,466
turn it into a month, so on and so forth. So these

171
00:12:19,488 --> 00:12:22,874
are all great reasons for using Clickhouse

172
00:12:22,922 --> 00:12:26,480
and that make it particularly effective for this kind of

173
00:12:27,090 --> 00:12:30,270
application. Speaking of Grafana,

174
00:12:30,690 --> 00:12:34,482
Clickhouse pairs really well with Grafana when you're building

175
00:12:34,536 --> 00:12:37,826
observability applications. In fact, there's a

176
00:12:37,848 --> 00:12:41,362
pretty good chance that many of you who are listening to this talk already

177
00:12:41,416 --> 00:12:45,250
use Grafana for this purpose. Why is Grafana good?

178
00:12:45,320 --> 00:12:49,170
Well, first of all, it's built around display of time series data.

179
00:12:49,320 --> 00:12:52,722
It's very simple to install. It has piles of data sources.

180
00:12:52,786 --> 00:12:55,718
So we will be using a data source that can read Clickhouse data, but it

181
00:12:55,724 --> 00:12:58,518
can also read prometheus it can read mySql,

182
00:12:58,694 --> 00:13:01,450
you name it. If there's a database,

183
00:13:03,870 --> 00:13:07,798
Grafana can connect to it and use it. Moreover, for displaying

184
00:13:07,814 --> 00:13:11,054
the data, it has a pile of plugins. This example on the right just shows

185
00:13:11,092 --> 00:13:15,070
a few of them, but time series

186
00:13:17,010 --> 00:13:21,230
sort of heat maps, tabular displays,

187
00:13:21,730 --> 00:13:24,638
and they're very easy to set up and apply to the data.

188
00:13:24,804 --> 00:13:28,594
One of the things that makes it particularly strong for

189
00:13:28,632 --> 00:13:31,938
monitoring, it has very good zoom in and zoom out.

190
00:13:32,024 --> 00:13:35,730
So the ability to look at different timescales, to look at different series

191
00:13:36,710 --> 00:13:40,326
at a particular time sales scale, these are all things that you like to

192
00:13:40,348 --> 00:13:42,040
have when you're trying to drill in,

193
00:13:43,210 --> 00:13:46,822
understand the data, sift through what you're seeing, and then zero

194
00:13:46,876 --> 00:13:51,110
in on a problem. And that's

195
00:13:51,450 --> 00:13:55,226
taken together, this makes it great for monitoring dashboards. And then the

196
00:13:55,248 --> 00:13:58,086
final thing which makes it a good match for Clickhouse is it's open source,

197
00:13:58,118 --> 00:14:02,254
in this case AGPL 30. So how

198
00:14:02,292 --> 00:14:05,406
do we go then to build an

199
00:14:05,428 --> 00:14:08,286
actual monitoring application?

200
00:14:08,468 --> 00:14:11,706
What we're going to do is start with those VM stat

201
00:14:11,738 --> 00:14:15,214
commands that I showed you a few slides ago, and we're actually going to

202
00:14:15,252 --> 00:14:18,446
turn that into data in a table in clickhouse

203
00:14:18,558 --> 00:14:22,066
and then display it in Grafana. So let's dig in and show how

204
00:14:22,088 --> 00:14:25,974
to do that. It's really not that hard. So the first thing is

205
00:14:26,012 --> 00:14:28,600
we need to generate the VM stat data.

206
00:14:29,130 --> 00:14:32,514
So here is a simple python

207
00:14:32,562 --> 00:14:35,894
script. It's about 14 lines that

208
00:14:35,932 --> 00:14:39,266
is actually going to run VMStat at 1

209
00:14:39,308 --> 00:14:42,826
second intervals and then basically split the

210
00:14:42,848 --> 00:14:45,894
results up and stick them in a JSON document.

211
00:14:46,022 --> 00:14:49,290
If you look around, there are plenty of tools that will do this automatically.

212
00:14:49,870 --> 00:14:52,902
I just wrote it myself because it's really simple to do.

213
00:14:53,056 --> 00:14:56,046
Data collectors are just not that hard to write.

214
00:14:56,228 --> 00:14:59,566
So you can read the code, and if you carefully look at

215
00:14:59,588 --> 00:15:03,850
it, you can prove to yourself that it's eventually doing adjacent, that it's

216
00:15:04,010 --> 00:15:07,634
basically constructing a dictionary and then dumping it out

217
00:15:07,672 --> 00:15:11,058
as JSON key value properties. To understand the data, it's a

218
00:15:11,064 --> 00:15:14,594
little bit easier just to go look at it. So here's the output that you

219
00:15:14,632 --> 00:15:19,302
get. So the key value pairs, you get a timestamp. That's really

220
00:15:19,356 --> 00:15:22,562
important because that's your time ordering. And then you get a bunch of properties,

221
00:15:22,626 --> 00:15:26,086
including the host and things like

222
00:15:26,108 --> 00:15:29,580
that, and then actual measurements, like for example,

223
00:15:30,750 --> 00:15:34,666
the idle time here, which is 98%. So this is

224
00:15:34,688 --> 00:15:37,958
the data that we're going to be loading into clickhouse.

225
00:15:38,134 --> 00:15:41,790
So the next thing we need to do is we need to design

226
00:15:41,860 --> 00:15:45,258
a clickhouse table to hold the data. So clickhouse,

227
00:15:45,274 --> 00:15:49,034
unlike a database like Prometheus, for example, does require

228
00:15:49,162 --> 00:15:52,702
data to be in a tabular format. But Clickhouse is

229
00:15:52,756 --> 00:15:56,462
very, very tolerant of what it considers to be a table.

230
00:15:56,606 --> 00:16:00,542
In this particular case, we're taking a pretty conventional approach.

231
00:16:00,606 --> 00:16:03,742
So we're going to take things like the timestamp,

232
00:16:03,806 --> 00:16:07,350
the day, the host, and we're going to consider those to be

233
00:16:07,500 --> 00:16:10,866
dimensions. So these are the properties of the measurement,

234
00:16:11,058 --> 00:16:14,982
and then what we have is the measurements themselves. So these

235
00:16:15,036 --> 00:16:18,694
are just all the data that we get out of the vm

236
00:16:18,742 --> 00:16:22,586
stat command. So the amount of free memory, amount of

237
00:16:22,768 --> 00:16:27,174
buffer cache, the different amounts of percentage

238
00:16:27,222 --> 00:16:30,714
of time, sort of ways that the CPU is using its

239
00:16:30,752 --> 00:16:34,746
time, so on and so forth. One thing to notice, if you've used SQL

240
00:16:34,778 --> 00:16:38,014
databases before, is down at the bottom. We have this

241
00:16:38,052 --> 00:16:41,338
engine equals merge tree. So for MySQL

242
00:16:41,514 --> 00:16:45,146
users, this will be familiar. This is a particular way of organizing

243
00:16:45,178 --> 00:16:48,914
a table merge tree. In Clickhouse is the workhorse table for

244
00:16:48,952 --> 00:16:52,754
large data or for big data, and it has

245
00:16:52,872 --> 00:16:56,878
partitioning built into it. So you have to give clickhouse

246
00:16:56,894 --> 00:16:59,314
a bit of a clue how you want the data broken up. In this case,

247
00:16:59,352 --> 00:17:02,662
we're doing it by day. This would be appropriate if we're building a system,

248
00:17:02,716 --> 00:17:06,210
for example, it holds a year of data, and then we also give ordering.

249
00:17:06,290 --> 00:17:09,762
This is something in analytical databases that's critical.

250
00:17:09,826 --> 00:17:12,954
You need to give a sort order to the data. And if you do this

251
00:17:12,992 --> 00:17:16,838
correctly, and this here, we're sorting by the host followed

252
00:17:16,854 --> 00:17:20,854
by the timestamp. This will order the data in such a way

253
00:17:20,992 --> 00:17:24,506
that, among other things, the values between successive rows

254
00:17:24,538 --> 00:17:28,000
will be very similar and will compress incredibly well.

255
00:17:28,370 --> 00:17:31,534
So next step, we've got the table, we've got the

256
00:17:31,572 --> 00:17:35,438
data. Let's get the data loaded into that table in clickhouse.

257
00:17:35,614 --> 00:17:39,298
So the actual SQl insert command to do this,

258
00:17:39,384 --> 00:17:42,658
if we've got the data, is really simple. So this is

259
00:17:42,664 --> 00:17:46,814
a JSON, each row format, every measurement results in a JSON document.

260
00:17:46,942 --> 00:17:50,338
So the top command is how we do that in SQL.

261
00:17:50,434 --> 00:17:53,240
The actual command to get this done is a little bit different.

262
00:17:54,090 --> 00:17:58,138
For example, we can go ahead and use curl to post this data.

263
00:17:58,304 --> 00:18:02,106
So this is an

264
00:18:02,128 --> 00:18:05,866
actual command that loads some of this, a file containing this data,

265
00:18:06,048 --> 00:18:09,350
and this is it. So literally two lines.

266
00:18:09,430 --> 00:18:13,526
Well, got to construct the insert command with Earl

267
00:18:13,558 --> 00:18:17,214
encoding, but that's it. So very simple to get it loaded. You can of course

268
00:18:17,252 --> 00:18:20,126
write a python script. That's in fact what I did because it's a little bit

269
00:18:20,148 --> 00:18:23,490
easier to control it than running inside shell.

270
00:18:23,990 --> 00:18:27,950
And then the final thing is you're going to want to construct

271
00:18:28,030 --> 00:18:31,506
a grafana dashboard. So in this particular

272
00:18:31,608 --> 00:18:34,906
case I've constructed, and we'll see this in action

273
00:18:34,958 --> 00:18:38,534
in just a few minutes. I've gone

274
00:18:38,572 --> 00:18:42,406
ahead and constructed a simple display that shows me

275
00:18:42,588 --> 00:18:46,006
my cpu, that's the top display, and then a

276
00:18:46,028 --> 00:18:51,754
more detailed cpu usage graph that

277
00:18:51,792 --> 00:18:55,286
actually breaks down the components

278
00:18:55,318 --> 00:18:59,226
of the CPU usage and then memory usage. The bottom two are done

279
00:18:59,248 --> 00:19:01,760
by host, so there's a little selector on top.

280
00:19:03,490 --> 00:19:07,326
When you're using clickhouse. There are a couple of plugins that

281
00:19:07,348 --> 00:19:10,878
you can use for Grafana. I prefer to use the

282
00:19:10,884 --> 00:19:14,026
one that we maintain, which is the alternative plugin for clickhouse.

283
00:19:14,058 --> 00:19:17,586
It's been around for years. It's had about 12 million downloads at

284
00:19:17,608 --> 00:19:21,266
this point. Incredibly popular, used across thousands and thousands of

285
00:19:21,288 --> 00:19:24,966
dashboards. So that's the one that's used to construct this

286
00:19:24,988 --> 00:19:27,400
display. And then finally,

287
00:19:31,930 --> 00:19:35,286
once you have this all set up, not only do you have the

288
00:19:35,308 --> 00:19:38,854
display, so you can go and look at this

289
00:19:38,892 --> 00:19:42,422
information directly through the display,

290
00:19:42,486 --> 00:19:45,162
play around with it as we'll do in a couple of minutes.

291
00:19:45,296 --> 00:19:48,890
But you have the full power of SQL and you can ask any question

292
00:19:48,960 --> 00:19:52,266
you want. You can go ahead and do this interactively

293
00:19:52,298 --> 00:19:55,966
off the command line, turn it into further displays. For example, this is

294
00:19:55,988 --> 00:19:59,966
a query that just shows all

295
00:19:59,988 --> 00:20:03,962
the hosts that had greater than 25% load

296
00:20:04,106 --> 00:20:07,426
for at least a minute in the last 24 hours. And it also sums the

297
00:20:07,448 --> 00:20:10,526
number of minutes that had that. So this is a way of seeing which hosts

298
00:20:10,558 --> 00:20:14,260
are running hot. So that's the system.

299
00:20:14,630 --> 00:20:18,950
It's really not very complicated. This is in total

300
00:20:19,020 --> 00:20:22,482
about 100 lines of code that was sufficient

301
00:20:22,546 --> 00:20:26,038
to get this. So let's go out and bounce out of

302
00:20:26,044 --> 00:20:29,226
the slides and go have a look at the system actually at

303
00:20:29,248 --> 00:20:32,810
the ground level. So here we go.

304
00:20:32,960 --> 00:20:35,820
This is the system that we saw a few minutes ago.

305
00:20:36,190 --> 00:20:39,686
And you can see the way that this is set up.

306
00:20:39,728 --> 00:20:43,582
It's monitoring a couple of hosts that I run in my home

307
00:20:43,636 --> 00:20:51,326
office. They're called logos two and had

308
00:20:51,348 --> 00:20:54,782
a temporary glitch in the audio there. So they run two hosts,

309
00:20:54,846 --> 00:20:58,846
logos two and logos three. And I can select particular hosts.

310
00:20:58,878 --> 00:21:02,306
You can see how this selector allows this to change them.

311
00:21:02,488 --> 00:21:06,086
I can look at all of them, in which case I don't get

312
00:21:06,108 --> 00:21:08,790
cpu specific, cpu and memory usage.

313
00:21:10,490 --> 00:21:13,926
Let's go pick a specific one. We'll go pick logos two.

314
00:21:14,108 --> 00:21:17,702
And I can also change the timescale. This is super easy

315
00:21:17,756 --> 00:21:21,234
to do. So here we go. We can switch this to the last 30 minutes.

316
00:21:21,292 --> 00:21:24,922
We can go see what's going on here. Let's go to last hour and

317
00:21:24,976 --> 00:21:28,506
see if there's anything interesting. You can see there's been some activity on the

318
00:21:28,528 --> 00:21:31,520
system, on this logos too.

319
00:21:32,610 --> 00:21:36,560
And in fact you can distinguish the different

320
00:21:37,410 --> 00:21:40,798
traces. So right here, without really doing anything special, you have a

321
00:21:40,804 --> 00:21:44,298
lot of insight into the load levels on these systems and you

322
00:21:44,324 --> 00:21:48,114
can basically drill in to get

323
00:21:48,152 --> 00:21:51,762
much closer views. This is something I love about Grafana that I can actually

324
00:21:51,816 --> 00:21:55,582
come in and I can just select a very small section

325
00:21:55,646 --> 00:21:59,574
and then the display automatically zeroes in

326
00:21:59,612 --> 00:22:02,694
on the part that I want to look at. Let's go ahead and get this

327
00:22:02,732 --> 00:22:06,326
back to doing the last five minutes and

328
00:22:06,348 --> 00:22:08,778
let's put some load on the system. Let's test this thing out.

329
00:22:08,864 --> 00:22:13,014
So for that we have a couple of handy

330
00:22:13,062 --> 00:22:17,210
commands. A great command to bash on the

331
00:22:17,360 --> 00:22:21,306
cpu is sysbench. So this

332
00:22:21,328 --> 00:22:25,386
is something you can just say apt install. Pseudo. Apt install sysbench.

333
00:22:25,578 --> 00:22:28,906
What I'm going to run is a cpu test. This is just going to beat

334
00:22:28,938 --> 00:22:32,186
up on the cpus and we're going to let this run for a minute or

335
00:22:32,228 --> 00:22:36,130
two and we will basically

336
00:22:36,200 --> 00:22:39,586
be able to see

337
00:22:39,608 --> 00:22:42,114
this beating up on the system tools. Actually,

338
00:22:42,152 --> 00:22:45,506
I made a slight mistake there. Let me run it

339
00:22:45,528 --> 00:22:49,878
on the logos two host because that is

340
00:22:50,044 --> 00:22:53,686
actually a more capable system. So we're going to go ahead and run the

341
00:22:53,708 --> 00:22:57,254
cpu test while that's running and the data is collecting. Let me just

342
00:22:57,292 --> 00:23:00,826
show you this, that all the code that we're using here is actually available

343
00:23:00,928 --> 00:23:04,266
in a project called Clickhouse. SQL examples. Let's go to

344
00:23:04,288 --> 00:23:07,894
the open source monitoring directory. So for example, the dashboard

345
00:23:07,942 --> 00:23:13,354
that I'm showing you is there the little python routines

346
00:23:13,402 --> 00:23:16,800
that we have here. These are

347
00:23:17,730 --> 00:23:22,078
loading the data into clickhouse. And then the

348
00:23:22,244 --> 00:23:25,754
python script which I showed you that actually generated

349
00:23:25,802 --> 00:23:28,226
the data. It's all there. So if you want to go ahead and do this,

350
00:23:28,248 --> 00:23:31,586
and as I say, it's about 100 lines of code total to

351
00:23:31,608 --> 00:23:35,062
get this whole thing to run. And then actually running it is very simple.

352
00:23:35,116 --> 00:23:38,454
It's as simple, the collectors are as simple as

353
00:23:38,492 --> 00:23:42,486
the following. So just run the

354
00:23:42,508 --> 00:23:46,658
collector pipe into the consumer and up it goes to clickhouse.

355
00:23:46,834 --> 00:23:50,330
Great. So that test has been running in the background on logos two.

356
00:23:50,480 --> 00:23:54,106
So let's see what we have. And actually you

357
00:23:54,128 --> 00:23:57,660
can see that. We can actually see

358
00:23:58,030 --> 00:24:01,246
the test going on and we can

359
00:24:01,268 --> 00:24:04,586
see the effect on the cpu. We ran

360
00:24:04,618 --> 00:24:07,806
it first on logos three. So right there.

361
00:24:07,988 --> 00:24:11,082
Ran it here on logos

362
00:24:11,146 --> 00:24:14,042
two. So we can see the cpu.

363
00:24:14,186 --> 00:24:17,954
Now what's a little bit more interesting is to bash on this a bit and

364
00:24:17,992 --> 00:24:21,422
actually do some work on, show the effect on the memory.

365
00:24:21,486 --> 00:24:25,286
That's a little bit more fun. Let's run another program.

366
00:24:25,388 --> 00:24:29,142
Let's kill the cpu test. And we're going to run a program called

367
00:24:29,196 --> 00:24:32,520
stress. So here it is.

368
00:24:33,290 --> 00:24:36,600
And this is a program that can

369
00:24:37,050 --> 00:24:39,654
beat up on your system, but it can also use a lot of memory.

370
00:24:39,702 --> 00:24:44,634
So this is basically spawning four threads. They're each going to eat about four

371
00:24:44,672 --> 00:24:48,342
g's. And you have to love any performance

372
00:24:48,406 --> 00:24:52,174
test program that calls its workers hogs. So off

373
00:24:52,212 --> 00:24:57,086
they go. And we'll actually see these coming up in

374
00:24:57,108 --> 00:25:01,070
the display. Let's go ahead and actually,

375
00:25:01,140 --> 00:25:04,222
let's change the time. Okay,

376
00:25:04,276 --> 00:25:08,862
so we can see these actually starting to use resources.

377
00:25:08,926 --> 00:25:12,514
There's the memory coming up. This is not actually putting enough

378
00:25:12,552 --> 00:25:15,846
load on the system. Let's beat it up a little bit more. Let's go ahead

379
00:25:15,868 --> 00:25:19,426
and add eight threads.

380
00:25:19,618 --> 00:25:23,046
So go ahead and put that in and give

381
00:25:23,068 --> 00:25:26,246
it a minute or two. And what we'll see

382
00:25:26,268 --> 00:25:29,158
now is this will put very heavy load on the system.

383
00:25:29,244 --> 00:25:33,034
So we'll start to see in this memory usage, we will start to see

384
00:25:33,072 --> 00:25:37,162
this climbing very rapidly. Colors here are probably not the best,

385
00:25:37,296 --> 00:25:41,206
but here we can see that it's actually putting heavy cpu

386
00:25:41,238 --> 00:25:45,226
load on this system. We can also see that up here. It's basically pegged

387
00:25:45,258 --> 00:25:49,082
at 100%. So this machine is just getting hammered.

388
00:25:49,226 --> 00:25:53,070
In fact, what's happening right here, this is kind of interesting.

389
00:25:53,220 --> 00:25:56,746
You basically, let me get that back

390
00:25:56,788 --> 00:26:00,574
to five minutes. It's zooming in too quickly. We've actually got gaps in collection.

391
00:26:00,702 --> 00:26:04,066
And what that indicates is the machine is so loaded that the collector is

392
00:26:04,088 --> 00:26:07,826
not even generating data. So that's the demo.

393
00:26:08,008 --> 00:26:11,686
This is something that I put together. I'm having all kinds of fun with

394
00:26:11,708 --> 00:26:14,726
this at the cost of about 100 lines of code. Of course,

395
00:26:14,748 --> 00:26:18,646
if you want to productize it, you're going to end up also storing

396
00:26:18,678 --> 00:26:23,222
the system configuration and managing Grafana, managing clickhouse.

397
00:26:23,366 --> 00:26:27,260
But the point is, you can build this system yourself

398
00:26:28,110 --> 00:26:31,882
and basically monitor anything you want and collect practically

399
00:26:31,946 --> 00:26:34,734
any kind of data you want. Okay,

400
00:26:34,852 --> 00:26:38,222
so let's go back to the slides and dig

401
00:26:38,276 --> 00:26:41,946
in further to some final notes.

402
00:26:42,058 --> 00:26:45,314
So if you're going to build a monitoring system, you can use

403
00:26:45,352 --> 00:26:49,220
Python, as I did, but you don't have to.

404
00:26:50,470 --> 00:26:54,340
One of the things that's great about Clickhouse is it's a very popular project,

405
00:26:57,930 --> 00:27:01,138
certainly among the most popular analytic databases

406
00:27:01,314 --> 00:27:05,170
across all of GitHub. It has a huge number of libraries

407
00:27:05,250 --> 00:27:08,934
and software packages that work with it, everything from Kafka to

408
00:27:08,972 --> 00:27:12,866
airflow. And then for display, as we saw Grafana,

409
00:27:12,898 --> 00:27:16,294
superset, cubejs, bunch of different client libraries.

410
00:27:16,422 --> 00:27:20,058
We do a lot of work with Golang in our work, but if you

411
00:27:20,064 --> 00:27:23,260
like Java, if you like Python, the drivers are all there.

412
00:27:23,650 --> 00:27:27,594
And then if you want to run it on kubernetes, there's the altinity operator

413
00:27:27,642 --> 00:27:31,614
for Clickhouse, which I mentioned at the start of this talk. This allows you

414
00:27:31,652 --> 00:27:35,842
to run Clickhouse very efficiently on

415
00:27:35,896 --> 00:27:39,314
kubernetes, which is turning out to be a really great

416
00:27:39,352 --> 00:27:43,106
place to run data. But of course you can run it on anything you

417
00:27:43,128 --> 00:27:47,430
want. Clickhouse runs great anywhere. You can run it on vms, of course,

418
00:27:47,580 --> 00:27:50,934
use ansible to manage it, so on and so

419
00:27:50,972 --> 00:27:53,926
forth. So where can you find out more?

420
00:27:54,028 --> 00:27:57,658
Well, there's the official docs for both the clickhouse project as well

421
00:27:57,664 --> 00:27:59,980
as Grafana. So these are shown here.

422
00:28:01,310 --> 00:28:05,462
We do in the course of our work with Clickhouse,

423
00:28:05,526 --> 00:28:09,414
as well as other products like Grafana, we write blog articles,

424
00:28:09,542 --> 00:28:13,702
we do a huge number of talks on on

425
00:28:13,776 --> 00:28:17,706
that we post on YouTube concerning topics related

426
00:28:17,738 --> 00:28:21,114
to running Clickhouse, as well as integration with other tools.

427
00:28:21,242 --> 00:28:24,786
We have a knowledge base that you can use to learn more about how to

428
00:28:24,808 --> 00:28:28,386
solve specific problems, particularly if you're operating at scale. And then it's just a

429
00:28:28,408 --> 00:28:31,842
pile of other opensource associated with Clickhouse. There's a

430
00:28:31,896 --> 00:28:35,278
very large community around this.

431
00:28:35,464 --> 00:28:38,360
We get thousands of contributions per year,

432
00:28:39,210 --> 00:28:43,478
ranging from sort of simple elements on issues

433
00:28:43,564 --> 00:28:47,482
all the way up to things like prs. Last year about 392

434
00:28:47,536 --> 00:28:50,170
people, unique people on GitHub,

435
00:28:50,510 --> 00:28:53,850
submitted prs that were actually merged into Clickhouse.

436
00:28:54,670 --> 00:28:58,780
So that is my talk. Thank you very much

437
00:28:59,710 --> 00:29:03,662
and go out there and have fun. I'd like to thank the comp 42

438
00:29:03,716 --> 00:29:07,006
folks once again for setting this conference up.

439
00:29:07,028 --> 00:29:10,446
It's great to present here and if you want to contact me, I'll be

440
00:29:10,468 --> 00:29:14,058
hanging out on discord as part of the conference.

441
00:29:14,154 --> 00:29:17,318
But you can also get to me at alternative.

442
00:29:17,354 --> 00:29:21,010
You can just go to the website, do contact us. We have a slack channel

443
00:29:21,080 --> 00:29:24,594
that you can join and you can just join that channel

444
00:29:24,632 --> 00:29:27,842
and dm me, or you can find me on LinkedIn. And once again,

445
00:29:27,896 --> 00:29:31,106
altinity. We do alternity cloud, which is a cloud for

446
00:29:31,128 --> 00:29:35,790
Clickhouse. We do builds of clickhouse, stable build and the alternative Kubernetes

447
00:29:35,870 --> 00:29:39,642
operator. Those are just a few of the many things that we do to help

448
00:29:39,696 --> 00:29:44,166
people operate Clickhouse at scale and build applications

449
00:29:44,198 --> 00:29:47,434
like the one I just showed you. So thanks again. Have a great

450
00:29:47,472 --> 00:29:47,560
day.

