1
00:00:25,410 --> 00:00:29,446
You. Hello, everybody. How are

2
00:00:29,468 --> 00:00:33,334
you doing? I'm really hoping that you're having a good day. It looks like an

3
00:00:33,372 --> 00:00:36,854
exciting day today at comforty two chaos engineering with

4
00:00:36,892 --> 00:00:40,486
plenty of good content. Looking forward to that. I'm going to talk to

5
00:00:40,508 --> 00:00:44,134
you about chaos engineering in 2021. I know

6
00:00:44,172 --> 00:00:47,570
that this is a lot like, we all have very high expectations

7
00:00:47,650 --> 00:00:51,646
of what 2021 will bring, given how the

8
00:00:51,668 --> 00:00:55,038
last twelve months went and how challenging they were. But I'm going to

9
00:00:55,044 --> 00:00:58,414
try to focus on what I hope chaos engineering is

10
00:00:58,452 --> 00:01:02,110
going to achieve soon, where I think it is in 2021,

11
00:01:02,180 --> 00:01:06,206
the journey that we've made so far together. And spoiler

12
00:01:06,238 --> 00:01:10,066
alert, I'm going to argue that it should be boring and

13
00:01:10,088 --> 00:01:13,794
that boring is a good thing. So one of the things that

14
00:01:13,912 --> 00:01:17,494
the last twelve months tested was our

15
00:01:17,532 --> 00:01:21,538
resilience. And we started talking about resilience and reliability

16
00:01:21,634 --> 00:01:25,746
much more because we realized that things that we were taking for granted couldn't

17
00:01:25,778 --> 00:01:29,274
be taken for granted anymore. Things like going outside

18
00:01:29,392 --> 00:01:33,302
and going for a walk or shopping and getting the basics

19
00:01:33,366 --> 00:01:36,870
or even the infrastructure and electricity.

20
00:01:36,950 --> 00:01:40,254
Turned out that all of that is actually put into

21
00:01:40,292 --> 00:01:43,342
question when there are unprecedented events happening.

22
00:01:43,476 --> 00:01:47,418
All our systems are being tested. We talk about reliability

23
00:01:47,514 --> 00:01:52,110
a lot and resilience, things like the resilience of our

24
00:01:52,260 --> 00:01:56,014
healthcare systems, the reliability of our infrastructure

25
00:01:56,062 --> 00:01:59,330
and stuff like that. It's probably a good moment to kind of

26
00:01:59,480 --> 00:02:02,546
look at what we really mean about know.

27
00:02:02,568 --> 00:02:05,854
We talk about reliability in terms of software engineering

28
00:02:05,902 --> 00:02:08,930
kind of all the time. But if you take a look at the definition,

29
00:02:09,010 --> 00:02:13,506
this is from Oxford Dictionary, is the quality of being trustworthy

30
00:02:13,618 --> 00:02:17,158
or of performing consistently well. And one of

31
00:02:17,164 --> 00:02:21,086
the examples they give is the fundamental aspect to building relationships

32
00:02:21,138 --> 00:02:24,682
is providing reliability. So I was thinking of a good example of

33
00:02:24,736 --> 00:02:28,026
what a really reliable system would be. And I came up

34
00:02:28,048 --> 00:02:31,982
with this. The pyramids, if you think about that, they have been

35
00:02:32,116 --> 00:02:35,546
performing remarkably reliably for thousands

36
00:02:35,578 --> 00:02:39,070
of years. They're still working as expected. They were built,

37
00:02:39,140 --> 00:02:42,638
there's still a tomb, there's still a bunch of rocks put together,

38
00:02:42,804 --> 00:02:46,722
and chances are then a few millennia from now, they're still

39
00:02:46,776 --> 00:02:50,366
going to be standing there virtually unchanged. So it's

40
00:02:50,398 --> 00:02:54,242
a really reliable system. When you think about it. It's not maybe has

41
00:02:54,296 --> 00:02:57,814
exciting as kubernetes, but it's a really reliable system,

42
00:02:57,932 --> 00:03:00,962
or perhaps it's more exciting. And then we have the resilience,

43
00:03:01,026 --> 00:03:04,946
right? The resilience, again from the Oxford Dictionary, is defined

44
00:03:04,978 --> 00:03:08,658
as the capacity to recover quickly from difficulties.

45
00:03:08,754 --> 00:03:12,614
And another definition they also give there is the ability of a substance

46
00:03:12,662 --> 00:03:15,574
or object to spring back into shape.

47
00:03:15,622 --> 00:03:19,482
And I think I might like the second one even more than the first

48
00:03:19,536 --> 00:03:22,926
one, because the springing back into shape, it kind

49
00:03:22,948 --> 00:03:26,330
of gives you this image of pressing or pushing

50
00:03:26,410 --> 00:03:30,430
or compressing, and then the thing comes back

51
00:03:30,580 --> 00:03:33,838
springing back into shape, elasticity of it, right?

52
00:03:33,924 --> 00:03:37,794
So 2020 has been a real test of resiliency on

53
00:03:37,832 --> 00:03:41,346
all kinds of ways. Our lives have been modified from

54
00:03:41,368 --> 00:03:45,150
one day to another. Things that we were taking for granted, like going on

55
00:03:45,160 --> 00:03:48,742
holidays, going outside, have been tested, and the

56
00:03:48,796 --> 00:03:52,370
resilience of our institutions, of our infrastructure,

57
00:03:52,450 --> 00:03:56,214
of our companies, of our teams, we spent a year

58
00:03:56,332 --> 00:04:00,202
basically video conferencing with people and trying to maintain social

59
00:04:00,256 --> 00:04:03,898
interactions with them. All of that has testing whether we can still

60
00:04:03,984 --> 00:04:07,174
be pushed and compressed and then go and spring

61
00:04:07,222 --> 00:04:10,566
back into the previous shape. It's interesting to talk

62
00:04:10,608 --> 00:04:14,490
about this in the context of chaos engineering, because chaos engineering

63
00:04:14,570 --> 00:04:18,510
is all about testing the resiliency and the reliability.

64
00:04:19,010 --> 00:04:22,846
I'm probably talking to two very distinct groups of people here.

65
00:04:22,948 --> 00:04:26,622
Some of you came to this conference to learn about chaos engineering

66
00:04:26,686 --> 00:04:30,638
and how to adopt that and how to get started with that, kind of ramping

67
00:04:30,654 --> 00:04:34,578
up the adoption curve. And some of you might be practicing that

68
00:04:34,664 --> 00:04:38,654
and might be preaching that and are looking for an update

69
00:04:38,702 --> 00:04:41,830
on what's new. So I'm going to try to address both points of view.

70
00:04:41,900 --> 00:04:44,982
I think it's fair to say that, at least in my point of view,

71
00:04:45,036 --> 00:04:48,354
chaos Engineering is at an interesting crossroads

72
00:04:48,402 --> 00:04:52,154
right now, at an interesting point in the timeline, because there are

73
00:04:52,192 --> 00:04:55,642
different trends and phenomena in software engineering that kind of

74
00:04:55,696 --> 00:04:59,610
put chaos engineering in a nice spot for the adoption. And also

75
00:04:59,760 --> 00:05:03,174
we are slowly moving onto that adoption

76
00:05:03,222 --> 00:05:06,942
curve, and we can see that by a number of companies that start doing that,

77
00:05:06,996 --> 00:05:10,398
a number of companies that offer services and stuff like that.

78
00:05:10,484 --> 00:05:13,630
But why is it an interesting crossroad? I think there are different

79
00:05:13,700 --> 00:05:16,862
trends that kind of made it possible for chaos

80
00:05:16,926 --> 00:05:20,194
engineering, kind of made it really necessary for

81
00:05:20,232 --> 00:05:23,442
chaos engineering to emerge. I wanted to mention a few things.

82
00:05:23,496 --> 00:05:27,538
I wanted to mention scale, cloud, native and complexity.

83
00:05:27,634 --> 00:05:31,078
So let's start with the scale. Scale is a little bit,

84
00:05:31,164 --> 00:05:35,154
it's probably the easiest one to explain. Here in our global connected

85
00:05:35,202 --> 00:05:38,546
world, we start having bigger systems. We have bigger

86
00:05:38,578 --> 00:05:42,246
companies that manage not hundreds of thousands, but hundreds

87
00:05:42,278 --> 00:05:46,170
of millions of users. And the systems that we build are getting

88
00:05:46,240 --> 00:05:50,118
bigger, they're getting more complex, they have more features, they have bigger scale.

89
00:05:50,214 --> 00:05:54,026
And from the geographical point of view, we have users

90
00:05:54,058 --> 00:05:58,042
from all over the world. It's all connected, and the scale just becomes bigger,

91
00:05:58,106 --> 00:06:01,722
which increases the complexity. Another kind of factor,

92
00:06:01,786 --> 00:06:05,034
a trend that makes chaos engineering

93
00:06:05,082 --> 00:06:09,054
so important right now is the cloud. We've been on this bandwagon

94
00:06:09,102 --> 00:06:12,674
for a number of years now. When you're using someone else's computer

95
00:06:12,792 --> 00:06:16,558
and you're calling it a cloud, and if you're starting a company right

96
00:06:16,584 --> 00:06:20,242
now with any kind of stack, chances are that you're

97
00:06:20,306 --> 00:06:23,570
probably using some kind of cloud. You're probably going to AWS

98
00:06:23,650 --> 00:06:27,222
or Azure or Google and you're using someone

99
00:06:27,276 --> 00:06:30,726
else's computer. And that layer means that it solves

100
00:06:30,758 --> 00:06:33,862
some of the problems, but it also adds extra layer of complexity,

101
00:06:33,926 --> 00:06:37,866
because now you not only have to understand your application and the

102
00:06:37,888 --> 00:06:41,226
underlying operating system, but you also need to understand how

103
00:06:41,248 --> 00:06:45,454
the cloud is built underneath because most of the time you can probably not think

104
00:06:45,492 --> 00:06:48,878
about that and get away with that. But if things go wrong and

105
00:06:48,964 --> 00:06:52,430
get complicated and you need to debug something, you need to understand that

106
00:06:52,500 --> 00:06:55,922
you also have tools that you need to use to talk to the cloud

107
00:06:55,976 --> 00:06:59,410
provider. You have the APIs, you have caveats, you have

108
00:06:59,480 --> 00:07:03,054
quirks that you need to work with and the differences

109
00:07:03,102 --> 00:07:06,686
in operating between the different clouds. But it's also a driver

110
00:07:06,718 --> 00:07:10,454
that kind of goes back to the scale, because now you can literally go

111
00:07:10,492 --> 00:07:13,926
to a cloud provider and say, oh, tomorrow I need

112
00:07:13,948 --> 00:07:17,734
ten times more resources than I have today, or in an hour I need ten

113
00:07:17,772 --> 00:07:21,686
times more resources, and chances are that you'll be able to get that. So that's

114
00:07:21,718 --> 00:07:26,038
another trend. Then with cloud came the cloud native ecosystem

115
00:07:26,134 --> 00:07:29,938
that has flourished and absolutely grew

116
00:07:30,054 --> 00:07:33,150
enormously in the last few years. And the prime example

117
00:07:33,220 --> 00:07:36,238
of that being kubernetes in this short time,

118
00:07:36,324 --> 00:07:39,882
it basically became the de facto API

119
00:07:39,946 --> 00:07:43,534
for scheduling, orchestrating containers across a

120
00:07:43,572 --> 00:07:47,582
fleet of vms. That was also made possible and made popular

121
00:07:47,646 --> 00:07:51,186
because of the cloud. Right. And with kubernetes, you get another layer. So now

122
00:07:51,208 --> 00:07:54,898
you have the cloud, then you have the Kubernetes layer that works great

123
00:07:54,984 --> 00:07:58,526
and has some amazing features that you can leverage and

124
00:07:58,568 --> 00:08:02,582
that you get for free. But you also need to understand these

125
00:08:02,636 --> 00:08:05,894
are things that can go wrong. And when they do go wrong,

126
00:08:05,932 --> 00:08:09,446
you need to understand how to debug that. You need to understand where to

127
00:08:09,468 --> 00:08:13,126
look for trouble. There's another part of the learning curve that becomes

128
00:08:13,158 --> 00:08:17,302
relevant to you, and that kind of all boils down to the extra layers

129
00:08:17,366 --> 00:08:21,642
of complexity that if you are deploying software today,

130
00:08:21,776 --> 00:08:25,294
it's no longer just about the software. I mean, it probably never has

131
00:08:25,332 --> 00:08:29,070
been, but it's not about just the bugs or memory leak. Now obviously

132
00:08:29,140 --> 00:08:32,954
I know that you guys don't do bugs or memory leaks,

133
00:08:33,082 --> 00:08:36,322
I do. But even if you write the come that works

134
00:08:36,376 --> 00:08:39,998
perfectly, you can still mess things up with configuration

135
00:08:40,094 --> 00:08:43,426
or versioning. If you have a dependency that you point out

136
00:08:43,448 --> 00:08:46,786
to the wrong version or you point out to the wrong dependency,

137
00:08:46,898 --> 00:08:50,758
you have a problem. So even if all the bricks involved are

138
00:08:50,844 --> 00:08:54,294
coded correctly, you can still mess things up. Then there

139
00:08:54,332 --> 00:08:59,286
are things, the embarrassing things like certificates, expiring licenses

140
00:08:59,478 --> 00:09:03,322
probably. Once again you never had this problem, but I

141
00:09:03,376 --> 00:09:07,066
suspect that the certificates expiring on people

142
00:09:07,168 --> 00:09:09,990
is more popular than they want to admit.

143
00:09:10,070 --> 00:09:14,062
Then because of the fact that we have this increasing scale and everything

144
00:09:14,116 --> 00:09:17,422
is in the cloud and more and more

145
00:09:17,476 --> 00:09:20,526
distributed, we have this entire family of

146
00:09:20,548 --> 00:09:23,342
problems that come from the connectivity. Connectivity.

147
00:09:23,406 --> 00:09:27,262
Networking is pretty hard. Latencies, how do latencies

148
00:09:27,326 --> 00:09:31,278
affect your systems? Do you survive them? What about timeouts?

149
00:09:31,374 --> 00:09:34,706
Are the timeouts reasonable? Are the timeouts aligned between

150
00:09:34,728 --> 00:09:38,770
the different components so that the whole thing makes sense? What about retries?

151
00:09:38,850 --> 00:09:42,706
If you get a timeout, how do you retry? Do you get that thundering

152
00:09:42,738 --> 00:09:46,642
Hertz problem? Do you exponential backup? Do you have circuit breakers?

153
00:09:46,706 --> 00:09:50,358
For example, if you retry enough times do your circuit break,

154
00:09:50,444 --> 00:09:53,850
when do you come out of circuit breakers? How do you do all of that?

155
00:09:53,920 --> 00:09:57,850
And then on top of that you also have now new bricks and

156
00:09:57,920 --> 00:10:01,434
new things in the equation. If you're using kubernetes, you're probably using

157
00:10:01,472 --> 00:10:04,974
an overlay network. That means that you have to now understand how

158
00:10:05,012 --> 00:10:08,366
that overlay network is set up and how it

159
00:10:08,468 --> 00:10:11,966
fits with all the other components. Maybe you have a service mesh on top

160
00:10:11,988 --> 00:10:15,602
of that that also includes its own features and

161
00:10:15,736 --> 00:10:19,554
these extra layers. They solve come problems, but they obviously

162
00:10:19,752 --> 00:10:23,102
include their own problems and their own complexity

163
00:10:23,166 --> 00:10:26,594
that you have to somehow manage. Then if you look at the infrastructure

164
00:10:26,642 --> 00:10:30,418
level, you build for redundancy, which is great. In theory

165
00:10:30,514 --> 00:10:34,710
it should be easy, but it's another bit of complexity

166
00:10:35,050 --> 00:10:38,326
scheduling. If you're using kubernetes, like I mentioned, you have to

167
00:10:38,348 --> 00:10:41,574
understand how it's scheduled, you have to understand what the weak

168
00:10:41,622 --> 00:10:45,002
points are, what are the actions that you might

169
00:10:45,056 --> 00:10:48,806
take that will break the scheduling or things like self healing.

170
00:10:48,918 --> 00:10:52,762
It's a wonderful feature of kubernetes and to be able to get self healing for

171
00:10:52,816 --> 00:10:56,254
free. But what happens when that stops working? You now build

172
00:10:56,292 --> 00:10:59,406
your application in a way that relies on the behavior of

173
00:10:59,428 --> 00:11:03,406
kubernetes to bring it back to healthy state. What happens if it doesn't? Or what

174
00:11:03,428 --> 00:11:06,834
happens if you made it stop doing that because you

175
00:11:06,872 --> 00:11:10,770
misconfigured something, then obviously the isolation, all of that makes sense.

176
00:11:10,840 --> 00:11:14,014
If you're making good use of your machines,

177
00:11:14,142 --> 00:11:17,286
that means that you're going to be sharing things. That means that

178
00:11:17,308 --> 00:11:21,286
you have to take care of things like resource starvation. Do you understand

179
00:11:21,388 --> 00:11:25,442
well how the cpu limits are actually implemented?

180
00:11:25,506 --> 00:11:28,998
Do you understand what happens to the process if it goes

181
00:11:29,084 --> 00:11:33,034
and tries to get more ram that's allowed to? Do you have

182
00:11:33,152 --> 00:11:36,810
alerting on that? Will you notice that? Will you know about this?

183
00:11:36,880 --> 00:11:39,930
These are all kinds of problems that we now have to deal with.

184
00:11:40,000 --> 00:11:44,078
And these are all the kind of things that even if you're just starting and

185
00:11:44,164 --> 00:11:47,722
you start your own company, your stack is probably going to involve

186
00:11:47,786 --> 00:11:51,454
all of this wonderful and amazing features at the same time. All of this

187
00:11:51,492 --> 00:11:54,834
extra complexity to deal with. And then obviously, on top

188
00:11:54,872 --> 00:11:58,674
of that, this was all technical. We still have humans involved, we still

189
00:11:58,712 --> 00:12:02,594
have operations. Operations that have the overhead of

190
00:12:02,632 --> 00:12:05,874
having to understand that. Operations that can make mistakes,

191
00:12:06,002 --> 00:12:09,862
incident responses, tribal knowledge, all of that is still has

192
00:12:09,916 --> 00:12:13,634
applicable to all of that. None of the technical solutions can eliminate

193
00:12:13,682 --> 00:12:17,590
these things. So the complexity, it's basically why

194
00:12:17,660 --> 00:12:21,514
chaos engineering is so relevant, because we can go and

195
00:12:21,552 --> 00:12:25,142
we can test the systems as a whole and experiment

196
00:12:25,206 --> 00:12:28,838
on them to see how they cope with all of that complexity.

197
00:12:28,934 --> 00:12:33,322
So for those of you who are new to it, chaos Engineering is the discipline

198
00:12:33,386 --> 00:12:36,510
of experimenting on a system in order to build

199
00:12:36,580 --> 00:12:40,122
confidence in a system's capability to withstand turbulent

200
00:12:40,186 --> 00:12:43,586
conditions in production. And this is what I consider the

201
00:12:43,608 --> 00:12:47,554
canonical definition, circa 2015 from the

202
00:12:47,592 --> 00:12:50,894
principlesofchaos.org from the Netflix gang.

203
00:12:50,942 --> 00:12:54,174
If you remove the fluff from it, it's the experimenting

204
00:12:54,222 --> 00:12:57,954
to build confidence to withstand turbulent conditions.

205
00:12:58,082 --> 00:13:01,174
That's it. That's really what it's about.

206
00:13:01,292 --> 00:13:04,486
And that's why it's so relevant, and that is why it can

207
00:13:04,508 --> 00:13:07,878
be useful on so many levels. Because if you think about it,

208
00:13:07,884 --> 00:13:11,786
it doesn't replace the different ways of testing. You start with your

209
00:13:11,808 --> 00:13:15,510
unit test. When you focus on a small subset of a component

210
00:13:15,590 --> 00:13:19,382
to verify some kind of behavior, this is great. Then you have the integration

211
00:13:19,446 --> 00:13:23,346
tests where you take components and you put them together, and you verify

212
00:13:23,398 --> 00:13:27,690
that they work together. Another layer is probably the end to end tests,

213
00:13:27,770 --> 00:13:31,722
where you take the system as a whole, you spin up a small instance,

214
00:13:31,786 --> 00:13:35,374
and you verify some happy path. And then that's where chaos

215
00:13:35,422 --> 00:13:39,138
engineering kind of comes in, as an extra layer of

216
00:13:39,224 --> 00:13:42,882
verifying things. When you take the system as a whole and you

217
00:13:42,936 --> 00:13:45,954
experiment on this, you verify that when

218
00:13:45,992 --> 00:13:49,606
things happen to it, the unhappy path, when things happen to the

219
00:13:49,628 --> 00:13:53,414
system and might have nothing to do with the correctness of the

220
00:13:53,452 --> 00:13:57,330
code that you wrote. The system continues working as designed.

221
00:13:57,410 --> 00:14:00,906
And when we design the skills experiments, it's basically like

222
00:14:00,928 --> 00:14:04,106
the game of what if, right? It's all about thinking,

223
00:14:04,208 --> 00:14:07,674
what if. What if the network latency increases? What if

224
00:14:07,712 --> 00:14:11,130
traffic spikes? What if the database becomes slow or

225
00:14:11,200 --> 00:14:14,574
we trigger a circuit breaker? What if the application needs to

226
00:14:14,612 --> 00:14:18,478
heal? The what ifs about the system as a whole.

227
00:14:18,644 --> 00:14:22,702
You've verified your unit test, your integration test. But what happens

228
00:14:22,836 --> 00:14:26,542
when you don't have enough cpu? Or what happens if the slowness

229
00:14:26,606 --> 00:14:30,178
happens the way that you didn't expect it to? It's basically like the

230
00:14:30,344 --> 00:14:33,822
what if book by Randall Monroe. One of my favorite reads,

231
00:14:33,886 --> 00:14:38,002
where you go and ask yourself hypothetical questions. Probably not the

232
00:14:38,056 --> 00:14:41,622
kinds that you cover here, like how could we build a Lego bridge between

233
00:14:41,676 --> 00:14:44,658
London and New York? By the way, I really recommend that book if you haven't

234
00:14:44,674 --> 00:14:48,706
read it. But the kinds of what ifs that are realistic

235
00:14:48,818 --> 00:14:52,778
to your system. Right? And speaking of the chaos experiments, we've kind

236
00:14:52,784 --> 00:14:56,570
of bolded down to four steps. Now we're calling them

237
00:14:56,640 --> 00:15:00,694
experiments because we want to underline the kind of scientific

238
00:15:00,822 --> 00:15:04,746
behavior of that. If you really apply the scientific method

239
00:15:04,778 --> 00:15:08,058
to that, it boils down to four steps. You ensure

240
00:15:08,154 --> 00:15:11,854
observability first, and this is important, observability is this

241
00:15:11,892 --> 00:15:15,390
nice word about basically being able to measure

242
00:15:15,470 --> 00:15:18,990
something reliably so that you can observe a behavior.

243
00:15:19,070 --> 00:15:22,910
So for example, if you have some kind of API server

244
00:15:22,990 --> 00:15:26,454
and you serve requests over the Internet, if you pick a

245
00:15:26,492 --> 00:15:30,134
variable that you can reliably measure, you can build your

246
00:15:30,172 --> 00:15:34,102
observability on that. So if you, for example, measure the

247
00:15:34,156 --> 00:15:37,942
request per second, or average latency, or whatever

248
00:15:37,996 --> 00:15:41,370
it is that your metric is, you can build your observability on that.

249
00:15:41,440 --> 00:15:44,570
Then measuring the steady state, steady state

250
00:15:44,640 --> 00:15:47,974
is just a fancy way of saying that this is the normal range.

251
00:15:48,022 --> 00:15:51,866
So let's say that, for example, we measure the request per second

252
00:15:51,968 --> 00:15:55,806
and our normal range with the hardware that we have. And the setup that

253
00:15:55,828 --> 00:15:59,102
we have is about 100,000 requests per second, right?

254
00:15:59,156 --> 00:16:02,542
Let's say that plus, minus five or 10%

255
00:16:02,596 --> 00:16:05,874
of that is our normal range. This is the steady state. And the first

256
00:16:05,912 --> 00:16:09,858
step is when we start having fun, we go and we think

257
00:16:09,944 --> 00:16:13,442
about what if, what happens if 20%

258
00:16:13,496 --> 00:16:16,782
of the servers go down? What happens if the servers

259
00:16:16,846 --> 00:16:20,482
don't have enough cpu? What happens if one of the servers

260
00:16:20,546 --> 00:16:23,654
restarts? All kinds of things that we can take a look

261
00:16:23,692 --> 00:16:27,442
at and we can actually implement as an experiment.

262
00:16:27,506 --> 00:16:31,174
And if you go and form this hypothesis and you have the right

263
00:16:31,212 --> 00:16:34,374
observability and, you know, your steady state, then you can run the experiment.

264
00:16:34,422 --> 00:16:38,026
And ideally, we spend a lot of time trying to automate that and trying

265
00:16:38,048 --> 00:16:41,434
to make sure that this is repeatable and verifying that,

266
00:16:41,472 --> 00:16:45,274
because we're working on an entire system. So it's easy to influence

267
00:16:45,322 --> 00:16:48,462
multiple variables at the same time. And sometimes it's not obvious

268
00:16:48,516 --> 00:16:51,870
to isolate what you want to do, but you run the experiment, and then

269
00:16:51,940 --> 00:16:55,674
the nice thing about the experiment is that whatever the outcome is,

270
00:16:55,732 --> 00:16:59,154
you're good, because if it works, that's great. That means

271
00:16:59,192 --> 00:17:02,898
that you get more confident in your system. You know that in this particular

272
00:17:02,984 --> 00:17:06,642
situation, the system behaves the way that you expect it to if

273
00:17:06,696 --> 00:17:10,582
it broke. On the other hand, that's also great, because that means that you found

274
00:17:10,636 --> 00:17:14,150
a problem and you found a problem that you can fix before the problem

275
00:17:14,220 --> 00:17:17,558
found you, or before a client came and complained to you.

276
00:17:17,644 --> 00:17:21,398
So this is the ideal situation. These are the chaos experiments,

277
00:17:21,494 --> 00:17:25,226
these are the trends that all put us in this

278
00:17:25,248 --> 00:17:29,014
kind of interesting position on the innovation adoption

279
00:17:29,062 --> 00:17:32,938
lifecycle. This is what typically looks like it's a bowl curve.

280
00:17:33,034 --> 00:17:36,734
You will see that when you read about adoption of

281
00:17:36,772 --> 00:17:40,110
new technology. Every new technology goes through something like this,

282
00:17:40,180 --> 00:17:43,862
whether it's smartphones or tablets or wearables or whatnot.

283
00:17:43,946 --> 00:17:47,410
And it basically means that we have a very small group of first

284
00:17:47,480 --> 00:17:51,074
innovators, people who see enough value in the new

285
00:17:51,112 --> 00:17:54,766
technology to put up with the terrible uis,

286
00:17:54,878 --> 00:17:58,470
terrible UX, or bugs, or just bad

287
00:17:58,540 --> 00:18:02,310
first versions. Then after that we have a slightly bigger group

288
00:18:02,380 --> 00:18:06,470
of early adopters who are still able to put up with a certain

289
00:18:06,540 --> 00:18:09,970
amount of things and come lies that are not ironed out yet,

290
00:18:10,060 --> 00:18:13,734
but still see enough value to drive the adoption.

291
00:18:13,782 --> 00:18:17,306
And then that group grows and grows, and eventually, when we hit a certain

292
00:18:17,408 --> 00:18:21,286
critical mass, it becomes an early majority, the late majority,

293
00:18:21,398 --> 00:18:24,430
where most of the people already use the technology,

294
00:18:24,580 --> 00:18:27,934
and then potentially some laggards towards the end

295
00:18:27,972 --> 00:18:31,646
of the curve, the right hand side of the curve. So what happens to a

296
00:18:31,668 --> 00:18:35,554
technology is that it goes from exciting and new and

297
00:18:35,592 --> 00:18:39,214
unusual, and as it goes and gets more and more adoption,

298
00:18:39,262 --> 00:18:43,630
it turns into boring and just part of the landscape.

299
00:18:43,710 --> 00:18:47,294
And I'm going to argue here that we really want chaos

300
00:18:47,342 --> 00:18:50,774
engineering to be boring and be part

301
00:18:50,812 --> 00:18:54,070
of the landscape, and that this year might be just a moment

302
00:18:54,140 --> 00:18:57,878
when it becomes a fact of life. And boring is good, because when you

303
00:18:57,884 --> 00:19:01,446
think about smartphones a few years ago, every new version

304
00:19:01,478 --> 00:19:04,934
of a smartphone that got released has significantly

305
00:19:04,982 --> 00:19:08,666
better than the previous one, got all these new features and all

306
00:19:08,688 --> 00:19:12,810
of that. But at some point, the speed of innovation plateaus.

307
00:19:12,890 --> 00:19:16,510
It becomes more and more boring. Every new smartphone now basically

308
00:19:16,580 --> 00:19:19,870
looks the same, has roughly the same better life, and until

309
00:19:19,940 --> 00:19:24,426
there is a new breakthrough. Looking at you foldable

310
00:19:24,538 --> 00:19:28,542
phones, we are at the boring stage. And that means that smartphone,

311
00:19:28,606 --> 00:19:32,146
everybody has one. That means that it's just part of the landscape. That means that

312
00:19:32,168 --> 00:19:35,470
there are parts of the world where you don't have a pc, but statistically,

313
00:19:35,550 --> 00:19:39,030
you do have a smartphone, and that is a good thing. That means that everybody

314
00:19:39,100 --> 00:19:43,446
has access to that, and everybody can get value out of that. Another example

315
00:19:43,628 --> 00:19:47,142
is rocket science. We're getting to the point now

316
00:19:47,196 --> 00:19:50,522
with SpaceX, where no one's really watching their

317
00:19:50,576 --> 00:19:53,914
launches anymore because they got boring. First time

318
00:19:53,952 --> 00:19:57,766
I saw that rocket go up there, and then one of the stages

319
00:19:57,958 --> 00:20:01,626
land, kind of like in science fiction movies. It was

320
00:20:01,648 --> 00:20:05,342
really, really exciting, but now just kind of happens and doesn't make

321
00:20:05,396 --> 00:20:08,378
headlines anymore because the rockets keep landing.

322
00:20:08,474 --> 00:20:11,694
The starship didn't land particularly well for the last

323
00:20:11,732 --> 00:20:15,314
two tries, so that made news. But when it gets boring, no one

324
00:20:15,352 --> 00:20:18,610
will be talking about it. Right? Same for flights before,

325
00:20:18,680 --> 00:20:22,270
COVID you could just jump on a plane and be reasonably

326
00:20:22,350 --> 00:20:25,634
sure to be delivered safely to your destination at

327
00:20:25,672 --> 00:20:29,014
a reasonable price. Something that people not

328
00:20:29,052 --> 00:20:32,514
that long ago couldn't even imagine. So boring

329
00:20:32,562 --> 00:20:35,986
is good. And the closer we get to boring with chaos engineering,

330
00:20:36,018 --> 00:20:39,798
the better, because everything improves, right? If you

331
00:20:39,964 --> 00:20:43,350
looked at it in 2015, there has the chaos monkey,

332
00:20:43,430 --> 00:20:46,746
and you had a choice of either use chaos monkey or write your

333
00:20:46,768 --> 00:20:50,054
own tool. If you take a look at it today, there are dozens

334
00:20:50,102 --> 00:20:53,690
of open source projects that you can use. There are tools for pretty much anything

335
00:20:53,760 --> 00:20:56,894
you can think of. There are dozens of commercial tools that you can use.

336
00:20:56,932 --> 00:21:00,526
You can actually go and pay someone come money, so that they bring the

337
00:21:00,548 --> 00:21:03,806
knowledge for you in house, and you can get started really quickly.

338
00:21:03,908 --> 00:21:07,090
Things are improving, and then we need the critical mass

339
00:21:07,160 --> 00:21:10,834
of companies that start seeing the value, and we're seeing the

340
00:21:10,872 --> 00:21:14,610
increase in that. We're seeing companies that have been historically very

341
00:21:14,680 --> 00:21:17,750
risk averse, that become more and more interested in that.

342
00:21:17,820 --> 00:21:21,458
And this is how we get it on the roadmap.

343
00:21:21,554 --> 00:21:25,042
We get it boring, and boring is good. Unfortunately,

344
00:21:25,186 --> 00:21:29,270
it's not all rosy quite yet. There are still things that we need to address,

345
00:21:29,420 --> 00:21:32,842
and I'm really hoping that 2021 is when we do that.

346
00:21:32,896 --> 00:21:36,394
This is a little poll that I did on LinkedIn towards the end of last

347
00:21:36,432 --> 00:21:40,294
year. And I asked people, what's blocking you from doing chaos

348
00:21:40,342 --> 00:21:44,514
engineering? And the top two responses were difficult to generate.

349
00:21:44,582 --> 00:21:48,510
Buying with 50% and inadequate training with 27.

350
00:21:48,580 --> 00:21:52,126
The missing or hard to use tools only got 11% of

351
00:21:52,148 --> 00:21:55,490
the votes. So after speaking a lot to

352
00:21:55,560 --> 00:21:59,250
different people in different companies, I identified the

353
00:21:59,320 --> 00:22:03,426
myths that still kind of float around

354
00:22:03,528 --> 00:22:07,070
chaos engineering as one of the top contributors

355
00:22:07,150 --> 00:22:10,646
to why generating buy in for chaos engineering is still a

356
00:22:10,668 --> 00:22:14,134
little bit tricky in 2021. And so I wanted to

357
00:22:14,172 --> 00:22:18,162
cover some of that with you so that you're prepared for having these conversations

358
00:22:18,306 --> 00:22:21,818
so that when that arrives and someone tries to use that,

359
00:22:21,904 --> 00:22:25,322
you can debunk them. One of the things is the

360
00:22:25,456 --> 00:22:29,450
it's chaos monkey, right? Kind of situation. It's an interesting one to

361
00:22:29,520 --> 00:22:33,242
kind of talk about, because, yeah, in a way, it was all kind of started

362
00:22:33,296 --> 00:22:37,546
by SCS monkey back in the day. Netflix moved on to AWS

363
00:22:37,658 --> 00:22:41,834
2010 eleven, I think, and they started doing chaos

364
00:22:41,882 --> 00:22:45,614
monkey. Eventually they spread it, and that's how the chaos engineering name

365
00:22:45,732 --> 00:22:49,554
was populated. Right. That is a testament to chaos monkey being

366
00:22:49,592 --> 00:22:53,554
a good name, because everybody seems to know it, but it's so much more right

367
00:22:53,592 --> 00:22:56,974
now. It's like the entire spectrum. You have the chaos

368
00:22:57,022 --> 00:23:00,566
monkey on one side, where you don't really need to know that

369
00:23:00,588 --> 00:23:04,402
much of your system. You can set it up very easily,

370
00:23:04,546 --> 00:23:07,926
and you can randomly break things and see what happens. And this

371
00:23:07,948 --> 00:23:11,382
is valuable. This is how it all started. It allows you to detect

372
00:23:11,446 --> 00:23:15,322
things that you missed in other layers of testing. It allows you to test

373
00:23:15,376 --> 00:23:18,954
for emerging properties, the things that happen

374
00:23:19,152 --> 00:23:22,506
because of the interactions between different components that

375
00:23:22,528 --> 00:23:25,926
you wouldn't have caught with your unit tests or integration tests.

376
00:23:25,958 --> 00:23:29,054
And this is great. This is a very good start where you don't know where

377
00:23:29,092 --> 00:23:32,206
to start, but there is the entire side of the kind of other side

378
00:23:32,228 --> 00:23:36,370
of the spectrum where you can start by analyzing your system and

379
00:23:36,440 --> 00:23:40,706
start from the weak points of your system, and design very

380
00:23:40,808 --> 00:23:44,178
custom tailored experiments that

381
00:23:44,264 --> 00:23:47,734
deal with very particular failures that you expect to

382
00:23:47,772 --> 00:23:51,302
do. And the fact that we have this entire spectrum and you can find

383
00:23:51,356 --> 00:23:54,370
your happy point, your happy spot,

384
00:23:54,450 --> 00:23:58,402
anywhere on this line, means that adoption

385
00:23:58,466 --> 00:24:01,718
is possible for different companies with different constraints,

386
00:24:01,814 --> 00:24:05,894
different possibilities for what kind of risk is acceptable

387
00:24:05,942 --> 00:24:09,494
to take. Right. Another thing that I keep hearing is that it's testing

388
00:24:09,542 --> 00:24:13,006
production. And I'm expecting this one is probably never going

389
00:24:13,028 --> 00:24:16,814
to go away, because there was so much about breaking things randomly in

390
00:24:16,852 --> 00:24:20,414
production at the beginning. And it's also understandable because it is the

391
00:24:20,452 --> 00:24:24,570
holy grail. Ideally, we would all be so amazingly

392
00:24:24,650 --> 00:24:27,950
confident and have done so much testing with all the other phases

393
00:24:28,030 --> 00:24:31,826
that we would inject the failure in the production system. Because if

394
00:24:31,848 --> 00:24:35,586
you think about it, that's where the real data is and everything else is

395
00:24:35,608 --> 00:24:39,126
an approximation. We will never be 100% sure.

396
00:24:39,228 --> 00:24:42,898
We can never 100% test a system because it's

397
00:24:42,994 --> 00:24:46,774
a copy, a clone, an approximation of what the production one is.

398
00:24:46,812 --> 00:24:50,374
But it obviously doesn't mean that you have to start there. And it doesn't mean

399
00:24:50,412 --> 00:24:53,942
that if it's not in production, it's not case engineering.

400
00:24:54,006 --> 00:24:57,814
You can get a lot of value, and in some cases, you probably can't

401
00:24:57,862 --> 00:25:01,882
ever go to production. If people will die, if you accidentally

402
00:25:01,946 --> 00:25:05,406
detect a failure in your system, probably never going

403
00:25:05,428 --> 00:25:09,230
to be acceptable. But that's okay, because everybody can apply

404
00:25:09,300 --> 00:25:13,166
that to the level where it makes sense for them. And if

405
00:25:13,188 --> 00:25:16,382
you can go all the way to the production, that's great, and we should probably

406
00:25:16,436 --> 00:25:20,638
talk. But it doesn't mean that this should hamper

407
00:25:20,734 --> 00:25:24,242
your adoption. Another one is, it's only for

408
00:25:24,296 --> 00:25:28,114
x massively distributed systems, or microservices,

409
00:25:28,242 --> 00:25:31,622
or golang, or kubernetes, or cloud.

410
00:25:31,756 --> 00:25:35,558
And this is also a myth, because kind of like similar to

411
00:25:35,644 --> 00:25:39,222
adapting to come engineering to your system, it doesn't really matter

412
00:25:39,276 --> 00:25:42,502
what your system is. You can still use the same methodology,

413
00:25:42,646 --> 00:25:46,182
you can still apply the same scientific framework to designing

414
00:25:46,246 --> 00:25:49,370
and running this experiment. And in my book,

415
00:25:49,440 --> 00:25:52,474
I'm going to give a link at the end, I have an entire chapter that

416
00:25:52,512 --> 00:25:55,738
talks about a single process that's legacy that you're

417
00:25:55,754 --> 00:25:59,342
not even taking a look at the source code of, that you can play with

418
00:25:59,396 --> 00:26:03,046
and tinker and experiments to verify that the retry logic

419
00:26:03,098 --> 00:26:06,626
that it supposedly has is working on

420
00:26:06,648 --> 00:26:10,098
a single process, on a single machine, without a distributed system

421
00:26:10,184 --> 00:26:13,346
or without kubernetes. So it's really important to

422
00:26:13,368 --> 00:26:17,234
stress that it's flexible enough that you can get value out

423
00:26:17,272 --> 00:26:20,614
of applying this to pretty much any system as big or as

424
00:26:20,652 --> 00:26:23,958
small or distributed as it is. And obviously, if you

425
00:26:23,964 --> 00:26:27,334
have a massive, amazing system and you do that

426
00:26:27,372 --> 00:26:30,994
at scale, that makes for a nice, better blog post. But you can apply

427
00:26:31,052 --> 00:26:34,534
that to anywhere, and there is no reason to not harvest that low hanging

428
00:26:34,582 --> 00:26:38,426
fruit and their nice return on investment. Another one that

429
00:26:38,448 --> 00:26:41,642
I keep hearing is that our organization is not mature enough

430
00:26:41,696 --> 00:26:45,534
for cash engineering. And I get it. It probably

431
00:26:45,652 --> 00:26:49,214
comes from just being a little bit intimidated by the big guys and

432
00:26:49,252 --> 00:26:52,686
thinking, okay, yeah, but we're not Netflix or Google, so we might not

433
00:26:52,708 --> 00:26:56,274
be mature enough. It might be coming from just not

434
00:26:56,312 --> 00:26:59,860
being confident that confident in the existing test.

435
00:27:00,470 --> 00:27:03,790
If you expect things to break, that can be intimidating.

436
00:27:03,870 --> 00:27:07,346
But once again, this is a flexible framework that

437
00:27:07,368 --> 00:27:10,534
you can apply to whatever makes sense for you.

438
00:27:10,572 --> 00:27:14,614
So if you don't feel you're mature enough to run

439
00:27:14,652 --> 00:27:17,686
that on a big part of a system, you can start with a small part

440
00:27:17,708 --> 00:27:21,434
of a system. It's something that really can be applied at

441
00:27:21,472 --> 00:27:25,386
any level of maturity. And obviously the value you get will differ,

442
00:27:25,488 --> 00:27:28,842
but it's good to get into the habit of doing that anyway.

443
00:27:28,976 --> 00:27:32,238
And you can already get value and then connected to that

444
00:27:32,324 --> 00:27:36,606
is like this one. I literally get it pretty much every time.

445
00:27:36,708 --> 00:27:40,810
We already have chaos, wink wink, kind of joke.

446
00:27:40,890 --> 00:27:44,894
And if you're saying things like that, you don't really understand what chaos

447
00:27:44,942 --> 00:27:48,994
engineering is about, because it's not about increasing the amount of chaos or

448
00:27:49,032 --> 00:27:53,406
increasing the number of things that can go wrong and unknowns.

449
00:27:53,438 --> 00:27:57,330
It's about decreasing that by injecting failure. That's very

450
00:27:57,400 --> 00:28:00,802
controlled, that you understand. If you just inject random

451
00:28:00,866 --> 00:28:04,306
failure into your system, that's probably not going to teach you much apart

452
00:28:04,338 --> 00:28:07,282
from potentially, oh, your system doesn't work when something breaks.

453
00:28:07,346 --> 00:28:10,986
But if you know exactly what you're injecting and it's a

454
00:28:11,008 --> 00:28:14,678
reasonable thing to expect to happen to your system, the popular kinds

455
00:28:14,694 --> 00:28:18,026
of failures, you decrease the amount of chaos, you know

456
00:28:18,128 --> 00:28:21,222
how it's going to behave. So that's no longer an unknown,

457
00:28:21,286 --> 00:28:24,554
that's something that you control, you can fix, you can modify.

458
00:28:24,682 --> 00:28:27,946
So if you're saying that, have a think about what chaos

459
00:28:27,978 --> 00:28:31,306
in uni is really about, because it's not about increasing

460
00:28:31,338 --> 00:28:35,430
chaos, it's about decreasing that and finally breaking

461
00:28:35,450 --> 00:28:39,026
things randomly. Kind of goes back to chaos monkey. And I kind of

462
00:28:39,048 --> 00:28:43,214
touched on that, talking about the spectrum. But it's really, really important to stress

463
00:28:43,262 --> 00:28:46,786
that the randomness, it's kind of like the practice of fuzzing when

464
00:28:46,808 --> 00:28:50,674
you generate random inputs or random race

465
00:28:50,722 --> 00:28:54,262
conditions and random orders, and you verify that the system works

466
00:28:54,316 --> 00:28:57,862
well, and this is great, but it's just part of it now, and there is

467
00:28:57,916 --> 00:29:01,674
more to it. And if the randomness doesn't work in your case,

468
00:29:01,792 --> 00:29:04,822
you should probably look at the other side of the spectrum.

469
00:29:04,886 --> 00:29:08,762
Okay, so this is the myths that I keep hearing, and if you

470
00:29:08,816 --> 00:29:12,810
are able to talk about them, you're already in a good spot to

471
00:29:12,880 --> 00:29:16,222
get cursed in your roadmap. But there's also two

472
00:29:16,356 --> 00:29:19,710
conversations that you need to be prepared to have

473
00:29:19,780 --> 00:29:23,514
with other people. And these are the things that I called getting called

474
00:29:23,572 --> 00:29:26,990
less and risk versus reward or sharks versus

475
00:29:27,070 --> 00:29:30,814
hamburger kind of situation. So let's start with getting cold

476
00:29:30,862 --> 00:29:34,546
less. Because if you're talking about cursor engineering and to

477
00:29:34,568 --> 00:29:38,274
the people who actually get cold at night, this is likely to land

478
00:29:38,392 --> 00:29:41,830
as the best argument that you can use. If there is something

479
00:29:41,900 --> 00:29:45,158
that can be done so that I don't have to wake up at night

480
00:29:45,244 --> 00:29:48,614
and log in and context switch and try

481
00:29:48,652 --> 00:29:51,706
to figure out what happened and get coffee. Anything I can

482
00:29:51,728 --> 00:29:55,526
do to avoid that is great. So the selling point of curse engineering

483
00:29:55,558 --> 00:29:59,498
for these people is just that you can do it during working hours.

484
00:29:59,584 --> 00:30:02,810
The more we experiment and more we find things

485
00:30:02,880 --> 00:30:06,058
that break during working hours. That means that you don't

486
00:30:06,074 --> 00:30:09,486
have to be cold at night. So typically that lands really well. And this

487
00:30:09,508 --> 00:30:13,322
is a very simple argument in favor of gas engineering.

488
00:30:13,466 --> 00:30:17,262
But the more complicated conversation typically

489
00:30:17,326 --> 00:30:21,058
is with your manager or someone who needs to okay doing something like that.

490
00:30:21,144 --> 00:30:24,626
And this is the risk versus reward situation. So it

491
00:30:24,648 --> 00:30:28,118
turns out that when you think about that, we tend to be pretty

492
00:30:28,204 --> 00:30:31,858
terrible at estimating the risks and rewards

493
00:30:31,954 --> 00:30:35,810
of things. And there's the entire evolutionary

494
00:30:35,890 --> 00:30:39,622
history behind that. But if we don't put numbers on a napkin,

495
00:30:39,686 --> 00:30:43,114
back of the napkin, we're probably going to make some bad

496
00:30:43,152 --> 00:30:46,246
decisions if we just follow our instincts.

497
00:30:46,278 --> 00:30:50,534
And to illustrate that, I would like you to think about how likely

498
00:30:50,582 --> 00:30:53,838
you are to be in danger of a shark attack.

499
00:30:53,924 --> 00:30:57,310
How likely are you to die of a shark attack?

500
00:30:57,380 --> 00:31:01,002
How scared should you be? And if you ask a person in the streets,

501
00:31:01,066 --> 00:31:04,554
chances are that they're going to overestimate that risk very,

502
00:31:04,612 --> 00:31:08,466
very strongly. If my googling skills didn't fail me,

503
00:31:08,568 --> 00:31:11,858
2020 has three fatal shark attacks in

504
00:31:11,864 --> 00:31:14,818
the US, the country that tends to have most of them.

505
00:31:14,904 --> 00:31:18,534
Whereas if you take a look at the actual things that kill people,

506
00:31:18,652 --> 00:31:23,078
heart disease number one, takes about 650,000

507
00:31:23,164 --> 00:31:26,262
people every year in the United States,

508
00:31:26,396 --> 00:31:29,862
meaning that the likelihood of you dying from eating

509
00:31:29,926 --> 00:31:34,006
hamburgers that look really good but are unhealthy

510
00:31:34,038 --> 00:31:37,482
for you and increase your bad cholesterol is so much higher

511
00:31:37,536 --> 00:31:40,682
than getting killed by a shark that it should really not

512
00:31:40,736 --> 00:31:44,426
be difficult, one, to compare, but because of the fact that sharks

513
00:31:44,458 --> 00:31:48,238
have this teeth and they look scary. And also,

514
00:31:48,324 --> 00:31:51,102
there was a movie about them, or maybe two.

515
00:31:51,236 --> 00:31:55,314
I'm looking at you, the meg. The fact that the

516
00:31:55,352 --> 00:31:58,526
stats doesn't support your fear doesn't prevent

517
00:31:58,558 --> 00:32:02,082
you from being scared of it, right? So next time

518
00:32:02,136 --> 00:32:05,790
you look at that hamburger, think about it, because that hamburgers

519
00:32:05,870 --> 00:32:09,266
is potentially much more dangerous to you than sharks.

520
00:32:09,298 --> 00:32:12,870
But what does all of that have to do with chaos engineering. If you put

521
00:32:12,940 --> 00:32:16,982
numbers together for the risks that are involved in

522
00:32:17,036 --> 00:32:21,026
doing some of this curse engineering, and you do things like monitoring

523
00:32:21,058 --> 00:32:24,570
the blast radius and minimizing the blast radius, and you do things

524
00:32:24,640 --> 00:32:28,106
like making sure that you only apply the experiments to

525
00:32:28,128 --> 00:32:31,518
a small subset of things, and you don't just rush into production and

526
00:32:31,524 --> 00:32:35,002
you do things properly. The risks can be controlled,

527
00:32:35,066 --> 00:32:39,034
can be managed, and they might be your shark

528
00:32:39,162 --> 00:32:43,026
that you're scared of, whereas you should be scared of

529
00:32:43,048 --> 00:32:46,654
that hamburger instead. So with these two conversations

530
00:32:46,782 --> 00:32:51,060
and the myths and understanding the trends and kind of like interesting

531
00:32:52,070 --> 00:32:56,094
crossroads moment for chaos engineering on the adoption curve

532
00:32:56,142 --> 00:32:59,734
right now, we're finding ourselves in this very interesting point in

533
00:32:59,772 --> 00:33:03,174
time when we can make the push and we can get

534
00:33:03,212 --> 00:33:07,154
the curse engineering over that bump where it becomes boring

535
00:33:07,202 --> 00:33:11,274
and it becomes a part of that landscape. If you would like to learn more

536
00:33:11,392 --> 00:33:15,194
about curse engineering, I published a book about it last

537
00:33:15,232 --> 00:33:18,650
year. It's called curse engineering. Site reliability through control

538
00:33:18,720 --> 00:33:22,314
disruption. It's from money. It's a complement to existing

539
00:33:22,362 --> 00:33:26,362
books where I try to bring a very, very practical approach

540
00:33:26,426 --> 00:33:29,790
and cover all the tools that you might need and show you

541
00:33:29,860 --> 00:33:33,694
and kind of demystify that using chaos engineering can

542
00:33:33,732 --> 00:33:37,042
be fun, can be easy to start with. You don't necessarily need

543
00:33:37,096 --> 00:33:40,894
very complicated or sophisticated tools. You can reuse

544
00:33:40,942 --> 00:33:44,642
things that you already know and that all you need to do is change

545
00:33:44,696 --> 00:33:48,318
a little bit your mindset and then use all those tools. So the book

546
00:33:48,344 --> 00:33:52,086
is covering various different levels of your tech stack that you

547
00:33:52,108 --> 00:33:55,478
might be exposed to. It's starting with, like I mentioned, a single

548
00:33:55,564 --> 00:33:58,886
binary that you don't know what it's doing, apart from

549
00:33:58,908 --> 00:34:02,774
the fact that you know it should be handing the errors, and you manipulate

550
00:34:02,822 --> 00:34:06,714
system calls to verify that the retry logic works okay

551
00:34:06,832 --> 00:34:11,066
through things like containers and how they break, how they work, what's actually

552
00:34:11,168 --> 00:34:14,798
Linux containers, what's actually under the hood, how that works,

553
00:34:14,884 --> 00:34:18,286
how to understand what happens when you hit the limitations of

554
00:34:18,308 --> 00:34:22,186
Docker, how to implement your Liu container, and how to test applications

555
00:34:22,218 --> 00:34:25,530
that run in containers all the way to Kubernetes,

556
00:34:25,610 --> 00:34:29,042
and how to test Kubernetes itself. If you happen to be one of the people

557
00:34:29,096 --> 00:34:32,866
who are responsible for a Kubernetes based platform, where you

558
00:34:33,048 --> 00:34:36,086
need to make sure that it runs, how it's built under the

559
00:34:36,108 --> 00:34:39,830
hood, how it breaks, what the fragile points are,

560
00:34:39,900 --> 00:34:44,034
and also testing kubernetes, the software that runs on kubernetes,

561
00:34:44,162 --> 00:34:47,734
to verify that the ongoing verification of things like

562
00:34:47,772 --> 00:34:52,054
slas and various other things like working with JVM and injecting bytecode

563
00:34:52,102 --> 00:34:55,530
to trigger exceptions to verify that the

564
00:34:55,600 --> 00:34:59,594
behavior is the way that you expected, all the way up to JavaScript and

565
00:34:59,632 --> 00:35:03,034
working with browser to verify that your uis that

566
00:35:03,072 --> 00:35:06,762
get more and more complex and more and more fancy are actually behaving

567
00:35:06,826 --> 00:35:10,254
the way that you want it. So there's a discount code that you can see

568
00:35:10,292 --> 00:35:13,566
here. I think there's also going to be

569
00:35:13,588 --> 00:35:16,866
some free copies if you want to grab some for the conference. And let

570
00:35:16,888 --> 00:35:20,126
me know if you'd like to connect. This is LinkedIn.

571
00:35:20,238 --> 00:35:23,758
If you scan this code, that should bring you to my profile.

572
00:35:23,854 --> 00:35:27,766
Probably the best way to reach out to me the photo credits and

573
00:35:27,868 --> 00:35:30,920
if you are interested in this kind of problems,

574
00:35:31,450 --> 00:35:35,346
we are hiring at Bloomberg. Go to bloomberg.com careers

575
00:35:35,458 --> 00:35:38,742
and reach out to us. And with that, thank you very much.

576
00:35:38,796 --> 00:35:42,326
I'm really hoping that the 2021 will be the time when

577
00:35:42,348 --> 00:35:45,526
we get the chaos engineering to be the boring part. We get

578
00:35:45,548 --> 00:35:49,174
it over the hump of the adoption and it's going to be part of

579
00:35:49,212 --> 00:35:52,430
everybody's framework, because there's really no reason to not

580
00:35:52,500 --> 00:35:56,058
get all of that return on investment because it's

581
00:35:56,074 --> 00:35:59,646
easy. So thank you very much. Have a great day at the conference, enjoy the

582
00:35:59,668 --> 00:36:01,580
rest of it, and I'll speak to you soon.

