1
00:00:25,490 --> 00:00:28,802
Hi, very good day to everyone. My name is Ang, based in Singapore

2
00:00:28,866 --> 00:00:32,594
and today I'll be talking about beginning your own data engineering projects.

3
00:00:32,722 --> 00:00:37,058
Before we start, let me talk a little bit about what is data engineering

4
00:00:37,154 --> 00:00:41,266
currently in my role, I help drive a lot of data engineering projects

5
00:00:41,298 --> 00:00:44,610
at work, mainly to help my company become more data driven.

6
00:00:44,690 --> 00:00:48,786
And data engineering actually helps to ensure consistent

7
00:00:48,818 --> 00:00:52,126
data flow for data data scientists or data users, typically in

8
00:00:52,148 --> 00:00:55,674
the form of a data warehouse to enable large scale data mining,

9
00:00:55,722 --> 00:00:59,710
analytics and also for reporting purposes. If youre also thinking

10
00:00:59,780 --> 00:01:02,830
of collecting more data on your own for your

11
00:01:02,900 --> 00:01:06,354
own data science projects, this talk might also be useful for

12
00:01:06,392 --> 00:01:10,066
you. As you know, data engineers are also important because

13
00:01:10,168 --> 00:01:13,246
they help build the foundation for data driven enterprises.

14
00:01:13,438 --> 00:01:16,750
Usually before we get to do data science or

15
00:01:16,760 --> 00:01:20,486
data analytics, the most important thing is to have quality data.

16
00:01:20,668 --> 00:01:23,794
So with all these data engineering projects,

17
00:01:23,922 --> 00:01:26,946
what we are trying to do is to build data pipelines,

18
00:01:27,058 --> 00:01:31,050
and we actually help to transfer

19
00:01:31,200 --> 00:01:34,758
data from one place to another through these pipelines.

20
00:01:34,854 --> 00:01:38,230
And so instead of transporting

21
00:01:38,310 --> 00:01:41,614
liquid or gas, what we are trying to do over here is

22
00:01:41,652 --> 00:01:45,440
to transport data. So the typical process actually

23
00:01:45,810 --> 00:01:49,662
involves extracting data from one place and

24
00:01:49,716 --> 00:01:53,678
then transforming them before loading them into the data warehouse.

25
00:01:53,774 --> 00:01:56,974
So this process is called ETL extract,

26
00:01:57,022 --> 00:02:00,466
transform, load. And at my workplace currently,

27
00:02:00,648 --> 00:02:04,500
we make use of APIs that points to

28
00:02:04,890 --> 00:02:08,294
different sources. Currently what we have

29
00:02:08,332 --> 00:02:11,542
is different kinds of campaigns begin run on different

30
00:02:11,596 --> 00:02:15,174
advertising platforms. So these advertising platforms have a lot

31
00:02:15,212 --> 00:02:18,294
of data collecting related to the campaigns.

32
00:02:18,422 --> 00:02:22,678
What we have is to try to build pipelines

33
00:02:22,774 --> 00:02:26,346
that pull data from these platforms, then store them in

34
00:02:26,368 --> 00:02:30,138
our own data warehouse, which is on Google Cloud platform.

35
00:02:30,304 --> 00:02:34,334
So depending on your context, depending on your company, you might have in house

36
00:02:34,372 --> 00:02:38,074
systems that contain transactional

37
00:02:38,122 --> 00:02:41,966
data, and then you want to use them for analysis. So what you

38
00:02:41,988 --> 00:02:46,034
have to do is to extract them and then probably do

39
00:02:46,072 --> 00:02:49,954
some cleaning or transformation before loading them into your

40
00:02:49,992 --> 00:02:54,114
data warehouse, which you can then use for data analysis. So we

41
00:02:54,152 --> 00:02:57,462
want to automate this process as much as possible to save time

42
00:02:57,516 --> 00:03:01,586
and ensure consistency and accuracy. So as you know, like the source

43
00:03:01,618 --> 00:03:05,320
data, for example, if it's containing transactional data,

44
00:03:05,770 --> 00:03:09,962
there might be some errors, data here and there, or some

45
00:03:10,016 --> 00:03:13,690
test cases here and there that are being stored and that

46
00:03:13,760 --> 00:03:17,562
should not be used for analysis. So during the

47
00:03:17,616 --> 00:03:21,158
transformation process, this is where we do the data cleaning. We want to

48
00:03:21,184 --> 00:03:25,326
check that whatever data that we have for our analysis data

49
00:03:25,508 --> 00:03:29,422
is as accurate as possible. So the data

50
00:03:29,476 --> 00:03:33,314
warehouse should be the place where there is a single source of truth and

51
00:03:33,352 --> 00:03:37,166
it should contain the most accurate data. So today I'll

52
00:03:37,198 --> 00:03:40,786
be sharing about a few use cases. So the

53
00:03:40,808 --> 00:03:44,082
first one is related towards data

54
00:03:44,136 --> 00:03:47,702
science, distinct. What I am trying to do over

55
00:03:47,756 --> 00:03:51,346
here is that for those who are not so familiar

56
00:03:51,458 --> 00:03:55,634
about towards data science, it actually is a publication hosted

57
00:03:55,682 --> 00:03:59,978
on medium where different authors, different writers actually

58
00:04:00,144 --> 00:04:02,460
write about what they have done,

59
00:04:03,310 --> 00:04:06,458
what they have realized through the different

60
00:04:06,544 --> 00:04:10,410
data science project that they do on their own, or be it

61
00:04:10,480 --> 00:04:14,094
for their work as well. So there are

62
00:04:14,132 --> 00:04:17,562
a lot of articles being published on a day to day basis.

63
00:04:17,706 --> 00:04:21,898
And then what I try to do over here is that I want to extract

64
00:04:21,994 --> 00:04:25,634
the titles of these articles and store them

65
00:04:25,672 --> 00:04:29,534
somewhere so that I can use for further analysis

66
00:04:29,582 --> 00:04:33,410
later on. So as you know, because sometimes

67
00:04:33,480 --> 00:04:36,786
we want to do our own site projects like we

68
00:04:36,808 --> 00:04:39,190
want to just to practice.

69
00:04:40,090 --> 00:04:43,686
There are some, I would say constraints, say for

70
00:04:43,708 --> 00:04:46,966
example limited to budget, where we might not be

71
00:04:46,988 --> 00:04:50,646
able to afford a data warehouse or storage

72
00:04:50,758 --> 00:04:54,202
space. So I think a good starting

73
00:04:54,256 --> 00:04:57,962
point is to make use of Google sheets. So the Google sheets API is

74
00:04:58,016 --> 00:05:01,678
free and also Google data studio is so free.

75
00:05:01,764 --> 00:05:05,550
So if you are considering of starting

76
00:05:05,620 --> 00:05:09,086
your own data engineering projects, this can be something

77
00:05:09,188 --> 00:05:12,882
you can consider. So what I do over here is

78
00:05:12,936 --> 00:05:16,642
that we do the ETL process from towards data

79
00:05:16,696 --> 00:05:20,622
science website, the medium publication,

80
00:05:20,766 --> 00:05:24,206
and then we do some transformation,

81
00:05:24,398 --> 00:05:28,194
data cleaning, data preparation in Python

82
00:05:28,242 --> 00:05:32,278
script. And also after that we load this

83
00:05:32,364 --> 00:05:35,510
clean data into a Google sheet.

84
00:05:35,930 --> 00:05:39,610
And then subsequently I can use data

85
00:05:39,680 --> 00:05:43,366
studio to connect to this Google sheet and then present it in a dashboard

86
00:05:43,398 --> 00:05:47,770
manner whereby it's friendly for deep dive.

87
00:05:48,270 --> 00:05:51,818
So this is what towards data science looks like in

88
00:05:51,824 --> 00:05:55,034
terms of the web interface. So if you go to just

89
00:05:55,072 --> 00:05:58,606
towardsdatascience.com you can see there

90
00:05:58,628 --> 00:06:02,646
are many, many different articles published. So this is just a screenshot

91
00:06:02,778 --> 00:06:06,130
that I've taken. So what we want to do is to

92
00:06:06,200 --> 00:06:09,358
extract the data related to author

93
00:06:09,454 --> 00:06:12,878
and also recency and post title.

94
00:06:12,974 --> 00:06:16,974
And then subsequently I added a column on the time extracted.

95
00:06:17,022 --> 00:06:20,406
So I know when exactly I have run the script or when

96
00:06:20,428 --> 00:06:23,880
the script is being run in the future if I look back.

97
00:06:24,490 --> 00:06:28,246
So what we do is we need to use

98
00:06:28,348 --> 00:06:31,990
some web scraping tools, some web scraping packages

99
00:06:32,070 --> 00:06:35,754
within Python to extract the data on the website.

100
00:06:35,952 --> 00:06:39,290
And then as you can see, I have imported beautiful

101
00:06:39,360 --> 00:06:42,846
soup and selenium. These are the two main

102
00:06:42,868 --> 00:06:46,494
packages necessary for the web scraping. And then

103
00:06:46,612 --> 00:06:50,314
numpy and pandas are used for transformation.

104
00:06:50,442 --> 00:06:54,094
And then Gspread data frame and gspread

105
00:06:54,142 --> 00:06:57,602
is being used to push the data to

106
00:06:57,656 --> 00:07:01,618
the Google sheet. I would say this talk will be

107
00:07:01,784 --> 00:07:05,154
perhaps maybe more relevant to you if you are

108
00:07:05,192 --> 00:07:09,286
really familiar with Python. So you

109
00:07:09,308 --> 00:07:12,802
know exactly what the syntax

110
00:07:12,866 --> 00:07:16,374
means. So I have also shared the

111
00:07:16,412 --> 00:07:20,214
instructions on how to use the Google API in this slide.

112
00:07:20,262 --> 00:07:23,626
So subsequently if you were to download the

113
00:07:23,648 --> 00:07:27,306
slides and click on the links here, they will direct you

114
00:07:27,408 --> 00:07:31,002
to the Google API GitHub

115
00:07:31,066 --> 00:07:35,866
page. This is where it contains more documentation

116
00:07:35,978 --> 00:07:39,646
on how to exactly get it started. And also to

117
00:07:39,668 --> 00:07:43,282
use gspread and G spread data frame you need to install

118
00:07:43,336 --> 00:07:46,574
the two packages. So you run Pip

119
00:07:46,622 --> 00:07:50,638
install g spread and also pip install gspread data frame.

120
00:07:50,734 --> 00:07:54,354
So I would say the instructions on the page

121
00:07:54,472 --> 00:07:57,766
is pretty detailed and pretty clear.

122
00:07:57,868 --> 00:08:01,174
So if you just follow through you should be able to get the

123
00:08:01,212 --> 00:08:05,398
API working. So there are no charges for using

124
00:08:05,484 --> 00:08:09,722
this API, for using Google sheets API and then over

125
00:08:09,776 --> 00:08:12,986
here. So the authentication part is, I would

126
00:08:13,008 --> 00:08:16,906
say probably maybe the most important part because

127
00:08:17,008 --> 00:08:20,186
if it doesn't work then

128
00:08:20,288 --> 00:08:24,186
you probably wouldn't be able to do the

129
00:08:24,208 --> 00:08:27,678
engineering side of it effectively. Because what we are

130
00:08:27,684 --> 00:08:31,226
trying to do is to automate the process, right? So if you can't

131
00:08:31,258 --> 00:08:35,026
authenticate the API, you wouldn't be able to automate the process and

132
00:08:35,048 --> 00:08:39,300
then the data wouldn't be able to get loaded in.

133
00:08:40,150 --> 00:08:43,874
So next, what you see over here is also

134
00:08:43,992 --> 00:08:47,846
something that I have gotten off the tutorial or

135
00:08:47,868 --> 00:08:52,194
the instructions relating to how to use Google API.

136
00:08:52,322 --> 00:08:56,322
So as you can see, there are some parameters

137
00:08:56,386 --> 00:09:00,534
you have to just change. You can actually create a new Google sheet

138
00:09:00,582 --> 00:09:03,654
first and then you look at the id of the sheet.

139
00:09:03,702 --> 00:09:07,690
So it should be the last part of the URL

140
00:09:08,030 --> 00:09:11,386
and then you can copy that sheet id and

141
00:09:11,408 --> 00:09:15,166
paste it here to replace it that is applicable in your case.

142
00:09:15,268 --> 00:09:18,654
And then over here, after you have set

143
00:09:18,692 --> 00:09:22,842
up the API, there should be, I would say a password,

144
00:09:22,906 --> 00:09:26,850
a credential token file which you have to download to your local

145
00:09:27,000 --> 00:09:31,042
and then it should be a JSON file. And this is where

146
00:09:31,176 --> 00:09:34,766
in order to get the API working, the credentials

147
00:09:34,798 --> 00:09:38,614
will be read from this JSON file. And then currently what

148
00:09:38,652 --> 00:09:42,326
we are using are the spreadsheet API. So we need

149
00:09:42,348 --> 00:09:46,770
to include in our scopes. So after we do the authentication,

150
00:09:46,930 --> 00:09:50,602
then the packages G spread and

151
00:09:50,736 --> 00:09:54,634
G spread data frame will work. So over here what

152
00:09:54,672 --> 00:09:58,774
I'm trying to do is that I have created the sheet

153
00:09:58,822 --> 00:10:02,266
API, I mean sorry, I've created the Google sheet already and

154
00:10:02,288 --> 00:10:05,754
then I am reading in sheet one. So I didn't

155
00:10:05,802 --> 00:10:09,326
really change the name of the sheet. So if you change the name of

156
00:10:09,348 --> 00:10:13,066
the sheet then probably you need to change your worksheet

157
00:10:13,098 --> 00:10:16,398
name instead of sheet one youre just change it to whatever worksheet

158
00:10:16,414 --> 00:10:19,842
name you have. I am getting whatever existing data

159
00:10:19,896 --> 00:10:23,154
there is. So if there is no existing data, it's an

160
00:10:23,192 --> 00:10:26,654
empty spreadsheet then yeah there isn't

161
00:10:26,702 --> 00:10:30,086
anything. But because I am going to do the update on

162
00:10:30,108 --> 00:10:33,522
a daily basis, there are existing data already in the sheet.

163
00:10:33,586 --> 00:10:37,334
So I want to retrieve them first and then what I do next is

164
00:10:37,372 --> 00:10:40,534
I want to append more data into the sheet.

165
00:10:40,662 --> 00:10:43,846
So the next step after I get the data from the sheet,

166
00:10:43,878 --> 00:10:47,626
I will scrape the data from the website. So this

167
00:10:47,648 --> 00:10:51,790
is where web scraping comes in. I access the

168
00:10:51,860 --> 00:10:55,214
site first towards data science and then

169
00:10:55,252 --> 00:10:59,166
you notice I actually have to make use of selenium because there are

170
00:10:59,188 --> 00:11:02,618
certain parts on the website where we have to

171
00:11:02,724 --> 00:11:06,290
basically trigger for it to load even

172
00:11:06,360 --> 00:11:09,614
more. So how do I scrape title

173
00:11:09,662 --> 00:11:13,570
off the website? I will have to find

174
00:11:13,640 --> 00:11:17,746
out text whereby all

175
00:11:17,768 --> 00:11:21,842
these titles or all these data related to the title

176
00:11:21,906 --> 00:11:25,442
or author is being typed to in the HTML

177
00:11:25,506 --> 00:11:28,690
text. So when I click inspect,

178
00:11:28,770 --> 00:11:32,250
then you return me and show me the HTML behind it. And then

179
00:11:32,320 --> 00:11:36,042
I see that it is being typed to the a tag. Okay, so over

180
00:11:36,096 --> 00:11:39,034
here you can see there is the a class whatever.

181
00:11:39,152 --> 00:11:42,430
So what I mentioned just now is so that there are certain

182
00:11:42,500 --> 00:11:46,526
parts of the website that require some interaction in

183
00:11:46,548 --> 00:11:50,126
order for it to load. Furthermore, in this case I have

184
00:11:50,148 --> 00:11:53,226
to click show more to load youre articles

185
00:11:53,338 --> 00:11:56,970
because there are a lot of articles being published in one day. So of

186
00:11:56,980 --> 00:12:00,466
course I'm running this script on a daily basis. I want to make sure

187
00:12:00,568 --> 00:12:04,434
that all the articles that are published in one day is being shown to me.

188
00:12:04,472 --> 00:12:08,214
So I think if you just load the page on its own then

189
00:12:08,332 --> 00:12:12,150
there are maybe, I think only ten articles being shown without

190
00:12:12,220 --> 00:12:15,414
clicking show more. So I

191
00:12:15,452 --> 00:12:19,514
have to in a way make the

192
00:12:19,552 --> 00:12:23,706
script interact with the page to click show more so that

193
00:12:23,808 --> 00:12:26,970
I can get more articles being script.

194
00:12:27,310 --> 00:12:31,054
So I actually set can arbitrary figure which

195
00:12:31,092 --> 00:12:34,542
is 20 arbitrary number

196
00:12:34,676 --> 00:12:37,982
20 where I click on the show more button 20

197
00:12:38,036 --> 00:12:41,470
times. So every time I click show more then probably

198
00:12:41,540 --> 00:12:44,882
I think another ten articles will load and then after

199
00:12:44,936 --> 00:12:48,318
that I will click show more again to load

200
00:12:48,414 --> 00:12:52,642
the next ten articles. So I do this for

201
00:12:52,696 --> 00:12:56,226
about 20 times, but this is in a way not done by

202
00:12:56,248 --> 00:12:59,414
me, manually done by the script. It's automated so it's very

203
00:12:59,452 --> 00:13:02,694
easy. And then in between each click I

204
00:13:02,732 --> 00:13:06,214
actually have a lack of 2 seconds. So you can see over

205
00:13:06,252 --> 00:13:09,542
here there's a time sleep bracket two. So this is what I'm doing

206
00:13:09,596 --> 00:13:12,922
because in a way I also want to make sure that

207
00:13:13,056 --> 00:13:16,842
the page doesn't hang or also like the page doesn't lock me

208
00:13:16,896 --> 00:13:20,586
out. Because sometimes there are some checks behind whereby they

209
00:13:20,608 --> 00:13:24,490
think that a bot, which I mean, in this case it is a bot,

210
00:13:24,650 --> 00:13:27,998
but we want to try to overcome this to

211
00:13:28,004 --> 00:13:32,222
make it look like more human in a sense. So by building in

212
00:13:32,276 --> 00:13:35,826
this kind of time lapse, then it will mimic a more

213
00:13:35,848 --> 00:13:39,122
human behavior and ensure that the script is able to run

214
00:13:39,176 --> 00:13:42,898
true. So next, after I have

215
00:13:42,984 --> 00:13:47,026
what I do over here is actually a loop. So I

216
00:13:47,048 --> 00:13:50,866
will look for the ATAG and then extract

217
00:13:50,898 --> 00:13:54,022
it out, get the text, and then over

218
00:13:54,076 --> 00:13:57,606
here youre can see there is another loop that

219
00:13:57,628 --> 00:14:00,022
I run through again, second for loop.

220
00:14:00,166 --> 00:14:03,674
So let me just show you again what the

221
00:14:03,712 --> 00:14:07,722
output is. After I have script the text type to the a type.

222
00:14:07,856 --> 00:14:11,194
So this is what you will get. After you run

223
00:14:11,232 --> 00:14:15,530
the script, you will get the data returned on the

224
00:14:15,600 --> 00:14:19,502
author and the recent c, and then the title. And then

225
00:14:19,556 --> 00:14:23,840
there is also like some other more random text

226
00:14:24,390 --> 00:14:27,970
that appears and it's not very consistent. So some

227
00:14:28,040 --> 00:14:31,794
of these other data that you see or other text that you see

228
00:14:31,912 --> 00:14:34,850
being returned is tied to, for example,

229
00:14:34,920 --> 00:14:38,662
the caption of a photo, or like

230
00:14:38,796 --> 00:14:42,166
the youre of the photo or image that

231
00:14:42,188 --> 00:14:45,750
was added into the article.

232
00:14:48,430 --> 00:14:52,058
What I realized is that there is a pattern. There is always

233
00:14:52,144 --> 00:14:55,974
like tree blanks

234
00:14:56,102 --> 00:14:59,942
in between each post, or rather

235
00:15:00,016 --> 00:15:03,422
tree blank in between the data between each post.

236
00:15:03,556 --> 00:15:07,550
So what we need is just the first tree elements

237
00:15:08,210 --> 00:15:11,614
of each chunk that is separated by

238
00:15:11,732 --> 00:15:14,994
the tree spaces. So what

239
00:15:15,032 --> 00:15:18,994
I did is I will check whether the

240
00:15:19,032 --> 00:15:22,674
last element is similar, or rather

241
00:15:22,712 --> 00:15:26,670
like the current element is similar to the previous two element.

242
00:15:26,750 --> 00:15:30,214
Okay, so over here you can see equals equals I

243
00:15:30,252 --> 00:15:33,574
plus one, and also to check whether I plus one

244
00:15:33,612 --> 00:15:37,174
equals equals I plus two. So this is what I'm doing. So once I

245
00:15:37,212 --> 00:15:41,274
hit, so as I run through the loop, I will check whether this

246
00:15:41,312 --> 00:15:44,826
element is similar to the next one, which should

247
00:15:44,848 --> 00:15:48,170
be a blank space, and then it's similar to the one below it

248
00:15:48,240 --> 00:15:51,594
as well. So if all three are similar, that means

249
00:15:51,632 --> 00:15:55,134
they are actually off spaces. Then that is the

250
00:15:55,252 --> 00:15:58,702
row, the element on a position that I want.

251
00:15:58,836 --> 00:16:02,362
Okay, so I store all this element

252
00:16:02,426 --> 00:16:06,594
position that I want, and then I do another loop to

253
00:16:06,632 --> 00:16:09,778
extract the text type to this element position.

254
00:16:09,944 --> 00:16:12,990
After that I convert it into a data frame,

255
00:16:13,070 --> 00:16:16,534
and then I will also add in a

256
00:16:16,572 --> 00:16:20,646
new column, which is the time now as an

257
00:16:20,668 --> 00:16:23,814
indication of just to let me know when this

258
00:16:23,852 --> 00:16:27,222
job is being run. So, based on the later

259
00:16:27,276 --> 00:16:30,886
data that I have script, I will append it to existing

260
00:16:30,918 --> 00:16:34,906
data in the Google sheet that I have. And then I will

261
00:16:35,008 --> 00:16:38,086
check whether there are duplicates or not based on author

262
00:16:38,118 --> 00:16:42,346
and post title. Because since I have arbitrary chosen

263
00:16:42,458 --> 00:16:45,950
to click show more 20 times. Right. There could be

264
00:16:46,020 --> 00:16:49,646
cases whereby the data is already scraped in the

265
00:16:49,668 --> 00:16:52,866
previous day. So just wanted to make sure

266
00:16:52,888 --> 00:16:56,722
that there are no overlaps, there are no duplicates. So I will

267
00:16:56,856 --> 00:17:00,718
drop the duplicates using again like pandas.

268
00:17:00,814 --> 00:17:04,798
And then I will also fill missing values with space

269
00:17:04,904 --> 00:17:08,774
if there is. And then I

270
00:17:08,812 --> 00:17:12,530
will drop the index whereby it's

271
00:17:12,690 --> 00:17:16,710
missing. Okay? And then in certain cases

272
00:17:19,790 --> 00:17:23,350
the first row after it does the sorting is empty.

273
00:17:23,510 --> 00:17:27,862
So I will drop it and then after that I will update

274
00:17:27,926 --> 00:17:31,998
my google sheet. So this is what the last line of

275
00:17:32,004 --> 00:17:35,246
code w dot update is doing. Okay,

276
00:17:35,428 --> 00:17:39,450
so this is the output based on the quote

277
00:17:39,530 --> 00:17:43,106
just now. You see where I am actually looping through

278
00:17:43,208 --> 00:17:46,754
the element position and then I

279
00:17:46,792 --> 00:17:49,380
get the text type to that position.

280
00:17:50,150 --> 00:17:54,078
A little bit of note is that there are also few cases

281
00:17:54,174 --> 00:17:58,006
whereby the title is not

282
00:17:58,108 --> 00:18:01,926
in the third line in the chunk that

283
00:18:01,948 --> 00:18:05,254
is being scraped off in the different for each post.

284
00:18:05,372 --> 00:18:08,886
Okay, so there could be certain cases whereby the data is

285
00:18:08,908 --> 00:18:12,854
not so clean, where it actually extracts the image

286
00:18:12,982 --> 00:18:16,826
caption in

287
00:18:16,848 --> 00:18:20,410
an earlier position and then the title appear later.

288
00:18:20,560 --> 00:18:23,754
So in those cases, those caption

289
00:18:23,882 --> 00:18:27,274
actually in general have less than 20 characters.

290
00:18:27,322 --> 00:18:30,800
So this is where I built in the if else over here.

291
00:18:31,330 --> 00:18:35,362
In the case, if the number of characters is less than 20,

292
00:18:35,496 --> 00:18:39,618
that means that the title should be in the seven

293
00:18:39,704 --> 00:18:43,426
position, okay? If not, yeah, it should be

294
00:18:43,608 --> 00:18:46,742
earlier and then this is how I identify it.

295
00:18:46,876 --> 00:18:50,502
So in order to

296
00:18:50,556 --> 00:18:54,438
automate the entire process, now that I have done the

297
00:18:54,604 --> 00:18:57,874
Python script to do the web scraping and updating

298
00:18:57,922 --> 00:19:01,558
of Google Sheet, I want to make it run on a daily basis.

299
00:19:01,654 --> 00:19:05,114
Okay? So I have to schedule it, but if not,

300
00:19:05,312 --> 00:19:09,114
it's actually very easy. Now that we have the Python script, you can just

301
00:19:09,152 --> 00:19:15,694
open the command prompt, the anal over

302
00:19:15,732 --> 00:19:19,722
here. What I have is a virtual environment. So I activate my virtual environment

303
00:19:19,786 --> 00:19:23,166
first and then I run my Python script. So else you

304
00:19:23,188 --> 00:19:26,538
want, you can also set up a Chrome job locally.

305
00:19:26,634 --> 00:19:29,758
Okay, so for example, in my case I'm using Windows

306
00:19:29,854 --> 00:19:33,186
and then what I can do is set it

307
00:19:33,208 --> 00:19:37,126
up using the Windows task scheduler. I look at the

308
00:19:37,308 --> 00:19:41,014
instructions over here, right. What you need to do is you can

309
00:19:41,052 --> 00:19:44,280
create basic tasks and then,

310
00:19:44,810 --> 00:19:48,530
yeah, this is what you will see when you launch

311
00:19:48,610 --> 00:19:52,186
administrative tools, create basic tasks and

312
00:19:52,208 --> 00:19:55,930
then there are certain boxes which you

313
00:19:56,000 --> 00:19:59,942
have to fill in. So you locate where your python exe

314
00:20:00,006 --> 00:20:03,760
is. So you paste the path over here and then

315
00:20:04,210 --> 00:20:07,802
just also identify where your script is being located

316
00:20:07,866 --> 00:20:10,926
at and then paste the location of your script in

317
00:20:10,948 --> 00:20:14,874
the arguments. So the rest

318
00:20:14,932 --> 00:20:18,386
of the parts that you can fix you can set up

319
00:20:18,408 --> 00:20:22,354
in the task schedule is pretty straightforward, like when you

320
00:20:22,392 --> 00:20:26,498
want to run it, like what time, et cetera. So it's pretty straightforward,

321
00:20:26,674 --> 00:20:30,098
which also if you click on the stack overthrow.

322
00:20:30,194 --> 00:20:34,838
Sorry. If youre search this link,

323
00:20:34,924 --> 00:20:37,698
you should be able to see the entire instructions.

324
00:20:37,874 --> 00:20:40,630
So this is what we will get.

325
00:20:40,700 --> 00:20:44,570
Okay. And then of course I think there are still certain cases whereby

326
00:20:45,070 --> 00:20:48,726
based on the logic that I've come up with, for example the checking

327
00:20:48,758 --> 00:20:52,058
of the number of characters, there could still be outliers here

328
00:20:52,064 --> 00:20:55,918
and there. So in those cases they might still need

329
00:20:56,004 --> 00:20:59,262
some manual intervention. But on the whole

330
00:20:59,316 --> 00:21:03,282
I would say this is 90% or even

331
00:21:03,336 --> 00:21:06,180
like 95% cleaned data.

332
00:21:06,630 --> 00:21:10,290
So from this Google sheet we can actually

333
00:21:10,360 --> 00:21:13,842
use Google data studio to connect to the Google data

334
00:21:13,896 --> 00:21:18,498
sheet and then create nicer looking interface

335
00:21:18,594 --> 00:21:22,214
for user to play around or to explore the data.

336
00:21:22,332 --> 00:21:25,990
Okay, so over here we can see

337
00:21:26,060 --> 00:21:29,546
that what I have done is actually just collecting the

338
00:21:29,568 --> 00:21:33,354
data over a six week period between 21st November 20,

339
00:21:33,392 --> 00:21:37,066
22nd January 2021. And yeah,

340
00:21:37,168 --> 00:21:43,614
there are so many articles being published in a day to

341
00:21:43,652 --> 00:21:47,198
me is pretty crazy. So maybe just give me a

342
00:21:47,204 --> 00:21:50,880
while, I can load the data studio page.

343
00:21:51,810 --> 00:21:55,470
So this is what we have in the Google sheets.

344
00:21:55,630 --> 00:21:59,182
What's interesting is that there are multiple topics,

345
00:21:59,246 --> 00:22:03,410
I mean multiple articles on pretty similar topics. And then

346
00:22:03,560 --> 00:22:07,014
from here, if youre interested, you can make use of the data to

347
00:22:07,052 --> 00:22:10,342
do some text mining to identify what

348
00:22:10,396 --> 00:22:14,214
are some of the more popular domains within

349
00:22:14,412 --> 00:22:18,230
data science at the moment. Whether is it relating

350
00:22:18,310 --> 00:22:22,138
to certain kinds of tools or

351
00:22:22,304 --> 00:22:25,610
algorithms, you can search for the keywords within

352
00:22:25,680 --> 00:22:29,242
these articles. Yeah, as you can see that there are really

353
00:22:29,296 --> 00:22:33,802
quite a number of articles that are being published over six

354
00:22:33,856 --> 00:22:35,580
weeks, over 1000.

355
00:22:37,010 --> 00:22:40,590
It's like 1680

356
00:22:40,930 --> 00:22:44,766
over a six week period. Okay, so now I'll

357
00:22:44,798 --> 00:22:48,622
be talking about the second use case on creating

358
00:22:48,686 --> 00:22:51,970
a blended face based on multiple images.

359
00:22:52,310 --> 00:22:55,990
So in this case also we are getting data,

360
00:22:56,140 --> 00:23:00,022
in this case image of the Internet. And then I

361
00:23:00,076 --> 00:23:03,910
am saving it into my local drive, my local folder.

362
00:23:05,450 --> 00:23:09,194
What I did after that was to make use

363
00:23:09,232 --> 00:23:13,206
of open source library that was hosted

364
00:23:13,238 --> 00:23:16,314
on GitHub to create a blended face.

365
00:23:16,512 --> 00:23:20,234
So in this case, the process is a little

366
00:23:20,272 --> 00:23:24,306
bit different in the sense that we will do extract

367
00:23:24,358 --> 00:23:27,674
and loading first before we do the transformation.

368
00:23:27,802 --> 00:23:32,080
So actually there is no one single rule when it comes to

369
00:23:32,530 --> 00:23:36,350
data engineering, like it can be also

370
00:23:36,500 --> 00:23:39,714
done, not necessarily that it has to be

371
00:23:39,832 --> 00:23:42,622
ETL, like abstract, transform, load.

372
00:23:42,686 --> 00:23:46,006
It can be also abstract and load first before doing the

373
00:23:46,028 --> 00:23:49,554
transformation. So in some organizations

374
00:23:49,602 --> 00:23:53,750
they actually have data lakes. So this is where they have data

375
00:23:53,820 --> 00:23:57,590
from different places, and then they just pull it all into

376
00:23:57,660 --> 00:24:01,082
one single location. First they call it the data

377
00:24:01,136 --> 00:24:04,842
lake, and then after that, so it can be clean

378
00:24:04,896 --> 00:24:08,810
or unclean data altogether in this data lake, and then they

379
00:24:08,880 --> 00:24:11,040
do the transformation after that.

380
00:24:12,050 --> 00:24:15,774
But based on my work experiences so far,

381
00:24:15,972 --> 00:24:19,678
I typically do ETL most of

382
00:24:19,684 --> 00:24:23,134
the time, which is extract, transform, then load

383
00:24:23,182 --> 00:24:26,830
the data. So again, it depends on your organization,

384
00:24:26,990 --> 00:24:30,562
your structure, how actually you

385
00:24:30,616 --> 00:24:33,140
want to store the data.

386
00:24:33,510 --> 00:24:36,786
So in this case, what I'm

387
00:24:36,818 --> 00:24:40,790
trying to do is to create a blended face,

388
00:24:40,860 --> 00:24:44,374
blended average face based on the

389
00:24:44,412 --> 00:24:47,938
candidates of different political parties in

390
00:24:47,964 --> 00:24:51,706
Singapore. So last year in 2020,

391
00:24:51,808 --> 00:24:55,594
there was a general election, and then the street science, which is

392
00:24:55,632 --> 00:24:58,102
a news platform, a news outlet,

393
00:24:58,246 --> 00:25:02,140
they have designed a website to basically

394
00:25:02,850 --> 00:25:06,554
let people know who the candidates are from the different political

395
00:25:06,602 --> 00:25:09,898
parties and then where they are contesting.

396
00:25:09,994 --> 00:25:13,474
So they have uploaded images of all these

397
00:25:13,512 --> 00:25:17,262
political candidates and then they are publicly

398
00:25:17,326 --> 00:25:21,154
accessible. So what I did was to scrape the

399
00:25:21,192 --> 00:25:24,930
image of all these political parties

400
00:25:25,430 --> 00:25:28,854
off this website into my local drive.

401
00:25:29,052 --> 00:25:32,658
So again, I made use of the web scripting tools,

402
00:25:32,754 --> 00:25:36,854
beautiful soup and selenium. And then also I

403
00:25:36,892 --> 00:25:40,326
imported Wii, which is for regular expressions,

404
00:25:40,518 --> 00:25:44,150
because I need to extract the format of the image

405
00:25:44,230 --> 00:25:47,478
of later based on the URL.

406
00:25:47,574 --> 00:25:51,820
So subsequently also, I mean, there is

407
00:25:52,270 --> 00:25:56,414
this library that I use URL request or so for accessing the

408
00:25:56,452 --> 00:25:59,822
website. And what you see over

409
00:25:59,876 --> 00:26:04,254
here is the same thing as the

410
00:26:04,292 --> 00:26:07,922
previous use case. Okay, so this page is also

411
00:26:07,976 --> 00:26:12,146
interactive in the sense that I have to actually scroll down

412
00:26:12,248 --> 00:26:16,246
in order for the page to load entirely. So what

413
00:26:16,348 --> 00:26:19,686
we need to do when we use Python to access

414
00:26:19,788 --> 00:26:22,600
is to include more arguments in the options.

415
00:26:23,050 --> 00:26:26,406
So when we use the webdriver to

416
00:26:26,428 --> 00:26:30,026
access the page, it will allow the page to

417
00:26:30,048 --> 00:26:33,414
load entirely. So what I added over here is headless

418
00:26:33,462 --> 00:26:37,082
argument and a disabled gpu. So this, to overcome this,

419
00:26:37,216 --> 00:26:40,618
and then also similarly like using beautiful

420
00:26:40,704 --> 00:26:43,886
soup, I look for the text that all these images are

421
00:26:43,908 --> 00:26:47,086
tied to, which in this case is IMG, and then

422
00:26:47,188 --> 00:26:50,190
I run a loop to script the images. Also,

423
00:26:50,340 --> 00:26:53,746
if you can see, there's a bit of trial and error here where I need

424
00:26:53,768 --> 00:26:57,780
to identify the location of the first image that I'm interested in.

425
00:26:58,150 --> 00:27:02,206
Because when extract the information typed

426
00:27:02,238 --> 00:27:05,474
to the IMG text, there are other information,

427
00:27:05,592 --> 00:27:09,318
other images that I am not interested in. For example,

428
00:27:09,404 --> 00:27:13,286
it can be like the map. So after a bit

429
00:27:13,308 --> 00:27:16,454
of trial, I identified that the first image that I want is

430
00:27:16,492 --> 00:27:20,634
in the very first position of

431
00:27:20,672 --> 00:27:24,602
the IMG tag. And then there are x

432
00:27:24,656 --> 00:27:28,282
number of images I need to script, right? So I identify like okay, the last

433
00:27:28,416 --> 00:27:31,246
element is in the position three, six, nine.

434
00:27:31,348 --> 00:27:35,454
So I run the loop and then I

435
00:27:35,492 --> 00:27:39,280
basically filter out things that not necessary. Like you can see

436
00:27:40,290 --> 00:27:43,822
there is also additional subtext

437
00:27:43,886 --> 00:27:47,666
like alts that is blank. And then in

438
00:27:47,688 --> 00:27:52,066
this case, this is where I will skip those, because they

439
00:27:52,088 --> 00:27:55,234
shouldn't be blank. And then I get the

440
00:27:55,272 --> 00:27:58,722
source of the URL, the picture, the image,

441
00:27:58,786 --> 00:28:02,278
and then I scrape it off with the name

442
00:28:02,364 --> 00:28:06,486
type to the image. Okay, so if

443
00:28:06,508 --> 00:28:10,046
you can see, I also have included

444
00:28:10,178 --> 00:28:14,294
this minus four colon, which is to extract the format

445
00:28:14,342 --> 00:28:18,220
of the image like it can be JPG or

446
00:28:18,590 --> 00:28:22,814
PNG. So I save it accordingly into my local drive.

447
00:28:22,932 --> 00:28:26,074
And then after all the image is being script

448
00:28:26,122 --> 00:28:30,270
successfully. Then I make use of this open source library

449
00:28:30,610 --> 00:28:34,126
that was created by this person.

450
00:28:34,308 --> 00:28:37,886
He hosted it on GitHub, which I'm very thankful

451
00:28:37,918 --> 00:28:41,426
for because it makes things much easier. So I think this

452
00:28:41,448 --> 00:28:44,820
is also like the wonders of open source, right? You can do

453
00:28:45,130 --> 00:28:48,646
a lot of things much quicker, much easier based on

454
00:28:48,668 --> 00:28:51,190
what other people have developed or created.

455
00:28:53,130 --> 00:28:55,750
If you go into this URL,

456
00:28:56,410 --> 00:28:59,846
then you will see the scripts that

457
00:28:59,868 --> 00:29:02,490
are developed to create a blended image,

458
00:29:02,910 --> 00:29:06,154
average face. So what you need is just to

459
00:29:06,192 --> 00:29:09,494
change the file path that pine

460
00:29:09,542 --> 00:29:13,854
to those images where you have saved it locally. And then

461
00:29:13,972 --> 00:29:17,838
you run the script, you run the package, and yeah,

462
00:29:17,924 --> 00:29:22,526
the instructions is actually all on

463
00:29:22,548 --> 00:29:26,100
GitHub for this library. So if you

464
00:29:27,030 --> 00:29:30,322
go through the instructions on how to

465
00:29:30,376 --> 00:29:33,586
download it and how to run it, you'll be able to get the

466
00:29:33,608 --> 00:29:36,822
output very quickly as well. So this is what I got

467
00:29:36,956 --> 00:29:40,246
after running the script. And yeah,

468
00:29:40,348 --> 00:29:44,402
for each political party, if at least ten candidates, I created

469
00:29:44,546 --> 00:29:48,190
one phase for each of them, which is the average phase.

470
00:29:48,290 --> 00:29:51,882
Okay, so just wrapping up in terms

471
00:29:51,936 --> 00:29:55,274
of data engineering projects, first I guess we have to

472
00:29:55,312 --> 00:29:58,906
start off with thinking what analysis you or your

473
00:29:58,928 --> 00:30:02,414
data science team want to do here. From there we will think

474
00:30:02,452 --> 00:30:06,602
about what data is necessary, and then we will write a Python script to automate

475
00:30:06,666 --> 00:30:09,360
ETL or ERT process.

476
00:30:09,810 --> 00:30:13,294
And yeah, this will allow us to

477
00:30:13,412 --> 00:30:17,266
generate insights much quicker based on the data that we have

478
00:30:17,288 --> 00:30:20,766
collected so far. Okay, so I actually maintain

479
00:30:20,798 --> 00:30:24,402
a data science block called data double confirm. And over here

480
00:30:24,456 --> 00:30:28,006
I cover a lot of other things related to

481
00:30:28,108 --> 00:30:32,422
various data science tasks. So web scraping is

482
00:30:32,556 --> 00:30:36,002
one big component related to data collection,

483
00:30:36,146 --> 00:30:40,074
because I think there is basically a lot of,

484
00:30:40,272 --> 00:30:43,734
I would say, development in the algorithms

485
00:30:43,862 --> 00:30:46,778
realm and also in deep learning.

486
00:30:46,864 --> 00:30:50,414
So sometimes what we need is the data, which is

487
00:30:50,452 --> 00:30:54,062
very important, which is why I'm still talking about data

488
00:30:54,116 --> 00:30:57,614
engineering. And then from there, with the data we have, we can

489
00:30:57,652 --> 00:31:01,546
perform data visualization or statistical analysis

490
00:31:01,658 --> 00:31:06,334
that can give us insights and help

491
00:31:06,372 --> 00:31:09,854
us make more informed decisions. So this

492
00:31:09,892 --> 00:31:13,146
is the URL of my data science

493
00:31:13,178 --> 00:31:16,466
blog, and also feel free to connect with me via

494
00:31:16,498 --> 00:31:18,500
LinkedIn or Twitter. Thank you.

