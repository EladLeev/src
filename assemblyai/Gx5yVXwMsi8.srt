1
00:00:20,010 --> 00:00:23,774
I'm really excited for this opportunity to share what's possible for

2
00:00:23,812 --> 00:00:26,920
robotics with AWS us. Let's get started.

3
00:00:27,610 --> 00:00:31,202
The use of robots continue to accelerate as industries

4
00:00:31,266 --> 00:00:35,058
and organizations realize the role of automation

5
00:00:35,154 --> 00:00:38,710
in improving productivity, gaining business continuity,

6
00:00:38,790 --> 00:00:41,610
and ensuring flexibility and adaptability.

7
00:00:42,110 --> 00:00:45,750
We see increased adoptions across all major sectors,

8
00:00:45,830 --> 00:00:50,354
including industrial services, consumer agricultures

9
00:00:50,422 --> 00:00:54,734
and more. The number of robots is estimated to grow to 20 million

10
00:00:54,852 --> 00:00:58,494
by 2030. People have been

11
00:00:58,532 --> 00:01:01,882
creating devices and machines to perform mundane

12
00:01:01,946 --> 00:01:04,674
tasks for hundreds of years. However,

13
00:01:04,872 --> 00:01:07,890
the ability to create a device that can sense,

14
00:01:08,040 --> 00:01:11,070
compute and act with a degree of autonomy

15
00:01:11,150 --> 00:01:14,674
is relatively new. The first generations of

16
00:01:14,712 --> 00:01:18,454
robots, such as the robotic arm, could perform simple,

17
00:01:18,572 --> 00:01:22,386
systematic tasks. They were largely pre programmed

18
00:01:22,418 --> 00:01:25,990
or human directed. Since those early years

19
00:01:26,060 --> 00:01:30,202
in robotics, there have been remarkable advancements in

20
00:01:30,256 --> 00:01:33,994
computing, sensor technology and machine learning

21
00:01:34,112 --> 00:01:37,930
that has enabled a shift towards next generation robots.

22
00:01:38,350 --> 00:01:42,094
Robots with a wide range of autonomy, powered by the

23
00:01:42,132 --> 00:01:46,074
advanced data analytics and natural human interfaces

24
00:01:46,202 --> 00:01:49,402
that can perform advanced tasks in collaboration

25
00:01:49,466 --> 00:01:53,282
with humans. We believe that

26
00:01:53,336 --> 00:01:56,606
connecting robots to the cloud and cross edges

27
00:01:56,718 --> 00:02:00,750
is powering the next evolution in robotics. With AWS,

28
00:02:00,830 --> 00:02:05,290
connected robots can capture, store and aggregate limitless

29
00:02:05,390 --> 00:02:09,618
amounts of data to train and develop new functionality,

30
00:02:09,794 --> 00:02:13,682
optimize and drive efficiency. Connected robots

31
00:02:13,746 --> 00:02:17,094
can update themselves to ensure reliability and

32
00:02:17,132 --> 00:02:21,014
reduce downtime. Operators and technicians can remotely

33
00:02:21,062 --> 00:02:25,082
connect and interact. So why do customers choose

34
00:02:25,136 --> 00:02:28,726
AWS to build, deploy and manage their robots?

35
00:02:28,918 --> 00:02:32,874
First, AWS helps you build mature solutions

36
00:02:32,922 --> 00:02:36,810
from day one. With AWS, you can become agile

37
00:02:36,890 --> 00:02:40,922
in your experimentation and innovation. With AWS,

38
00:02:40,986 --> 00:02:44,702
you can deliver a differentiated value proposition by collecting

39
00:02:44,766 --> 00:02:48,210
and harness the power of data to drive innovation.

40
00:02:48,790 --> 00:02:51,534
AWS provides unmatched scalability,

41
00:02:51,662 --> 00:02:55,306
continuity and reliability. The elasticity

42
00:02:55,358 --> 00:02:59,362
of the cloud means more compute power to provide increasing

43
00:02:59,426 --> 00:03:02,470
levels of autonomy and facilitate orchestration.

44
00:03:03,050 --> 00:03:06,406
AWS enable you to extract the full value of

45
00:03:06,428 --> 00:03:10,582
robots automation. Whether you are looking to reduce downtime

46
00:03:10,646 --> 00:03:12,780
or implement a new business case,

47
00:03:13,630 --> 00:03:18,090
let's look at how customers are using AWS for robots.

48
00:03:18,910 --> 00:03:22,618
Robot builders use AWS to build intelligent

49
00:03:22,714 --> 00:03:26,442
connected robots faster. You are able to train robots

50
00:03:26,506 --> 00:03:30,490
to execute complex tasks, collect telemetry data securely,

51
00:03:30,650 --> 00:03:34,574
and continuously improve your robotics devices and

52
00:03:34,612 --> 00:03:38,258
generations software. In addition, you are able to test the

53
00:03:38,264 --> 00:03:42,638
new capabilities using simulation across a variety of scenarios

54
00:03:42,814 --> 00:03:47,106
as well as accelerate development. By using pretrained machine learnings

55
00:03:47,138 --> 00:03:50,774
and artificial intelligence model, you are able

56
00:03:50,812 --> 00:03:54,742
to build for scalability and operating robots solution at

57
00:03:54,796 --> 00:03:58,390
scale as well as maintaining the solution remotely.

58
00:03:58,470 --> 00:04:02,566
With cloudbased software provisioning and over the air update,

59
00:04:02,758 --> 00:04:06,982
you are able to connect and monitor homogeneous robot fleets

60
00:04:07,046 --> 00:04:11,194
with vendor integrations and the ability to build applications

61
00:04:11,242 --> 00:04:13,790
that help optimize traffic flow.

62
00:04:15,330 --> 00:04:19,102
We have a portfolio of services that can help support

63
00:04:19,236 --> 00:04:22,786
all of these use tasks, whether you want to connect and

64
00:04:22,808 --> 00:04:26,690
manage your robotics application or to enable new workflows.

65
00:04:27,030 --> 00:04:30,962
At AWS, we always start with connectivity and

66
00:04:31,016 --> 00:04:34,290
collecting data from the devices to enable many

67
00:04:34,360 --> 00:04:38,354
other capabilities and use cases. You can leverage AWS

68
00:04:38,402 --> 00:04:42,210
services to add capabilities such as over the air update,

69
00:04:42,370 --> 00:04:46,098
predictive preventive maintenance generations,

70
00:04:46,194 --> 00:04:49,370
or machine learning interfaces at the edge

71
00:04:49,870 --> 00:04:54,010
if you are operating robots at a scale and are looking

72
00:04:54,080 --> 00:04:57,514
for a single view glass, we have services that can

73
00:04:57,552 --> 00:05:01,198
help you monitor desperate robots fleets more

74
00:05:01,284 --> 00:05:05,070
efficiently. Interfaces of robotic things where

75
00:05:05,140 --> 00:05:08,506
intelligent devices can monitor events,

76
00:05:08,698 --> 00:05:11,710
fuse sensor data from variety of sources,

77
00:05:12,130 --> 00:05:16,066
use local and distributed intelligence to determine the best course of

78
00:05:16,088 --> 00:05:19,966
action and then act to control or manipulate objects

79
00:05:19,998 --> 00:05:24,466
in the real world. It have four aspects sense connect

80
00:05:24,648 --> 00:05:27,926
learn activate the sense is

81
00:05:27,948 --> 00:05:32,146
about collect and process data streams from sensors,

82
00:05:32,258 --> 00:05:35,474
lighters, cameras and other sensor

83
00:05:35,522 --> 00:05:39,606
in the environment and store the data in Amazon SS three or Amazon Kinesis

84
00:05:39,638 --> 00:05:43,018
data stream. Connect is about sending the data to

85
00:05:43,024 --> 00:05:47,734
the cloud or even connect and interpret with other robotics

86
00:05:47,862 --> 00:05:52,110
devices, system and equipments learn

87
00:05:52,180 --> 00:05:55,966
is about using the data collected from the robots and run

88
00:05:56,068 --> 00:05:59,946
and train machine learning models.

89
00:06:00,058 --> 00:06:03,486
Then deploy the model in the robot

90
00:06:03,598 --> 00:06:07,486
where robot can execute complex function and workloads

91
00:06:07,598 --> 00:06:11,998
and make decisions. Accoutate is about interacting

92
00:06:12,014 --> 00:06:15,950
the environment in a safe manner, interact with human with other robots

93
00:06:16,030 --> 00:06:19,782
and other automation safely. Let's look

94
00:06:19,836 --> 00:06:22,230
at common use cases architecture.

95
00:06:24,330 --> 00:06:28,246
In this architecture, the robot is collecting sensor data

96
00:06:28,348 --> 00:06:32,754
and log data and transfers the data to the cloud using AWS

97
00:06:32,802 --> 00:06:35,926
IoT core where the data could be routed

98
00:06:35,958 --> 00:06:39,302
to be stored in varieties of database systems

99
00:06:39,446 --> 00:06:43,082
and also can be stored to build analytics data lake

100
00:06:43,226 --> 00:06:46,714
which could be consumed by visualization tools

101
00:06:46,762 --> 00:06:50,270
in order to provide reports and life dashboard.

102
00:06:50,850 --> 00:06:54,546
In the same time, IoT core providing a function which

103
00:06:54,568 --> 00:06:58,878
is called device shadow which is a mainly adjacent document

104
00:06:58,974 --> 00:07:02,702
representation of the actual and the desired

105
00:07:02,766 --> 00:07:06,242
state of the robots. So an application can

106
00:07:06,296 --> 00:07:09,654
interact with the robots through a shadow where the application

107
00:07:09,772 --> 00:07:13,686
sets the desired state for the robot and when the

108
00:07:13,708 --> 00:07:17,566
robot is connected to the Internet, it will connect to the IoT core,

109
00:07:17,698 --> 00:07:21,978
fetch the desired state from the shadow and start act

110
00:07:22,064 --> 00:07:25,180
to present this desired state.

111
00:07:25,550 --> 00:07:28,378
Also, when the robot state itself is a change,

112
00:07:28,464 --> 00:07:31,966
it will connect to the IoT core and update the

113
00:07:31,988 --> 00:07:34,320
shadow. Say this is my current state.

114
00:07:35,250 --> 00:07:38,746
AWS IoT core lets you connect billions

115
00:07:38,858 --> 00:07:42,534
of IoT devices and robots and route

116
00:07:42,602 --> 00:07:45,982
trillions of messages to AWS services without managing

117
00:07:46,046 --> 00:07:50,050
any infrastructure. IoT core is a managed cloud

118
00:07:50,120 --> 00:07:53,214
platform that lets connected devices

119
00:07:53,342 --> 00:07:57,350
easily and securely interact with a cloud

120
00:07:57,420 --> 00:07:59,960
application and with other services.

121
00:08:01,370 --> 00:08:05,154
There are different communication protocols

122
00:08:05,202 --> 00:08:08,214
including MQTT, HTTPs,

123
00:08:08,342 --> 00:08:14,598
MQTT over Websocket and LoRaWAN AWS.

124
00:08:14,694 --> 00:08:18,522
IoT core also secure device connections and

125
00:08:18,576 --> 00:08:22,318
data with mutual authentication and end to end

126
00:08:22,404 --> 00:08:25,498
encryption. IoT core can filter,

127
00:08:25,594 --> 00:08:29,322
transform and act upon devices data on the fly

128
00:08:29,466 --> 00:08:32,190
based on the business defined rules.

129
00:08:32,950 --> 00:08:36,078
Another architecture is about software development and managed

130
00:08:36,094 --> 00:08:40,318
application architecture where you have two fleets of robots,

131
00:08:40,414 --> 00:08:43,714
fleet a and fleet b, and you want to

132
00:08:43,752 --> 00:08:47,506
deploy different version of your application like an

133
00:08:47,528 --> 00:08:51,234
a b testing. For example, you can use IoT core

134
00:08:51,282 --> 00:08:54,870
to create things group. In this case we create

135
00:08:54,940 --> 00:08:58,386
thing group a and things group b and ensures that

136
00:08:58,428 --> 00:09:02,042
all robots in fleet a is under things

137
00:09:02,096 --> 00:09:05,162
group a and all robots in fleet b is

138
00:09:05,216 --> 00:09:08,934
under things group b. Then you can use IoT green grass

139
00:09:08,982 --> 00:09:13,294
to deploy software a to group a

140
00:09:13,412 --> 00:09:17,086
and software group b to group b,

141
00:09:17,268 --> 00:09:20,346
which in turn will need to be deployed

142
00:09:20,378 --> 00:09:22,880
to only fleet a and fleet b.

143
00:09:23,430 --> 00:09:27,086
So IoT Greengrass is an open source edge runtime

144
00:09:27,198 --> 00:09:29,890
and cloud services for building,

145
00:09:30,040 --> 00:09:34,382
deploying and managing device software. Iot greengrass

146
00:09:34,446 --> 00:09:37,894
make it easy to bring intelligence to edge devices such

147
00:09:37,932 --> 00:09:42,066
AWS, anomaly detection and powering autonomous devices.

148
00:09:42,258 --> 00:09:46,166
You can deploy new or legacy app across fleets using

149
00:09:46,348 --> 00:09:49,962
Java, bison node js or even running

150
00:09:50,016 --> 00:09:53,670
a container image. IoT greengrass can collect,

151
00:09:53,750 --> 00:09:57,110
aggregate, filter and send data locally,

152
00:09:57,270 --> 00:10:01,290
also manage and control what data go to the cloud for optimized

153
00:10:01,370 --> 00:10:03,230
analytics and storage.

154
00:10:04,290 --> 00:10:07,534
Another architecture is where

155
00:10:07,572 --> 00:10:11,342
you want to run machine learning at the edge and this is also

156
00:10:11,396 --> 00:10:14,626
using IoT greengrass as it make it easily to

157
00:10:14,648 --> 00:10:18,770
perform machine learning inference locally on robots

158
00:10:19,190 --> 00:10:22,926
using models that are created and trained

159
00:10:23,038 --> 00:10:26,786
and optimized in the cloud. IoT green grass gives

160
00:10:26,808 --> 00:10:29,986
you the flexibility to use machine learning trained in

161
00:10:30,008 --> 00:10:33,462
Amazon Sagemaker or even to bring your

162
00:10:33,516 --> 00:10:37,670
own trained model and save it. In Amazon SSV,

163
00:10:38,110 --> 00:10:41,290
you can use machine learning model that are built,

164
00:10:41,440 --> 00:10:45,590
trained and optimized in the cloud and run interfaces

165
00:10:45,750 --> 00:10:49,770
on robots. For example, you can build a predictive

166
00:10:50,370 --> 00:10:53,946
model in sagemaker for sense detection

167
00:10:53,978 --> 00:10:57,866
analysis, optimize it to run on any camera

168
00:10:58,058 --> 00:11:01,934
and then deploy it to predict suspicious activity and

169
00:11:01,972 --> 00:11:05,842
send an alert data gathered from

170
00:11:05,896 --> 00:11:09,582
the robots itself running on it. Green grass

171
00:11:09,646 --> 00:11:13,362
can be sent back to stagemaker where it can be

172
00:11:13,416 --> 00:11:17,026
tagged so it can be used continuously to improve

173
00:11:17,058 --> 00:11:19,640
the quality of the machine learning model.

174
00:11:20,570 --> 00:11:24,566
In this architecture, the user want to stream videos

175
00:11:24,668 --> 00:11:28,200
from robots and also have a playback through a mobile application.

176
00:11:29,050 --> 00:11:32,890
Amazon can use things video stream make IoT easy

177
00:11:32,960 --> 00:11:36,954
to securely stream videos from connected robots to AWS for

178
00:11:36,992 --> 00:11:41,390
analytics, machine learning, playback and other processing.

179
00:11:42,130 --> 00:11:45,642
Kinesis video stream automatically provision and scale

180
00:11:45,706 --> 00:11:49,838
all infrastructure needed to ingest streaming video data

181
00:11:49,924 --> 00:11:53,070
from millions of devices. IoT durably

182
00:11:53,150 --> 00:11:56,834
stores, encrypt and index videos data in your

183
00:11:56,872 --> 00:12:00,386
stream and allow you to access your data through an easy

184
00:12:00,488 --> 00:12:04,094
to use API. Kinesis video streams enable

185
00:12:04,142 --> 00:12:08,098
you to play back video for live or on demand viewing.

186
00:12:08,274 --> 00:12:12,466
Quickly build applications that take advantage of computer vision

187
00:12:12,578 --> 00:12:16,274
and video analytics through integration with Amazon recognition

188
00:12:16,322 --> 00:12:19,494
video and libraries for machine learning frameworks

189
00:12:19,542 --> 00:12:23,130
such as Tensorflow and OpenCV.

190
00:12:23,710 --> 00:12:27,158
Kinesis video stream is also supporting WebRTC.

191
00:12:27,334 --> 00:12:30,750
This is an open source project that enable real time

192
00:12:30,820 --> 00:12:34,142
media streaming and interaction between web

193
00:12:34,196 --> 00:12:38,186
browsers and mobile application and connected

194
00:12:38,218 --> 00:12:40,750
robots via simple API.

195
00:12:41,650 --> 00:12:44,802
Case video stream supports media ingestions over

196
00:12:44,856 --> 00:12:47,518
WeBRC connection for secure storage,

197
00:12:47,614 --> 00:12:51,666
playback and analytics processing in

198
00:12:51,688 --> 00:12:55,254
things architecture the user want to use a

199
00:12:55,292 --> 00:12:58,742
simulation to test the same robot application

200
00:12:58,876 --> 00:13:02,374
in different simulation walls, world one and world

201
00:13:02,412 --> 00:13:06,086
two. And here I would like to mention three

202
00:13:06,188 --> 00:13:10,134
of the core benefits we have heard from customer for using simulation

203
00:13:10,182 --> 00:13:12,940
in robotics development. First,

204
00:13:13,390 --> 00:13:17,014
the ability to reproduce and test exact scenarios

205
00:13:17,062 --> 00:13:20,460
that have triggered unexpected behavior in the past,

206
00:13:20,770 --> 00:13:23,898
including edgy cases and unsafe condition.

207
00:13:24,074 --> 00:13:27,710
This is difficult when testing with physical robots.

208
00:13:28,050 --> 00:13:31,662
Second, you can speed up the clock and run

209
00:13:31,716 --> 00:13:34,098
simulation faster than real time,

210
00:13:34,264 --> 00:13:37,554
producing results in a fraction of time it would take

211
00:13:37,592 --> 00:13:40,500
on physical robots. Third,

212
00:13:40,950 --> 00:13:44,922
you can expand test coverage by programmatically testing

213
00:13:45,006 --> 00:13:48,406
many scenarios as here we are testing simulation one and

214
00:13:48,428 --> 00:13:52,102
simulation two and this can be multiplied using

215
00:13:52,156 --> 00:13:54,818
parapterized and repeatable simulation.

216
00:13:54,994 --> 00:13:58,090
Robomaker is a cloud service that make IoT easy

217
00:13:58,160 --> 00:14:01,660
to build, test and deploy robotics application.

218
00:14:02,910 --> 00:14:06,954
In this architecture, we want to build an application that

219
00:14:06,992 --> 00:14:10,286
can work with robots from system a and also robots from

220
00:14:10,308 --> 00:14:14,554
system b where you have different robots from various vendors

221
00:14:14,602 --> 00:14:18,480
and you want them to work seamlessly with each other.

222
00:14:18,850 --> 00:14:22,526
So AWS IoT robo Runner make it easy to

223
00:14:22,548 --> 00:14:26,010
build application for optimizing fleets of diverse

224
00:14:26,090 --> 00:14:30,046
robots. IoT Robo Runner provides

225
00:14:30,078 --> 00:14:33,810
central data repository for storing and using data

226
00:14:33,960 --> 00:14:37,270
from different robots management system and

227
00:14:37,340 --> 00:14:40,882
enterprise system. Once robots are connected,

228
00:14:40,946 --> 00:14:44,434
you can use sample application and software development

229
00:14:44,482 --> 00:14:47,866
libraries to build management application on

230
00:14:47,888 --> 00:14:50,570
top of the centralized data repository.

231
00:14:51,710 --> 00:14:55,770
IoT Roborunner help you to build complex management application

232
00:14:55,920 --> 00:14:59,290
that require robot's interoperability such as

233
00:14:59,360 --> 00:15:03,866
task orchestration and view information in a single unified

234
00:15:03,978 --> 00:15:07,406
display. As you can

235
00:15:07,428 --> 00:15:11,498
see here, you can use a robots runner for designing

236
00:15:11,594 --> 00:15:15,762
a shared place. You can define the entry points and

237
00:15:15,816 --> 00:15:19,262
the exit points. Robots wait at the entry

238
00:15:19,326 --> 00:15:22,770
points if they are not cleared for entry. By the time

239
00:15:22,840 --> 00:15:27,062
they get there, robots notify system

240
00:15:27,196 --> 00:15:30,760
of exiting the space at exit points.

241
00:15:31,770 --> 00:15:35,542
In this architecture we will walk through a sample solution that

242
00:15:35,596 --> 00:15:39,734
implements CI CD pipeline with automated scenarios

243
00:15:39,782 --> 00:15:42,842
based testing and simulation. First,

244
00:15:42,976 --> 00:15:47,014
AWS code pipeline is a fully managed continuous

245
00:15:47,062 --> 00:15:51,230
delivery service. You can connect with your code repositories in

246
00:15:51,380 --> 00:15:55,146
GitHub, GitLab, BitBucket and automate

247
00:15:55,178 --> 00:15:58,826
a set of build and test actions at each stage

248
00:15:58,858 --> 00:16:02,154
of your release. Once the code pipeline

249
00:16:02,202 --> 00:16:05,650
is set up, developers work in agile sprints and build

250
00:16:05,720 --> 00:16:09,842
new functionality in a feature branch. When ready,

251
00:16:09,976 --> 00:16:13,874
they submit a board request with new code. The board

252
00:16:13,912 --> 00:16:18,150
request gets reviewed and eventually merged into integration branch.

253
00:16:18,890 --> 00:16:23,298
This starts first two stages in the CI CD pipeline.

254
00:16:23,474 --> 00:16:26,898
First, the source of code is copied into a build server

255
00:16:26,994 --> 00:16:30,246
running in AWS code. Build a fully

256
00:16:30,278 --> 00:16:33,706
managed continuous integration tool that will run the

257
00:16:33,728 --> 00:16:37,338
ROS and Docker build command. Then copy build

258
00:16:37,424 --> 00:16:40,794
container into Amazon Elastic container registry,

259
00:16:40,922 --> 00:16:44,110
a center repository of container images.

260
00:16:45,010 --> 00:16:48,254
Once the container images are built and published, the next

261
00:16:48,292 --> 00:16:52,266
step is to run batch of tests. Automated tests in robomaker

262
00:16:52,298 --> 00:16:56,580
simulation in this solution, we use AWS step function,

263
00:16:57,510 --> 00:17:00,786
a low code solution for building state machine in the

264
00:17:00,808 --> 00:17:04,574
cloud to track and send notification on the progress

265
00:17:04,622 --> 00:17:09,582
of the simulation. Rotest results are automatically copied

266
00:17:09,646 --> 00:17:13,234
from a robomaker simulation to Amazon SS three bucket

267
00:17:13,362 --> 00:17:16,818
where they can be queried, analyzed and visualized

268
00:17:16,914 --> 00:17:19,770
using Amazon Athena and Amazon quicksight.

269
00:17:20,190 --> 00:17:23,180
Once a test in the simulation pass,

270
00:17:23,790 --> 00:17:27,610
the new container images can then be automatically deployed to real

271
00:17:27,680 --> 00:17:31,162
robots in a test area using AWS IoT green

272
00:17:31,216 --> 00:17:31,930
grass.

273
00:17:35,630 --> 00:17:39,386
Results from the test on the real robots can also be uploaded

274
00:17:39,418 --> 00:17:44,026
to Amazon SS three, then can combined with simulation test results.

275
00:17:44,218 --> 00:17:48,078
After all of the test validation check tasks,

276
00:17:48,164 --> 00:17:50,930
the code can be merged into the main branch,

277
00:17:52,230 --> 00:17:55,990
then it be deployed over the air to production fleet.

278
00:17:58,010 --> 00:18:01,794
In things architecture, we are using a spot developed by Boston

279
00:18:01,842 --> 00:18:05,442
Dynamics for industrial facilities inspection

280
00:18:05,586 --> 00:18:08,746
with AWS services. So a spot is a

281
00:18:08,768 --> 00:18:12,474
robot developed by Boston Dynamics and we are using it to

282
00:18:12,512 --> 00:18:16,730
run two inference deployed by AWS

283
00:18:17,230 --> 00:18:20,682
IoT greengrass. So we are running

284
00:18:20,816 --> 00:18:24,282
AWS it greengrass which is hosting two interfaces.

285
00:18:24,346 --> 00:18:28,286
One is running locally to detect a valve and

286
00:18:28,388 --> 00:18:31,866
a second one is running remotely to detect the state of the valve

287
00:18:31,898 --> 00:18:35,634
if the valve is open or closed, and the spot will be

288
00:18:35,672 --> 00:18:39,522
an auto walk mission where it detects the two valves and

289
00:18:39,576 --> 00:18:43,042
uploads the data to Amazon SS three. Once the data

290
00:18:43,096 --> 00:18:46,774
uploaded to Amazon SS three, we are using lambda function to

291
00:18:46,812 --> 00:18:50,658
read the data and update DynamDb and Cloudwatch

292
00:18:50,754 --> 00:18:54,754
through GraphQL API by AWS

293
00:18:54,802 --> 00:18:58,490
Appsync we're also hosting a dashboard

294
00:18:58,830 --> 00:19:03,366
dashboard which displays the state of the vault represented

295
00:19:03,398 --> 00:19:06,650
in Amazon Dynamodb and also in Cloudwatch.

296
00:19:07,070 --> 00:19:10,654
Let's watch a real life demo that being presented on

297
00:19:10,692 --> 00:19:17,470
reinvent this weekend.

298
00:19:41,600 --> 00:19:44,990
Right now, spot is detecting the valve over there.

299
00:19:46,720 --> 00:19:51,040
It will go in can auto OC mission to detect the second valve

300
00:19:52,820 --> 00:19:56,156
over there and update

301
00:19:56,188 --> 00:19:59,872
the state of the valve on a dashboard that being

302
00:19:59,926 --> 00:20:01,970
hosted on a monitor here.

303
00:20:08,350 --> 00:20:11,310
So it detects the valve image.

304
00:20:11,650 --> 00:20:15,454
It's going to send the image itself to a Tensorflow model

305
00:20:15,652 --> 00:20:19,386
and based on that one it was updated

306
00:20:19,498 --> 00:20:23,258
a file to Amazon SS three where a lambda function will process the

307
00:20:23,284 --> 00:20:27,074
file, update the lost state. It will

308
00:20:27,112 --> 00:20:29,380
go back to where the mission started.

309
00:20:31,050 --> 00:20:34,326
And this is just like a pose from a

310
00:20:34,348 --> 00:20:38,434
spot to indicate it's uploading data and eventually

311
00:20:38,482 --> 00:20:41,670
the data will be updated on the dashboard.

312
00:20:42,730 --> 00:20:46,262
Robots are doing amazing things and AWS

313
00:20:46,326 --> 00:20:49,882
is helping the builders behind the robots you see in this video

314
00:20:49,936 --> 00:20:54,310
innovate in our Amazon fulfillment centers,

315
00:20:54,470 --> 00:20:57,722
you see robots helping make our environment

316
00:20:57,786 --> 00:21:01,200
safe for people and deliver value to Amazon customer.

317
00:21:01,730 --> 00:21:05,120
This is also true for many of our industrial customer.

318
00:21:05,570 --> 00:21:08,646
Other customers are focused on using robots to solve

319
00:21:08,698 --> 00:21:11,950
some of humanity's biggest challenges,

320
00:21:12,030 --> 00:21:15,490
such as making our world more sustainable.

321
00:21:15,990 --> 00:21:19,474
So we are just getting started but see the

322
00:21:19,512 --> 00:21:21,700
incredible potential in the space.

323
00:21:25,370 --> 00:21:28,040
It's one small step per man,

324
00:21:29,610 --> 00:21:32,470
one diaphragm permanent.

325
00:22:21,530 --> 00:22:25,046
Let's get started today. So I would encourage

326
00:22:25,078 --> 00:22:29,050
you to look at AWS robotics blogs. Where are

327
00:22:29,120 --> 00:22:33,014
there many blogs talking about robots connectivity,

328
00:22:33,142 --> 00:22:38,490
robots simulation, robots integration

329
00:22:38,570 --> 00:22:42,686
from different vendors using apprunner and robomaker. Also have

330
00:22:42,708 --> 00:22:45,902
a look at IoT greengrass as we've been talking before,

331
00:22:46,036 --> 00:22:50,250
it's a core component that can help you run a smart

332
00:22:50,410 --> 00:22:54,414
software at the edge. IoT can help you with

333
00:22:54,612 --> 00:22:58,350
over the air update and running interfaces at the edge.

334
00:22:58,930 --> 00:23:02,854
Also, there are open source sets available

335
00:23:02,972 --> 00:23:06,150
in AWS samples and AWS robotics.

336
00:23:06,970 --> 00:23:11,014
Thank you very much for joining my session and please

337
00:23:11,052 --> 00:23:12,050
give me your feedback.

