1
00:00:39,250 --> 00:00:42,998
Hi there, I'm Wen Chao, a software engineer at

2
00:00:43,164 --> 00:00:46,422
today. I'm so glad to have the opportunity to share

3
00:00:46,476 --> 00:00:49,882
with you some exciting updates and innovations

4
00:00:49,986 --> 00:00:52,686
of core gateway. Before that,

5
00:00:52,868 --> 00:00:56,622
let me spend some time to tell you a little bit more about

6
00:00:56,676 --> 00:01:00,334
our company and our products. Core is a

7
00:01:00,372 --> 00:01:04,686
company that provides API gateway products and solutions.

8
00:01:04,878 --> 00:01:08,686
We have been focusing on helping our clients

9
00:01:08,878 --> 00:01:12,878
manage and secure their APIs

10
00:01:12,974 --> 00:01:16,982
effectively, empowering global companies

11
00:01:17,116 --> 00:01:21,190
to embrace an API first approach.

12
00:01:21,530 --> 00:01:24,738
The Core Gateway is an open youre,

13
00:01:24,834 --> 00:01:28,234
scalable and high performance API gateway product

14
00:01:28,432 --> 00:01:32,486
that provides features such as authentication,

15
00:01:32,598 --> 00:01:35,770
traffic control, HTTP transformation,

16
00:01:36,110 --> 00:01:40,246
logging and monitoring. In addition to the open source

17
00:01:40,278 --> 00:01:43,790
gateway, Kong also provides enterprise evolutions

18
00:01:44,130 --> 00:01:48,142
which may offer advanced features and

19
00:01:48,276 --> 00:01:51,950
support for organizations with specific requirements.

20
00:01:52,370 --> 00:01:56,370
We also provide a cloud native solution called

21
00:01:56,440 --> 00:02:00,498
Connect which enables you to manage APIs across

22
00:02:00,584 --> 00:02:03,540
any cloud team from one place.

23
00:02:04,310 --> 00:02:08,070
Connect provides a hosted unified control plan

24
00:02:08,220 --> 00:02:12,018
to manage gateways and ingress controllers.

25
00:02:12,194 --> 00:02:16,354
This empowers your organizations to reduce

26
00:02:16,482 --> 00:02:19,994
operational overhead, speed up time to the

27
00:02:20,032 --> 00:02:23,750
market, increase security, and achieving

28
00:02:23,830 --> 00:02:26,650
federated governance at scale.

29
00:02:27,230 --> 00:02:31,354
So in today's presentation, I will be focusing on

30
00:02:31,472 --> 00:02:35,530
core gateway from several aspects. The presentation

31
00:02:35,610 --> 00:02:39,434
will be divided into three parts, each focusing

32
00:02:39,482 --> 00:02:44,370
on different aspects of our enhancements and innovations.

33
00:02:44,950 --> 00:02:48,642
First and foremost, I will be introducing the

34
00:02:48,696 --> 00:02:52,686
significant improvements with making some plugins

35
00:02:52,878 --> 00:02:56,934
and exploring the key innovations for some

36
00:02:57,052 --> 00:03:01,350
core components that are powering up the core gateway.

37
00:03:01,690 --> 00:03:05,922
Following that, I will show you how we do our performance

38
00:03:05,986 --> 00:03:10,342
at can. In the third part, let's explore

39
00:03:10,486 --> 00:03:13,990
some skills in fine tuning the performance

40
00:03:14,150 --> 00:03:17,900
for an enhanced user experience.

41
00:03:19,630 --> 00:03:23,290
Core has hundreds of out of box plugins

42
00:03:23,450 --> 00:03:26,798
to address various use cases without

43
00:03:26,884 --> 00:03:30,586
introducing any platform complexity.

44
00:03:30,778 --> 00:03:35,262
This makes comb flexible enough to boost developer

45
00:03:35,326 --> 00:03:39,650
productivity and ensure operational

46
00:03:40,470 --> 00:03:44,034
resilience. In today's talk, we will

47
00:03:44,072 --> 00:03:48,274
just have a quick look at the latest improvements

48
00:03:48,402 --> 00:03:50,950
in some widely used plugins.

49
00:03:51,850 --> 00:03:55,670
So let's start with the plumsus plugin.

50
00:03:56,250 --> 00:04:00,680
As a commonly used cornerstone in modern monitoring system,

51
00:04:01,530 --> 00:04:05,450
plums use always plays a crucial role in core gateway.

52
00:04:05,790 --> 00:04:09,734
While it can perform well in scenarios with a relatively

53
00:04:09,782 --> 00:04:12,894
small number of roads, we recognize the

54
00:04:12,932 --> 00:04:16,874
decline in performance as the number of loads

55
00:04:16,922 --> 00:04:18,990
increases substantially.

56
00:04:19,650 --> 00:04:22,726
So we took this as a challenge

57
00:04:22,858 --> 00:04:26,260
and the followings are what we have worked out.

58
00:04:26,870 --> 00:04:30,386
As illustrated on this slide. We have already

59
00:04:30,488 --> 00:04:33,874
accelerated metric scrapping for the

60
00:04:33,912 --> 00:04:37,954
promise use plugin, and we have also implemented

61
00:04:38,002 --> 00:04:41,270
a reduction in high cardinality metrics.

62
00:04:41,850 --> 00:04:45,014
These enhancements eventually result in an

63
00:04:45,052 --> 00:04:48,014
improvement in the p 99 latency,

64
00:04:48,162 --> 00:04:51,914
which means it now can run smoothly even in large

65
00:04:51,952 --> 00:04:56,378
scale systems. Three factors contribute to

66
00:04:56,544 --> 00:04:59,530
these improvements. Firstly,

67
00:05:00,290 --> 00:05:03,786
we've adopted the table pool for the creation

68
00:05:03,898 --> 00:05:07,694
of temporary tables, which optimize the

69
00:05:07,732 --> 00:05:11,562
process and mitigates the cost of excessive table

70
00:05:11,626 --> 00:05:15,074
creation during promiscuous scrapping so that

71
00:05:15,112 --> 00:05:18,766
we can avoid performance hit of creating

72
00:05:18,798 --> 00:05:20,580
new tables all the time.

73
00:05:21,910 --> 00:05:26,402
Secondly, a few yield functions have

74
00:05:26,456 --> 00:05:30,494
been introduced in critical

75
00:05:30,622 --> 00:05:35,042
core sports, which allow the luajit

76
00:05:35,106 --> 00:05:39,340
compiler to release the control of cpu and

77
00:05:40,110 --> 00:05:43,802
give it to other request processing events rather

78
00:05:43,856 --> 00:05:46,940
than holding the cpu for too long time.

79
00:05:47,550 --> 00:05:51,802
This improves the responsiveness

80
00:05:51,946 --> 00:05:54,080
of the gateway quite a lot.

81
00:05:54,770 --> 00:05:58,762
Lastly, we've also replaced

82
00:05:58,826 --> 00:06:02,746
all the NYI functionalities involved in this plugin

83
00:06:02,858 --> 00:06:07,014
with those compatible with the Luigi

84
00:06:07,082 --> 00:06:10,414
compiler, which completely harnesses

85
00:06:10,542 --> 00:06:13,650
the performance benefits of Lua Vm.

86
00:06:16,330 --> 00:06:19,714
These changes collaboratively contribute

87
00:06:19,762 --> 00:06:23,234
to the performance boosts. The following bar chart

88
00:06:23,362 --> 00:06:27,566
shows a comparison of the p 99 latency

89
00:06:27,698 --> 00:06:31,130
between 3.3 and 3.4 version

90
00:06:31,710 --> 00:06:35,110
we just need to pay attention

91
00:06:35,190 --> 00:06:38,054
on the red and the green columns.

92
00:06:38,182 --> 00:06:41,742
The red one stands for version 3.3

93
00:06:41,876 --> 00:06:45,950
and the green one stands for version 3.4.

94
00:06:46,100 --> 00:06:50,042
Here we can see a significant enhancements

95
00:06:50,186 --> 00:06:54,190
of the 3.4 in the p 99 latency.

96
00:06:54,690 --> 00:06:58,222
Now let's move on to the red limiting

97
00:06:58,286 --> 00:07:02,402
plugin, which is a plugin for the purpose of

98
00:07:02,536 --> 00:07:05,906
traffic control. For this plugin,

99
00:07:06,098 --> 00:07:09,526
we've also got a notable increase both in p

100
00:07:09,548 --> 00:07:13,378
99 latency and RPS. The enhancement

101
00:07:13,554 --> 00:07:18,062
is mainly attributed to the introduction

102
00:07:18,226 --> 00:07:22,534
of a batch queue, which is a fundamental

103
00:07:22,662 --> 00:07:26,330
components newly introduced in 3.4,

104
00:07:26,480 --> 00:07:30,522
especially when the back end of the red limiting

105
00:07:30,586 --> 00:07:34,366
counter is can external storage. With the

106
00:07:34,388 --> 00:07:38,430
help of this queue, there is no need to

107
00:07:38,500 --> 00:07:41,710
communicate with external storage for every

108
00:07:41,780 --> 00:07:45,780
single request. It just put data

109
00:07:47,590 --> 00:07:51,522
in this internal in memory batch queue almost

110
00:07:51,656 --> 00:07:55,342
without any latency after every time the counters

111
00:07:55,406 --> 00:07:59,238
are calculated. When the queue flush these

112
00:07:59,324 --> 00:08:02,950
counters to external storage asynchronously,

113
00:08:03,450 --> 00:08:07,706
so it optimize the method of

114
00:08:07,728 --> 00:08:11,414
communication between the plugin and external

115
00:08:11,462 --> 00:08:14,486
storage, particularly beneficial

116
00:08:14,598 --> 00:08:17,290
in high concurrency scenarios.

117
00:08:18,830 --> 00:08:22,362
We've also introduced a new configuration

118
00:08:22,506 --> 00:08:25,754
sync rate so that we can even decide

119
00:08:25,802 --> 00:08:29,594
the flush frequency. The graph

120
00:08:29,642 --> 00:08:33,182
below illustrates the test result

121
00:08:33,316 --> 00:08:37,010
we did compared to a Goland based open

122
00:08:37,080 --> 00:08:40,180
source gateway. As we can see,

123
00:08:40,790 --> 00:08:44,974
the performance in P 99 and the RPS is

124
00:08:45,032 --> 00:08:48,310
nearly twice as good as the competitor.

125
00:08:48,970 --> 00:08:52,050
Now that we have discussed

126
00:08:52,130 --> 00:08:55,346
the improvements in plugins,

127
00:08:55,538 --> 00:08:59,418
let's delve into

128
00:08:59,584 --> 00:09:03,206
the enhancements in some core

129
00:09:03,318 --> 00:09:07,462
components. To begin with our discussion

130
00:09:07,606 --> 00:09:11,534
about the core components, let's first focus on

131
00:09:11,572 --> 00:09:15,246
the rotor components, which plays a

132
00:09:15,268 --> 00:09:18,910
key role in road pattern matching.

133
00:09:19,410 --> 00:09:23,382
Why is there a need for improvements

134
00:09:23,466 --> 00:09:29,022
in the rotor? Well, given that users

135
00:09:29,086 --> 00:09:33,374
heavily depend on its flexibility and efficiency

136
00:09:33,502 --> 00:09:37,830
to config their routing requirements effectively,

137
00:09:38,170 --> 00:09:42,214
the rotor stands out as the cornerstone of

138
00:09:42,252 --> 00:09:44,870
a gateway. Moreover,

139
00:09:45,610 --> 00:09:49,434
nearly every individual request should

140
00:09:49,472 --> 00:09:53,850
be checked by the rotor to determine if it matches

141
00:09:55,150 --> 00:09:58,300
with any API pass.

142
00:09:59,550 --> 00:10:05,310
The config API pass we've set as

143
00:10:05,380 --> 00:10:09,386
traffic volume increases, the rotor

144
00:10:09,578 --> 00:10:12,914
becomes a critical focal point and

145
00:10:12,952 --> 00:10:16,354
its performance emerges as a

146
00:10:16,392 --> 00:10:20,580
key factor. Since version 3.0,

147
00:10:21,590 --> 00:10:25,794
we introduced a new rotor implementation,

148
00:10:25,922 --> 00:10:29,814
which is called ATC rotor. The ATC rotor is

149
00:10:29,852 --> 00:10:34,310
a DESL based approach, and it's written

150
00:10:34,970 --> 00:10:36,710
in rust.

151
00:10:38,030 --> 00:10:44,474
The main reason for

152
00:10:44,512 --> 00:10:48,554
us to upgrade to a DSL based approach is to

153
00:10:48,752 --> 00:10:52,122
enhance flexibility. The new rotor

154
00:10:52,186 --> 00:10:56,138
empowers users to set roads

155
00:10:56,314 --> 00:10:59,070
with expressions.

156
00:11:00,370 --> 00:11:03,774
Configuring roads using expressions allows

157
00:11:03,822 --> 00:11:07,438
for more flexibility and better performance

158
00:11:07,534 --> 00:11:10,850
without losing readability

159
00:11:11,830 --> 00:11:16,150
when dealing with complex or large configuration.

160
00:11:16,730 --> 00:11:20,434
Furthermore, we rewrite

161
00:11:20,482 --> 00:11:24,294
it in rust, as rust is not

162
00:11:24,332 --> 00:11:27,030
only generally quite efficient,

163
00:11:27,630 --> 00:11:30,550
comparable to c language,

164
00:11:30,710 --> 00:11:34,090
providing high performance with

165
00:11:34,160 --> 00:11:38,470
low level control, but also is much safer

166
00:11:38,550 --> 00:11:42,430
than c in terms of its handling of memory.

167
00:11:43,250 --> 00:11:46,874
So it's ideal as a replacement

168
00:11:47,002 --> 00:11:51,162
for the language of choice for the development

169
00:11:51,306 --> 00:11:56,226
of a new module. Although it

170
00:11:56,248 --> 00:11:59,698
is never such an easy thing

171
00:11:59,784 --> 00:12:03,598
to rewrite a brand new module

172
00:12:03,694 --> 00:12:07,174
completely in rust. Because rust is not a

173
00:12:07,212 --> 00:12:09,830
native supported language in operation,

174
00:12:12,330 --> 00:12:15,878
several engineers invested a

175
00:12:15,964 --> 00:12:19,100
considerable amount of effort in this project.

176
00:12:19,710 --> 00:12:23,020
The results proved that

177
00:12:23,390 --> 00:12:26,886
everything was worthwhile. The new rotor

178
00:12:26,998 --> 00:12:31,358
can reduce review

179
00:12:31,444 --> 00:12:34,400
time when routes change.

180
00:12:35,090 --> 00:12:38,602
Meanwhile, it increases runtime

181
00:12:38,666 --> 00:12:42,218
performance when routing requests,

182
00:12:42,394 --> 00:12:46,594
and it reduce p 99 latency from 1.5

183
00:12:46,712 --> 00:12:51,234
seconds to 0.1. Second testing with

184
00:12:51,432 --> 00:12:56,674
10,000 rows besides

185
00:12:56,722 --> 00:12:59,590
the rewritten of the roads,

186
00:13:00,890 --> 00:13:05,942
we've made two more optimizations for

187
00:13:05,996 --> 00:13:09,174
the logic of the rotor. Firstly,

188
00:13:09,302 --> 00:13:12,134
we optimize the road rebuild logic.

189
00:13:12,262 --> 00:13:18,118
Secondly, configuration conditional

190
00:13:18,214 --> 00:13:22,270
rebuild of the roads was introduced,

191
00:13:22,930 --> 00:13:27,002
which means the road builder

192
00:13:27,146 --> 00:13:31,482
becomes more intelligent. They can be conscious

193
00:13:31,546 --> 00:13:35,246
about if roads have been changed and f

194
00:13:35,348 --> 00:13:42,050
needs are rebuilt. Indeed, these optimizations

195
00:13:42,470 --> 00:13:45,830
also contribute to the final enhancements.

196
00:13:49,530 --> 00:13:52,902
The second core components that I want to share

197
00:13:52,956 --> 00:13:56,374
with you is the improvement in

198
00:13:56,412 --> 00:13:59,290
our approach to configuration management.

199
00:14:00,030 --> 00:14:03,894
In our previous iterations, we rely

200
00:14:03,942 --> 00:14:07,878
on a shared memory as the configuration storage,

201
00:14:08,054 --> 00:14:11,942
which is a kind of native approach that provides

202
00:14:12,006 --> 00:14:16,126
node level storage accessible across

203
00:14:16,228 --> 00:14:18,830
all worker processes.

204
00:14:19,250 --> 00:14:22,910
While this was effective

205
00:14:23,330 --> 00:14:26,922
in ensuring fasting memory storage,

206
00:14:27,066 --> 00:14:30,766
we recognized the need for enhancement due

207
00:14:30,798 --> 00:14:34,350
to excessive memory consumption.

208
00:14:34,510 --> 00:14:38,370
Another concern is that shared

209
00:14:38,450 --> 00:14:41,958
memory is not database like

210
00:14:42,044 --> 00:14:46,738
storage, it does not support transactional

211
00:14:46,834 --> 00:14:50,860
operation. In some cases, this is an

212
00:14:51,950 --> 00:14:55,318
unacceptable reality

213
00:14:55,414 --> 00:14:58,986
because the ability to

214
00:14:59,168 --> 00:15:02,990
guarantee atomicity,

215
00:15:03,650 --> 00:15:07,086
which means we would need to use core

216
00:15:07,188 --> 00:15:10,350
code to ensure data consistent.

217
00:15:11,090 --> 00:15:16,158
This will increase the complexity

218
00:15:16,254 --> 00:15:19,570
of the code. From version 3.0,

219
00:15:19,720 --> 00:15:23,998
we handle configuration by introducing LMDB

220
00:15:24,094 --> 00:15:27,334
as the back end for corn, which is

221
00:15:27,452 --> 00:15:30,610
an in memory transactional database.

222
00:15:30,770 --> 00:15:34,578
By doing this, we've not only achieved

223
00:15:34,754 --> 00:15:38,550
a notable reduction in memory usage,

224
00:15:39,130 --> 00:15:43,290
but also unlocked the power of transactional storage.

225
00:15:44,270 --> 00:15:48,730
The benchmark results here also

226
00:15:48,800 --> 00:15:52,510
reveals that LMDb significantly improves

227
00:15:53,090 --> 00:15:57,610
rps during rebuilds with constant

228
00:15:57,690 --> 00:15:59,390
config push.

229
00:16:00,930 --> 00:16:04,850
What's more, we observed a remarkable

230
00:16:05,590 --> 00:16:10,100
50% to 70% additional job

231
00:16:11,910 --> 00:16:16,290
in rebuild time, particularly noticeable

232
00:16:16,370 --> 00:16:19,910
in cases with a large number of workers.

233
00:16:20,330 --> 00:16:24,066
Congate way is based on engines

234
00:16:24,098 --> 00:16:27,254
and openrest. It has a

235
00:16:27,292 --> 00:16:32,662
unique master worker architect for efficient

236
00:16:32,726 --> 00:16:36,614
cpu usage by forking multiple

237
00:16:36,662 --> 00:16:40,146
worker processes and it faces

238
00:16:40,278 --> 00:16:44,314
information sharing challenges across workers

239
00:16:44,442 --> 00:16:48,666
due to the isolated nature of worker processes.

240
00:16:48,858 --> 00:16:52,790
To address the communication needs between workers,

241
00:16:52,970 --> 00:16:57,380
especially for tasks like rotor rebuilding and

242
00:16:58,950 --> 00:17:03,058
health checking, core designed an

243
00:17:03,224 --> 00:17:05,990
event propagation mechanism.

244
00:17:06,890 --> 00:17:10,998
Previously, the event components we

245
00:17:11,084 --> 00:17:14,194
implemented relied on shared

246
00:17:14,242 --> 00:17:17,774
memory. Though this approach

247
00:17:17,842 --> 00:17:22,278
was effectively it had drawbacks

248
00:17:22,454 --> 00:17:26,650
such as high lock overhead. Accessing a

249
00:17:26,720 --> 00:17:30,006
single shared memory requires a lock to

250
00:17:30,128 --> 00:17:33,918
avoid race continuous and keep

251
00:17:34,004 --> 00:17:38,410
consistency. This lead to high overhead

252
00:17:38,570 --> 00:17:42,470
when there are numerous

253
00:17:42,570 --> 00:17:46,066
workers, which is considered as a

254
00:17:46,088 --> 00:17:50,210
kind of inefficiency. Another concern is

255
00:17:50,280 --> 00:17:54,114
the size of the shared memory. It should

256
00:17:54,152 --> 00:17:58,478
be decided and allocated in advance

257
00:17:58,654 --> 00:18:03,154
when NX starts and it cannot be changed

258
00:18:03,202 --> 00:18:07,222
during the runtime. It means that regardless of

259
00:18:07,276 --> 00:18:11,302
whether this part of shared memory is fully utilized,

260
00:18:11,446 --> 00:18:16,058
it has already been allocated to

261
00:18:16,144 --> 00:18:19,530
enhance efficiency and avoid risks.

262
00:18:19,950 --> 00:18:23,806
We upgraded to a new event lab called

263
00:18:23,908 --> 00:18:27,278
Lua rest events. The new event

264
00:18:27,364 --> 00:18:31,630
library operates as a classic publish

265
00:18:32,550 --> 00:18:36,626
subscribe model and is implemented by a

266
00:18:36,648 --> 00:18:40,900
pure Lua code, eliminating the need of

267
00:18:41,510 --> 00:18:42,850
shared memory.

268
00:18:44,310 --> 00:18:48,658
We did best to compare

269
00:18:48,754 --> 00:18:51,350
the two event libraries.

270
00:18:52,010 --> 00:18:55,414
We designed two test cases to do this.

271
00:18:55,612 --> 00:18:59,846
One is to post 1000 events

272
00:18:59,958 --> 00:19:03,350
per five milliseconds.

273
00:19:03,510 --> 00:19:08,138
The other is to post 10,000 events per 100

274
00:19:08,224 --> 00:19:11,994
milliseconds. And the benchmark

275
00:19:12,042 --> 00:19:15,934
here reveals that under

276
00:19:15,972 --> 00:19:20,026
the low pressure condition, the two libraries

277
00:19:20,218 --> 00:19:23,726
have nearly the same performance in the RPS,

278
00:19:23,918 --> 00:19:27,422
while the new library wins

279
00:19:27,486 --> 00:19:29,730
when the workload increases.

280
00:19:30,630 --> 00:19:34,718
Considering that performance is essential for

281
00:19:34,904 --> 00:19:38,054
commercial products to deliver can

282
00:19:38,172 --> 00:19:41,570
excellent user experience, we carefully

283
00:19:41,650 --> 00:19:45,014
designed a testing system and

284
00:19:45,052 --> 00:19:48,710
a set of statistical

285
00:19:48,870 --> 00:19:52,938
methods. Here is

286
00:19:53,104 --> 00:19:56,620
the infrastructure of the testing system.

287
00:19:57,390 --> 00:20:00,940
Two bare metals have been set up.

288
00:20:01,410 --> 00:20:04,910
One works as a load generator

289
00:20:05,810 --> 00:20:09,022
and the other is used for the deployment of

290
00:20:09,076 --> 00:20:13,350
core. We even bind different components,

291
00:20:13,530 --> 00:20:18,126
the NIC upstreams

292
00:20:18,158 --> 00:20:21,220
and the core gateway to different groups of

293
00:20:21,750 --> 00:20:25,650
CPU trying to get rid of CPU

294
00:20:25,730 --> 00:20:29,270
competition and we connect

295
00:20:29,420 --> 00:20:33,526
these two buried metals with a

296
00:20:33,628 --> 00:20:37,990
private network to fully utilize the network,

297
00:20:39,390 --> 00:20:42,970
the network bandwise, and we even

298
00:20:43,040 --> 00:20:46,970
put upstream and cone gateway on the loopback

299
00:20:47,470 --> 00:20:50,842
to give WRK more

300
00:20:50,896 --> 00:20:51,930
bandwidth.

301
00:20:54,590 --> 00:20:58,574
Here are the methods we used to check all the

302
00:20:58,692 --> 00:21:01,338
key metrics, rps,

303
00:21:01,434 --> 00:21:05,982
latency and memory usage and this graph

304
00:21:06,046 --> 00:21:11,106
shows how we check the decrease in

305
00:21:11,208 --> 00:21:14,890
metrics. We compare two groups of sample

306
00:21:14,990 --> 00:21:18,280
results to see if there's a significant difference.

307
00:21:18,970 --> 00:21:23,462
We do the test every day and

308
00:21:23,516 --> 00:21:27,222
if the difference happens the test is

309
00:21:27,276 --> 00:21:30,886
considered a failure and we will get a notification

310
00:21:30,998 --> 00:21:34,586
about it. Among two groups of

311
00:21:34,688 --> 00:21:38,186
results, one comes from the test result

312
00:21:38,288 --> 00:21:41,840
which is a poster release per test

313
00:21:42,450 --> 00:21:46,894
being carried out for a new version release and

314
00:21:46,932 --> 00:21:49,978
the results will be persisted

315
00:21:50,154 --> 00:21:53,714
in storage. The other one is tracked every

316
00:21:53,752 --> 00:21:57,342
day and persists in the same storage

317
00:21:57,486 --> 00:22:01,250
in case we need to review it. The charts

318
00:22:01,590 --> 00:22:05,166
illustrated on this slide tend

319
00:22:05,198 --> 00:22:10,450
to show you the effect of our methods.

320
00:22:11,050 --> 00:22:14,422
As you can see, it is easy to find

321
00:22:14,476 --> 00:22:18,070
out the organizations between the test cases.

322
00:22:19,630 --> 00:22:23,322
When the difference is found, a perf will be

323
00:22:23,376 --> 00:22:26,810
run to collect the perf data and

324
00:22:26,880 --> 00:22:30,574
generate the flame graph so that

325
00:22:30,612 --> 00:22:34,830
we can have an insight into details.

326
00:22:35,410 --> 00:22:38,622
From the flame graph we can generally find

327
00:22:38,676 --> 00:22:42,538
out the problematic function and

328
00:22:42,644 --> 00:22:46,194
after we made changes in our

329
00:22:46,232 --> 00:22:50,178
core to fix the issue, we run the perfect

330
00:22:50,264 --> 00:22:52,980
can to confirm the issue has gone.

331
00:22:53,430 --> 00:22:57,494
This is just an example to show you how

332
00:22:57,532 --> 00:23:01,366
we do performance. So in

333
00:23:01,388 --> 00:23:04,902
the last part of today's presentation, let's learn

334
00:23:04,956 --> 00:23:09,370
some practical insights into tuning the performance.

335
00:23:10,190 --> 00:23:14,566
Sometimes performance tuning can be intricate

336
00:23:14,758 --> 00:23:17,866
and today I will just talk about

337
00:23:17,968 --> 00:23:22,282
some common and fundamental strategies

338
00:23:22,346 --> 00:23:26,142
as examples. For more in depth information,

339
00:23:26,276 --> 00:23:30,474
please explore the technical blocks

340
00:23:30,522 --> 00:23:33,140
on core official website.

341
00:23:33,830 --> 00:23:37,102
So core as an API gateway,

342
00:23:37,246 --> 00:23:41,362
it comes out of box being

343
00:23:41,416 --> 00:23:45,114
well tuned for proxying traffic.

344
00:23:45,262 --> 00:23:50,070
However, there are some cases where cone is used for processing

345
00:23:50,890 --> 00:23:55,122
requests with large payload. So let's

346
00:23:55,186 --> 00:23:57,640
explore this case first.

347
00:23:58,670 --> 00:24:02,506
Core utilizes request buffer to hold the

348
00:24:02,528 --> 00:24:06,854
payload and when the payload exceeds

349
00:24:06,982 --> 00:24:08,220
the buffer size,

350
00:24:10,590 --> 00:24:13,934
the ink results to use

351
00:24:14,132 --> 00:24:18,030
the disk to buffer the request.

352
00:24:18,370 --> 00:24:21,630
This can avoid excessive memory usage,

353
00:24:22,310 --> 00:24:26,530
preventing potential memory shortage that

354
00:24:26,600 --> 00:24:29,810
could lead to extensive disk operations.

355
00:24:30,150 --> 00:24:32,020
Such as such,

356
00:24:32,630 --> 00:24:36,478
disk operations can be costly

357
00:24:36,574 --> 00:24:40,390
and may block NX event

358
00:24:40,460 --> 00:24:43,618
loop resulting in performance

359
00:24:43,794 --> 00:24:47,142
degradation. To mitigate this,

360
00:24:47,276 --> 00:24:51,126
a simple solution is to increase the buffer

361
00:24:51,158 --> 00:24:56,620
size defaulted at 8000

362
00:24:58,430 --> 00:25:01,798
as illustrated in this graph.

363
00:25:01,974 --> 00:25:05,710
The larger the buffer size is, the better

364
00:25:05,780 --> 00:25:09,360
performance we can get. Another case

365
00:25:09,730 --> 00:25:14,222
arises when corn receives

366
00:25:14,286 --> 00:25:18,450
responses from upstream with large

367
00:25:18,520 --> 00:25:22,146
payloads. Similar to the previous case,

368
00:25:22,328 --> 00:25:26,450
a buffer is in place for catching response

369
00:25:26,530 --> 00:25:30,258
bodies. The same principle holds

370
00:25:30,274 --> 00:25:34,066
true as demonstrated

371
00:25:34,178 --> 00:25:37,542
in this graph. Although buffering can

372
00:25:37,596 --> 00:25:40,882
increase performance, some cases

373
00:25:40,946 --> 00:25:44,582
are not suitable to enable it, including real

374
00:25:44,636 --> 00:25:48,374
time cases, long term connection or

375
00:25:48,492 --> 00:25:52,110
streaming transform transmission or

376
00:25:52,180 --> 00:25:55,390
streaming processing. And also

377
00:25:55,460 --> 00:25:58,942
we should not enable it when memory of the

378
00:25:58,996 --> 00:26:03,194
environment is limited. Okay, I think that's

379
00:26:03,242 --> 00:26:07,150
almost everything I would like to cover in today's presentation.

380
00:26:07,730 --> 00:26:10,554
Due to the limitations of the slides,

381
00:26:10,682 --> 00:26:14,882
some topics may not have been fully discussed.

382
00:26:15,066 --> 00:26:18,902
If you have any question, please feel

383
00:26:18,956 --> 00:26:22,006
free to reach out to me. I will be glad to

384
00:26:22,028 --> 00:26:23,460
help you. Thank you.

