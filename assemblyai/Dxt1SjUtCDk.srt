1
00:00:28,050 --> 00:00:31,714
Hi, welcome. My name is Carolina and I work at Nuaware.

2
00:00:31,842 --> 00:00:35,554
We're a software distribution company that specializes in cloud native

3
00:00:35,602 --> 00:00:39,286
technologies. We also partner with some of the newer, innovative vendors that

4
00:00:39,308 --> 00:00:43,106
enter the european market. So for over a year now, I've been speaking to teams

5
00:00:43,138 --> 00:00:46,002
looking into chaos engineering almost on a daily basis.

6
00:00:46,146 --> 00:00:49,962
So I thought it would be a good idea to compile some of the answers,

7
00:00:50,066 --> 00:00:53,742
common questions, success stories, and cautionary tales into

8
00:00:53,796 --> 00:00:57,450
a short guide for anyone that's interested in onboarding the practice.

9
00:00:57,530 --> 00:01:00,638
So, in other words, this talk is a brief before, during,

10
00:01:00,724 --> 00:01:04,286
and after of making chaos successful. So I hope you'll find it

11
00:01:04,308 --> 00:01:07,842
useful. And let's jump in. Before we dive into each step, I wanted to quickly

12
00:01:07,896 --> 00:01:10,690
provide you guys with a baseline definition. In short,

13
00:01:10,760 --> 00:01:14,222
I like to introduce chaos engineering as a scientific method

14
00:01:14,286 --> 00:01:17,950
to uncover failures in our system before they actually happen. So it

15
00:01:17,960 --> 00:01:21,894
allows us to reflect the real world scenarios that are unpredictable for

16
00:01:21,932 --> 00:01:25,250
traditional testing, to make sure that it's not just our applications,

17
00:01:25,330 --> 00:01:28,258
infrastructure and network that are designed for reliability,

18
00:01:28,354 --> 00:01:32,006
but also processes and people. So, through controlled use of attacks,

19
00:01:32,038 --> 00:01:36,374
we test how a system responds under various conditions. And apologies

20
00:01:36,422 --> 00:01:40,106
for using this very topical analogy here, but it works very similarly to

21
00:01:40,128 --> 00:01:43,866
a vaccine. So the injection of a pathogen in order to build up immunity

22
00:01:43,898 --> 00:01:47,226
to it and preventing potentially devastating illnesses.

23
00:01:47,338 --> 00:01:50,958
So, without further ado, let's dive into some of the considerations to

24
00:01:50,964 --> 00:01:54,250
be made before onboarding chaos engineering. For any organization

25
00:01:54,330 --> 00:01:57,746
that's unsure whether they will benefit from it, I always like to start from

26
00:01:57,768 --> 00:02:01,586
a business angle. We all know that downtime costs a lot of money. There are

27
00:02:01,608 --> 00:02:05,186
countless of examples in the news almost on a daily basis that remind us of

28
00:02:05,208 --> 00:02:08,546
that. So the average cost of downtime is currently estimated to

29
00:02:08,568 --> 00:02:11,286
average at around $100,000 an hour. So,

30
00:02:11,308 --> 00:02:14,934
unsurprisingly, it's becoming a KPI for many engineering teams at this

31
00:02:14,972 --> 00:02:18,614
point. So the bottom line here is, if your company's profitability is

32
00:02:18,652 --> 00:02:21,850
dependent on the underlying systems, the short answer is yes,

33
00:02:21,920 --> 00:02:25,706
you will benefit from chaos engineering. Now, while understanding the

34
00:02:25,728 --> 00:02:29,146
advantages of chaos testing, a lot of organizations believe that

35
00:02:29,168 --> 00:02:32,826
it's too early for them, as in, what's the point of chaos engineering

36
00:02:32,858 --> 00:02:36,574
in a legacy system? So, chaos engineering, of course, emerged from

37
00:02:36,612 --> 00:02:40,906
the increasing complexity of our systems, which, through the introduction of automation,

38
00:02:41,018 --> 00:02:45,382
containers, orchestrators and the like, have become almost unpredictable.

39
00:02:45,466 --> 00:02:48,862
Now, while chaos engineering does shine in distributed systems,

40
00:02:48,926 --> 00:02:52,942
it's mostly because the experiments can be performed at a larger scale.

41
00:02:53,006 --> 00:02:56,966
But at any scale, it will provide us with priceless knowledge about the behavior of

42
00:02:56,988 --> 00:03:00,322
our system immediate benefit for chaos testing legacy systems

43
00:03:00,386 --> 00:03:03,522
is the same as any other. Quoting Satya Nadella

44
00:03:03,586 --> 00:03:06,946
all companies are software companies, regardless of how cutting

45
00:03:06,978 --> 00:03:10,734
edge their estates are. Downtime will hurt customer facing applications

46
00:03:10,802 --> 00:03:14,726
run on legacy systems as much as those containerized on kubernetes.

47
00:03:14,838 --> 00:03:18,314
So everything that makes our systems more resilient will bring value.

48
00:03:18,432 --> 00:03:21,706
Now, the static nature of bare metal may appear to be more

49
00:03:21,728 --> 00:03:25,210
predictable, but very rarely will you come across an IT department

50
00:03:25,290 --> 00:03:28,686
without any of its functions being outsourced. So regardless of how

51
00:03:28,708 --> 00:03:32,366
well you know the behavior of your own system, how do you test, or in

52
00:03:32,388 --> 00:03:35,826
this case, simulate outages in third party providers? I'll let you

53
00:03:35,848 --> 00:03:39,534
guess the answer. But looking beyond immediate benefits of chaos testing

54
00:03:39,582 --> 00:03:44,302
legacy systems, sooner or later you'll migrate to the cloud, start containerizing

55
00:03:44,366 --> 00:03:48,070
or building out a Kubernetes platform so the earlier teams familiarize themselves

56
00:03:48,140 --> 00:03:51,666
with chaos engineering. The smoother and more reliable those transitions

57
00:03:51,698 --> 00:03:55,462
will be later on. But another aspect of the is it too

58
00:03:55,516 --> 00:03:59,314
early? Question is, do I have everything chaos, chaos chaos engineering

59
00:03:59,362 --> 00:04:02,906
practice lead a successful chaos engineering practice is characterized by

60
00:04:02,928 --> 00:04:06,714
its scientific approach. So in here, I really like the analogy of

61
00:04:06,752 --> 00:04:10,850
performing surgery. Would you do that with a blindfold and a hammer, or a microscope

62
00:04:10,870 --> 00:04:14,634
and a scalpel? Now, our microscope here would be monitoring

63
00:04:14,682 --> 00:04:17,854
observability and as much data or metrics you can

64
00:04:17,892 --> 00:04:21,422
collect while running those experiments. So monitoring is

65
00:04:21,476 --> 00:04:24,478
so, so important while running a chaos engineering test,

66
00:04:24,564 --> 00:04:27,774
as without it, we won't fully understand a the steady

67
00:04:27,822 --> 00:04:31,170
state of our application, but also b how a system

68
00:04:31,240 --> 00:04:35,026
starts to behave under stress. So baseline metrics to track here would be

69
00:04:35,048 --> 00:04:38,674
resource consumption, state and performance. But it's also

70
00:04:38,712 --> 00:04:42,742
paramount to have high visibility of the levels of service tied to your application.

71
00:04:42,876 --> 00:04:46,230
So the reason for that is twofold. A breach will result in a fine,

72
00:04:46,300 --> 00:04:49,850
of course, but also running a successful chaos experiment should

73
00:04:49,920 --> 00:04:53,702
increase those over time. Now here I'm talking about service level objectives,

74
00:04:53,766 --> 00:04:57,126
indicators and agreements, but very valuable metrics

75
00:04:57,158 --> 00:05:01,270
to collect, especially if you want to track. The efficiency of chaos engineering

76
00:05:01,350 --> 00:05:04,906
as part of your incident response would be meantime to detect,

77
00:05:05,018 --> 00:05:08,426
meantime to recovery, and meantime between failures.

78
00:05:08,618 --> 00:05:11,422
Now this brings us to a question that could be a topic of its own.

79
00:05:11,476 --> 00:05:14,974
So I've tried to summarize the main points. Should we build or buy

80
00:05:15,012 --> 00:05:18,354
our chaos engineering tools? So, in a nutshell, in house build

81
00:05:18,392 --> 00:05:22,130
tools are configured in line with your application or environment's exact

82
00:05:22,200 --> 00:05:25,502
needs, so that it seamlessly integrates with other environments,

83
00:05:25,566 --> 00:05:29,634
like your monitoring and development pipelines. Now, there's also a security benefit

84
00:05:29,682 --> 00:05:33,090
as all connections will stay within your company's internal network.

85
00:05:33,170 --> 00:05:36,310
Now, this is dependent on the internal processes you have in place,

86
00:05:36,380 --> 00:05:40,590
but building a tool often shortens feedback cycle between development and production.

87
00:05:40,690 --> 00:05:44,294
And of course, building in house gives you full control over the product roadmap,

88
00:05:44,342 --> 00:05:48,474
so you can release features when your organization needs them most.

89
00:05:48,592 --> 00:05:52,486
Now, the biggest argument for buying is, ironically, its cost effectiveness.

90
00:05:52,598 --> 00:05:56,574
With chaos engineering being a new practice, a specialized talent is

91
00:05:56,612 --> 00:06:00,462
very rare and expensive. Now, dedicating and upskilling an internal team

92
00:06:00,516 --> 00:06:04,630
to build a tool can also be quite costly. Even if we're talking about customizing

93
00:06:04,650 --> 00:06:08,206
an open source project with an enterprise grade tool, you will be provided

94
00:06:08,238 --> 00:06:11,826
with regular product updates and a dedicated support team. Now,

95
00:06:11,848 --> 00:06:16,174
to build a sophisticated fault injection platform takes roughly 14

96
00:06:16,222 --> 00:06:19,858
to 18 months of several engineers to build and maintain.

97
00:06:19,954 --> 00:06:22,550
Now, buying will shorten that time down to minutes,

98
00:06:22,700 --> 00:06:26,066
but bot tools are often much more compatible with varied

99
00:06:26,098 --> 00:06:29,974
application stacks and infrastructure types, so they're easy to scale. Most build

100
00:06:30,012 --> 00:06:33,370
tools also won't have automation features, at least not right away.

101
00:06:33,440 --> 00:06:37,366
Now, at this point, we should know why we're starting with chaos engineering

102
00:06:37,398 --> 00:06:40,474
and what we'll use. So where do we start then? The best way

103
00:06:40,512 --> 00:06:43,822
would be by brainstorming what should be experimented on to begin with.

104
00:06:43,876 --> 00:06:47,610
Now here I highly, highly discourage from going straight to production.

105
00:06:47,690 --> 00:06:51,326
I'd recommend picking a precisely defined small portion of the system,

106
00:06:51,428 --> 00:06:54,638
preferably in dev or test. Next step would be to state a

107
00:06:54,644 --> 00:06:58,626
hypothesis. What do we think will happen if we run this attack? Then we

108
00:06:58,648 --> 00:07:01,934
design an experiment again, one with a magnitude that's

109
00:07:01,982 --> 00:07:05,246
way smaller than we think has the potential to cause any failure

110
00:07:05,278 --> 00:07:09,014
or unpredictable behavior in our system. Now, after the experiment is

111
00:07:09,052 --> 00:07:12,658
complete, we carefully examined our monitoring and observability.

112
00:07:12,754 --> 00:07:15,846
Tools are showing and analyzed those metrics, as well

113
00:07:15,868 --> 00:07:19,526
as other system data. Our findings should drive how we prioritize our

114
00:07:19,548 --> 00:07:23,110
efforts, so mitigating any failures we've uncovered with this experiment

115
00:07:23,190 --> 00:07:27,142
immediately. Then we will follow our workup by running the same chaos experiment

116
00:07:27,206 --> 00:07:30,694
again to confirm our fix was effective. Doing this repeatedly,

117
00:07:30,822 --> 00:07:34,638
starting small and fixing what we find each time, will quickly add up.

118
00:07:34,724 --> 00:07:38,334
Now, our system gradually learns resiliency against a growing number of

119
00:07:38,372 --> 00:07:42,062
failure scenarios. So let's say we've run our first

120
00:07:42,116 --> 00:07:45,778
experiment. Now, how do we know it's working? One point

121
00:07:45,864 --> 00:07:49,246
worth mentioning is that when we're stating a hypothesis,

122
00:07:49,358 --> 00:07:52,450
we're looking to disprove, or in fact prove it.

123
00:07:52,520 --> 00:07:56,286
A lot of organizations tend to run chaos tests and don't uncover faults

124
00:07:56,318 --> 00:07:59,958
in their systems immediately. Now, this can happen, but it's important

125
00:08:00,044 --> 00:08:03,218
to remember that chaos engineering is a practice and is meant

126
00:08:03,234 --> 00:08:07,382
to be repeated. The knowledge that our systems will withstand a particular failure is

127
00:08:07,436 --> 00:08:10,762
just as valuable as discovering an issue and fixing it.

128
00:08:10,816 --> 00:08:14,474
Now, if one attack hasn't uncovered any unexpected behaviors, you can

129
00:08:14,512 --> 00:08:17,786
increase the magnitude of your attack or change the type of failure you're

130
00:08:17,808 --> 00:08:21,254
simulating until it breaks again. If it doesn't,

131
00:08:21,302 --> 00:08:25,322
great. We've still learned something. You can also continue under the same experiment

132
00:08:25,386 --> 00:08:28,638
somewhere else and repeat the process. Any piece of information

133
00:08:28,724 --> 00:08:31,966
about the resiliency, or lack thereof, of our limits of our

134
00:08:31,988 --> 00:08:36,034
systems, means that we've conducted a successful experiment. Now, at this

135
00:08:36,072 --> 00:08:39,634
point, we've pretty much onboarded chaos engineering. So where do we go

136
00:08:39,672 --> 00:08:42,942
from there? One of the most fruitful chaos engineering practices

137
00:08:43,006 --> 00:08:46,626
are game days. It's a dedicated day for running experiments on a

138
00:08:46,648 --> 00:08:50,354
specific portion of your system with a team. The goal is to eventually invite

139
00:08:50,402 --> 00:08:53,574
the bigger part of the organization, but these are very good when

140
00:08:53,612 --> 00:08:57,270
introducing chaos engineering for the first time as well. It allows focus on

141
00:08:57,340 --> 00:09:00,450
preselected potential failure points and improve on

142
00:09:00,460 --> 00:09:03,786
them immediately after. Now, once confident in the practice, a lot of

143
00:09:03,808 --> 00:09:07,194
organisation also work on automating chaos tests, such as

144
00:09:07,232 --> 00:09:10,950
incorporating some of them into their CI CD pipelines. But regardless,

145
00:09:11,030 --> 00:09:14,526
it's always a good idea to run game days on a regular basis, even if

146
00:09:14,548 --> 00:09:18,046
to propagate the reliability culture across the organization. And on

147
00:09:18,068 --> 00:09:21,534
that note, once you're running regular chaos tests, it's a good point to

148
00:09:21,572 --> 00:09:25,466
start educating other teams or departments on how they can benefit

149
00:09:25,498 --> 00:09:29,358
from it. Some organization build out chaos engineering centers of excellence.

150
00:09:29,454 --> 00:09:32,754
Some will upscale a champion per department and some

151
00:09:32,792 --> 00:09:36,482
just run regular game days. But regardless of what route would make the most

152
00:09:36,536 --> 00:09:39,686
sense for you, there's a multitude of use cases for

153
00:09:39,708 --> 00:09:43,506
this practice. You can replicate the most common kubernetes failures to ensure

154
00:09:43,538 --> 00:09:47,254
that your platform is configured properly. You can also test your

155
00:09:47,292 --> 00:09:51,558
disaster recovery. For example, run through a playbook with simulated scenarios.

156
00:09:51,654 --> 00:09:55,274
Another good use case is verifying your monitoring configurations to

157
00:09:55,312 --> 00:09:59,194
avoid missed alerts or prolonged outages. Now you can also test out different

158
00:09:59,232 --> 00:10:02,518
monitoring tools with case engineering. If you're unsure which one to

159
00:10:02,544 --> 00:10:06,094
buy. You can also validate that your systems are configured for

160
00:10:06,132 --> 00:10:10,106
resiliency as you migrate to the cloud. But chaos engineering

161
00:10:10,138 --> 00:10:13,226
can also help train new engineers in a controlled environment.

162
00:10:13,338 --> 00:10:17,026
Or you can test the response times for existing teams. Last but

163
00:10:17,048 --> 00:10:20,590
not least, chaos testing is very good at helping mitigate dependency

164
00:10:20,670 --> 00:10:24,894
failures, ensuring your systems fail gracefully, and identifying

165
00:10:24,942 --> 00:10:28,254
critical services. Now this of course, is a nonexhaustive

166
00:10:28,302 --> 00:10:31,606
list, but just a collection of the most popular use cases I've come

167
00:10:31,628 --> 00:10:34,838
across so far. And with that, we've come to an end.

168
00:10:34,924 --> 00:10:38,146
So thank you very much for watching my talk on how to onboard

169
00:10:38,178 --> 00:10:41,730
chaos engineering. I really wanted to showcase how straightforward

170
00:10:41,810 --> 00:10:45,382
introducing chaos engineering to an organization can be,

171
00:10:45,516 --> 00:10:48,962
and I hope it worked, and I hope you found it useful.

172
00:10:49,106 --> 00:10:52,766
Now, there are more depth talks from amazing speakers this year, so please make

173
00:10:52,788 --> 00:10:56,426
sure you check those out. And if you have any further questions, don't hesitate

174
00:10:56,458 --> 00:10:57,820
to reach out. Thank you very much.

