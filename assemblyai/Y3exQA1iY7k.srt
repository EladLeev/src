1
00:00:27,810 --> 00:00:31,494
Hello everyone. Today I'm going to talk about

2
00:00:31,532 --> 00:00:34,150
the aspects of microservice interactions,

3
00:00:34,890 --> 00:00:38,214
and you might wonder what does Devstar is

4
00:00:38,252 --> 00:00:41,814
doing on the screen. I put this

5
00:00:41,852 --> 00:00:45,862
image here because microservice architectures are

6
00:00:45,916 --> 00:00:49,286
also often referred to as

7
00:00:49,388 --> 00:00:53,070
Dev star architectures. This is coming from

8
00:00:53,140 --> 00:00:56,794
these very complex interaction diagrams

9
00:00:56,842 --> 00:01:02,030
that were generated in well known microservice infrastructures

10
00:01:02,470 --> 00:01:06,770
like you can see on the screen. Left one is from Netflix,

11
00:01:07,110 --> 00:01:10,770
the middle one you can see from one from Twitter,

12
00:01:11,110 --> 00:01:13,650
and the right one is from Amazon.

13
00:01:15,450 --> 00:01:19,014
So having this huge amount of

14
00:01:19,052 --> 00:01:23,030
network communications has its own implications.

15
00:01:23,370 --> 00:01:29,094
And that's why I think it's very important to understand the

16
00:01:29,132 --> 00:01:32,882
upcoming challenges of a network communication

17
00:01:33,026 --> 00:01:37,106
at such a large scale. So that drove

18
00:01:37,138 --> 00:01:40,030
me into this topic, into this area,

19
00:01:40,220 --> 00:01:44,670
and encouraged me to look behind

20
00:01:44,740 --> 00:01:48,574
the scenes and understand the details. And that's how this

21
00:01:48,612 --> 00:01:52,790
presentation was born. So the first section,

22
00:01:52,970 --> 00:01:57,394
let's talk about the

23
00:01:57,432 --> 00:02:01,582
reasons, the driving

24
00:02:01,646 --> 00:02:05,570
forces that are affecting these communications

25
00:02:05,650 --> 00:02:08,934
channels. But first, I would

26
00:02:08,972 --> 00:02:12,166
like to just have a little recap on

27
00:02:12,268 --> 00:02:16,354
what are the main characteristics of the microservice

28
00:02:16,402 --> 00:02:20,714
architecture style and why

29
00:02:20,752 --> 00:02:24,554
are we doing this overall, what's the benefit if we are doing it

30
00:02:24,592 --> 00:02:29,706
well? So one

31
00:02:29,728 --> 00:02:35,326
of the most important aspect of these things is

32
00:02:35,428 --> 00:02:39,470
that microservices are independently deployable.

33
00:02:39,970 --> 00:02:43,778
So I should be able to deploy a single

34
00:02:43,864 --> 00:02:47,342
service without others really noticing

35
00:02:47,406 --> 00:02:51,394
that on top of that comes another

36
00:02:51,592 --> 00:02:54,826
important functionality, autonomity.

37
00:02:54,958 --> 00:02:58,134
So this means that a single feature often

38
00:02:58,252 --> 00:03:01,346
is mapped to a single microservice.

39
00:03:01,458 --> 00:03:05,282
So I'm able to deliver a single feature, a single functional

40
00:03:05,346 --> 00:03:09,494
change, by just modifying one microservice

41
00:03:09,542 --> 00:03:11,930
and ending up in one deployment.

42
00:03:13,470 --> 00:03:18,010
What's also important is that microservices are polyglot.

43
00:03:19,230 --> 00:03:22,366
They're also polyglot in

44
00:03:22,388 --> 00:03:25,914
terms of database usage or database technology choices,

45
00:03:26,042 --> 00:03:29,994
also in language choices, but also in other technological aspects.

46
00:03:30,122 --> 00:03:33,760
So let's say if I want to come up with a

47
00:03:34,370 --> 00:03:38,426
JVM upgrade, I don't have to upgrade

48
00:03:38,458 --> 00:03:42,226
all the services all at once and then deploy them in a

49
00:03:42,248 --> 00:03:45,002
single coordinated, huge deployment.

50
00:03:45,166 --> 00:03:48,822
I'm free to go with a single service only. And then other

51
00:03:48,876 --> 00:03:52,850
teams owning other services are also free to deploy

52
00:03:52,930 --> 00:03:56,370
when they think that's suitable,

53
00:03:56,530 --> 00:03:59,626
that goes on with other technologies. So for instance, if I

54
00:03:59,648 --> 00:04:03,594
want to change from traditional rest based communication to

55
00:04:03,632 --> 00:04:06,250
GRPC, same thing applies.

56
00:04:08,830 --> 00:04:12,734
So for this to work perfectly, we need one

57
00:04:12,772 --> 00:04:15,866
single ingredient, namely network calls.

58
00:04:15,978 --> 00:04:19,710
So you can introduce dependencies between

59
00:04:19,780 --> 00:04:24,098
services in many ways, but network call

60
00:04:24,184 --> 00:04:27,746
is one of the most efficient way of doing that.

61
00:04:27,928 --> 00:04:31,362
So if you have dependencies by

62
00:04:31,416 --> 00:04:35,066
using libraries shared data or anything else, it won't

63
00:04:35,118 --> 00:04:38,354
work. As well as network communication, a single network

64
00:04:38,402 --> 00:04:42,322
communication. So that's why we need to deep dive

65
00:04:42,466 --> 00:04:46,214
into these network calls and understand how we can

66
00:04:46,332 --> 00:04:47,720
optimize them.

67
00:04:50,190 --> 00:04:54,502
These network calls have driving forces, I think about driving

68
00:04:54,566 --> 00:04:58,458
forces. I think we have around

69
00:04:58,624 --> 00:05:01,594
five of the most important ones. These are,

70
00:05:01,632 --> 00:05:04,886
namely latency, availability, reliability,

71
00:05:05,078 --> 00:05:09,022
queuing theory, and Conway slow. So let's go through each them,

72
00:05:09,076 --> 00:05:13,614
each by each, and let's understand, how do they affect microservice

73
00:05:13,662 --> 00:05:16,850
communications. First one is latency.

74
00:05:17,990 --> 00:05:22,050
Latency has a had limit. That's the

75
00:05:22,120 --> 00:05:25,778
speed of light. So if you take the two costs of

76
00:05:25,784 --> 00:05:26,580
the US,

77
00:05:31,110 --> 00:05:35,318
the runtrip time for the light to travel through the

78
00:05:35,484 --> 00:05:40,746
left side to the right side is 27

79
00:05:40,848 --> 00:05:45,050
milliseconds. Okay? But in practice,

80
00:05:45,470 --> 00:05:49,146
we cannot reach this number because the speed of

81
00:05:49,168 --> 00:05:52,618
light is also affected by the density of the material it's

82
00:05:52,634 --> 00:05:55,930
passing through. So, for instance, in fiber optics,

83
00:05:56,010 --> 00:05:59,998
if this would be just a single cable, it's more like

84
00:06:00,084 --> 00:06:01,870
41 milliseconds.

85
00:06:03,010 --> 00:06:06,330
I got this from this website I

86
00:06:06,340 --> 00:06:10,734
have on the references section, which simulates

87
00:06:10,862 --> 00:06:14,526
the expected roundtips time in two parts

88
00:06:14,638 --> 00:06:15,830
of the globe.

89
00:06:18,890 --> 00:06:22,614
So that's where these numbers are coming from. But there are many other

90
00:06:22,812 --> 00:06:26,546
pages where they are expecting you or explaining

91
00:06:26,578 --> 00:06:30,122
you the expected round trip time in

92
00:06:30,176 --> 00:06:33,254
different data centers for a cloud provider.

93
00:06:33,302 --> 00:06:36,678
So, in case of AWS,

94
00:06:36,854 --> 00:06:40,054
this is realistically more like 50

95
00:06:40,112 --> 00:06:43,694
to 60 milliseconds, but it depends on which

96
00:06:43,812 --> 00:06:47,118
region are you connecting to? Another region. But why is

97
00:06:47,124 --> 00:06:51,470
this important for us? So this gives you the minimum latency

98
00:06:52,550 --> 00:06:55,220
if you connect from one region to another.

99
00:06:55,670 --> 00:06:59,106
So if you want to come up with

100
00:06:59,128 --> 00:07:03,250
a multiregion deployment, because of various reasons,

101
00:07:03,590 --> 00:07:07,298
you always have to think about the data synchronization between the regions.

102
00:07:07,394 --> 00:07:11,142
And this will be the minimum latency until

103
00:07:11,276 --> 00:07:13,510
data arrives to the other edge.

104
00:07:13,930 --> 00:07:17,422
And I say this is the minimum latency,

105
00:07:17,506 --> 00:07:20,954
because in reality, when you have more

106
00:07:20,992 --> 00:07:24,902
pressure on your data layer, work will stockpile

107
00:07:24,966 --> 00:07:28,602
up, and this will go up to 506 hundred

108
00:07:28,656 --> 00:07:32,922
milliseconds. So 60 milliseconds

109
00:07:32,986 --> 00:07:36,826
is the optimistic duration.

110
00:07:37,018 --> 00:07:40,510
So if you want to have a synchronous write,

111
00:07:40,580 --> 00:07:44,722
let's say each synchronous write will have at least 40 to

112
00:07:44,776 --> 00:07:48,782
600 milliseconds of latency. This has to be considered

113
00:07:48,846 --> 00:07:52,322
if you are planning to do something which has

114
00:07:52,456 --> 00:07:54,690
its own low latency requirements.

115
00:07:56,410 --> 00:08:00,450
Also, it's important to understand the correlation between latency

116
00:08:00,530 --> 00:08:05,270
and throughput. This comes from another research. So they try to simulate

117
00:08:06,890 --> 00:08:10,854
two things by creating fictional

118
00:08:10,902 --> 00:08:14,506
website. They measured the

119
00:08:14,528 --> 00:08:18,234
page load time and measured the bandwidth, and they were interested

120
00:08:18,352 --> 00:08:21,930
in how the page load time varies based on

121
00:08:22,000 --> 00:08:25,514
the bandwidth and based on the roundtree time. So if you increase

122
00:08:25,562 --> 00:08:29,246
the bandwidth, if you increase the throughput, then you will see that the

123
00:08:29,428 --> 00:08:33,682
benefits are diminishing. Very early page

124
00:08:33,736 --> 00:08:37,166
load time actually maps to around hundreds of requests

125
00:08:37,198 --> 00:08:41,060
of latency because roughly that's the amount of

126
00:08:41,990 --> 00:08:45,060
independent requests required to load whole page.

127
00:08:45,510 --> 00:08:48,834
Okay, but if you try to change the round trip

128
00:08:48,882 --> 00:08:52,210
time, the page load time is really linearly decreasing.

129
00:08:52,290 --> 00:08:55,782
So what does this tell us? First of all, there is

130
00:08:55,836 --> 00:08:58,906
no direct correlation or direct

131
00:08:59,008 --> 00:09:02,534
effect between page load time and bandwidth

132
00:09:02,582 --> 00:09:06,250
or latency and throughput. So this means also that

133
00:09:06,320 --> 00:09:09,942
if you rely on barely,

134
00:09:10,006 --> 00:09:13,594
just on the scalability or auto scaling capacities on your

135
00:09:13,632 --> 00:09:17,470
system, that is not going to have a positive effect on the latency.

136
00:09:18,770 --> 00:09:22,250
So if you want to be efficient in terms of latency,

137
00:09:22,330 --> 00:09:25,646
you have to think about other solutions, not just scaling

138
00:09:25,678 --> 00:09:28,850
out or giving more juice for your instances.

139
00:09:29,990 --> 00:09:33,954
Another thing is availability. Thinking about

140
00:09:34,072 --> 00:09:37,682
availability, I always think in dependency graphs

141
00:09:37,746 --> 00:09:41,366
because that's what determines the availability as a whole.

142
00:09:41,548 --> 00:09:44,834
So let's see here a simplified machine

143
00:09:44,882 --> 00:09:48,282
architecture. For a single transaction I need

144
00:09:48,336 --> 00:09:52,506
all the components to

145
00:09:52,528 --> 00:09:55,814
behave as expected. So I need both cpu,

146
00:09:55,862 --> 00:09:59,574
the memory, the network and the disk component.

147
00:09:59,702 --> 00:10:03,610
So what if we give it each by each availability

148
00:10:03,690 --> 00:10:07,134
number? So let's say each component has now 99% of

149
00:10:07,172 --> 00:10:11,130
availability. This will form dependency

150
00:10:11,210 --> 00:10:15,380
graph and gives us the overall availability of 96%

151
00:10:15,910 --> 00:10:19,540
just because the combination of the components had its own

152
00:10:20,870 --> 00:10:25,106
probability of failure. If the individual

153
00:10:25,208 --> 00:10:28,302
elements have their own probability of failure.

154
00:10:28,446 --> 00:10:30,440
That's how maths work actually.

155
00:10:31,530 --> 00:10:34,790
But if I scale this out to client server model,

156
00:10:34,860 --> 00:10:38,102
you can see that now I have more dependencies between

157
00:10:38,156 --> 00:10:42,250
components. Again, let's say that each component has now 99%

158
00:10:42,320 --> 00:10:46,186
of chance of being successful. Now the

159
00:10:46,208 --> 00:10:49,740
availability will drop to 88 5%.

160
00:10:50,690 --> 00:10:54,462
So with each single dependency your

161
00:10:54,516 --> 00:10:58,714
availability will decrease. But it also depends

162
00:10:58,762 --> 00:11:02,206
on how the dependency is introduced and

163
00:11:02,228 --> 00:11:05,170
in which part of your architecture is introduced.

164
00:11:05,750 --> 00:11:09,220
It depends on your dependency graph as well.

165
00:11:10,710 --> 00:11:14,562
You can do the maths by hand if you're interested in the availability numbers.

166
00:11:14,696 --> 00:11:19,334
I use my own availability simulator which

167
00:11:19,372 --> 00:11:23,142
is just running a couple of cycles and testing each

168
00:11:23,196 --> 00:11:26,726
connection and failing them randomly based on these numbers given.

169
00:11:26,828 --> 00:11:29,210
I also have this in references section.

170
00:11:30,990 --> 00:11:34,874
Okay, now for

171
00:11:34,912 --> 00:11:38,378
reliability, I think the most important

172
00:11:38,544 --> 00:11:41,790
use case is a single client server communication.

173
00:11:42,290 --> 00:11:46,138
So let's say that we have a transaction

174
00:11:46,234 --> 00:11:50,046
that's changing the state of the whole system. So let's say this

175
00:11:50,068 --> 00:11:55,010
is a bright operation. For instance, how things

176
00:11:55,080 --> 00:11:59,394
can fail. Let's go through them

177
00:11:59,432 --> 00:12:01,700
sequentially. First of all,

178
00:12:03,110 --> 00:12:06,642
the write operation can fail when client

179
00:12:06,706 --> 00:12:10,118
sends the request to the server. Then it

180
00:12:10,124 --> 00:12:13,720
can also fail by being processed on the server itself.

181
00:12:14,730 --> 00:12:18,866
But it can also fail when server successfully

182
00:12:19,058 --> 00:12:22,218
processed the write operation and it responds back to

183
00:12:22,224 --> 00:12:24,620
the client. Here comes the problem.

184
00:12:25,070 --> 00:12:28,714
Client can just simply retry the

185
00:12:28,752 --> 00:12:31,782
write operation on the first two cases,

186
00:12:31,926 --> 00:12:35,946
but after server successfully process the write operation,

187
00:12:36,138 --> 00:12:40,078
client does not know what to do. You may

188
00:12:40,244 --> 00:12:44,666
come over this problem by using item potent operations

189
00:12:44,778 --> 00:12:48,810
or something similar, but in other cases

190
00:12:48,890 --> 00:12:52,100
devious solution is not so obvious. And also,

191
00:12:52,470 --> 00:12:55,554
of course I can fail on client side

192
00:12:55,672 --> 00:12:59,346
by the request is being processed. But why

193
00:12:59,368 --> 00:13:03,282
it's important for us. So first of all, that's why for instance,

194
00:13:03,426 --> 00:13:06,806
two phase commit was not really working on

195
00:13:06,828 --> 00:13:10,266
a larger scale. Because if you arrive on a

196
00:13:10,288 --> 00:13:13,846
commit phase, if one of the request is failing on a commit

197
00:13:13,878 --> 00:13:17,674
phase, the client or the coordinator does not really know

198
00:13:17,712 --> 00:13:21,722
how to proceed because other previous operations

199
00:13:21,786 --> 00:13:25,514
are already committed. And I just got one single failure.

200
00:13:25,562 --> 00:13:29,454
Should I just re request the failed node to

201
00:13:29,572 --> 00:13:35,322
commit its changes again, risking duplicated

202
00:13:35,386 --> 00:13:39,410
write or something similar? Or should they just abort the whole operation?

203
00:13:40,070 --> 00:13:44,498
Another example is, let's say exactly

204
00:13:44,584 --> 00:13:48,470
once message semantics. So the server always

205
00:13:48,540 --> 00:13:51,830
have to acknowledge if one message is processed.

206
00:13:52,410 --> 00:13:56,342
If I fail after the message is

207
00:13:56,396 --> 00:13:59,722
processed, the client cannot do anything else,

208
00:13:59,776 --> 00:14:03,082
just resend the message. And that's why we often

209
00:14:03,136 --> 00:14:06,682
have at least once message semantics instead of

210
00:14:06,736 --> 00:14:10,662
exactly once message semantics mapping

211
00:14:10,726 --> 00:14:14,378
again. So mapping reliability statistically

212
00:14:14,474 --> 00:14:17,774
to more requests. We were talking about roundtree time

213
00:14:17,812 --> 00:14:21,454
and talking about that. Or, sorry, not page load time. Yeah, page load time

214
00:14:21,492 --> 00:14:24,774
and talking about that. Page load time usually involves

215
00:14:24,842 --> 00:14:28,130
hundreds of requests, and it needs hundreds of requests to succeed.

216
00:14:28,470 --> 00:14:31,634
So let's say we have theoretically a single request that has

217
00:14:31,672 --> 00:14:35,678
99% of the probability of being successful.

218
00:14:35,854 --> 00:14:39,346
If we have hundreds of requests with the same characteristics,

219
00:14:39,458 --> 00:14:42,806
we cannot just say that less than 40% of the

220
00:14:42,828 --> 00:14:46,386
chance will be that the hundreds

221
00:14:46,418 --> 00:14:49,770
of requests will succeed. All hundreds of requests.

222
00:14:50,350 --> 00:14:53,846
Because we have so many permutations of these hundreds

223
00:14:53,878 --> 00:14:55,850
of requests being failed,

224
00:14:57,550 --> 00:15:01,550
this drops our probability with around

225
00:15:01,620 --> 00:15:05,514
60%. Just for the statistical reasons,

226
00:15:05,642 --> 00:15:09,630
we can't really fight maths here. These are the hard facts.

227
00:15:10,930 --> 00:15:15,962
So this actually resulted

228
00:15:16,026 --> 00:15:19,586
as a couple of artifacts that I

229
00:15:19,608 --> 00:15:23,186
think are very popular in

230
00:15:23,208 --> 00:15:27,078
the engineering world. One of them is these latency numbers.

231
00:15:27,164 --> 00:15:30,294
Every programmer should know that

232
00:15:30,412 --> 00:15:33,686
presented in many forms like this one. This is

233
00:15:33,708 --> 00:15:37,478
coming again from a web page where you have a slider and can change

234
00:15:37,644 --> 00:15:41,290
the year and see how these latency numbers

235
00:15:41,360 --> 00:15:45,142
have changed these are the recent numbers. So, for instance,

236
00:15:45,286 --> 00:15:49,354
if we investigate the main memory reference and the latency for

237
00:15:49,392 --> 00:15:52,982
a typical main memory reference, that's around 100 nanoseconds,

238
00:15:53,126 --> 00:15:56,746
and the runtrip time in same data center as in a cloud infrastructure

239
00:15:56,778 --> 00:15:59,934
is around 500 milliseconds. Why should we

240
00:15:59,972 --> 00:16:03,502
care, you might ask? Because these numbers are fast enough.

241
00:16:03,556 --> 00:16:07,026
So they are very fast, and they improved a

242
00:16:07,048 --> 00:16:11,006
lot in the recent years. 500 nanoseconds

243
00:16:11,118 --> 00:16:14,386
is something I should not care about, right? So if

244
00:16:14,408 --> 00:16:18,050
you. Let's say you want to introduce a caching strategy and you need to

245
00:16:18,120 --> 00:16:21,906
choose between an in memory solution, or maybe a distributed

246
00:16:21,938 --> 00:16:25,462
caching solution, because you want to share it with

247
00:16:25,516 --> 00:16:28,934
multiple services and you want to offer it as a separate

248
00:16:28,982 --> 00:16:32,460
service, think about the runtime time. So,

249
00:16:33,870 --> 00:16:37,242
difference between the in memory and

250
00:16:37,296 --> 00:16:41,102
the distributed solution caching solution is

251
00:16:41,236 --> 00:16:45,230
in terms of latency, is around

252
00:16:45,300 --> 00:16:49,102
5000 more if you choose

253
00:16:49,236 --> 00:16:52,686
distributed cache than if you choose an in

254
00:16:52,708 --> 00:16:54,110
memory cache solution.

255
00:16:55,970 --> 00:17:00,558
Another one. Another paper is this fallacy of distributed computing.

256
00:17:00,734 --> 00:17:04,402
You can find it in Wikipedia. If you look at the top three

257
00:17:04,456 --> 00:17:08,374
of these policies, then I think it's clear that

258
00:17:08,412 --> 00:17:11,320
we covered plenty of aspects of those,

259
00:17:11,770 --> 00:17:14,520
but the others are also important.

260
00:17:15,930 --> 00:17:18,950
But now I want to talk about something else,

261
00:17:19,020 --> 00:17:22,650
talk about the queuing theory and spend a little bit more time.

262
00:17:22,720 --> 00:17:27,722
Because in my experience, how I saw people

263
00:17:27,856 --> 00:17:31,482
in the engineering area are not really familiar with queuing theory

264
00:17:31,546 --> 00:17:35,486
and not really thinking in queues. But in

265
00:17:35,508 --> 00:17:39,530
practice, I think queues are everywhere in a modern architecture

266
00:17:39,690 --> 00:17:43,346
at large scale, also in small scale. So it's very good to

267
00:17:43,368 --> 00:17:47,090
understand the basics. So here comes the basics of queuing theory.

268
00:17:47,510 --> 00:17:50,850
By talking about queues,

269
00:17:51,910 --> 00:17:55,330
I think you can think about the simplified model

270
00:17:55,400 --> 00:17:59,042
that you can see on the screen. So you have queues of these orange marbles,

271
00:17:59,186 --> 00:18:02,690
a single queue of these orange marbles that needs to be processed.

272
00:18:02,850 --> 00:18:06,454
Then you have something on the right side

273
00:18:06,572 --> 00:18:10,234
that is processing the marbles and

274
00:18:10,432 --> 00:18:14,154
producing these green marbles on the right side of the screen. And then

275
00:18:14,192 --> 00:18:17,494
you have these metrics around the queue that determines the queue

276
00:18:17,542 --> 00:18:21,742
performance. We are interested in these four,

277
00:18:21,796 --> 00:18:25,086
mainly so there is execution time needed for a

278
00:18:25,108 --> 00:18:28,480
single node to process an orange marble and

279
00:18:29,490 --> 00:18:32,794
create a green marble. Then there is this

280
00:18:32,932 --> 00:18:37,410
departure rate, meaning the rate of

281
00:18:37,560 --> 00:18:41,186
green marbles being processed. Then we

282
00:18:41,208 --> 00:18:44,994
have the latency that requires duration of

283
00:18:45,112 --> 00:18:48,202
either a single marble traversing cv

284
00:18:48,366 --> 00:18:52,018
up to the right side when it becomes to green marble,

285
00:18:52,194 --> 00:18:55,414
or the overall duration required for all the

286
00:18:55,452 --> 00:18:58,602
marbles being processed. And we have then the

287
00:18:58,656 --> 00:19:02,058
arrival rate on the very left side, which is

288
00:19:02,064 --> 00:19:06,010
the rate of the orange marbles arriving in the queue.

289
00:19:07,470 --> 00:19:11,420
So the most basic question is, what happens

290
00:19:11,730 --> 00:19:15,514
when the arrival rate is much larger than the departure

291
00:19:15,562 --> 00:19:18,430
rate? So, this happens with us all the time,

292
00:19:18,580 --> 00:19:22,526
actually. So if you have something that's publicly available on the

293
00:19:22,548 --> 00:19:26,034
web, you don't have control over the user base

294
00:19:26,152 --> 00:19:29,854
and their usage statistics

295
00:19:29,982 --> 00:19:32,900
and often have to operate in this area.

296
00:19:34,150 --> 00:19:37,522
So, in this case, if you just accept

297
00:19:37,586 --> 00:19:41,554
all the requests and try to process them, you are guaranteed

298
00:19:41,602 --> 00:19:45,606
to fail after a certain period. And you

299
00:19:45,628 --> 00:19:48,810
have to introduce something. Right. This something is called

300
00:19:48,880 --> 00:19:52,540
back pressure or rate limiting. Okay. So,

301
00:19:53,070 --> 00:19:56,970
often in the edge, you have rate limiter service that

302
00:19:57,040 --> 00:20:00,622
determines which endpoint is

303
00:20:00,676 --> 00:20:04,670
limited to what

304
00:20:04,820 --> 00:20:09,294
sort of throughput, and tries to keep the

305
00:20:09,492 --> 00:20:14,050
right side or protected from higher

306
00:20:14,120 --> 00:20:17,746
aid than usual, and tries to introduce some sort of

307
00:20:17,768 --> 00:20:21,634
a logic on limiting those

308
00:20:21,672 --> 00:20:25,730
clients who maybe misbehave or limiting

309
00:20:26,230 --> 00:20:29,590
requests to a specific service overall.

310
00:20:30,330 --> 00:20:33,478
So, this is an important topic, and we will talk about it a.

311
00:20:33,484 --> 00:20:35,480
But more in the second half.

312
00:20:36,970 --> 00:20:40,534
So, coming back to queuing theory, let's have a couple of practical

313
00:20:40,582 --> 00:20:43,990
examples. So, in this simplified scenario,

314
00:20:44,070 --> 00:20:47,898
the execution time is 100 milliseconds. So what

315
00:20:47,984 --> 00:20:51,006
is the throughput? In this case,

316
00:20:51,188 --> 00:20:55,210
we produce ten marbles per second. Because we produce

317
00:20:55,290 --> 00:20:59,466
a single marble each 100 millisecond. The overall latency

318
00:20:59,578 --> 00:21:03,714
for processing all these eight marbles is 800 milliseconds. It's eight

319
00:21:03,752 --> 00:21:07,474
times 100 milliseconds. Right. Very simple. Now, what if

320
00:21:07,512 --> 00:21:11,118
I try to parallelize now and have a singular queue,

321
00:21:11,214 --> 00:21:15,342
but have doubled deburkers? Now I

322
00:21:15,496 --> 00:21:19,586
produce two marbles in each 100 millisecond.

323
00:21:19,698 --> 00:21:23,426
So I have 20 marbles per second as my throughput

324
00:21:23,618 --> 00:21:27,918
or as my departure rate. The latency is also housed

325
00:21:28,034 --> 00:21:32,570
because now I can produce

326
00:21:33,870 --> 00:21:38,730
four times two marbles overall

327
00:21:39,970 --> 00:21:44,590
in four times 100 milliseconds. And that comes up to 400 milliseconds.

328
00:21:45,570 --> 00:21:49,146
Okay, but what if I divide

329
00:21:49,178 --> 00:21:53,538
the work like that? So what? Instead of

330
00:21:53,704 --> 00:21:56,830
having a single parallelized operation,

331
00:21:56,910 --> 00:22:00,722
I try to split the work in

332
00:22:00,776 --> 00:22:05,806
two halves, which can be finished in two times 50 millisecond.

333
00:22:05,998 --> 00:22:10,102
Let's see the numbers. Now, I can produce a single marble within

334
00:22:10,156 --> 00:22:13,654
50 millisecond that comes up with this throughput as before,

335
00:22:13,772 --> 00:22:17,442
as in the previous example, as 20 marbles per second.

336
00:22:17,516 --> 00:22:21,226
So I still improved, doubled my throughput. But how is my

337
00:22:21,248 --> 00:22:24,890
latency changed? My latency will be still 800

338
00:22:24,960 --> 00:22:28,250
milliseconds because I need a single marble,

339
00:22:29,390 --> 00:22:32,574
100 millisecond to travel through from the left side to the right

340
00:22:32,612 --> 00:22:36,506
side. Right. I need two times 50 milliseconds for a single marble

341
00:22:36,618 --> 00:22:40,110
to become an orange marble, to become green marble.

342
00:22:40,930 --> 00:22:44,366
So interestingly, latency did not change, but throughput

343
00:22:44,398 --> 00:22:49,534
increased. And that's the magic, I think, of these reactive

344
00:22:49,582 --> 00:22:53,758
libraries that are becoming very popular these days. So by simply

345
00:22:53,854 --> 00:22:57,366
declaring my work in a different way,

346
00:22:57,468 --> 00:23:00,870
it allows me to have higher level of parallelization.

347
00:23:01,210 --> 00:23:05,042
By splitting my workload into smaller chunks

348
00:23:05,186 --> 00:23:09,174
and introducing more queues and processing

349
00:23:09,222 --> 00:23:13,014
them in smaller units.

350
00:23:13,062 --> 00:23:16,502
Overall it increases my parallelization,

351
00:23:16,646 --> 00:23:20,278
even though I'm not aware of that, because in the code

352
00:23:20,384 --> 00:23:24,366
everything seems sequential calls,

353
00:23:24,388 --> 00:23:28,174
right? So if I have a bottleneck, let's say, then the numbers are

354
00:23:28,212 --> 00:23:31,498
changed as following. So I still have ten marbles

355
00:23:31,514 --> 00:23:35,380
per second, because the bottleneck keeps me

356
00:23:36,950 --> 00:23:40,690
from processing a single marble within

357
00:23:40,760 --> 00:23:44,146
50 millisecond. And it

358
00:23:44,168 --> 00:23:47,394
just allows me to have a green marble in every

359
00:23:47,432 --> 00:23:50,646
100 millisecond, because that's where the bottleneck is.

360
00:23:50,748 --> 00:23:54,178
And in total, I need 100 plus 50 milliseconds

361
00:23:54,194 --> 00:23:58,250
for a single marble to go through. So my latency again is increased to

362
00:23:58,320 --> 00:24:01,750
1200 milliseconds. Were are many other scenarios,

363
00:24:01,830 --> 00:24:05,306
but I think you can do the math easily in

364
00:24:05,328 --> 00:24:09,290
your head. I have a few formulas

365
00:24:09,890 --> 00:24:12,858
that maybe not really precise,

366
00:24:12,954 --> 00:24:16,986
but it's enough to me to understand what's

367
00:24:17,018 --> 00:24:20,846
going on. What's really important, as you can see that the

368
00:24:20,868 --> 00:24:24,382
throughput is not really depending

369
00:24:24,446 --> 00:24:25,890
on the queue length.

370
00:24:29,110 --> 00:24:33,358
It's behaving a little bit differently than the latency.

371
00:24:33,534 --> 00:24:38,214
So as you've seen before in the example were,

372
00:24:38,252 --> 00:24:41,640
I was talking about this research with

373
00:24:42,010 --> 00:24:46,450
page load time and bandwidth, throughput and latency

374
00:24:46,610 --> 00:24:48,940
does not really depend on each other.

375
00:24:51,630 --> 00:24:55,002
What other things can you do with queues? What's very important

376
00:24:55,136 --> 00:24:58,906
is that for each queue you can provide its own quality of

377
00:24:58,928 --> 00:25:02,538
service. So these numbers can be independently provided

378
00:25:02,634 --> 00:25:07,390
for each queue. So let's say if you have producer,

379
00:25:07,810 --> 00:25:11,390
a single producer like the one who is producing the

380
00:25:11,460 --> 00:25:15,054
orange marbles, which needs higher demand,

381
00:25:15,182 --> 00:25:18,514
you can separate it to its own dedicated channel and

382
00:25:18,552 --> 00:25:22,260
it won't affect those which are producing. Want to process

383
00:25:22,630 --> 00:25:26,390
blue marbles and yellow marbles and won't choke the system

384
00:25:26,460 --> 00:25:28,710
so easily with its own requests.

385
00:25:29,610 --> 00:25:33,126
So by separating them to different channels, you can

386
00:25:33,148 --> 00:25:37,000
offer a separate quality of service to each channel.

387
00:25:37,790 --> 00:25:41,574
And this can be done in a couple of ways in microservice.

388
00:25:41,702 --> 00:25:46,794
First of all, a single service concentrates on one specific

389
00:25:46,912 --> 00:25:50,494
workload. And that forms its own queue and

390
00:25:50,532 --> 00:25:53,726
its own special way of optimizing for that kind

391
00:25:53,748 --> 00:25:57,534
of workload. It's independent from other services, but can

392
00:25:57,572 --> 00:25:59,920
be introduced also in a single service.

393
00:26:01,890 --> 00:26:06,094
If you have messaging and use multiple channels for multiple

394
00:26:06,222 --> 00:26:09,714
clients. I will have again, a detailed, practical example

395
00:26:09,832 --> 00:26:13,010
of how we use this feature.

396
00:26:14,890 --> 00:26:21,382
So, about Conway's law. Just very quickly thinking

397
00:26:21,436 --> 00:26:25,238
about Conway's law, I always just consider how many

398
00:26:25,404 --> 00:26:29,734
scenarios we have with

399
00:26:29,772 --> 00:26:32,978
communication, with single communication,

400
00:26:33,074 --> 00:26:36,950
considering teams. Okay, so in this scenario,

401
00:26:38,090 --> 00:26:42,302
in the two part, these two

402
00:26:42,356 --> 00:26:46,714
sides of the communication, a single team is controlling

403
00:26:46,762 --> 00:26:51,310
the change for each side. This is the easiest scenario

404
00:26:51,890 --> 00:26:56,646
because you can do whatever you want. You can proceed

405
00:26:56,698 --> 00:27:00,094
as fast as you would like to. Now we have these scenarios

406
00:27:00,142 --> 00:27:03,234
when a shared responsibility is on one of the other side.

407
00:27:03,272 --> 00:27:06,534
This is some sort of an anti pattern. This is

408
00:27:06,572 --> 00:27:10,486
not frequently used. Only companies use on

409
00:27:10,508 --> 00:27:14,118
those occasions when they don't really need to

410
00:27:14,204 --> 00:27:18,220
change so many things in a legacy service,

411
00:27:20,910 --> 00:27:25,366
or they don't really know how to separate the ownership

412
00:27:25,558 --> 00:27:28,220
of maybe a bigger chunk of code.

413
00:27:28,910 --> 00:27:32,446
This slows things down radically. This is

414
00:27:32,468 --> 00:27:35,962
when you have to be very careful. This is when you need to introduce nonbreaking

415
00:27:36,026 --> 00:27:40,126
changes or have the legacy endpoint live

416
00:27:40,228 --> 00:27:43,822
for a very long time. Now,

417
00:27:43,876 --> 00:27:47,054
there is this more healthier scenario when you have multiple

418
00:27:47,102 --> 00:27:50,146
consumers and you are the producer side or you are the

419
00:27:50,168 --> 00:27:53,826
consumer and there are multiple producers. This can happen in

420
00:27:53,848 --> 00:27:58,358
many situations. What's important to understand, I think that

421
00:27:58,524 --> 00:28:01,990
the service ownership does not necessarily come

422
00:28:02,060 --> 00:28:05,750
with the schema ownership. You are free to

423
00:28:05,900 --> 00:28:09,420
move the scheme ownership to the other side,

424
00:28:10,350 --> 00:28:13,754
back and forth, however you feel it's more

425
00:28:13,792 --> 00:28:17,034
suitable. This comes had in a couple

426
00:28:17,072 --> 00:28:20,894
of situations. So let's say that this is an event

427
00:28:20,932 --> 00:28:25,054
based system and who should control, in this case

428
00:28:25,172 --> 00:28:29,002
the schema, the message producers who are producing

429
00:28:29,146 --> 00:28:32,874
the events themselves, should they tell for the other teams

430
00:28:32,922 --> 00:28:36,610
that, yeah, there's going to be a schema change and be aware of that,

431
00:28:36,680 --> 00:28:40,274
and then just contact all the other teams, see if they

432
00:28:40,312 --> 00:28:43,780
are ready for accepting the new event.

433
00:28:44,150 --> 00:28:47,602
Or should we do it in a different way? Should the consumers

434
00:28:47,666 --> 00:28:51,446
be controlling the scheme ownership and tell the producer that,

435
00:28:51,548 --> 00:28:54,470
okay, we are expecting these kind of messages.

436
00:28:55,290 --> 00:28:59,286
From now on, we are accepting this kind

437
00:28:59,308 --> 00:29:02,394
of change, but not ready for another change and so on.

438
00:29:02,592 --> 00:29:06,262
There are tools and techniques

439
00:29:06,406 --> 00:29:10,454
on how to do this, and it helps visibility.

440
00:29:10,582 --> 00:29:13,774
It had testability helps with many things.

441
00:29:13,812 --> 00:29:17,722
There are also schema registries that you can introduce. You can switch

442
00:29:17,786 --> 00:29:21,710
from something that's schema s

443
00:29:21,780 --> 00:29:26,702
like traditional rest based API,

444
00:29:26,766 --> 00:29:30,462
which is offering just simple JSON

445
00:29:30,526 --> 00:29:34,420
based communication to a more

446
00:29:35,590 --> 00:29:40,002
conservative way of communicating, using strongly typed

447
00:29:40,146 --> 00:29:43,378
APIs like graphQL, GrPC,

448
00:29:43,474 --> 00:29:47,320
or maybe introducing schemas into events or messages as well.

449
00:29:49,950 --> 00:29:53,786
And there is this more most complicated scenario, when there

450
00:29:53,808 --> 00:29:55,980
are multiple teams in each side,

451
00:29:56,910 --> 00:30:01,200
producer and consumer side, that's when you need something more

452
00:30:01,970 --> 00:30:05,594
advanced, or the most advanced things for controlling schemas.

453
00:30:05,642 --> 00:30:10,378
Something like schema Federation, that's storing

454
00:30:10,474 --> 00:30:14,386
different versions and kinds of schemas and schema changes in a

455
00:30:14,408 --> 00:30:18,206
controlled way, most preferably in a venture controlled

456
00:30:18,238 --> 00:30:23,154
way. Okay, so this

457
00:30:23,192 --> 00:30:26,502
is where the first part ends. Now I would like to just

458
00:30:26,556 --> 00:30:30,262
quickly introduce you the toolbox or the

459
00:30:30,316 --> 00:30:34,200
things that I consider and jump right to the next

460
00:30:34,810 --> 00:30:38,714
section. And we will talk about practical examples and

461
00:30:38,752 --> 00:30:42,246
situations that I faced. And I would like to guide

462
00:30:42,278 --> 00:30:45,900
you how we improved situations each by each.

463
00:30:46,830 --> 00:30:51,246
So the tools that I use, usually you

464
00:30:51,268 --> 00:30:53,950
can do something like cqrs,

465
00:30:55,570 --> 00:30:58,698
meaning that you can separate the write and read path.

466
00:30:58,794 --> 00:31:01,982
If you need something special on the read side, or maybe something

467
00:31:02,036 --> 00:31:05,634
special on the right side, then we

468
00:31:05,672 --> 00:31:09,490
talked a lot about schemas. You can introduce contract based testing.

469
00:31:09,910 --> 00:31:13,300
It helps to move the schema ownership to the other side.

470
00:31:15,290 --> 00:31:18,646
Then you can introduce caching. We saw in

471
00:31:18,668 --> 00:31:22,550
the latency part how caching can improve the latency.

472
00:31:23,530 --> 00:31:27,154
With caching, you have to think about data freshness

473
00:31:27,282 --> 00:31:31,654
and multi write helps. Here I

474
00:31:31,692 --> 00:31:36,726
call multi write something that keeps

475
00:31:36,758 --> 00:31:40,558
the cached values fresh in a proactive way.

476
00:31:40,644 --> 00:31:44,942
So if you grab a fresh value from

477
00:31:44,996 --> 00:31:49,082
one side of your system because one of the clients

478
00:31:49,146 --> 00:31:52,754
needs that, you need to proactively write it to other

479
00:31:52,792 --> 00:31:56,562
cache instances to keep the data fresh and

480
00:31:56,616 --> 00:31:59,570
reduce the number of cache misses.

481
00:32:00,710 --> 00:32:04,082
Then you can switch from synchronous to

482
00:32:04,136 --> 00:32:07,474
asynchronous communication by keeping the original

483
00:32:07,522 --> 00:32:11,000
API, by introducing polling, by introducing maybe

484
00:32:11,370 --> 00:32:15,522
synchronous API that sends forward the request

485
00:32:15,586 --> 00:32:19,046
to a message queue and then just

486
00:32:19,148 --> 00:32:23,146
send simple response back to the client. There are

487
00:32:23,168 --> 00:32:27,302
also design practices or design principles

488
00:32:27,366 --> 00:32:30,810
that you can rely on like cloud native and twelve factor.

489
00:32:30,890 --> 00:32:34,270
I won't cover these, just thought it's good to mention

490
00:32:34,340 --> 00:32:38,174
them. Auto scaling can be

491
00:32:38,212 --> 00:32:43,130
effective in many ways and

492
00:32:43,300 --> 00:32:46,814
auto scaling has a positive effect on throughput,

493
00:32:46,862 --> 00:32:50,674
but not on latency. As we discussed, I talked about

494
00:32:50,712 --> 00:32:54,206
back pressure back in this section when talking about queuing

495
00:32:54,238 --> 00:32:57,318
theory, when you have higher

496
00:32:57,404 --> 00:32:59,720
arrival rate than departure rate,

497
00:33:01,050 --> 00:33:05,074
if you need large scale transactions, then you can introduce

498
00:33:05,122 --> 00:33:08,126
sagas in a microservice architecture. You can do it in a

499
00:33:08,128 --> 00:33:11,594
couple of ways. You can control the transaction either

500
00:33:11,632 --> 00:33:15,290
by using orchestration or choreography.

501
00:33:17,550 --> 00:33:21,134
You can introduce a service mesh. I think service meshes are

502
00:33:21,172 --> 00:33:25,002
important because there are many ways to fine tune the communication

503
00:33:25,066 --> 00:33:27,950
channels inside service mesh.

504
00:33:28,370 --> 00:33:30,670
It improves your observability.

505
00:33:32,130 --> 00:33:36,030
It helps you with certain kind of security aspects

506
00:33:36,110 --> 00:33:39,326
and you can introduce many resiliency patterns,

507
00:33:39,518 --> 00:33:43,666
us configurations inside service meshes like

508
00:33:43,848 --> 00:33:47,894
security breakers, timeouts, retries and so on and

509
00:33:47,932 --> 00:33:51,762
so forth. You can be conscious

510
00:33:51,826 --> 00:33:54,978
about your technology choices. So for instance,

511
00:33:55,074 --> 00:33:58,674
if you choose GRPC over traditional

512
00:33:58,722 --> 00:34:02,198
rest based communication, you can expect lower latency

513
00:34:02,294 --> 00:34:05,930
because usually GRPC has less

514
00:34:06,000 --> 00:34:08,490
round trips during a communication,

515
00:34:08,990 --> 00:34:13,114
during a request reasons, and payload is smaller

516
00:34:13,162 --> 00:34:16,910
because it's binary based. So probably you have more throughput.

517
00:34:18,610 --> 00:34:22,634
Messaging has many patterns, so if asynchronous communication

518
00:34:22,682 --> 00:34:25,906
is not enough for you, then you can introduce messaging in one

519
00:34:25,928 --> 00:34:29,966
of the sides. Switch from synchronous to asynchronous communications

520
00:34:30,158 --> 00:34:33,694
and then you are free to use all those messaging patterns

521
00:34:33,742 --> 00:34:37,078
which will increase the robustness of the communication itself

522
00:34:37,164 --> 00:34:39,240
and maybe help in a specific situation.

523
00:34:41,210 --> 00:34:44,694
If you choose your concurrency model well,

524
00:34:44,892 --> 00:34:48,406
it will have higher throughput,

525
00:34:48,598 --> 00:34:52,490
probably won't have a positive effect on the latency, but have higher

526
00:34:52,560 --> 00:34:56,406
throughput with less resource. So it will introduce

527
00:34:56,438 --> 00:35:00,730
more channels, more queues, but not necessarily more threads.

528
00:35:01,170 --> 00:35:04,942
This is very well used

529
00:35:04,996 --> 00:35:08,794
and I think a well settled technology coming in with reactive

530
00:35:08,842 --> 00:35:12,862
programming or coroutines, so they are good

531
00:35:12,916 --> 00:35:16,766
choices if you want to save resources with

532
00:35:16,788 --> 00:35:20,082
your communication. Then there are

533
00:35:20,136 --> 00:35:23,826
these resiliency patterns I think many people know

534
00:35:23,928 --> 00:35:27,750
because they are widely used in

535
00:35:27,900 --> 00:35:31,174
the microservice world. Also, service meshes offer them

536
00:35:31,212 --> 00:35:35,110
by default. There are also libraries that are providing

537
00:35:36,090 --> 00:35:39,158
most of these. So there are circuit breakers, bulkheads,

538
00:35:39,174 --> 00:35:43,306
retries, timeouts, just to name the most important

539
00:35:43,408 --> 00:35:47,654
parts. Timeout comes with all the libraries which are communicating

540
00:35:47,702 --> 00:35:51,550
with the network. Then you have observability

541
00:35:52,130 --> 00:35:55,566
to just review the whole and understand if you

542
00:35:55,588 --> 00:35:59,358
are improved or not. Now let's jump to the example

543
00:35:59,444 --> 00:36:03,062
part. So I will pick a couple of practical examples

544
00:36:03,146 --> 00:36:08,274
that I've met, and I will go through how

545
00:36:08,472 --> 00:36:12,530
in a specific situation things are improved

546
00:36:12,870 --> 00:36:16,326
with what kind of practices. Okay, so one

547
00:36:16,348 --> 00:36:20,438
of the examples I like is coming from the

548
00:36:20,524 --> 00:36:24,262
distributed database called Cassandra, and this is called

549
00:36:24,316 --> 00:36:27,974
this technique called rapid read protection. This is how it works.

550
00:36:28,092 --> 00:36:31,980
So let's say a client needs to read the data

551
00:36:32,350 --> 00:36:35,722
from the database, and it needs the data

552
00:36:35,776 --> 00:36:39,146
to be up to date. So then the client goes to this

553
00:36:39,168 --> 00:36:42,286
so called coordinator node that you can see on the top left,

554
00:36:42,388 --> 00:36:47,454
and the coordinator node then gets

555
00:36:47,492 --> 00:36:51,550
the data from each replicas, then aggregates the data

556
00:36:51,620 --> 00:36:55,410
based on its freshness, and then sends back the update data to the client.

557
00:36:56,550 --> 00:36:59,842
Now what happens if one of the requests is

558
00:36:59,896 --> 00:37:03,102
being slow? Instead of waiting for the request,

559
00:37:03,166 --> 00:37:06,854
the coordinator node is going to fire a so

560
00:37:06,892 --> 00:37:10,642
called backup request, hoping that this backup request

561
00:37:10,706 --> 00:37:13,970
will finish faster,

562
00:37:14,130 --> 00:37:19,850
and hoping that the coordinator node will

563
00:37:19,920 --> 00:37:23,530
send back the data to the client also faster.

564
00:37:24,110 --> 00:37:27,718
Why is this happening? Isn't this just a waste

565
00:37:27,734 --> 00:37:31,326
of effort and just a lot

566
00:37:31,348 --> 00:37:35,482
of complication? Shouldn't be more efficient

567
00:37:35,546 --> 00:37:38,926
if we just wait for

568
00:37:38,948 --> 00:37:42,446
that request to finish and just fail if there is no

569
00:37:42,468 --> 00:37:46,066
answer. So if you think about availability again,

570
00:37:46,168 --> 00:37:49,854
let's say we have 99% of availability for each node

571
00:37:49,902 --> 00:37:53,330
to be successful, successfully responding

572
00:37:55,030 --> 00:37:59,250
the payload to the coordinator node.

573
00:38:00,150 --> 00:38:03,890
In this case of 1% of a chance. When we have failure,

574
00:38:04,050 --> 00:38:07,606
we will still have 1% of chance, or 99% of the

575
00:38:07,628 --> 00:38:10,726
chance to be successful if we use a backup request.

576
00:38:10,758 --> 00:38:15,158
So the overall availability for this simplified scenario

577
00:38:15,254 --> 00:38:20,460
for the simplified query is increased. It's now around 99 99%.

578
00:38:21,250 --> 00:38:25,920
Okay? And if you would also investigate the P 99

579
00:38:26,290 --> 00:38:30,110
latency numbers, we will also see a decrease

580
00:38:30,690 --> 00:38:34,180
compared to the scenario if we would turn this off,

581
00:38:35,670 --> 00:38:39,650
because we won't see timeouts that often. It's true

582
00:38:39,720 --> 00:38:42,738
that overall we have some situations when

583
00:38:42,824 --> 00:38:47,320
it would have been better to wait for

584
00:38:47,770 --> 00:38:51,190
the answer to arrive instead of just going on with another

585
00:38:51,260 --> 00:38:55,074
backup request, because that can also fail. But overall at large scale,

586
00:38:55,122 --> 00:38:58,460
statistically we are still better, still performing better.

587
00:38:59,950 --> 00:39:03,466
Okay, now the other use case is coming from a scenario where

588
00:39:03,488 --> 00:39:06,902
I had to design a system with a read heavy workload,

589
00:39:06,966 --> 00:39:08,730
a very read heavy workload,

590
00:39:09,970 --> 00:39:14,746
and the writes were theoretically

591
00:39:14,938 --> 00:39:18,414
almost immutable. They were not changing at all.

592
00:39:18,532 --> 00:39:22,394
So we just created objects in this

593
00:39:22,452 --> 00:39:25,694
part of the architecture. We were hardly changing

594
00:39:25,742 --> 00:39:27,060
or updating them.

595
00:39:30,070 --> 00:39:33,342
I needed to reach a very low latency requirement.

596
00:39:33,486 --> 00:39:37,702
So this as

597
00:39:37,756 --> 00:39:41,446
the result meant that I had

598
00:39:41,468 --> 00:39:44,934
to prevent scenarios were I had to deal with cold cache with

599
00:39:44,972 --> 00:39:48,358
something that comes and reads

600
00:39:48,374 --> 00:39:52,026
up all the data from the database. And I

601
00:39:52,048 --> 00:39:55,610
couldn't use any distributed cache solution for this situation because

602
00:39:55,680 --> 00:39:59,580
it would have hurt latency so much that

603
00:40:01,170 --> 00:40:05,310
it would have been impossible to meet this low latency requirement.

604
00:40:06,690 --> 00:40:10,254
Were comes cqrs in the play. So instead of just trying

605
00:40:10,292 --> 00:40:13,918
to put everything in a single service and try to fine tune

606
00:40:14,014 --> 00:40:17,826
and optimize that and try to benchmark things and try

607
00:40:17,848 --> 00:40:20,450
to find a bottleneck and improve,

608
00:40:22,310 --> 00:40:25,966
you should think in a larger picture

609
00:40:26,078 --> 00:40:29,574
and you should use the techniques that I talked about. So we

610
00:40:29,612 --> 00:40:32,774
separated the bright path that you can see on the left because we are not

611
00:40:32,812 --> 00:40:35,830
really interested in the latency of the bright operations.

612
00:40:36,250 --> 00:40:40,442
It was not critical for these writes to

613
00:40:40,496 --> 00:40:43,690
happen immediately. So we went

614
00:40:43,760 --> 00:40:47,786
on to AWS queue and then continued the

615
00:40:47,808 --> 00:40:51,134
write to the database side. And it was

616
00:40:51,172 --> 00:40:55,022
true that we were having large write workloads, but because

617
00:40:55,076 --> 00:40:58,960
we were writing through a queue, it helps us to keep

618
00:41:00,050 --> 00:41:05,282
the write operations on the dynamodb lower and

619
00:41:05,336 --> 00:41:08,942
iron out these bursty

620
00:41:09,006 --> 00:41:12,510
operations and help to keep the write capacity unit

621
00:41:12,590 --> 00:41:15,686
lower than usual. Now for

622
00:41:15,708 --> 00:41:18,914
the reads, we introduced a distributed

623
00:41:18,962 --> 00:41:23,554
in memory cache solution with hazelcast that was also replicating

624
00:41:23,682 --> 00:41:27,158
between each read node. So this

625
00:41:27,324 --> 00:41:30,986
resulted us a couple of things. Like if I scale out

626
00:41:31,088 --> 00:41:34,554
and have fresh read node, it should not come up with

627
00:41:34,592 --> 00:41:38,406
an empty memory database. So immediately when a new read node

628
00:41:38,438 --> 00:41:41,754
comes in, it starts synchronizing with the other read

629
00:41:41,792 --> 00:41:45,050
nodes, which helps the data being fresh.

630
00:41:45,130 --> 00:41:48,330
Also, when we have a cache miss in one of the read nodes,

631
00:41:48,410 --> 00:41:52,614
it finds new data. By going to DynamoDB, it immediately

632
00:41:52,682 --> 00:41:56,066
proactively starts replicating this data and writing it

633
00:41:56,088 --> 00:42:00,370
to other need nodes. So that's how we solved

634
00:42:00,790 --> 00:42:04,942
keeping the cache warm with multi writes, with using replication

635
00:42:05,006 --> 00:42:08,934
as well. And the bright instance or

636
00:42:08,972 --> 00:42:12,790
the bright service responsible for the writes

637
00:42:13,290 --> 00:42:16,774
had nothing to do with Hazelcast was

638
00:42:16,812 --> 00:42:20,194
not aware of this complicated configuration

639
00:42:20,322 --> 00:42:22,410
of the memory cache solution.

640
00:42:22,750 --> 00:42:26,314
Also, the read instances did not

641
00:42:26,352 --> 00:42:30,286
have to have the SQS based libraries and did

642
00:42:30,308 --> 00:42:33,486
not have to do anything with SQS at

643
00:42:33,508 --> 00:42:37,166
all with that communication with the access and so on and so

644
00:42:37,188 --> 00:42:41,150
forth. So this simplifies your architecture overall even further.

645
00:42:42,870 --> 00:42:46,258
Okay, now with client libraries, I have a

646
00:42:46,264 --> 00:42:49,694
couple of stories. I treat client

647
00:42:49,742 --> 00:42:53,186
libraries as a double edged sword because I

648
00:42:53,208 --> 00:42:56,760
think it's very hard to design them in an effective way.

649
00:42:57,690 --> 00:43:00,770
They are not considering all these extensibility

650
00:43:00,850 --> 00:43:04,118
options and not all the features can be

651
00:43:04,124 --> 00:43:07,266
turned off in this specific

652
00:43:07,388 --> 00:43:08,490
client library.

653
00:43:10,750 --> 00:43:14,854
This library was included in many, many services as dependencies,

654
00:43:14,982 --> 00:43:18,602
as default dependencies, and this was the only way of use

655
00:43:18,656 --> 00:43:21,786
a specific shared feature of the whole architecture.

656
00:43:21,978 --> 00:43:25,198
This specific client library downloaded a

657
00:43:25,204 --> 00:43:29,214
zipped archive at startup from Amazon s three

658
00:43:29,332 --> 00:43:32,854
and started decompressing

659
00:43:32,922 --> 00:43:36,770
it during startup just to get the configuration data

660
00:43:36,920 --> 00:43:40,786
required for this library to work effectively. This was

661
00:43:40,808 --> 00:43:45,086
the single bottleneck for the startup of the whole service and

662
00:43:45,128 --> 00:43:48,502
unfortunately hidden behind

663
00:43:48,556 --> 00:43:52,070
the scenes, it messed up the startup for

664
00:43:52,220 --> 00:43:55,250
all the development environments.

665
00:43:55,410 --> 00:43:58,934
So all the developers suffered from the startup

666
00:43:58,982 --> 00:44:01,610
penalty introduced by this client library.

667
00:44:02,110 --> 00:44:05,820
Unfortunately, there was no way to turn off the library itself

668
00:44:06,350 --> 00:44:09,674
because the maintaining team wanted to keep it as

669
00:44:09,712 --> 00:44:12,798
simple as possible. They did not want

670
00:44:12,884 --> 00:44:17,854
anyone to change

671
00:44:17,972 --> 00:44:21,246
certain functionalities of the library and they thought that if

672
00:44:21,268 --> 00:44:24,690
they introduced too many features for the library for customization,

673
00:44:25,110 --> 00:44:28,622
then they will have hard times with the maintenance of the library

674
00:44:28,686 --> 00:44:32,482
because were could be plenty of ways teams are using

675
00:44:32,536 --> 00:44:36,354
that. So how

676
00:44:36,392 --> 00:44:37,700
we solve this problem.

677
00:44:39,910 --> 00:44:43,314
Okay, sorry. So another issue with client libraries

678
00:44:43,362 --> 00:44:47,240
was that it often messed up the deployments. So because

679
00:44:47,790 --> 00:44:52,454
health check was depending on the functionality

680
00:44:52,502 --> 00:44:55,946
of the library itself, midbed was failure during downloading the

681
00:44:55,968 --> 00:44:59,786
data. The service often just timed out and

682
00:44:59,888 --> 00:45:04,110
failed to meet the health check requirements. That messed up the deployment.

683
00:45:04,610 --> 00:45:08,186
So even deployments were negatively affected by the usage

684
00:45:08,218 --> 00:45:11,386
of this library. So how do we solve

685
00:45:11,418 --> 00:45:15,458
this issue? We just simply shift things on

686
00:45:15,464 --> 00:45:19,086
the left side during delivery, during deployment.

687
00:45:19,198 --> 00:45:23,054
What this meant for us, we just picked a mock

688
00:45:23,102 --> 00:45:27,302
s three. S three solution, containerized that using

689
00:45:27,356 --> 00:45:31,938
Docker and used it as a sidecar container. We prepared

690
00:45:32,034 --> 00:45:37,606
two mock libraries in

691
00:45:37,628 --> 00:45:41,466
this mock storage. One of them was empty, meaning that it

692
00:45:41,488 --> 00:45:45,100
was very fast to download and extract that, and the other one

693
00:45:45,550 --> 00:45:48,554
downloaded the same way,

694
00:45:48,592 --> 00:45:51,774
downloaded the data from the s three bucket, then extracted that and

695
00:45:51,812 --> 00:45:55,230
just had that certain configuration which

696
00:45:55,300 --> 00:45:58,734
was meaningful for us, and we put that into

697
00:45:58,852 --> 00:46:02,802
this mock container. The thing why this worked was very

698
00:46:02,856 --> 00:46:07,074
simple. It was because they

699
00:46:07,112 --> 00:46:10,786
did not have a feature switch or a queue switch for the library itself,

700
00:46:10,888 --> 00:46:15,118
but the URL was configurable because each specific

701
00:46:15,304 --> 00:46:19,190
environment had its own bucket configuration.

702
00:46:20,570 --> 00:46:24,598
Another way other teams solved this problem was introducing some kind of

703
00:46:24,684 --> 00:46:28,650
programmatic proxy, which was still trying

704
00:46:28,720 --> 00:46:32,890
to keep things on when this connection was not available.

705
00:46:33,040 --> 00:46:37,062
And behind the scenes try to kick in this client library

706
00:46:37,126 --> 00:46:40,574
to retry loading up the data and

707
00:46:40,692 --> 00:46:45,790
delaying the downloading of this data

708
00:46:45,860 --> 00:46:49,518
that was important for them. Okay. Rather thing,

709
00:46:49,604 --> 00:46:53,674
another technique that I want to talk about is what I call region

710
00:46:53,722 --> 00:46:57,374
pinning. This might be not necessarily the appropriate

711
00:46:57,422 --> 00:47:00,674
name for this technique, but this is how everybody was

712
00:47:00,712 --> 00:47:03,620
using that. So this is how I refer to that as well.

713
00:47:04,070 --> 00:47:06,070
Imagine the following scenario.

714
00:47:07,690 --> 00:47:12,034
We had to migrate to the cloud with our own whole architecture,

715
00:47:12,082 --> 00:47:15,510
the whole system, and then had to make it multiregion.

716
00:47:16,090 --> 00:47:19,770
And one of the problem

717
00:47:19,840 --> 00:47:23,366
was what we found was with shopping calls.

718
00:47:23,558 --> 00:47:26,874
So each service was, there was not

719
00:47:26,912 --> 00:47:31,146
a dedicated shopping cart service. Each service was managing

720
00:47:31,178 --> 00:47:35,562
the shopping cart through a shared database,

721
00:47:35,626 --> 00:47:39,406
namely a shared Cassandra cluster. So each service

722
00:47:39,508 --> 00:47:44,110
was reading the shopping cart from a shant Cassandra cluster through a library,

723
00:47:44,270 --> 00:47:47,906
and was updating the shopping cart when was necessary through a

724
00:47:48,008 --> 00:47:51,186
shared library, and then putting the

725
00:47:51,208 --> 00:47:57,094
data to this shared database. Okay. And we

726
00:47:57,132 --> 00:48:00,646
could not really rely on the replication lag because it

727
00:48:00,668 --> 00:48:03,954
was uncontrollable. So the minimum

728
00:48:04,002 --> 00:48:07,154
replication lag was around 60 milliseconds.

729
00:48:07,202 --> 00:48:10,342
But in practice, if you have more pressure on the database,

730
00:48:10,406 --> 00:48:14,774
this could have been easily increased to 500 600 milliseconds

731
00:48:14,902 --> 00:48:17,500
based on the load itself. Okay.

732
00:48:20,190 --> 00:48:24,046
And we were afraid that these shopping carts will disappear. So if one

733
00:48:24,068 --> 00:48:27,710
of the service or the traffic for one of the service is

734
00:48:27,780 --> 00:48:31,626
put to the right side during the user journey, we were afraid

735
00:48:31,658 --> 00:48:34,846
that when the shopping cart was being loaded because of

736
00:48:34,868 --> 00:48:38,354
the replication lag, it was not available, it was empty, or maybe

737
00:48:38,392 --> 00:48:41,922
it's not containing the up to date changes. So how do we

738
00:48:41,976 --> 00:48:45,834
distinguish between users that have up to date shopping cart

739
00:48:45,982 --> 00:48:49,574
in the left region from those that, let's say,

740
00:48:49,692 --> 00:48:52,998
don't have yet shopping cart or started their journey on

741
00:48:53,004 --> 00:48:57,000
the right region? That's where region pinning is coming into play.

742
00:48:57,390 --> 00:49:01,430
We flag the originating

743
00:49:01,510 --> 00:49:05,082
region for each user by using a cookie, and when

744
00:49:05,136 --> 00:49:08,406
moving those certain users who need their shopping

745
00:49:08,438 --> 00:49:11,742
cart, but started their user journey in the left

746
00:49:11,796 --> 00:49:15,262
region to the right side. Based on this cookie, we reach

747
00:49:15,316 --> 00:49:18,942
out through this white line,

748
00:49:19,076 --> 00:49:22,246
paying that 60 millisecond latency penalty,

749
00:49:22,378 --> 00:49:26,242
but loading the data consistently and having

750
00:49:26,296 --> 00:49:29,506
the shopping cart. So with this way we

751
00:49:29,528 --> 00:49:33,700
won't penalize calls the users because we are not going through

752
00:49:36,230 --> 00:49:39,430
this white line for all the users all the time.

753
00:49:39,500 --> 00:49:43,400
We just proactively select those users which needs

754
00:49:44,650 --> 00:49:48,298
more consistency but should be okay

755
00:49:48,384 --> 00:49:50,250
with this latest increase.

756
00:49:53,950 --> 00:49:57,910
In another example, I was using the same approach

757
00:49:58,070 --> 00:50:01,710
when I was designing an auction solution.

758
00:50:02,050 --> 00:50:05,274
So here we had to keep a strict

759
00:50:05,322 --> 00:50:10,302
ordering of the biddings. And no

760
00:50:10,356 --> 00:50:13,474
matter which message provider I looked at,

761
00:50:13,592 --> 00:50:17,394
I figured out that there is no way to keep

762
00:50:17,432 --> 00:50:21,566
the messages in order when I rely on synchronization

763
00:50:21,678 --> 00:50:25,858
between regions. Okay, so if the auction started

764
00:50:25,944 --> 00:50:29,054
on the left side in the

765
00:50:29,192 --> 00:50:33,190
kafka cluster on the left side, and somebody wanted

766
00:50:33,260 --> 00:50:36,726
to participate in that auction on

767
00:50:36,748 --> 00:50:40,262
the other region, they had to pay for this latency

768
00:50:40,326 --> 00:50:44,106
penalty proactively, but right to

769
00:50:44,128 --> 00:50:47,274
the kafka cluster still on the left side.

770
00:50:47,472 --> 00:50:51,402
And then for other services or participants

771
00:50:51,466 --> 00:50:54,814
which were not latency sensitive, the data was

772
00:50:54,852 --> 00:50:58,366
still available, maybe later in a synchronous way through the

773
00:50:58,388 --> 00:51:02,750
mirror maker on the right side. So they could have continued

774
00:51:04,150 --> 00:51:07,794
processing this data or maybe showing this data a bit

775
00:51:07,832 --> 00:51:11,762
later. But those who wanted to participate in bidding had

776
00:51:11,816 --> 00:51:15,490
to write consistently on the same region

777
00:51:15,570 --> 00:51:19,174
as the auction is originated. This is similar

778
00:51:19,292 --> 00:51:22,674
to the things we used to in the gaming

779
00:51:22,722 --> 00:51:26,422
world. So if there's a game started in

780
00:51:26,476 --> 00:51:29,974
one certain region, then when other players

781
00:51:30,022 --> 00:51:33,462
are participating, they have to consider the latency penalty

782
00:51:33,526 --> 00:51:36,886
and maybe not perform that well. But this keeps

783
00:51:36,918 --> 00:51:40,446
the game consistent. Okay, the last example is

784
00:51:40,468 --> 00:51:44,138
coming from the journey of optimizing

785
00:51:44,314 --> 00:51:48,110
a single service in a couple of iterations.

786
00:51:52,870 --> 00:51:56,594
It. So I would explain the

787
00:51:56,632 --> 00:52:00,242
behavior and the functionality of the service a bit more at first.

788
00:52:00,296 --> 00:52:03,662
Okay, so this service had two endpoints.

789
00:52:03,806 --> 00:52:07,000
First of all, it was a very simple key value store.

790
00:52:11,770 --> 00:52:15,880
You can see the value endpoint on the right side of the service,

791
00:52:16,970 --> 00:52:20,346
which reads the values for a certain key given.

792
00:52:20,448 --> 00:52:23,834
Okay? And that's coming to specific

793
00:52:23,952 --> 00:52:27,814
DynamoDB table, which has its own read capacity

794
00:52:27,862 --> 00:52:31,870
unit defined. But we have also another endpoint.

795
00:52:32,610 --> 00:52:36,606
This is how things were when we got the service,

796
00:52:36,708 --> 00:52:40,766
when we started maintaining that. So this is what we

797
00:52:40,788 --> 00:52:44,690
kept. This was the endpoint of the recent keys.

798
00:52:45,110 --> 00:52:49,278
What designpoint did was that it gave back to the clients

799
00:52:49,454 --> 00:52:53,426
the keys that were being accessed by that specific

800
00:52:53,528 --> 00:52:57,494
client in the last seven days. That went off

801
00:52:57,532 --> 00:53:01,522
to another table, which was statistical

802
00:53:01,586 --> 00:53:05,254
table that you can see on the left, which had their own read

803
00:53:05,292 --> 00:53:08,982
and write capacity unit defined. Now, both tables had

804
00:53:09,036 --> 00:53:12,378
auto scaling configured, but there was a problem. As you can see on

805
00:53:12,384 --> 00:53:15,882
the right side diagram, there were one specific client which was

806
00:53:15,936 --> 00:53:19,286
firing up 15,000 requests to this values

807
00:53:19,318 --> 00:53:23,006
endpoint, because it had 15,000 keys in

808
00:53:23,028 --> 00:53:26,734
this usually being used. And it

809
00:53:26,772 --> 00:53:30,702
went with one single request to this recent keys. Endpoint found

810
00:53:30,756 --> 00:53:34,526
that in the last seven days it was using these 15,000 keys,

811
00:53:34,638 --> 00:53:37,970
and iteratively it went through calls, the keys bashing

812
00:53:38,470 --> 00:53:42,050
sequentially, these values endpoint increasing traffic

813
00:53:42,390 --> 00:53:45,880
and the dynamodb auto scaling was not catching up.

814
00:53:48,010 --> 00:53:51,734
There was a time window defined by this

815
00:53:51,852 --> 00:53:55,826
red line that you can see on the screen when DynamoDB

816
00:53:55,858 --> 00:53:59,402
was throttling because it failed to meet the capacity needs

817
00:53:59,536 --> 00:54:02,582
for the traffic. And unfortunately,

818
00:54:02,726 --> 00:54:05,610
when the traffic burst was over,

819
00:54:05,760 --> 00:54:10,130
auto scaling increased for DynamoDB, but then the increase decreased

820
00:54:10,230 --> 00:54:14,430
immediately after that. Okay, now one of the problem was that

821
00:54:14,580 --> 00:54:18,474
the statistics table was written

822
00:54:18,602 --> 00:54:22,458
sequentially before the read happens, when getting the values,

823
00:54:22,554 --> 00:54:26,034
just to keep the statistics fresh. So in

824
00:54:26,072 --> 00:54:29,326
maintenance, during maintenance, we had to keep the write capacity units

825
00:54:29,358 --> 00:54:32,722
the same as the read capacity units for the two

826
00:54:32,776 --> 00:54:36,214
tables, because auto scaling was hard to

827
00:54:36,252 --> 00:54:42,246
fine tune, and because if

828
00:54:42,268 --> 00:54:46,114
the write capacity units are lower, we are just failing

829
00:54:46,162 --> 00:54:50,170
by writing the statistics table. And the reads were also failing.

830
00:54:50,510 --> 00:54:54,134
So what we did first was separating this critical path

831
00:54:54,182 --> 00:54:58,282
from the rest. So we put

832
00:54:58,336 --> 00:55:01,862
the update operation just in a different thread

833
00:55:02,006 --> 00:55:05,520
and just deployed solution. Things worked

834
00:55:06,130 --> 00:55:09,662
bit better than before, but after a short

835
00:55:09,716 --> 00:55:13,426
period of time, there were other problems were seen,

836
00:55:13,528 --> 00:55:17,234
that for some reason the

837
00:55:17,272 --> 00:55:20,910
memory consumption of the services were increasing

838
00:55:21,070 --> 00:55:24,990
and the container orchestrator started killing the services

839
00:55:25,080 --> 00:55:30,166
themselves. And the

840
00:55:30,188 --> 00:55:33,670
reason of that was in the flow of the implementation.

841
00:55:35,450 --> 00:55:39,286
This was the

842
00:55:39,308 --> 00:55:43,110
way we created the thread that

843
00:55:43,180 --> 00:55:46,594
was dealing with this change. This is in Java,

844
00:55:46,642 --> 00:55:50,206
but you don't necessarily have to understand Java to understand this use case.

845
00:55:50,348 --> 00:55:54,186
We just created a separate

846
00:55:54,218 --> 00:55:58,254
thread that had a separate queue that is

847
00:55:58,292 --> 00:56:01,962
processing these requests when they are coming in and updating the statistics

848
00:56:02,026 --> 00:56:05,060
table. Now where comes the problem?

849
00:56:05,510 --> 00:56:09,954
What do you think? Where is the problem with

850
00:56:09,992 --> 00:56:13,806
this implementation that's causing the memory increase and the killing

851
00:56:13,838 --> 00:56:17,320
of the instances from time to time and restarting them?

852
00:56:18,410 --> 00:56:21,954
Well, it's not obvious because calls the implementation details,

853
00:56:22,002 --> 00:56:24,838
but this is the single place when there is a problem.

854
00:56:24,924 --> 00:56:28,810
So very often Java

855
00:56:29,790 --> 00:56:34,170
old school threaded implementations are coming with this unbounded queue,

856
00:56:34,510 --> 00:56:38,314
meaning you have a limitless queue. And what happens when you have

857
00:56:38,512 --> 00:56:41,966
an increased arrival rate and your departure rate

858
00:56:41,988 --> 00:56:45,950
is much lower? You will have this unbounded queue filled up,

859
00:56:46,020 --> 00:56:49,806
your latency will increase up to infinity, and your

860
00:56:49,828 --> 00:56:53,458
memory consumption will also increase up

861
00:56:53,464 --> 00:56:58,306
to infinity, up to where

862
00:56:58,328 --> 00:57:02,050
you can hold this data, because you

863
00:57:02,120 --> 00:57:05,060
have a very huge queue sitting there for no reason.

864
00:57:06,070 --> 00:57:09,826
And actually this is not even true, because this is not an unbounded

865
00:57:09,858 --> 00:57:13,062
queue, it's not erasing data automatically, it gets

866
00:57:13,116 --> 00:57:17,026
full, it's just a very huge queue because it's

867
00:57:17,058 --> 00:57:19,626
implemented by using the maximum value.

868
00:57:19,728 --> 00:57:23,210
Java is declaring for integers,

869
00:57:23,550 --> 00:57:27,210
right? So we can do better than that. So we

870
00:57:27,280 --> 00:57:31,178
iterated with this implementation and we just simply

871
00:57:31,354 --> 00:57:35,182
challenges to a more sophisticated solution. We started using

872
00:57:35,236 --> 00:57:38,522
resiliency for J, which is a library

873
00:57:38,666 --> 00:57:42,742
for implementing resiliency patterns in Java,

874
00:57:42,906 --> 00:57:46,930
and we wrapped the call with a bulkhead which now

875
00:57:47,000 --> 00:57:51,410
had a limited queue capacity, up to 25 items.

876
00:57:52,390 --> 00:57:55,986
And when that queue was full, bulkhead was throwing

877
00:57:56,018 --> 00:57:59,702
an exception. Now we could have exception because

878
00:57:59,756 --> 00:58:03,414
of two cases. First of all, because DynamoDB is starting

879
00:58:03,532 --> 00:58:06,966
throttling, it's just rejecting our request. Or the

880
00:58:06,988 --> 00:58:10,554
bulkhead was full, so were wrapped this whole thing, this whole

881
00:58:10,592 --> 00:58:14,598
calls into a circuit breaker, and the circuit breaker

882
00:58:14,694 --> 00:58:17,830
just opened when it saw these two exceptions,

883
00:58:17,990 --> 00:58:21,434
give the whole thing a pause and then started again updating

884
00:58:21,482 --> 00:58:25,582
the statistics. And this

885
00:58:25,636 --> 00:58:30,174
helped us recover from the situation from

886
00:58:30,212 --> 00:58:33,478
before. We did not have these memory

887
00:58:33,514 --> 00:58:36,946
issues, the clients did not really notice anything

888
00:58:37,048 --> 00:58:41,486
at all. And we looked actually at the metrics of the circuit breaker,

889
00:58:41,598 --> 00:58:46,434
looked at the statistics of when they were opened

890
00:58:46,482 --> 00:58:49,880
up, and it was not a big number.

891
00:58:50,730 --> 00:58:54,054
So statistics not really suffered because of that.

892
00:58:54,172 --> 00:58:57,602
And normally behaving clients could just

893
00:58:57,676 --> 00:59:01,322
keep up with their normal operation, maybe having a couple

894
00:59:01,376 --> 00:59:06,842
of more cache misses than usual, but with

895
00:59:06,896 --> 00:59:10,822
metrics, and with this solution and investigating metrics,

896
00:59:10,886 --> 00:59:14,266
we thought we were fine, so we dropped the right copper units.

897
00:59:14,378 --> 00:59:18,240
Finally, for a statistics table that did not have to match

898
00:59:18,610 --> 00:59:23,006
with the read part of

899
00:59:23,028 --> 00:59:26,800
the values table. Okay.

900
00:59:27,170 --> 00:59:30,702
But we really wanted to reduce also the read capacity unit

901
00:59:30,766 --> 00:59:34,462
for the reads. So again, let's go back to the baseline

902
00:59:34,526 --> 00:59:37,878
and talk about what we have with a bit more detail. So we

903
00:59:37,884 --> 00:59:42,194
have 15,000 items coming in, 15,000 requests

904
00:59:42,242 --> 00:59:44,950
like almost instantly.

905
00:59:45,850 --> 00:59:49,786
And then we had these services packed in

906
00:59:49,808 --> 00:59:52,940
an auto scaling group that was cpu based.

907
00:59:53,310 --> 00:59:57,894
And then Dynamodb was throttling and giving us back HTTP

908
00:59:57,942 --> 01:00:01,422
400 errors on any case when we

909
01:00:01,476 --> 01:00:03,630
breached our read capacity unit.

910
01:00:08,130 --> 01:00:11,710
So what if we do retry? So what if we have this HTTP 400

911
01:00:11,780 --> 01:00:15,122
errors? We just retry the request and hope now that the read

912
01:00:15,176 --> 01:00:18,020
capacity unit was catching up,

913
01:00:19,190 --> 01:00:22,542
unfortunately, this was not introducing any fairness

914
01:00:22,606 --> 01:00:27,042
to this whole solution. So when this misbehaving

915
01:00:27,106 --> 01:00:29,990
client came with this 15,000 keys,

916
01:00:31,530 --> 01:00:35,298
it just choked the whole system with its own request.

917
01:00:35,394 --> 01:00:39,894
Other clients had to wait until DynamoDB

918
01:00:39,942 --> 01:00:43,562
was catching up to get their own answers. Okay,

919
01:00:43,616 --> 01:00:47,494
so it was not introducing any fairness for this whole scenario.

920
01:00:47,542 --> 01:00:51,822
We wanted to do better. We tried also built

921
01:00:51,876 --> 01:00:56,266
in rate limiting, but we did not go into production because for obvious reasons,

922
01:00:56,298 --> 01:00:59,354
it was not working very well. So you can introduce

923
01:00:59,482 --> 01:01:03,450
a quite okay ish implementation with resilience

924
01:01:03,530 --> 01:01:07,090
for j that also has rate limiting. It's quite precise.

925
01:01:07,510 --> 01:01:10,706
So you can have 40 operations per second for

926
01:01:10,728 --> 01:01:14,414
each instance. Now with two instances, you have in total of 80

927
01:01:14,462 --> 01:01:18,142
operations per second. Now, when the auto scaling

928
01:01:18,206 --> 01:01:21,702
kicks in immediately, you have another instance having

929
01:01:21,756 --> 01:01:25,346
40 operations per second, which now also increasing

930
01:01:25,378 --> 01:01:29,002
your rate limit and your capacity, which is not true, of course,

931
01:01:29,056 --> 01:01:32,646
because the original capacity is determined

932
01:01:32,678 --> 01:01:36,234
by the dynamoDb's current capacity and by its

933
01:01:36,272 --> 01:01:38,380
auto scaling characteristics. Okay,

934
01:01:40,770 --> 01:01:42,720
we tried many other things,

935
01:01:43,490 --> 01:01:47,054
but one of these was this one obviously did not

936
01:01:47,092 --> 01:01:50,910
work well. We could have put

937
01:01:50,980 --> 01:01:54,526
rate limiting into something that's used centrally,

938
01:01:54,638 --> 01:01:57,586
namely into the service mesh. So in this case,

939
01:01:57,688 --> 01:02:02,574
istio was also offering rate limiting. It was using redis

940
01:02:02,622 --> 01:02:05,750
to keep the states of each client

941
01:02:06,810 --> 01:02:10,630
to control the rate limits. Problem was that it

942
01:02:10,700 --> 01:02:14,374
introduced an API change. So instead of giving these requests a

943
01:02:14,412 --> 01:02:18,366
post or maybe slowing down the clients, which are misbehaving

944
01:02:18,498 --> 01:02:23,062
similarly to what back pressure does, it immediately

945
01:02:23,126 --> 01:02:26,810
gave them another kind of HTTP response. And we thought

946
01:02:26,880 --> 01:02:30,454
that something, that it caused more trouble

947
01:02:30,502 --> 01:02:33,726
than solve solutions. So we

948
01:02:33,748 --> 01:02:37,374
did not go on with this change. Instead of that, we went

949
01:02:37,412 --> 01:02:42,654
back to queuing theory. So if we simplify this into

950
01:02:42,692 --> 01:02:47,010
a simpler queue, this is what happens. So we have 15,000 items coming

951
01:02:47,080 --> 01:02:51,074
in, you have couple of executors that are on the right side,

952
01:02:51,272 --> 01:02:54,814
and the throughput, and to determine the throughput,

953
01:02:54,862 --> 01:02:58,646
you need the latency. So the overall latency was

954
01:02:58,748 --> 01:03:02,098
equal to the timeout configuration of the client,

955
01:03:02,274 --> 01:03:05,894
which was not easy to figure out, or not difficult to figure out,

956
01:03:05,932 --> 01:03:09,226
sorry. Because it was the default being used. I don't

957
01:03:09,248 --> 01:03:12,806
know where this number is coming from, but for every library

958
01:03:12,838 --> 01:03:16,874
it looks like timeout is 30 seconds. So we

959
01:03:16,912 --> 01:03:20,334
have 30 seconds to consume and

960
01:03:20,372 --> 01:03:23,114
problems these 15,000 items.

961
01:03:23,242 --> 01:03:26,442
This gives us the overall throughput of 500 operations

962
01:03:26,506 --> 01:03:29,994
per second, regardless of the number of executors.

963
01:03:30,122 --> 01:03:33,454
So with two instances working on

964
01:03:33,492 --> 01:03:37,262
that, we need to complete each operation in four milliseconds,

965
01:03:37,406 --> 01:03:41,246
which seems to be nearly impossible. We do not really want to bother

966
01:03:41,278 --> 01:03:44,754
with optimizing the whole thing, but if you scale this out

967
01:03:44,792 --> 01:03:48,742
to five nodes, to five workers, it's a more

968
01:03:48,796 --> 01:03:52,706
user friendly number. It's now ten milliseconds, which seem

969
01:03:52,738 --> 01:03:56,262
to be doable. So we think that if we can

970
01:03:56,316 --> 01:04:00,250
slow down these 15,000 requests

971
01:04:00,750 --> 01:04:04,422
not to be processed immediately,

972
01:04:04,486 --> 01:04:08,150
but instead of them to be processed within 30 seconds,

973
01:04:08,310 --> 01:04:12,270
close to a number that's 30 seconds with five worker,

974
01:04:13,330 --> 01:04:16,766
if we have the execution duration of ten milliseconds for

975
01:04:16,788 --> 01:04:20,762
each worker, we can do it in a sensible

976
01:04:20,826 --> 01:04:24,114
way. So we tried to

977
01:04:24,152 --> 01:04:26,820
find a technology that allows us to do that.

978
01:04:28,070 --> 01:04:32,066
We were looking towards RabbitMQ because a

979
01:04:32,088 --> 01:04:35,526
couple of very interesting features. RabbitMQ has

980
01:04:35,548 --> 01:04:39,106
this queue overflow behavior. So if you set this overflow

981
01:04:39,138 --> 01:04:44,850
setting for a queue, then it will immediately reject

982
01:04:44,930 --> 01:04:48,614
and not consume those requests that was put into the queue.

983
01:04:48,662 --> 01:04:53,020
So I think it will wait on the client side until

984
01:04:53,470 --> 01:04:57,382
the queue still has capacity and until the consumers are catching

985
01:04:57,446 --> 01:05:00,826
up instead of failing them. It's failing after a

986
01:05:00,848 --> 01:05:05,166
certain amount of time. It also have another rate limiting or

987
01:05:05,348 --> 01:05:08,686
flow rate behavior, but it's bound to

988
01:05:08,708 --> 01:05:12,446
the memory and cpusage to RabbitMQ that you can see on the

989
01:05:12,468 --> 01:05:16,494
middle left on the screen. That's very hard to control. This is

990
01:05:16,612 --> 01:05:21,374
not what we were looking for, but still was quite promising then.

991
01:05:21,412 --> 01:05:25,010
What's important is that you can define your prefetch settings

992
01:05:25,130 --> 01:05:28,262
by queues that you can see on the top right

993
01:05:28,316 --> 01:05:32,262
corner. So you can have a single channel of

994
01:05:32,316 --> 01:05:36,018
connection that's connecting through different consumers

995
01:05:36,034 --> 01:05:39,466
to different queues, and have different settings for

996
01:05:39,488 --> 01:05:43,306
each queue. So if you have one of

997
01:05:43,328 --> 01:05:46,540
the clients coming into one queue, you can have

998
01:05:48,030 --> 01:05:51,834
a specific configuration for that single queue.

999
01:05:51,882 --> 01:05:55,674
Now for another queue, you can either have the same or different configuration

1000
01:05:55,722 --> 01:05:59,274
as well, and you have many options to acknowledge

1001
01:05:59,322 --> 01:06:03,074
the request. So when DynamoDB starts throttling, so you just

1002
01:06:03,112 --> 01:06:06,734
simply acknowledge or not negatively

1003
01:06:06,782 --> 01:06:10,994
acknowledge or reject the

1004
01:06:11,032 --> 01:06:14,434
message from the worker side and you can retry with

1005
01:06:14,472 --> 01:06:17,030
the next interactions or with the next worker.

1006
01:06:17,930 --> 01:06:21,814
So here was our setup. Basically we had a service or

1007
01:06:21,852 --> 01:06:26,018
the array of services now having an auto scaling

1008
01:06:26,034 --> 01:06:30,818
group. This was using so

1009
01:06:30,844 --> 01:06:34,298
calls fake boundary. This is how I calls that. Maybe there's a

1010
01:06:34,304 --> 01:06:38,054
better name. And instead of being synchronous, it was asynchronous,

1011
01:06:38,182 --> 01:06:42,282
but it used request reply queues and we separated

1012
01:06:42,426 --> 01:06:46,762
the request by each API key. So each client fortunately

1013
01:06:46,826 --> 01:06:50,686
forwarded their own API key in the header and

1014
01:06:50,788 --> 01:06:54,034
in each queue in each channel. We had

1015
01:06:54,072 --> 01:06:58,046
the same configuration, the same configuration of overflow

1016
01:06:58,078 --> 01:07:02,238
settings, the same configuration of prefetch rate, and each worker

1017
01:07:02,334 --> 01:07:06,174
connected to all the queues at the same time

1018
01:07:06,232 --> 01:07:08,760
and have their own auto scaling group.

1019
01:07:10,010 --> 01:07:13,186
Creating a new queue if we saw a fresh API

1020
01:07:13,218 --> 01:07:17,446
key was not a problem because the configuration was in service itself.

1021
01:07:17,548 --> 01:07:21,506
So this had something like global configuration

1022
01:07:21,618 --> 01:07:25,210
available and connecting to a new queue

1023
01:07:25,710 --> 01:07:29,082
when we see a new queue from the worker side seemed to be a bit

1024
01:07:29,136 --> 01:07:32,606
more complicated but doable, because RevitMQ is

1025
01:07:32,628 --> 01:07:35,438
offering management API as well,

1026
01:07:35,604 --> 01:07:38,490
which helps you discover if there's a fresh queue.

1027
01:07:38,570 --> 01:07:42,074
And opening up a new connection seemed

1028
01:07:42,122 --> 01:07:46,190
to be, and having a new unit

1029
01:07:46,350 --> 01:07:50,066
which is consuming that connection seemed to be

1030
01:07:50,168 --> 01:07:53,794
not a big deal. And then

1031
01:07:53,992 --> 01:07:57,394
we could connect to Dynamodb and reject

1032
01:07:57,442 --> 01:08:01,494
the request if there's throttling, but hope that this

1033
01:08:01,612 --> 01:08:05,286
will have the effect what we desired for. Again, we need

1034
01:08:05,308 --> 01:08:09,482
to slow down misbehaving clients. So if there's more orange marble coming

1035
01:08:09,536 --> 01:08:13,050
in, we have to say after a while that orange marbles have to wait.

1036
01:08:13,120 --> 01:08:17,210
Why? Processing the blue and the yellow marbles in their own pace,

1037
01:08:18,350 --> 01:08:21,646
and this is the metrics that I've got.

1038
01:08:21,828 --> 01:08:25,162
Unfortunately this is not from the real scenario.

1039
01:08:25,226 --> 01:08:28,878
I had to rebuild it in a sandbox for certain

1040
01:08:28,964 --> 01:08:32,922
reasons, but it's available under my GitHub profile,

1041
01:08:32,986 --> 01:08:36,194
so you can try it by your own if you want to. So this

1042
01:08:36,232 --> 01:08:39,662
was the baseline of direct reads. Now directly

1043
01:08:39,726 --> 01:08:42,866
coming from the service of the database, you can see that we have

1044
01:08:42,888 --> 01:08:46,274
the throughput of 300 operations per second, and we

1045
01:08:46,312 --> 01:08:50,310
complete all the reads in 15 seconds.

1046
01:08:50,810 --> 01:08:53,830
And the response time is around ten milliseconds.

1047
01:08:54,410 --> 01:08:57,842
This is quite good because it's quite

1048
01:08:57,996 --> 01:09:01,306
close to this scenario with five workers that you want

1049
01:09:01,328 --> 01:09:04,714
to reach, and the response time distribution is

1050
01:09:04,752 --> 01:09:08,682
very tight. So it's everything from going

1051
01:09:08,736 --> 01:09:12,382
to zero to one eight.

1052
01:09:12,516 --> 01:09:16,426
I don't know what the unit is, maybe this graph

1053
01:09:16,458 --> 01:09:20,314
is messed up, sorry for that. But for the other diagrams

1054
01:09:20,362 --> 01:09:24,366
you can see that these numbers change at least. So with single worker

1055
01:09:24,398 --> 01:09:28,018
what we can see now is that rate limiting is working

1056
01:09:28,104 --> 01:09:31,394
as expected or the back pressure is working as expected. So now

1057
01:09:31,432 --> 01:09:35,022
instead of doing

1058
01:09:35,096 --> 01:09:39,046
all these reads within 15 seconds we

1059
01:09:39,068 --> 01:09:42,630
just give it a pause and we do it instead of that

1060
01:09:42,780 --> 01:09:45,798
in I think two minutes,

1061
01:09:45,884 --> 01:09:49,514
yeah, in around two minutes. So it's not

1062
01:09:49,632 --> 01:09:54,006
reaching that 30 seconds goal what we aimed for, but it's

1063
01:09:54,038 --> 01:09:58,858
giving the request a pause when we have more process

1064
01:09:59,024 --> 01:10:02,814
than what we want to. The reasons time average is a bit

1065
01:10:02,852 --> 01:10:04,190
higher than expected,

1066
01:10:06,850 --> 01:10:10,334
but at least

1067
01:10:10,372 --> 01:10:13,582
we have, this is just a single worker, so at least

1068
01:10:13,636 --> 01:10:17,426
we have this set up as we

1069
01:10:17,448 --> 01:10:20,606
want to. And you can see this from the response time distribution.

1070
01:10:20,798 --> 01:10:24,114
So the response time distribution now is going to up

1071
01:10:24,152 --> 01:10:27,526
to 14 units. So it's better than

1072
01:10:27,548 --> 01:10:31,094
it was before. And now with five workers we tried it out also with

1073
01:10:31,132 --> 01:10:35,510
five workers we saw that with five workers

1074
01:10:35,850 --> 01:10:38,982
we succeeded to reach our goals.

1075
01:10:39,126 --> 01:10:43,146
So this exactly takes 30

1076
01:10:43,248 --> 01:10:47,018
seconds as expected. And interestingly in

1077
01:10:47,184 --> 01:10:50,634
one point we measured even a higher throughput. So the original

1078
01:10:50,682 --> 01:10:54,110
throughput was very close to the

1079
01:10:54,260 --> 01:10:57,358
baseline and in one case it was a

1080
01:10:57,364 --> 01:11:00,990
bit even higher, even 500 operations per second.

1081
01:11:01,140 --> 01:11:04,946
It also shows that throughput has nothing to

1082
01:11:04,968 --> 01:11:08,258
do really with latency. It has some relations

1083
01:11:08,344 --> 01:11:12,126
with latency, but we are able to meet even higher throughput

1084
01:11:12,318 --> 01:11:15,786
even if we have higher latency for some of the clients,

1085
01:11:15,838 --> 01:11:19,826
for some of the channels and the worker execution time and response

1086
01:11:19,858 --> 01:11:23,382
time. Now, especially if you see the worker execution time,

1087
01:11:23,436 --> 01:11:27,810
you can see that now it's getting short to getting to

1088
01:11:27,980 --> 01:11:32,140
ten milliseconds at the end where we want it to be.

1089
01:11:32,510 --> 01:11:35,866
And the response time distribution is again a bit

1090
01:11:35,968 --> 01:11:39,274
better distributed than before. So this was

1091
01:11:39,312 --> 01:11:43,182
quite promising for us. So that's all I wanted

1092
01:11:43,236 --> 01:11:46,622
to say and present to you. Thank you so much

1093
01:11:46,676 --> 01:11:50,334
for listening again. I was orthes Margaret and

1094
01:11:50,372 --> 01:11:54,290
I work as associate chief software engineer at EPAm.

1095
01:11:54,870 --> 01:11:58,290
If you have questions related to the things I talked about,

1096
01:11:58,360 --> 01:12:02,494
just feel free to reach out to me by using either Twitter

1097
01:12:02,542 --> 01:12:06,114
or LinkedIn or feel free to visit my GitHub

1098
01:12:06,162 --> 01:12:09,938
profile. But I have an availability simulator

1099
01:12:10,034 --> 01:12:13,942
that helps you to get these availability numbers that I was talking about

1100
01:12:14,076 --> 01:12:17,586
and have this example sandbox of back pressure

1101
01:12:17,618 --> 01:12:22,006
and ray limiting I was presenting to you and

1102
01:12:22,028 --> 01:12:25,414
I get plenty of plenty of references. If you are interested in more just

1103
01:12:25,452 --> 01:12:29,394
feel free to look at the end of the slides and discover

1104
01:12:29,442 --> 01:12:33,278
a couple of the things I talked about even in more detail. Thank you

1105
01:12:33,284 --> 01:12:36,686
very much again for listening. I hope you had a great time and

1106
01:12:36,708 --> 01:12:37,340
learned something new.

