1
00:01:28,210 --> 00:01:31,826
Hello all, welcome to the session. Why should you bother

2
00:01:31,858 --> 00:01:35,750
about carpenter a cluster auto scaling solution for Kubernetes

3
00:01:36,330 --> 00:01:40,234
I am Raja Ganesan, working as a cloud architect at AWS.

4
00:01:40,402 --> 00:01:44,014
I have more than a decade's worth of experience in building software and

5
00:01:44,052 --> 00:01:47,982
high performance streams in the last several years. My main focus and

6
00:01:48,036 --> 00:01:51,214
interest are in building scalable systems, containers and

7
00:01:51,252 --> 00:01:54,346
observability. Before we dive

8
00:01:54,378 --> 00:01:57,918
into carpenter, let's see, why do we need an autoscaling solution

9
00:01:58,014 --> 00:02:01,314
for your Kubernetes cluster? Some sites prefer to

10
00:02:01,352 --> 00:02:04,734
over provision their Kubernetes cluster, while the others prefer

11
00:02:04,782 --> 00:02:08,434
to have an autoscaling solution in place to meet their unexpected

12
00:02:08,482 --> 00:02:11,778
compute needs. Auto scaling in Kubernetes

13
00:02:11,874 --> 00:02:15,126
is nothing but automatically adjusting the capacity of

14
00:02:15,148 --> 00:02:18,966
the Kubernetes cluster and provide a predictable study performance

15
00:02:18,998 --> 00:02:22,794
for your workloads. Some of the factors which may

16
00:02:22,832 --> 00:02:26,186
influence to implement an autoscaling solution for your

17
00:02:26,208 --> 00:02:30,666
Kubernetes cluster are resiliency, which means recovery

18
00:02:30,698 --> 00:02:35,194
from an unexpected failure or load, or even an scheduled or unscheduled

19
00:02:35,242 --> 00:02:39,226
interruption. The next could be cost optimization.

20
00:02:39,418 --> 00:02:43,070
You might want to run your Kubernetes cluster at an optimal

21
00:02:43,510 --> 00:02:46,882
state by making sure that you choose the right size

22
00:02:46,936 --> 00:02:50,546
of the resources. Last but not least,

23
00:02:50,648 --> 00:02:54,110
you might want to design for high availability,

24
00:02:54,270 --> 00:02:57,490
which means your workloads are available consistently

25
00:02:57,570 --> 00:03:00,790
in a predictable manner to serve your uses requests.

26
00:03:02,730 --> 00:03:06,786
Having said that, we can broadly classify the Kubernetes autoscaling

27
00:03:06,818 --> 00:03:10,140
solutions into two categories. One is

28
00:03:10,750 --> 00:03:14,266
scaling the underlying machines or nodes that powers the

29
00:03:14,288 --> 00:03:18,326
Kubernetes clusters. The other one is to scale the number of instances

30
00:03:18,358 --> 00:03:20,542
of an individual workload, in other words,

31
00:03:20,596 --> 00:03:24,490
pods, carpenter and cluster autoscaler

32
00:03:24,570 --> 00:03:28,250
falls into the earlier one, which helps you to scale

33
00:03:28,330 --> 00:03:30,990
the number of nodes of your Kubernetes cluster.

34
00:03:33,030 --> 00:03:36,754
Before we talk about Karpenter, let's quickly see about

35
00:03:36,792 --> 00:03:39,380
cluster autoscaler and how it works.

36
00:03:40,230 --> 00:03:43,650
Cluster autoscaler is nothing but an

37
00:03:43,720 --> 00:03:47,590
autoscaling solution for kubernetes from the Kubernetes Special Interest

38
00:03:47,660 --> 00:03:51,990
group with the implementation for most of the major cloud providers.

39
00:03:52,330 --> 00:03:56,022
The way the cluster autoscale works is by keeping a watch

40
00:03:56,076 --> 00:04:00,246
on the Kubernetes API server and works along with the Kube scheduler

41
00:04:00,358 --> 00:04:04,330
to find a new place for pods when it becomes unschedulable.

42
00:04:05,390 --> 00:04:08,726
When you're using cluster autoscaler, it always assumes

43
00:04:08,758 --> 00:04:11,674
your Kubernetes cluster have some sort of grouping,

44
00:04:11,802 --> 00:04:16,170
in other words, node groups. When a pod is unschedulable,

45
00:04:16,330 --> 00:04:20,106
cluster autoscaler will try to increase the node group size by adding

46
00:04:20,138 --> 00:04:23,570
new nodes of the same size which are already

47
00:04:23,640 --> 00:04:27,518
present in the node group. It is a straightforward

48
00:04:27,614 --> 00:04:31,730
process if you have only one node group in your cluster.

49
00:04:32,550 --> 00:04:36,454
When you have more than one node group and more than

50
00:04:36,492 --> 00:04:40,354
one node group matches the scheduling criteria of your pods,

51
00:04:40,482 --> 00:04:44,150
then cluster autoscale uses expanders to choose

52
00:04:44,220 --> 00:04:46,630
the right node group to expand.

53
00:04:48,430 --> 00:04:51,610
Let's take a closer look at cluster autoscaler.

54
00:04:52,030 --> 00:04:55,914
If a node type or an instance type is not available for

55
00:04:55,952 --> 00:04:59,526
any reason, then cluster autoscaler cannot acquire the required

56
00:04:59,558 --> 00:05:03,342
compute for your Kubernetes cluster. In this case,

57
00:05:03,476 --> 00:05:07,530
the cluster autoscaler will attempt a retry with a pre configured

58
00:05:07,610 --> 00:05:11,594
timeout value. Another important factor

59
00:05:11,722 --> 00:05:15,374
is when you're running in multiple availability zone,

60
00:05:15,502 --> 00:05:18,690
you might want your pods to be evenly distributed.

61
00:05:19,270 --> 00:05:22,546
In this case cluster cluster autoscaling Karpenter the

62
00:05:22,568 --> 00:05:26,200
underlying cloud provider's zonal rebalancing process.

63
00:05:26,970 --> 00:05:30,514
For example, in AWS, it uses

64
00:05:30,642 --> 00:05:34,802
the autoscaling group's easy rebalance process to periodically

65
00:05:34,866 --> 00:05:38,662
check whether the workloads are evenly distributed

66
00:05:38,726 --> 00:05:42,614
across the availability zones. If it is not, it will terminate

67
00:05:42,662 --> 00:05:46,730
the nodes so that your workloads can be scheduled elsewhere.

68
00:05:50,830 --> 00:05:54,658
Let's try to understand how the cluster auto scaler

69
00:05:54,694 --> 00:05:57,886
works by looking at an example and a

70
00:05:57,908 --> 00:06:01,646
quick disclaimer. In reality, the cluster autoscaling Karpenter

71
00:06:01,748 --> 00:06:04,478
multiple steps before provisioning a node,

72
00:06:04,574 --> 00:06:08,420
but for the sake of the explanation, I have oversimplified it.

73
00:06:09,910 --> 00:06:14,046
Let's assume that we are running our workload in on AWS

74
00:06:14,238 --> 00:06:18,406
and we have this example cluster with a single node group which has

75
00:06:18,428 --> 00:06:22,390
a minimum size of one node and maximum size of ten nodes.

76
00:06:22,890 --> 00:06:26,114
It is primarily consists of t three medium

77
00:06:26,162 --> 00:06:29,366
instance type and currently one node

78
00:06:29,398 --> 00:06:31,900
is running with several pods in it.

79
00:06:33,470 --> 00:06:36,906
And we have two pods which are waiting to be

80
00:06:36,928 --> 00:06:40,730
scheduled of four and five replicas respectively.

81
00:06:41,330 --> 00:06:45,306
And these pods require 250 millicore cpu

82
00:06:45,418 --> 00:06:47,950
and one gig memory in minimum.

83
00:06:50,690 --> 00:06:53,470
When these new pods are waiting to be scheduled,

84
00:06:53,890 --> 00:06:58,270
let's see what the cluster autoscaler will do during

85
00:06:58,340 --> 00:07:02,514
the process of request for new nodes. Cluster autoscale will attempt to

86
00:07:02,552 --> 00:07:06,566
determine the cpu and memory resources in an ASG based on

87
00:07:06,588 --> 00:07:09,510
its launch configuration and launch template.

88
00:07:11,290 --> 00:07:15,282
To increase the number of nodes, cluster autoscaler will set the desired

89
00:07:15,346 --> 00:07:18,490
replicas that it needs in the ASG configuration.

90
00:07:19,230 --> 00:07:23,402
In this example, it will set the desired replicas to be

91
00:07:23,536 --> 00:07:27,174
four so that it can schedule all the pending pods

92
00:07:27,222 --> 00:07:30,380
at once. And remember,

93
00:07:30,750 --> 00:07:34,286
cluster autoscale always assumes that your nodes running in

94
00:07:34,308 --> 00:07:37,150
a node group are always exactly equivalent.

95
00:07:37,650 --> 00:07:41,086
In AWS, it is always recommended to use a flexible set

96
00:07:41,108 --> 00:07:44,750
of instance types so that same amount of virtual cpu

97
00:07:44,830 --> 00:07:48,834
and memory resources so that you can get a consistent performance from

98
00:07:48,872 --> 00:07:50,770
your cluster autoscaling solution.

99
00:07:53,750 --> 00:07:57,498
And one important thing to remember here is cluster autoscale

100
00:07:57,534 --> 00:08:01,174
works well for scheduling across multiple availability zone when

101
00:08:01,212 --> 00:08:04,982
your workloads have no zone specific storage requirements and

102
00:08:05,036 --> 00:08:08,810
other pod affinity or node affinity based on the zones.

103
00:08:10,830 --> 00:08:14,282
A quick recap. You had nine pods, sorry,

104
00:08:14,336 --> 00:08:18,374
two pods and nine replicas that needs to be scheduled and your initial

105
00:08:18,422 --> 00:08:22,642
cluster size did not meet the requirements. And cluster autoscale

106
00:08:22,726 --> 00:08:25,966
calculated the total resources required for

107
00:08:25,988 --> 00:08:29,450
your pods to be scheduled and provisioned three nodes

108
00:08:29,530 --> 00:08:33,870
of the same size so that you can schedule all the pods

109
00:08:37,890 --> 00:08:41,246
having seen the cluster autoscaling Karpenter let's

110
00:08:41,278 --> 00:08:44,820
talk about carpenter, which is the main topic of the discussion today.

111
00:08:49,110 --> 00:08:52,142
Karpenter is a fully open source, cloud agnostic,

112
00:08:52,206 --> 00:08:55,778
high performance cluster autoscaling solution for kubernetes clusters.

113
00:08:55,954 --> 00:08:59,786
It provisions node for your Kubernetes cluster in a groupless way in

114
00:08:59,808 --> 00:09:03,750
bother words, if you use carpenter, you don't have to use node groups

115
00:09:03,830 --> 00:09:07,514
and avoid meddling with a configuration in another layer like

116
00:09:07,632 --> 00:09:10,090
node groups or auto scaling groups.

117
00:09:11,330 --> 00:09:14,590
Karpenter provisions the right resources, in other words,

118
00:09:14,660 --> 00:09:19,018
nodes directly for your Kubernetes cluster based on the scheduling constraints

119
00:09:19,114 --> 00:09:22,906
given in the pod specifications such as resource requests,

120
00:09:23,018 --> 00:09:25,070
node affinity, etc.

121
00:09:26,470 --> 00:09:30,254
Carpenter avoids unnecessary API calls between your kubernetes

122
00:09:30,302 --> 00:09:33,250
cluster and the underlying cloud provider's APIs.

123
00:09:35,290 --> 00:09:39,362
And finally, Carpenter uses the best suitable instance

124
00:09:39,426 --> 00:09:43,378
type to provision in order to accommodate the pending pods.

125
00:09:43,474 --> 00:09:47,042
If you remember, cluster autoscaler will attempt to provision

126
00:09:47,106 --> 00:09:51,158
the instance type of the same size of the other nodes

127
00:09:51,174 --> 00:09:52,300
in the node group.

128
00:09:56,030 --> 00:09:59,626
Before we see how the karpenter works, we need to understand what

129
00:09:59,648 --> 00:10:02,846
is Kube scheduler. Kube scheduler is an

130
00:10:02,868 --> 00:10:06,286
implementation of a control loop which regularly checks the

131
00:10:06,308 --> 00:10:10,030
Kubernetes control plane to make sure the cluster's current state

132
00:10:10,180 --> 00:10:13,646
matches the desired state. In other words, if there

133
00:10:13,668 --> 00:10:17,726
are parts that needs to be scheduled or evicted, then Kube scheduler

134
00:10:17,758 --> 00:10:21,890
is the one that does it. Carpenter works

135
00:10:21,960 --> 00:10:26,062
along with the Kube scheduler, very similar to cluster rotor scaler to periodically

136
00:10:26,126 --> 00:10:30,066
check for the pods in pending state with the reason unschedulable equal

137
00:10:30,098 --> 00:10:34,466
to true, carpenter waits

138
00:10:34,498 --> 00:10:38,140
for these events and provision new nodes for the pods to run.

139
00:10:39,070 --> 00:10:42,426
When a node becomes empty and there are

140
00:10:42,448 --> 00:10:46,470
no running workloads, then carpenter will attempt to deprovision

141
00:10:46,630 --> 00:10:48,650
or delete these nodes.

142
00:10:50,530 --> 00:10:54,154
In short, if there are more compute needed for your kubernetes

143
00:10:54,202 --> 00:10:58,106
cluster, then carpenter will provision additional nodes

144
00:10:58,298 --> 00:11:02,042
and if your cluster is underutilized,

145
00:11:02,106 --> 00:11:06,322
then karpenter will check the utilization and

146
00:11:06,376 --> 00:11:10,130
see if there are any nodes that can be deleted.

147
00:11:12,390 --> 00:11:16,130
Let's take the same example that we saw earlier and again,

148
00:11:16,280 --> 00:11:20,358
assuming that we are running our workloads on AWS and we have

149
00:11:20,524 --> 00:11:24,198
two workload that needs to be scheduled, blue and green ones

150
00:11:24,284 --> 00:11:28,778
of four and five replicas and each of which requires 250

151
00:11:28,864 --> 00:11:31,930
milli core cpu and one gig memory.

152
00:11:32,590 --> 00:11:36,406
And remember, we are not using any sort of grouping

153
00:11:36,438 --> 00:11:40,246
mechanism or node groups because we are using carpenter. In this

154
00:11:40,288 --> 00:11:44,062
example, the initial capacity of the cluster was

155
00:11:44,116 --> 00:11:47,802
provisioned by Carpenter, which has one t three medium instance

156
00:11:47,946 --> 00:11:51,470
with a limit of two virtual cpu and four gig memory.

157
00:11:53,330 --> 00:11:56,626
When the new pods are waiting to be scheduled, carpenter will look

158
00:11:56,648 --> 00:12:00,066
at the pods that needs to be scheduled and what might be the

159
00:12:00,088 --> 00:12:03,460
most suitable instance type to schedule these pods quickly.

160
00:12:04,390 --> 00:12:08,326
And Carpenter has its own internal algorithm to

161
00:12:08,348 --> 00:12:12,870
select the most optimal instance type from the pool of available instances.

162
00:12:15,370 --> 00:12:18,966
Secondly, carpenter interacts directly with the compute

163
00:12:18,998 --> 00:12:22,774
provider's API. In this example, since we are using AWS,

164
00:12:22,902 --> 00:12:26,854
it interacts directly with the AWS easy to fleet API

165
00:12:26,982 --> 00:12:29,450
to provision additional resources.

166
00:12:31,250 --> 00:12:35,134
And since our example have

167
00:12:35,332 --> 00:12:38,442
two pods with total of nine replicas,

168
00:12:38,506 --> 00:12:41,838
karpenter will attempt to choose the right instance to choose

169
00:12:41,924 --> 00:12:45,922
to schedule the pods as soon as possible and

170
00:12:45,976 --> 00:12:49,970
by calculating the total resources required. It may choose

171
00:12:50,120 --> 00:12:53,634
an instance type of m five x large which has

172
00:12:53,672 --> 00:12:56,966
enough resources to schedule all the pods. If you

173
00:12:56,988 --> 00:13:00,806
look closely, it is not the same size as the earlier one which is

174
00:13:00,828 --> 00:13:04,470
a t three medium and which has a two virtual cpu

175
00:13:05,850 --> 00:13:09,566
and four gig memory and m five xlarge

176
00:13:09,618 --> 00:13:13,642
have four virtual cpu and 16 gig memory which have

177
00:13:13,696 --> 00:13:17,082
adequate resources to schedule all the pending pods at one

178
00:13:17,136 --> 00:13:17,740
go.

179
00:13:23,200 --> 00:13:26,636
Having seen how carpenter works by an

180
00:13:26,658 --> 00:13:30,700
example, let's see how to use carpenter.

181
00:13:31,440 --> 00:13:35,072
In order to use carpenter, you have to install something called

182
00:13:35,206 --> 00:13:38,480
provision. Provisioners are nothing but

183
00:13:38,550 --> 00:13:42,716
kubernetes. Carpenter's custom resources that runs

184
00:13:42,748 --> 00:13:46,076
inside a Kubernetes cluster. It uses

185
00:13:46,108 --> 00:13:49,088
a subset of kubernetes wellknown labels such as zone,

186
00:13:49,184 --> 00:13:52,356
instance type and operating system when creating a

187
00:13:52,378 --> 00:13:55,060
node for instance.

188
00:13:57,480 --> 00:14:01,284
For instance, if a pod has no scheduling constraints defined,

189
00:14:01,332 --> 00:14:05,080
then carpenter can choose from wide range of options available

190
00:14:05,150 --> 00:14:08,440
from your cloud providers to provision the new nodes.

191
00:14:10,220 --> 00:14:13,756
A node provision by carpenter can expire by

192
00:14:13,778 --> 00:14:17,180
a number of factors. The first one could be

193
00:14:17,250 --> 00:14:20,984
by using the property called TTL seconds until expire,

194
00:14:21,112 --> 00:14:25,216
which means when this TTL is reached, karpenter will drain all

195
00:14:25,238 --> 00:14:28,396
the pods running in this node and schedule it elsewhere,

196
00:14:28,588 --> 00:14:32,480
and this node gets deleted. Once the TTL is reached,

197
00:14:33,380 --> 00:14:36,936
the second factor is when the node becomes empty,

198
00:14:37,068 --> 00:14:40,516
meaning when there are no running workloads in the

199
00:14:40,538 --> 00:14:43,860
node, then carpenter will attempt to delete this node.

200
00:14:44,440 --> 00:14:48,404
Karpenter places a node empty TTL on this node which

201
00:14:48,442 --> 00:14:52,672
is controlled again by the property called TTL seconds after empty

202
00:14:52,816 --> 00:14:56,664
and it will check whether the node has any running workload or

203
00:14:56,702 --> 00:15:00,088
not and when the TTL is reached the

204
00:15:00,094 --> 00:15:03,640
node gets deleted. And the third factor

205
00:15:03,720 --> 00:15:06,812
when a node gets deleted is you can either

206
00:15:06,866 --> 00:15:10,156
delete it manually or by a process very similar to

207
00:15:10,178 --> 00:15:14,636
KubectL delete node command one

208
00:15:14,658 --> 00:15:18,444
of the interesting feature about carpenter is using the mix

209
00:15:18,492 --> 00:15:22,444
of spot and on demand instance types. When specifying

210
00:15:22,492 --> 00:15:26,888
both, carpenter always gives priority to the spot instances by default.

211
00:15:27,084 --> 00:15:32,784
Thus it ensures that cluster

212
00:15:32,832 --> 00:15:35,060
have a cost optimization.

213
00:15:37,400 --> 00:15:41,208
My bad, carpenter always gives priority to

214
00:15:41,214 --> 00:15:45,176
the spot instances by default for any reason. If it is unable to

215
00:15:45,198 --> 00:15:48,504
acquire the spot capacity, carpenter will request for on

216
00:15:48,542 --> 00:15:51,856
demand resources. Thus it will ensure

217
00:15:51,908 --> 00:15:55,484
that you have a reduced cost of your kubernetes cluster and

218
00:15:55,522 --> 00:15:59,032
also you can choose spot instances

219
00:15:59,096 --> 00:16:02,300
if your workloads can withstand interruptions.

220
00:16:05,040 --> 00:16:08,412
Finally, if you do not have any constraints on which

221
00:16:08,466 --> 00:16:11,984
zones your pods can be scheduled, carpenter can choose from

222
00:16:12,022 --> 00:16:15,904
wide range of instance time that we've seen earlier. If you want

223
00:16:15,942 --> 00:16:20,300
more control, you can enforce which zone your parts can be scheduled

224
00:16:20,380 --> 00:16:23,780
by using a topology spread constraints

225
00:16:30,630 --> 00:16:33,942
let's see some of the operational considerations when using

226
00:16:33,996 --> 00:16:38,002
carpenter. If you are on AWS, carpenter creates

227
00:16:38,066 --> 00:16:41,442
launch templates automatically for you with the latest eks

228
00:16:41,506 --> 00:16:46,178
optimized AMI and encrypted EBS volumes,

229
00:16:46,354 --> 00:16:49,706
and some users might not prefer this. If these

230
00:16:49,728 --> 00:16:53,594
are not sufficient, then a user can feel free to create your own

231
00:16:53,632 --> 00:16:56,954
custom launch template with the AMI of your choice and

232
00:16:56,992 --> 00:17:00,670
other security attributes. Next,

233
00:17:00,820 --> 00:17:04,778
let's talk about node upgrades, which is one of the interesting feature

234
00:17:04,874 --> 00:17:08,110
or factor. Asked by many sites.

235
00:17:09,010 --> 00:17:13,122
The most straightforward mechanism to perform node upgrades is

236
00:17:13,176 --> 00:17:17,570
by setting the TTL seconds until expired property that we saw earlier.

237
00:17:18,470 --> 00:17:22,414
When karpenter provisions a new node, it will automatically

238
00:17:22,462 --> 00:17:25,910
picks up the latest AMI configured in the launch templates.

239
00:17:29,290 --> 00:17:33,366
The next important consideration that we're going to discuss is

240
00:17:33,548 --> 00:17:37,130
whether to use single provision or multiple provisioners.

241
00:17:37,550 --> 00:17:41,558
For most sites, using a single provisioner is more than sufficient

242
00:17:41,654 --> 00:17:45,462
to meet your needs, but there are certain situations

243
00:17:45,526 --> 00:17:48,418
where you might have to use multiple provisioners,

244
00:17:48,534 --> 00:17:51,726
for example having a separate provisioner for

245
00:17:51,828 --> 00:17:55,470
cpu based resources and GPU based resources.

246
00:17:57,250 --> 00:18:01,374
The second important factor could be when you want to have a dedicated

247
00:18:01,422 --> 00:18:04,866
provisioner for each teams so that

248
00:18:04,968 --> 00:18:08,514
they can manage their own constraints, and also it can

249
00:18:08,552 --> 00:18:11,490
provide you better handle for cost attribution.

250
00:18:12,950 --> 00:18:16,514
One important thing to remember is when you're using multiple

251
00:18:16,562 --> 00:18:20,790
provisioners, it is always recommended to make sure that these

252
00:18:20,860 --> 00:18:23,190
provisioners are mutually exclusive.

253
00:18:25,950 --> 00:18:29,414
Otherwise, when a pod is pinning to be scheduled,

254
00:18:29,462 --> 00:18:33,434
karpenter loops through each provisioner by evaluating the one which

255
00:18:33,472 --> 00:18:37,782
matches the scheduling constraints. If there are multiple provisioners

256
00:18:37,846 --> 00:18:42,190
which matches the constraints, then karpenter chooses the one randomly.

257
00:18:45,650 --> 00:18:49,226
Another important feature which is supported by compender

258
00:18:49,258 --> 00:18:52,674
is use of bottle rocket. Bottle rocket is an open

259
00:18:52,712 --> 00:18:56,882
source Linux operating system which is purpose built for running

260
00:18:56,936 --> 00:19:00,126
containers with improved security and performance.

261
00:19:00,318 --> 00:19:04,406
Carpenter supports use of bottle rocket by

262
00:19:04,588 --> 00:19:07,910
specifying that in the launch template in AWS.

263
00:19:10,330 --> 00:19:14,290
The last consideration that we're going to see today is topology

264
00:19:14,370 --> 00:19:17,814
spread constraint when running a kubernetes cluster

265
00:19:17,862 --> 00:19:21,370
in production, many sites wants to optimize for availability.

266
00:19:22,030 --> 00:19:26,250
Karpenter recommends the use of topology spread constraint

267
00:19:27,150 --> 00:19:30,586
instead of the pod affinity to spread the pod

268
00:19:30,618 --> 00:19:33,882
placement across the availability zones and capacity

269
00:19:33,946 --> 00:19:34,830
types.

270
00:19:39,670 --> 00:19:43,022
As we have briefly discussed what is Karpenter?

271
00:19:43,086 --> 00:19:46,558
How carpenter works let's recap.

272
00:19:46,734 --> 00:19:50,646
Karpenter takes a fresh look at the cluster rotate scaling solution for

273
00:19:50,668 --> 00:19:54,470
kubernetes clusters. It aims to provide more direct control

274
00:19:54,540 --> 00:19:58,666
for site operators and developers to acquire new capacity for your

275
00:19:58,688 --> 00:20:00,780
workloads as quickly as possible.

276
00:20:02,030 --> 00:20:06,246
Carpenter provides several improvements over the existing kubernetes

277
00:20:06,358 --> 00:20:09,674
autoscaling solutions, such as taking advantage

278
00:20:09,722 --> 00:20:13,386
of the wide range of instance types available from your cloud provider

279
00:20:13,498 --> 00:20:17,870
so you are not restricted to instance type of the similar sizes.

280
00:20:19,170 --> 00:20:22,590
Next, Karpenter works in a groupless fashion,

281
00:20:22,750 --> 00:20:26,638
thus avoiding interacting with additional orchestration layer

282
00:20:26,734 --> 00:20:30,306
such as autoscaling group, et cetera. Thus when there

283
00:20:30,328 --> 00:20:34,642
are failures, your retry time is considerably

284
00:20:34,706 --> 00:20:38,690
reduced. The last improvement

285
00:20:38,770 --> 00:20:42,054
is when a node is launched by a carpenter. It binds the

286
00:20:42,092 --> 00:20:45,190
pods that are scheduled immediately to that node,

287
00:20:46,090 --> 00:20:48,710
so when the node provisioning is in progress,

288
00:20:49,130 --> 00:20:52,626
kubelet can start preparing the container runtime, pre pull container

289
00:20:52,658 --> 00:20:55,906
images, etc. So that the pod becomes available to serve

290
00:20:55,938 --> 00:20:59,434
quickly. Karpenter is quite easier

291
00:20:59,482 --> 00:21:02,862
to set up and you can follow the getting started guide in the link

292
00:21:02,916 --> 00:21:04,000
provided here.

293
00:21:07,330 --> 00:21:10,574
We have come to the end of the session. I would like to thank my

294
00:21:10,612 --> 00:21:14,126
colleague Aldrid who helped me immensely to prepare for this session and

295
00:21:14,148 --> 00:21:17,502
his support. And finally, I would like to thank you

296
00:21:17,556 --> 00:21:21,374
for listening to this talk and if you have any questions please feel

297
00:21:21,412 --> 00:21:22,300
free to reach out to me.

