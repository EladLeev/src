1
00:00:25,570 --> 00:00:28,834
You. Hello everyone, my name is Mohammed

2
00:00:28,882 --> 00:00:32,262
Azmil. I'm the founding engineer of imesh. Since past few years

3
00:00:32,316 --> 00:00:36,546
I've worked on various cloud native applications ranging from high performance backends

4
00:00:36,578 --> 00:00:39,862
to blockchains to full stack apps. With imesh, I help

5
00:00:39,916 --> 00:00:43,362
enterprise adopt Istio into their large scale production workloads.

6
00:00:43,506 --> 00:00:46,662
I'm very thankful to Con 42 for having me here,

7
00:00:46,796 --> 00:00:50,814
and today I'll be discussing on ambient mesh, which sync is the new

8
00:00:50,852 --> 00:00:54,398
sidecarless and faster istio for zero. Trust a

9
00:00:54,404 --> 00:00:57,246
bit about imesh. We formed in 2023.

10
00:00:57,428 --> 00:01:00,702
We offer enterprise grade istio support, and we also offer

11
00:01:00,756 --> 00:01:03,966
a dashboard called Imesh Dashboard, which you can use to

12
00:01:03,988 --> 00:01:07,474
monitor your istio workload with a mission to simplify and

13
00:01:07,512 --> 00:01:10,766
secure your microservices in the cloud. In today's

14
00:01:10,798 --> 00:01:14,142
session, we'll be having a look at understanding what is istio service mesh?

15
00:01:14,206 --> 00:01:18,206
What are the limitations of the existing service mesh, and we'll

16
00:01:18,238 --> 00:01:21,894
be understanding what istio ambient mesh is all about. What are the security

17
00:01:22,012 --> 00:01:26,120
and other benefit that it comes with. We'll be having a demo as well

18
00:01:26,490 --> 00:01:30,026
where we'll be using istio ambit mesh with mtls. We'll be

19
00:01:30,048 --> 00:01:34,086
applying l four and l seven authorization policies, do traffic management

20
00:01:34,278 --> 00:01:38,102
and do observability with open source tool that come with Istio,

21
00:01:38,166 --> 00:01:41,978
and a lot more. So we have quite a lot of meshes to cover,

22
00:01:42,064 --> 00:01:45,306
and I'm sure it's a tongue twister for you as well. So buckle

23
00:01:45,338 --> 00:01:49,130
up. And here we go. So in istio documentation,

24
00:01:49,210 --> 00:01:52,574
you can find that a service mesh is a dedicated infrastructure layer that

25
00:01:52,612 --> 00:01:55,966
helps you add capabilities like observability, traffic management and

26
00:01:55,988 --> 00:01:59,458
security without adding them to your code. So for someone who

27
00:01:59,464 --> 00:02:02,898
is not aware of service Imesh, it's basically a layer. It's kind of like a

28
00:02:02,904 --> 00:02:06,306
pluggable layer, which you add while you're deploying your workload so that you

29
00:02:06,328 --> 00:02:09,640
get observability, traffic management and security out of the box.

30
00:02:10,330 --> 00:02:14,402
Let's have a look at istio service mesh architects, the one with the sidecars.

31
00:02:14,546 --> 00:02:18,006
So as you can see, there are two major components to it, one being the

32
00:02:18,028 --> 00:02:21,946
control plane, another being the data plane. So the control plane, which is

33
00:02:21,968 --> 00:02:25,466
essentially istio, is responsible for injecting sidecars to all

34
00:02:25,488 --> 00:02:29,222
the workloads when a namespace is enabled for istio injection.

35
00:02:29,366 --> 00:02:33,426
So if the namespace is enabled for istio injection, all the pods

36
00:02:33,478 --> 00:02:37,098
are going to be deployed along with a sidecar,

37
00:02:37,194 --> 00:02:40,606
which is nothing but on web proxy. And this on web proxy is

38
00:02:40,628 --> 00:02:45,274
the central point where you do traffic management, observability and mtls,

39
00:02:45,322 --> 00:02:49,082
all those things that istio provides. Now, once the sidecar is deployed

40
00:02:49,146 --> 00:02:52,142
to a particular servit, it internally intercepts the traffic.

41
00:02:52,206 --> 00:02:55,714
Whatever is going to the service is now going through the onboard proxy and

42
00:02:55,752 --> 00:02:59,426
then to the service. This is the traditional istio servicemas

43
00:02:59,458 --> 00:03:03,078
architecture. Istio also provides ingress and egress gateways which

44
00:03:03,084 --> 00:03:06,838
are envoy proxies themselves, but later on that once we go to

45
00:03:06,844 --> 00:03:10,318
the demo. There are various limitations to the sidecar

46
00:03:10,354 --> 00:03:13,498
approach, the most pertinent one being the sidecar itself.

47
00:03:13,664 --> 00:03:17,766
Since each of your workload needs to be injected with the sidecar, it results

48
00:03:17,798 --> 00:03:21,642
in modification of the pod spec and it redirects the traffic within

49
00:03:21,696 --> 00:03:25,406
the pod. Also, if you have a massive workload like 4000 or

50
00:03:25,428 --> 00:03:28,734
5000 pods, or even more, even the

51
00:03:28,772 --> 00:03:32,458
lightweight sidecar, when it is deployed on such a massive workload,

52
00:03:32,554 --> 00:03:36,714
causes a massive resource utilization. The traffic capture

53
00:03:36,762 --> 00:03:40,434
stb processing, which are very computationally expensive, are done all

54
00:03:40,472 --> 00:03:43,842
at the sitecar level. It manages l four routing, l seven

55
00:03:43,896 --> 00:03:47,662
routing, l seven observability, l four observability

56
00:03:47,726 --> 00:03:51,202
there's quite a lot of going on, and if you have to update the sidecar,

57
00:03:51,266 --> 00:03:54,662
then good luck. You have to restart the entire application. There is no

58
00:03:54,716 --> 00:03:58,658
midway approach that you can update half of the sidecar

59
00:03:58,674 --> 00:04:02,902
and leave the rest running in an older version. The service first protocol

60
00:04:02,966 --> 00:04:06,646
is also a massive issue with the current service. Mesh architects

61
00:04:06,678 --> 00:04:10,314
with the sidecar as it impacts the permissive mtls and it's very

62
00:04:10,352 --> 00:04:14,046
tough to work with and it doesn't even work properly for

63
00:04:14,068 --> 00:04:17,534
nonconformant HTTP implementations. How about

64
00:04:17,572 --> 00:04:20,826
we break it down? Instead of one sidecar dealing

65
00:04:20,858 --> 00:04:24,626
with l four and l seven and doing everything in between, we separate the

66
00:04:24,648 --> 00:04:28,254
concerns using two different layer, one for the l four secure

67
00:04:28,302 --> 00:04:31,838
overlay, another for the l seven processing layer. The MTLS

68
00:04:31,854 --> 00:04:35,198
tunneling, simple authorization, TCP metric logging

69
00:04:35,214 --> 00:04:38,518
and routing. All of them can be taken care at the l four level,

70
00:04:38,684 --> 00:04:42,386
whereas in the l seven layer you have rich authorization policies, metrics,

71
00:04:42,418 --> 00:04:46,146
logging, routing and everything that you need. The ambient mesh

72
00:04:46,178 --> 00:04:50,050
does exactly the same thing. It takes care of l four and l seven responsibilities

73
00:04:50,130 --> 00:04:53,146
differently. There are two components to it. One is the

74
00:04:53,168 --> 00:04:56,054
ztunnel, another is the waypoint proxy. Ztunnel,

75
00:04:56,102 --> 00:04:59,686
short for zero trust tunnel, helps you to securely connect and authenticate

76
00:04:59,718 --> 00:05:03,146
your workloads in the mesh. It is extremely lightweight as it

77
00:05:03,168 --> 00:05:06,606
does not do anything related to l seven processing. All it does is

78
00:05:06,628 --> 00:05:09,786
provide mtls and l four authorization. Waypoint proxy,

79
00:05:09,818 --> 00:05:13,646
however, is the one providing you with l seven capabilities and

80
00:05:13,668 --> 00:05:17,186
it is not mandatory to deploy waypoint proxy. You can deploy it

81
00:05:17,208 --> 00:05:20,706
on basis of a namespace or maybe even service account depending on

82
00:05:20,728 --> 00:05:24,226
your l seven use cases. So ztunnel, if it sees that

83
00:05:24,248 --> 00:05:28,086
there is a waypoint proxy, will pass all its traffic through it so that the

84
00:05:28,108 --> 00:05:32,054
l seven policies are being evaluated there, and it

85
00:05:32,092 --> 00:05:35,602
again goes back through the other ztunnel to the respective workload.

86
00:05:35,746 --> 00:05:39,334
And you can see that waypoint proxies are nothing but envoy proxies themselves.

87
00:05:39,452 --> 00:05:42,502
And ztunnel is a different application which is written in rust,

88
00:05:42,566 --> 00:05:45,898
making it very performant. Waypoint proxies are like

89
00:05:45,984 --> 00:05:49,626
simple kubernetes pods which are deployed in a particular namespace. So if

90
00:05:49,648 --> 00:05:53,022
your traffic is higher you can easily scale them up or down, which is

91
00:05:53,076 --> 00:05:56,682
pretty easy. Let's understand what is HTTPs

92
00:05:56,746 --> 00:06:00,810
connect tunnel ztunnel uses this to communicate with other ztunnel

93
00:06:00,890 --> 00:06:04,074
and waypoint proxies as well. It uses Hbone,

94
00:06:04,122 --> 00:06:08,222
the HTTP based overlay network environment, to encapsulate

95
00:06:08,286 --> 00:06:12,066
all the traffic that is coming through the ztunnel into a single port which

96
00:06:12,088 --> 00:06:16,354
is exported, as in this case that is port 15,008.

97
00:06:16,472 --> 00:06:21,014
You can see the Ztunnel might be listening at port 84,

98
00:06:21,052 --> 00:06:24,198
three and other particular ports from a particular

99
00:06:24,284 --> 00:06:28,118
workload, but at the end of the day it's kind of encapsulating everything

100
00:06:28,204 --> 00:06:32,026
in the Hbone and exported through one single port.

101
00:06:32,128 --> 00:06:36,058
As a result, the ztunnel doesn't has to manage a lot of

102
00:06:36,224 --> 00:06:39,722
connections with the other ztunnel or waypoint proxy, making it very simple

103
00:06:39,776 --> 00:06:43,438
as there is only one port which is exported and the entire

104
00:06:43,524 --> 00:06:47,022
traffic is kind of going in this kind of packet as you can see below.

105
00:06:47,156 --> 00:06:50,842
So this clear encapsulation provides better interoperability

106
00:06:50,906 --> 00:06:54,602
with load balancer infrastructure. It also works well with the existing

107
00:06:54,666 --> 00:06:58,146
envoycar, the istio sitecar mesh. And as a

108
00:06:58,168 --> 00:07:01,314
result we also have a better support for server first

109
00:07:01,352 --> 00:07:04,974
protocols for your MySQL and other databases where the server

110
00:07:05,022 --> 00:07:08,278
kind of makes the first connection. Now let's understand

111
00:07:08,364 --> 00:07:11,974
how secure Ztunnel actually is. For those of you who know

112
00:07:12,012 --> 00:07:15,510
Spiffy, the secure production identity framework for everyone,

113
00:07:15,660 --> 00:07:19,254
it's similar to the workload attestation process. So here

114
00:07:19,292 --> 00:07:22,566
you can see the sleep pod is trying to interact with the Echo

115
00:07:22,598 --> 00:07:25,962
server on a different node and everything is going through

116
00:07:26,016 --> 00:07:29,322
ztunnel and then a waypoint proxy and the communication is happening.

117
00:07:29,456 --> 00:07:33,406
So in between we have SvID, which is spiffy, verifiable identity to

118
00:07:33,428 --> 00:07:37,274
verify a particular service here. Since the spiffy

119
00:07:37,322 --> 00:07:40,506
says sorry, the Svid says that it's from NS NS

120
00:07:40,538 --> 00:07:44,350
one SA sle sleep. We know that it's coming from the sleep service

121
00:07:44,500 --> 00:07:47,166
and then it has to go to the echo service. And this is how the

122
00:07:47,188 --> 00:07:50,530
Zetaner goals from where to route the traffic. So what's happening

123
00:07:50,600 --> 00:07:54,450
is for generating the certificates for a particular service

124
00:07:54,520 --> 00:07:57,938
or particular workload. So istio is the one that acts

125
00:07:57,954 --> 00:08:00,902
as the root ca. You can configure it to use a different root ca,

126
00:08:00,956 --> 00:08:04,870
but istio in this case acts as the certificate authority.

127
00:08:05,290 --> 00:08:08,694
Zunnel here assumes the identity of a workload which is running

128
00:08:08,732 --> 00:08:12,458
on its node, and it acts as a node agent

129
00:08:12,544 --> 00:08:16,058
in terms of spiffy. It then sends a CSR request to

130
00:08:16,064 --> 00:08:19,930
the istio control plane on behalf of this particular service

131
00:08:20,080 --> 00:08:23,946
to get its identity back. Istio control plane then signs

132
00:08:23,978 --> 00:08:27,514
it and returns the X 509 certificate to the ztunnel.

133
00:08:27,562 --> 00:08:31,530
Here istio control plane is kind of acting like the spiffy server

134
00:08:31,690 --> 00:08:35,326
which signs the certificates for a particular workload, and this is how the

135
00:08:35,348 --> 00:08:38,850
entire certificates are being managed in the service mesh.

136
00:08:39,270 --> 00:08:43,010
Now let's have a look at ztunnel in action. This is not the actual demo.

137
00:08:43,080 --> 00:08:46,434
We'll be having a demo further in the session. This is just to see

138
00:08:46,472 --> 00:08:49,974
ztunnel in action. And as shown in example here we have

139
00:08:50,012 --> 00:08:53,382
a sleep pod which is trying to talk to an echo server in a different

140
00:08:53,436 --> 00:08:57,350
node. This is a local kind cluster and I'm using two nodes here

141
00:08:57,420 --> 00:09:00,902
which I'll simply use TCP dump and listen for port 80

142
00:09:00,966 --> 00:09:04,762
and the port 1508 which is the hbone port and

143
00:09:04,816 --> 00:09:08,154
the same thing here as well. I'll be doing that and if

144
00:09:08,192 --> 00:09:11,982
I run this I get the response. And also

145
00:09:12,036 --> 00:09:14,830
here you can see the traffic is not encrypted.

146
00:09:15,410 --> 00:09:21,262
We can see the request body and the entire response zero trust

147
00:09:21,316 --> 00:09:25,374
adding spaces below. Now let's install istio and enable the namespaces

148
00:09:25,422 --> 00:09:28,974
to use the Istio ambient meshtio

149
00:09:29,022 --> 00:09:32,318
CtL install set profile

150
00:09:32,414 --> 00:09:36,114
ambient. We'll be having a look at all the configurations and

151
00:09:36,152 --> 00:09:39,686
all add in the session. This is just to show ztanel in

152
00:09:39,708 --> 00:09:44,040
action. Let's give it some time to install everything

153
00:09:44,650 --> 00:09:47,834
and once it is done we just need to label the

154
00:09:47,872 --> 00:09:51,450
namespaces with istio IO

155
00:09:52,750 --> 00:09:56,054
dataplane mode

156
00:09:56,102 --> 00:09:59,702
equal ambient so that the ambient

157
00:09:59,766 --> 00:10:03,054
mesh, the namespaces are kind of in the

158
00:10:03,092 --> 00:10:06,560
mesh. Now I'm running the same thing.

159
00:10:07,010 --> 00:10:11,066
We are getting the response back, but if you see here the entire traffic

160
00:10:11,098 --> 00:10:14,274
is encrypted. You're not seeing anything that

161
00:10:14,312 --> 00:10:17,746
we saw previously, and these are

162
00:10:17,768 --> 00:10:21,342
kind of the logs for the different ztunnels and all communicating.

163
00:10:21,486 --> 00:10:23,730
But every traffic here is encrypted.

164
00:10:24,710 --> 00:10:28,406
So yeah, this, you can see the ztanel in action and how it

165
00:10:28,428 --> 00:10:31,960
does mtls. It's just pretty easy. It works out of the box.

166
00:10:32,650 --> 00:10:36,354
Now, what could be a better time than this to discuss the benefits of ambient

167
00:10:36,402 --> 00:10:39,962
mesh? We trust saw Ztanel in action, and this is as easy

168
00:10:40,016 --> 00:10:43,306
as it can be. So the first benefit is that

169
00:10:43,328 --> 00:10:46,774
it can be used without modifying an existing workload.

170
00:10:46,902 --> 00:10:50,598
It is a non invasive method of using the service mesh.

171
00:10:50,694 --> 00:10:54,106
You don't have to modify pod spec or do any internal traffic

172
00:10:54,138 --> 00:10:57,258
rerouting in a pod. I just labeled the namespace,

173
00:10:57,354 --> 00:11:00,846
everything was working, and once we go to the demo, we'll be able to see

174
00:11:00,868 --> 00:11:04,074
it even further in action. There is zero downtime.

175
00:11:04,122 --> 00:11:07,426
You don't have to wait for any sidecar to be injected and running so that

176
00:11:07,448 --> 00:11:10,850
you can say that hey, my service mesh is up and running.

177
00:11:11,000 --> 00:11:14,578
It is very minimal in terms of cost because

178
00:11:14,664 --> 00:11:18,774
you're not deploying sidecars for every particular workload. So that

179
00:11:18,812 --> 00:11:22,086
makes it really cheap, both from the cost perspective and

180
00:11:22,108 --> 00:11:25,522
the resource utilization perspective. It is interoperable

181
00:11:25,586 --> 00:11:28,838
with all the sidecar based istio, even non istio workloads.

182
00:11:28,854 --> 00:11:32,634
So yeah, this is kind of like the

183
00:11:32,672 --> 00:11:35,130
best solution of istio you could ever imagine.

184
00:11:35,710 --> 00:11:39,354
Okay, without any delay, I'll be jumping straight to the demo,

185
00:11:39,472 --> 00:11:42,974
since after seeing ztanel in action, I'm sure you might be wanting the whole

186
00:11:43,012 --> 00:11:46,254
picture. So let's hope everything goes well.

187
00:11:46,452 --> 00:11:50,474
So this is my cluster setup for the demo. I'm using aks cluster

188
00:11:50,522 --> 00:11:54,154
with two nodes, and this is the node specification,

189
00:11:54,202 --> 00:11:58,334
as you can see. And I'll be using the Istio version of 118

190
00:11:58,462 --> 00:12:01,874
three. So the way this demo will progress is that

191
00:12:01,912 --> 00:12:05,486
we'll be starting with this particular architecture, and we'll kind of keep adding

192
00:12:05,518 --> 00:12:09,734
bits and pieces to it. And as you can see, there is just one

193
00:12:09,772 --> 00:12:12,866
namespace in node one and another namespace in node

194
00:12:12,898 --> 00:12:16,294
two. Each namespace is going to have sleep and not

195
00:12:16,332 --> 00:12:19,914
sleep pods. However, the echo server in ns one is just one

196
00:12:19,952 --> 00:12:23,814
deployment and ns two. It has two versions,

197
00:12:23,862 --> 00:12:27,594
v one and v two, for us to understand traffic management as well.

198
00:12:27,792 --> 00:12:31,230
Now, we'll quickly go over the configuration for these deployments.

199
00:12:31,570 --> 00:12:35,466
So here you can see we have two files, resources ns one and resources

200
00:12:35,498 --> 00:12:38,714
ns two, responsible for kind of deploying

201
00:12:38,762 --> 00:12:42,462
various workloads in different namespaces. So I am

202
00:12:42,516 --> 00:12:45,930
defining the namespace here, and I'm also labeling it with Istio

203
00:12:46,010 --> 00:12:49,394
IO data plane mode ambient, so it is taken into

204
00:12:49,432 --> 00:12:52,466
the ambient mesh when you install it. So right now we won't be

205
00:12:52,488 --> 00:12:56,786
having any istio. So whatever you install here, it's simply going to kind

206
00:12:56,808 --> 00:13:00,406
of be there. And then I have service account for

207
00:13:00,428 --> 00:13:03,746
echo service and I'm using service account for the waypoint proxies,

208
00:13:03,858 --> 00:13:07,922
that's why I'm creating it. There is an echo service and then the deployment.

209
00:13:08,066 --> 00:13:11,610
Same thing with sleep server and not sleep. There is a service

210
00:13:11,680 --> 00:13:15,210
account, a service and a deployment. The only thing different

211
00:13:15,280 --> 00:13:19,306
in resources ns two is the namespace, and the Echo server deployment is

212
00:13:19,328 --> 00:13:21,360
having both v one and v two here.

213
00:13:22,450 --> 00:13:26,090
So now we'll quickly install the workloads.

214
00:13:26,170 --> 00:13:29,230
So all we have to do is apply those two files,

215
00:13:29,890 --> 00:13:32,400
ns one resources, ns one.

216
00:13:32,870 --> 00:13:35,460
Here it's creating the namespace services,

217
00:13:35,990 --> 00:13:39,220
deploying everything switch for some time,

218
00:13:39,670 --> 00:13:43,380
and we'll also deploy ns two. And done.

219
00:13:43,830 --> 00:13:47,960
Awesome. So let's see what we have. In NS one.

220
00:13:48,970 --> 00:13:53,010
We have our echo server, we have a not sleep and sleep pods,

221
00:13:53,170 --> 00:13:56,360
the services, the deployments, everything is up and running.

222
00:13:56,730 --> 00:14:00,534
Let's go to ns two. And we do have two

223
00:14:00,572 --> 00:14:03,530
echo server here, one for V one, another for V two.

224
00:14:03,680 --> 00:14:07,034
So everything up and running here as well. Let's have a look at the

225
00:14:07,072 --> 00:14:11,450
pods and see if they are deployed in the right nodes.

226
00:14:14,930 --> 00:14:18,350
The namespace two is running on this particular node,

227
00:14:19,570 --> 00:14:22,800
one and ns one.

228
00:14:23,330 --> 00:14:26,290
All the workloads would be running in the node.

229
00:14:27,590 --> 00:14:31,310
Now moving to step two, what we'll be doing is we'll be installing istio,

230
00:14:31,390 --> 00:14:35,206
and in this case I'll be installing istio with the ingress gateways as

231
00:14:35,228 --> 00:14:39,874
well. In the trailer you had seen just the ambient mesh

232
00:14:39,922 --> 00:14:43,346
istio, but here we'll be needing the istio ingress gateways.

233
00:14:43,538 --> 00:14:47,190
So yeah, let's just copy the command

234
00:14:47,630 --> 00:14:50,090
from here. Oops,

235
00:14:52,590 --> 00:14:56,406
this much? Yeah, I'm lazy. I'm just copy

236
00:14:56,438 --> 00:14:59,740
pasting commands. Oh, my bad. What did I do?

237
00:15:00,050 --> 00:15:02,910
Command not found. Yikes.

238
00:15:03,970 --> 00:15:06,480
Looks like demo gods are not with me.

239
00:15:07,570 --> 00:15:11,360
Let's give it some time. It's going to take some time to install

240
00:15:13,010 --> 00:15:16,594
istio. Core has installed. We have the istio D here,

241
00:15:16,792 --> 00:15:20,274
the CNIs ingress gateways, everything is going to get

242
00:15:20,312 --> 00:15:24,242
installed. We'll be having a look at the istio namespace as well,

243
00:15:24,376 --> 00:15:28,102
to see what all things it has. So once

244
00:15:28,156 --> 00:15:31,110
it's done for injection, okay,

245
00:15:31,180 --> 00:15:34,486
cool. Get ns. We'll be having an

246
00:15:34,508 --> 00:15:37,350
istio system namespace, which is by default.

247
00:15:37,710 --> 00:15:41,510
And let's see kubectl

248
00:15:41,590 --> 00:15:44,380
get all steer system.

249
00:15:44,990 --> 00:15:48,010
Okay, maybe I need to reduce the size

250
00:15:48,080 --> 00:15:51,814
a bit. Yeah, so here you can see we have the Steocanis

251
00:15:51,862 --> 00:15:55,246
that are running on different different one for

252
00:15:55,268 --> 00:15:58,730
each node we have the ingress gateway, we have the control brain,

253
00:15:58,810 --> 00:16:01,966
and we have a ztunnel which is running on both the

254
00:16:01,988 --> 00:16:05,986
nodes. We have the ingress gateway as well. And if

255
00:16:06,008 --> 00:16:09,940
we have a look at the pods, you can see

256
00:16:10,390 --> 00:16:14,034
one ztunnel is running in this particular node, another ztunnel is running

257
00:16:14,072 --> 00:16:17,910
in this node. So we have installed Istio successfully

258
00:16:18,490 --> 00:16:22,246
and well after that

259
00:16:22,348 --> 00:16:26,242
we'll also be installing the gateway APIs.

260
00:16:26,306 --> 00:16:30,394
We'll be needing this ahead in the demo.

261
00:16:30,592 --> 00:16:33,526
Since I've already installed that in my cluster,

262
00:16:33,718 --> 00:16:37,034
I won't be running it. You can simply use this command to install it.

263
00:16:37,072 --> 00:16:40,474
This is just to install the Kubernetes gateway

264
00:16:40,522 --> 00:16:44,410
APIs. Now since we have installed Istio,

265
00:16:44,490 --> 00:16:48,794
let's install Prometheus, Grafana and Kiali. All the observability tools

266
00:16:48,922 --> 00:16:52,726
here as well. So I don't have the ML files

267
00:16:52,778 --> 00:16:55,906
here, I have it in a different location. So I'll simply be

268
00:16:55,928 --> 00:17:01,140
going there Opensource 18.3

269
00:17:01,670 --> 00:17:04,930
and then we can simply copy paste the command.

270
00:17:05,350 --> 00:17:09,682
However, I don't recommend copy pasting it from PDF anyways.

271
00:17:09,826 --> 00:17:13,426
You'll be having everything in GitHub. I'll make sure I'll add a proper readme

272
00:17:13,458 --> 00:17:16,230
to it as well so you can use it properly.

273
00:17:16,890 --> 00:17:21,930
Okay, Prometheus is installed. Let's install Yali

274
00:17:22,510 --> 00:17:26,442
and Grafana as well. Awesome. With this

275
00:17:26,496 --> 00:17:30,074
we have installed istio ambient mesh and all

276
00:17:30,112 --> 00:17:33,262
the related add ons to it.

277
00:17:33,396 --> 00:17:37,146
Now let's go ahead. Now in the next step we'll be labeling

278
00:17:37,178 --> 00:17:40,842
our namespace so that it is ambit Imesh enabled,

279
00:17:40,906 --> 00:17:44,366
which I've already done in my configuration. So if

280
00:17:44,388 --> 00:17:48,046
you haven't done it, or maybe you're using a different namespace

281
00:17:48,078 --> 00:17:52,450
which was pre created, you can simply label it like this. Now let's test

282
00:17:52,520 --> 00:17:56,502
it with the Kubectl exec command and see if it's working

283
00:17:56,556 --> 00:17:58,760
or not. Okay,

284
00:17:59,450 --> 00:18:03,538
and while we are here, we might also need to execute

285
00:18:03,554 --> 00:18:06,694
into the Z tunnels as

286
00:18:06,732 --> 00:18:09,660
well, so I'll be doing that.

287
00:18:10,270 --> 00:18:14,214
Before that, let me paste this command and format

288
00:18:14,262 --> 00:18:15,500
it so it works.

289
00:18:17,470 --> 00:18:21,600
Let's log the ztunnels cubectl log.

290
00:18:26,530 --> 00:18:30,014
You can see it's kind of having all the

291
00:18:30,052 --> 00:18:33,620
xds and all service discovery and all is going on.

292
00:18:34,070 --> 00:18:36,450
Okay, what was the other pod?

293
00:18:37,190 --> 00:18:40,574
Cool. So we have these eternal logs printed

294
00:18:40,622 --> 00:18:43,490
here. Now let's run this command.

295
00:18:45,510 --> 00:18:49,006
Nice. We are having a response back. It's coming from

296
00:18:49,048 --> 00:18:52,294
v two. And yeah, in this case it might come from v two. V one.

297
00:18:52,332 --> 00:18:55,142
We are not doing any load balancing stuff yet,

298
00:18:55,196 --> 00:18:58,360
so let it come from wherever it is coming.

299
00:18:59,050 --> 00:19:02,326
Yeah. Now we are having it from v one. And you can see it's

300
00:19:02,358 --> 00:19:05,834
logging all the inbound and outbound proxies. This, you can see,

301
00:19:05,872 --> 00:19:10,060
is basically the kind of Ip of this particular

302
00:19:10,830 --> 00:19:14,240
services, these ztunnel services.

303
00:19:15,090 --> 00:19:19,146
And we have the logs here as well, that it is having an inbound consumption.

304
00:19:19,258 --> 00:19:22,906
We have a residue printed here as well. So we know from where the traffic

305
00:19:22,938 --> 00:19:26,486
is coming. So, yeah, you can see the Z tunnel

306
00:19:26,538 --> 00:19:30,020
is actually working pretty fine here.

307
00:19:30,630 --> 00:19:34,562
And maybe if I do get

308
00:19:34,616 --> 00:19:38,040
SVC system.

309
00:19:38,890 --> 00:19:41,240
Okay, let's see what we have next.

310
00:19:41,770 --> 00:19:45,206
We have already enabled ambient mesh. And next we'll be

311
00:19:45,228 --> 00:19:48,902
having a look at the authorization policies. So we'll be doing two things

312
00:19:48,956 --> 00:19:53,206
here. One is that first, by default we'll block all the requests.

313
00:19:53,398 --> 00:19:56,822
And in the next, what we'll be doing, we'll be specifically

314
00:19:56,886 --> 00:20:00,106
allowing ns one sleep, ns one to talk to

315
00:20:00,128 --> 00:20:03,246
echo ns two. Now, let's have a look at

316
00:20:03,268 --> 00:20:07,326
the configurations at first. So this is the deny all l four.

317
00:20:07,428 --> 00:20:10,814
Basically, if you create an authorization policy here,

318
00:20:10,852 --> 00:20:14,838
I'm just giving the name of the authorization policy and which namespace it belongs

319
00:20:14,874 --> 00:20:18,254
to. So then you can see that if you don't

320
00:20:18,302 --> 00:20:21,986
provide any specifications to it by default, if you simply create an

321
00:20:22,008 --> 00:20:25,266
authorization policy, it's going to deny everything. So let's

322
00:20:25,298 --> 00:20:27,640
see that in action. I'm going here.

323
00:20:28,250 --> 00:20:31,462
Cubectl, apply f, deny all

324
00:20:31,516 --> 00:20:35,510
l four and just created the authorization policies.

325
00:20:36,090 --> 00:20:39,354
And. Yeah, let's sde

326
00:20:39,552 --> 00:20:40,780
if that works.

327
00:20:42,990 --> 00:20:46,634
Copied the command here. So here we're trying to

328
00:20:46,672 --> 00:20:50,166
talk from sleep ns one to echo server

329
00:20:50,198 --> 00:20:54,794
ns two. And we'll be getting a command

330
00:20:54,842 --> 00:20:58,350
terminated with exit code 56. So this is the

331
00:20:58,500 --> 00:21:02,186
policy is basically blocking us to do that even in the same namespace.

332
00:21:02,218 --> 00:21:04,660
If I just want to talk to echo server one,

333
00:21:06,470 --> 00:21:10,254
it's still going to block it. It won't allow anything, any, any communication.

334
00:21:10,302 --> 00:21:13,986
It won't allow. So now let's have a look at

335
00:21:14,008 --> 00:21:17,606
how to enable communications for NS two. Echo server in

336
00:21:17,628 --> 00:21:21,094
NS two. This is the configuration to

337
00:21:21,132 --> 00:21:24,694
it. Sorry, this one. So again, similar to

338
00:21:24,732 --> 00:21:28,406
the deny all one, we are creating an

339
00:21:28,428 --> 00:21:32,326
authorization policy and we are naming it as allow inbound to echo

340
00:21:32,358 --> 00:21:35,946
ns two. You can name it anything you want. And the

341
00:21:35,968 --> 00:21:38,140
namespace as well. We are giving here,

342
00:21:38,590 --> 00:21:42,154
and in the specification, we are simply giving the

343
00:21:42,192 --> 00:21:45,998
match label for the app, which is echo server ns two, and we

344
00:21:46,004 --> 00:21:49,454
are giving the rule. So this action could be either you can allow it,

345
00:21:49,492 --> 00:21:52,954
you can deny, it depends. All of these you can see in the istio

346
00:21:53,002 --> 00:21:56,900
documentation regarding the authorization policies. It's pretty clear there.

347
00:21:58,470 --> 00:22:01,486
So here you can see from, I'm just giving a source.

348
00:22:01,598 --> 00:22:04,626
And in opensource I have to list the principle. So here I'm listing that.

349
00:22:04,648 --> 00:22:08,754
Okay, you allow only the request that is coming from sleep

350
00:22:08,882 --> 00:22:12,294
account ns one. This is very similar to the SVid you are seeing.

351
00:22:12,412 --> 00:22:15,942
It's saying that in local cluster from NS one, just allow

352
00:22:15,996 --> 00:22:19,514
sleep service one to kind of send requests to this guy.

353
00:22:19,632 --> 00:22:22,460
Anything else would be blocked. So let's apply this policy.

354
00:22:23,070 --> 00:22:26,058
Cubectl apply f,

355
00:22:26,144 --> 00:22:29,530
allow communications from Echo ns two,

356
00:22:29,600 --> 00:22:33,006
Yaml once it is applied. Now if we

357
00:22:33,028 --> 00:22:37,262
go back and we try to communicate from NS one sleep to

358
00:22:37,316 --> 00:22:40,830
echo server in NS two, this should work and this

359
00:22:40,900 --> 00:22:44,158
should give us the response back and we have it.

360
00:22:44,244 --> 00:22:47,954
And now if I try to do the same thing with not

361
00:22:47,992 --> 00:22:51,886
sleep, which is in NS one, it would give an error

362
00:22:51,918 --> 00:22:55,630
to me. And even if I tried to do it with sleep,

363
00:22:55,710 --> 00:22:59,560
which is in the same namespace as that of Echo server, that is ns two,

364
00:23:00,170 --> 00:23:03,718
it won't work. Right. So different exit code it is

365
00:23:03,724 --> 00:23:07,206
giving. You can refer the documentation for this. But yeah, on the whole, the L

366
00:23:07,228 --> 00:23:11,226
four authorization policies are working great here. So as

367
00:23:11,248 --> 00:23:14,460
we can see, it is blocking the traffic and it is doing all the things.

368
00:23:16,670 --> 00:23:20,950
Now, next we have got allowing traffic through the ingress gateway.

369
00:23:21,110 --> 00:23:25,086
So this is, we have seen that we are communicating through the,

370
00:23:25,268 --> 00:23:28,366
you can say we are kind of exec into the sleep pod and then we

371
00:23:28,388 --> 00:23:31,786
were just writing curl command to communicate. Now what we'll

372
00:23:31,818 --> 00:23:34,986
do next is we'll allow Ingress Gateway, which is the Istio

373
00:23:35,018 --> 00:23:37,906
ingress gateway, to communicate with Echo NS one,

374
00:23:38,088 --> 00:23:41,826
the Echo server in n namespace one. For this we need to apply the

375
00:23:41,848 --> 00:23:45,314
Istio gateway Yaml file, which is the standard istio gateway. If I

376
00:23:45,352 --> 00:23:48,690
simply go there and you can see in the configuration

377
00:23:50,710 --> 00:23:54,246
I bills, just increase the font size a bit. You can

378
00:23:54,268 --> 00:23:56,150
see we are creating a gateway.

379
00:23:56,970 --> 00:24:00,906
Again, it's just the Kubernetes gateway type. The only selector is this

380
00:24:01,008 --> 00:24:04,266
istio ingress gateway. Okay. And then we are

381
00:24:04,288 --> 00:24:07,542
saying that for port 80, just allow from all the hosts.

382
00:24:07,606 --> 00:24:10,630
So this is pretty standard gateway configuration.

383
00:24:10,790 --> 00:24:14,106
Same thing with the virtual service, the istio

384
00:24:14,138 --> 00:24:17,774
virtual service that we have seen up until now with the sidecar ones

385
00:24:17,812 --> 00:24:21,146
as well. It's just giving a name for a particular namespace.

386
00:24:21,178 --> 00:24:24,574
I'm specifying that use this particular gateway, which is gateway NS

387
00:24:24,622 --> 00:24:28,574
one that I've made here. And all you have to do is route

388
00:24:28,622 --> 00:24:31,540
all the traffics to the NS one service.

389
00:24:32,310 --> 00:24:35,446
I need to apply this file. So if I

390
00:24:35,468 --> 00:24:39,702
go here and I do kubectl, apply f

391
00:24:39,756 --> 00:24:42,070
istiogateway yaml,

392
00:24:42,490 --> 00:24:45,702
it's going to apply everything. And you can also

393
00:24:45,756 --> 00:24:49,946
do it, you can just get the gateway right.

394
00:24:50,048 --> 00:24:53,260
We have applied Gateway NS one, and also the virtual service,

395
00:24:54,030 --> 00:24:57,386
it's associated with this particular gateway. And this is

396
00:24:57,408 --> 00:25:01,482
the external ip for the ingress gateway. Now let's

397
00:25:01,546 --> 00:25:05,120
copy and go over to Google Chrome and see.

398
00:25:05,570 --> 00:25:09,438
So right now you'll be seeing that the connection error is happening because

399
00:25:09,604 --> 00:25:13,470
remember, we applied deny all. So yeah,

400
00:25:13,540 --> 00:25:16,558
it still won't allow the traffic until you enable it externally.

401
00:25:16,654 --> 00:25:20,066
So there's another file which is simply saying similar to what we saw for

402
00:25:20,088 --> 00:25:23,374
the Echo NS two that allow all the inbound

403
00:25:23,422 --> 00:25:26,418
traffic that is coming from the Istio ingress gateway.

404
00:25:26,514 --> 00:25:29,606
Now, istio ingress Gateway itself has a service account, and this is

405
00:25:29,628 --> 00:25:33,174
just what we are referring to right now. Let me apply this

406
00:25:33,212 --> 00:25:38,290
file communication

407
00:25:38,370 --> 00:25:43,254
to echo NS one. And once the authorization

408
00:25:43,302 --> 00:25:46,314
policy is applied, simply go ahead and refresh it.

409
00:25:46,352 --> 00:25:49,434
And now you can see I'm getting everything. You can also see I'm getting

410
00:25:49,472 --> 00:25:52,398
the envoy headers here. So yeah, nice.

411
00:25:52,484 --> 00:25:56,302
We have an external gateway is to ingress gateway, and that is

412
00:25:56,356 --> 00:25:59,578
working well with the ambient mesh workloads.

413
00:25:59,754 --> 00:26:03,186
So moving ahead, we'll be having a look

414
00:26:03,208 --> 00:26:06,626
at the l seven authorization policies. We'll see how to

415
00:26:06,648 --> 00:26:10,530
generate the waypoint proxies. How do you apply waypoint proxies?

416
00:26:11,910 --> 00:26:15,640
So what we'll be doing is we'll be restricting the

417
00:26:16,250 --> 00:26:20,594
sleep ns one to echo ns two communication to only get request.

418
00:26:20,722 --> 00:26:23,910
We won't allow any post or delete or any such request.

419
00:26:24,490 --> 00:26:27,350
So let's first generate the waypoint,

420
00:26:27,710 --> 00:26:31,402
and before using there you can also

421
00:26:31,456 --> 00:26:35,500
get the authorization policies, whatever you applied using the get authorization policy

422
00:26:36,110 --> 00:26:39,594
command in Kubectl. In this case, in NS one,

423
00:26:39,632 --> 00:26:43,386
I think only two were applied, one for the inbound from the

424
00:26:43,568 --> 00:26:45,950
istio ingress, and another was the denial.

425
00:26:46,450 --> 00:26:49,806
Now let's have a look at how to generate the waypoint proxy. You can

426
00:26:49,828 --> 00:26:53,210
use the istio command. X stands for experimental

427
00:26:53,370 --> 00:26:57,378
waypoint generate nns two

428
00:26:57,544 --> 00:27:02,194
and s, which is not the service, but the service account,

429
00:27:02,392 --> 00:27:06,566
which in this case was echo server service

430
00:27:06,668 --> 00:27:11,042
account ns two. Let me quickly verify

431
00:27:11,106 --> 00:27:14,326
it. If that's the case, yeah, eco service account. Okay. Not the

432
00:27:14,348 --> 00:27:16,920
server. Nice.

433
00:27:17,850 --> 00:27:21,226
Okay, so this, as you can see, it has

434
00:27:21,248 --> 00:27:24,826
generated the standard Kubernetes gateway API, yaml. You can see

435
00:27:24,848 --> 00:27:28,630
the kind is gateway. The only difference is that you have an annotation

436
00:27:28,710 --> 00:27:32,266
which specifies that this is for which service account.

437
00:27:32,448 --> 00:27:36,366
In this case, it is for echo service account ns two. And we

438
00:27:36,388 --> 00:27:39,966
have the name here as well. It's just taking the same service account name and

439
00:27:39,988 --> 00:27:42,640
putting it as a name and namespaces. Ns two,

440
00:27:43,810 --> 00:27:47,694
in specification, we are specifying that the gateway class is istio waypoint

441
00:27:47,742 --> 00:27:51,198
and just a listener for the hbone. For the Hbone.

442
00:27:51,214 --> 00:27:54,818
Okay, so what I've done is actually you

443
00:27:54,824 --> 00:27:58,310
can directly apply it from here as well instead of generate. If you hit

444
00:27:58,460 --> 00:28:02,258
apply, it would still work. So what I've

445
00:28:02,274 --> 00:28:05,954
done is I've copied this over to a file and I've just changed the gateway

446
00:28:06,002 --> 00:28:09,942
name so it won't be confusing because we already

447
00:28:09,996 --> 00:28:13,610
have the names similar to what we had in the service account name.

448
00:28:13,760 --> 00:28:17,974
So let's apply this file instead. Again, it's a standard Yaml

449
00:28:18,022 --> 00:28:20,780
file. You can apply, it doesn't matter.

450
00:28:21,550 --> 00:28:25,440
Waypoint proxy, yaml enter.

451
00:28:26,210 --> 00:28:29,470
Nice. We have created the gateway.

452
00:28:29,890 --> 00:28:32,174
Okay, now let's have a look.

453
00:28:32,372 --> 00:28:36,350
Cubectl, get gateway.

454
00:28:37,490 --> 00:28:41,010
NS two. See, we have an echo service.

455
00:28:41,080 --> 00:28:44,194
And to gateway, the class is istio proxy. It has

456
00:28:44,232 --> 00:28:48,014
its own address as well, which you can see the gateway

457
00:28:48,062 --> 00:28:53,346
name and the istio vapor and proxy. But let's

458
00:28:53,378 --> 00:28:55,720
also get the pod in ns two.

459
00:28:56,250 --> 00:28:59,586
Now you can see there is a service, the waypoint

460
00:28:59,618 --> 00:29:02,380
proxy pod here as well, which is running.

461
00:29:02,750 --> 00:29:05,958
So as we had seen, that waypoint is just a pod.

462
00:29:06,054 --> 00:29:08,778
It's nothing different. It's just a pod that is running.

463
00:29:08,944 --> 00:29:11,662
You can scale it as and when you like.

464
00:29:11,796 --> 00:29:16,046
Let's just log it and see what

465
00:29:16,068 --> 00:29:19,646
is happening there. Pretty standard on

466
00:29:19,668 --> 00:29:23,922
web proxy logs here. You can see nothing fancy here.

467
00:29:24,056 --> 00:29:28,482
Anyways, so we have deployed that. Now let's again

468
00:29:28,536 --> 00:29:31,540
try to query the ns two.

469
00:29:31,990 --> 00:29:36,030
Echo service is ns two with service

470
00:29:36,120 --> 00:29:40,242
one. So right now, I haven't applied any l seven policies

471
00:29:40,306 --> 00:29:43,942
yet. We simply have the waypoint proxy and we are just

472
00:29:43,996 --> 00:29:47,926
doing the curl command now. Trust, see what happens if

473
00:29:47,948 --> 00:29:51,106
I run it. So previously we had already applied the l four

474
00:29:51,148 --> 00:29:54,394
authorization policies to deny all, and then we

475
00:29:54,432 --> 00:29:58,938
allowed the communication. But since I have applied the Waypoint proxy as well,

476
00:29:59,104 --> 00:30:02,842
I am getting an RBAC error here saying

477
00:30:02,896 --> 00:30:06,586
that access is denied because right now we have the Waypoint proxy,

478
00:30:06,618 --> 00:30:09,678
but we are again not allowing what things we need to do.

479
00:30:09,844 --> 00:30:14,010
So the default is basically it's getting access denied in

480
00:30:14,020 --> 00:30:17,474
this case. So in order for us to allow the get

481
00:30:17,512 --> 00:30:21,054
request, we can make another policy authorization

482
00:30:21,102 --> 00:30:24,820
policy. In this case it is going to be an authorization policy

483
00:30:25,510 --> 00:30:29,302
for l seven. As you can see, I am having a gateway name

484
00:30:29,356 --> 00:30:32,514
associated with it, and in this case I'm using the same gateway

485
00:30:32,562 --> 00:30:35,986
name which we had used in the case of waypoint proxy.

486
00:30:36,098 --> 00:30:39,450
So this authorization policy would be applied for that gateway.

487
00:30:39,950 --> 00:30:43,514
We are allowing the request and the rule is

488
00:30:43,552 --> 00:30:47,546
simple. Whatever was previously there from the

489
00:30:47,648 --> 00:30:52,006
source and the principle is basically this. And two here stands

490
00:30:52,038 --> 00:30:55,902
for what kind of operations are we going to do? You can again refer

491
00:30:55,956 --> 00:30:59,546
to the documentation for further kind of in depth

492
00:30:59,578 --> 00:31:02,414
guide as to what these configuration would do.

493
00:31:02,612 --> 00:31:06,034
And in this case we are simply using operations and saying that,

494
00:31:06,072 --> 00:31:09,746
okay, allow, method, get. We are not using anything else.

495
00:31:09,848 --> 00:31:13,390
We are allowing that from this particular cluster

496
00:31:13,550 --> 00:31:17,030
to echo server, just allow methods, that is get request.

497
00:31:17,530 --> 00:31:21,590
Let's apply this file Uctl apply f

498
00:31:21,660 --> 00:31:25,000
allow get request to echo ns two.

499
00:31:25,770 --> 00:31:29,340
Now it is created. Now if I try to do the same thing,

500
00:31:29,950 --> 00:31:33,770
I'm getting a successful response. Let's try it with post

501
00:31:33,840 --> 00:31:37,290
request. And we are getting an access denied.

502
00:31:37,710 --> 00:31:41,194
Cool. So this is working. This is the l

503
00:31:41,232 --> 00:31:44,550
seven network policies in action,

504
00:31:44,630 --> 00:31:47,674
as you can see. So this was a very basic

505
00:31:47,722 --> 00:31:51,610
example of how we would apply the l seven authorization policies.

506
00:31:51,770 --> 00:31:55,214
So similar to what we have seen in l four authorization

507
00:31:55,262 --> 00:31:59,410
policies. If you don't provide a spec selector by default, it will block everything.

508
00:31:59,480 --> 00:32:02,740
We have already seen that since we had not applied anything.

509
00:32:03,190 --> 00:32:07,250
And in this case also if you don't apply anything

510
00:32:07,400 --> 00:32:11,334
in operations, the two is basically specifying what all operations you

511
00:32:11,372 --> 00:32:14,662
want. So if I simply comment this out and I don't apply,

512
00:32:14,716 --> 00:32:18,034
it will allow everything that in this case we are restricting

513
00:32:18,082 --> 00:32:21,846
only to get, it will allow get postponed and all those things in context

514
00:32:21,878 --> 00:32:25,066
of this example. But since we are restricting to the set

515
00:32:25,088 --> 00:32:28,394
of operations as methods get only, it will

516
00:32:28,432 --> 00:32:33,214
only apply the get request to kind of apply

517
00:32:33,252 --> 00:32:36,894
the get request to pass through. Now. Next we'll be

518
00:32:36,932 --> 00:32:40,458
having a look at traffic management. So we'll

519
00:32:40,474 --> 00:32:44,254
be doing canary release here we have already seen from sleep ns one.

520
00:32:44,292 --> 00:32:48,434
We are able to query echo ns two with the get request. Now we

521
00:32:48,472 --> 00:32:52,034
want that. Okay, the 90% of traffic should be ending up at v

522
00:32:52,072 --> 00:32:55,366
one and the 10% at v two. So to do

523
00:32:55,388 --> 00:32:59,042
this we'll be applying a canary Yaml file.

524
00:32:59,186 --> 00:33:03,720
So as you can see, this is pretty standard.

525
00:33:04,250 --> 00:33:08,022
You can see whatever you would do in istio with

526
00:33:08,076 --> 00:33:11,234
sidecars a virtual service that is applied

527
00:33:11,282 --> 00:33:14,986
on a particular host. In this case, this is the host of Echo server which

528
00:33:15,008 --> 00:33:18,602
is in ns two. And in route destination, we are only

529
00:33:18,656 --> 00:33:22,602
saying that, okay, for subset V one, make 90% of the

530
00:33:22,656 --> 00:33:26,490
request approximately go to subset V one for this particular host.

531
00:33:26,650 --> 00:33:30,046
And in the other one we are saying that, okay, for 10% of

532
00:33:30,068 --> 00:33:33,342
the time take it to subset V two. Now the definition of these

533
00:33:33,396 --> 00:33:36,786
subsets are there in the destination rule that we

534
00:33:36,808 --> 00:33:40,654
have set here. The destination rule is again applied on ns

535
00:33:40,702 --> 00:33:44,258
two and it is selecting echo server service ns two.

536
00:33:44,344 --> 00:33:48,178
I haven't applied a full qualified domain name here because it's in the same namespace.

537
00:33:48,194 --> 00:33:52,582
So I'm just showing this example that yeah, it would still work without the

538
00:33:52,636 --> 00:33:56,310
FQDN. Okay, so in

539
00:33:56,460 --> 00:34:00,294
subset V one we are simply selecting echo server app ns two and the

540
00:34:00,332 --> 00:34:03,766
version of the app which is v one. And in V two we are simply

541
00:34:03,798 --> 00:34:07,494
selecting echo server app that is ns two and the version

542
00:34:07,542 --> 00:34:10,586
which is v two. Pretty standard. Basically if you

543
00:34:10,608 --> 00:34:14,110
apply this to the sidecar enabled service mesh, it would still work,

544
00:34:14,180 --> 00:34:17,834
nothing different. So now we are going to do kubectl

545
00:34:17,882 --> 00:34:22,614
apply my f canary.

546
00:34:22,762 --> 00:34:25,540
Yaml. Let's wait.

547
00:34:26,150 --> 00:34:29,762
Nice. Let's see kubectl get

548
00:34:29,816 --> 00:34:33,380
destination rule. In ns two

549
00:34:33,830 --> 00:34:37,880
we have the echo service one and also the virtual service.

550
00:34:38,810 --> 00:34:42,710
Awesome. Now let's test it. So what we'll be

551
00:34:42,780 --> 00:34:46,246
doing is so we'll run

552
00:34:46,268 --> 00:34:50,874
a command which will send 100 request from the

553
00:34:50,992 --> 00:34:54,662
sleep of NS one to the echo server of NS

554
00:34:54,726 --> 00:34:58,300
two. If I can,

555
00:34:59,390 --> 00:35:02,800
oops, I can properly format it.

556
00:35:03,410 --> 00:35:07,246
We should be good to go. Yeah, let's run it.

557
00:35:07,428 --> 00:35:11,246
It's going to take some time because it's going to fire 100 kind

558
00:35:11,268 --> 00:35:15,506
of requests to this particular service.

559
00:35:15,608 --> 00:35:18,740
And as you can see for version one,

560
00:35:19,350 --> 00:35:22,866
I'm just doing a grab to get whatever version it is

561
00:35:22,888 --> 00:35:26,706
there. In the logs, we can see that 91 of them have this echo

562
00:35:26,738 --> 00:35:30,678
server deployment ns two version one. And if I simply change it

563
00:35:30,684 --> 00:35:34,854
to version two, we'll approximately get ten

564
00:35:34,892 --> 00:35:37,110
requests. In this case. Okay, it's 16.

565
00:35:37,930 --> 00:35:41,786
That's still fine. Again, it's not going to be a perfect division of

566
00:35:41,808 --> 00:35:45,114
percentage. It will try to approximate it as much as it can.

567
00:35:45,312 --> 00:35:48,774
So we are seeing here different number, and you can see the variation

568
00:35:48,822 --> 00:35:52,334
is quite huge. So, okay, now let's see,

569
00:35:52,372 --> 00:35:55,966
what exactly was it sending here? If I simply do

570
00:35:55,988 --> 00:35:59,406
the standard curl command instead of any grep or

571
00:35:59,428 --> 00:36:02,990
anything. So let's see what we are actually picking.

572
00:36:07,190 --> 00:36:10,722
So we already have a hostname that was being printed and

573
00:36:10,776 --> 00:36:14,574
for different part it's going to be different. So this is what the grep

574
00:36:14,622 --> 00:36:17,846
was doing and we are simply counting the occurrences. So in this

575
00:36:17,868 --> 00:36:20,600
case it is again version one. Let's run it again.

576
00:36:22,410 --> 00:36:25,960
It's version one. Again version one.

577
00:36:28,010 --> 00:36:31,606
Since 90% of request is going to version one, we are most likely

578
00:36:31,718 --> 00:36:34,906
to get this. So as you can see,

579
00:36:34,928 --> 00:36:38,970
canary releases is working well. You can play around, do whatever

580
00:36:39,040 --> 00:36:43,870
kind of 50 50, you can do a blue green or whatever

581
00:36:43,940 --> 00:36:47,374
things you could do with istio. You can do the same

582
00:36:47,412 --> 00:36:50,958
thing with istio ambient mesh, and it's nothing

583
00:36:51,044 --> 00:36:54,100
short and it's pretty performant as we can see right now.

584
00:36:54,550 --> 00:36:58,626
So after traffic management, let's have a look at the observability and

585
00:36:58,648 --> 00:37:02,514
debugging capabilities of istio similar to that what we are

586
00:37:02,552 --> 00:37:06,262
seeing in sidecar. We'll get the same things here as well.

587
00:37:06,396 --> 00:37:08,982
So I'll be using. Let's go to Prometheus first.

588
00:37:09,036 --> 00:37:13,350
Istio dashboard Prometheus.

589
00:37:14,090 --> 00:37:18,150
So you can see I'm having all the istio related matrix

590
00:37:18,990 --> 00:37:22,778
request total. Yeah, I'm getting everything

591
00:37:22,864 --> 00:37:27,350
you can see from destination service gateways.

592
00:37:27,510 --> 00:37:30,394
This is, by the way, the ingress ones here.

593
00:37:30,432 --> 00:37:33,390
Also you can see the app was issue ingress.

594
00:37:33,810 --> 00:37:37,134
You won't see a lot whatever we have done, from the curl to

595
00:37:37,172 --> 00:37:40,270
the ns one, which we had tested previously

596
00:37:41,010 --> 00:37:44,954
before applying the deny all, because those are some of the TCP metrics.

597
00:37:45,082 --> 00:37:48,674
The L seven ones would come from envoy itself.

598
00:37:48,792 --> 00:37:51,938
So in this case, ingress gateway is envoy. So that's why we are

599
00:37:51,944 --> 00:37:55,202
having L seven capabilities. And the waypoint proxy itself

600
00:37:55,336 --> 00:37:58,822
is an envoy proxy. That's why we are having all the L seven

601
00:37:58,876 --> 00:38:02,114
metrics from there as well. However, all the L four metrics

602
00:38:02,162 --> 00:38:06,086
were there. Once you apply the ztunnel, you'll be

603
00:38:06,108 --> 00:38:09,530
getting all the tcp consumption opened, lost total,

604
00:38:09,600 --> 00:38:13,030
all those counts. Everything would be there. So the matrix,

605
00:38:13,190 --> 00:38:16,106
as you can see in Prometheus, it's working great.

606
00:38:16,288 --> 00:38:18,700
Let's have a look at Kiali as well.

607
00:38:20,290 --> 00:38:23,546
If I go to graph and I'm

608
00:38:23,578 --> 00:38:26,000
choosing last one, r maybe.

609
00:38:26,450 --> 00:38:29,360
Okay, I'll first. Okay,

610
00:38:29,890 --> 00:38:33,534
so now you can see in sleep ns one to echo service.

611
00:38:33,732 --> 00:38:36,946
We are already seeing the traffic flowing to version one

612
00:38:36,968 --> 00:38:40,706
and version two. The green ones are the previous ones which

613
00:38:40,728 --> 00:38:43,906
we had deployed before the service mesh was there.

614
00:38:44,008 --> 00:38:47,642
So this is just the general ones, whatever that has been captured.

615
00:38:47,806 --> 00:38:51,030
Once the service mesh is here, you can see that in the.

616
00:38:51,180 --> 00:38:54,886
If I go to app graph itself, you can see everything is

617
00:38:54,908 --> 00:38:58,758
in blue, is kind of tcp encrypted. Okay,

618
00:38:58,844 --> 00:39:01,900
where is this version app graph? Right.

619
00:39:02,350 --> 00:39:05,738
We had some of the failures as well, which is being captured here.

620
00:39:05,904 --> 00:39:10,410
So yeah, pretty much everything is working in the observability section.

621
00:39:10,770 --> 00:39:13,120
Let's have a look at Grafana as well.

622
00:39:16,290 --> 00:39:19,534
I'm going to the available dashboard. You can however choose to create

623
00:39:19,572 --> 00:39:22,878
your own dashboard. Go to the control plane dashboard

624
00:39:22,894 --> 00:39:25,380
and see what kind of metrics we are getting.

625
00:39:26,230 --> 00:39:29,934
So we have a cpu usage and we have our endpoint

626
00:39:29,982 --> 00:39:33,780
listener pilots. Everything, whatever we had seen

627
00:39:34,470 --> 00:39:37,780
in the istio sidecar version, we are getting everything here.

628
00:39:38,250 --> 00:39:41,430
Let's go to some other dashboard, maybe the mesh.

629
00:39:42,810 --> 00:39:47,014
Okay, what's there in the mesh one? Yeah, some of the information

630
00:39:47,132 --> 00:39:50,502
related to mesh, how many authorization policies we have applied,

631
00:39:50,646 --> 00:39:52,620
what interval and all those things.

632
00:39:53,310 --> 00:39:56,854
So yeah, we also have the service workload request

633
00:39:56,902 --> 00:40:00,426
and all those things, whatever was captured here. So you

634
00:40:00,448 --> 00:40:04,618
can also do pretty much everything what you were doing with istio related to observability

635
00:40:04,714 --> 00:40:08,734
here. So yeah, that pretty much sums it all.

636
00:40:08,932 --> 00:40:12,094
So apart from observability, we can also

637
00:40:12,132 --> 00:40:14,926
use istio CTL to debug our proxies.

638
00:40:15,118 --> 00:40:18,594
So let's have a look at that. So you can use

639
00:40:18,712 --> 00:40:22,546
istio CTL proxy status to get the

640
00:40:22,568 --> 00:40:26,194
status of all the proxies deployed. So in this

641
00:40:26,232 --> 00:40:29,254
case you can see especially the envoy proxies in this case,

642
00:40:29,292 --> 00:40:32,534
which is the waypoint and the ingress gateway, they all

643
00:40:32,572 --> 00:40:36,114
have the endpoint discovery service and ecds

644
00:40:36,162 --> 00:40:39,626
and all those things, they are synced and all the information that you can see

645
00:40:39,648 --> 00:40:42,694
you're getting. The ztunnel, however, is not an envoy proxy.

646
00:40:42,742 --> 00:40:45,994
So all of these things are ignored. You can also

647
00:40:46,032 --> 00:40:49,834
use istio proxy config to have

648
00:40:49,872 --> 00:40:53,420
a deeper understanding of the consumption of these particular

649
00:40:53,730 --> 00:40:57,086
envoy proxies. If you just go to the help section, you'll have

650
00:40:57,108 --> 00:41:00,926
a list of all the available commands that are there and you

651
00:41:00,948 --> 00:41:04,930
can use it to debug your proxies. So let's try

652
00:41:05,000 --> 00:41:07,940
something like proxy get all.

653
00:41:08,630 --> 00:41:12,254
So we'll be retrieving all the configuration for this specific envoy

654
00:41:12,382 --> 00:41:15,860
proxy. In this case we'll be using the Echo service one.

655
00:41:18,150 --> 00:41:21,074
And make sure that once you're giving the name of the pod,

656
00:41:21,122 --> 00:41:24,694
you also give the namespace after the dot. So this is how

657
00:41:24,732 --> 00:41:28,294
it works. So let's see what we are getting. We can see

658
00:41:28,332 --> 00:41:31,658
that we are getting entire overview of all the

659
00:41:31,824 --> 00:41:35,366
full qualified domain name services it has, what port they listened

660
00:41:35,398 --> 00:41:39,034
on, or what is the direction of the traffic. Also what

661
00:41:39,072 --> 00:41:42,150
virtual services you are using for this particular inbound.

662
00:41:42,310 --> 00:41:45,322
So this is a pretty handy tool for you to debug,

663
00:41:45,466 --> 00:41:49,358
and it also gives information about the root ca as well, as you

664
00:41:49,364 --> 00:41:53,022
can see here. So you can also play around

665
00:41:53,076 --> 00:41:56,702
with this, and istio CTL is a very powerful tool

666
00:41:56,756 --> 00:41:58,480
for you to debug as well.

667
00:42:00,290 --> 00:42:03,950
That's all for the observability and debugging. Okay,

668
00:42:04,020 --> 00:42:08,258
so if you have been this far, then good job. So we have successfully

669
00:42:08,434 --> 00:42:11,686
added ambient imesh to our cluster, and if

670
00:42:11,708 --> 00:42:15,110
you have any questions related to it, feel free to reach out

671
00:42:15,180 --> 00:42:18,914
to me on LinkedIn, or maybe even just create an issue in the GitHub

672
00:42:18,962 --> 00:42:22,646
repository that I'd shared. So thank you everyone.

673
00:42:22,748 --> 00:42:26,466
Once again, thank you to conf 42 for having me. This is MD

674
00:42:26,498 --> 00:42:27,380
Azmal signing off.

