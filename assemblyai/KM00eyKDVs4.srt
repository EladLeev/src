1
00:00:25,970 --> 00:00:30,050
Good day, everyone. The topic for today would be machine learning engineering

2
00:00:30,130 --> 00:00:33,506
with Python. The title of my talk is machine

3
00:00:33,538 --> 00:00:37,270
learning engineering done right, designing and building

4
00:00:37,420 --> 00:00:40,662
complex intelligent systems and workflows with

5
00:00:40,716 --> 00:00:44,578
Python. So first I will introduce myself. I am Joshua Arvin

6
00:00:44,594 --> 00:00:48,354
Lat and I am the chief technology officer of Nuworks Interactive

7
00:00:48,402 --> 00:00:51,898
Labs. I am also an AWS machine learning hero,

8
00:00:51,994 --> 00:00:55,582
and I'm also one of the subject matter experts who

9
00:00:55,636 --> 00:00:59,470
has helped contribute to the AWS certified machine learning specialty exam.

10
00:00:59,540 --> 00:01:03,322
So if you were to take that exam, most likely one of the questions there

11
00:01:03,396 --> 00:01:07,042
was probably from me. I'm also the author of a machine learning

12
00:01:07,096 --> 00:01:11,358
and machine learning engineering book on AWS called Amazon Sagemaker

13
00:01:11,374 --> 00:01:15,894
Cookbook. So Amazon Sagemaker is a machine learning service

14
00:01:16,012 --> 00:01:20,034
and platform from AWS where you can perform experiments

15
00:01:20,082 --> 00:01:23,570
and deployments. So you can use your favorite

16
00:01:23,650 --> 00:01:27,350
machine learning and deep learning framework and still use that with

17
00:01:27,420 --> 00:01:30,714
Sagemaker. And you're able to make the most out of

18
00:01:30,752 --> 00:01:34,346
Sagemaker by using a lot of its capabilities to

19
00:01:34,368 --> 00:01:38,166
help you get your machine learning experiments and deployments successful.

20
00:01:38,278 --> 00:01:41,914
Today, we will talk about ten things. The first one would be

21
00:01:41,952 --> 00:01:45,902
understanding the needs of the business and the customers when dealing with machine learning

22
00:01:45,956 --> 00:01:49,774
and machine learning engineering requirements. The second one would be

23
00:01:49,892 --> 00:01:52,990
about knowing when to write production level python code.

24
00:01:53,060 --> 00:01:56,686
The third one would be enforcing practical Python coding guidelines

25
00:01:56,718 --> 00:02:00,306
for your team. The fourth one would be using Python design patterns and

26
00:02:00,328 --> 00:02:04,174
metaprogramming techniques. The fifth one would be utilizing continuous

27
00:02:04,222 --> 00:02:07,894
integration and deployment pipelines. The 6th one would be on making

28
00:02:07,932 --> 00:02:10,994
the most out of ML frameworks and ML platforms.

29
00:02:11,122 --> 00:02:15,074
The 6th one would be working with automated ML Bias

30
00:02:15,122 --> 00:02:18,402
detection and ML explainability capabilities.

31
00:02:18,546 --> 00:02:21,754
The 8th one would be on reaping the benefits of cloud computing for

32
00:02:21,792 --> 00:02:24,954
automated hyperparameter optimization jobs. So of course

33
00:02:24,992 --> 00:02:28,838
we'll explain what HBO is when we talk about that slide.

34
00:02:28,934 --> 00:02:32,270
And then number nine would be optimizing cost by using

35
00:02:32,340 --> 00:02:35,662
transient ML instances for training models. So later

36
00:02:35,716 --> 00:02:39,274
we'll talk about a quick example on how to fine tune

37
00:02:39,402 --> 00:02:42,974
Bert models later when using Sagemaker. And the number

38
00:02:43,012 --> 00:02:46,526
ten would be securing machine learning environments. So without further

39
00:02:46,558 --> 00:02:50,222
ado, let's have a quick game. You can see a bunch of apples.

40
00:02:50,366 --> 00:02:54,034
So in this game, if you guess it correctly, I may

41
00:02:54,072 --> 00:02:57,346
give you a price. So basically, how would this game work?

42
00:02:57,448 --> 00:03:00,914
So within the next ten to 15 seconds, the goal

43
00:03:00,962 --> 00:03:04,530
here is for us to count the number of apples in this slide.

44
00:03:04,610 --> 00:03:08,586
So again, within the next ten to 15 seconds, I want you guys to

45
00:03:08,608 --> 00:03:12,422
count the number of apples in this slide. So, timer starts

46
00:03:12,486 --> 00:03:14,890
now I'll have a quick countdown. Ten.

47
00:03:15,040 --> 00:03:20,106
9876-5432 and

48
00:03:20,208 --> 00:03:23,102
one. All right, so time's up.

49
00:03:23,236 --> 00:03:26,542
So again, the goal is to count the number of apples. So if you have

50
00:03:26,596 --> 00:03:30,810
answered, let's say, 18. Drumroll, please. That's incorrect.

51
00:03:30,890 --> 00:03:34,786
Unfortunately, that's not the correct answer. So how about

52
00:03:34,888 --> 00:03:38,030
2020 apples? Unfortunately,

53
00:03:38,190 --> 00:03:42,434
that's also incorrect. So what's the correct answer here? The correct answer here

54
00:03:42,472 --> 00:03:46,318
is that it's not possible to count the number of apples in this slide.

55
00:03:46,414 --> 00:03:49,798
So that's sad news for all of us. So the question is, why?

56
00:03:49,884 --> 00:03:53,526
So the first thing here, if you look at the screen, is that we

57
00:03:53,548 --> 00:03:57,346
cannot see the apples underneath this first layer of apples.

58
00:03:57,538 --> 00:04:00,794
And the same way goes when dealing with our day to day

59
00:04:00,832 --> 00:04:04,726
jobs. Sometimes when we are dealing with technical requirements, when we're

60
00:04:04,758 --> 00:04:08,486
using these awesome tools and frameworks to solve our jobs,

61
00:04:08,598 --> 00:04:12,014
the problem there is that we become too focused on what we're doing,

62
00:04:12,132 --> 00:04:15,374
and we tend to forget what the business and the customers need.

63
00:04:15,492 --> 00:04:19,310
So the technique here is to first, let's listen,

64
00:04:19,460 --> 00:04:23,598
let's understand what the context is, because maybe

65
00:04:23,764 --> 00:04:26,978
we may be able to provide the best solution without

66
00:04:27,064 --> 00:04:30,370
any coding work at all. And there may be times

67
00:04:30,440 --> 00:04:33,906
when we can just use a specific AI or machine learning service

68
00:04:34,008 --> 00:04:37,778
where with ten lines of python code, you would be able to solve the customers

69
00:04:37,864 --> 00:04:41,334
problems. So being able to listen to the needs of the customers and

70
00:04:41,372 --> 00:04:45,046
being able to listen to the needs of the business, that's the

71
00:04:45,068 --> 00:04:48,646
number one priority that you have to think about as a

72
00:04:48,668 --> 00:04:52,186
professional. You do not have to be a manager or a

73
00:04:52,208 --> 00:04:56,474
boss to know about these things, because if you're working on something,

74
00:04:56,592 --> 00:05:00,234
you need to make sure that your customers are winning and the business

75
00:05:00,272 --> 00:05:03,886
is winning as well. All right, so the second topic would be

76
00:05:03,908 --> 00:05:07,662
on knowing when to write production level Python code.

77
00:05:07,796 --> 00:05:11,802
So, of course, for us who have been working and using Python

78
00:05:11,866 --> 00:05:15,682
for the past couple of years, you are probably aware that

79
00:05:15,816 --> 00:05:19,314
there's different ways CTO use Python. Let's say that you

80
00:05:19,352 --> 00:05:23,106
are a data scientist and you want to explore the data and to

81
00:05:23,128 --> 00:05:26,866
show a couple of charts showing the properties

82
00:05:27,058 --> 00:05:30,950
and basically the relationships of the data

83
00:05:31,020 --> 00:05:35,366
points in your data set, then that would fall under

84
00:05:35,468 --> 00:05:39,130
machine learning experiment, and you may use tools like

85
00:05:39,200 --> 00:05:42,362
Jupyter notebook to demonstrate and show

86
00:05:42,416 --> 00:05:45,914
the output of your Python code. So there you may not need

87
00:05:45,952 --> 00:05:49,782
engineering techniques to work on your Python

88
00:05:49,846 --> 00:05:53,578
code managing. Even if you're not following certain set of rules,

89
00:05:53,674 --> 00:05:57,914
then that's okay, because that's just for demonstration purposes.

90
00:05:58,042 --> 00:06:02,058
But when you need CTO work on systems,

91
00:06:02,234 --> 00:06:05,794
then definitely you have to follow the engineering techniques and

92
00:06:05,832 --> 00:06:08,850
guidelines to get that to work. So, for example,

93
00:06:09,000 --> 00:06:12,482
if you were to build a machine learning prediction endpoint using

94
00:06:12,536 --> 00:06:15,854
flask and Python, then you would need to follow,

95
00:06:15,912 --> 00:06:19,526
let's say peP eight or the other coding guidelines, as well as

96
00:06:19,708 --> 00:06:23,222
applying the engineering techniques to make sure that

97
00:06:23,276 --> 00:06:27,430
your website or your endpoint is always up and running

98
00:06:27,580 --> 00:06:30,714
and it's going to return a response in less than

99
00:06:30,752 --> 00:06:34,634
1 second, for example. So making sure that your python code is clean is

100
00:06:34,672 --> 00:06:39,110
essential when you're working on engineering tasks.

101
00:06:39,270 --> 00:06:43,482
The third one would be on enforcing practical Python coding guidelines

102
00:06:43,546 --> 00:06:47,290
for your team. So now let's talk about how do you manage teams.

103
00:06:47,370 --> 00:06:51,678
So this is very important for ML engineering managers or maybe

104
00:06:51,844 --> 00:06:55,774
data science leaders, right? So let's say that you have a data science

105
00:06:55,822 --> 00:06:59,774
team and you have a team there focused on building machine

106
00:06:59,822 --> 00:07:03,426
learning, engineering platforms or endpoints, then this

107
00:07:03,448 --> 00:07:06,726
is for you. So what has worked for me in the past is when we

108
00:07:06,748 --> 00:07:10,806
were building a machine learning endpoint for a

109
00:07:10,828 --> 00:07:14,930
product, I realized that it's going to be a bit tricky

110
00:07:15,010 --> 00:07:18,242
when dealing with multiple developers and engineers.

111
00:07:18,386 --> 00:07:22,646
Right? So the goal there, before you can actually perform code reviews,

112
00:07:22,678 --> 00:07:25,926
is that it's better to set standards for the company. So if you're

113
00:07:25,958 --> 00:07:29,930
a CTO, then this is going to be one of your roles, because having

114
00:07:30,000 --> 00:07:33,078
rules allow people and professionals in your

115
00:07:33,104 --> 00:07:37,214
team to have some sort of way to accomplish their work,

116
00:07:37,332 --> 00:07:40,910
right? So if you have rules, if you have standards, then that's going to help

117
00:07:40,980 --> 00:07:45,298
your people perform their jobs better. So one of the rules that definitely

118
00:07:45,384 --> 00:07:49,634
has helped me in the past would be the 20 line rule. So here we

119
00:07:49,672 --> 00:07:53,586
have something like the maximum number of lines inside

120
00:07:53,688 --> 00:07:57,190
a python function or method. So let's say that you have

121
00:07:57,260 --> 00:08:00,454
a function called load model. If the number

122
00:08:00,492 --> 00:08:04,134
of lines in that function exceeds 20 lines, let's say

123
00:08:04,172 --> 00:08:07,702
25 lines, then you have to divide that function into,

124
00:08:07,756 --> 00:08:11,702
let's say three or four sub functions. This allows your code to be cleaner

125
00:08:11,766 --> 00:08:15,126
and more organized. The second one would be following the pep

126
00:08:15,158 --> 00:08:18,838
eight guidelines, or a similar set of guidelines when using Python.

127
00:08:18,934 --> 00:08:22,426
So having something like that would definitely be helpful for your team,

128
00:08:22,528 --> 00:08:25,870
regardless if you're trying to build a machine learning platform

129
00:08:25,940 --> 00:08:29,102
or not. So if you're using Python, try to look at pep eight.

130
00:08:29,236 --> 00:08:33,162
The third one would be avoidance of try catch blocks.

131
00:08:33,306 --> 00:08:36,878
So why? The goal here is to be able to detect

132
00:08:36,974 --> 00:08:40,402
errors as early as possible. The problem when using try

133
00:08:40,456 --> 00:08:44,466
catch blocks, if you're not careful enough, is that sometimes if

134
00:08:44,488 --> 00:08:48,226
you have a transformation and then you just wrap a transaction with try catch

135
00:08:48,258 --> 00:08:51,686
blocks, sometimes the error disappears and you

136
00:08:51,708 --> 00:08:55,206
may not have the ability to debug the problems when

137
00:08:55,228 --> 00:08:59,222
you're dealing with production endpoints and environments. So here,

138
00:08:59,276 --> 00:09:03,274
let's say that you have 10,000 transactions, and then

139
00:09:03,312 --> 00:09:08,202
for some reason in your logs, you only have 9950

140
00:09:08,336 --> 00:09:11,898
records there. So what happened to the 50 records? What went wrong?

141
00:09:11,984 --> 00:09:15,262
So the goal there is to be able to have some

142
00:09:15,316 --> 00:09:18,586
way to know what happened to those 50 transactions.

143
00:09:18,698 --> 00:09:22,622
So if you don't have logs and if you suddenly use a try

144
00:09:22,676 --> 00:09:25,898
catch block to prevent that endpoint from failing,

145
00:09:25,994 --> 00:09:29,246
then you would have no way to debug what went wrong, and those records

146
00:09:29,278 --> 00:09:32,894
and transactions may be lost. The fourth one would be writing testable

147
00:09:32,942 --> 00:09:36,318
Python code. So when you're building systems,

148
00:09:36,414 --> 00:09:40,370
it's very important for us to know that it is an iterative

149
00:09:40,450 --> 00:09:44,086
process. So when you're writing function, you're not just writing one

150
00:09:44,108 --> 00:09:48,098
big block of code. You want to write functions and methods

151
00:09:48,194 --> 00:09:51,738
and classes that allow you to easily debug this code,

152
00:09:51,824 --> 00:09:55,354
let's say with a console. So it's not just about having a web

153
00:09:55,392 --> 00:09:58,922
application ready there, it's about having a console also

154
00:09:58,976 --> 00:10:03,066
to easily debug how a function behaves. So even if you're not practicing

155
00:10:03,258 --> 00:10:06,894
automated testing inside your company, then at least make your code

156
00:10:07,012 --> 00:10:09,982
testable. So try to take a look at that.

157
00:10:10,116 --> 00:10:13,714
The next one would be using Python design patterns and

158
00:10:13,752 --> 00:10:16,882
metaprogramming techniques. So I won't discuss here

159
00:10:17,016 --> 00:10:20,718
all the different Python design patterns and metaprogramming techniques,

160
00:10:20,814 --> 00:10:24,702
but I would mention some of the recommended goals

161
00:10:24,846 --> 00:10:28,150
and techniques that you can use in your company. So one example

162
00:10:28,220 --> 00:10:31,846
of this would be to write your own convenience library that wraps and

163
00:10:31,868 --> 00:10:35,202
abstracts certain operations. This is especially useful

164
00:10:35,266 --> 00:10:38,854
when you're working with a larger team and when you're using a lot of

165
00:10:38,892 --> 00:10:42,134
tools and sdks to perform your job. So let's say

166
00:10:42,172 --> 00:10:45,658
that you have a senior engineer and then you have a junior engineer. Then you

167
00:10:45,664 --> 00:10:49,382
can have your senior engineer work on this so that that senior engineer

168
00:10:49,446 --> 00:10:53,182
can, let's say, prepare a convenience library that works something like,

169
00:10:53,236 --> 00:10:56,442
let's say an orm. So something like SQL alchemy,

170
00:10:56,506 --> 00:11:00,382
where some Python classes and objects would help you perform your job

171
00:11:00,436 --> 00:11:04,014
better. And the junior developers or the mid level developers

172
00:11:04,142 --> 00:11:07,554
would not need to care about the internal details or

173
00:11:07,592 --> 00:11:11,534
the abstracted automation parts when working with your convenience

174
00:11:11,582 --> 00:11:14,786
library. So you can make use of design patterns and

175
00:11:14,808 --> 00:11:18,262
metaprogramming techniques to speed up the work and also

176
00:11:18,316 --> 00:11:21,794
abstract the unnecessary details from your other developers

177
00:11:21,842 --> 00:11:25,606
and engineers. Of course, perform this or do this when it

178
00:11:25,628 --> 00:11:29,654
makes sense. So if you're going to spend three weeks to work

179
00:11:29,692 --> 00:11:33,514
on this and your project is going to last for four weeks,

180
00:11:33,552 --> 00:11:36,586
these, that may not be the best use of your time. But if

181
00:11:36,608 --> 00:11:39,946
you have a super amazing engineer in your team who can work on

182
00:11:39,968 --> 00:11:43,238
something like this for two days, and then you can make the most out of

183
00:11:43,264 --> 00:11:46,746
those two days worth of work for the next three weeks, then that's

184
00:11:46,778 --> 00:11:50,874
a good use of time. The next one would be on utilizing continuous

185
00:11:50,922 --> 00:11:54,462
integration and deployment pipelines. Of course, at the start,

186
00:11:54,516 --> 00:11:58,146
you will be working on these things manually in the sense where,

187
00:11:58,248 --> 00:12:01,618
okay, I need to copy my model, put it inside

188
00:12:01,704 --> 00:12:05,614
a container or something, and then deploy it inside AWS

189
00:12:05,662 --> 00:12:09,270
lambda. So AWS Lambda is a service where you can write

190
00:12:09,340 --> 00:12:12,646
python code and then deploy it in that function as

191
00:12:12,668 --> 00:12:16,434
a service service. So there, the advantage

192
00:12:16,482 --> 00:12:19,750
there is that you only pay for what you use in AWS Lambda.

193
00:12:19,830 --> 00:12:23,318
So enough about AWS Lambda. Let's talk about this topic.

194
00:12:23,414 --> 00:12:27,146
So when you're building something, it usually takes three or

195
00:12:27,168 --> 00:12:30,394
four steps to come up with a deployment package. Of course

196
00:12:30,432 --> 00:12:33,806
you want that deployment package to be final and tested and

197
00:12:33,828 --> 00:12:37,482
working. And also when you're performing multiple deployments,

198
00:12:37,546 --> 00:12:40,814
let's say, per week, and there are a lot of users already using

199
00:12:40,852 --> 00:12:44,346
your system, we should find a way to make sure that the deployment

200
00:12:44,378 --> 00:12:47,746
package is 100% stable. Or if we detect that

201
00:12:47,768 --> 00:12:51,714
there's something wrong with that deployment package, then we should be able to roll back

202
00:12:51,752 --> 00:12:55,694
and revert to a previous deployment package. So knowing about continuous

203
00:12:55,742 --> 00:12:59,874
integration and deployment pipelines and all these other alternatives

204
00:12:59,922 --> 00:13:03,986
similar to that one would help your team work on these types of requirements

205
00:13:04,098 --> 00:13:07,654
better. So this is going to be super helpful, especially when your team

206
00:13:07,692 --> 00:13:11,354
is growing and when you want to enforce standards. So what happens here

207
00:13:11,392 --> 00:13:15,018
is that when one of your python engineers is working on some code,

208
00:13:15,104 --> 00:13:18,934
that person pushes some code to a repo, and then the integration

209
00:13:18,982 --> 00:13:22,174
pipeline activates it, performing some test,

210
00:13:22,292 --> 00:13:26,362
and then maybe at some point there's going to be an approval

211
00:13:26,426 --> 00:13:30,014
manual approval portion there where the engineering manager can just click

212
00:13:30,052 --> 00:13:34,406
on yes, after reviewing the results, and then perform the deployment.

213
00:13:34,538 --> 00:13:37,426
All right, so we're halfway through. We're almost done.

214
00:13:37,528 --> 00:13:41,694
Number six would be making the most out of ML frameworks and ML

215
00:13:41,742 --> 00:13:45,570
platforms. So these are actually three options, not just two.

216
00:13:45,640 --> 00:13:49,474
So the third option here is using, let's say, existing AI

217
00:13:49,522 --> 00:13:52,454
and ML services, where with five to ten lines of code,

218
00:13:52,492 --> 00:13:56,226
you may come up with text to speech, or maybe extracting

219
00:13:56,258 --> 00:13:59,558
text from images but for now, for the sake of complexity,

220
00:13:59,654 --> 00:14:03,530
let's talk about two things. The first one would be building

221
00:14:03,600 --> 00:14:07,066
everything from scratch, and then the second one would be

222
00:14:07,168 --> 00:14:10,506
using frameworks and platforms at

223
00:14:10,528 --> 00:14:13,822
the start, as developers and engineers, we always

224
00:14:13,956 --> 00:14:17,194
have that tendency to build everything from scratch.

225
00:14:17,322 --> 00:14:20,682
So when you are about CTO, learn an existing framework,

226
00:14:20,826 --> 00:14:24,766
there's always a tendency to say, oh, that's going

227
00:14:24,788 --> 00:14:29,326
to take me one week to two weeks to learn that framework. Let's say tensorflow

228
00:14:29,358 --> 00:14:33,154
or Pytorch, or Mxnet. And that's probably true. Sometimes the

229
00:14:33,192 --> 00:14:36,978
examples in the Internet may not work right away, or sometimes you just

230
00:14:36,984 --> 00:14:40,966
have the tendency to enjoy coding. When you're trying to learn about

231
00:14:41,068 --> 00:14:44,614
programming and machine learning and machine learning engineering, you can

232
00:14:44,652 --> 00:14:48,338
try learning these things yourself. But when you have to work with a team,

233
00:14:48,444 --> 00:14:52,202
and when you have to work in a company where the real

234
00:14:52,256 --> 00:14:56,246
things happen, let's say people resigning or people being replaced,

235
00:14:56,358 --> 00:14:59,910
and you trying to work on existing platforms

236
00:14:59,990 --> 00:15:03,566
and engineering systems, then you have to know that

237
00:15:03,588 --> 00:15:07,530
it's more practical in the long run to use machine planning frameworks

238
00:15:07,610 --> 00:15:10,862
and ML platforms. Of course, it may not always

239
00:15:10,916 --> 00:15:14,350
be the case, but being able to do both is the first step.

240
00:15:14,420 --> 00:15:18,226
And then the second step there is knowing when to use what. Because if

241
00:15:18,248 --> 00:15:22,290
you're going to build everything from scratch, these, of course, you have to make sure

242
00:15:22,360 --> 00:15:26,126
that all the requirements and the potential hidden features

243
00:15:26,238 --> 00:15:29,926
may not be supported in your custom code, and it might take you longer to

244
00:15:29,948 --> 00:15:33,494
build that. So being familiar with one or two or three or

245
00:15:33,532 --> 00:15:37,270
more ML frameworks and tools and platforms would definitely

246
00:15:37,340 --> 00:15:41,174
not just help you, but help your company accomplish your goals in a much faster

247
00:15:41,222 --> 00:15:45,110
way. If you were to use an ML platform, let's say, with Sagemaker,

248
00:15:45,190 --> 00:15:48,534
then you can also make use of its existing capabilities

249
00:15:48,582 --> 00:15:52,446
and features. Because for one thing, when you're running machine learning workflows and

250
00:15:52,468 --> 00:15:56,174
workloads in the cloud, you will realize that some

251
00:15:56,212 --> 00:15:59,946
of those experiments will require bigger machines,

252
00:16:00,138 --> 00:16:03,566
and sometimes not just one, but two or three or more. So if you were

253
00:16:03,588 --> 00:16:07,122
to build this yourself, it might take you two to three months to build something

254
00:16:07,176 --> 00:16:10,510
that's super flexible and something that can easily evolve

255
00:16:10,590 --> 00:16:14,114
to more complex use cases. But if you were

256
00:16:14,152 --> 00:16:17,682
to use an ML platform, learning it, let's say, would be

257
00:16:17,736 --> 00:16:21,282
two to three days, and then using it would take an additional

258
00:16:21,346 --> 00:16:24,614
day. So that would be three days. All in all, instead of trying to build

259
00:16:24,652 --> 00:16:27,846
everything from scratch, where you will build it for like

260
00:16:27,868 --> 00:16:31,142
two to three months, only to realize, oh, there's no debugger,

261
00:16:31,206 --> 00:16:34,774
there's no model monitor and all the other high tech features

262
00:16:34,822 --> 00:16:38,730
that the platform or the framework has already provided. So here

263
00:16:38,800 --> 00:16:42,614
let's say that we want to modify the number of computers

264
00:16:42,662 --> 00:16:46,414
or servers or instances that we will use for training the

265
00:16:46,452 --> 00:16:50,238
model and performing hyperparameter tuning jobs. Then here,

266
00:16:50,324 --> 00:16:52,926
if you look at the screen, you can see that, oh, you just set the

267
00:16:52,948 --> 00:16:56,234
parameter to six, and then you can just have six ML

268
00:16:56,282 --> 00:16:59,902
instances there. And then if you want just one instance for model deployment

269
00:16:59,966 --> 00:17:03,538
for that inference endpoint, then you can specify that as well with just a

270
00:17:03,544 --> 00:17:07,522
few lines of code. The advantage these is that the infrastructure is abstracted

271
00:17:07,586 --> 00:17:11,446
and you can just use Python and the objects and classes in

272
00:17:11,468 --> 00:17:14,486
the SDK provided CTO access and manage the

273
00:17:14,508 --> 00:17:18,514
resources. Here, in addition to using ML platforms and frameworks,

274
00:17:18,642 --> 00:17:22,474
you will only also find a lot of documentation online when using these

275
00:17:22,512 --> 00:17:25,930
tools and frameworks and trying to get this to work in different

276
00:17:26,000 --> 00:17:29,706
types of environments. So if you were to build things using your

277
00:17:29,728 --> 00:17:33,626
own custom code, sometimes these disadvantage there is that the errors

278
00:17:33,658 --> 00:17:36,890
are also custom. So when you try to look for the solution

279
00:17:36,970 --> 00:17:40,462
in, let's say, stack overflow, you may not find that right away

280
00:17:40,516 --> 00:17:43,726
unless you are very experienced. Here, you can also make the

281
00:17:43,748 --> 00:17:46,766
most out of, let's say, AWS lambda serverless.

282
00:17:46,878 --> 00:17:50,610
So if you are just calling an endpoint four times

283
00:17:50,680 --> 00:17:54,226
per day, then why have an instance for it? So with this

284
00:17:54,248 --> 00:17:57,766
one, you can technically get these almost for free. Because if

285
00:17:57,788 --> 00:18:01,446
you're just going to use AWS lambda for 4 seconds for

286
00:18:01,468 --> 00:18:05,538
a day, and it's under the free tier, then it's much, much cheaper

287
00:18:05,634 --> 00:18:09,426
than having an ML instance running there with your

288
00:18:09,548 --> 00:18:13,094
endpoint, right? With your deployed model. So you can use AWS

289
00:18:13,142 --> 00:18:16,678
lambda, let's say with scikit learn, you can use it with Tensorflow,

290
00:18:16,774 --> 00:18:19,898
and you can maybe deploy Facebook profit inside a

291
00:18:19,904 --> 00:18:23,598
lambda function, Facebook profit model inside a lambda function.

292
00:18:23,684 --> 00:18:27,310
And you can combine that with, let's say API gateway, which is a service

293
00:18:27,380 --> 00:18:31,098
that allows users to deploy an endpoint

294
00:18:31,274 --> 00:18:35,018
and then having that endpoint trigger

295
00:18:35,114 --> 00:18:38,706
the AWS lambda function that you have prepared. There are also a

296
00:18:38,728 --> 00:18:42,658
lot of deployment solutions out there. Of course it's super important if

297
00:18:42,664 --> 00:18:46,782
you know how to build this from scratch, but there are ways to speed

298
00:18:46,846 --> 00:18:50,902
up solving these types of problems in just a couple of hours.

299
00:18:51,036 --> 00:18:54,646
So let's say that if you were to build something for four weeks, maybe you

300
00:18:54,668 --> 00:18:58,214
can do the same thing in two to three weeks, especially if your team is

301
00:18:58,252 --> 00:19:01,870
already using that platform or these tools.

302
00:19:01,970 --> 00:19:05,734
So the first one would be deploying a model in an easy to instance.

303
00:19:05,782 --> 00:19:09,386
So that's one of the most customizable options out there. So if you

304
00:19:09,408 --> 00:19:12,810
want to build everything from scratch, then yes, you can deploy that inside

305
00:19:12,880 --> 00:19:16,846
an easy to instance or alternative using a different platform. The second one would

306
00:19:16,868 --> 00:19:21,194
be deploying the model in a container in an easy to instance.

307
00:19:21,322 --> 00:19:24,814
The third one would be using a built in algorithm for

308
00:19:24,852 --> 00:19:28,834
training and then deploying that in a sagemaker endpoint with, let's say ten

309
00:19:28,872 --> 00:19:32,114
lines of code. So this is very helpful when you're trying to

310
00:19:32,152 --> 00:19:36,194
provide proof of concept work to your boss before approving

311
00:19:36,322 --> 00:19:40,150
your machine learning project. The fourth one would be using custom

312
00:19:40,220 --> 00:19:44,134
containers. So building your own docker container images and

313
00:19:44,172 --> 00:19:48,066
then using that and deploying that in a sagemaker

314
00:19:48,098 --> 00:19:51,482
endpoint with just a couple of lines of code. The advantage there

315
00:19:51,536 --> 00:19:55,286
is that you can make the most out of an existing platform's

316
00:19:55,318 --> 00:19:59,210
features, let's say model monitor, to help you detect model

317
00:19:59,280 --> 00:20:03,546
drift. So what is model drift? Most of the machine learning practitioners

318
00:20:03,658 --> 00:20:07,086
are only aware of how to train, build and

319
00:20:07,108 --> 00:20:10,986
deploy a model, but in reality that model deployed

320
00:20:11,018 --> 00:20:14,362
in production may degrade over a couple of weeks

321
00:20:14,436 --> 00:20:17,790
or a couple of months. So being able to detect model drift

322
00:20:17,870 --> 00:20:21,150
and being able to replace that model is essential.

323
00:20:21,230 --> 00:20:25,286
And knowing that models really degrade over time is an

324
00:20:25,308 --> 00:20:30,050
essential item for veteran machine learning practitioners.

325
00:20:30,210 --> 00:20:33,874
A model can also be deployed inside a lambda function as shared

326
00:20:33,922 --> 00:20:38,178
earlier, and then we can also use lambda

327
00:20:38,274 --> 00:20:41,562
to trigger a sagemaker endpoint. So if you have deployed a model

328
00:20:41,616 --> 00:20:45,414
in a sagemaker endpoint, then you can use AWS lambda

329
00:20:45,542 --> 00:20:48,694
to perform some custom things first before triggering

330
00:20:48,742 --> 00:20:51,498
the sagemaker endpoint, giving you that flexibility,

331
00:20:51,674 --> 00:20:55,994
especially if you need to pre process your data first before performing

332
00:20:56,042 --> 00:21:00,426
the prediction. Also, you can use API gateway mapping templates

333
00:21:00,538 --> 00:21:03,986
with sagemaker so that there's going to be

334
00:21:04,088 --> 00:21:07,074
no lambda function between those two services.

335
00:21:07,192 --> 00:21:10,942
Here you just use VTL to map

336
00:21:11,006 --> 00:21:14,430
the input data to the sagemaker endpoint directly,

337
00:21:14,590 --> 00:21:17,474
and then you can also deploy the model inside Fargate.

338
00:21:17,522 --> 00:21:20,966
So Fargate is a service which can help you work on

339
00:21:21,068 --> 00:21:24,662
containers and container images in AWS. So feel

340
00:21:24,716 --> 00:21:27,894
free to use these concepts when you're using other

341
00:21:27,932 --> 00:21:31,450
platforms as well, because most likely they may have similar

342
00:21:31,600 --> 00:21:34,954
services out there, especially if your team is already using those

343
00:21:34,992 --> 00:21:38,474
platforms when you're using, let's say, Sagemaker. I would like to

344
00:21:38,512 --> 00:21:41,774
add that these are also combinations where for more

345
00:21:41,812 --> 00:21:45,882
complex use cases, you can technically deploy multiple

346
00:21:45,946 --> 00:21:49,886
models inside a single machine learning instance. So there

347
00:21:49,908 --> 00:21:54,014
are different use cases there. And you can also deploy sagemaker

348
00:21:54,142 --> 00:21:58,274
multicontainer endpoints where that endpoint can

349
00:21:58,312 --> 00:22:01,998
have multiple containers with different models. So let's

350
00:22:02,014 --> 00:22:05,922
say that you're using a custom model using a specific deep learning

351
00:22:05,976 --> 00:22:10,054
framework. Then you can have that model inside these container. And then

352
00:22:10,172 --> 00:22:13,190
let's say you have four containers these,

353
00:22:13,260 --> 00:22:16,694
then you can deploy that inside a single endpoint. So you can just

354
00:22:16,732 --> 00:22:20,326
select which container to use when you're performing

355
00:22:20,358 --> 00:22:24,262
inference. So very useful when you're trying to compare different models

356
00:22:24,326 --> 00:22:27,894
in a production environment. The next one would be setting

357
00:22:27,942 --> 00:22:31,382
up a b testing using production variants in sales maker.

358
00:22:31,446 --> 00:22:35,658
So let's say you are running a model deployed in an endpoint.

359
00:22:35,834 --> 00:22:40,142
You can do let's say 80 20 where 80% of the traffic is

360
00:22:40,276 --> 00:22:43,854
being handled by one model and then a new

361
00:22:43,892 --> 00:22:47,554
model is going to handle 20% of the traffic. And then you're going

362
00:22:47,592 --> 00:22:51,490
to compare the performance of those two models before trying

363
00:22:51,560 --> 00:22:54,866
to replace the first model with the second one. So of course

364
00:22:54,968 --> 00:22:58,454
if the second model is performing better than the first one, then that's the time

365
00:22:58,492 --> 00:23:01,714
to replace the first model. And then here you can also deploy

366
00:23:01,762 --> 00:23:05,250
the model inside a lambda function with containers.

367
00:23:05,330 --> 00:23:08,054
So recently I think about five months ago,

368
00:23:08,172 --> 00:23:11,938
AWS has released this feature where in addition to writing

369
00:23:12,034 --> 00:23:15,926
Python code inside a lambda function, you can use your own custom container

370
00:23:15,958 --> 00:23:19,494
images to load your model. So this is very helpful.

371
00:23:19,542 --> 00:23:22,782
Also when you're using deep learning frameworks and

372
00:23:22,836 --> 00:23:26,302
trying to get that to work with AWS Lambda. So here

373
00:23:26,356 --> 00:23:29,854
you can make the most out of both worlds where you can use

374
00:23:29,892 --> 00:23:33,450
your customization capabilities, especially your DevOps skills,

375
00:23:33,530 --> 00:23:36,814
to prepare that custom container image, loading the model

376
00:23:36,932 --> 00:23:40,594
and then it's going to work with AWS Lambda where you just

377
00:23:40,632 --> 00:23:43,954
pay for what you use. So if you're just using it for 3 seconds per

378
00:23:43,992 --> 00:23:47,158
day, then you're only going to pay for 3 seconds per day, which is

379
00:23:47,164 --> 00:23:51,394
super cool. You can also use, let's say the data science

380
00:23:51,522 --> 00:23:54,614
libraries with Sagemaker. So if you were to use

381
00:23:54,652 --> 00:23:58,630
that these you can easily build machine learning workflows using

382
00:23:58,700 --> 00:24:02,218
AWSF functions. So here you can see that

383
00:24:02,304 --> 00:24:05,514
you can automate the entire process. And if you want to

384
00:24:05,552 --> 00:24:08,954
perform model retraining, you can do something like this.

385
00:24:09,072 --> 00:24:12,974
So let's say that you have uploaded your files or your new

386
00:24:13,012 --> 00:24:16,734
data in a bucket or in a storage service, let's say s

387
00:24:16,772 --> 00:24:20,254
three, then this automation workflow would help

388
00:24:20,292 --> 00:24:24,118
you automatically trigger the training step and then evaluation

389
00:24:24,234 --> 00:24:27,554
and then deployment. And then if, let's say that

390
00:24:27,592 --> 00:24:30,770
code is performing better than your previous model,

391
00:24:30,840 --> 00:24:34,514
then you can automatically replace your existing model in production so,

392
00:24:34,552 --> 00:24:38,198
pretty cool, because you're not just stuck with manual steps, but you

393
00:24:38,204 --> 00:24:40,040
can just leave these running,

394
00:24:41,930 --> 00:24:45,270
especially if you want to work on other machine learning projects.

395
00:24:45,690 --> 00:24:49,478
You can also make the most out of, let's say, sagemaker clarify

396
00:24:49,574 --> 00:24:53,354
to automate bias detection. So here we can

397
00:24:53,392 --> 00:24:57,338
see that you have your data and maybe your model,

398
00:24:57,504 --> 00:25:01,526
and then you pass those as parameters. Cto your sagemaker

399
00:25:01,558 --> 00:25:04,602
clarify jobs. So, of course,

400
00:25:04,736 --> 00:25:08,206
what's bias? So, ML Bias is something that you

401
00:25:08,228 --> 00:25:11,774
would probably be aware of if you've been working in the industry for quite some

402
00:25:11,812 --> 00:25:15,598
time. And when you're deploying a model and using that in production,

403
00:25:15,694 --> 00:25:19,570
it's not just about performing these right prediction, it's also about

404
00:25:19,720 --> 00:25:23,074
making sure that you're following the guidelines and that making sure

405
00:25:23,112 --> 00:25:26,770
that your model is not bias towards certain groups.

406
00:25:27,350 --> 00:25:31,026
So, we will not talk about this in detail here, but it's important to note

407
00:25:31,058 --> 00:25:34,498
that there are a lot of metrics that you can check when checking

408
00:25:34,594 --> 00:25:38,030
and working with bias. Let's say class imbalance,

409
00:25:38,130 --> 00:25:41,866
or maybe DPPL, or maybe treatment and quality. And here

410
00:25:41,968 --> 00:25:45,514
you can fix your data after you have detected that your

411
00:25:45,552 --> 00:25:49,754
data has issues after reviewing these metrics here.

412
00:25:49,952 --> 00:25:54,074
So here's some sample python code when using Sagemaker clarify.

413
00:25:54,202 --> 00:25:57,966
So here you just specify, let's say, the instance count, the instance type,

414
00:25:58,068 --> 00:26:01,342
and then you pass in your data, and then maybe a few

415
00:26:01,396 --> 00:26:05,714
configuration parameters. So instead of you trying to learn how

416
00:26:05,752 --> 00:26:09,730
to detect bias and planning all these formulas yourself,

417
00:26:09,880 --> 00:26:13,262
why not just use a tools which can provide you the metrics

418
00:26:13,326 --> 00:26:16,254
right away, as you can see here. So if you were to detect,

419
00:26:16,302 --> 00:26:19,606
let's say, class imbalance with, let's say, ten to 15 lines of

420
00:26:19,628 --> 00:26:22,982
code waiting for ten minutes, then you would have something like this,

421
00:26:23,036 --> 00:26:26,774
where you can detect if there's class imbalance in

422
00:26:26,812 --> 00:26:30,310
your data set the same way with ML explainability.

423
00:26:30,470 --> 00:26:34,742
So what is ML explainability? So, if you are working with more complex

424
00:26:34,886 --> 00:26:38,954
algorithms and models, of course the output, these may

425
00:26:38,992 --> 00:26:41,718
not necessarily be easily explainable.

426
00:26:41,814 --> 00:26:45,134
And the more we are able to explain a model better,

427
00:26:45,252 --> 00:26:48,942
the more we are able to get an organization to use that model

428
00:26:48,996 --> 00:26:52,654
or algorithm. So, in ML explainability, let's say

429
00:26:52,692 --> 00:26:56,706
for this one, you can use shaped sales to explain your model.

430
00:26:56,808 --> 00:27:00,226
So here, how do we interpret this? We can say that out of

431
00:27:00,248 --> 00:27:04,082
the four features that we have in our training data

432
00:27:04,136 --> 00:27:07,778
set, we can say that only two features actually contribute

433
00:27:07,874 --> 00:27:11,254
to the final outcome of the prediction of the model.

434
00:27:11,372 --> 00:27:14,726
So, let's say that there's ABC and D. Only A and B

435
00:27:14,828 --> 00:27:18,162
actually contribute to the final outcome when performing

436
00:27:18,226 --> 00:27:21,546
the prediction. So this is an example of what we

437
00:27:21,568 --> 00:27:25,130
will get if we were to use Sagemaker, clarify to use

438
00:27:25,200 --> 00:27:28,586
and compute the Shaq values after you

439
00:27:28,608 --> 00:27:32,746
have passed your data. So the next one is a really exciting topic.

440
00:27:32,778 --> 00:27:36,106
So this is called automated hyperparameter

441
00:27:36,138 --> 00:27:39,886
optimization. So what is automated hyperparameter optimization? So the

442
00:27:39,908 --> 00:27:43,154
first one is understanding what hyperparameters are.

443
00:27:43,192 --> 00:27:46,674
Hyperparameters are configuration parameters that you can

444
00:27:46,712 --> 00:27:50,754
set before the training job. So one training experiment, one set of

445
00:27:50,792 --> 00:27:54,766
hyperparameters. Of course, when you're trying to create models,

446
00:27:54,878 --> 00:27:58,614
it's critical that we all know that after one

447
00:27:58,652 --> 00:28:02,438
experiment, we are not really sure if that model

448
00:28:02,524 --> 00:28:06,354
is these best model for that problem. So the technique

449
00:28:06,402 --> 00:28:09,766
there is to configure the hyperparameter values,

450
00:28:09,878 --> 00:28:14,070
perform the experiments again, and compare the evaluation metrics

451
00:28:14,150 --> 00:28:17,386
with the evaluation metrics of a previous model. So of

452
00:28:17,408 --> 00:28:20,826
course this would be very time consuming. So how do we solve

453
00:28:20,858 --> 00:28:24,314
this in a more practical manner? So, with cloud computing,

454
00:28:24,442 --> 00:28:28,586
you can easily spin up a lot of resources,

455
00:28:28,698 --> 00:28:32,574
let's say for three minutes each, and then perform one

456
00:28:32,612 --> 00:28:36,126
training experiment for each, let's say ML instance.

457
00:28:36,238 --> 00:28:40,162
So after 15 minutes, you would be able to come up with,

458
00:28:40,216 --> 00:28:44,318
let's say 15 different experiments and 15 different models,

459
00:28:44,414 --> 00:28:48,102
and come up with a fine tuned model

460
00:28:48,236 --> 00:28:51,906
where that model has the best metric values compared

461
00:28:51,938 --> 00:28:55,718
to the other models produced by the tuning job. In a similar

462
00:28:55,804 --> 00:28:59,738
fashion, you can perform automated hyperparameter tuning across

463
00:28:59,824 --> 00:29:02,954
different model families. So let's say that you have

464
00:29:03,072 --> 00:29:06,682
here in the screen, you have a custom algorithm using

465
00:29:06,736 --> 00:29:10,422
Apache Mxnet deep learning framework, and the second model family

466
00:29:10,496 --> 00:29:13,466
would be using the linear learner built in algorithm.

467
00:29:13,578 --> 00:29:16,926
Then you can perform a single hyperparameter tuning job

468
00:29:17,028 --> 00:29:20,894
where this model family would use a certain set of

469
00:29:20,932 --> 00:29:24,546
configuration hyperparameter ranges, and then the second family would use

470
00:29:24,568 --> 00:29:28,082
a different set of configuration parameters. Those training

471
00:29:28,136 --> 00:29:31,906
jobs would run, and then the best model would be used in

472
00:29:31,928 --> 00:29:35,614
the final model deployment step here in optimizing

473
00:29:35,662 --> 00:29:39,138
cost by using transient ML instances for training models,

474
00:29:39,234 --> 00:29:43,126
we can make the most out of transient ML instances where the

475
00:29:43,148 --> 00:29:46,898
ML instances would run for, let's say ten minutes, and then it's

476
00:29:46,914 --> 00:29:50,634
going to turn off automatically. So this is very helpful when you're trying

477
00:29:50,672 --> 00:29:54,342
to train or fine tune existing models

478
00:29:54,406 --> 00:29:57,626
where you would need a lot of resources. The example of this

479
00:29:57,648 --> 00:30:00,886
one would, let's say, be using BErT models.

480
00:30:00,918 --> 00:30:04,270
Let's say you have hugging face and then you have BErt, and you would need,

481
00:30:04,340 --> 00:30:07,902
let's say p two x large instances, which are super

482
00:30:07,956 --> 00:30:12,014
expensive, right? But if you were to run that in just two to three minutes,

483
00:30:12,132 --> 00:30:15,938
then that's better compared to running that

484
00:30:16,024 --> 00:30:19,794
same large instance for 3 hours. So having

485
00:30:19,912 --> 00:30:24,210
transient ML instances to run your training jobs is super important

486
00:30:24,280 --> 00:30:28,482
when managing cost. Finally, when securing machine learning environments,

487
00:30:28,626 --> 00:30:32,630
it's critical that you take care of both process and

488
00:30:32,700 --> 00:30:36,326
tech side of things. So knowing about principle of

489
00:30:36,348 --> 00:30:39,870
least privilege is important because of course, when you're preparing

490
00:30:39,890 --> 00:30:43,354
your environments, you have to prepare and manage the security

491
00:30:43,472 --> 00:30:47,286
configuration first and make sure that from the beginning this is properly

492
00:30:47,318 --> 00:30:50,470
set up so that you can leave your engineers

493
00:30:50,550 --> 00:30:53,790
working without you having to worry about security every day.

494
00:30:53,860 --> 00:30:57,066
So set the rules, set the guidelines, set the restrictions

495
00:30:57,178 --> 00:31:00,814
so that these can only perform what they should be doing and does

496
00:31:00,852 --> 00:31:03,642
not apply only to humans.

497
00:31:03,786 --> 00:31:07,298
This can also be used when dealing with resources in

498
00:31:07,304 --> 00:31:11,342
the cloud. So here this is an example of a potential risk

499
00:31:11,406 --> 00:31:14,702
when using a library. So here, this library

500
00:31:14,766 --> 00:31:17,666
allows you cto load and save models.

501
00:31:17,778 --> 00:31:21,330
But if you were to use a model from an untrusted source,

502
00:31:21,410 --> 00:31:24,454
and that model may run, let's say,

503
00:31:24,492 --> 00:31:28,490
arbitrary malicious code, when you load the model these

504
00:31:28,560 --> 00:31:31,818
technically your system has been compromised. So what can you

505
00:31:31,824 --> 00:31:35,786
do here? These you can solve this problem by limiting the

506
00:31:35,808 --> 00:31:39,858
permissions for that set of resources loading

507
00:31:39,974 --> 00:31:44,250
this model. So let's say that you have a container using Python

508
00:31:44,410 --> 00:31:47,722
loading this model from an unauthorized source,

509
00:31:47,866 --> 00:31:51,662
then you can limit that resource to only perform

510
00:31:51,716 --> 00:31:55,426
certain actions. So if, let's say, case one, you have super

511
00:31:55,528 --> 00:31:59,058
admin permissions for that resource, and that

512
00:31:59,144 --> 00:32:03,170
model has been loaded in that resource, then the problem there is that

513
00:32:03,240 --> 00:32:06,902
malicious code can perform super admin actions. On case

514
00:32:06,956 --> 00:32:10,854
two, if that resource loading that model has

515
00:32:10,892 --> 00:32:15,126
limited permissions, then the advantage these is then

516
00:32:15,148 --> 00:32:18,598
the malicious code can only perform a limited set of actions as well. So at

517
00:32:18,604 --> 00:32:21,800
least you can limit the damage when an accident happens.

518
00:32:22,410 --> 00:32:26,214
So that's pretty much it. So thank you again for

519
00:32:26,332 --> 00:32:29,474
listening to my talk. So again, you have learned

520
00:32:29,522 --> 00:32:33,950
a lot in this short session, so make sure to use that knowledge

521
00:32:34,290 --> 00:32:37,438
in your day to day machine planning

522
00:32:37,604 --> 00:32:40,910
life. So thank you again and feel free to reach out to me

523
00:32:40,980 --> 00:32:44,542
via email or LinkedIn. So thank you again and

524
00:32:44,596 --> 00:32:46,460
hope you learned something from my talk.

