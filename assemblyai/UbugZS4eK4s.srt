1
00:00:00,410 --> 00:00:06,126
Jamaica, taken up real

2
00:00:06,148 --> 00:00:09,530
time feedback into the behavior of your distributed systems

3
00:00:09,610 --> 00:00:13,374
and observing changes exceptions. Errors in

4
00:00:13,412 --> 00:00:17,914
real time allows you to not only experiment with confidence, but respond

5
00:00:18,042 --> 00:00:20,480
instantly to get things working again.

6
00:00:24,610 --> 00:01:04,686
Close my

7
00:01:04,708 --> 00:01:08,414
name is Paul Marsicovetere and today I'm going to talk about optimizing

8
00:01:08,462 --> 00:01:12,082
incident response thanks to chaos engineering a little bit about

9
00:01:12,136 --> 00:01:15,982
myself I'm a senior cloud infrastructure engineer at Formidable

10
00:01:16,046 --> 00:01:19,366
in Toronto and I've been here since October 2020.

11
00:01:19,468 --> 00:01:22,994
Formidable partners with many different companies to help build the modern

12
00:01:23,042 --> 00:01:26,520
web and design solutions to complex technical problems.

13
00:01:27,210 --> 00:01:31,222
Previously I was working in SRE for Benevity in Calgary

14
00:01:31,286 --> 00:01:34,166
for three years, and while I'm originally from Melbourne,

15
00:01:34,198 --> 00:01:37,306
Australia, I've been happily living in Canada for

16
00:01:37,328 --> 00:01:40,814
over ten years now. You can get in touch with me on Twitter at

17
00:01:40,852 --> 00:01:44,334
paulmastecloud, on LinkedIn, and via email.

18
00:01:44,532 --> 00:01:48,794
I'm always open to chat with anyone about anything cloud computing,

19
00:01:48,842 --> 00:01:52,154
SRE or DevOps related. I run a serverless

20
00:01:52,202 --> 00:01:55,774
blog called the Cloud on my mind in my spare time as well.

21
00:01:55,892 --> 00:01:59,106
So as an agenda today I'm going to talk about why it

22
00:01:59,128 --> 00:02:02,382
is that we would choose Chaos engineering and what is chaos

23
00:02:02,446 --> 00:02:05,886
engineering. I'll then move on to how chaos engineering can help

24
00:02:05,928 --> 00:02:09,490
in practice, and we'll be wrapping up with some lessons learned.

25
00:02:09,650 --> 00:02:13,474
So what exactly is chaos engineering? The best definition

26
00:02:13,522 --> 00:02:17,090
of chaos engineering I've come across is from Colton Andrus,

27
00:02:17,170 --> 00:02:21,254
co founder and CTO of Gremlin, who defines chaos engineering

28
00:02:21,302 --> 00:02:24,806
as thoughtful, planned experiments designed to reveal

29
00:02:24,838 --> 00:02:28,534
the weakness in our systems. The main attraction of chaos

30
00:02:28,582 --> 00:02:32,206
engineering for me is the idea to simply break the system and

31
00:02:32,228 --> 00:02:35,642
see what happens, which is a far cry from the traditional

32
00:02:35,786 --> 00:02:39,502
keep everything running at all times and at all costs type thinking

33
00:02:39,556 --> 00:02:43,342
and mentality. With that said, why would you want to use

34
00:02:43,396 --> 00:02:46,994
chaos engineering? Chaos Engineering is a discipline that

35
00:02:47,032 --> 00:02:50,146
will continue to grow organically as we depend more and

36
00:02:50,168 --> 00:02:53,614
more on cloud providers in the industry. Because of the nature

37
00:02:53,662 --> 00:02:57,538
of cloud computing, we need more assurances of availability

38
00:02:57,714 --> 00:03:01,030
as new and unexpected outages continue to occur.

39
00:03:01,370 --> 00:03:04,934
Unit and integration testing can only really take us so

40
00:03:04,972 --> 00:03:08,566
far, as these methods typically verify how functions

41
00:03:08,598 --> 00:03:12,010
or application code is supposed to respond. However,

42
00:03:12,080 --> 00:03:16,086
chaos engineering can show you how an application actually responds

43
00:03:16,198 --> 00:03:19,926
to a wide range of failures. These can be anything

44
00:03:20,048 --> 00:03:23,822
from removing access to a network file share to

45
00:03:23,876 --> 00:03:27,566
deletion of database tables to monitor how the services

46
00:03:27,668 --> 00:03:31,306
respond and the range of failures are performing endless.

47
00:03:31,498 --> 00:03:35,214
I also find there is a freedom provided by using these chaos

48
00:03:35,262 --> 00:03:39,314
engineering experiments with zero expectations of how an application

49
00:03:39,432 --> 00:03:43,166
or service will respond when injecting the particular failures.

50
00:03:43,358 --> 00:03:47,334
It is liberating as often we can expect applications and services

51
00:03:47,452 --> 00:03:50,550
to return results in some particular fashion during

52
00:03:50,620 --> 00:03:53,798
unit or integration testing. But now we can actually

53
00:03:53,884 --> 00:03:57,238
say let's try and break the application to see what happens

54
00:03:57,324 --> 00:04:01,206
and let the creativity flow from there. So everything explained

55
00:04:01,238 --> 00:04:05,174
is all well and good in theory, but how do chaos engineering experiments

56
00:04:05,222 --> 00:04:09,114
help in the real world? Well, this is where I'll describe how a

57
00:04:09,152 --> 00:04:13,130
small chaos engineering experiments actually led to key outcomes

58
00:04:13,210 --> 00:04:16,122
that were later utilized during a production incident.

59
00:04:16,266 --> 00:04:20,010
I'll discuss about the chaos engineering experiment setup,

60
00:04:20,170 --> 00:04:22,902
what was performed when we ran the experiments,

61
00:04:23,066 --> 00:04:26,574
and finally what happened after some real chaos occurred

62
00:04:26,622 --> 00:04:30,146
in our production environment. The experiments was set up as

63
00:04:30,168 --> 00:04:34,366
follows. Take a Kubernetes cluster in a nonproduction environment

64
00:04:34,478 --> 00:04:37,862
and inject failures to the running pods and record how the

65
00:04:37,916 --> 00:04:42,034
service responds. That's it. Nothing groundbreaking,

66
00:04:42,162 --> 00:04:45,734
but something that not a lot of teams would focus on when it comes to

67
00:04:45,772 --> 00:04:49,706
creating services. We chose production as this was

68
00:04:49,728 --> 00:04:52,954
our first chaos engineering experiment and while we had

69
00:04:52,992 --> 00:04:56,426
the confidence in these production system, there was no need

70
00:04:56,448 --> 00:05:00,394
to cause unintentional outages for our end users. As per

71
00:05:00,432 --> 00:05:03,822
the diagram, the particular service experimented on

72
00:05:03,876 --> 00:05:07,582
RaN pods in a parent child architecture where the parent

73
00:05:07,636 --> 00:05:11,706
was an orchestrator that can on a Kubernetes node that would spin

74
00:05:11,738 --> 00:05:15,586
up child pods when requested. The child pod logs were

75
00:05:15,608 --> 00:05:18,978
also streamed in real time via a web page outside

76
00:05:19,064 --> 00:05:22,146
of kubernetes where the clients would view logs of their

77
00:05:22,168 --> 00:05:25,506
job requests. The experiment itself was designed

78
00:05:25,538 --> 00:05:29,606
to inject termination to child pods, the parent pod and

79
00:05:29,628 --> 00:05:33,410
the underlying kubernetes node during simulated scheduled tasks

80
00:05:33,490 --> 00:05:37,078
while the job requests ran. The failures and errors that

81
00:05:37,084 --> 00:05:40,966
were returned during each of these tests was recorded in a document

82
00:05:41,078 --> 00:05:44,666
and the child pod logs web page was observed so that

83
00:05:44,688 --> 00:05:48,234
we could also understand the client experience as well during these

84
00:05:48,272 --> 00:05:51,206
failures at the time of the experiment,

85
00:05:51,318 --> 00:05:55,114
the most interesting finding was actually the drift between the logs

86
00:05:55,162 --> 00:05:59,038
web page and the Kubernetes pod logs on the cluster itself,

87
00:05:59,204 --> 00:06:02,666
along with some small bug findings and expected failure modes

88
00:06:02,698 --> 00:06:06,382
that incurred in certain conditions. All experiment

89
00:06:06,446 --> 00:06:09,794
events and outcomes were recorded in a document and were then

90
00:06:09,832 --> 00:06:13,938
discussed later at a team meeting. The service resilience was now understood a

91
00:06:13,944 --> 00:06:17,210
bit further, whereas compared to before the experiments,

92
00:06:17,390 --> 00:06:20,726
certain failure modes like when the child or the parent of the

93
00:06:20,748 --> 00:06:24,534
child pods went offline, they weren't was well known. So some

94
00:06:24,572 --> 00:06:27,810
weeks later a production issue actually arose

95
00:06:27,890 --> 00:06:32,122
when the parent pod was in an error state and many child pods were running

96
00:06:32,176 --> 00:06:35,290
that could not be deleted safely without potentially taking

97
00:06:35,360 --> 00:06:38,826
further downstream services offline while looking for a

98
00:06:38,848 --> 00:06:42,926
solution mid incident to safely delete the parent pod without taking those

99
00:06:42,948 --> 00:06:46,474
child pods offline, the chaos engineering experiments document

100
00:06:46,522 --> 00:06:49,854
was reviewed. Thankfully, it turns out we had

101
00:06:49,892 --> 00:06:53,346
documented a safe command to delete the parent pod that

102
00:06:53,368 --> 00:06:57,154
would not affect the running child pods. We had recorded this

103
00:06:57,192 --> 00:07:01,134
command during the chaos engineering experiment to show how failures

104
00:07:01,182 --> 00:07:04,702
were injected and their outcomes. Interestingly,

105
00:07:04,846 --> 00:07:08,626
there was also a very unsafe command documented to delete

106
00:07:08,658 --> 00:07:12,546
the parent pod that would have had negative effects for the child pods

107
00:07:12,578 --> 00:07:15,974
and downstream services. I'm sure you can all guess which

108
00:07:16,012 --> 00:07:19,530
command was chosen to resolve this issue. So, as a result

109
00:07:19,600 --> 00:07:23,254
of the chaos engineering experiment and then the production outage

110
00:07:23,302 --> 00:07:26,410
a few weeks later, what kind of lessons were learned?

111
00:07:26,750 --> 00:07:30,870
For me, what was most satisfying about the incident response

112
00:07:30,950 --> 00:07:34,874
was not the decreased meantime to resolution or MTTR,

113
00:07:35,002 --> 00:07:39,050
but rather reflecting on what the chaos engineering experiment provided.

114
00:07:39,210 --> 00:07:42,874
The experiment itself was not designed to help streamline

115
00:07:42,922 --> 00:07:47,022
our incident response and reduce MTTR. The experiment

116
00:07:47,086 --> 00:07:51,026
was designed to inject failures and observe the service response so

117
00:07:51,048 --> 00:07:54,414
that we could gain a sense of the service's resiliency and document

118
00:07:54,462 --> 00:07:57,718
those findings. The outcome that was obtained from the

119
00:07:57,724 --> 00:08:01,714
Chaos engineering experiment was a production in MTTR

120
00:08:01,762 --> 00:08:05,426
for a production incident, along with some odd bugs and behaviors

121
00:08:05,458 --> 00:08:08,910
that were eventually turned into fixes and feature requests.

122
00:08:09,090 --> 00:08:12,886
I'm so thankful that we documented the chaos engineering experiment

123
00:08:12,998 --> 00:08:16,710
and the outcomes as without it, the production incident

124
00:08:16,790 --> 00:08:20,266
definitely would have occurred for a longer time, and we may have had

125
00:08:20,288 --> 00:08:23,806
to have taken an educated guess at the commands to resolve the

126
00:08:23,828 --> 00:08:27,118
issue. This is never a good place to be when you're in

127
00:08:27,204 --> 00:08:31,050
mid incident. Some engineers may think of nonproduction

128
00:08:31,130 --> 00:08:34,626
primarily as a place to test out feature changes

129
00:08:34,728 --> 00:08:38,174
to make sure that these don't cause errors, to trial

130
00:08:38,222 --> 00:08:41,470
out memory or cpu increases or decreases

131
00:08:41,630 --> 00:08:44,930
to see if these improve performance, or to apply

132
00:08:45,000 --> 00:08:48,214
patches before they hit production to observe any

133
00:08:48,252 --> 00:08:51,734
issues. However, with chaos engineering, we can now

134
00:08:51,772 --> 00:08:55,762
also think of nonproduction as a place to safely inject failures

135
00:08:55,906 --> 00:08:59,174
and then take those learnings to our higher level production

136
00:08:59,222 --> 00:09:02,554
environments. Capturing those experiment results can

137
00:09:02,592 --> 00:09:06,422
be huge and can act as a point of reference during an unintended incident.

138
00:09:06,486 --> 00:09:10,006
As I've demonstrated further, after more confidence is

139
00:09:10,048 --> 00:09:13,546
built, you can run the chaos engineering experiments directly

140
00:09:13,578 --> 00:09:17,562
in production to further verify the availability and resiliency

141
00:09:17,626 --> 00:09:21,134
of your service. Lastly, when we

142
00:09:21,172 --> 00:09:25,002
create service offerings or set up new technologies like kubernetes,

143
00:09:25,146 --> 00:09:28,002
we tend to think about simply getting the service to work,

144
00:09:28,136 --> 00:09:31,586
and that in of itself is no small fee. It's often an

145
00:09:31,608 --> 00:09:34,674
underrated milestone. However, when we start to use our

146
00:09:34,712 --> 00:09:38,802
imagination and try to break the service in creative

147
00:09:38,866 --> 00:09:42,322
or esoteric ways and introduce some chaos.

148
00:09:42,466 --> 00:09:46,214
Some very interesting results can be captured. These results and

149
00:09:46,252 --> 00:09:49,814
learnings can then be applied to the moneymaker production and

150
00:09:49,852 --> 00:09:53,862
can be very helpful when it matters mid incident. So with

151
00:09:53,916 --> 00:09:57,286
that, thank you all for tuning in and listening to this talk. And thank you

152
00:09:57,308 --> 00:10:01,006
to Comp 42 for providing the opportunity city. I look

153
00:10:01,028 --> 00:10:04,446
forward to hearing from everyone about your chaos engineering experiments and

154
00:10:04,468 --> 00:10:05,710
journeys in the future.

