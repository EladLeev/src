1
00:00:25,170 --> 00:00:27,560
Hi. This is our talk. Don't waste data.

2
00:00:30,490 --> 00:00:33,942
All right, everyone, so we

3
00:00:33,996 --> 00:00:37,350
are going to start off by talking about a classic problem.

4
00:00:37,500 --> 00:00:41,734
We all remember the traveling salesman problem that we studied during our

5
00:00:41,772 --> 00:00:44,518
undergrad days. But after that,

6
00:00:44,604 --> 00:00:48,614
that entire problem took a backseat right

7
00:00:48,652 --> 00:00:51,694
after we turned in our test papers and things like that,

8
00:00:51,812 --> 00:00:55,146
but were at lob. We take that traveling

9
00:00:55,178 --> 00:00:58,894
salesman problem, or as we call it, the traveling mail piece

10
00:00:58,932 --> 00:01:02,778
problem, a little bit more seriously. As a print and mail company,

11
00:01:02,964 --> 00:01:06,962
we generate digitally the

12
00:01:07,016 --> 00:01:11,074
mail pieces and route it to our print partners, as we

13
00:01:11,112 --> 00:01:15,274
say in our company moto, to connect the offline and online walls.

14
00:01:15,422 --> 00:01:18,840
And we have to deal with this every single day.

15
00:01:19,290 --> 00:01:22,950
So, as we know, the USPS mailpieces

16
00:01:23,290 --> 00:01:27,350
and the mailing times have been impacted severely

17
00:01:27,790 --> 00:01:31,834
during the pandemic, and we want to make sure that

18
00:01:31,952 --> 00:01:35,482
these mail pieces that we are responsible for gets to its

19
00:01:35,536 --> 00:01:37,660
destination as fast as possible.

20
00:01:38,590 --> 00:01:42,602
So in this case, just like the traveling salesman

21
00:01:42,666 --> 00:01:46,842
problem, the mailpieces make multiple hops through the various

22
00:01:46,906 --> 00:01:50,666
distribution centers that are across the entire USPS network.

23
00:01:50,858 --> 00:01:54,242
So some of these hop, as we call

24
00:01:54,296 --> 00:01:58,158
them, are beyond our control because we literally do not control the usPs,

25
00:01:58,334 --> 00:02:01,442
but some of the hops are

26
00:02:01,496 --> 00:02:04,970
within our control. That includes which mail partner

27
00:02:05,070 --> 00:02:07,974
is the origination point for this mail piece.

28
00:02:08,172 --> 00:02:12,066
That's why we spent a lot of time working on a routing

29
00:02:12,098 --> 00:02:15,234
engine, which uses a modified geolocation

30
00:02:15,282 --> 00:02:19,542
algorithm to determine which print partners are the closest

31
00:02:19,606 --> 00:02:24,262
to the destination of the mail piece. That potentially

32
00:02:24,406 --> 00:02:27,660
could be cutting down a transit time, right?

33
00:02:28,270 --> 00:02:31,966
But is that right? Over to Aditi to

34
00:02:32,148 --> 00:02:36,094
talk more about this problem? As it so happens,

35
00:02:36,212 --> 00:02:39,034
in a twist worthy of M. Night Shyamalan,

36
00:02:39,082 --> 00:02:41,790
our current approach may not be the best one.

37
00:02:41,940 --> 00:02:45,794
Only time and more performance results will tell. But as

38
00:02:45,832 --> 00:02:49,966
Anisha and I will discuss, sometimes the most obvious solution

39
00:02:50,078 --> 00:02:53,490
isn't necessarily the best one. Sometimes it takes

40
00:02:53,560 --> 00:02:56,706
creative thinking and data analysis to unearth surprising

41
00:02:56,738 --> 00:03:00,230
improvements. Our project began on a whim.

42
00:03:00,730 --> 00:03:03,974
Like many companies, lob.com has databases of

43
00:03:04,012 --> 00:03:07,350
information on the mail pieces we send. And like many

44
00:03:07,420 --> 00:03:11,402
companies which aren't strictly ML oriented, we've conducted very

45
00:03:11,456 --> 00:03:14,774
little analysis on this data using machine learning tools.

46
00:03:14,902 --> 00:03:18,086
There's a conception that machine learning is somehow

47
00:03:18,118 --> 00:03:22,320
inaccessible, that it needs expert guidance and

48
00:03:22,850 --> 00:03:26,190
a dedicated ML team to really implement.

49
00:03:26,850 --> 00:03:30,634
As a bunning ML student, someone who has taken a couple of ML

50
00:03:30,682 --> 00:03:34,358
classes but is still an amateur, I saw untapped

51
00:03:34,394 --> 00:03:37,358
potential for improvements to a number of our systems,

52
00:03:37,454 --> 00:03:41,630
foremost being our routing engine. Is it possible, I wondered,

53
00:03:41,710 --> 00:03:45,722
that we were taking a less efficient approach to print partner selection

54
00:03:45,886 --> 00:03:49,746
could data analysis tell a different story than the one we had assumed

55
00:03:49,778 --> 00:03:52,710
would hold true over to Anisha?

56
00:03:54,810 --> 00:03:58,338
All right, so before we talk about the analysis,

57
00:03:58,434 --> 00:04:01,814
or what Aditi and I unearthed,

58
00:04:01,942 --> 00:04:05,594
I want us to get some baselines here around the

59
00:04:05,632 --> 00:04:09,066
context of the data that we are talking about. So the

60
00:04:09,088 --> 00:04:13,758
USPS mail distribution network splits nearly 42,000

61
00:04:13,924 --> 00:04:17,806
destination zip codes in the US in various fixed hierarchies and

62
00:04:17,828 --> 00:04:21,354
webs, the top most being the 29 national

63
00:04:21,402 --> 00:04:25,246
distribution centers, followed by 450 regional

64
00:04:25,358 --> 00:04:29,058
sectional distribution centers. Millions of

65
00:04:29,144 --> 00:04:33,246
mail pieces move through these systems every day, and each mail

66
00:04:33,278 --> 00:04:36,870
piece can generate up to 105 different

67
00:04:36,940 --> 00:04:40,722
operations codes that translate to different business logic

68
00:04:40,786 --> 00:04:44,274
around what stage of the mailing

69
00:04:44,322 --> 00:04:47,478
process is that mail piece on? So, as you

70
00:04:47,484 --> 00:04:51,786
can see, this particular system has been built

71
00:04:51,888 --> 00:04:55,386
to create messy data. So adity and

72
00:04:55,408 --> 00:04:59,062
I brainstorm to pick up the highest fidelity signals

73
00:04:59,126 --> 00:05:01,786
that we can from this messy ops codes,

74
00:05:01,898 --> 00:05:05,434
focusing specifically on the mail entry and exit

75
00:05:05,482 --> 00:05:09,114
signals. That helped us to translate to a simple metric.

76
00:05:09,242 --> 00:05:12,930
What are the transit times that a male piece takes when it

77
00:05:13,000 --> 00:05:16,802
goes through a particular partner and a particular NDC and

78
00:05:16,856 --> 00:05:20,306
SCF networks? We calculated the

79
00:05:20,328 --> 00:05:24,018
difference between the earliest and the latest ops code

80
00:05:24,104 --> 00:05:27,286
timestamps for each mail piece in order to figure

81
00:05:27,308 --> 00:05:30,274
out this transit, and that was our target metric.

82
00:05:30,402 --> 00:05:34,054
Although we discussed a more elaborate version of this

83
00:05:34,092 --> 00:05:38,406
project that involves using vectors of these ops codes

84
00:05:38,598 --> 00:05:41,846
and also talking about specific network routes.

85
00:05:41,958 --> 00:05:45,626
But for the purpose of the first iteration, we narrowed down to

86
00:05:45,648 --> 00:05:49,254
zip codes associated with specific partners with solid

87
00:05:49,302 --> 00:05:52,506
historical data, and the corresponding destination zip

88
00:05:52,538 --> 00:05:56,254
codes for the mail pieces over to adity to talk in

89
00:05:56,292 --> 00:05:59,886
detail about our model. Once we

90
00:05:59,908 --> 00:06:03,438
had determined the shape of the data and the features we should focus on,

91
00:06:03,524 --> 00:06:06,926
I set about creating a model with a wealth of ML tools

92
00:06:06,958 --> 00:06:10,866
available across languages like Python and Julia, this was hardly a

93
00:06:10,888 --> 00:06:14,914
daunting task. For the purpose of this, I used scikit learn,

94
00:06:15,032 --> 00:06:18,662
one of the most popular ML libraries around, and plugged the data

95
00:06:18,716 --> 00:06:22,070
into a random forest regression as input. As mentioned

96
00:06:22,140 --> 00:06:25,746
earlier, I used the zip codes of the print partner and the destination

97
00:06:25,778 --> 00:06:29,634
of the mail piece. Our output target was the metric we had calculated

98
00:06:29,682 --> 00:06:33,334
during pre processing the difference in days between the earliest

99
00:06:33,382 --> 00:06:37,114
and latest USPS events recorded for each mail piece. In other

100
00:06:37,152 --> 00:06:41,242
words, the mailpiece's time and transit, even just using 50%

101
00:06:41,296 --> 00:06:44,378
of the data or 500k data points due

102
00:06:44,394 --> 00:06:47,866
to databricks memory constraints, the difference between the model's

103
00:06:47,898 --> 00:06:52,202
predictions and the average error calculated using non machine learning methods

104
00:06:52,266 --> 00:06:56,114
was astonishing. The error margin dropped from 1.43

105
00:06:56,152 --> 00:06:59,486
days to zero point 86. Print partners

106
00:06:59,518 --> 00:07:03,502
which were further geographically from the destinations sometimes yielded

107
00:07:03,566 --> 00:07:06,694
lower times in transit, a conclusion which may not

108
00:07:06,732 --> 00:07:10,482
seem intuitive, but one which does take into account historical

109
00:07:10,546 --> 00:07:14,120
data in a way our current routing algorithm does not.

110
00:07:14,810 --> 00:07:18,838
This approach can also account for seasonal fluctuations

111
00:07:18,934 --> 00:07:22,634
in a way that geolocation doesn't, necessarily, because those

112
00:07:22,672 --> 00:07:25,980
seasonal patterns will show up in the historical data.

113
00:07:26,510 --> 00:07:30,426
So one print partner might be closer to

114
00:07:30,448 --> 00:07:33,786
a destination, but maybe it gets flooded around Christmas,

115
00:07:33,898 --> 00:07:37,678
and it's actually faster to send it to another print partner which is a

116
00:07:37,684 --> 00:07:40,814
little further away. And that's something that you can

117
00:07:40,852 --> 00:07:44,946
get from the data that we found, but not necessarily something that

118
00:07:44,968 --> 00:07:49,214
would show up using just a strict geolocation

119
00:07:49,342 --> 00:07:52,706
approach. Although we are in the early stages of

120
00:07:52,728 --> 00:07:56,482
this project and have not yet plugged this into the live router to aggregate

121
00:07:56,546 --> 00:07:59,794
performance data, these preliminary numbers justify

122
00:07:59,842 --> 00:08:02,918
our decision to analyze exists historical data,

123
00:08:03,084 --> 00:08:06,802
even though our routing algorithm already seemed to be optimally

124
00:08:06,866 --> 00:08:09,260
configured according to common sense.

125
00:08:11,550 --> 00:08:15,734
So what's the takeaway? This project began as something shockingly

126
00:08:15,782 --> 00:08:19,034
simple. No in depth knowledge of ML was

127
00:08:19,072 --> 00:08:22,446
required. I didn't have to do any calculus or

128
00:08:22,468 --> 00:08:25,854
write my own custom model. The most challenging part was

129
00:08:25,892 --> 00:08:29,566
deciding which data features to use. Although Anisha and

130
00:08:29,588 --> 00:08:33,818
I have plans to further refine our work, which could add complexity,

131
00:08:33,914 --> 00:08:37,358
the baseline project was finished in a matter of two days during a

132
00:08:37,364 --> 00:08:40,734
work hackathon, which leads really to the point

133
00:08:40,772 --> 00:08:44,234
of this talk. Our efforts can and could easily

134
00:08:44,282 --> 00:08:48,262
be replicated it in a digital world packed with data about

135
00:08:48,316 --> 00:08:51,494
practically anything. A company doesn't need to have an

136
00:08:51,532 --> 00:08:54,466
ML focus, or a dedicated ML department,

137
00:08:54,578 --> 00:08:58,054
or even an ML expert in order to make breakthroughs and

138
00:08:58,092 --> 00:09:02,310
improvements to their processes using machine learning tools.

139
00:09:02,650 --> 00:09:06,194
All you really need to do is pull up scikit documentation

140
00:09:06,322 --> 00:09:07,620
and put your data to work.

