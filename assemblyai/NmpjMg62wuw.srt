1
00:00:00,410 --> 00:00:06,126
Jamaica make up real

2
00:00:06,148 --> 00:00:09,902
time feedback into the behavior of your distributed systems and

3
00:00:09,956 --> 00:00:12,490
observing changes exceptions.

4
00:00:12,570 --> 00:00:16,014
Errors in real time allows you to not only experiment with

5
00:00:16,052 --> 00:00:19,822
confidence, but respond instantly to get things working

6
00:00:19,876 --> 00:00:20,480
again.

7
00:00:24,610 --> 00:01:05,522
Close hello

8
00:01:05,576 --> 00:01:09,426
friends. Thank you everybody for taking the

9
00:01:09,448 --> 00:01:12,962
time to join our session today with you.

10
00:01:13,016 --> 00:01:16,238
It's me, Soumen Chatterjee, partner Solution architect

11
00:01:16,334 --> 00:01:19,454
AWS and my colleague Natalie, senior applied

12
00:01:19,502 --> 00:01:22,914
scientist AWS taking you through the cows

13
00:01:22,962 --> 00:01:27,254
engineering at at at. At. At. At. At the

14
00:01:27,292 --> 00:01:31,142
age of AI and know that AI is omnipresent in our everyday

15
00:01:31,206 --> 00:01:34,474
life. Embedded AI is embraced in

16
00:01:34,512 --> 00:01:38,106
almost all types of business fabric. AI is at

17
00:01:38,128 --> 00:01:41,546
the heart of everything we do. Chaos engineering,

18
00:01:41,578 --> 00:01:45,418
on the other hand, has become a no brainer testing approach.

19
00:01:45,594 --> 00:01:48,874
Adversarial ML techniques and attack spectrums

20
00:01:49,002 --> 00:01:52,442
raises the unique question, if ML model testing

21
00:01:52,506 --> 00:01:55,902
still benefit from chaos engineering approach, how do we

22
00:01:55,956 --> 00:01:59,298
adopt the Cows engineering strategies within

23
00:01:59,384 --> 00:02:03,250
ML lifecycle? This session will guide you through building

24
00:02:03,320 --> 00:02:07,154
an approach of chaos engineering at at the age of

25
00:02:07,202 --> 00:02:11,014
AI. And let's quickly

26
00:02:11,212 --> 00:02:14,866
take you through the key topics or our agenda

27
00:02:14,898 --> 00:02:18,538
today in the session. Everything fails all the time.

28
00:02:18,704 --> 00:02:22,538
Introducing machine learning and its complexity model

29
00:02:22,624 --> 00:02:26,694
performs and predicts may not fail adversarial

30
00:02:26,822 --> 00:02:31,026
introducing a significant business impact AWS Sagemaker

31
00:02:31,078 --> 00:02:35,066
model debug and monitoring chaos engineering as a continuum

32
00:02:35,178 --> 00:02:38,634
product thinking versus every time everywhere.

33
00:02:38,762 --> 00:02:42,158
Can you guess what will happen here? There is a man

34
00:02:42,244 --> 00:02:45,470
walking, then decide to go over the fence.

35
00:02:45,970 --> 00:02:48,320
Let's see what happens.

36
00:03:01,920 --> 00:03:03,390
Still time to guess.

37
00:03:07,430 --> 00:03:08,180
See,

38
00:03:11,590 --> 00:03:12,340
yes.

39
00:03:15,030 --> 00:03:19,138
So, so that is very,

40
00:03:19,224 --> 00:03:22,954
very familiar, right? Isn't it? So, in our every

41
00:03:23,032 --> 00:03:26,602
day life, when we do our programs, we write

42
00:03:26,656 --> 00:03:30,282
programs, we write our model. It follows very similar

43
00:03:30,416 --> 00:03:33,798
situation. Sometimes it is very unpredictable.

44
00:03:33,974 --> 00:03:37,102
One simple things can shut down

45
00:03:37,156 --> 00:03:42,094
your whole system, or it

46
00:03:42,132 --> 00:03:46,474
can shut down your program. It can do a completely incorrect

47
00:03:46,522 --> 00:03:49,538
prediction. So everything fails all the time.

48
00:03:49,704 --> 00:03:53,540
That's really a great quote from our

49
00:03:53,910 --> 00:03:56,020
CTO of Amazon.com.

50
00:03:58,390 --> 00:04:01,846
What about machine learning? Can it fail too?

51
00:04:02,028 --> 00:04:06,034
Let's have a quick review of what ML

52
00:04:06,082 --> 00:04:10,214
does. So what is machine learning? So we

53
00:04:10,252 --> 00:04:13,498
all know what machine learning is in

54
00:04:13,504 --> 00:04:16,826
a simple way. So using technology

55
00:04:16,928 --> 00:04:20,550
to discover trends and patterns, use compute,

56
00:04:20,630 --> 00:04:24,118
mathematical, complex computation mathematically

57
00:04:24,294 --> 00:04:28,270
to predict models based on factual past data.

58
00:04:28,420 --> 00:04:31,834
So past data, statistics and probability theory

59
00:04:31,962 --> 00:04:35,582
are the key tools used to build machine learning models and make

60
00:04:35,636 --> 00:04:39,774
predictions. So where traditional business analytics aims

61
00:04:39,822 --> 00:04:43,778
at answering questions about past events, machine learning

62
00:04:43,864 --> 00:04:47,454
aims at answering questions about the possibilities

63
00:04:47,582 --> 00:04:51,046
or probabilities of the future events. So when

64
00:04:51,068 --> 00:04:54,770
to use machine learning? There are four key categories,

65
00:04:54,850 --> 00:04:58,102
I would say, where we tend to use

66
00:04:58,236 --> 00:05:02,198
machine learning more than anything else.

67
00:05:02,364 --> 00:05:06,042
So category one is use ML when you can't code

68
00:05:06,096 --> 00:05:10,422
it. So complex tasks where deterministic solutions don't suffice,

69
00:05:10,566 --> 00:05:15,110
like recognizing speech or images. Category two,

70
00:05:15,280 --> 00:05:19,738
use ML when you can't scale it. So replace repetitive tasks

71
00:05:19,834 --> 00:05:21,870
needing humanlike expertise.

72
00:05:22,770 --> 00:05:25,310
Example would be recommendations,

73
00:05:25,810 --> 00:05:29,650
spam, fraud detection, machine translation.

74
00:05:30,070 --> 00:05:34,254
Category three is where you need to adapt or personalize

75
00:05:34,382 --> 00:05:37,380
like recommendation engine or personalization engine.

76
00:05:37,990 --> 00:05:42,230
And the fourth category where you can't track it like

77
00:05:42,300 --> 00:05:46,006
automated driving. And imagine all this category. As you

78
00:05:46,028 --> 00:05:48,920
know that all this category is very much dependent on the data,

79
00:05:50,330 --> 00:05:53,546
your model will be as good as your data.

80
00:05:53,728 --> 00:05:57,418
So in category one, for example, you are in a

81
00:05:57,424 --> 00:06:00,698
manufacturing unit or a car manufacturing company.

82
00:06:00,784 --> 00:06:06,986
Or it could be a production lab for

83
00:06:07,088 --> 00:06:11,142
any fast moving goods or any food supply

84
00:06:11,286 --> 00:06:14,694
company. And they are heavily

85
00:06:14,742 --> 00:06:18,450
dependent on machine learning, especially computer

86
00:06:18,520 --> 00:06:22,370
evasions and images for the production quality

87
00:06:22,440 --> 00:06:25,566
detections or finding any faults

88
00:06:25,598 --> 00:06:28,834
in the system. And guess what?

89
00:06:29,032 --> 00:06:32,374
If you get a program which

90
00:06:32,412 --> 00:06:35,942
manipulates your model through a different

91
00:06:35,996 --> 00:06:39,814
set of data input and then that will change the

92
00:06:39,852 --> 00:06:43,202
prediction or the behavior of your model

93
00:06:43,356 --> 00:06:46,806
and that will be fatal. Sometimes that could be fatal,

94
00:06:46,838 --> 00:06:51,142
right? If it is a manufacturing company and you are not able to detect

95
00:06:51,286 --> 00:06:54,746
your production issues through

96
00:06:54,768 --> 00:06:58,634
the images, that could be a fatal

97
00:06:58,682 --> 00:07:02,718
things. So these kind of things

98
00:07:02,884 --> 00:07:07,034
we call the data drift. Like if we manage to drift

99
00:07:07,082 --> 00:07:10,850
your data to a different segment of

100
00:07:11,000 --> 00:07:14,194
the data and that can change the

101
00:07:14,232 --> 00:07:18,126
behavior of your model. Similarly, in the category

102
00:07:18,158 --> 00:07:21,800
two where we can bypass a lot of content

103
00:07:22,250 --> 00:07:25,974
instead of spam, it can look

104
00:07:26,012 --> 00:07:29,634
like a legitimate good content and that can go to your normal

105
00:07:29,682 --> 00:07:33,514
folders like an email spam, right? And EU and we

106
00:07:33,552 --> 00:07:37,754
all are facing that every day. You will see that there are

107
00:07:37,952 --> 00:07:41,866
lot of models like a lot of content which

108
00:07:41,888 --> 00:07:46,634
are bypassing and coming to our main folder which

109
00:07:46,672 --> 00:07:50,522
are good candidates for spam. Now imagine

110
00:07:50,586 --> 00:07:55,070
the fourth category where you are building your automated driving.

111
00:07:56,050 --> 00:08:00,174
A lot of things, I would say, if not everything are

112
00:08:00,212 --> 00:08:03,906
AI AML dependent, right? If your model is

113
00:08:03,928 --> 00:08:07,874
not able to detect the right level of speed from

114
00:08:07,912 --> 00:08:11,062
the speed signs outside or

115
00:08:11,196 --> 00:08:14,866
stop signs, or not able to detect the objects

116
00:08:15,058 --> 00:08:18,914
on the way. So that could even be another fatal

117
00:08:18,962 --> 00:08:22,534
example. It could be life threatening for the

118
00:08:22,572 --> 00:08:25,866
users of the particular automated car.

119
00:08:25,968 --> 00:08:29,820
So these are the scenario where machine learnings are

120
00:08:31,150 --> 00:08:35,114
greatly, I think, great examples and had been

121
00:08:35,152 --> 00:08:38,218
adopted significantly across the industry,

122
00:08:38,314 --> 00:08:42,234
but at the same time adversarial or adversarial input,

123
00:08:42,282 --> 00:08:43,680
I would say, right,

124
00:08:46,050 --> 00:08:49,586
that trade vector introducing a lot

125
00:08:49,608 --> 00:08:53,666
of different kind of challenges for

126
00:08:53,688 --> 00:08:56,834
your industry and how that impacts your

127
00:08:56,872 --> 00:09:00,674
business. And that's one

128
00:09:00,712 --> 00:09:04,580
of the key intention to introduce today.

129
00:09:05,210 --> 00:09:09,170
Little later, I think Natalie will take you through some of the great examples

130
00:09:09,250 --> 00:09:13,114
of adversarial input of the data and how that

131
00:09:13,152 --> 00:09:16,442
impacts your machine learning, its prediction quality

132
00:09:16,576 --> 00:09:20,090
and how that disturbs the model prediction.

133
00:09:22,030 --> 00:09:25,414
So AI and AML systems are actually very

134
00:09:25,472 --> 00:09:29,214
complex. It's not because

135
00:09:29,412 --> 00:09:33,850
it's a complex algorithm, it is because it's iterative

136
00:09:34,010 --> 00:09:37,354
and it has got many steps, especially the stage

137
00:09:37,402 --> 00:09:41,314
one itself, like preparation or prepared stage, right, where you

138
00:09:41,352 --> 00:09:45,010
collect and prepare training data. And this data,

139
00:09:45,160 --> 00:09:48,290
one of the key criteria for this data.

140
00:09:48,440 --> 00:09:52,294
For a very successful or accurate, or I would say high

141
00:09:52,332 --> 00:09:56,370
performing models, you need to have representative

142
00:09:56,450 --> 00:09:59,954
data or samples you need to collect.

143
00:10:00,002 --> 00:10:03,306
And that's not an easy stage, actually. So the

144
00:10:03,328 --> 00:10:06,634
building is relatively easy in terms

145
00:10:06,672 --> 00:10:09,606
of the model, which you call the algorithm,

146
00:10:09,638 --> 00:10:12,954
right? Or that's the heart of your

147
00:10:12,992 --> 00:10:17,166
code, but that's not heart of the whole thing.

148
00:10:17,268 --> 00:10:20,798
So model or algorithm is just a one part of

149
00:10:20,884 --> 00:10:24,766
the successful ML ecosystem. So data,

150
00:10:24,868 --> 00:10:28,042
how you train it or how you tune it

151
00:10:28,196 --> 00:10:32,018
and how you manage your evasions, how you train and

152
00:10:32,104 --> 00:10:36,386
debug are the key part for any

153
00:10:36,488 --> 00:10:40,082
successful model deploy or delivery, and then

154
00:10:40,136 --> 00:10:44,070
how you manage and scale and monitor

155
00:10:44,570 --> 00:10:48,534
its predictions and accuracy quality. So that is

156
00:10:48,652 --> 00:10:52,654
another complex aspect of a successful

157
00:10:52,802 --> 00:10:53,660
full model.

158
00:10:57,550 --> 00:11:00,870
So traditional software and program testing.

159
00:11:00,950 --> 00:11:04,394
So traditional programs are deterministic, as you all

160
00:11:04,432 --> 00:11:08,106
know. Right? So it's based on a fixed set of heuristic

161
00:11:08,218 --> 00:11:10,826
rules. Generally, testing,

162
00:11:10,938 --> 00:11:14,714
traditional software includes unit testing,

163
00:11:14,762 --> 00:11:18,434
regression and integration testing. But in

164
00:11:18,472 --> 00:11:21,406
our ML system, in the world of ML,

165
00:11:21,518 --> 00:11:25,502
ML systems are not heuristics, they are stochastics

166
00:11:25,566 --> 00:11:29,186
and probabilistic. What it means that starting from

167
00:11:29,288 --> 00:11:32,690
left to right, all the stages,

168
00:11:32,770 --> 00:11:35,558
like pretrain, post train,

169
00:11:35,644 --> 00:11:39,014
integrate every stage, it goes through

170
00:11:39,212 --> 00:11:42,710
a flow of data and comes

171
00:11:42,780 --> 00:11:45,910
as an output or model. And every stage,

172
00:11:46,070 --> 00:11:50,074
every different data set or data flow, changes these

173
00:11:50,192 --> 00:11:54,214
and refine this model as a final outcome

174
00:11:54,262 --> 00:11:58,218
of the model. So, model learns from the data provided

175
00:11:58,394 --> 00:12:01,440
and used for the training all the time.

176
00:12:03,970 --> 00:12:07,386
And now, coming to the, in the context

177
00:12:07,418 --> 00:12:11,140
of chaos engineering, right. In the chaos engineering, we follow

178
00:12:12,070 --> 00:12:16,020
all the usual steps, right? But I think there are two key

179
00:12:17,030 --> 00:12:20,638
stage, I would say are worth noting, which are different,

180
00:12:20,824 --> 00:12:24,130
quite different compared to traditional software

181
00:12:24,210 --> 00:12:28,982
and machine learning. So there is state

182
00:12:29,036 --> 00:12:32,470
called steady state, right? But in the ML system, there is no

183
00:12:32,620 --> 00:12:36,698
as such steady state, because models are

184
00:12:36,784 --> 00:12:40,426
not steady. Like, if you think that your model is

185
00:12:40,528 --> 00:12:43,946
just static model and it

186
00:12:43,968 --> 00:12:47,870
is now functioning, maybe 98% accuracy and everybody

187
00:12:47,940 --> 00:12:52,446
is happy, it may not function

188
00:12:52,548 --> 00:12:55,802
continuously like that. Similarly,

189
00:12:55,946 --> 00:12:59,822
the verify stage, we inspect metrics

190
00:12:59,886 --> 00:13:03,634
and plots summarizing model performance, not like

191
00:13:03,672 --> 00:13:07,490
verifying certain test that it passed or failed.

192
00:13:08,310 --> 00:13:11,938
So it is quite different, especially these two stage

193
00:13:12,114 --> 00:13:15,430
compared to any other software and models

194
00:13:16,970 --> 00:13:20,486
moving to the next one. So model evaluation and

195
00:13:20,508 --> 00:13:24,342
model test. So ML systems, unlike traditional models,

196
00:13:24,406 --> 00:13:28,218
does not produce a report of specific

197
00:13:28,304 --> 00:13:32,086
behaviors and metrics, such as how many tests passed

198
00:13:32,118 --> 00:13:36,170
or failed or it has got 100% code coverage,

199
00:13:36,990 --> 00:13:40,394
so on and so forth. Instead of that,

200
00:13:40,432 --> 00:13:44,158
what do we do here? We perform the model evaluation towards

201
00:13:44,244 --> 00:13:47,962
performance analysis. Whereas model

202
00:13:48,036 --> 00:13:50,798
test is an approach towards error analysis,

203
00:13:50,974 --> 00:13:54,866
developing a model test for ML systems can offer a

204
00:13:54,888 --> 00:13:58,130
systematic approach towards error analysis.

205
00:13:58,710 --> 00:14:02,474
For machine learning model, we inspect metrics and plot

206
00:14:02,542 --> 00:14:06,054
summarizing models performance over evaluation data

207
00:14:06,092 --> 00:14:06,950
sets.

208
00:14:11,050 --> 00:14:14,802
So now we are entering into

209
00:14:14,956 --> 00:14:21,914
adversarial machine learning stage and

210
00:14:21,952 --> 00:14:23,850
it can get more complicated.

211
00:14:24,990 --> 00:14:28,654
Machine learning obviously we are talking here when

212
00:14:28,692 --> 00:14:32,586
adverse serial input gets introduced. So ML models

213
00:14:32,618 --> 00:14:36,666
are vulnerable to such inputs. So adversarial

214
00:14:36,778 --> 00:14:41,054
machine learning is a machine learning method that crafts

215
00:14:41,102 --> 00:14:45,134
input to trick machine learning models to strategies alter

216
00:14:45,182 --> 00:14:50,078
the model output and there are three different kind of attacks

217
00:14:50,254 --> 00:14:53,102
are predominantly observed,

218
00:14:53,246 --> 00:14:56,482
evasion, poison and model extraction.

219
00:14:56,626 --> 00:15:00,386
So types of detection method in individual input samples

220
00:15:00,418 --> 00:15:02,070
and distribution ships.

221
00:15:06,670 --> 00:15:08,570
So chaos in practice,

222
00:15:15,390 --> 00:15:19,022
I think watch this for another few

223
00:15:19,076 --> 00:15:22,000
seconds and you will really enjoy it.

224
00:15:29,250 --> 00:15:33,166
So chaos engineering is the discipline of experimenting

225
00:15:33,278 --> 00:15:36,866
on a distributed system in order to build confidence in

226
00:15:36,888 --> 00:15:40,574
the system's capability to withstand turbulent

227
00:15:40,622 --> 00:15:42,210
conditions in production.

228
00:15:46,650 --> 00:15:50,466
Break your system on purpose, find out their weakness

229
00:15:50,578 --> 00:15:54,198
and fix them before they break when least

230
00:15:54,284 --> 00:15:58,362
expected. So I'm going to hand over to

231
00:15:58,496 --> 00:16:01,946
my colleague Natalie now. She will take you through how

232
00:16:01,968 --> 00:16:05,914
to detect adversarial samples. She has prepared a few

233
00:16:06,032 --> 00:16:09,950
fantastic examples from our lab. She will also take you through

234
00:16:10,100 --> 00:16:13,194
our AWS edge maker tools

235
00:16:13,242 --> 00:16:16,446
for model monitoring, debugging and how

236
00:16:16,468 --> 00:16:20,114
to handle adversarial input in

237
00:16:20,152 --> 00:16:24,590
our model management and model lifecycle.

238
00:16:24,750 --> 00:16:28,434
Over to you, Natalie. Thank you. Let's start with

239
00:16:28,472 --> 00:16:29,860
an example first.

240
00:16:32,470 --> 00:16:36,166
Quite often, adversal samples are very sophisticated and

241
00:16:36,268 --> 00:16:40,086
difficult to distinguish from normal samples. On this

242
00:16:40,108 --> 00:16:43,574
slide here, I'm showing you an image that I have been taken from

243
00:16:43,612 --> 00:16:47,274
the Calltech 101 test data set.

244
00:16:47,472 --> 00:16:51,206
We can see that the model correctly predicts the image

245
00:16:51,238 --> 00:16:54,762
class starfish. Next, I use the same

246
00:16:54,816 --> 00:16:58,314
image and I apply an attack on this image.

247
00:16:58,442 --> 00:17:01,694
I'm using the attack technique PGD, which stands for

248
00:17:01,732 --> 00:17:05,754
projected gradient descent. This technique

249
00:17:05,802 --> 00:17:09,326
uses an epsilon parameter to define the amount of noise that

250
00:17:09,348 --> 00:17:12,934
is added to the input. The higher the amount of noise,

251
00:17:13,002 --> 00:17:15,810
the more likely the attack is going to be successful.

252
00:17:16,310 --> 00:17:20,082
What we see now is that we can barely distinguish the original test

253
00:17:20,136 --> 00:17:23,640
image on the left from the adversarial sample on the right.

254
00:17:24,090 --> 00:17:28,120
But the model does no longer predict the correct image class.

255
00:17:29,370 --> 00:17:32,130
Next, I increase the epsilon parameter.

256
00:17:32,290 --> 00:17:35,894
Again, the model does not predict the correct image class, and we

257
00:17:35,932 --> 00:17:39,290
cannot see much of a difference between the images.

258
00:17:40,110 --> 00:17:43,226
When I increase the epsilon parameter even further to

259
00:17:43,328 --> 00:17:46,726
zero five, we finally see some artifacts

260
00:17:46,758 --> 00:17:49,790
that have been introduced into the input image.

261
00:17:52,290 --> 00:17:55,370
Adversarial samples can have devastating business impact.

262
00:17:55,450 --> 00:17:59,520
Imagine, for instance, you have an autonomous driving application,

263
00:18:00,290 --> 00:18:04,062
and as part of this application you have a traffic sign classification

264
00:18:04,206 --> 00:18:07,714
models. The image that I'm showing here is taken

265
00:18:07,752 --> 00:18:12,154
from the german traffic sign data set. We see on the left side the original

266
00:18:12,222 --> 00:18:15,746
input image that shows a speed limit

267
00:18:15,778 --> 00:18:19,174
sign of 80 km/hour again,

268
00:18:19,292 --> 00:18:22,694
in the image in the middle, we can see barely a

269
00:18:22,732 --> 00:18:26,006
difference, but the model can no longer correctly

270
00:18:26,038 --> 00:18:29,370
predict it. It predicts now the traffic sign,

271
00:18:29,440 --> 00:18:33,754
AWS stop sign. And when

272
00:18:33,792 --> 00:18:37,406
we compare the difference between the both, which is indicated here on the

273
00:18:37,428 --> 00:18:40,830
right, we see some difference in the inputs.

274
00:18:41,410 --> 00:18:44,910
So now how can we detect these adversarial samples?

275
00:18:48,130 --> 00:18:51,854
Let's take a look at the model input distributions. To do that,

276
00:18:51,892 --> 00:18:56,630
we use TSNE. TSNE stands for dedistributed

277
00:18:56,650 --> 00:19:00,046
stochastic neighbor embedding. It's a technique that allows

278
00:19:00,078 --> 00:19:03,954
you to take highly dimensional data and map it into a two or three dimensional

279
00:19:04,002 --> 00:19:07,270
space. The image that I'm showing here is

280
00:19:07,340 --> 00:19:11,814
I have taken a set of test images that

281
00:19:11,852 --> 00:19:15,686
are indicated as orange data points. And then I applied the

282
00:19:15,708 --> 00:19:19,446
PGD attack on it. These samples are presented

283
00:19:19,478 --> 00:19:22,490
as adversal samples, indicated as blue data points.

284
00:19:22,640 --> 00:19:26,406
Then, for each of these images, I compute the TSNE embedding

285
00:19:26,518 --> 00:19:29,606
and visualize it. After all, in this two dimensional

286
00:19:29,638 --> 00:19:33,626
space. So each data point that you see here in the image presents

287
00:19:33,658 --> 00:19:37,566
the embedding. For an input image. What we see is that there is

288
00:19:37,588 --> 00:19:41,086
no difference between the orange and the blue data points.

289
00:19:41,268 --> 00:19:45,074
That means if we would use now a

290
00:19:45,112 --> 00:19:49,294
technique to distinguish adversarial and normal inputs

291
00:19:49,342 --> 00:19:52,850
in the input space, like the images,

292
00:19:53,010 --> 00:19:57,394
you would not be able to distinguish them because the distributions

293
00:19:57,442 --> 00:20:01,766
look very similar when

294
00:20:01,788 --> 00:20:04,946
you look at deep neural networks, then these are models that consist

295
00:20:04,978 --> 00:20:09,206
of multiple layers. Each layer learns different kind of features

296
00:20:09,238 --> 00:20:12,490
of the inputs, and they create different representations.

297
00:20:12,990 --> 00:20:16,022
So, the same analysis that I have shown you on the previous slide,

298
00:20:16,086 --> 00:20:20,346
I'm repeating now this analysis for different representations produced

299
00:20:20,378 --> 00:20:23,870
by different layers. In the model, the layer zero

300
00:20:23,940 --> 00:20:29,198
corresponds to our input layer. So that is basically

301
00:20:29,284 --> 00:20:32,686
the input images and again, we don't see much

302
00:20:32,708 --> 00:20:35,730
of a difference between adversarial and normal samples.

303
00:20:37,110 --> 00:20:40,962
Next, I take the activation outputs of layer four and

304
00:20:41,016 --> 00:20:44,246
I repeat this analysis. And again there

305
00:20:44,268 --> 00:20:47,830
is not much of a difference. In layer eight

306
00:20:47,900 --> 00:20:51,794
we see now that normal and adversarial samples cluster

307
00:20:51,842 --> 00:20:55,980
slightly differently, the same we observe in layer twelve.

308
00:20:56,830 --> 00:21:00,022
When we go to layer 14 we see an even larger

309
00:21:00,086 --> 00:21:03,514
difference, and in layer 15 we see now

310
00:21:03,552 --> 00:21:07,290
a clear distinction between the adversarial and the normal samples.

311
00:21:08,190 --> 00:21:11,406
I have done this analysis on a REsnet 18 model. So the

312
00:21:11,428 --> 00:21:14,910
layer 15 was the pen ultimate layer. In my model,

313
00:21:15,060 --> 00:21:18,234
the penultimate layer is the layer before the classification

314
00:21:18,362 --> 00:21:22,150
is done. And when we create adversely

315
00:21:22,170 --> 00:21:25,522
samples, then the goal is to change

316
00:21:25,576 --> 00:21:30,194
the model prediction. So that means that before

317
00:21:30,232 --> 00:21:33,430
the outputs go into the classification layer, they have to create different

318
00:21:33,500 --> 00:21:36,870
representations to lead to a different classification.

319
00:21:37,610 --> 00:21:40,726
What we also observe is that in initial layers we

320
00:21:40,748 --> 00:21:44,626
cannot well distinguish between adversarial and normal samples because the initial

321
00:21:44,658 --> 00:21:48,326
layers learn mainly basic features of your inputs,

322
00:21:48,438 --> 00:21:52,342
while the deeper layers of your model learn more complex patterns

323
00:21:52,406 --> 00:21:56,394
of the input data. So this analysis shows us

324
00:21:56,432 --> 00:21:59,426
if you want to detect adversely samples,

325
00:21:59,558 --> 00:22:02,686
then we need to use the representations that are produced by the

326
00:22:02,708 --> 00:22:05,150
deeper layers of a deep neural network.

327
00:22:06,130 --> 00:22:09,854
What we can do is we can now apply statistical test to

328
00:22:09,892 --> 00:22:13,554
distinguish between these distributions. We can use

329
00:22:13,592 --> 00:22:17,154
a two sample test using MMT, which stands for maximum mean

330
00:22:17,192 --> 00:22:20,686
discrepancy. MMD is a kernel based metric

331
00:22:20,718 --> 00:22:24,390
that allows you to measure the similarity between two distributions.

332
00:22:25,130 --> 00:22:28,674
The distributions we are going to compare are these layer

333
00:22:28,722 --> 00:22:32,466
representations captured from the intermediate layers of your deep neural

334
00:22:32,498 --> 00:22:35,946
network, and we capture them during the validation phase in

335
00:22:35,968 --> 00:22:40,730
the training because these presentations

336
00:22:41,070 --> 00:22:44,474
present the normal samples and

337
00:22:44,512 --> 00:22:47,858
then during inference we capture the same layer representations

338
00:22:48,054 --> 00:22:52,222
and then try to see if the inference data matches the

339
00:22:52,356 --> 00:22:54,640
data during training.

340
00:22:59,490 --> 00:23:02,954
Now I would like to show you how you can detect adversarial inputs

341
00:23:03,002 --> 00:23:06,978
using Amazon Sage mega model monitor and debugger. So basically,

342
00:23:07,064 --> 00:23:10,706
the analysis that I have shown on the previous slide we

343
00:23:10,728 --> 00:23:15,190
are now going to deploy on Amazon Sagemaker and run it in production.

344
00:23:16,570 --> 00:23:19,714
First, let me give just a brief overview of what Amazon

345
00:23:19,762 --> 00:23:23,394
Sagemaker is. Sagemaker is our fully managed

346
00:23:23,442 --> 00:23:27,114
machine learning service that you can use to more easily build, train and

347
00:23:27,152 --> 00:23:30,602
deploy machine learning models. When you think about machine learning,

348
00:23:30,656 --> 00:23:34,234
then it's not just about creating and training a model, but there are many

349
00:23:34,272 --> 00:23:37,738
different steps involved. For instance, you need to

350
00:23:37,744 --> 00:23:41,050
create training data set. You need to build and train models.

351
00:23:41,130 --> 00:23:43,710
You need to perform hyperparameter tuning.

352
00:23:44,290 --> 00:23:48,618
Then you maybe want to compile the model for faster inference

353
00:23:48,794 --> 00:23:52,610
and then you need to deploy the model to the cloud or to the edge.

354
00:23:53,350 --> 00:23:57,282
Amazon Sagemaker provides features for each different step

355
00:23:57,336 --> 00:24:00,994
in this machine learning lifecycle. And as

356
00:24:01,032 --> 00:24:04,386
part of the workflow that I'm going to show in a few slides,

357
00:24:04,578 --> 00:24:08,738
the main features I'm going to use is Sagemaker debugger and Sagemaker

358
00:24:08,754 --> 00:24:12,454
model monitor let's take a brief look at what

359
00:24:12,492 --> 00:24:14,280
Sagemaker model monitor is.

360
00:24:15,450 --> 00:24:18,826
Model monitor is a feature of Amazon Sagemaker that allows you to

361
00:24:18,848 --> 00:24:22,582
detect data drift. So once you have trained an Amazon

362
00:24:22,726 --> 00:24:25,786
a model on Amazon Sagemaker, you can now deploy it

363
00:24:25,808 --> 00:24:26,970
as an endpoint.

364
00:24:29,170 --> 00:24:32,190
Now when users are interacting with your endpoint,

365
00:24:32,530 --> 00:24:36,298
model Monitor will now automatically capture requests and predictions

366
00:24:36,394 --> 00:24:39,230
and upload them to the Amazon S three bucket.

367
00:24:40,610 --> 00:24:44,466
Model monitor will also create a baseline processing job.

368
00:24:44,648 --> 00:24:47,746
It basically takes the training data and computes some

369
00:24:47,768 --> 00:24:52,062
statistics on it. So for instance, assume you have a tabular data set.

370
00:24:52,216 --> 00:24:56,278
Now this baseline processing job will check

371
00:24:56,364 --> 00:24:59,926
the different columns in your tabular data set, such as

372
00:24:59,948 --> 00:25:03,430
the min and max value average, and then

373
00:25:03,580 --> 00:25:07,178
in the deployment phase, you can now specify a

374
00:25:07,184 --> 00:25:10,906
scheduled monitoring job that will run once an hour or once a

375
00:25:10,928 --> 00:25:14,086
day. You can specify the monitoring interval

376
00:25:14,278 --> 00:25:17,754
and this job will basically take the requests and

377
00:25:17,792 --> 00:25:21,374
predictions and compare them against the baseline. As an

378
00:25:21,412 --> 00:25:25,198
example, let's assume in your tabular data set column one was always between

379
00:25:25,284 --> 00:25:28,734
a value of zero and ten during training, and now during

380
00:25:28,772 --> 00:25:32,162
inference, it has a value of -100 to plus

381
00:25:32,216 --> 00:25:35,966
100. Model monitor would automatically detect

382
00:25:35,998 --> 00:25:39,790
this problem and record these violations

383
00:25:39,870 --> 00:25:42,914
and statistics in an output file that is uploaded to Amazon

384
00:25:42,962 --> 00:25:47,110
S three, and it will also publish some metrics to Amazon Cloudwatch.

385
00:25:52,090 --> 00:25:56,038
Let's take a brief look on what Sagemaker

386
00:25:56,054 --> 00:25:59,466
debugger is. Sage Maker Debugger is

387
00:25:59,488 --> 00:26:03,270
a feature that provides you utilities

388
00:26:03,350 --> 00:26:06,378
to record and load tensors from your model training.

389
00:26:06,464 --> 00:26:10,790
It's typically used for training, but you can also use it for inference.

390
00:26:10,950 --> 00:26:13,954
It comes with an API that is called SM debug.

391
00:26:14,022 --> 00:26:17,194
It's open sourced, it's a framework agnostic and concise

392
00:26:17,242 --> 00:26:20,526
API to record and load tensors. It supports the

393
00:26:20,548 --> 00:26:24,126
major machine learning frameworks and it also provides the concept

394
00:26:24,158 --> 00:26:27,554
of rules to automatically detect issues, which is

395
00:26:27,592 --> 00:26:31,150
very useful as part of the training. You can also customize

396
00:26:31,230 --> 00:26:33,570
and extend sagemaker debugger.

397
00:26:34,950 --> 00:26:38,566
If you use debugger as part of Amazon Sagemaker, you can use built in

398
00:26:38,588 --> 00:26:42,614
rules as well as offload the rule analysis to separate instances and

399
00:26:42,652 --> 00:26:46,154
specify rule actions and notifications as

400
00:26:46,192 --> 00:26:49,494
part of the workflow to detect adversarial inputs I mainly

401
00:26:49,542 --> 00:26:53,450
use the SM debug API to capture the layer representations.

402
00:26:55,630 --> 00:26:59,214
Let's take a brief look at the SM debug API. So with just

403
00:26:59,252 --> 00:27:02,682
a few lines of code, you can enable debugger to capture

404
00:27:02,826 --> 00:27:06,366
certain layers from your model. So as we have

405
00:27:06,388 --> 00:27:10,306
shown on the previous slides, I was analyzing the

406
00:27:10,328 --> 00:27:14,034
TSNE embeddings and that

407
00:27:14,072 --> 00:27:17,294
I computed on the different representations produced by the deep

408
00:27:17,342 --> 00:27:20,414
neural network. I use the activation outputs.

409
00:27:20,462 --> 00:27:24,490
So what I do, I specify now a debugger hook configuration

410
00:27:24,670 --> 00:27:27,846
where I can just specify a regular expression of all

411
00:27:27,868 --> 00:27:31,094
the tensors that I want to have collected, and I specify the

412
00:27:31,132 --> 00:27:35,320
output path where this data should be uploaded to.

413
00:27:35,690 --> 00:27:38,906
So with just a few lines of code, I can now enable debugger and

414
00:27:38,928 --> 00:27:42,378
capture this data. And then once you have captured the data,

415
00:27:42,544 --> 00:27:46,006
you can easily access the data using the same API.

416
00:27:46,198 --> 00:27:50,074
You specify where the data has been recorded and with

417
00:27:50,112 --> 00:27:53,642
that object that is created the trial object, you can now start iterating

418
00:27:53,706 --> 00:27:57,006
over all the different inference requests that have been recorded,

419
00:27:57,188 --> 00:28:00,580
access the tensor, and then do a computation on that.

420
00:28:01,510 --> 00:28:05,614
So now I would like to show you the system design for detecting adversal

421
00:28:05,662 --> 00:28:09,970
inputs using Sagemaker model monitor and debugger.

422
00:28:11,030 --> 00:28:14,882
So first I train the model on Amazon Sagemaker

423
00:28:15,026 --> 00:28:18,834
and enable debugger to capture the layer

424
00:28:18,882 --> 00:28:22,066
representations. These tensors

425
00:28:22,098 --> 00:28:25,554
are then uploaded to Amazon s three. Once the model

426
00:28:25,612 --> 00:28:30,230
has been trained, I will now deploy it as an endpoint on Amazon sagemaker.

427
00:28:30,310 --> 00:28:34,154
And in the endpoint I also have debugger enabled to

428
00:28:34,192 --> 00:28:36,970
capture the layer representations during inference.

429
00:28:37,790 --> 00:28:41,594
Now my users may interact with the model and send some inference

430
00:28:41,642 --> 00:28:44,670
requests, and my model performs predictions.

431
00:28:45,410 --> 00:28:49,326
The layer representations as well as the model inputs are recorded in

432
00:28:49,348 --> 00:28:53,346
Amazon history. Now I use a custom

433
00:28:53,528 --> 00:28:57,486
model monitor to basically run this two sample

434
00:28:57,518 --> 00:29:00,850
test using MMD. I run this every hour.

435
00:29:00,920 --> 00:29:04,230
So every hour it will capture the layer representations that were

436
00:29:04,300 --> 00:29:08,594
recorded during the training and compare that with the layer representations

437
00:29:08,722 --> 00:29:10,470
recorded during inference.

438
00:29:12,410 --> 00:29:16,026
And it then runs this two sample test and if an

439
00:29:16,048 --> 00:29:19,702
issue has found it will record some metrics

440
00:29:19,846 --> 00:29:23,722
as well as write an output file to Amazon S three with the

441
00:29:23,776 --> 00:29:27,082
recorded violations. And basically this custom model

442
00:29:27,136 --> 00:29:30,414
monitor will now output the result of which kind of

443
00:29:30,452 --> 00:29:34,606
images have been the most likely adversarial. So as

444
00:29:34,628 --> 00:29:38,206
a user, you can then download this file from Amazon S three and perform

445
00:29:38,308 --> 00:29:40,030
some further investigations.

446
00:29:41,990 --> 00:29:45,998
You can also use Amazon Sagemaker Studio to get some further insights.

447
00:29:46,094 --> 00:29:49,780
Amazon Sagemaker Studio is a machine learning IDE,

448
00:29:50,310 --> 00:29:53,902
and you can check, for instance, the execution

449
00:29:53,966 --> 00:29:57,702
of each of these model monitoring jobs. What we see here

450
00:29:57,756 --> 00:30:01,558
is that in the first hour the model monitoring job did not find

451
00:30:01,644 --> 00:30:05,302
any issue, and in the subsequent hour I send adversarial

452
00:30:05,366 --> 00:30:09,690
samples against the endpoint. So an issue was detected by model monitor.

453
00:30:11,230 --> 00:30:14,726
The custom model monitoring container also outputs some metrics

454
00:30:14,758 --> 00:30:18,890
to Amazon Cloudwatch metrics, such AWS, the inference request

455
00:30:18,970 --> 00:30:22,746
process, and a detection rate that is indicated AWS orange

456
00:30:22,778 --> 00:30:26,462
line shear that indicates how many of these inference requests were

457
00:30:26,516 --> 00:30:30,362
detected as adversarial. And this is computed

458
00:30:30,426 --> 00:30:34,782
for every hour. So you can use that to determine if there was an attacker

459
00:30:34,926 --> 00:30:38,626
active at a specific time frame. What we see here is that the

460
00:30:38,648 --> 00:30:43,046
detection rate was roughly about 100% at

461
00:30:43,148 --> 00:30:46,594
05:00 a.m. To 06:00 a.m. And then it started dropping.

462
00:30:46,642 --> 00:30:50,230
So around 07:00 a.m. The attacker was no longer active.

463
00:30:52,730 --> 00:30:56,294
Sagemaker Debugger stores the tensors in Amazon s

464
00:30:56,332 --> 00:30:59,674
three so as a further analysis, you can now, with just

465
00:30:59,712 --> 00:31:04,010
a few lines of code, use the SM debug API to do some further analysis.

466
00:31:04,510 --> 00:31:08,670
You can now just create this trial object to access the

467
00:31:08,820 --> 00:31:12,330
tensors that have been recorded during inference,

468
00:31:12,490 --> 00:31:16,394
iterate over each inference request, and visualize,

469
00:31:16,442 --> 00:31:19,920
for instance, the TSNE embeddings for each of these

470
00:31:21,190 --> 00:31:25,666
tensors to see. How does the distribution of

471
00:31:25,688 --> 00:31:29,234
the representations during inference compare with

472
00:31:29,272 --> 00:31:31,460
the ones that have been recorded during training?

473
00:31:33,450 --> 00:31:36,470
With that, I would like to conclude my session.

474
00:31:36,890 --> 00:31:40,486
Thank you, Natalie, for taking us through those

475
00:31:40,588 --> 00:31:43,766
great examples of adversarial input and how

476
00:31:43,788 --> 00:31:47,734
to deal with that. We needed to build systems

477
00:31:47,782 --> 00:31:51,050
that embrace failures as a national occurrence.

478
00:31:51,630 --> 00:31:55,402
Another fantastic example, and quote, I always

479
00:31:55,536 --> 00:31:59,734
refer from our Amazon CTO.

480
00:31:59,862 --> 00:32:03,994
So chaos engineering as a continuum. Let's build confident ML

481
00:32:04,042 --> 00:32:07,582
systems that withstand turbulent conditions and

482
00:32:07,636 --> 00:32:11,506
adversarial inputs every time it runs, not just in production or

483
00:32:11,528 --> 00:32:13,380
any particular moment of time.

484
00:32:17,350 --> 00:32:20,674
And I would like to thank you all for

485
00:32:20,712 --> 00:32:24,206
your time today, your interest in our session,

486
00:32:24,318 --> 00:32:27,314
and spending time with us today.

487
00:32:27,512 --> 00:32:30,626
So thank you for that. And this is not

488
00:32:30,648 --> 00:32:35,050
the end. If you really wanted to know more or wanted

489
00:32:35,120 --> 00:32:38,842
to be in touch, please feel free to reach

490
00:32:38,896 --> 00:32:42,746
us, or you can connect us through LinkedIn. Once again,

491
00:32:42,848 --> 00:32:43,980
thanks everybody.

