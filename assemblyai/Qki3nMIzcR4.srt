1
00:01:54,330 --> 00:01:57,854
Good evening. My name is Darren Richardson and I'm here

2
00:01:57,892 --> 00:02:01,550
to give you a brief introduction into the

3
00:02:01,700 --> 00:02:06,230
threat modeling process and how we may apply this to dev ops platforms.

4
00:02:06,570 --> 00:02:09,686
Also, we'll take a quick look at the stride threat model and

5
00:02:09,708 --> 00:02:13,506
how it's changed, or in this case hasn't changed

6
00:02:13,538 --> 00:02:17,240
over time. So we're going to dive in straight to the big question,

7
00:02:17,630 --> 00:02:21,194
what is threat modeling? The answer to this

8
00:02:21,232 --> 00:02:24,890
is simple. Threat modeling is a methodology used to identify,

9
00:02:25,230 --> 00:02:27,850
understand, communicate threats.

10
00:02:28,750 --> 00:02:32,686
At its core, the idea is to understand the

11
00:02:32,708 --> 00:02:35,886
flow of data in a system, understand how the

12
00:02:35,908 --> 00:02:39,486
trust changes for that data, to define the

13
00:02:39,508 --> 00:02:43,050
boundaries where that trust changes, and then to

14
00:02:43,140 --> 00:02:47,090
define mitigation for those switches of trust.

15
00:02:47,240 --> 00:02:51,182
The threat model represents all this in a structured manner.

16
00:02:51,246 --> 00:02:55,314
So dataflow diagrams leads into threat boundary

17
00:02:55,362 --> 00:02:59,320
diagrams and such. But in its most condensed form,

18
00:03:00,010 --> 00:03:04,706
a threat model is just structured

19
00:03:04,898 --> 00:03:08,860
approach to mapping a system and its security.

20
00:03:10,590 --> 00:03:14,010
And now we've discussed what it is, let's discuss what it is

21
00:03:14,080 --> 00:03:17,660
not. And that's restricted because

22
00:03:18,830 --> 00:03:22,766
threat modeling can be applied pretty much anywhere. So you can

23
00:03:22,788 --> 00:03:26,350
apply it to software, applications, networks, systems and even

24
00:03:26,420 --> 00:03:30,398
business processes. If there is any area

25
00:03:30,484 --> 00:03:35,102
where you want to examine how things are done securely,

26
00:03:35,166 --> 00:03:38,914
you can apply threat modeling. And in our case, we're going to

27
00:03:38,952 --> 00:03:41,570
use a DevOps environment.

28
00:03:43,910 --> 00:03:47,810
There are three aspects of threat modeling. You initially

29
00:03:47,890 --> 00:03:51,560
map and diagram the data flow. So, for example,

30
00:03:52,090 --> 00:03:55,734
in our system, we're going to be switching between a lot of different

31
00:03:55,932 --> 00:03:59,598
tools, and the core of threat modeling

32
00:03:59,634 --> 00:04:03,210
is understanding this data flow. So knowing how your data

33
00:04:03,280 --> 00:04:06,666
moves, how your data switches hands, as it

34
00:04:06,688 --> 00:04:09,542
were, and then we break it down to trust boundaries.

35
00:04:09,606 --> 00:04:12,574
And this is, in short,

36
00:04:12,692 --> 00:04:16,318
where the controller ownership of the

37
00:04:16,404 --> 00:04:19,822
data changes. And then we apply the threat model

38
00:04:19,876 --> 00:04:21,790
to this list of trust boundaries.

39
00:04:24,150 --> 00:04:28,174
We'll move on to a little quote from the security development lifecycle

40
00:04:28,222 --> 00:04:30,722
from Microsoft. I like to include this here.

41
00:04:30,776 --> 00:04:34,466
It's basically the best summary of

42
00:04:34,488 --> 00:04:37,878
threat modeling I think there is, if you can just summarize it in

43
00:04:37,884 --> 00:04:41,174
one sentence. So let's move quickly on

44
00:04:41,212 --> 00:04:44,966
to stride analysis. Now, stride analysis is

45
00:04:44,988 --> 00:04:48,394
the threat model I tend to use, and I use it for a few

46
00:04:48,432 --> 00:04:51,866
reasons, but let's quickly dive into it and see what

47
00:04:51,888 --> 00:04:55,222
it is. So stride

48
00:04:55,286 --> 00:04:58,730
itself is a threat model

49
00:04:58,800 --> 00:05:01,518
put out by Microsoft in April 1,

50
00:05:01,684 --> 00:05:04,240
1999, and it's still in use today.

51
00:05:05,490 --> 00:05:09,134
And I understand that's quite weird. I don't think any of us

52
00:05:09,252 --> 00:05:12,670
are thinking, well, I want something

53
00:05:12,740 --> 00:05:16,002
secure, I'm going to use something from 1999

54
00:05:16,136 --> 00:05:19,986
that's just extremely unusual. And the

55
00:05:20,008 --> 00:05:23,458
fact it was released on April the first also makes me

56
00:05:23,544 --> 00:05:27,286
kind of worry about it. But in my opinion, it's a

57
00:05:27,308 --> 00:05:31,222
great model that breaks threats down into these six

58
00:05:31,276 --> 00:05:34,514
distinct categories, and it's got a good mnemonic,

59
00:05:34,562 --> 00:05:38,134
so I like to use it. There are other interesting models you can use that

60
00:05:38,172 --> 00:05:42,346
all have really good acronyms. So you can use pasta if

61
00:05:42,368 --> 00:05:45,478
you like. You can do a vast analysis.

62
00:05:45,574 --> 00:05:48,634
You can have dread. That's always a good one,

63
00:05:48,672 --> 00:05:50,090
but I prefer stride.

64
00:05:51,810 --> 00:05:55,246
We're going to quickly go through the six parts of the

65
00:05:55,268 --> 00:05:59,822
stride monotony. So we have authentication, which is

66
00:05:59,956 --> 00:06:03,246
spoofing. So an attack against the property of authentication,

67
00:06:03,358 --> 00:06:07,182
impersonating a user to log in, or generating

68
00:06:07,246 --> 00:06:10,926
false websites, like anything to trick

69
00:06:10,958 --> 00:06:14,420
people's traffic into somewhere it shouldn't be.

70
00:06:15,850 --> 00:06:19,698
We have tampering, which is an attack against integrity.

71
00:06:19,794 --> 00:06:23,880
So modifications made to a system where they shouldn't have been possible.

72
00:06:25,690 --> 00:06:29,594
Repudiation, which is the only word on here, which is

73
00:06:29,632 --> 00:06:33,002
slightly unusual. So what repudiation means is

74
00:06:33,056 --> 00:06:36,794
the ability to deny something has

75
00:06:36,832 --> 00:06:40,654
happened. So plausible deniability is a great way

76
00:06:40,692 --> 00:06:44,014
to describe it. So if you keep

77
00:06:44,052 --> 00:06:47,514
your logs locally on the server, if they are not properly

78
00:06:47,562 --> 00:06:50,910
protected, then you are vulnerable to repudiation.

79
00:06:53,430 --> 00:06:56,850
Information disclosure is another big one.

80
00:06:56,920 --> 00:07:00,498
Attack against confidentiality. So, access to source code,

81
00:07:00,664 --> 00:07:04,178
publishing of customer or client information,

82
00:07:04,344 --> 00:07:08,134
or any confidential information, really, most companies have

83
00:07:08,172 --> 00:07:11,222
confidentiality levels that are set, and anything

84
00:07:11,276 --> 00:07:15,160
that's not public, if that reaches, or anything that's not

85
00:07:16,010 --> 00:07:19,494
categorized as public, if that reaches someone, it shouldn't,

86
00:07:19,622 --> 00:07:21,610
that's information disclosure.

87
00:07:22,590 --> 00:07:25,820
Denial of service is of course a standard one.

88
00:07:26,590 --> 00:07:30,314
Degrading or denying access to the service through

89
00:07:30,352 --> 00:07:33,310
any means, even cutting cables, when you think about it,

90
00:07:33,380 --> 00:07:35,920
can be considered a denial of service attack.

91
00:07:37,410 --> 00:07:41,498
And then of course the elevation of privilege or escalation of privilege,

92
00:07:41,674 --> 00:07:44,690
whichever word you prefer, an attack against authorization.

93
00:07:45,510 --> 00:07:49,042
So allowing the execution of remote code while

94
00:07:49,096 --> 00:07:52,354
unauthorized, or for example,

95
00:07:52,472 --> 00:07:55,846
elevating limited user to admin level, or even picking the

96
00:07:55,868 --> 00:07:59,960
lock, because if you're applying this methodology to,

97
00:08:01,130 --> 00:08:03,970
for example, a physical presence,

98
00:08:04,050 --> 00:08:06,790
then that's another escalation of privilege.

99
00:08:07,630 --> 00:08:11,686
So before we go on, because threat

100
00:08:11,718 --> 00:08:15,946
modeling can get quite complicated, I decided we would start with

101
00:08:16,128 --> 00:08:20,566
stride, as it is originally intended. So we're

102
00:08:20,598 --> 00:08:23,440
going to head back to 1999,

103
00:08:23,970 --> 00:08:27,422
and we're going to start by making a data flow diagram for

104
00:08:27,476 --> 00:08:30,880
a system as it should have been in 1999.

105
00:08:31,650 --> 00:08:34,420
Now, I have a minor caveat here,

106
00:08:34,870 --> 00:08:38,274
in that I was twelve years

107
00:08:38,312 --> 00:08:42,018
old in 1999, so I was not

108
00:08:42,104 --> 00:08:45,922
an IT professional at the time. I was a slightly disobedient

109
00:08:45,986 --> 00:08:49,506
secondary school student, I spent most of my time skipping

110
00:08:49,538 --> 00:08:53,766
homework and playing video games. So this entire

111
00:08:53,868 --> 00:08:57,234
section is based on wild speculation

112
00:08:57,282 --> 00:09:01,450
of what I assume the IT business

113
00:09:01,520 --> 00:09:05,820
was like in the late 90s, which is based on my personal

114
00:09:06,190 --> 00:09:10,698
cultural touch point, which is of course the american sitcom,

115
00:09:10,794 --> 00:09:14,190
more specifically friends and Frasier.

116
00:09:15,650 --> 00:09:19,310
So we're going to assume that every data

117
00:09:19,380 --> 00:09:22,958
handover that happened in the time was done in a 90s style

118
00:09:22,974 --> 00:09:26,558
coffee shop. So here is our data flow diagram

119
00:09:26,734 --> 00:09:30,718
circa 99. We have our developer Steve,

120
00:09:30,814 --> 00:09:34,370
who keeps his source code on his laptop.

121
00:09:34,890 --> 00:09:39,014
He gives the data to Dave, who works in operations in

122
00:09:39,052 --> 00:09:41,480
the 90s style coffee shop.

123
00:09:42,650 --> 00:09:47,186
The data transitions through operations

124
00:09:47,298 --> 00:09:50,798
to installation via Dave's

125
00:09:50,834 --> 00:09:54,154
car. And I seem to recall that floppy disks were this

126
00:09:54,192 --> 00:09:57,994
big. You can see this is Dave driving his floppy disk to

127
00:09:58,032 --> 00:10:00,560
the server room or the data center.

128
00:10:01,570 --> 00:10:05,310
The data is installed through the

129
00:10:05,460 --> 00:10:08,906
three and a half inch floppy drive. Maybe I got inches

130
00:10:08,938 --> 00:10:12,080
and feet confused with this illustration. I'm not sure.

131
00:10:12,390 --> 00:10:14,530
And then the communication interface,

132
00:10:14,870 --> 00:10:18,286
HTTP port 80, and our end user

133
00:10:18,398 --> 00:10:22,162
Barry can watch highly compressed cat videos over

134
00:10:22,216 --> 00:10:25,654
dial up. This is how I'm going to assume that

135
00:10:25,692 --> 00:10:29,346
these services existed in the 90s for the sake

136
00:10:29,378 --> 00:10:33,190
of the next section, which is trust boundaries.

137
00:10:33,610 --> 00:10:37,570
Now let's pause here a minute and discuss

138
00:10:37,660 --> 00:10:40,300
what a trust boundary actually is.

139
00:10:42,110 --> 00:10:45,914
Now, there's a wiki definition here of the trust

140
00:10:45,952 --> 00:10:49,450
boundary being a term used in computer science, blah, blah, blah.

141
00:10:50,050 --> 00:10:53,200
I want to break it down to a very simple term because

142
00:10:54,530 --> 00:10:57,886
trust is where ownership of the data changes.

143
00:10:58,068 --> 00:11:00,480
Where control of the data changes.

144
00:11:01,970 --> 00:11:05,314
When you are installing and then you

145
00:11:05,432 --> 00:11:08,978
have the installation interface with the three and a

146
00:11:08,984 --> 00:11:12,670
half inch floppy disk, you have the user interface

147
00:11:12,750 --> 00:11:16,134
over HTTP. As you install, they change.

148
00:11:16,332 --> 00:11:18,600
So the ownership of that changes.

149
00:11:19,050 --> 00:11:22,838
And in most practical places it means as data

150
00:11:22,924 --> 00:11:26,694
transits, it doesn't always mean that because you can go

151
00:11:26,732 --> 00:11:29,930
very deep in these threat models and for example,

152
00:11:30,080 --> 00:11:34,170
take a look at some kind of single

153
00:11:34,240 --> 00:11:37,130
system and go to a deep process level.

154
00:11:37,280 --> 00:11:40,102
And at that point the data is not really transiting.

155
00:11:40,246 --> 00:11:43,678
But I would go in this case with number one here.

156
00:11:43,844 --> 00:11:46,910
In most practical places it means data transit.

157
00:11:48,530 --> 00:11:52,318
So when you do the threat boundaries, you end up with something like this.

158
00:11:52,484 --> 00:11:55,714
There is a threat boundary as the data is handed over. In our

159
00:11:55,752 --> 00:11:59,346
90s coffee shop, there is another data

160
00:11:59,528 --> 00:12:03,730
transition as it switches over to the installation interface.

161
00:12:04,390 --> 00:12:07,842
And a third transition boundary,

162
00:12:07,906 --> 00:12:11,666
this threat boundary when the end user is connecting

163
00:12:11,698 --> 00:12:15,538
to the interface. So this is the context of threat

164
00:12:15,554 --> 00:12:19,514
modeling. This is what we care about. These are the places where data

165
00:12:19,632 --> 00:12:23,254
shifts and the controller,

166
00:12:23,302 --> 00:12:27,018
the owner of data, changes. And then

167
00:12:27,104 --> 00:12:30,574
the next or final step in this process

168
00:12:30,692 --> 00:12:34,606
is to apply the stride analysis. So as

169
00:12:34,628 --> 00:12:38,270
we've gone through spoofing, tampering, repudiation,

170
00:12:39,650 --> 00:12:43,150
information disclosure, escalation of privilege,

171
00:12:46,550 --> 00:12:49,620
these six things are the stride threat model,

172
00:12:53,430 --> 00:12:56,130
and all we do here is apply stride.

173
00:12:57,050 --> 00:13:01,090
So for each threat boundary, we should look at the inputs and outputs

174
00:13:01,250 --> 00:13:04,646
we should consider and note potential threats based on the

175
00:13:04,668 --> 00:13:08,550
stride and then plan mitigations for the device threats.

176
00:13:09,370 --> 00:13:14,650
So I'm not going to do the whole 90s data

177
00:13:14,720 --> 00:13:17,578
flow because I think that would take a bit too much time and I want

178
00:13:17,584 --> 00:13:21,374
to get into actual modern systems. So I've just made up a

179
00:13:21,412 --> 00:13:25,406
short example. So, for example, in the

180
00:13:25,588 --> 00:13:29,790
handover boundary here of data handover spoofing,

181
00:13:30,210 --> 00:13:34,046
is it possible to establish the identity of either the developer

182
00:13:34,078 --> 00:13:37,038
or operations? So maybe with id cards,

183
00:13:37,134 --> 00:13:40,658
but failing that, it would be possible to pretend to be one of

184
00:13:40,664 --> 00:13:45,174
them. Information disclosure. They're in a public

185
00:13:45,212 --> 00:13:48,678
coffee shop, so it's very easy to overhear what's happening,

186
00:13:48,764 --> 00:13:51,174
overhear the installation information,

187
00:13:51,372 --> 00:13:55,442
overhear basically what's going to happen over the installation

188
00:13:55,506 --> 00:13:59,260
process. For tampering, we can look at,

189
00:13:59,950 --> 00:14:03,334
for example, the data transition of ops going to the installation

190
00:14:03,382 --> 00:14:07,482
there. Does he go straight to the data

191
00:14:07,536 --> 00:14:11,354
center and install the software on the server, or does

192
00:14:11,392 --> 00:14:15,038
he leave it in his car overnight? Can it be tampered with? Is it possible

193
00:14:15,124 --> 00:14:18,894
to distract him? Maybe the spy skills walk by because it's the

194
00:14:18,932 --> 00:14:22,446
late ninety s and he's distracted long enough to have

195
00:14:22,468 --> 00:14:23,810
a disc switched.

196
00:14:24,790 --> 00:14:28,366
Repudiation is the threat that hasn't changed.

197
00:14:28,398 --> 00:14:32,574
In my opinion, it's the only one that still relies on logging

198
00:14:32,622 --> 00:14:36,562
and everything. And denial of service. Say he loses

199
00:14:36,626 --> 00:14:40,502
disk two of eight and the installation corrupts, the service is down,

200
00:14:40,556 --> 00:14:43,830
and our end user, Barry, cannot watch

201
00:14:43,900 --> 00:14:45,640
his compressed cat videos.

202
00:14:46,810 --> 00:14:50,202
So here is how I

203
00:14:50,256 --> 00:14:55,798
believe the threat model was supposed to be applied.

204
00:14:55,974 --> 00:14:59,270
It probably was a little more complex,

205
00:14:59,350 --> 00:15:03,114
but, you know, I think we should hop

206
00:15:03,162 --> 00:15:06,878
straight to modern times. So we have

207
00:15:06,964 --> 00:15:09,998
a data flow diagram in 2020,

208
00:15:10,084 --> 00:15:14,162
now, 23 years later,

209
00:15:14,296 --> 00:15:18,402
here we are. It's kind of impossible to sum up how much security has

210
00:15:18,456 --> 00:15:22,446
changed over the last two decades. I defy

211
00:15:22,478 --> 00:15:25,638
anyone to fit that amount of changes

212
00:15:25,724 --> 00:15:29,686
into a 25 minutes talk.

213
00:15:29,868 --> 00:15:33,266
So if we say what has changed,

214
00:15:33,298 --> 00:15:37,010
I think we can just say everything except

215
00:15:37,170 --> 00:15:40,790
the stride threat model because it hasn't been updated

216
00:15:40,870 --> 00:15:44,746
since the release in 99. So when we ask how

217
00:15:44,768 --> 00:15:48,502
does it change the application of it, it simply doesn't.

218
00:15:48,646 --> 00:15:51,706
The only thing that happens is it gets more complicated.

219
00:15:51,898 --> 00:15:55,946
The transition to modern day is not as jarring

220
00:15:55,978 --> 00:15:59,680
as you might initially think when you see

221
00:16:00,690 --> 00:16:03,890
this. Now this is a

222
00:16:03,960 --> 00:16:07,746
very simple diagram of

223
00:16:07,848 --> 00:16:11,154
a more modern workflow. Now there are

224
00:16:11,192 --> 00:16:14,702
some parts missing here, but this is just a very simplified throw

225
00:16:14,846 --> 00:16:18,354
flow. Don't let the scale fool

226
00:16:18,402 --> 00:16:21,670
you. Everything we're going to do is exactly the same.

227
00:16:21,820 --> 00:16:24,982
So we have the developer Steve Jr. Steve's son

228
00:16:25,036 --> 00:16:28,406
has joined the game now and he's developing

229
00:16:28,438 --> 00:16:32,070
on his laptop. He pushes his code to GitHub,

230
00:16:32,230 --> 00:16:35,020
GitLab, BitBucket. Wherever you store code,

231
00:16:35,390 --> 00:16:38,934
we have a pipeline system. Jenkins collects source

232
00:16:38,982 --> 00:16:42,526
code, tests with Sonarcube, secures in the

233
00:16:42,548 --> 00:16:45,370
pipelines with a couple of open source tools,

234
00:16:45,530 --> 00:16:49,166
builds a docker image, stores that docker image in

235
00:16:49,188 --> 00:16:52,218
artifactory has some infrastructure as code,

236
00:16:52,404 --> 00:16:56,418
an installation interface, maybe a server in the middle here which is

237
00:16:56,504 --> 00:16:59,694
pushing out these updates through SSH

238
00:16:59,822 --> 00:17:03,758
using something like ansible containers

239
00:17:03,854 --> 00:17:07,682
which contain the workloads and a communication interface.

240
00:17:07,746 --> 00:17:10,870
And now our end user, 23 years later,

241
00:17:10,940 --> 00:17:14,674
Barry is able to enjoy extremely

242
00:17:14,722 --> 00:17:18,054
hd cat videos over 100 megabits connection.

243
00:17:18,102 --> 00:17:20,140
So he's extremely happy with this.

244
00:17:22,510 --> 00:17:26,618
The trust boundaries in 2022

245
00:17:26,784 --> 00:17:30,330
are exactly the same. Now this slide, if you're wondering, is actually

246
00:17:30,400 --> 00:17:34,186
exactly the same slide as I showed in the trust boundaries

247
00:17:34,218 --> 00:17:37,790
for 99. The reason for this is this is going out as a video,

248
00:17:37,860 --> 00:17:41,454
so I want people to have the references nearby in case they

249
00:17:41,492 --> 00:17:46,482
are going back and forward. So here is the trust

250
00:17:46,536 --> 00:17:50,434
boundary again. In most practical places it means data

251
00:17:50,552 --> 00:17:54,254
transit. So as the data moves and the owner,

252
00:17:54,302 --> 00:17:57,622
the controller of that data, changes, that's what we care

253
00:17:57,676 --> 00:18:01,094
about. And then applying this methodology, we end

254
00:18:01,132 --> 00:18:04,280
up with something that looks a lot like this.

255
00:18:06,250 --> 00:18:12,730
And I understand this might look a little bit complex.

256
00:18:13,390 --> 00:18:16,298
Initially, we have things we never had before.

257
00:18:16,464 --> 00:18:19,782
We had entirely internal threat boundaries.

258
00:18:19,846 --> 00:18:23,738
Here we have a nested trust boundary.

259
00:18:23,914 --> 00:18:27,034
We have systems within systems,

260
00:18:27,082 --> 00:18:30,270
built upon systems. So it's not just one person

261
00:18:30,340 --> 00:18:34,430
handing over code and another person installing it. This is automated

262
00:18:34,510 --> 00:18:38,034
infrastructure, this is DevOps, and this is

263
00:18:38,072 --> 00:18:41,394
the trust boundary as we would apply it to this

264
00:18:41,432 --> 00:18:44,910
system. So getting to

265
00:18:45,000 --> 00:18:48,834
the meat of this conversation is applying

266
00:18:48,882 --> 00:18:54,038
the stride analysis circuit 2022

267
00:18:54,204 --> 00:18:57,866
to our system. And we're going to go through all of

268
00:18:57,888 --> 00:19:01,850
these boundaries. So we'll get started with the source code repository.

269
00:19:03,310 --> 00:19:07,158
The source code repository, obviously potential

270
00:19:07,254 --> 00:19:10,654
IPR. That's what we're worried about here. This is where

271
00:19:10,772 --> 00:19:14,400
your dna is stored. This is where your

272
00:19:14,850 --> 00:19:19,354
company's core code is collected.

273
00:19:19,482 --> 00:19:23,090
So spoofing any kind of

274
00:19:23,160 --> 00:19:26,340
authentication against it might cause problems.

275
00:19:26,870 --> 00:19:30,610
Tampering could lead to the modification of source,

276
00:19:31,270 --> 00:19:34,946
which is problematic. If that source then makes it

277
00:19:34,968 --> 00:19:39,134
through to the end. Product repudiation

278
00:19:39,182 --> 00:19:42,230
threats act as logs can be deleted,

279
00:19:43,290 --> 00:19:47,882
which would make changes untrackable. So you have no

280
00:19:47,936 --> 00:19:51,898
chain of authority, you have no way to track

281
00:19:51,984 --> 00:19:55,610
what is happening. Information disclosure,

282
00:19:56,350 --> 00:20:00,460
IPR leakage, mostly if someone leaks the source code,

283
00:20:00,850 --> 00:20:04,666
loss of reputation, loss of innovation,

284
00:20:04,858 --> 00:20:09,102
and then denial of service. Service loss prevents builds being

285
00:20:09,156 --> 00:20:12,946
committed or elevation of privileges. So an

286
00:20:12,968 --> 00:20:16,770
unauthorized admin access leads to code based modification.

287
00:20:18,070 --> 00:20:21,678
The thing we don't talk about at any point here is mitigations,

288
00:20:21,854 --> 00:20:25,366
because we should. Let me clear

289
00:20:25,388 --> 00:20:29,990
that up. So threat modeling process should always have mitigations.

290
00:20:30,490 --> 00:20:34,182
But these are examples. These are things that are

291
00:20:34,236 --> 00:20:39,126
mostly, for example, handled by the source

292
00:20:39,158 --> 00:20:43,430
code repository. So spoofing already covered,

293
00:20:43,510 --> 00:20:47,114
tampering that is handled by principle of least

294
00:20:47,152 --> 00:20:51,230
privilege, repudiation, non changeable logs,

295
00:20:51,570 --> 00:20:55,534
all these things are very clear. So in

296
00:20:55,572 --> 00:20:59,802
most situations, in this example, the mitigations

297
00:20:59,866 --> 00:21:02,914
are quite obvious. But this shouldn't be all you have.

298
00:21:02,952 --> 00:21:05,860
You should go as deep as you can.

299
00:21:06,950 --> 00:21:10,306
Basically, any concern you have for this

300
00:21:10,408 --> 00:21:13,442
particular section should be brought up in here.

301
00:21:13,576 --> 00:21:17,030
And there's no rules saying that it should just be one point.

302
00:21:17,180 --> 00:21:20,486
If you have four or five concerns for spoofing or

303
00:21:20,508 --> 00:21:23,638
tampering, write four or five concerns here.

304
00:21:23,804 --> 00:21:25,640
This is absolutely fine.

305
00:21:27,710 --> 00:21:31,226
Next, pipelines. So this was

306
00:21:31,248 --> 00:21:35,014
our pipeline boundary. This contains the pipeline

307
00:21:35,062 --> 00:21:38,794
configuration. Read and write access

308
00:21:38,912 --> 00:21:42,010
to, well, read from git to write to binary storage.

309
00:21:42,090 --> 00:21:45,486
So theoretically there's a lot of damage that could

310
00:21:45,508 --> 00:21:47,440
be done from this point because,

311
00:21:48,210 --> 00:21:51,406
well, I mean, one of

312
00:21:51,428 --> 00:21:55,330
my colleagues said something that hadn't really struck me the other day,

313
00:21:55,400 --> 00:21:58,558
that Jenkins,

314
00:21:58,654 --> 00:22:01,794
or pipelines in general are just

315
00:22:01,832 --> 00:22:04,130
formalized remote code execution.

316
00:22:04,970 --> 00:22:08,950
And he's absolutely right.

317
00:22:09,100 --> 00:22:12,374
So the ability to edit the code after

318
00:22:12,412 --> 00:22:16,082
it's been read from git, the ability to write additional binaries

319
00:22:16,146 --> 00:22:19,514
to the storage. There's a lot of dangerous things that

320
00:22:19,552 --> 00:22:22,874
an attacker could do if they got hold of the

321
00:22:22,912 --> 00:22:26,522
pipelines. So spoofing code

322
00:22:26,576 --> 00:22:31,370
base could lead to injected code tampering,

323
00:22:31,450 --> 00:22:35,034
modification of pipelines. I mean, if you modified

324
00:22:35,082 --> 00:22:37,760
the pipelines, you can build basically anything you want.

325
00:22:38,210 --> 00:22:41,886
Repudiation is the one threat that stays essentially the

326
00:22:41,908 --> 00:22:45,982
same throughout the entire time. It's just log everything centrally,

327
00:22:46,046 --> 00:22:49,790
make sure it's not editable. That's the mitigation

328
00:22:49,870 --> 00:22:52,734
for all repudiation threats.

329
00:22:52,782 --> 00:22:56,406
If someone can tell me a repudiation threat that is

330
00:22:56,428 --> 00:22:59,446
not mitigated by central logging, I would love to hear it,

331
00:22:59,468 --> 00:23:02,440
because I've so far not encountered one.

332
00:23:03,370 --> 00:23:06,662
Information disclosure, same risks as the source code.

333
00:23:06,716 --> 00:23:10,314
It's IPR loss, so losing your

334
00:23:10,432 --> 00:23:14,042
intellectual property rights. Denial of service

335
00:23:14,176 --> 00:23:17,654
in acute situations could prevent the deployment of emergency

336
00:23:17,702 --> 00:23:21,274
patches and elevation

337
00:23:21,322 --> 00:23:24,730
of privilege. Alter build pipelines, disable or skip tests

338
00:23:24,810 --> 00:23:26,080
this sort of thing,

339
00:23:27,970 --> 00:23:31,662
static code analysis tool, quickly go through this one

340
00:23:31,716 --> 00:23:35,458
because in my opinion it's a, let's say less than

341
00:23:35,544 --> 00:23:38,926
critical area. It only returns

342
00:23:38,958 --> 00:23:42,242
pass or fail values. So the best you can do

343
00:23:42,296 --> 00:23:45,890
really is force the test to skip.

344
00:23:48,150 --> 00:23:52,630
Now there's an in pipeline security tool that I want to talk about. So like

345
00:23:52,700 --> 00:23:56,950
Aqua security, put this trivia tool out. It's one of my favorite tools for dealing

346
00:23:57,390 --> 00:24:00,700
with docker images as they're being built,

347
00:24:01,630 --> 00:24:05,066
but it opens the spoofing angle to an

348
00:24:05,088 --> 00:24:08,550
external attack. If you're able to spoof the domains

349
00:24:08,630 --> 00:24:11,854
or download the wrong tools, this would give you

350
00:24:11,892 --> 00:24:15,806
a lot of power inside the pipeline. Tampering obviously

351
00:24:15,908 --> 00:24:19,534
this is the build step, so any tampering could lead to

352
00:24:19,572 --> 00:24:23,518
corrupted software. Information disclosure

353
00:24:23,614 --> 00:24:26,466
it's again more about the IPR here.

354
00:24:26,648 --> 00:24:30,510
Denial of service to prevent security checks

355
00:24:30,670 --> 00:24:34,066
and then you basically force the idea of should we

356
00:24:34,088 --> 00:24:38,006
do the security check if we can't or should we

357
00:24:38,028 --> 00:24:41,350
skip it or should we pause the deployment?

358
00:24:43,530 --> 00:24:47,206
Storage is another big area, so this

359
00:24:47,228 --> 00:24:50,570
is where our binaries are stored for deployment.

360
00:24:51,390 --> 00:24:54,380
If we have a compromise here,

361
00:24:54,750 --> 00:25:00,410
then someone can upload their own binaries or

362
00:25:00,480 --> 00:25:04,074
maybe even download as reverse engineer them. So spoofing

363
00:25:04,122 --> 00:25:07,918
for credentials to allow for upload and download when we

364
00:25:07,924 --> 00:25:11,726
don't want them, tampering changes

365
00:25:11,828 --> 00:25:15,326
to the images which could then be distributed.

366
00:25:15,518 --> 00:25:18,846
Information disclosure here might be through reverse

367
00:25:18,878 --> 00:25:23,374
engineering the binaries or examining

368
00:25:23,422 --> 00:25:27,046
the images. Denial of service if

369
00:25:27,068 --> 00:25:31,126
this is our single source of truth for our deployments and

370
00:25:31,148 --> 00:25:36,006
we deny service here, then basically all our later

371
00:25:36,108 --> 00:25:38,410
containers are stuck.

372
00:25:39,870 --> 00:25:43,354
Might lead to some problems trying to update and

373
00:25:43,392 --> 00:25:45,370
then reload of altered images,

374
00:25:46,270 --> 00:25:50,290
which would be if you are able to elevate privileges

375
00:25:50,390 --> 00:25:53,774
and then upload the new image might

376
00:25:53,892 --> 00:25:55,280
pose a problem.

377
00:25:57,650 --> 00:26:01,386
Installation interface is kind of a very basic SSH,

378
00:26:01,498 --> 00:26:05,286
so standard sort of threatens

379
00:26:05,338 --> 00:26:08,270
here. So tampering of the SSH, config,

380
00:26:08,430 --> 00:26:12,260
spoofing the credentials, leaking server information,

381
00:26:13,190 --> 00:26:16,854
basic stuff. Also killing the service prevents any kind of

382
00:26:16,892 --> 00:26:20,806
access the

383
00:26:20,828 --> 00:26:24,726
container boundary where obviously if

384
00:26:24,748 --> 00:26:27,960
we install the wrong containers, anything can happen.

385
00:26:28,430 --> 00:26:31,450
Could include like a coin miner,

386
00:26:32,910 --> 00:26:37,290
basically any kind of malicious container we can think of tampering

387
00:26:38,190 --> 00:26:40,650
if we're able to download the wrong images,

388
00:26:41,890 --> 00:26:45,594
information disclosure is again leaked. IPR denial

389
00:26:45,642 --> 00:26:49,326
of service this is the service. So denial of

390
00:26:49,348 --> 00:26:52,658
access to the containers and the services down an elevation of

391
00:26:52,664 --> 00:26:56,162
privileges might look like container escape. Something along

392
00:26:56,216 --> 00:26:59,874
those lines. And then finally we have the

393
00:26:59,912 --> 00:27:03,826
communications interface, which is accepting traffic from

394
00:27:03,848 --> 00:27:07,110
the whole Internet. So if there is a login,

395
00:27:07,930 --> 00:27:10,978
obviously spoofed logins to the web browser.

396
00:27:11,154 --> 00:27:14,626
Tampering could be tampered with, serve malware,

397
00:27:14,738 --> 00:27:17,270
any kind of illicit materials,

398
00:27:18,830 --> 00:27:22,314
information disclosure, for example, robots that should

399
00:27:22,352 --> 00:27:26,566
be removed. This kind of information could be private.

400
00:27:26,758 --> 00:27:28,890
And then login elevation.

401
00:27:29,630 --> 00:27:33,146
So this was the stride analysis.

402
00:27:33,258 --> 00:27:37,162
Now, this isn't supposed to be a kind of catch

403
00:27:37,226 --> 00:27:40,814
all for the stride. This is more me

404
00:27:40,852 --> 00:27:44,082
talking for half an hour to get you thinking

405
00:27:44,216 --> 00:27:48,110
about stride, because in your systems,

406
00:27:48,190 --> 00:27:51,250
you're going to have a much more realistic work case.

407
00:27:51,400 --> 00:27:55,638
You're going to have a better

408
00:27:55,724 --> 00:27:59,494
understanding of your data flow, you're going to have a much

409
00:27:59,532 --> 00:28:03,750
more in depth analysis. You may have noticed more trust

410
00:28:03,820 --> 00:28:07,366
boundaries. So the idea here is

411
00:28:07,388 --> 00:28:11,450
not to give, like, an in depth understanding

412
00:28:12,430 --> 00:28:16,406
of an exact model, but just to get you in the right headspace

413
00:28:16,438 --> 00:28:20,006
to be able to generate your own. And that

414
00:28:20,048 --> 00:28:23,614
being said, I like the stride model,

415
00:28:23,732 --> 00:28:26,720
but there is a problem.

416
00:28:27,570 --> 00:28:31,562
In my experience, it's extremely effective in certain situations,

417
00:28:31,626 --> 00:28:35,442
so in most

418
00:28:35,496 --> 00:28:39,134
cases it's great. But stride, in my opinion,

419
00:28:39,182 --> 00:28:43,010
has been built to look

420
00:28:43,080 --> 00:28:47,110
externally. Stride, in my opinion,

421
00:28:47,530 --> 00:28:49,910
only seeks external threats.

422
00:28:50,650 --> 00:28:54,194
And it makes the assumption that every attacker is an external

423
00:28:54,242 --> 00:28:57,754
attacker, that every threat is malicious instead

424
00:28:57,792 --> 00:29:00,938
of clumsy, let's say.

425
00:29:01,024 --> 00:29:04,700
But history,

426
00:29:05,150 --> 00:29:09,062
which in 1999 they didn't have the benefit of knowing,

427
00:29:09,206 --> 00:29:12,554
has demonstrated that not every attacker

428
00:29:12,602 --> 00:29:16,670
is external or not every problem is malicious.

429
00:29:17,730 --> 00:29:21,760
So we have these two areas for

430
00:29:23,350 --> 00:29:26,574
mishandling and malice, which I don't

431
00:29:26,702 --> 00:29:30,562
have a good answer of where they should be

432
00:29:30,616 --> 00:29:33,902
in the stride threat model. Like mishandling,

433
00:29:34,046 --> 00:29:37,874
misconfiguration, mismanagement. Those are generally

434
00:29:37,922 --> 00:29:41,350
what lead to the

435
00:29:41,420 --> 00:29:43,880
rest of the stride model, really.

436
00:29:44,330 --> 00:29:49,446
So most of those issues are problems with configuration

437
00:29:49,558 --> 00:29:53,766
or some kind of mishandling, and then internal

438
00:29:53,798 --> 00:29:57,014
malice. If you have an employee

439
00:29:57,062 --> 00:30:00,650
with an axe to grind, then it's the principle of least privilege

440
00:30:01,570 --> 00:30:05,520
that should be keeping them out. But they're going to have a much easier time

441
00:30:06,370 --> 00:30:09,434
with threats that you have perhaps not considered,

442
00:30:09,482 --> 00:30:13,146
because stride looks externally.

443
00:30:13,258 --> 00:30:16,290
So if you are doing a stride analysis,

444
00:30:17,670 --> 00:30:21,426
I would keep these two in mind. I bring this up because every time

445
00:30:21,448 --> 00:30:24,754
I've done stride, I've had this question from

446
00:30:24,792 --> 00:30:28,714
someone. Someone has asked malice and mishandling

447
00:30:28,782 --> 00:30:32,214
questions. How do we deal with a misconfiguration? Where do we put

448
00:30:32,252 --> 00:30:36,562
that? And in truth, I don't have a good answer for you. My recommendation

449
00:30:36,626 --> 00:30:40,522
is, keep this in mind, because as I've written here,

450
00:30:40,656 --> 00:30:44,134
82% of attacks are caused by this human element.

451
00:30:44,262 --> 00:30:47,770
So the threat model is great.

452
00:30:47,840 --> 00:30:51,500
But make sure you aim the threat model internally, too.

453
00:30:53,250 --> 00:30:57,194
And then finally we have the steps of what's

454
00:30:57,242 --> 00:31:01,118
next. Now the obvious thing is

455
00:31:01,204 --> 00:31:04,974
do it all over again, because stride

456
00:31:05,022 --> 00:31:08,482
is an iterative process. So you do it,

457
00:31:08,536 --> 00:31:12,354
you validate it, you do it again. You schedule it every six

458
00:31:12,392 --> 00:31:16,290
months, sit down and ask if your service or platform or whatever

459
00:31:16,440 --> 00:31:19,880
has changed enough to warrant a revisit to the threat model.

460
00:31:20,970 --> 00:31:24,742
The idea is that this is not something you do once and then forget about

461
00:31:24,796 --> 00:31:28,140
it. You keep doing it. As your

462
00:31:28,670 --> 00:31:31,914
service evolves, you continue to

463
00:31:31,952 --> 00:31:35,210
visit this threat modeling, or if you

464
00:31:35,280 --> 00:31:38,090
prefer a reference to 90 sitcoms,

465
00:31:38,830 --> 00:31:42,030
join us again in the next season of threat modeling.

466
00:31:43,410 --> 00:31:47,134
Thank you. If you have any comments or questions,

467
00:31:47,252 --> 00:31:51,530
I will be lurking around conf 42 discord

468
00:31:51,610 --> 00:31:55,678
for a while. You can send me a message on LinkedIn. My information

469
00:31:55,764 --> 00:31:59,520
is here. My LinkedIn is LinkedIn.com

470
00:32:00,050 --> 00:32:03,840
greatbushybeard, so feel free to look me up.

471
00:32:04,610 --> 00:32:05,138
Bye.

