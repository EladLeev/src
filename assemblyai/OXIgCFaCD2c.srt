1
00:00:22,010 --> 00:00:25,682
Hello and welcome to today's session on building machine learning environments

2
00:00:25,746 --> 00:00:29,206
for regulatory customers. In today's session, we will be looking at

3
00:00:29,228 --> 00:00:33,090
the best practices of building machine learning environments on AWS

4
00:00:33,170 --> 00:00:36,818
for regulatory customers. These customers can be in banking,

5
00:00:36,914 --> 00:00:40,182
insurance, life sciences, healthcare, energy,

6
00:00:40,316 --> 00:00:44,246
etc. Regulated customers are using machine learning models

7
00:00:44,278 --> 00:00:48,026
in order to transform their businesses. There are different use cases which you

8
00:00:48,048 --> 00:00:52,746
may be already aware of, for example, fraud detection, market surveillance,

9
00:00:52,858 --> 00:00:56,814
trade execution and even pharmaceuticals. Machine learning has

10
00:00:56,852 --> 00:01:00,366
the ability to learn from your business data

11
00:01:00,548 --> 00:01:03,786
and create these predictions which can be used

12
00:01:03,828 --> 00:01:06,818
for improving your processes and your business.

13
00:01:06,984 --> 00:01:11,006
But we should be very careful in terms of how these models

14
00:01:11,038 --> 00:01:15,106
are being deployed, what kind of security guardrails are being applied and

15
00:01:15,128 --> 00:01:18,534
what are the regulatory requirements whenever you are running such

16
00:01:18,572 --> 00:01:22,242
models, and finally, to ensure that they are secure.

17
00:01:22,386 --> 00:01:25,458
With that being said, let's get started with today's session.

18
00:01:25,634 --> 00:01:29,580
So machine learning went from being this aspirational technology

19
00:01:30,510 --> 00:01:33,754
to a mainstream technology extremely fast. For a very

20
00:01:33,792 --> 00:01:38,026
long time, the technology was limited to these few technology companies

21
00:01:38,128 --> 00:01:41,846
and hardcore academic researchers because there was simply

22
00:01:41,878 --> 00:01:45,406
no access for machine learning toolkit to a normal developer like

23
00:01:45,428 --> 00:01:49,466
you and me. But things have begun to change. When cloud computing entered

24
00:01:49,498 --> 00:01:53,250
mainstream, the compute power and the data became more

25
00:01:53,320 --> 00:01:57,186
available. And quite literally, machine learning is now making an

26
00:01:57,208 --> 00:02:00,414
impact across every industry, be it fashion,

27
00:02:00,542 --> 00:02:03,694
retail, real estate, healthcare,

28
00:02:03,822 --> 00:02:07,286
there are many more industries. It's moving from being

29
00:02:07,388 --> 00:02:11,414
on the periphery of the technology ecosystem to

30
00:02:11,452 --> 00:02:14,790
now being a core part of every business and industry.

31
00:02:15,130 --> 00:02:18,922
Here at AWS, we have been seeing a tipping point where

32
00:02:18,976 --> 00:02:22,234
AI and ML in the enterprise is addressing the

33
00:02:22,272 --> 00:02:26,538
use cases that create measurable results. The customer

34
00:02:26,624 --> 00:02:30,602
experience is being transformed via capabilities such as conversational

35
00:02:30,666 --> 00:02:34,554
user interfaces, smart biometric authentication,

36
00:02:34,682 --> 00:02:38,190
personalization and even recommendation. The business

37
00:02:38,260 --> 00:02:42,026
operations are also being improved. For example, in retail,

38
00:02:42,138 --> 00:02:45,806
AI and ML was able to reduce the error rates

39
00:02:45,838 --> 00:02:49,086
by 30% to 50%. Automation is making supply

40
00:02:49,118 --> 00:02:52,386
chain management more efficient. We can kind

41
00:02:52,408 --> 00:02:55,826
of conclude here that AI and ML is ultimately helping

42
00:02:55,858 --> 00:02:58,790
the companies make better and faster decisions.

43
00:02:59,290 --> 00:03:02,886
Machine learning is by far the most disruptive technology in

44
00:03:02,908 --> 00:03:06,550
the recent years, and today, more than 100,000

45
00:03:06,620 --> 00:03:11,594
customers use AWS for

46
00:03:11,632 --> 00:03:15,258
running machine learning workloads and for creating more personalized customer

47
00:03:15,344 --> 00:03:19,126
experience, or for even developing personalized pharmaceuticals,

48
00:03:19,158 --> 00:03:22,394
for that matter. Now, let's look at the AWS

49
00:03:22,442 --> 00:03:26,160
ML stack. I'll be talking through the different

50
00:03:27,250 --> 00:03:30,638
services which are being offered by AWS. As part of

51
00:03:30,644 --> 00:03:34,000
the ML stack, we are innovating on behalf of our customers

52
00:03:34,310 --> 00:03:37,902
to deliver the broadest and deepest set of machine learning capabilities

53
00:03:37,966 --> 00:03:41,998
for the builders at each layer of the stack, we are investing

54
00:03:42,014 --> 00:03:45,906
in removing the undifferentiated heavy lifting so that your teams

55
00:03:45,938 --> 00:03:49,606
can move faster. These services are applicable across a

56
00:03:49,628 --> 00:03:53,302
broad spectrum of companies, and we have also heard from customers

57
00:03:53,436 --> 00:03:56,920
that they want specific solutions that are purpose built.

58
00:03:57,390 --> 00:04:01,334
So let's go layer by layer here. The first layer

59
00:04:01,382 --> 00:04:05,878
are the AI services, which would be services like Lex

60
00:04:06,054 --> 00:04:09,194
speed, services like poly and transcribe code,

61
00:04:09,232 --> 00:04:12,890
and DevOps services like code Guru and DevOps Guru. These services

62
00:04:12,960 --> 00:04:16,746
are essentially the pre trained models and they provide ready made intelligence

63
00:04:16,778 --> 00:04:20,858
to your applications and workflows. It helps you do things like personalizing

64
00:04:20,874 --> 00:04:24,926
the customer experience, forecasting the business metrics,

65
00:04:25,038 --> 00:04:29,246
translating the conversations, extracting meaning or extracting meaning

66
00:04:29,278 --> 00:04:32,622
from different documents. Essentially,

67
00:04:32,686 --> 00:04:36,742
the AI services is to make machine learning more available to

68
00:04:36,796 --> 00:04:40,146
developers who are not the core machine

69
00:04:40,178 --> 00:04:43,814
learning developers. These are your developers who would want to just

70
00:04:43,852 --> 00:04:47,000
invoke an API and get some outcome out of that.

71
00:04:47,550 --> 00:04:50,826
With machine learning stack number two, that is

72
00:04:50,848 --> 00:04:54,038
the middle layer you have the Amazon Sage maker.

73
00:04:54,214 --> 00:04:57,814
With Amazon Sagemaker, it gives you the ability

74
00:04:57,942 --> 00:05:01,126
to build, train and deploy machine learning models,

75
00:05:01,238 --> 00:05:04,926
and it provided every developer and data scientist the ability to do that.

76
00:05:05,028 --> 00:05:09,386
It removes the complexity from each and every step of the machine learning workflow

77
00:05:09,418 --> 00:05:13,138
so you can easily deploy your models. Towards the end of

78
00:05:13,144 --> 00:05:16,866
the session, we will see a code example on how a model can be

79
00:05:16,888 --> 00:05:20,402
deployed by using Sagemaker Studio. The last

80
00:05:20,456 --> 00:05:23,486
layer is machine learning frameworks

81
00:05:23,518 --> 00:05:27,234
and infrastructure. This is basically tensorflow Pytorch,

82
00:05:27,282 --> 00:05:30,726
and this is basically for folks who are

83
00:05:30,828 --> 00:05:34,374
experts in machine learning and they would want to develop their own

84
00:05:34,412 --> 00:05:38,670
framework of their own choosing by using the deep learning AmIs

85
00:05:38,770 --> 00:05:42,806
and they can fully configure this solution. Obviously in today's session

86
00:05:42,838 --> 00:05:46,666
I wouldn't be going through each and every layer of

87
00:05:46,688 --> 00:05:51,310
the stack, but rather I'll be focusing upon Amazon Sagemaker.

88
00:05:52,210 --> 00:05:55,626
So Amazon Sagemaker has been built to make machine

89
00:05:55,658 --> 00:05:59,226
learning more accessible. And as I mentioned before, it helps

90
00:05:59,258 --> 00:06:02,734
you build, train and deploy machine learning models quickly

91
00:06:02,852 --> 00:06:06,500
and at a lower cost by provided the tools required for it.

92
00:06:06,870 --> 00:06:10,194
In fact, we have launched 50 plus capabilities of

93
00:06:10,232 --> 00:06:13,714
machine learning in Amazon Sagemaker in past year

94
00:06:13,752 --> 00:06:17,186
alone. And finally with SageMaker Studio, it brings

95
00:06:17,218 --> 00:06:19,670
it all together on a single pane of glass.

96
00:06:20,730 --> 00:06:24,486
So to summarize on SageMaker itself, it's the most complete end

97
00:06:24,508 --> 00:06:28,006
to end machine learning service. Sagemaker has a lot of

98
00:06:28,028 --> 00:06:31,334
features, and obviously we wouldn't be covering all the features

99
00:06:31,382 --> 00:06:35,782
today, but it can go through these four main pillars

100
00:06:35,846 --> 00:06:40,330
which are there. First off, it provided users with an integrated workbench.

101
00:06:40,770 --> 00:06:44,442
The users can launch Jupyter notebooks, they can launch Jupyter

102
00:06:44,506 --> 00:06:48,254
lab experiments, and they can instantly see these things on the

103
00:06:48,292 --> 00:06:51,466
Sagemaker studio. Sagemaker also provides

104
00:06:51,498 --> 00:06:54,398
complete experiment management, data preparation,

105
00:06:54,494 --> 00:06:57,986
pipeline automation and orchestration. So if you were

106
00:06:58,008 --> 00:07:01,346
to look at the overview of Sagemaker, it will

107
00:07:01,368 --> 00:07:04,786
help you prepare your data, it will help you

108
00:07:04,808 --> 00:07:08,294
building your model. You can train and tune your model and

109
00:07:08,332 --> 00:07:12,294
ultimately deploy and manage your model. These are the four categories that

110
00:07:12,332 --> 00:07:15,846
really addresses the needs that machine learning builders have

111
00:07:16,028 --> 00:07:19,290
when they are dealing with each stage of a model's lifecycle.

112
00:07:19,630 --> 00:07:23,946
With that being said, let's move on to see

113
00:07:24,128 --> 00:07:28,010
how to build the machine learning environment on AWS.

114
00:07:28,350 --> 00:07:32,062
So what did our customers ask? The customers asked for

115
00:07:32,116 --> 00:07:35,502
a solution which can enable the business data

116
00:07:35,556 --> 00:07:39,630
scientist to deliver a secure machine learning based solution

117
00:07:40,050 --> 00:07:43,934
and where they can train their models on

118
00:07:43,972 --> 00:07:47,362
highly sensitive data. And this data can be customer data,

119
00:07:47,416 --> 00:07:51,042
it can be company data, but essentially the security would be the priority number

120
00:07:51,096 --> 00:07:54,306
one here. And for this kind of an ask,

121
00:07:54,488 --> 00:07:59,090
let's come up with a tentative environment

122
00:07:59,170 --> 00:08:01,880
or constraints or requirements here.

123
00:08:02,490 --> 00:08:06,258
Obviously, there wouldn't be any Internet connectivity in the AWS accounts

124
00:08:06,274 --> 00:08:10,166
of such customers because you wouldn't want such accounts

125
00:08:10,198 --> 00:08:13,526
to be having direct Internet access. So most of these accounts

126
00:08:13,558 --> 00:08:17,286
that we are going to talk about are accounts which are having private

127
00:08:17,318 --> 00:08:20,950
VPC with no Internet connectivity. Second is when it

128
00:08:20,960 --> 00:08:24,078
comes to large enterprise customers, you always have a

129
00:08:24,084 --> 00:08:27,294
cloud engineering team, and cloud engineering team

130
00:08:27,332 --> 00:08:30,974
is responsible for the platform itself. They are responsible for making

131
00:08:31,012 --> 00:08:35,402
the platform secure. They are responsible for building reusable solutions

132
00:08:35,466 --> 00:08:39,102
which can be leveraged by the applications team. They are responsible

133
00:08:39,166 --> 00:08:42,754
for monitoring the platform. But if you

134
00:08:42,792 --> 00:08:46,642
rely too much on the core engineering team, the application team

135
00:08:46,696 --> 00:08:50,918
would feel that it's a bottleneck because they would want to do something and

136
00:08:51,084 --> 00:08:54,566
you want to give the autonomy to the application

137
00:08:54,668 --> 00:08:58,438
team to build their own infrastructure as and when needed. So that's

138
00:08:58,454 --> 00:09:01,962
where the self service model comes in, where the application team

139
00:09:02,016 --> 00:09:05,606
should have the capability of provisioning the machine

140
00:09:05,638 --> 00:09:09,930
learning resources. The third point would be centralized governance.

141
00:09:10,530 --> 00:09:14,202
The centralized governance and guardrails for the infrastructure

142
00:09:14,266 --> 00:09:18,094
is also an important part, because if as an application team

143
00:09:18,132 --> 00:09:21,662
member, I am building something and then

144
00:09:21,796 --> 00:09:25,366
I'm deploying it as much as I am responsible

145
00:09:25,418 --> 00:09:29,454
for managing that solution, there has to be a centralized governance

146
00:09:29,582 --> 00:09:33,474
from a security office and also from the platform team.

147
00:09:33,512 --> 00:09:36,938
In this case, it would be the cloud engineering team on what kind of guardrails

148
00:09:36,974 --> 00:09:40,802
is being applied on the infrastructure. The last part is the observability

149
00:09:40,866 --> 00:09:43,922
of the solution itself with all these requirements.

150
00:09:43,986 --> 00:09:48,220
Let's look at the target architecture. The target architecture would be

151
00:09:48,590 --> 00:09:52,694
where you would want to leverage the multi account structure of AWS

152
00:09:52,742 --> 00:09:56,358
workspaces, a private VPC network,

153
00:09:56,454 --> 00:09:59,766
and all the traffic going over VPC endpoints.

154
00:09:59,958 --> 00:10:03,726
Pypy Mirror using AWS code artifact so why

155
00:10:03,748 --> 00:10:07,326
would you need a Pypy mirror? Well, as an application

156
00:10:07,428 --> 00:10:11,146
team, if I am deploying certain models on AWS

157
00:10:11,178 --> 00:10:15,166
in that secure environment, I also need a capability of installing

158
00:10:15,198 --> 00:10:19,810
new libraries. Now I can install these new libraries by

159
00:10:19,960 --> 00:10:23,570
directly connecting to the Internet, which is not available to me.

160
00:10:23,720 --> 00:10:26,806
So obviously I need a pipeline mirror from where these libraries can be

161
00:10:26,828 --> 00:10:30,674
downloaded and installed on my notebook or studio. And these libraries

162
00:10:30,722 --> 00:10:34,566
are on top of what already comes out of the notebook and studio by

163
00:10:34,588 --> 00:10:38,042
default by AWS AWS service

164
00:10:38,096 --> 00:10:42,054
catalog for provisioning the resources, Amazon Cloudwatch

165
00:10:42,102 --> 00:10:45,990
for observability and finally transit gateway for network connectivity

166
00:10:46,070 --> 00:10:49,546
to corporate data centers. I won't be talking about

167
00:10:49,568 --> 00:10:53,242
the transit gateway part today. It is mainly as

168
00:10:53,296 --> 00:10:56,686
an informational point that is being included here.

169
00:10:56,788 --> 00:10:59,518
But we will touch upon all the other points that you have seen in the

170
00:10:59,524 --> 00:11:03,360
target architecture. Now let's look at the architecture diagram here.

171
00:11:03,970 --> 00:11:07,346
This is the diagram where I have tried to depict all the points that I

172
00:11:07,368 --> 00:11:10,962
mentioned in the previous slide. You can see that there are

173
00:11:11,016 --> 00:11:14,690
four accounts. Ignore the sagemaker service

174
00:11:14,760 --> 00:11:18,278
account, we'll come to that later. You have an application account

175
00:11:18,364 --> 00:11:22,070
which is the main account. So let's say an application team Alpha

176
00:11:22,490 --> 00:11:26,214
wants to deploy the application in

177
00:11:26,252 --> 00:11:29,418
that account. So that will be the account that they'll be using. You have a

178
00:11:29,424 --> 00:11:33,626
security account, and that security account is a

179
00:11:33,648 --> 00:11:37,642
customer security account being managed by the CSO, possibly where

180
00:11:37,696 --> 00:11:40,906
all the cloud trail logs are coming in. All the flow logs are coming in.

181
00:11:40,928 --> 00:11:44,286
As you see in the diagram which is being analyzed, it is

182
00:11:44,308 --> 00:11:47,662
being worked upon to see if there is any kind of bad

183
00:11:47,716 --> 00:11:51,054
traffic, any suspicious activity which is happening.

184
00:11:51,252 --> 00:11:55,262
You have a customer networking account and the networking account is where you

185
00:11:55,316 --> 00:11:59,266
have the transit gateway which is being shared. And finally

186
00:11:59,368 --> 00:12:02,482
you have a shared services account where you would want to keep

187
00:12:02,536 --> 00:12:05,826
a code artifact which is a pypy mirror. It's kind of like

188
00:12:05,848 --> 00:12:09,474
a central repository where all the different teams

189
00:12:09,522 --> 00:12:13,126
would be able to pull down their libraries as per

190
00:12:13,148 --> 00:12:16,806
their liking. So let's go step by step. The first

191
00:12:16,828 --> 00:12:20,026
thing would be the customer application account. You can see that there is a

192
00:12:20,048 --> 00:12:24,038
VPC here, and within that VPC there are three private subnets.

193
00:12:24,214 --> 00:12:28,070
Within the three private subnets you see two ENIs

194
00:12:28,230 --> 00:12:31,454
and the two Enis are pointing to the

195
00:12:31,492 --> 00:12:35,498
Amazon Sagemaker notebook and Sagemaker studio.

196
00:12:35,674 --> 00:12:39,578
The notebook and studio is not residing in your account. Rather they are residing

197
00:12:39,594 --> 00:12:43,466
in a separate sagemaker service account which is transparent

198
00:12:43,498 --> 00:12:46,610
to the customer. You wouldn't be seeing

199
00:12:46,680 --> 00:12:49,806
that account at all. What you will be seeing is an instance

200
00:12:49,838 --> 00:12:53,394
of notebook running in your account and an instance of studio which is running

201
00:12:53,432 --> 00:12:56,638
in your account. And for the VPC,

202
00:12:56,734 --> 00:13:00,118
you would want to have the VPC endpoints because it's a

203
00:13:00,124 --> 00:13:03,858
VPC which is having no Internet connectivity and everything is private.

204
00:13:03,954 --> 00:13:07,126
The only way that you can access the AWS services

205
00:13:07,228 --> 00:13:10,742
like ECR S three sts kms

206
00:13:10,886 --> 00:13:14,806
is via the VPC endpoint. You would also want the VPC endpoint

207
00:13:14,838 --> 00:13:18,790
for accessing the code artifact. So this is the overall architecture.

208
00:13:18,950 --> 00:13:22,566
If I am going to provision this kind of a structure,

209
00:13:22,758 --> 00:13:26,318
the first thing that I have to ensure is any notebook or studio which is

210
00:13:26,324 --> 00:13:29,834
being provisioned is being provisioned in that VPC.

211
00:13:29,962 --> 00:13:33,726
Because if I give the application team complete access on provisioning

212
00:13:33,758 --> 00:13:37,662
a notebook as per their liking, they can also provision a notebook

213
00:13:37,726 --> 00:13:41,506
without using the given VPC, which will

214
00:13:41,608 --> 00:13:45,758
enable it to run with Internet connectivity. So there are certain guardrails

215
00:13:45,774 --> 00:13:49,634
which you want to enforce on the notebook or the studio

216
00:13:49,682 --> 00:13:53,494
which is being provisioned by the application team. The second

217
00:13:53,532 --> 00:13:57,426
thing is obviously the network. Whenever I'm creating a notebook

218
00:13:57,538 --> 00:14:01,618
and a studio, I want the Enis to be residing in

219
00:14:01,644 --> 00:14:04,300
that new VPC which I have created for the account.

220
00:14:05,470 --> 00:14:08,618
So this new VPC which I have created for the account is what you are

221
00:14:08,624 --> 00:14:12,474
seeing as the application team VPC. The studio EFS directory

222
00:14:12,522 --> 00:14:16,110
is again automatically created when you are provisioning the Sagemaker studio.

223
00:14:16,450 --> 00:14:20,800
Now that you have an idea of the architecture, let's go into

224
00:14:21,250 --> 00:14:24,862
the implementation side of it on how you're going to actually provision

225
00:14:24,926 --> 00:14:28,594
these. So before we go into the provisioning part of it, we want to

226
00:14:28,632 --> 00:14:32,194
understand the service catalog piece

227
00:14:32,312 --> 00:14:36,194
and how it is going to add value here. I spoke earlier about

228
00:14:36,312 --> 00:14:40,102
organizations having a central cloud, engineering having a

229
00:14:40,156 --> 00:14:43,782
central security, and then the application team itself. In this case,

230
00:14:43,836 --> 00:14:47,334
those would be the folks who are the end users. As an application team

231
00:14:47,372 --> 00:14:50,562
member, what I need is speed.

232
00:14:50,706 --> 00:14:53,658
I want to create a notebook, I want to delete a notebook, I want to

233
00:14:53,664 --> 00:14:57,494
create a studio, run a machine learning algorithm

234
00:14:57,542 --> 00:15:01,482
in there, and I want to immediately run some POC. Obviously,

235
00:15:01,616 --> 00:15:05,534
if I am not having a self service model, I wouldn't be having the

236
00:15:05,572 --> 00:15:09,262
speed or the agility which I'm looking for as an application team,

237
00:15:09,396 --> 00:15:12,910
especially when I'm using AWS

238
00:15:13,430 --> 00:15:16,786
for all provisioning activities. On the other side

239
00:15:16,808 --> 00:15:20,786
of the spectrum, we have the security team or the

240
00:15:20,968 --> 00:15:24,482
central engineering team which wants to ensure that there

241
00:15:24,536 --> 00:15:27,986
is compliance. There is standardization, there is curation.

242
00:15:28,178 --> 00:15:31,506
A simple example is there are ten different app teams

243
00:15:31,538 --> 00:15:34,840
who want to create notebooks, and all of them

244
00:15:35,370 --> 00:15:38,998
have a slight variation in the notebook that they are creating.

245
00:15:39,174 --> 00:15:42,870
Some of them want a notebook which is a 50 gb

246
00:15:43,030 --> 00:15:47,226
volume is available. Others would want 25 gb if

247
00:15:47,248 --> 00:15:50,746
there is any specific model or image which they

248
00:15:50,768 --> 00:15:53,806
want to add to their notebook, or there is a new library which they want

249
00:15:53,828 --> 00:15:57,646
to add to the notebook, or they want to have a new lifecycle to

250
00:15:57,668 --> 00:16:01,358
the notebook. And these are things which can

251
00:16:01,444 --> 00:16:05,594
differ as per the team which is trying to create the notebook.

252
00:16:05,722 --> 00:16:09,854
As a central engineering team, they would want to create these reusable patterns

253
00:16:09,902 --> 00:16:13,378
which can be used across teams, more like templates. So if

254
00:16:13,384 --> 00:16:17,026
you want to do that, how would you do that? So that's where AWS

255
00:16:17,058 --> 00:16:20,546
service catalog helps you. It helps the central

256
00:16:20,578 --> 00:16:23,974
engineering team accomplish their goal of security,

257
00:16:24,092 --> 00:16:27,494
curation, compliance, standardization, and it helps the

258
00:16:27,532 --> 00:16:30,998
application teams to accomplish their goal of

259
00:16:31,164 --> 00:16:34,794
speed, agility, self service model, and obviously the time

260
00:16:34,832 --> 00:16:38,186
to market how quickly they can create a PoC and run with it and see

261
00:16:38,208 --> 00:16:42,222
what kind of an outcome is there. Now, before we go into the specifics of

262
00:16:42,356 --> 00:16:46,446
the service catalog, we want to understand few items which

263
00:16:46,468 --> 00:16:49,966
are there in service catalog. The first thing is a product.

264
00:16:50,148 --> 00:16:53,918
Now, a product can be a cloud formation template. If I

265
00:16:53,924 --> 00:16:57,934
am having a cloud formation template, which is creating

266
00:16:57,982 --> 00:17:01,886
an EC two instance, or it is creating a notebook instance,

267
00:17:01,918 --> 00:17:06,018
since we are talking about Sagemaker, that can be equated to a product.

268
00:17:06,184 --> 00:17:09,926
Once I create a product, the next step would be to put

269
00:17:09,948 --> 00:17:13,266
it in a portfolio. Now, this portfolio can be created

270
00:17:13,298 --> 00:17:16,786
by the core engineering team. So let's say a core engineering team creates

271
00:17:16,818 --> 00:17:20,534
a portfolio named Central IT engineering, and it puts a product in there,

272
00:17:20,572 --> 00:17:24,010
which is a sagemaker notebook cloud formation template.

273
00:17:24,350 --> 00:17:28,234
I know that that particular cloud formation template has all the

274
00:17:28,352 --> 00:17:31,886
guardrails which I am expecting for

275
00:17:31,988 --> 00:17:35,626
any notebook which would be coming up. Example would be no Internet

276
00:17:35,658 --> 00:17:40,026
connectivity. There is direct Internet connectivity is put in as false,

277
00:17:40,138 --> 00:17:43,954
no root access. They would be having

278
00:17:44,152 --> 00:17:47,666
the network interfaces. That would be the VPC where

279
00:17:47,688 --> 00:17:51,522
it is supposed to be run. And also you

280
00:17:51,576 --> 00:17:55,138
maybe want to associate a git repository to it. That would be

281
00:17:55,144 --> 00:17:58,614
a code commit repository. So these are the, I would say

282
00:17:58,652 --> 00:18:02,274
guardrails, which the application team wouldn't

283
00:18:02,322 --> 00:18:05,814
want to keep repeating, but the central team wants to enforce it.

284
00:18:05,932 --> 00:18:08,962
So the central team can create a cloud formation template,

285
00:18:09,106 --> 00:18:12,650
and they can put that as a product into AWS service

286
00:18:12,720 --> 00:18:15,974
catalog. Once it goes into the AWS services catalog,

287
00:18:16,102 --> 00:18:19,706
it would then go on into the portfolio. Once it ends up

288
00:18:19,728 --> 00:18:22,986
in the portfolio, you can have constraints associated with

289
00:18:23,008 --> 00:18:26,654
it. There are different kinds of constraints that you would want to have. So there

290
00:18:26,692 --> 00:18:30,718
can be a launch constraint where you're saying that only

291
00:18:30,804 --> 00:18:34,386
these roles would be allowed to launch this product.

292
00:18:34,568 --> 00:18:38,850
And additionally, you can add certain roles to the groups

293
00:18:39,430 --> 00:18:43,282
which would allow only certain app team members

294
00:18:43,416 --> 00:18:46,834
or certain app teams itself to be able to view that

295
00:18:46,872 --> 00:18:50,262
portfolio and operate on that portfolio, or invoke the product.

296
00:18:50,316 --> 00:18:54,118
And those kind of constraints can be added as well. Once the

297
00:18:54,124 --> 00:18:57,670
product list is available, the users can see the products

298
00:18:57,740 --> 00:19:01,130
and they will be able to launch the product. Now, when they launch the product,

299
00:19:01,200 --> 00:19:04,886
obviously the maximum they can do is pass the parameters. They wouldn't

300
00:19:04,918 --> 00:19:08,806
be able to change the product and remove the guardrails,

301
00:19:08,838 --> 00:19:12,746
which I had put in as a central engineering team, into the product

302
00:19:12,848 --> 00:19:16,446
as cloud formation templates. And finally, when the

303
00:19:16,468 --> 00:19:20,506
product is launched, you would be having a provision product as an output.

304
00:19:20,618 --> 00:19:24,354
And this provided product would be a resource which

305
00:19:24,392 --> 00:19:28,434
would be a sagemaker studio, or it would be a

306
00:19:28,632 --> 00:19:32,498
sagemaker notebook, which the application team can use.

307
00:19:32,664 --> 00:19:36,478
And this is where the segregation happens. As an administrator,

308
00:19:36,574 --> 00:19:40,534
I am able to control the product that an application team can use

309
00:19:40,652 --> 00:19:44,920
and also apply the guardrails which an application team would want to use.

310
00:19:45,370 --> 00:19:48,780
And that's the whole advantage of having the self service model.

311
00:19:49,230 --> 00:19:53,382
With the self service model, you will be able to leverage

312
00:19:53,446 --> 00:19:57,270
the infrastructure as code and define your infrastructure,

313
00:19:57,430 --> 00:20:01,174
your compute layer, your storage and other cloud resources,

314
00:20:01,302 --> 00:20:04,426
and using a JSoN or a YamL,

315
00:20:04,538 --> 00:20:08,062
or even terraform scripts or files. Once you have these

316
00:20:08,116 --> 00:20:11,774
things, you can put them as a product and then this product

317
00:20:11,892 --> 00:20:15,426
will be standardized AWS, a best practice across your

318
00:20:15,528 --> 00:20:19,474
organization by this central engineering team. And that

319
00:20:19,512 --> 00:20:22,818
can be one version of the product. An example is

320
00:20:22,904 --> 00:20:26,466
today it's Sagemaker. Tomorrow, if you have example

321
00:20:26,568 --> 00:20:30,326
of a three tier stack with EC two rds and

322
00:20:30,348 --> 00:20:33,926
S three, you can obviously make use of that and you will be

323
00:20:33,948 --> 00:20:37,254
able to have a standardized format. Okay, this is how my

324
00:20:37,292 --> 00:20:40,714
three tier stack is going to be. And multiple app teams can go ahead

325
00:20:40,752 --> 00:20:45,126
and provision that. That's another example. So that's the whole advantage of AWS

326
00:20:45,158 --> 00:20:48,922
service catalog, where the customer can create AWS based

327
00:20:48,976 --> 00:20:52,442
solutions and the product can be exposed

328
00:20:52,506 --> 00:20:56,174
by the central engineering team to the application teams. And once

329
00:20:56,212 --> 00:20:59,454
it has been exposed, the application team would be just

330
00:20:59,492 --> 00:21:02,766
provisioning it and because it has been created by

331
00:21:02,788 --> 00:21:06,546
the central engineering team, you can have the constraints applied to

332
00:21:06,568 --> 00:21:10,562
it, you can have the security controls applied to it, any kind of

333
00:21:10,696 --> 00:21:14,494
tag enforcement, any kind of restrictions, like no Internet

334
00:21:14,542 --> 00:21:17,880
on the studio and no root access

335
00:21:19,130 --> 00:21:22,600
on the notebook, all these things can be put into place.

336
00:21:23,370 --> 00:21:26,454
Now let's look at the second part of the requirement, which we

337
00:21:26,492 --> 00:21:30,278
had spoken up earlier, that as an application team I

338
00:21:30,364 --> 00:21:33,846
want to install some new libraries into my studio or

339
00:21:33,868 --> 00:21:37,746
into my notebook. This is where you would need a pypy mirror.

340
00:21:37,938 --> 00:21:42,046
I will share a link towards the end of the end

341
00:21:42,068 --> 00:21:45,246
of this particular talk, which will give

342
00:21:45,268 --> 00:21:48,794
you steps on how you can set up a secure environment

343
00:21:48,842 --> 00:21:52,366
via a workshop. But before that, you would want

344
00:21:52,388 --> 00:21:55,838
to understand what AWS code artifact is bringing

345
00:21:55,854 --> 00:21:59,310
to the table. If you want to set up a pipi mirror,

346
00:21:59,470 --> 00:22:02,978
you can make use of AWS code artifact which is sitting in a

347
00:22:02,984 --> 00:22:06,294
shared services account. If you recollect from

348
00:22:06,332 --> 00:22:10,040
the previous architecture diagram that we had a look at,

349
00:22:10,810 --> 00:22:13,974
there was this shared services account which was having

350
00:22:14,012 --> 00:22:17,470
a code artifact. And in that code artifact

351
00:22:17,650 --> 00:22:21,162
you are able to put in your libraries and

352
00:22:21,296 --> 00:22:25,770
you can download the libraries from the upstream pypy library.

353
00:22:26,110 --> 00:22:29,674
This is a fully managed artifact repository service and

354
00:22:29,712 --> 00:22:33,186
it supports NPM maven Python Nougat

355
00:22:33,238 --> 00:22:37,786
package formats. And currently you can make use of AWS code artifact

356
00:22:37,898 --> 00:22:41,562
with different package managers like Maven cradle, et cetera.

357
00:22:41,706 --> 00:22:45,810
The idea here is to have AWS code artifact

358
00:22:46,230 --> 00:22:50,098
sit in a central shared services account and

359
00:22:50,184 --> 00:22:53,650
different application teams. As and when they have a requirement,

360
00:22:53,990 --> 00:22:57,334
they would be able to pull down the curated list of

361
00:22:57,372 --> 00:23:01,414
libraries from that code artifact repository service,

362
00:23:01,612 --> 00:23:05,238
and they can go ahead and install it in their notebook or in

363
00:23:05,244 --> 00:23:08,326
their studio. Now let's look a little bit in

364
00:23:08,348 --> 00:23:12,266
depth on code artifact and what it is doing the

365
00:23:12,288 --> 00:23:16,134
same thing that I explained just now. You can have a public artifact

366
00:23:16,182 --> 00:23:20,106
repository. In this case it would be a Pypy public repository and you

367
00:23:20,128 --> 00:23:23,934
can create a domain. Now what's a domain? A domain is a code

368
00:23:23,972 --> 00:23:27,098
artifact specific construct that allows grouping

369
00:23:27,114 --> 00:23:30,238
and managing multiple code artifact repositories together.

370
00:23:30,404 --> 00:23:34,254
So if an organization is creating a central repository for

371
00:23:34,292 --> 00:23:38,034
sharing packages, they can have this domain being created and it

372
00:23:38,072 --> 00:23:41,234
can be shared across multiple teams. And when you have

373
00:23:41,272 --> 00:23:45,082
a repository, it contains a set of packages. So I can have a package

374
00:23:45,166 --> 00:23:48,406
on service catalog tools, I can

375
00:23:48,428 --> 00:23:52,194
have a package on the request package of Python

376
00:23:52,322 --> 00:23:55,986
or even sagemaker sagemaker release,

377
00:23:56,018 --> 00:24:00,374
which is Sagemaker 2.0 release that we have with

378
00:24:00,492 --> 00:24:03,690
one of the PiP packages. So as you see on the right

379
00:24:03,760 --> 00:24:07,526
there is this pull application dependencies for development. The development

380
00:24:07,558 --> 00:24:10,826
team will be able to just pull these dependencies as and when

381
00:24:10,848 --> 00:24:14,222
they need it. And you can also have this integrated into your

382
00:24:14,276 --> 00:24:18,058
CI CD pipelines by using codebuilt or other tools.

383
00:24:18,154 --> 00:24:22,414
That's the whole point of having code artifact. So a quick revision we

384
00:24:22,452 --> 00:24:25,886
saw what's the impact of service catalog which helps

385
00:24:25,918 --> 00:24:29,214
you create these curated products which can be reused

386
00:24:29,262 --> 00:24:32,834
by different application teams, and code artifact helps you create this

387
00:24:32,872 --> 00:24:36,786
centralized repository of PIP dependencies which can again be reused

388
00:24:36,818 --> 00:24:40,326
by different application teams. In that way you're able to

389
00:24:40,428 --> 00:24:43,750
provide this centralized governance or certain aspects of

390
00:24:43,820 --> 00:24:47,990
the machine learning resources which you would be using.

391
00:24:48,140 --> 00:24:51,846
And along with that you give the flexibility to the application

392
00:24:51,948 --> 00:24:55,690
teams to have a self service model where they can just pull down a product

393
00:24:55,760 --> 00:24:58,842
from the service catalog, provision it, and then they can go about

394
00:24:58,896 --> 00:25:02,766
doing their own application specific development within the resources like a

395
00:25:02,788 --> 00:25:06,478
studio or a notebook. With all that said and done, let's have a

396
00:25:06,484 --> 00:25:10,122
look at how you will be building these infrastructure

397
00:25:10,186 --> 00:25:12,830
components by using AWS cloud formation.

398
00:25:13,410 --> 00:25:17,166
We spoke about VPC networking and we mentioned that it's

399
00:25:17,198 --> 00:25:21,054
going to be a private VPC. Now here you can see that it's a private

400
00:25:21,102 --> 00:25:24,782
subnet and I'm having map public IP on launch

401
00:25:24,846 --> 00:25:28,434
as false, which ensures that my subnet which is getting created

402
00:25:28,482 --> 00:25:31,906
on the VPC is a private subnet.

403
00:25:32,098 --> 00:25:35,686
If you have a look at the security group, I am only exposing four

404
00:25:35,708 --> 00:25:39,058
four three. So security group ingress and Egress

405
00:25:39,154 --> 00:25:42,362
is ensuring that only four four three traffic can come in and go out.

406
00:25:42,416 --> 00:25:45,980
And the cider IP is the cider IP of the VPC itself.

407
00:25:46,350 --> 00:25:50,386
You're not exposing the security group outside for ICMP

408
00:25:50,438 --> 00:25:54,094
pings or anything else other than four four three.

409
00:25:54,212 --> 00:25:58,334
And you know that four four three will be only going to your

410
00:25:58,372 --> 00:26:01,790
VPC endpoints. And because

411
00:26:01,860 --> 00:26:05,106
it's private VPC that you

412
00:26:05,128 --> 00:26:08,386
are using, you need the VPC endpoints for any communication with

413
00:26:08,408 --> 00:26:11,954
other AWS services. The second part

414
00:26:11,992 --> 00:26:15,330
is enabling the VPC endpoints. Here you have

415
00:26:15,400 --> 00:26:19,170
the sagemaker runtime VPC endpoint which is sagemaker

416
00:26:19,250 --> 00:26:22,482
runtime, and you have the sagemaker API endpoint.

417
00:26:22,546 --> 00:26:26,326
Without these endpoints you wouldn't be able to interact with sagemaker in

418
00:26:26,348 --> 00:26:30,450
a private VPC. You can see that there are three subnets which are provided

419
00:26:30,610 --> 00:26:34,310
subnet one, two and three. All three are created by the previous

420
00:26:34,390 --> 00:26:38,234
VPC network that we spoke about. And again, the VPC id

421
00:26:38,272 --> 00:26:42,042
is going to be the same VPC id. And you can see the private DNS

422
00:26:42,106 --> 00:26:45,710
is enabled as true. Going back to the previous slide,

423
00:26:46,210 --> 00:26:50,670
you would notice that in terms of the VPC networking,

424
00:26:51,170 --> 00:26:54,482
we have set in the map public

425
00:26:54,536 --> 00:26:58,386
IP as false. So none of these VPC subnets would

426
00:26:58,408 --> 00:27:00,930
be having connectivity to the Internet.

427
00:27:01,830 --> 00:27:05,138
The third part is the flow logs. We had seen that there

428
00:27:05,144 --> 00:27:08,722
is a central security account and that security account was responsible

429
00:27:08,786 --> 00:27:12,066
for analyzing the VPC flow logs.

430
00:27:12,178 --> 00:27:15,606
VPC flow logs allows you to look at the traffic which

431
00:27:15,628 --> 00:27:18,818
is flowing in and out of certain enis. And if you're applying

432
00:27:18,834 --> 00:27:22,314
at the VPC level, it would look at the entire VPC traffic and

433
00:27:22,352 --> 00:27:25,862
tell you which traffic has been accepted and which has been rejected.

434
00:27:26,006 --> 00:27:29,482
Because you are having it in a central account, you would want to keep it

435
00:27:29,536 --> 00:27:33,162
in s three and that s three bucket, the log

436
00:27:33,216 --> 00:27:36,590
destination you are giving. I'm just giving an example of say Doc example

437
00:27:36,660 --> 00:27:40,666
bucket and you would want to give some kind of structure like flow

438
00:27:40,698 --> 00:27:44,594
logs and the account number from where this flow log is coming up

439
00:27:44,792 --> 00:27:48,466
here. I'm putting all the traffic here for

440
00:27:48,568 --> 00:27:52,690
tracking purposes and a maximum aggregation interval of 60 seconds.

441
00:27:53,430 --> 00:27:57,394
So this is where the fun happens. You have the Amazon Sagemaker

442
00:27:57,442 --> 00:28:01,266
studio and you have the notebook. Within the Amazon Sagemaker studio

443
00:28:01,298 --> 00:28:06,040
and the notebook here you can see that the

444
00:28:06,650 --> 00:28:10,266
KMS key id and the role ARN has

445
00:28:10,288 --> 00:28:14,102
been provided because you're providing the KMS key id, you are ensuring

446
00:28:14,166 --> 00:28:18,090
that you're using a CMk as a customer managed key for

447
00:28:18,160 --> 00:28:21,674
encrypting the Sagemaker notebook. And the same has been applied

448
00:28:21,722 --> 00:28:25,690
for Sagemaker Studio as well in terms of the execution rule.

449
00:28:25,850 --> 00:28:29,326
So as a central engineering team, when I

450
00:28:29,348 --> 00:28:32,746
am creating these products, by ensuring that the direct Internet

451
00:28:32,778 --> 00:28:36,514
access is disabled in the notebook instance, and by

452
00:28:36,552 --> 00:28:40,050
ensuring that the app network access type is VPC only,

453
00:28:40,200 --> 00:28:43,442
I'm ensuring that notebook and studio is never going to communicate with

454
00:28:43,496 --> 00:28:46,710
any traffic outside the VPC.

455
00:28:47,130 --> 00:28:50,886
Secondly, the root access has also been disabled on the

456
00:28:50,908 --> 00:28:54,214
notebook. You would see that the security groups which are being

457
00:28:54,252 --> 00:28:57,906
imported is coming from the sagemaker environment and default

458
00:28:57,938 --> 00:29:01,482
security group and default security group id that

459
00:29:01,536 --> 00:29:04,662
has been imported from a previous stack. Now that previous

460
00:29:04,726 --> 00:29:08,634
stack is the VPC stack that we had seen earlier where

461
00:29:08,672 --> 00:29:12,022
the VPC has been created and it is exporting these ids

462
00:29:12,086 --> 00:29:16,046
so that it can be imported into another product. And finally you have

463
00:29:16,068 --> 00:29:19,886
a volume size being provided. But if as an

464
00:29:19,908 --> 00:29:23,342
application team, if I'm looking at this stack and if I'm looking at this cloud

465
00:29:23,396 --> 00:29:26,786
formation, there is no way I'm going to change the direct Internet access.

466
00:29:26,888 --> 00:29:30,386
There is no way I'm going to change the KMS ID. I can't get the

467
00:29:30,408 --> 00:29:34,258
root access enabled. So these kind of controls help you

468
00:29:34,344 --> 00:29:38,358
build the compliance into the product which is existing in

469
00:29:38,364 --> 00:29:42,326
the service catalog, and that way you will be able to share this product

470
00:29:42,428 --> 00:29:45,846
confidently with your application teams and you will be

471
00:29:45,868 --> 00:29:49,494
able to create this reusable pattern where multiple

472
00:29:49,542 --> 00:29:52,220
teams can go ahead and reuse this product.

473
00:29:53,390 --> 00:29:56,380
So that's everything on the cloud formation side of it.

474
00:29:57,230 --> 00:30:00,510
We also spoke about the multi account

475
00:30:00,580 --> 00:30:04,154
structure using AWS organizations. The multi

476
00:30:04,202 --> 00:30:07,310
account structure using AWS organization needs

477
00:30:07,380 --> 00:30:10,574
a service catalog and it

478
00:30:10,612 --> 00:30:14,306
also has the service control policies which are being applied now.

479
00:30:14,328 --> 00:30:17,490
What are these service control policies? Service control

480
00:30:17,560 --> 00:30:21,406
policies are applied at the OU level, which is the organization

481
00:30:21,518 --> 00:30:25,054
unit, and they help you provide these

482
00:30:25,112 --> 00:30:28,902
broad strokes on certain restrictions which you would want to

483
00:30:28,956 --> 00:30:33,270
apply across the organization. So in next slide,

484
00:30:34,090 --> 00:30:37,586
we will be looking at what kind of service

485
00:30:37,708 --> 00:30:40,220
control policies can be applied for data.

486
00:30:41,470 --> 00:30:45,334
We know that we can control the compliance and restrictions

487
00:30:45,382 --> 00:30:48,858
on a product side. What we don't know is

488
00:30:48,944 --> 00:30:52,242
how to ensure that the data is always encrypted.

489
00:30:52,406 --> 00:30:56,714
Well, that can be done by using this service control policies.

490
00:30:56,842 --> 00:31:00,094
If you are applying the service control policy at an OU level,

491
00:31:00,212 --> 00:31:03,874
I'm saying that whenever you are creating an automl job,

492
00:31:03,992 --> 00:31:07,858
or a model, or a labeling job, or a processing job, or a

493
00:31:07,864 --> 00:31:10,962
training job, in all cases it is

494
00:31:11,016 --> 00:31:14,642
mandatory to give a sagemaker volume KMs key.

495
00:31:14,776 --> 00:31:18,386
So you can see at the top that the effect has been marked as deny.

496
00:31:18,498 --> 00:31:22,210
That means in case you are not provided a KMS

497
00:31:22,290 --> 00:31:25,574
key for the volume, then these actions will not

498
00:31:25,612 --> 00:31:29,342
be executed and you will not be allowed to execute these actions.

499
00:31:29,506 --> 00:31:33,002
The same applies for the output KMS key. So this

500
00:31:33,056 --> 00:31:36,246
ensures that every time you're creating a model, you're creating

501
00:31:36,278 --> 00:31:40,170
a training job, you're creating a transformation job, or you're

502
00:31:40,670 --> 00:31:44,506
creating a processing job. These actions are governed

503
00:31:44,538 --> 00:31:48,766
by the fact that you need to use a KMS key for the

504
00:31:48,868 --> 00:31:52,446
encryption of the data, and this often happens to be one

505
00:31:52,468 --> 00:31:56,306
of the guardrails or the requirements when it comes to regulated customers,

506
00:31:56,488 --> 00:32:00,754
that all the data has to be encrypted in transit or

507
00:32:00,792 --> 00:32:04,402
at rest. AWS has the facility of

508
00:32:04,456 --> 00:32:08,094
using the KMS keys, but in order to enforce

509
00:32:08,142 --> 00:32:11,830
it at an organization level, you would want to include it in the service control

510
00:32:11,900 --> 00:32:15,282
policies. The same applies for the traffic and network.

511
00:32:15,426 --> 00:32:18,926
In case you want to have intercontainer traffic encryption,

512
00:32:19,058 --> 00:32:22,746
you again can apply it as a service control policy at an

513
00:32:22,768 --> 00:32:26,522
organization level, and by doing so, you will be able

514
00:32:26,576 --> 00:32:30,150
to ensure that every job which is created.

515
00:32:30,230 --> 00:32:34,030
So here you can see that the effect is again deny. And in case someone

516
00:32:34,100 --> 00:32:38,126
wants to create a processing job or a training job or a monitoring job,

517
00:32:38,228 --> 00:32:41,614
they have to keep the traffic encryption as true.

518
00:32:41,812 --> 00:32:45,394
Because if the intercontainer traffic encryption happens to be

519
00:32:45,432 --> 00:32:49,326
false. So the condition says if the intercontainer traffic encryption

520
00:32:49,358 --> 00:32:52,994
is false, then the action is to deny all of these.

521
00:32:53,192 --> 00:32:57,114
Same with network isolation. If the network isolation

522
00:32:57,182 --> 00:33:00,994
happens to be false, then deny these. So for these actions

523
00:33:01,042 --> 00:33:05,046
to work, you have to provide the network isolation, you have to

524
00:33:05,068 --> 00:33:08,540
enable the traffic encryption. And the same with before,

525
00:33:08,990 --> 00:33:12,826
I'll just go over these two policies again, just for the

526
00:33:12,848 --> 00:33:17,222
clarity purpose of it, what the policies are saying is deny

527
00:33:17,286 --> 00:33:21,258
these actions if the volume KMS key happens to

528
00:33:21,264 --> 00:33:24,858
be null. So if you can see the condition, if it is null,

529
00:33:24,954 --> 00:33:28,526
which is true, then deny these actions. So in short, you can

530
00:33:28,548 --> 00:33:32,554
only execute these actions. If you're using a KMS key for the volume

531
00:33:32,602 --> 00:33:36,546
encryption and for the KMS key for the output and

532
00:33:36,568 --> 00:33:40,366
with respect to traffic and network, in case the intercontainer

533
00:33:40,398 --> 00:33:44,402
traffic encryption is false, then you will not be able to do

534
00:33:44,456 --> 00:33:48,558
processing job and training job. And same applies with network isolation.

535
00:33:48,734 --> 00:33:52,914
It will be worth to visit the documentation on the service control policies

536
00:33:52,962 --> 00:33:56,118
on how they work. But essentially these are guardrails which you

537
00:33:56,124 --> 00:33:59,834
can apply at the organization unit level, across your

538
00:33:59,872 --> 00:34:03,498
organization by the central platform, the platform

539
00:34:03,584 --> 00:34:06,986
team, and that will help you enforce these for all

540
00:34:07,008 --> 00:34:10,638
the application teams which are trying to create a model or train a job,

541
00:34:10,724 --> 00:34:14,874
or create a processing job, or do some kind of monitoring schedule,

542
00:34:14,922 --> 00:34:17,662
et cetera. Moving on,

543
00:34:17,796 --> 00:34:21,614
certain AWS AI services may also store and

544
00:34:21,652 --> 00:34:25,970
use the customer content for processing these services and for

545
00:34:26,120 --> 00:34:29,854
continuously improving the Amazon AI services. As an AWS

546
00:34:29,902 --> 00:34:33,038
customer when it comes to regulated industry,

547
00:34:33,214 --> 00:34:37,270
these customers would want to opt out from using their

548
00:34:37,340 --> 00:34:40,386
data in terms of improving the Amazon AI

549
00:34:40,418 --> 00:34:44,086
services. So there is a mechanism for you to

550
00:34:44,108 --> 00:34:48,042
opt out of this by applying this particular

551
00:34:48,176 --> 00:34:51,610
policy at your root level. Essentially what the policy

552
00:34:51,680 --> 00:34:55,418
is saying is do not allow any data

553
00:34:55,584 --> 00:34:59,546
from all the accounts under this organization to

554
00:34:59,568 --> 00:35:03,454
be used for improvement of the Amazon AI services and

555
00:35:03,492 --> 00:35:07,054
technology. So as an AWS customer, you can

556
00:35:07,092 --> 00:35:11,166
just opt out from using the

557
00:35:11,348 --> 00:35:14,846
data to improve the Amazon AI services. Now,

558
00:35:14,868 --> 00:35:18,834
we spoke a lot about service catalog. We spoke a lot about

559
00:35:19,032 --> 00:35:22,574
the code artifact and how the guardrails can be put in. Now let's

560
00:35:22,622 --> 00:35:27,270
see some screenshots on how these things actually look on the AWS console.

561
00:35:28,090 --> 00:35:31,366
This is where you can see the provisioning of the products using

562
00:35:31,468 --> 00:35:34,902
AWS service catalog. So when you create a service

563
00:35:34,956 --> 00:35:38,406
catalog product as an application team member, this is how I will

564
00:35:38,428 --> 00:35:41,702
be looking at it. You can see there is a sagemaker studio

565
00:35:41,766 --> 00:35:44,794
user, there is a studio, there is a notebook, and then there is a data

566
00:35:44,832 --> 00:35:48,502
science environment. As an application team member,

567
00:35:48,646 --> 00:35:51,946
I can go ahead and click on the data science environment and I

568
00:35:51,968 --> 00:35:56,046
can provision it and you can see that the provisioning is happening

569
00:35:56,228 --> 00:35:58,974
where the data science environment is coming up.

570
00:35:59,092 --> 00:36:03,466
And once it has come up, you would be able to see the VPC

571
00:36:03,658 --> 00:36:07,026
which has been created as part of your data science environment. If you

572
00:36:07,048 --> 00:36:11,054
want to create multiple Sagemaker notebooks, just click on the Sagemaker notebook

573
00:36:11,102 --> 00:36:14,642
product that you see on row number three and then you will be able to

574
00:36:14,696 --> 00:36:18,600
provision the sagemaker notebook as well. So far

575
00:36:20,170 --> 00:36:23,654
we have seen about the preventative controls. Now we will be looking

576
00:36:23,692 --> 00:36:27,474
at the detective controls. You can make use of AWS config

577
00:36:27,522 --> 00:36:31,494
in order to enforce the detective controls. Now these detective controls

578
00:36:31,542 --> 00:36:34,806
would be by using the existing

579
00:36:34,918 --> 00:36:38,234
rules which are available in AWS config. And once

580
00:36:38,272 --> 00:36:41,774
you enable it, you can see the non compliant resources as you see

581
00:36:41,812 --> 00:36:45,406
in the screenshot where default security group

582
00:36:45,508 --> 00:36:49,454
is not closed or the sagemaker endpoint configuration is

583
00:36:49,492 --> 00:36:53,554
not yet available. These kind of controls are

584
00:36:53,592 --> 00:36:56,370
applied by using AWS config.

585
00:36:56,870 --> 00:37:00,606
The next slide is about the centralized governance

586
00:37:00,798 --> 00:37:04,546
and the centralized governance again using code artifacts. So here you can see there

587
00:37:04,568 --> 00:37:08,006
is a central it pypy mirror and you can see that

588
00:37:08,028 --> 00:37:11,874
it's connected to the public repository and it would be continuously

589
00:37:11,922 --> 00:37:15,510
downloading the packages that you need. And then the central

590
00:37:15,850 --> 00:37:20,470
it team or the platform team will be able to control what

591
00:37:20,540 --> 00:37:23,962
all packages will be part of this. And as an application team

592
00:37:24,016 --> 00:37:27,354
member, if this is residing in your shared services account or

593
00:37:27,392 --> 00:37:30,862
something, you can just connect to it and then you can download the

594
00:37:30,916 --> 00:37:34,314
required dependencies. And that's where the centralized governance

595
00:37:34,362 --> 00:37:37,674
of the PIP dependencies come into picture. We have spoken

596
00:37:37,722 --> 00:37:43,490
about so many different aspects of governance and monitoring

597
00:37:44,070 --> 00:37:47,714
and also on the service catalog side, let's see how this

598
00:37:47,752 --> 00:37:51,026
would look if you want to build an entire end

599
00:37:51,048 --> 00:37:54,610
to end pipeline on provisioning of the artifacts.

600
00:37:55,030 --> 00:37:58,982
So we have what is called AWS service catalog tools which

601
00:37:59,036 --> 00:38:02,146
allows you to build this sort of a pipeline

602
00:38:02,258 --> 00:38:05,586
by using CFN Nag cloudformation Rspec,

603
00:38:05,698 --> 00:38:10,086
and you can validate the cloudformation templates which are sitting in your git repository.

604
00:38:10,278 --> 00:38:13,802
And from there it can go ahead and provision these

605
00:38:13,856 --> 00:38:17,594
products into the accounts that you need or share these products into

606
00:38:17,632 --> 00:38:21,226
the accounts that you need. I have a link towards the end of

607
00:38:21,248 --> 00:38:25,194
this talk where you can go ahead and play around with the AWS

608
00:38:25,242 --> 00:38:28,910
service catalog tools. That is service catalog factory and service

609
00:38:28,980 --> 00:38:33,002
catalog puppet. Not to be confused with the open source puppet configuration

610
00:38:33,066 --> 00:38:36,594
tool. This is again open source tooling from

611
00:38:36,632 --> 00:38:39,794
AWS under AWS Labs, and you should be able to

612
00:38:39,832 --> 00:38:43,154
see the service catalog tools link at the end of this

613
00:38:43,192 --> 00:38:47,074
talk. By using the service catalog tools you can create these

614
00:38:47,112 --> 00:38:50,626
end to end pipelines, and these pipelines will be responsible

615
00:38:50,738 --> 00:38:54,386
for getting your cloud formation template from the git

616
00:38:54,418 --> 00:38:57,802
repository where you have and converting it into a product

617
00:38:57,856 --> 00:39:01,162
which can be shared with multiple accounts. And then those

618
00:39:01,216 --> 00:39:04,874
accounts will be having a similar view of how the application

619
00:39:04,992 --> 00:39:08,474
team and how the service catalog is

620
00:39:08,512 --> 00:39:12,474
running. I'm just putting back

621
00:39:12,592 --> 00:39:15,726
the next slide now. So this is how you will be

622
00:39:15,748 --> 00:39:19,294
writing a Jupyter notebook. And here you can see that

623
00:39:19,412 --> 00:39:22,874
when you are creating a Jupyter notebook, you are making use of

624
00:39:22,932 --> 00:39:26,466
the session, which is a sagemaker session, and you are passing the

625
00:39:26,488 --> 00:39:30,146
boto three session in here. By passing the

626
00:39:30,168 --> 00:39:33,934
boto three session, you are allowing the sagemaker

627
00:39:33,982 --> 00:39:37,730
session to Piggybank on the previous boto three calls.

628
00:39:37,810 --> 00:39:41,474
And the clients like sagemaker client and the Sagemaker runtime

629
00:39:41,522 --> 00:39:45,682
client can be reused by the sagemaker session for executing

630
00:39:45,746 --> 00:39:49,078
or executing your code like the estimator or deploying

631
00:39:49,094 --> 00:39:52,378
your model, et cetera. This is an example which you

632
00:39:52,384 --> 00:39:56,678
can get from the Sagemaker notebook, which is on the video games

633
00:39:56,854 --> 00:40:00,186
Xgboost algorithm, and it will allow you to

634
00:40:00,208 --> 00:40:04,154
just run through this example and see how you can run the notebook

635
00:40:04,202 --> 00:40:07,786
and it's available from Amazon. In the next slide

636
00:40:07,818 --> 00:40:11,566
you would see the guardrails which we are putting in. So the service

637
00:40:11,668 --> 00:40:15,374
control policies we had mentioned that without providing a volume

638
00:40:15,422 --> 00:40:18,610
kms key and an output kms key, you would not be able

639
00:40:18,680 --> 00:40:22,082
to run an estimator. And the same applies for

640
00:40:22,136 --> 00:40:25,966
enable network isolation and intercontainer traffic.

641
00:40:26,158 --> 00:40:29,926
So you can see that these four attributes have been passed here,

642
00:40:30,028 --> 00:40:32,866
along with the subnets and the security groups.

643
00:40:33,058 --> 00:40:36,946
That's the enforcement that you're doing or the guardrails that you're applying.

644
00:40:37,058 --> 00:40:40,454
And without these guardrails, it is possible to run

645
00:40:40,492 --> 00:40:44,278
the estimator and it is able to train and deploy a model. But that's

646
00:40:44,294 --> 00:40:48,374
not the point here, right? We are trying to enforce certain guardrails,

647
00:40:48,422 --> 00:40:52,526
especially when it comes to building a secure machine learning environment for

648
00:40:52,628 --> 00:40:56,174
regulated customers. And in that case, you want to run

649
00:40:56,212 --> 00:40:59,834
everything within a VPC. So the first architecture diagram

650
00:40:59,882 --> 00:41:03,870
that we saw where everything was running in a private VPC.

651
00:41:03,950 --> 00:41:07,502
We are enforcing it here by using the subnets,

652
00:41:07,566 --> 00:41:11,294
by using the security group ids, by using the volume

653
00:41:11,342 --> 00:41:15,614
KMS key, the output KMS key, the encryption intercontainer

654
00:41:15,662 --> 00:41:19,554
traffic being set as true, and finally the enable network isolation

655
00:41:19,602 --> 00:41:23,154
being set as true. With all these four or five factors

656
00:41:23,202 --> 00:41:27,490
which have been added to the existing estimator, you are ensuring

657
00:41:27,650 --> 00:41:31,894
that the sagemaker job which is being run, which is being trained,

658
00:41:32,022 --> 00:41:35,466
and the model which is being deployed. The second line that you see where

659
00:41:35,488 --> 00:41:38,714
the model is again having a KMS key ARN being

660
00:41:38,752 --> 00:41:42,874
passed around, all these things are encrypted as per the standards

661
00:41:42,922 --> 00:41:46,506
that you would be having within that organization. So that's

662
00:41:46,538 --> 00:41:49,790
the whole point of building these environments.

663
00:41:51,090 --> 00:41:54,878
In terms of whatever we have discussed before, we are applying that in

664
00:41:54,884 --> 00:41:58,274
the code and enforcing it, because if as an application team

665
00:41:58,312 --> 00:42:01,598
member, I decide to mark the network isolation AWS false,

666
00:42:01,694 --> 00:42:05,894
my estimator is not going to get deployed. It will give me an error because

667
00:42:06,092 --> 00:42:10,034
I'm not allowing someone to run a processing job without ensuring

668
00:42:10,082 --> 00:42:13,878
network isolation. So that's the advantage of making use of these

669
00:42:13,964 --> 00:42:18,022
guardrails and the service control policies, and also

670
00:42:18,076 --> 00:42:21,642
the service catalog products where you are able to enforce it for the different

671
00:42:21,696 --> 00:42:25,606
products. Finally, it comes to monitoring, monitoring the deployed

672
00:42:25,638 --> 00:42:29,226
models. How would you monitor it? Now here is an

673
00:42:29,248 --> 00:42:32,814
example of a model which has been deployed and there have been 45

674
00:42:32,852 --> 00:42:36,170
invocations of the model. So the model has an endpoint.

675
00:42:36,250 --> 00:42:40,030
And in the previous slide we saw that the model

676
00:42:40,100 --> 00:42:44,090
has been deployed with XGB deploy.

677
00:42:44,250 --> 00:42:47,694
It has been deployed on an ML M five X large instance.

678
00:42:47,742 --> 00:42:50,994
And there is a KMS key which is being used for encrypting it.

679
00:42:51,112 --> 00:42:55,026
In terms of the monitoring side, we have 45 invocations which

680
00:42:55,048 --> 00:42:58,358
have happened on the model. And this is where we are using cloud watch.

681
00:42:58,524 --> 00:43:02,610
There are no errors, no 500 errors, no 400 errors.

682
00:43:02,690 --> 00:43:06,498
And then you can also look at the model latency and the overhead latency.

683
00:43:06,674 --> 00:43:10,586
Now what's a model latency? That's the interval time taken by

684
00:43:10,608 --> 00:43:14,486
the model to respond to a request. And that's

685
00:43:14,518 --> 00:43:18,490
just from the viewpoint of sagemaker. So it would include the local

686
00:43:18,560 --> 00:43:22,218
communication that is happening to send the request

687
00:43:22,314 --> 00:43:26,010
and then to fetch the response and the overhead latency,

688
00:43:26,170 --> 00:43:30,234
that's the interval which is measured from the time sagemaker receives

689
00:43:30,282 --> 00:43:33,994
the request. So one is the model latency, which is the model invocation

690
00:43:34,042 --> 00:43:37,490
and the response time itself. And then there is the overhead latency.

691
00:43:38,230 --> 00:43:41,522
There are more dashboards which you can obviously building based on

692
00:43:41,576 --> 00:43:44,978
the metrics which are exposed in Cloudwatch. This is just an example

693
00:43:45,064 --> 00:43:48,166
on how you can leverage Cloudwatch for doing this.

694
00:43:48,348 --> 00:43:51,986
You can also look at the resource metrics like the cpu

695
00:43:52,018 --> 00:43:55,750
utilization of the model, the memory utilization and the disk utilization.

696
00:43:56,250 --> 00:44:00,380
So it gives you a very good visibility into the model itself.

697
00:44:00,990 --> 00:44:04,074
Along with this, the flow logs which were using earlier into

698
00:44:04,112 --> 00:44:07,638
a central security bucket. Those can be leveraged

699
00:44:07,654 --> 00:44:11,354
to look at network traffic. That gives you again a visibility into

700
00:44:11,392 --> 00:44:14,746
what is there in the network. And finally you have

701
00:44:14,848 --> 00:44:18,778
the cloud trail which is there for every API call of sagemaker.

702
00:44:18,954 --> 00:44:22,598
The best practice would be to monitoring these cloud trail events

703
00:44:22,634 --> 00:44:25,646
which are again happening. So to conclude,

704
00:44:25,758 --> 00:44:29,358
what did we learn? We are using multi account

705
00:44:29,454 --> 00:44:33,570
structure to improve the security and segregation of responsibilities.

706
00:44:33,990 --> 00:44:37,574
We are using SCPs and IAM policies to set

707
00:44:37,612 --> 00:44:40,962
up the preventative guardrails. We are leveraging AWS

708
00:44:41,026 --> 00:44:44,326
config for the detective controls. And finally, we are

709
00:44:44,348 --> 00:44:48,106
giving the application teams autonomy via self service products

710
00:44:48,208 --> 00:44:52,074
which are being shared by AWS service catalog using

711
00:44:52,112 --> 00:44:55,162
a combination of all these different features which are there

712
00:44:55,216 --> 00:44:59,114
on AWS. It gives you a capability to

713
00:44:59,152 --> 00:45:02,442
build a secure machine learning environment for

714
00:45:02,496 --> 00:45:05,902
a customer. And that's the whole, I would say,

715
00:45:05,956 --> 00:45:09,930
objective of this talk where I wanted to go through the best practices

716
00:45:10,010 --> 00:45:13,750
which can be applied when it comes to running Sagemaker,

717
00:45:13,930 --> 00:45:17,506
which is a managed compute and it gives you

718
00:45:17,608 --> 00:45:21,394
the capability of having all these different controls put

719
00:45:21,432 --> 00:45:25,346
in place. It gives you the capability of running your machine learning models

720
00:45:25,378 --> 00:45:29,160
at scale. And with the above mentioned security,

721
00:45:30,010 --> 00:45:33,894
I would say security practices, you can ensure that

722
00:45:33,932 --> 00:45:37,766
your workloads are running in a safe manner. And this

723
00:45:37,788 --> 00:45:41,402
is the last slide where it is basically the references that I have been

724
00:45:41,536 --> 00:45:45,466
talking about. You can go into the Sagemaker workshop and have

725
00:45:45,488 --> 00:45:49,366
a look at how service catalog has been used, how the controls

726
00:45:49,398 --> 00:45:53,050
have been put in. There are examples on GitHub,

727
00:45:53,130 --> 00:45:56,746
on Sagemaker and finally service catalog tools workshop

728
00:45:56,778 --> 00:46:00,606
as well, which gives you that centralized pipeline on code pipeline and

729
00:46:00,628 --> 00:46:03,758
how you can share the product with different teams. And do have a look at

730
00:46:03,764 --> 00:46:07,594
the white paper as well on machine learning with financial services on AWS

731
00:46:07,642 --> 00:46:10,974
which talks about how you can secure the data and what are the best

732
00:46:11,012 --> 00:46:14,734
practices when it comes to that being said, that brings me to the close

733
00:46:14,772 --> 00:46:18,534
of my talk. Thank you so much for your time and take

734
00:46:18,572 --> 00:46:18,918
care.

