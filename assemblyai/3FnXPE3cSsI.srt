1
00:01:28,370 --> 00:01:31,906
Hi everyone, this is Erez Berkner, CEO and co founder of Lumigo.

2
00:01:32,018 --> 00:01:35,554
And today we're going to talk about observability

3
00:01:35,682 --> 00:01:39,950
monitoring and what should we do when things go wrong.

4
00:01:40,140 --> 00:01:43,422
So think about this. It's 232

5
00:01:43,476 --> 00:01:47,562
in the morning, page of duty wakes you up and DynamoDB is failing

6
00:01:47,706 --> 00:01:51,360
again. What do you do?

7
00:01:51,970 --> 00:01:55,246
And I like to call this talk, what happened when DynamoDB

8
00:01:55,278 --> 00:01:59,374
explodes and I use DynamoDB,

9
00:01:59,422 --> 00:02:02,642
but I want to make it a bit broader. It's not just

10
00:02:02,696 --> 00:02:06,662
about DynamodB. DynamoDB is different in that it

11
00:02:06,716 --> 00:02:10,598
is a managed service. You don't control the server, you don't control

12
00:02:10,684 --> 00:02:14,118
the operating system and you're very limited with

13
00:02:14,124 --> 00:02:17,174
your visibility. So when we

14
00:02:17,212 --> 00:02:21,100
talk about Dynamodb, when we talk about managed services,

15
00:02:21,870 --> 00:02:25,594
I want to broader this to everything that is as

16
00:02:25,632 --> 00:02:28,778
a service. So we all know function as a service.

17
00:02:28,864 --> 00:02:32,478
But think about Q as a service or data as a

18
00:02:32,484 --> 00:02:36,126
service. Snowflake, DynamoDB storage as a service,

19
00:02:36,228 --> 00:02:39,694
even stripe Twilio, all the SaaS services, everything that

20
00:02:39,732 --> 00:02:43,438
you don't control, you don't deploy agent, you don't

21
00:02:43,534 --> 00:02:47,234
maintain the server, you don't write the code over there, you don't

22
00:02:47,272 --> 00:02:51,070
define the API. All of this really creates

23
00:02:51,150 --> 00:02:54,366
a challenge when it comes to monitoring, debugging,

24
00:02:54,398 --> 00:02:58,040
troubleshooting, and that's what we're going to focus on today.

25
00:02:58,730 --> 00:03:02,626
I like to call this broader sense of managed

26
00:03:02,658 --> 00:03:06,578
services serverless and it's a broad definition,

27
00:03:06,754 --> 00:03:10,314
but it really helps me define these

28
00:03:10,352 --> 00:03:14,538
core. There are no servers over here and those applications are

29
00:03:14,624 --> 00:03:18,646
usually very distributed with dozens or hundreds

30
00:03:18,678 --> 00:03:22,554
of services that keep changing and no longer three tier

31
00:03:22,602 --> 00:03:26,222
monoliths like we used to have. So across all these services,

32
00:03:26,356 --> 00:03:30,026
when DynamoDB actually explodes, it's very hard to zoom

33
00:03:30,058 --> 00:03:33,738
out and understand the context, understand the overall application health. How does

34
00:03:33,764 --> 00:03:37,326
that impact without having the actual connection

35
00:03:37,358 --> 00:03:39,700
and context between different services?

36
00:03:40,390 --> 00:03:43,506
So that would actually be the first thing we need to make

37
00:03:43,528 --> 00:03:47,302
sure that we have in order to understand what's going on. When something

38
00:03:47,356 --> 00:03:51,478
goes wrong, we first of all have to implement one way or another

39
00:03:51,564 --> 00:03:54,726
tracing. Cause it's distributed, it's distributed tracing. And the

40
00:03:54,748 --> 00:03:58,634
point is that this tracing will allow us to

41
00:03:58,832 --> 00:04:02,714
go back, go upstream and understand not

42
00:04:02,752 --> 00:04:05,930
just that this failed, but what happened before

43
00:04:06,000 --> 00:04:10,038
that, where did it originate from and what did it impact.

44
00:04:10,214 --> 00:04:13,962
So in things case I can find actually what is a customer facing API.

45
00:04:14,026 --> 00:04:17,710
Is this business critical and take a decision and assign a priority

46
00:04:18,130 --> 00:04:21,546
based on that. Now I want to use the concept

47
00:04:21,578 --> 00:04:24,866
of distributed tracing and take it to the next level and

48
00:04:24,968 --> 00:04:28,658
be able to look at. I like to call

49
00:04:28,664 --> 00:04:31,986
it a virtual stack trace. Like in the past I used to

50
00:04:32,008 --> 00:04:35,838
see exactly which functions were called in a monolith environment.

51
00:04:35,934 --> 00:04:38,930
I want to be able to do the same in a distributed environment.

52
00:04:39,090 --> 00:04:42,306
So seeing the inputs, these outputs, environment variables,

53
00:04:42,338 --> 00:04:45,862
the stack trace, the logs, everything that I can on

54
00:04:45,916 --> 00:04:49,050
each and every service along the path of that failure.

55
00:04:50,350 --> 00:04:54,394
And there are different ways to do that. One, you can use

56
00:04:54,432 --> 00:04:58,140
Cloudwatch and other cloud native friends.

57
00:04:59,470 --> 00:05:02,654
I think the main point here is that

58
00:05:02,772 --> 00:05:06,110
you can use code and implement

59
00:05:06,450 --> 00:05:09,166
in the containers, in the lambda before,

60
00:05:09,268 --> 00:05:14,906
after the managed service, different outputs,

61
00:05:14,938 --> 00:05:18,546
different logs that will allow you to understand what's going on. So you can

62
00:05:18,568 --> 00:05:21,698
log the request and you can log the payload and the output and

63
00:05:21,704 --> 00:05:25,414
the environment variables and can exception, you can catch and log

64
00:05:25,452 --> 00:05:28,390
the exception of course, and on the additional details.

65
00:05:29,930 --> 00:05:32,760
And that would actually work.

66
00:05:34,090 --> 00:05:37,480
The main problem is that it takes a lot of time,

67
00:05:37,870 --> 00:05:41,146
it requires a lot of maintenance, and you still need

68
00:05:41,168 --> 00:05:44,666
to connect the dots. So when

69
00:05:44,688 --> 00:05:48,906
you have millions it can work. When you have like few thousand,

70
00:05:49,008 --> 00:05:52,978
let's say invocations a month, when you have millions or billions,

71
00:05:53,174 --> 00:05:56,606
you can no longer connect the dots via timestamps. So you can no

72
00:05:56,628 --> 00:06:00,894
longer see that I have one request being through in that second you

73
00:06:00,932 --> 00:06:04,718
have thousands going through. And how can you differentiate

74
00:06:04,734 --> 00:06:08,274
the different logs from these different executions? So that becomes a problem.

75
00:06:08,392 --> 00:06:12,126
Drilling down to the specific log groups and log stream

76
00:06:12,158 --> 00:06:16,422
to understand how

77
00:06:16,476 --> 00:06:20,194
this connects. So you have this data islands

78
00:06:20,322 --> 00:06:22,280
but you're not able to connect these.

79
00:06:24,330 --> 00:06:28,170
But it's good for many of the cases, especially for dev

80
00:06:28,240 --> 00:06:31,478
and early production, before you scale,

81
00:06:31,574 --> 00:06:34,682
it's out of the box. It's easy, relatively easy

82
00:06:34,736 --> 00:06:38,278
I would say, to get started. It's supported

83
00:06:38,294 --> 00:06:42,206
by AWS, it's in the same cloud or

84
00:06:42,228 --> 00:06:46,170
cloud vendor. I think the biggest challenge, it's complicated,

85
00:06:46,250 --> 00:06:50,270
it's time consuming to implement, it's time consuming to

86
00:06:50,340 --> 00:06:53,506
make sense of it. You need to really know how to

87
00:06:53,528 --> 00:06:55,540
configure it to get power,

88
00:06:57,910 --> 00:07:01,842
visibility. And the biggest challenge is there's no

89
00:07:01,896 --> 00:07:05,822
good way to do event correlation, to trace. To understand the bigger picture.

90
00:07:05,966 --> 00:07:09,414
There are tools like x ray for example within AWS that allow

91
00:07:09,452 --> 00:07:12,726
you to do that, but they're also very limited. So they're not

92
00:07:12,828 --> 00:07:16,360
going across DynamoDb and

93
00:07:16,970 --> 00:07:20,780
Sri and event bridge and the other things that we started talking about.

94
00:07:22,430 --> 00:07:26,086
And therefore it's hard to understand the business impact, what is a customer facing

95
00:07:26,118 --> 00:07:29,990
API, how critical that is. The second option is to implement

96
00:07:30,070 --> 00:07:33,838
homebrewed solutions so I can actually build

97
00:07:33,924 --> 00:07:37,102
a distributed tracing system or better

98
00:07:37,156 --> 00:07:40,990
off. I can use something that is open source and that's great

99
00:07:41,060 --> 00:07:44,594
because there are different frameworks for that you might

100
00:07:44,632 --> 00:07:47,410
want to read if you don't know about Zipkin, about Jaeger,

101
00:07:48,150 --> 00:07:51,250
and in general about the framework of open telemetry.

102
00:07:52,470 --> 00:07:55,940
And this provides you a very nice

103
00:07:56,870 --> 00:08:00,790
common ground for implementing distributed tracing

104
00:08:01,530 --> 00:08:04,966
and getting the information back to you in

105
00:08:04,988 --> 00:08:08,262
a visual way, seeing latency,

106
00:08:08,326 --> 00:08:10,890
breakdown of the environment, getting traces.

107
00:08:11,310 --> 00:08:15,014
So if you planning to implement

108
00:08:15,142 --> 00:08:18,774
distributed tracing, I suggest starting with open telemetry

109
00:08:18,822 --> 00:08:22,334
as a first point. And if

110
00:08:22,372 --> 00:08:26,042
you do that, I really suggest reading this, a consistent approach

111
00:08:26,106 --> 00:08:29,786
to track correlation ideas through microservices by Kui.

112
00:08:29,898 --> 00:08:33,346
If you're not following can Kui, that's a great time to start doing

113
00:08:33,368 --> 00:08:37,106
that. If you are interested about distributed tracing, about managed services,

114
00:08:37,208 --> 00:08:41,026
about serverless, he is for me

115
00:08:41,048 --> 00:08:44,626
the number one guy out there and he

116
00:08:44,648 --> 00:08:48,774
blogs a lot. He has great workshops also to consider and

117
00:08:48,812 --> 00:08:53,698
books. But to our point, he is blogging a lot about correlation

118
00:08:53,794 --> 00:08:57,174
tracing, distributed tracing. So this would be a very

119
00:08:57,212 --> 00:09:00,940
strong rate for that. I think the

120
00:09:01,310 --> 00:09:04,902
pros for a homebrew solution, it gets as tailored

121
00:09:05,046 --> 00:09:08,762
as you can possibly get because you build it so you will have

122
00:09:08,816 --> 00:09:11,950
all the different perks that you want to have.

123
00:09:12,020 --> 00:09:15,486
It's your solution, it will be the best fitted to

124
00:09:15,508 --> 00:09:19,454
your needs. Open telemetry is supported by

125
00:09:19,492 --> 00:09:23,698
many vendors, so that's great as a standard to use that

126
00:09:23,784 --> 00:09:27,106
and then you can base future engagements based

127
00:09:27,128 --> 00:09:30,834
on that. And it's not cloud specific, so you can oppose to

128
00:09:30,872 --> 00:09:36,194
Cloudwatch for example, so you can actually use

129
00:09:36,232 --> 00:09:39,320
it across clouds and move cloud in between.

130
00:09:40,330 --> 00:09:43,942
The main challenge is that you build it so it's tailor fit,

131
00:09:43,996 --> 00:09:47,894
but it's very high touch. It does not solve the problem of

132
00:09:47,932 --> 00:09:51,386
managed services. So if you need to trace across DynamoDB, you still need

133
00:09:51,408 --> 00:09:55,002
to figure a way to get a correlation id, a request id

134
00:09:55,056 --> 00:09:58,490
across to the other side, across s three, across event

135
00:09:58,560 --> 00:10:00,250
bridge across kinesis.

136
00:10:01,570 --> 00:10:05,840
So it doesn't tackle managed services and

137
00:10:06,450 --> 00:10:09,354
components that are not supported like API, Gateway,

138
00:10:09,402 --> 00:10:11,440
Dynamodb and others that we mentioned.

139
00:10:12,530 --> 00:10:16,222
Luckily there are several cloud

140
00:10:16,276 --> 00:10:19,826
native monitoring solutions out these that

141
00:10:20,008 --> 00:10:24,034
were built for the modern environment to solve that

142
00:10:24,072 --> 00:10:27,986
problem. Exactly. To solve the distributed

143
00:10:28,018 --> 00:10:31,800
tracing and observability monitoring troubleshooting of

144
00:10:32,250 --> 00:10:36,146
the modern cloud native architectures. They're distributed,

145
00:10:36,258 --> 00:10:39,930
they granular, and these are multiple services

146
00:10:40,000 --> 00:10:43,354
over there. And what you can expect

147
00:10:43,472 --> 00:10:45,910
is usually those are SaaS platforms,

148
00:10:45,990 --> 00:10:49,706
so they get the traces to their

149
00:10:49,728 --> 00:10:53,626
back end and they process it to generate the view for you. So that's

150
00:10:53,658 --> 00:10:59,646
great in terms of maintenance it's a SaaS at

151
00:10:59,668 --> 00:11:02,446
the same time, by the way, it's a SaaS, so you need to be able

152
00:11:02,468 --> 00:11:04,800
to send information to those services.

153
00:11:06,290 --> 00:11:09,710
That's also for you to know,

154
00:11:09,780 --> 00:11:13,170
especially around privacy. Most of the vendor has

155
00:11:13,240 --> 00:11:16,786
a very good policies in place for GDPR,

156
00:11:16,898 --> 00:11:20,454
for ISO 270,

157
00:11:20,492 --> 00:11:22,870
zero one, et cetera.

158
00:11:25,130 --> 00:11:29,074
Most of the vendors also solving a larger problem, not just the distributed

159
00:11:29,122 --> 00:11:33,066
tracing and showing you what happened, but as I mentioned, kind of

160
00:11:33,088 --> 00:11:36,662
building the virtual stack Trace. So getting the inputs, the outputs

161
00:11:36,726 --> 00:11:40,794
and everything you need to know, it's much more and cost analysis

162
00:11:40,842 --> 00:11:44,686
and latency breakdown, et cetera, much more than just a

163
00:11:44,708 --> 00:11:46,880
map of services.

164
00:11:47,650 --> 00:11:51,598
And usually they're using

165
00:11:51,684 --> 00:11:55,966
code libraries integrated in one way or another and an API

166
00:11:56,078 --> 00:12:00,098
using can im role. I think

167
00:12:00,184 --> 00:12:03,694
pros are, you'll find out that those tools

168
00:12:03,742 --> 00:12:06,886
are usually very opinionated. You come with a

169
00:12:06,908 --> 00:12:10,774
set of pre configured alerts that

170
00:12:10,812 --> 00:12:14,166
you should know about. That's what it

171
00:12:14,188 --> 00:12:17,990
means to be niche focused.

172
00:12:18,570 --> 00:12:22,998
They provide more than just tracing. So many times you'll

173
00:12:23,014 --> 00:12:26,202
be amazed of what you can get and you say, wow, I can get this.

174
00:12:26,256 --> 00:12:29,994
And I was just looking for tracing. And they're very low touch, very easy

175
00:12:30,032 --> 00:12:33,870
to get started. Usually like few minutes, ten minutes,

176
00:12:33,940 --> 00:12:37,902
15 minutes to get started and actually see what's going on within

177
00:12:37,956 --> 00:12:41,502
your environment. On the other side, this is yet

178
00:12:41,556 --> 00:12:44,846
another third party platform among

179
00:12:44,878 --> 00:12:48,434
the many others that you probably have. And they provide more

180
00:12:48,472 --> 00:12:52,786
than just tracing, meaning you

181
00:12:52,808 --> 00:12:55,814
do get additional data. You do have,

182
00:12:55,852 --> 00:12:59,942
I want to say, more layers to the tools that

183
00:13:00,076 --> 00:13:03,526
might be great, might be even beyond what

184
00:13:03,548 --> 00:13:07,400
you're looking for if you're at an early stage. So just to

185
00:13:08,270 --> 00:13:09,500
remember that,

186
00:13:12,350 --> 00:13:16,378
and this is where I want to take one tool,

187
00:13:16,544 --> 00:13:20,870
our tool, Lumigo, and share with you how we actually do that in Lumigo.

188
00:13:20,950 --> 00:13:25,134
I think the main point is to see and

189
00:13:25,172 --> 00:13:28,606
understand what you should be aiming for. And it doesn't matter if you

190
00:13:28,628 --> 00:13:32,206
implement this with Cloudwatch or you're using open telemetry or

191
00:13:32,228 --> 00:13:34,814
Jaeger or using Lumigo or other tool.

192
00:13:35,012 --> 00:13:38,386
I want to share what are the abilities if you do

193
00:13:38,408 --> 00:13:43,220
it right, or what you should expect from well

194
00:13:44,230 --> 00:13:47,766
observed system. So that's the main reason why

195
00:13:47,788 --> 00:13:51,080
I want to share how this can look like.

196
00:13:51,770 --> 00:13:55,634
And the main thing is that one, you're getting the monitoring,

197
00:13:55,682 --> 00:13:58,950
the alert, the things that tells you that everything

198
00:13:59,020 --> 00:14:02,394
okay or not and what's not okay. And then it allows you

199
00:14:02,432 --> 00:14:05,898
the ability to drill down and actually debug and troubleshoot to find the

200
00:14:05,904 --> 00:14:09,674
root cause. So let me very

201
00:14:09,712 --> 00:14:13,502
quickly share with you how that looks like

202
00:14:13,636 --> 00:14:17,150
with Lumigo. This is our dashboard. It takes

203
00:14:17,300 --> 00:14:20,554
literally five minutes and five clicks, no code changes required

204
00:14:20,602 --> 00:14:23,970
in order to get started, and you get value,

205
00:14:24,040 --> 00:14:27,314
get alerts about errors that you probably never knew you

206
00:14:27,352 --> 00:14:31,234
have. As I mentioned, the dashboard is

207
00:14:31,272 --> 00:14:35,258
focused on the alerts, on things that are focused

208
00:14:35,294 --> 00:14:38,630
on cloud native. So it's no longer about cpu or I O,

209
00:14:38,700 --> 00:14:43,110
et cetera. It's about the number of failed invocations.

210
00:14:43,610 --> 00:14:47,426
It's about cold starts. It's about show

211
00:14:47,468 --> 00:14:51,686
me the biggest latency offenders I have in dozens

212
00:14:51,718 --> 00:14:55,274
of services that I have, because that becomes really a big

213
00:14:55,312 --> 00:14:59,034
problem. Runaway cost,

214
00:14:59,152 --> 00:15:02,942
function duration, timeout, all of these are things that

215
00:15:03,076 --> 00:15:07,326
you get very easy view and alerts on with

216
00:15:07,348 --> 00:15:10,654
cloud native tools. But at the same

217
00:15:10,692 --> 00:15:14,558
time, let's take a scenario when something actually goes wrong. So let's

218
00:15:14,574 --> 00:15:18,580
look at our issues and we'll find an issue that

219
00:15:19,350 --> 00:15:22,434
is occurring. And let's suppose we'll get alert on this

220
00:15:22,472 --> 00:15:25,814
for slack for page of duty. But let's assume we

221
00:15:25,852 --> 00:15:29,922
want to dive into this. We click on this specific error

222
00:15:29,986 --> 00:15:31,320
that is happening.

223
00:15:32,650 --> 00:15:36,114
Last happened three minutes ago. This is actually a live environment

224
00:15:36,162 --> 00:15:39,286
that I'm showing that is based on

225
00:15:39,308 --> 00:15:42,806
a cloud native architecture in AWS. And when we drill

226
00:15:42,838 --> 00:15:46,186
down, we can see a lot of information about that error. It's in

227
00:15:46,208 --> 00:15:49,894
a specific lambda. I can see that there was a deployment

228
00:15:49,942 --> 00:15:53,374
over here. I can see number of failures, and I can see that one by

229
00:15:53,412 --> 00:15:57,600
one, the actual failures that happen.

230
00:15:58,370 --> 00:16:02,046
And this is were we actually move to troubleshoot. So if

231
00:16:02,068 --> 00:16:05,858
we click on any of the invocation we actually starting,

232
00:16:05,944 --> 00:16:08,050
I like to call it a debugging session,

233
00:16:08,790 --> 00:16:12,130
which is how I mentioned

234
00:16:12,200 --> 00:16:15,554
about the virtual stack trace. This is where you actually can start

235
00:16:15,592 --> 00:16:19,062
looking at these virtual stacked trace. So what do we have over

236
00:16:19,116 --> 00:16:23,078
here? You can see this lambda, this is why we got here.

237
00:16:23,164 --> 00:16:26,662
This post to social failed. And you can see this event

238
00:16:26,716 --> 00:16:30,314
bridge. This is the service that triggered that

239
00:16:30,352 --> 00:16:34,042
lambda. As I mentioned, we want to see the

240
00:16:34,176 --> 00:16:37,766
full transaction, the end to end transaction show. So I'll ask Lumigo

241
00:16:37,798 --> 00:16:41,154
to calculate and go back and upstream

242
00:16:41,222 --> 00:16:45,098
and build the entire request from the very beginning,

243
00:16:45,194 --> 00:16:47,706
all the way to the different nodes.

244
00:16:47,898 --> 00:16:51,310
And this is what Lumigo built over here. And this

245
00:16:51,380 --> 00:16:55,646
is the core of what you should be targeting,

246
00:16:55,758 --> 00:16:59,054
having a direct view going from a failure,

247
00:16:59,102 --> 00:17:02,834
an internal failure, a dynamodb that failed, or lambda whatever, and then

248
00:17:02,872 --> 00:17:06,594
immediately being able to zoom in and understand, okay,

249
00:17:06,712 --> 00:17:10,722
this is a customer facing API. It's an upride.

250
00:17:10,786 --> 00:17:13,862
I can tell you it's a business critical API. So I need to fix it

251
00:17:13,916 --> 00:17:17,480
now and at the same time to be able to

252
00:17:18,190 --> 00:17:21,754
drill down and understand. Okay, let's click on this.

253
00:17:21,952 --> 00:17:25,514
And this is the added layer that I mentioned that I

254
00:17:25,552 --> 00:17:29,062
don't just get a map,

255
00:17:29,206 --> 00:17:32,686
I actually can click on any service and see a lot

256
00:17:32,708 --> 00:17:36,366
of information like the issue,

257
00:17:36,548 --> 00:17:40,474
the actual stacked race and the exception

258
00:17:40,522 --> 00:17:44,574
variables, the event that triggered the lambda, these environment

259
00:17:44,622 --> 00:17:47,698
variables, the logs, everything that has to

260
00:17:47,704 --> 00:17:51,220
do with this invocation, these are things that are generated by

261
00:17:52,550 --> 00:17:56,406
the vendor, in this case Lumigo, and most of them

262
00:17:56,588 --> 00:17:59,846
do not exist in AWS or in

263
00:17:59,868 --> 00:18:03,734
other regular tools. So in this

264
00:18:03,772 --> 00:18:07,506
case, details write id cannot be an empty string, that's a

265
00:18:07,548 --> 00:18:11,642
failure. And if I look at these event and

266
00:18:11,696 --> 00:18:15,674
I click to understand what these message actually that

267
00:18:15,792 --> 00:18:20,266
the lambda got, I can see that details write id was empty

268
00:18:20,298 --> 00:18:23,440
to begin with. So the lambda got an empty write id.

269
00:18:24,210 --> 00:18:28,062
Just by having that visibility that you get only

270
00:18:28,116 --> 00:18:32,030
in tools that are focused on cloud native applications.

271
00:18:32,530 --> 00:18:36,274
Just by seeing that, now I know that this

272
00:18:36,312 --> 00:18:40,162
lambda is not a problem. I need to understand why

273
00:18:40,216 --> 00:18:43,486
is this empty? Where is this coming from? So let's

274
00:18:43,518 --> 00:18:47,046
go upstream and let's go to the event bridge and we'll click on

275
00:18:47,068 --> 00:18:51,650
this. And now we can dive

276
00:18:51,730 --> 00:18:55,814
into the property that

277
00:18:55,852 --> 00:18:59,222
it got in the message and we can see that details run id

278
00:18:59,276 --> 00:19:02,310
is naturally empty also in eventbridge.

279
00:19:02,470 --> 00:19:05,946
So we can go upstream and look at the lambda that triggered that and

280
00:19:05,968 --> 00:19:09,338
we can go one by one and check all the different

281
00:19:09,504 --> 00:19:13,294
services, including things like dynamodb. What did you try

282
00:19:13,332 --> 00:19:16,938
to write? What was the outcome?

283
00:19:17,114 --> 00:19:20,714
In this case there's a failure. The provided key element

284
00:19:20,762 --> 00:19:24,370
does not match these scheme. There was a retry probably. So I see the second

285
00:19:24,440 --> 00:19:27,746
call was successful. So it really tells you the

286
00:19:27,768 --> 00:19:31,042
story of what happened in each and every

287
00:19:31,176 --> 00:19:35,540
service along the way, all the way to things like Twilio for example,

288
00:19:36,070 --> 00:19:40,482
that you can actually see. SMS was sent to this number and the response

289
00:19:40,546 --> 00:19:44,086
and so on. At the same time, you can also check

290
00:19:44,108 --> 00:19:47,686
this out in a timeline view. So to see if there are any

291
00:19:47,868 --> 00:19:50,860
latency issues, to see if there are any.

292
00:19:51,790 --> 00:19:55,882
You can see this is taking a second, so maybe

293
00:19:55,936 --> 00:19:59,466
I need to dive deeper and understanding what's taking the time and so on and

294
00:19:59,488 --> 00:20:02,560
so forth in a latency view.

295
00:20:03,570 --> 00:20:07,454
I want to stop over here. And again,

296
00:20:07,492 --> 00:20:11,822
this is just to give you the context of what you should expect from

297
00:20:11,956 --> 00:20:15,470
a modern distributed tracing that is focused on cloud native.

298
00:20:15,630 --> 00:20:19,074
To summarize, serverless and

299
00:20:19,112 --> 00:20:23,010
managed services in general really

300
00:20:23,080 --> 00:20:26,466
changes the way we develop, really changes the way we're

301
00:20:26,498 --> 00:20:29,480
doing things. There are many strong benefits.

302
00:20:29,930 --> 00:20:34,818
We didn't touch on that, a lot of accelerated development,

303
00:20:34,994 --> 00:20:38,294
but there are some new challenges, especially around visibility

304
00:20:38,422 --> 00:20:41,660
and troubleshooting of an application.

305
00:20:42,350 --> 00:20:46,518
We talked about three approaches to monitoring and troubleshooting

306
00:20:46,614 --> 00:20:49,926
distributed services. We talked about cloud native tools,

307
00:20:49,958 --> 00:20:52,902
we talked about homebrewed and open source solution.

308
00:20:52,966 --> 00:20:55,946
We talked about third party SaaS vendors.

309
00:20:56,138 --> 00:20:59,246
I think two,

310
00:20:59,268 --> 00:21:02,554
three main things that I wanted you to leave these session with.

311
00:21:02,612 --> 00:21:06,386
One is I think you saw what

312
00:21:06,408 --> 00:21:10,340
you should expect in these modern environment. Don't settle for

313
00:21:10,950 --> 00:21:14,590
what you used to have with having logs

314
00:21:14,750 --> 00:21:18,326
out there in a log aggregator and that's it.

315
00:21:18,508 --> 00:21:22,006
You can expect more. There are better tools, better technology

316
00:21:22,108 --> 00:21:25,506
to serve you. Number two, think about this upfront.

317
00:21:25,698 --> 00:21:28,986
It's much better to bake it in during dev,

318
00:21:29,088 --> 00:21:32,922
during preprod rather than after the fact. And third,

319
00:21:32,976 --> 00:21:37,002
consult. There are a lot of companies that are going through what you

320
00:21:37,056 --> 00:21:39,970
are going through or already have solutions.

321
00:21:40,150 --> 00:21:44,014
So consult with the community. There's a lot of resources and from

322
00:21:44,052 --> 00:21:48,654
my experience everybody is really happy to help and

323
00:21:48,692 --> 00:21:52,622
in that thought I also want to offer my

324
00:21:52,676 --> 00:21:56,800
help. I really enjoy talking

325
00:21:58,050 --> 00:22:03,966
with folks in our community and hearing about new project and new application

326
00:22:04,068 --> 00:22:07,798
being developed. So feel free. Even if you're not

327
00:22:07,884 --> 00:22:11,250
using Lumigo, even if it's just about managed services, distributed tracing

328
00:22:11,330 --> 00:22:15,190
observability to reach out. I'm available

329
00:22:15,260 --> 00:22:18,962
on this email or direct message me on Twitter

330
00:22:19,106 --> 00:22:22,662
and would love to try and do our best to help.

331
00:22:22,716 --> 00:22:25,330
Thank you very much and enjoy these rest of the conference.

