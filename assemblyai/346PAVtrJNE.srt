1
00:00:23,610 --> 00:00:27,350
Hi, I'm Timothy Span. My talk today

2
00:00:27,420 --> 00:00:30,902
is building real time applications with

3
00:00:30,956 --> 00:00:34,594
Pulsar running on kubernetes. I'm a developer

4
00:00:34,642 --> 00:00:38,486
advocate at Stream native. If you want

5
00:00:38,508 --> 00:00:41,990
more information, you could scan my QR code there. I've been working

6
00:00:42,060 --> 00:00:45,970
on the flip or Flippin stack that lets you easily

7
00:00:46,050 --> 00:00:49,622
build streaming applications for modern cloud

8
00:00:49,676 --> 00:00:52,926
and environments. Clearly kubernetes has to

9
00:00:52,948 --> 00:00:55,902
be a big part of that to be able to scale up, scale down,

10
00:00:56,036 --> 00:00:59,646
deploy things with ease on all different types of clouds and

11
00:00:59,668 --> 00:01:03,198
environments. I've been working with big data and streaming for

12
00:01:03,204 --> 00:01:06,180
a number of years with a number of different companies.

13
00:01:06,950 --> 00:01:09,886
If you want to keep up with me, I have a weekly newsletter.

14
00:01:09,998 --> 00:01:12,900
Lots of cool stuff there. Definitely check it out.

15
00:01:14,070 --> 00:01:17,430
Stream native is the company behind Pulsar

16
00:01:18,330 --> 00:01:22,338
and we help people with that, whether they're running in our kubernetes,

17
00:01:22,434 --> 00:01:25,990
in our cloud environment, or you need help on your own environment.

18
00:01:27,050 --> 00:01:31,078
Reach out to us. So Pulsar,

19
00:01:31,174 --> 00:01:34,746
as opposed to some other streaming and messaging system,

20
00:01:34,928 --> 00:01:38,860
was from the very beginning designed to

21
00:01:39,230 --> 00:01:43,034
run natively in the cloud and in containers,

22
00:01:43,162 --> 00:01:46,830
as it was designed with a separation of concerns between

23
00:01:46,980 --> 00:01:50,720
the different components in this streaming data platform.

24
00:01:51,330 --> 00:01:54,670
So that gives us support for hybrid and multicloud.

25
00:01:54,830 --> 00:01:58,222
Obviously all the different types of Kubernetes containers,

26
00:01:58,366 --> 00:02:02,114
great fit for microservices and designed to run

27
00:02:02,312 --> 00:02:07,506
in your cloud natively without any kind of squeezing

28
00:02:07,538 --> 00:02:10,966
to fit it in there. To show you how we run Pulsar and

29
00:02:10,988 --> 00:02:15,202
Kafka within K eight, you could see the infrastructure

30
00:02:15,266 --> 00:02:18,346
here a little bit. We've got protocol handlers to

31
00:02:18,368 --> 00:02:21,750
handle different messaging protocols such as Pulsar,

32
00:02:21,910 --> 00:02:25,740
KAfka, MQTT, AMQP 91,

33
00:02:26,270 --> 00:02:29,754
and those just plug and play as you want, pretty easy.

34
00:02:29,952 --> 00:02:33,182
Those go into topics, those are handled by

35
00:02:33,236 --> 00:02:37,054
the pulsar brokers, which scale up separately from the rest

36
00:02:37,092 --> 00:02:40,190
of the platform. That's how you interact, send a message,

37
00:02:40,260 --> 00:02:43,850
get a message, and what's nice there is, it's stateless.

38
00:02:43,930 --> 00:02:47,506
So very easy to scale those up and down based on the number of

39
00:02:47,528 --> 00:02:50,126
messages, based on the number of clients.

40
00:02:50,318 --> 00:02:54,686
Very dynamic. And then outside of that, we have Apache bookkeeper

41
00:02:54,718 --> 00:02:58,262
bookie nodes for storage. And we also

42
00:02:58,316 --> 00:03:01,894
tier out the tiered storage, as you see here, where you see

43
00:03:01,932 --> 00:03:05,190
Zookeeper. We now have expanded that to support

44
00:03:05,260 --> 00:03:08,746
Etsyd to make it easier to run in kubernetes, so you don't

45
00:03:08,768 --> 00:03:12,918
have to run another tool since you have Etsy

46
00:03:13,014 --> 00:03:16,582
D anyway. So under the covers

47
00:03:16,726 --> 00:03:20,154
we provide pulsar operators to make this

48
00:03:20,192 --> 00:03:23,546
easy to run it wherever it may be, and we have

49
00:03:23,568 --> 00:03:27,598
experience running that on premise public cloud, private cloud, wherever you

50
00:03:27,604 --> 00:03:32,090
want to run this. And just to give you an idea of the different pieces,

51
00:03:32,250 --> 00:03:36,302
a lot of things that are pretty standard for most Kubernetes

52
00:03:36,366 --> 00:03:39,918
native apps around Grafana, Prometheus,

53
00:03:40,094 --> 00:03:43,602
nothing going to be too different from other

54
00:03:43,656 --> 00:03:47,302
things that you're running. We have full command line

55
00:03:47,356 --> 00:03:50,742
tools to interact with what you need to. That's not part of the

56
00:03:50,796 --> 00:03:54,086
standard Kubernetes controls as well as

57
00:03:54,108 --> 00:03:57,666
a full rest endpoints for all the admin and

58
00:03:57,708 --> 00:04:01,354
functionality that you need. So you could easily use

59
00:04:01,392 --> 00:04:04,506
any kind of DevOps tool. Extrapolate that out from

60
00:04:04,528 --> 00:04:08,234
what you're doing elsewhere and this breaks that

61
00:04:08,272 --> 00:04:12,074
down in a pretty simple visual way.

62
00:04:12,192 --> 00:04:16,170
We've got those pulsar brokers, we do the routing, we do the connections.

63
00:04:16,330 --> 00:04:19,662
We have a little cache but you don't have big storage there.

64
00:04:19,716 --> 00:04:23,170
Thankfully this helps with the automatic load balancing

65
00:04:23,590 --> 00:04:27,074
and we break down those topics into segments to make it into

66
00:04:27,192 --> 00:04:31,534
more manageable sized chunks. All metadata

67
00:04:31,582 --> 00:04:34,370
service Discovery things you need in a cluster,

68
00:04:34,710 --> 00:04:38,600
we store that in a metadata store.

69
00:04:39,050 --> 00:04:43,174
This is an API that lets us add more. ETCD is

70
00:04:43,212 --> 00:04:47,110
probably the preferred one for Kubernetes environment.

71
00:04:47,470 --> 00:04:50,966
Bookkeeper also uses that same metadata

72
00:04:50,998 --> 00:04:54,540
store for metadata and service discovery. As you might expect,

73
00:04:54,990 --> 00:04:58,746
Pulsar sends those messages to bookkeeper where

74
00:04:58,768 --> 00:05:01,482
they're started and managed.

75
00:05:01,626 --> 00:05:04,974
And if they need to be tiered to tiered storage, that'll come

76
00:05:05,012 --> 00:05:08,366
out of there underneath the

77
00:05:08,388 --> 00:05:12,442
covers. When you're running this whole thing in your environment,

78
00:05:12,506 --> 00:05:16,146
however many Kubernetes clusters you may have, we've got

79
00:05:16,168 --> 00:05:20,014
this metadata quorum in there, we've got these cluster

80
00:05:20,062 --> 00:05:24,142
of brokers and we've got what we call a bookie ensemble

81
00:05:24,286 --> 00:05:27,654
which is a number of these data nodes. We keep

82
00:05:27,692 --> 00:05:30,982
that within one Kubernetes cluster would make sense.

83
00:05:31,116 --> 00:05:34,614
And then your global metadata store is probably

84
00:05:34,652 --> 00:05:38,326
your eTCD that's sitting in

85
00:05:38,348 --> 00:05:41,420
your Kubernetes environment. Just to keep that simple.

86
00:05:42,270 --> 00:05:45,814
Why do we have all this? Well, Pulsar lets us produce messages

87
00:05:45,862 --> 00:05:49,382
in, consume them out. This is a great way to wire

88
00:05:49,446 --> 00:05:53,514
together different Kubernetes applications, wireless stateless

89
00:05:53,562 --> 00:05:57,722
functions, things like if you want to do your own AWS,

90
00:05:57,786 --> 00:06:01,674
lambda type applications in a robust

91
00:06:01,722 --> 00:06:04,850
open source environment makes it easy to do that.

92
00:06:05,000 --> 00:06:08,686
It really is a nice way to asynchronously decouple

93
00:06:08,798 --> 00:06:12,834
different applications regardless of versions of

94
00:06:12,872 --> 00:06:16,402
things, operating system programming language,

95
00:06:16,546 --> 00:06:20,150
what type of apps, what type of protocols, what's the final

96
00:06:20,220 --> 00:06:23,842
destination for your data. Very straightforward

97
00:06:23,906 --> 00:06:27,974
for you to do that. And this is a very scalable environment

98
00:06:28,022 --> 00:06:31,322
and we mean scale. It's not just scale out, but sometimes

99
00:06:31,376 --> 00:06:34,454
you got to scale down based on workloads.

100
00:06:34,582 --> 00:06:38,666
What's nice with pulsar is topics that

101
00:06:38,768 --> 00:06:42,234
have your messaging data can easily be distributed

102
00:06:42,282 --> 00:06:46,014
and moved without you having to hand do things or bring down

103
00:06:46,052 --> 00:06:49,294
nodes. We could do this live and it'll be

104
00:06:49,332 --> 00:06:53,118
handled for you by the platform. And you're

105
00:06:53,134 --> 00:06:56,402
not concerned about that. You need to bring

106
00:06:56,456 --> 00:07:00,574
down a cluster or shrink it down. Very easy. Could set an offloading

107
00:07:00,622 --> 00:07:05,354
schedule. So whatever disk you might have locally

108
00:07:05,422 --> 00:07:09,074
or within Kubernetes storage can be offloaded

109
00:07:09,122 --> 00:07:12,546
to s three, HDFs, Azure,

110
00:07:12,658 --> 00:07:16,102
wherever you might have your storage there, minio anything

111
00:07:16,156 --> 00:07:19,020
that's s three compatible. Pretty easy.

112
00:07:19,550 --> 00:07:23,510
Now how do I build these apps? So I've got a Kubernetes

113
00:07:23,590 --> 00:07:27,386
based messaging streaming platform and

114
00:07:27,408 --> 00:07:30,818
I've got a way to deploy it store my messages.

115
00:07:31,014 --> 00:07:34,026
We also provide pulsar functions.

116
00:07:34,218 --> 00:07:37,710
These currently support Java, Python and go

117
00:07:37,860 --> 00:07:41,214
as your language of developing these functions. Can use any third

118
00:07:41,252 --> 00:07:44,862
party libraries that make sense with those languages

119
00:07:45,006 --> 00:07:49,054
and lets you build asynchronous microservices

120
00:07:49,182 --> 00:07:53,166
the easy way and deployed on Kubernetes.

121
00:07:53,358 --> 00:07:56,582
So with configuration that cloud be done from

122
00:07:56,636 --> 00:08:00,930
a command line tool, a rest endpoint or automated

123
00:08:01,090 --> 00:08:05,510
via code. I apply a number of input topics

124
00:08:05,850 --> 00:08:09,050
to this function that I have and specify

125
00:08:09,790 --> 00:08:13,450
an output topic perhaps, and a log topic perhaps.

126
00:08:14,190 --> 00:08:17,946
What's nice is within the function I can

127
00:08:18,128 --> 00:08:21,642
dynamically create new topics based on what the data

128
00:08:21,696 --> 00:08:25,374
looks like, what the data feed is, or whatever needs I have.

129
00:08:25,492 --> 00:08:29,070
So this is not in any way hard coded to a function.

130
00:08:29,220 --> 00:08:32,254
You could change this dynamically, which is great.

131
00:08:32,292 --> 00:08:36,114
When you're deploying these in different environments, you could always add

132
00:08:36,152 --> 00:08:40,194
another topic as an input. So I have a new set of data that wants

133
00:08:40,232 --> 00:08:43,634
to use my spell checking function. Just add

134
00:08:43,672 --> 00:08:47,026
it. One that does etl any bit of code you

135
00:08:47,048 --> 00:08:50,374
might have in Java or Python ago. Easy to put it there

136
00:08:50,412 --> 00:08:54,050
and have it get every message that comes into those topics

137
00:08:54,130 --> 00:08:57,538
event at a time, process them and do with what

138
00:08:57,564 --> 00:09:00,746
you want with it. Pretty easy to do. Great way

139
00:09:00,768 --> 00:09:04,698
to do your computation isolated from the standard

140
00:09:04,784 --> 00:09:08,554
cluster in the same kubernetes cluster or

141
00:09:08,592 --> 00:09:12,414
a separate one. We have a function mesh that makes it easy to

142
00:09:12,452 --> 00:09:15,662
run as many of these functions as you want and

143
00:09:15,716 --> 00:09:19,726
connect them together in a streaming pipeline. Now this

144
00:09:19,748 --> 00:09:23,239
is an example of the functions you don't have to use a

145
00:09:23,739 --> 00:09:27,726
lot of boilerplate pulsar code, not very specific to pulsar,

146
00:09:27,838 --> 00:09:31,060
but this is an example of a Python function.

147
00:09:31,910 --> 00:09:35,966
Very little you have to do here. I'm importing that function from pulsar,

148
00:09:36,078 --> 00:09:39,670
so I got to have the pulsar client create a class in Python,

149
00:09:41,050 --> 00:09:44,710
define my init there. And this is my major function

150
00:09:44,780 --> 00:09:48,358
here with the process. Self input is the input

151
00:09:48,374 --> 00:09:52,102
you're getting from whatever event. And context is an environment

152
00:09:52,166 --> 00:09:55,914
that you get from pulsar that can let you

153
00:09:56,032 --> 00:09:59,750
create a new topic, send something to a topic,

154
00:09:59,830 --> 00:10:03,340
get access to the logs, get access to

155
00:10:04,190 --> 00:10:07,774
shared data storage. Couple of different things you could do

156
00:10:07,812 --> 00:10:12,278
there. Very helpful. In this example, I just take that input

157
00:10:12,474 --> 00:10:16,194
that gets sent in from the event, process that to

158
00:10:16,232 --> 00:10:19,858
a sentiment analysis, and then just return

159
00:10:19,944 --> 00:10:24,414
a json. As you see here, nothing pretty tied

160
00:10:24,462 --> 00:10:27,794
to pulsar. This will just go on to whatever output

161
00:10:27,842 --> 00:10:31,734
topic you sent. If you wanted this to be routed to different topics, you'd use

162
00:10:31,772 --> 00:10:35,350
the context to do that. Pretty straightforward.

163
00:10:35,930 --> 00:10:39,686
So our function mesh lets you run these pulsar functions,

164
00:10:39,798 --> 00:10:43,194
and this was designed for kubernetes and

165
00:10:43,232 --> 00:10:46,966
it's a great way to put these together in a pipeline.

166
00:10:47,078 --> 00:10:50,880
One thing I didn't mention with pulsar functions is

167
00:10:51,410 --> 00:10:54,734
the dog fooding aspect. We have

168
00:10:54,772 --> 00:10:58,698
used these pulsar functions to build connectors

169
00:10:58,874 --> 00:11:02,666
for people to use in the platform. So there's

170
00:11:02,698 --> 00:11:05,906
ones created for Mongo, MySql, Kafka, tons of

171
00:11:05,928 --> 00:11:09,394
different things. And there's source and sync ones. What you do

172
00:11:09,432 --> 00:11:12,850
is point it to your database or whatever

173
00:11:12,920 --> 00:11:16,278
you have there, put in any criteria it needs.

174
00:11:16,444 --> 00:11:20,678
That just goes in a Yaml file, everyone's favorite. And then

175
00:11:20,844 --> 00:11:24,386
that gets deployed as a graph

176
00:11:24,578 --> 00:11:28,486
of connections here between all these functions you might have in a pipeline

177
00:11:28,598 --> 00:11:31,286
and that gets deployed to the function mesh.

178
00:11:31,478 --> 00:11:37,114
Pretty straightforward, as you see here. When we do a

179
00:11:37,152 --> 00:11:40,622
Kubernetes deploy of pulsar, we provide

180
00:11:40,676 --> 00:11:44,462
you with a couple of yamls that you cloud specify. Again, that bottom

181
00:11:44,516 --> 00:11:48,334
one's the function mesh that defines how we run

182
00:11:48,372 --> 00:11:51,914
these functions for you. Kubecontrol,

183
00:11:51,962 --> 00:11:55,506
everybody's favorite Kubernetes API. We've got the

184
00:11:55,608 --> 00:11:59,922
function mesh operators out there to deploy that within the

185
00:11:59,976 --> 00:12:03,554
cluster and that will

186
00:12:03,592 --> 00:12:06,918
connect to the pulsar cluster, which is probably an

187
00:12:07,084 --> 00:12:10,838
adjacent Kubernetes cluster to keep those

188
00:12:10,924 --> 00:12:14,130
separations of concerns there and scale up separately

189
00:12:14,290 --> 00:12:17,926
so you can have compute completely isolated from any of the

190
00:12:17,948 --> 00:12:22,518
data there. This makes it great for doing things like event sourcing

191
00:12:22,694 --> 00:12:26,426
or any high bandwidth workload you might have coming off

192
00:12:26,448 --> 00:12:29,798
of all these events. I have a lot of

193
00:12:29,904 --> 00:12:34,094
pulsar resources available for you. There's my contact

194
00:12:34,212 --> 00:12:38,010
information. I love chatting about streaming Pulsar,

195
00:12:38,170 --> 00:12:42,014
and if you've got any ideas for improving how

196
00:12:42,052 --> 00:12:45,326
to do this in maybe your own Kubernetes environment,

197
00:12:45,438 --> 00:12:48,994
or if you've got other tools that you think might

198
00:12:49,032 --> 00:12:52,900
be of assistance, please contact me.

199
00:12:53,670 --> 00:12:57,526
This dinosaur here, scan it and you'll get right there. Or here

200
00:12:57,548 --> 00:13:01,190
is the link within GitHub. Pretty straightforward.

201
00:13:01,530 --> 00:13:04,582
I have a couple minutes left, so I want to show you

202
00:13:04,716 --> 00:13:08,086
why you might want to use Pulsar for things.

203
00:13:08,268 --> 00:13:11,722
This is a very simple app run in one

204
00:13:11,776 --> 00:13:15,274
pod. It's just Python running an HTML page,

205
00:13:15,392 --> 00:13:19,110
but that HTML page has some Javascript,

206
00:13:19,190 --> 00:13:22,718
which makes a websocket call to Pulsar. So make sure you got

207
00:13:22,724 --> 00:13:26,206
to have all those ports opened up and that will get the

208
00:13:26,228 --> 00:13:29,934
data dynamically. So it consumes the

209
00:13:29,972 --> 00:13:34,810
data from the API and we're able to just

210
00:13:34,980 --> 00:13:38,386
stream that out as it comes in. We also have

211
00:13:38,408 --> 00:13:41,554
a management console that could sit outside of the

212
00:13:41,592 --> 00:13:45,138
environment. Very easy for you to use that. This is completely

213
00:13:45,224 --> 00:13:48,818
open source and it'll let you manage a number of clusters,

214
00:13:48,994 --> 00:13:52,434
all your tenants, namespaces and topics within the environment.

215
00:13:52,562 --> 00:13:55,910
Pulsar's multi tenant, which is great for having

216
00:13:55,980 --> 00:13:59,546
one people use that one cluster you have, regardless of

217
00:13:59,568 --> 00:14:02,646
their use cases, create a couple of tenants,

218
00:14:02,838 --> 00:14:06,346
maybe one's for Kafka users, one's for the

219
00:14:06,368 --> 00:14:10,054
public, whatever ones make sense. And underneath

220
00:14:10,102 --> 00:14:13,582
there you'll go to all the namespaces that make

221
00:14:13,636 --> 00:14:17,294
sense for that environment. And under there

222
00:14:17,332 --> 00:14:20,778
is where we have the topics related to that namespace.

223
00:14:20,874 --> 00:14:23,982
And you cloud have as many of these as you want, get access

224
00:14:24,036 --> 00:14:27,458
to all the topics, create new ones, delete ones. It's a

225
00:14:27,464 --> 00:14:31,182
nice little admin tool. If you want to see some example functions,

226
00:14:31,246 --> 00:14:34,370
I have them out there in my GitHub.

227
00:14:35,270 --> 00:14:39,106
One of them we'll dive into real quick is one I have for weather,

228
00:14:39,218 --> 00:14:42,454
which you saw that chart there. What this does,

229
00:14:42,492 --> 00:14:46,130
this is obviously Javas is a little more verbose than we're

230
00:14:46,290 --> 00:14:49,746
liking in other languages, but it implements a

231
00:14:49,788 --> 00:14:53,594
function, and again, that's that pulsar library there.

232
00:14:53,712 --> 00:14:56,934
And we say what is the input type, what's the output

233
00:14:56,982 --> 00:15:00,762
type? And we're just going to take raw bytes, the default format

234
00:15:00,826 --> 00:15:03,886
for pulsar messages, if you haven't specified a

235
00:15:03,908 --> 00:15:07,502
type or schema. And then we apply that

236
00:15:07,556 --> 00:15:11,402
context we get from Pulsar, and that context

237
00:15:11,466 --> 00:15:14,658
lets us get a logger. And it also does this most

238
00:15:14,744 --> 00:15:18,210
important feature here. Let me dynamically

239
00:15:18,630 --> 00:15:21,810
send a message. And at that same

240
00:15:21,880 --> 00:15:25,874
time, if I built a brand new topic, if the topic that

241
00:15:25,912 --> 00:15:29,074
I applied this message to doesn't

242
00:15:29,122 --> 00:15:33,046
exist. See that topic here? It will create it for us as

243
00:15:33,068 --> 00:15:36,306
long as you've set the security for that. Add any metadata

244
00:15:36,338 --> 00:15:40,306
properties I want to add to that message, add a key, easy to track

245
00:15:40,428 --> 00:15:43,802
and send it and we are away and I could send

246
00:15:43,856 --> 00:15:47,526
this dynamically to as many places as you want. Here I'm

247
00:15:47,558 --> 00:15:50,998
using a schema that I built from just a standard java

248
00:15:51,014 --> 00:15:54,666
bean if you notice here persistent. We could

249
00:15:54,688 --> 00:15:57,946
have non persistent messages if that makes sense in your environment.

250
00:15:58,058 --> 00:16:01,674
And here I've got my tenant, my namespace, my topic.

251
00:16:01,802 --> 00:16:05,760
That's as easy as it is. You saw how we do that in

252
00:16:06,130 --> 00:16:09,422
python, really easy. There is a lot of

253
00:16:09,476 --> 00:16:13,082
different connectors in and out of pulsar.

254
00:16:13,226 --> 00:16:16,134
Makes it very easy for you to take in data,

255
00:16:16,332 --> 00:16:19,990
scale it out in a real time messaging environment.

256
00:16:20,410 --> 00:16:23,830
I've been Tim Spann thank you for joining me today.

257
00:16:23,980 --> 00:16:25,460
Have fun with the rest of your talk.

