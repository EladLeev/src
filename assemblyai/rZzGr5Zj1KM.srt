1
00:02:14,770 --> 00:02:17,994
Hello and welcome. My name is Michael and I'm a site reliability

2
00:02:18,042 --> 00:02:21,438
engineer at Teleport and in today's talk we'll be talking about the

3
00:02:21,444 --> 00:02:25,086
four golden signals, what they are and how you can use them to create a

4
00:02:25,108 --> 00:02:28,446
solid foundation to monitor the health of your service. We'll also

5
00:02:28,468 --> 00:02:31,594
take what we've learnt today and apply it to a small service that I've written

6
00:02:31,642 --> 00:02:33,630
and monitor it using Grafana.

7
00:02:34,770 --> 00:02:38,242
Let's dive in, you as site reliability

8
00:02:38,306 --> 00:02:41,606
engineers, it's our mission to ensure our services are highly available,

9
00:02:41,708 --> 00:02:45,046
secure and scalable, and a big part of how this is achieved is

10
00:02:45,068 --> 00:02:48,278
through monitoring. But let's take a step back and explore why is it

11
00:02:48,284 --> 00:02:51,318
that we monitor in the first place? Alerting and troubleshooting are

12
00:02:51,324 --> 00:02:54,186
obvious ones. If something is broken, I want to know about it as soon as

13
00:02:54,208 --> 00:02:57,626
possible so I could fix it. And I'm also going to need to see my

14
00:02:57,648 --> 00:03:01,498
key metrics graph so I can pinpoint where the issue is during troubleshooting. But it

15
00:03:01,504 --> 00:03:04,554
can also help us in other parts of our job too. It allows us

16
00:03:04,592 --> 00:03:08,458
to understand our trends and answer questions like how quickly are my daily active

17
00:03:08,474 --> 00:03:12,414
users growing? Or how big is my database getting over time? The answer

18
00:03:12,452 --> 00:03:16,030
to these questions could be key in things such as capacity planning.

19
00:03:16,390 --> 00:03:19,554
We also use monitoring to understand the impact between changes.

20
00:03:19,752 --> 00:03:23,182
How much lower is my latency after this latest software rollout?

21
00:03:23,326 --> 00:03:27,406
Did we hit a regression with performance? It can even be helpful for experiments

22
00:03:27,438 --> 00:03:30,770
as well, where a software change is rolled out to a subset of our users.

23
00:03:30,930 --> 00:03:34,306
In this case, I'd want to understand was this change beneficial?

24
00:03:34,498 --> 00:03:37,734
Perhaps I'm running an ecommerce website and the change resulted in people

25
00:03:37,772 --> 00:03:41,474
finding products more easily or more quickly. Ultimately,

26
00:03:41,602 --> 00:03:44,250
we need to think of monitoring as a way to view the system's health,

27
00:03:44,320 --> 00:03:48,374
where anyone that supports the service can have a single source to determine the overall

28
00:03:48,422 --> 00:03:50,170
performance and availability.

29
00:03:52,910 --> 00:03:55,942
When I started my career ten means ago as a system administrator,

30
00:03:56,006 --> 00:03:59,086
I'd used Nagios to collect dozens of metrics across all of my

31
00:03:59,108 --> 00:04:03,722
hosts, from cpu usage to how many inodes were being utilized,

32
00:04:03,866 --> 00:04:07,306
and I'd get patient in the middle of the night for any one of them.

33
00:04:07,428 --> 00:04:11,138
But there wasn't always a clear understanding of what the impact of each metric was,

34
00:04:11,304 --> 00:04:13,780
or whether it was even an issue in the first place,

35
00:04:14,230 --> 00:04:17,758
say an in memory database using 70% of its available ram.

36
00:04:17,854 --> 00:04:21,158
Is this an issue? Perhaps not, but maybe it is if it

37
00:04:21,164 --> 00:04:24,338
was a HTTP server. If I'm

38
00:04:24,354 --> 00:04:28,006
going to be paged, I want to make sure that it's actionable and clear what

39
00:04:28,028 --> 00:04:31,914
the issue is so fast forward to today.

40
00:04:32,032 --> 00:04:35,478
Software is more complex, and now there's hundreds or thousands

41
00:04:35,494 --> 00:04:39,770
of different metrics across a distributed system that I could possibly monitor an alert on.

42
00:04:39,920 --> 00:04:43,118
Weve do I begin? Which ones should I focus on, and how do I

43
00:04:43,124 --> 00:04:46,000
make actionable alerting from the metrics that I do have?

44
00:04:48,130 --> 00:04:50,766
This is where the four golden signals come in.

45
00:04:50,948 --> 00:04:53,834
The four golden signals in monitoring are latency,

46
00:04:53,962 --> 00:04:57,694
traffic errors and saturation. And these key

47
00:04:57,732 --> 00:05:01,362
signals will not only help keep your metrics focuses helping you track down issues

48
00:05:01,416 --> 00:05:04,946
faster, but they also monitor what's important for your users too, which means

49
00:05:04,968 --> 00:05:08,070
you're not responding to spurious alerts that aren't meaningful.

50
00:05:08,570 --> 00:05:12,280
Let's dive into what each of these signals mean in greater detail.

51
00:05:14,330 --> 00:05:17,682
Latency is simply the time it takes to service a request.

52
00:05:17,826 --> 00:05:21,318
For websites or user facing services, this is especially

53
00:05:21,404 --> 00:05:24,550
important, or your users will get impatient and abandon their request.

54
00:05:25,310 --> 00:05:28,058
If it takes more than a second or two to open a web page,

55
00:05:28,144 --> 00:05:30,826
I'm probably not going to wait around and see how long it actually is going

56
00:05:30,848 --> 00:05:34,238
to take. Similarly, if it takes too long to add an item to

57
00:05:34,244 --> 00:05:37,614
a shopping cart, for instance, I'm probably going to abandon my shopping cart and look

58
00:05:37,652 --> 00:05:40,974
elsewhere. It's important, however, that you

59
00:05:41,012 --> 00:05:45,150
distinguish the latency between successful requests and unsuccessful requests.

60
00:05:45,650 --> 00:05:49,122
If one of the back ends to my service goes down and the

61
00:05:49,176 --> 00:05:52,514
front end starts services errors, it may be serving the errors rather

62
00:05:52,552 --> 00:05:55,790
quickly, which would be misleading in my graphs.

63
00:05:55,950 --> 00:05:59,400
That's not to say that you shouldn't graph error latency at all.

64
00:05:59,770 --> 00:06:03,586
After all, slow errors aren't any much better than fast errors,

65
00:06:03,778 --> 00:06:07,030
and it also might point to an issue that needs to be investigated.

66
00:06:07,610 --> 00:06:11,382
Latency is often something that the first thing you'll notice, your users will notice,

67
00:06:11,526 --> 00:06:14,570
and when this increases, so does their frustration.

68
00:06:17,150 --> 00:06:20,746
The next one is traffic, and this is a measurement for how

69
00:06:20,768 --> 00:06:24,442
much demand is placed on your service. For web servers,

70
00:06:24,506 --> 00:06:28,298
this would be HTTP requests per second. For an image processing

71
00:06:28,314 --> 00:06:31,920
pipeline, this could be how many images were processed per second.

72
00:06:32,290 --> 00:06:36,146
Whatever your service does, this metric should encapsulate how

73
00:06:36,168 --> 00:06:37,300
busy it is.

74
00:06:41,430 --> 00:06:45,522
Errors can range from explicit errors such as

75
00:06:45,576 --> 00:06:49,414
HTTP 500 errors, or failing GRPC requests based

76
00:06:49,452 --> 00:06:52,758
on error codes, but they could also be implicit too.

77
00:06:52,844 --> 00:06:56,258
Maybe your service is taking more than a second to respond to a request,

78
00:06:56,354 --> 00:07:00,294
and maybe your slos define a request taking

79
00:07:00,332 --> 00:07:04,234
more than a second as a failure. This would need to be measured and

80
00:07:04,272 --> 00:07:08,186
tracked, or take, for instance, serving the wrong content

81
00:07:08,288 --> 00:07:11,726
altogether. In the context of a web server, this could still

82
00:07:11,748 --> 00:07:15,066
be a HTTP 200 and be considered a successful

83
00:07:15,098 --> 00:07:18,298
request. But if you're serving the wrong content, then it's

84
00:07:18,314 --> 00:07:19,710
not really successful.

85
00:07:22,130 --> 00:07:25,614
And finally, saturation. This is the overall

86
00:07:25,662 --> 00:07:28,994
capacity of the service. This will require you to understand

87
00:07:29,112 --> 00:07:32,014
what resources are most constrained. For instance,

88
00:07:32,142 --> 00:07:35,762
maybe you're monitoring an I O intensive database. In this case,

89
00:07:35,816 --> 00:07:39,334
I'd want to take particular care to monitor the queue depth of I

90
00:07:39,372 --> 00:07:42,998
O operations for the disk. It'd also be important to

91
00:07:43,004 --> 00:07:46,706
monitor how quickly my disk would fill up. Whilst it's

92
00:07:46,738 --> 00:07:50,118
typical to use indirect signals such as cpu

93
00:07:50,214 --> 00:07:53,446
or memory or disk to measure saturation,

94
00:07:53,558 --> 00:07:57,610
you can also determine this with load testing and use static numbers too,

95
00:07:57,760 --> 00:08:01,358
in which case you would set alarms based on

96
00:08:01,444 --> 00:08:05,226
how soon your traffic approaches your limit. Whatever metric

97
00:08:05,258 --> 00:08:09,054
you pick, it needs to be clearly shown where the limit is and

98
00:08:09,092 --> 00:08:10,720
how close you are to it.

99
00:08:13,830 --> 00:08:17,634
Now we understand a bit more about each of these signals and what they

100
00:08:17,672 --> 00:08:21,860
mean. Let's put it into practice with monitoring a small service.

101
00:08:24,870 --> 00:08:28,402
Here we've got cat Gen. Catgen is a service that I wrote that has

102
00:08:28,456 --> 00:08:32,082
two components, a frontend and a backend. The backend

103
00:08:32,146 --> 00:08:35,830
is responsible for displaying this picture here of the cat,

104
00:08:35,900 --> 00:08:38,390
and then the front end is the static assets.

105
00:08:38,810 --> 00:08:42,506
So if I click on generate another, we'll see that a

106
00:08:42,528 --> 00:08:46,394
cat is served, and if I keep clicking, I'll keep

107
00:08:46,432 --> 00:08:50,006
getting cats. The back end has some artificial latency that's

108
00:08:50,038 --> 00:08:52,810
introduced just so we have something interesting to plot.

109
00:08:53,630 --> 00:08:57,790
So if I keep clicking this, I'll keep getting cats at varying speeds.

110
00:08:59,010 --> 00:09:02,698
Now the way that this is implemented is it's

111
00:09:02,714 --> 00:09:06,580
all running locally on Kubernetes. So I'll show you.

112
00:09:07,750 --> 00:09:10,580
I've got two pods here, a backend and a front end.

113
00:09:11,270 --> 00:09:16,094
Looking at the code, it's a

114
00:09:16,152 --> 00:09:20,390
simple golang binary where I register

115
00:09:21,290 --> 00:09:24,440
a Prometheus histogram right here,

116
00:09:24,890 --> 00:09:28,906
which is the HP request duration, and we'll use

117
00:09:28,928 --> 00:09:32,074
this to plot our latency. Now down

118
00:09:32,112 --> 00:09:35,846
here is the actual handler that serves

119
00:09:35,878 --> 00:09:39,526
the cats. So there's a directory called images that's

120
00:09:39,558 --> 00:09:44,270
embedded in the binary, and it will pick a random index and

121
00:09:44,340 --> 00:09:47,802
it will services this, write the JPEG header

122
00:09:47,866 --> 00:09:51,582
and then introduce a random amount of delay from zero

123
00:09:51,636 --> 00:09:54,846
to 50 milliseconds. And you

124
00:09:54,868 --> 00:09:58,434
can see that the handler is cat JPEG. So if I was

125
00:09:58,472 --> 00:10:01,282
to run this back end by itself,

126
00:10:01,416 --> 00:10:04,834
if I just simply visited cat JPEG, I would just get a

127
00:10:04,872 --> 00:10:08,034
cat JPEG. Similarly, if I hit metrics,

128
00:10:08,082 --> 00:10:10,920
I'd get my Prometheus endpoint as well.

129
00:10:12,890 --> 00:10:16,086
The front end is a

130
00:10:16,108 --> 00:10:19,850
collection of HTML and CSS and some Javascript

131
00:10:20,510 --> 00:10:24,726
that is served by an NginX server.

132
00:10:24,918 --> 00:10:28,454
If I look at the Nginx config you'll

133
00:10:28,502 --> 00:10:32,302
see that I define an upstream called backend and

134
00:10:32,356 --> 00:10:36,286
this server or the DNS name cat server comes from the

135
00:10:36,388 --> 00:10:39,866
Kubernetes service. So if I run a kubectl

136
00:10:39,898 --> 00:10:42,560
get service I'll see the cat services service.

137
00:10:42,930 --> 00:10:46,482
So that points at the back end. I set up my access

138
00:10:46,536 --> 00:10:50,510
logs and then I have two locations. The root

139
00:10:50,590 --> 00:10:54,350
is simply under usershare nginx HTML

140
00:10:54,430 --> 00:10:56,610
and reads the static files from disk.

141
00:10:58,470 --> 00:11:01,558
Beneath here we can see that that slash backend cat

142
00:11:01,644 --> 00:11:04,280
JPEG is proxied to the back end.

143
00:11:06,650 --> 00:11:10,680
So if I look at the source of Cat gen,

144
00:11:11,310 --> 00:11:14,586
if I view your page source you can see that I have

145
00:11:14,608 --> 00:11:18,278
an image that's served as backend cat jpeg.

146
00:11:18,374 --> 00:11:22,190
So this isn't actually served by the front end itself, services by the backend.

147
00:11:23,970 --> 00:11:27,200
Now how do we take what we've learned today and apply that?

148
00:11:27,570 --> 00:11:31,518
So today I'm going to use Grafana and

149
00:11:31,604 --> 00:11:35,158
I'm going to create a dashboard and weve going to start to plot

150
00:11:35,194 --> 00:11:38,798
some key metrics. So given that it's a HTTP server,

151
00:11:38,974 --> 00:11:42,834
there's a few things that I care about. So for

152
00:11:42,872 --> 00:11:47,094
my traffic, which is one of our golden signals, I'm interested in

153
00:11:47,212 --> 00:11:51,510
the amount of HTTP responses I'm getting. So let's

154
00:11:51,850 --> 00:11:55,474
have a sum of the Nginx HTTP

155
00:11:55,522 --> 00:11:59,190
response times seconds

156
00:12:01,050 --> 00:12:06,380
count. Rather we'll plot 30 seconds here

157
00:12:06,990 --> 00:12:10,458
and this will give us a nice graph of the amount of requests that I'm

158
00:12:10,464 --> 00:12:14,846
getting. So this is HTTP requests and

159
00:12:14,868 --> 00:12:18,762
you'll notice that I already have quite a few requests.

160
00:12:18,906 --> 00:12:25,042
So if I edit this and change

161
00:12:25,096 --> 00:12:29,106
the unit to request second, this is

162
00:12:29,128 --> 00:12:32,722
how many requests a second I'm getting. And I'm actually getting quite a number of

163
00:12:32,776 --> 00:12:37,160
requests already. The reason for that is

164
00:12:37,610 --> 00:12:41,480
I actually have curl running here and I can

165
00:12:41,930 --> 00:12:45,666
run that again and this is just generating

166
00:12:45,698 --> 00:12:49,160
artificial load if I go back to

167
00:12:49,530 --> 00:12:52,826
Grafana now. So this is my traffic which is

168
00:12:52,848 --> 00:12:56,266
one of our golden signals. Let's work on

169
00:12:56,368 --> 00:12:59,430
adding in, say errors.

170
00:12:59,590 --> 00:13:03,774
So because it is a HTTP server, I am primarily concerned with

171
00:13:03,972 --> 00:13:07,246
HTTP two hundred s and five hundred s. Those are my success and

172
00:13:07,268 --> 00:13:10,734
errors. So let's get

173
00:13:10,772 --> 00:13:14,286
in the sum of the rate of Nginx

174
00:13:14,398 --> 00:13:26,730
HTTP response county

175
00:13:26,750 --> 00:13:32,534
seconds and

176
00:13:32,572 --> 00:13:36,230
we want to break this out by status.

177
00:13:37,770 --> 00:13:40,726
You can see we've got the HTTP two hundred s here and then our non

178
00:13:40,758 --> 00:13:44,550
200 and we can see most of our requests

179
00:13:44,630 --> 00:13:49,846
are successful HTTP

180
00:13:49,878 --> 00:13:54,782
errors or HTTP status code and

181
00:13:54,836 --> 00:13:57,998
let's change the unit to request a

182
00:13:58,004 --> 00:14:01,280
second as well. Just to make that clear.

183
00:14:02,610 --> 00:14:06,398
Let's change it to last 15 minutes. So we've

184
00:14:06,414 --> 00:14:09,858
got errors, weve got our

185
00:14:10,024 --> 00:14:13,442
throughput, what else do we need?

186
00:14:13,576 --> 00:14:17,506
We also need latency, which is one of our golden

187
00:14:17,538 --> 00:14:21,030
signals. So latency,

188
00:14:21,450 --> 00:14:24,534
if you remember before we actually collecting that

189
00:14:24,572 --> 00:14:29,158
in our back end, if I go back to the

190
00:14:29,244 --> 00:14:33,538
back end on

191
00:14:33,564 --> 00:14:37,242
the back end go, you can see I've got the

192
00:14:37,376 --> 00:14:40,326
HTTP request getcat duration seconds.

193
00:14:40,438 --> 00:14:43,918
So let's copy that and go back

194
00:14:44,004 --> 00:14:48,560
here. And we also want the

195
00:14:50,050 --> 00:14:53,794
sum of the increase of that

196
00:14:53,832 --> 00:14:57,474
metric. And we'll pick 30

197
00:14:57,512 --> 00:15:00,580
seconds as our resolution or interval for that.

198
00:15:01,910 --> 00:15:05,700
And we also want to

199
00:15:06,250 --> 00:15:09,270
plot this as a heat map.

200
00:15:11,530 --> 00:15:15,878
So change

201
00:15:15,964 --> 00:15:20,966
our heat

202
00:15:20,998 --> 00:15:21,770
map.

203
00:15:26,910 --> 00:15:29,420
Where have I gone wrong here?

204
00:15:32,910 --> 00:15:34,250
Some increase.

205
00:15:36,750 --> 00:15:40,640
There we go, 2 seconds. And we'll call this,

206
00:15:42,290 --> 00:15:44,850
call this back end latency,

207
00:15:46,950 --> 00:15:49,730
change our unit to milliseconds.

208
00:15:52,070 --> 00:15:52,850
Um,

209
00:15:56,010 --> 00:15:57,510
sorry milliseconds.

210
00:16:01,530 --> 00:16:04,310
And let's change this to buckets.

211
00:16:06,090 --> 00:16:09,720
That's a lot of decimal. Let's change that to two.

212
00:16:10,190 --> 00:16:13,754
There we go. So we've got our back end

213
00:16:13,792 --> 00:16:17,674
latency, we've got our HTTP request, which is our throughput, we've got

214
00:16:17,712 --> 00:16:21,346
our status codes. So this service is looking pretty healthy.

215
00:16:21,398 --> 00:16:25,006
But there's one more golden signal that we're missing here and that is

216
00:16:25,108 --> 00:16:28,974
our saturation. So how full our service is.

217
00:16:29,172 --> 00:16:33,390
Now, the cat gen is primarily driven by CPU,

218
00:16:33,750 --> 00:16:37,202
so we're going to plot cpu. Now I already

219
00:16:37,256 --> 00:16:41,586
have a cpu usage in

220
00:16:41,608 --> 00:16:44,994
my library. See here we've got cpu usage broken out by

221
00:16:45,032 --> 00:16:48,910
front end and back end. Now one thing you'll notice about this

222
00:16:49,000 --> 00:16:51,926
is the front end seems to be doing a lot more work than the back

223
00:16:51,948 --> 00:16:55,574
end. A good reason for this is that

224
00:16:55,612 --> 00:16:59,426
my curl over in my terminal here is

225
00:16:59,468 --> 00:17:02,794
hitting the front end, but the back end is not actually serving those

226
00:17:02,832 --> 00:17:06,282
cat pitches. This only happens

227
00:17:06,336 --> 00:17:09,722
when you actually download the image, for instance. So this

228
00:17:09,776 --> 00:17:13,354
content is being services statically by the curl.

229
00:17:13,482 --> 00:17:17,582
So if I go to my terminal and

230
00:17:17,636 --> 00:17:20,846
run the curl manually, you'll see that I get

231
00:17:20,868 --> 00:17:24,178
the same output. What I don't get is

232
00:17:24,264 --> 00:17:27,506
the client actually downloading this image, which has happened to

233
00:17:27,528 --> 00:17:31,122
the back end. So you're actually seeing less

234
00:17:31,176 --> 00:17:35,634
load on the back end because of that right

235
00:17:35,672 --> 00:17:39,154
here. So this could tell me that maybe I need

236
00:17:39,192 --> 00:17:42,498
to scale up my front end, maybe I need to look at logs

237
00:17:42,514 --> 00:17:46,498
and see what's happening. In this instance, I'm being hit by essentially a bot,

238
00:17:46,594 --> 00:17:50,234
so maybe I need to throttle them, maybe I

239
00:17:50,272 --> 00:17:53,610
need to block them all together, for instance.

240
00:17:54,030 --> 00:17:57,626
But you can see here, this dashboard provides a

241
00:17:57,648 --> 00:18:01,046
pretty good overview of the status of what's happening with cat gen

242
00:18:01,088 --> 00:18:04,858
right now in terms of latency status codes,

243
00:18:04,954 --> 00:18:08,574
requests, and my saturation. I could

244
00:18:08,612 --> 00:18:12,238
also run some benchmarking and set

245
00:18:12,324 --> 00:18:15,418
static high watermarks for HP requests,

246
00:18:15,434 --> 00:18:19,362
for instance. So I do some load testing. I know that I can handle 500

247
00:18:19,416 --> 00:18:22,946
requests a second, for instance. I could set that within there, and I could set

248
00:18:22,968 --> 00:18:26,542
some alarming as well to know when I am reaching that or approaching that threshold

249
00:18:26,606 --> 00:18:30,614
and respond appropriately. Now, these four

250
00:18:30,652 --> 00:18:34,098
golden signals really help us set up a solid foundation, but it's

251
00:18:34,114 --> 00:18:37,894
not the only thing that I should monitor. There's certainly other things, and there's certainly

252
00:18:37,932 --> 00:18:41,466
other metrics as well. So if I go to

253
00:18:41,488 --> 00:18:47,242
add another metric, you could see all these metrics here are

254
00:18:47,296 --> 00:18:51,066
possibly things that I could add in, and you'll see that

255
00:18:51,088 --> 00:18:55,230
there's quite a number of them as well, things from the actual Kubernetes

256
00:18:55,570 --> 00:18:59,440
hosts themselves, the kubelets, Prometheus itself.

257
00:19:00,290 --> 00:19:03,866
So there are a number of things that we could be plotting, and that's

258
00:19:03,898 --> 00:19:07,566
not to say that we shouldn't plot them. The whole purpose of the four

259
00:19:07,588 --> 00:19:11,310
golden signals is really to give us a starting point to really understand

260
00:19:11,380 --> 00:19:14,730
what's going on with the service. Thank you for listening.

261
00:19:14,810 --> 00:19:17,958
I hope you've enjoyed today's talk on the four golden signals and how you

262
00:19:17,964 --> 00:19:20,502
might be able to apply them to your service. If you've got any questions,

263
00:19:20,556 --> 00:19:23,894
I'd love to chat. You can find me on LinkedIn, or feel free to reach

264
00:19:23,932 --> 00:19:25,860
out to me at Michaelmcalester at got.

