1
00:00:26,850 --> 00:00:29,160
Hi, everybody. Thank you so much for joining me.

2
00:00:30,330 --> 00:00:33,622
I am Nocnica Mellifera. Let me put on

3
00:00:33,676 --> 00:00:37,640
the full face to say hi. Hi, everybody. Thank you so much for coming out.

4
00:00:38,010 --> 00:00:41,682
This is open source observability with open telemetry.

5
00:00:41,826 --> 00:00:44,994
Noshanika Meyerflare. You can find me most places at serverless bomb.

6
00:00:45,042 --> 00:00:48,454
You can also just google the name Noshanika, turns out, and I come up.

7
00:00:48,492 --> 00:00:49,880
So that's fun.

8
00:00:51,570 --> 00:00:55,054
Given in association with telemetryhub.com. Go check it out

9
00:00:55,092 --> 00:00:58,382
while we're talking this through. Okay, so what

10
00:00:58,436 --> 00:01:01,966
is observability? Frankly, observability is

11
00:01:01,988 --> 00:01:05,374
a term is much more familiar on the west coast of the United States

12
00:01:05,412 --> 00:01:08,658
than it is across the entire tech sector. So I

13
00:01:08,664 --> 00:01:12,066
think it's fair to say, hey, hopefully you're here

14
00:01:12,088 --> 00:01:15,140
because you understood something about it or you've heard the term before.

15
00:01:15,690 --> 00:01:19,286
It is not a single tool or a special case or

16
00:01:19,308 --> 00:01:21,906
a standard. It is a design criteria.

17
00:01:22,098 --> 00:01:26,402
And I think of this as observability

18
00:01:26,466 --> 00:01:30,038
as being the time to understanding and not just know. People like charity majors

19
00:01:30,054 --> 00:01:33,580
and the open telemetry project have talked about defining it this way.

20
00:01:34,190 --> 00:01:37,754
We think of our time to understanding of a particular problem or

21
00:01:37,792 --> 00:01:41,706
issue or service interruption is the

22
00:01:41,728 --> 00:01:45,200
first half of your time to resolution. And so

23
00:01:46,130 --> 00:01:49,358
a lot of the time throughout this talk, I'm really going to be referring to

24
00:01:49,364 --> 00:01:52,974
these situations where a service is completely down or otherwise really not

25
00:01:53,012 --> 00:01:56,510
performing as it should. But you can also,

26
00:01:56,580 --> 00:01:59,986
observability can cover these cases where it's like, hey, why is this so slow for

27
00:02:00,008 --> 00:02:03,518
users in this region? Some people have some reports of a bud that we haven't

28
00:02:03,534 --> 00:02:07,442
been able to replicate. These are also problems that observability can address.

29
00:02:07,576 --> 00:02:10,786
Right. It's possible to have a fix without understanding

30
00:02:10,818 --> 00:02:14,438
the problem. This is an example where, hey, you know that eventually the

31
00:02:14,444 --> 00:02:18,006
service runs out of memory, so you go ahead and restart it. And we've all

32
00:02:18,028 --> 00:02:21,398
seen those setups where it's like, yeah, we just

33
00:02:21,404 --> 00:02:24,986
need to restart this thing every 24 hours because we know it's running out

34
00:02:25,008 --> 00:02:28,538
of memory. We don't know why. And so that's can example where we have

35
00:02:28,704 --> 00:02:32,510
no observability, really no understanding of the system, but we do have a fix.

36
00:02:32,660 --> 00:02:36,318
But without understanding, the stress of a particular problem

37
00:02:36,404 --> 00:02:40,094
is pretty high. Right. Much better to have some

38
00:02:40,132 --> 00:02:41,600
understanding of what's going on.

39
00:02:43,350 --> 00:02:44,100
Okay,

40
00:02:46,230 --> 00:02:50,046
so why are microservices a little bit harder

41
00:02:50,078 --> 00:02:54,100
for this? Why do they make the challenge larger? So let's talk about

42
00:02:55,450 --> 00:02:59,282
a historical time where we were really thinking about monoliths

43
00:02:59,346 --> 00:03:03,554
as ways of creating production

44
00:03:03,602 --> 00:03:07,558
software, right. In the era of the monolith, only a

45
00:03:07,564 --> 00:03:09,980
few people understood the whole system.

46
00:03:10,430 --> 00:03:13,994
So most people were working in little areas of it.

47
00:03:14,032 --> 00:03:17,658
They often felt like they were needing the

48
00:03:17,664 --> 00:03:21,342
expertise of a small group of people who really understood the whole system.

49
00:03:21,476 --> 00:03:24,480
But those who did understand the whole system,

50
00:03:24,930 --> 00:03:28,270
they had a very full explanation of problems

51
00:03:28,340 --> 00:03:31,662
that were happening on the stack. And the

52
00:03:31,716 --> 00:03:35,474
biggest problem with a monolithic architecture is

53
00:03:35,512 --> 00:03:38,690
actually not at all about how it performs.

54
00:03:39,430 --> 00:03:43,646
Some people will say, hey, we don't do monoliths anymore because they don't scale correctly.

55
00:03:43,758 --> 00:03:47,734
That can or cannot be true. That's not always the case. But the

56
00:03:47,772 --> 00:03:51,206
problem that monoliths really created was

57
00:03:51,228 --> 00:03:55,414
thats it often took months for someone to become can effective team

58
00:03:55,452 --> 00:03:58,982
member once they joined your community. And with a lot of people

59
00:03:59,036 --> 00:04:02,042
averaging just two years in a particular position,

60
00:04:02,176 --> 00:04:05,754
monoliths just don't work anymore. So you have to have these

61
00:04:05,792 --> 00:04:09,830
microservices so that people can get up to speed on a single microservice

62
00:04:09,910 --> 00:04:13,142
and be contributing within weeks instead

63
00:04:13,216 --> 00:04:16,574
of months. And so that's the

64
00:04:16,612 --> 00:04:19,706
reason for the migration. It's not really because a monolith performs

65
00:04:19,738 --> 00:04:22,814
so poorly. And one of the things thats monolith did a lot better was that

66
00:04:22,852 --> 00:04:26,114
all the information should be available on the stack at

67
00:04:26,152 --> 00:04:29,794
any time that you choose to stop and see what's going on.

68
00:04:29,912 --> 00:04:33,858
So a person who understands the monolith well can very quickly get to

69
00:04:33,864 --> 00:04:37,480
the bottom of a particular problem because all the information is available.

70
00:04:39,450 --> 00:04:43,442
So with microservices, right, someone understands

71
00:04:43,506 --> 00:04:47,094
each of the interconnected dots completely. They completely

72
00:04:47,212 --> 00:04:51,430
understand how that dot works. But nobody

73
00:04:51,590 --> 00:04:55,260
understands the map that covers all of these

74
00:04:56,270 --> 00:04:59,222
microservices obviously have a ton of performance advantages,

75
00:04:59,286 --> 00:05:02,766
scaling advantages, and again, that advantage with how quickly people can

76
00:05:02,788 --> 00:05:07,710
start contributing to the team. But with observability and

77
00:05:07,780 --> 00:05:11,214
any kind of understanding of can outage, in almost all

78
00:05:11,252 --> 00:05:15,198
cases, microservices are going to be a dead weight loss. So they're

79
00:05:15,214 --> 00:05:18,354
going to make the situation worse for being able to.

80
00:05:18,392 --> 00:05:21,986
Like for example, if it's 05:00 a.m. Or 03:00 a.m.

81
00:05:22,088 --> 00:05:25,726
Outage. Once everyone on the team is awake

82
00:05:25,758 --> 00:05:29,590
and the people who understand the system best are awake and have

83
00:05:29,740 --> 00:05:32,754
gotten connected with the monolith,

84
00:05:32,802 --> 00:05:36,134
somebody's going to understand what's going on. But very

85
00:05:36,172 --> 00:05:40,022
often with microservices, one of those common questions I have gotten

86
00:05:40,086 --> 00:05:43,434
when working with observability tools and people have

87
00:05:43,472 --> 00:05:46,634
these very deep microservice architectures, people just say,

88
00:05:46,672 --> 00:05:50,438
hey, on a normal request, no problem. With request, no failure,

89
00:05:50,614 --> 00:05:54,286
how do I find out which services are being

90
00:05:54,308 --> 00:05:57,886
hit by that request? So a simple question like, hey, when they come and check,

91
00:05:57,908 --> 00:06:01,246
but from our ecommerce store, what services are

92
00:06:01,268 --> 00:06:05,186
involved in that checkout? Okay, so that shows you this is

93
00:06:05,208 --> 00:06:08,978
an oversimplified version of microservices, right? They really are

94
00:06:09,144 --> 00:06:12,846
multifaceted, very, very complex, and quickly build to a complexity

95
00:06:12,878 --> 00:06:16,166
where it's very hard to even understand where a successful request is

96
00:06:16,188 --> 00:06:19,814
going. And so we

97
00:06:19,852 --> 00:06:23,526
can move very quickly to thats chaos where it's very hard for

98
00:06:23,548 --> 00:06:26,710
us to understand what's going on inside a microservice architecture.

99
00:06:28,570 --> 00:06:31,618
Okay, let's talk about how we solve this with observability.

100
00:06:31,714 --> 00:06:34,858
There are three major components to observability that we

101
00:06:34,864 --> 00:06:37,558
need to ensure. I'm going to be a little bit quick with this because we're

102
00:06:37,574 --> 00:06:41,418
going a little bit deeper into concepts after this. So we're going

103
00:06:41,424 --> 00:06:44,862
to zip through this just a little bit. But there's really good

104
00:06:44,916 --> 00:06:48,634
write ups on opentelemetry IO about the concepts of logged traces

105
00:06:48,682 --> 00:06:51,150
and metrics, which are the three pillars of observability.

106
00:06:52,530 --> 00:06:55,918
So let's start with metrics, right? When you don't

107
00:06:55,934 --> 00:06:59,566
know what's happening, count something I actually have lost where I got that quotation

108
00:06:59,598 --> 00:07:01,570
from. This is a quotation from a statistician.

109
00:07:04,550 --> 00:07:08,514
One of the concepts for what is a metric is that the speedometer

110
00:07:08,562 --> 00:07:10,950
on your car is a metric,

111
00:07:11,690 --> 00:07:14,920
numerical measurement of a complex system.

112
00:07:15,370 --> 00:07:18,562
So instead of saying, hey, you've just passed slough

113
00:07:18,626 --> 00:07:21,978
and you're going into this next place, and then you're going to get

114
00:07:21,984 --> 00:07:25,722
there in this much time, or these other things, a metric is a very simple

115
00:07:25,776 --> 00:07:28,220
measurement. Hey, you're currently going this fast.

116
00:07:30,350 --> 00:07:33,830
They're very easy, or they should be an easy way to get

117
00:07:33,840 --> 00:07:37,114
a high level view. This is a nuclear control station.

118
00:07:37,162 --> 00:07:40,814
So it really gives you a sense of how you can get so

119
00:07:40,852 --> 00:07:44,222
many metrics very quickly that you dont have a very quick and easy

120
00:07:44,276 --> 00:07:47,838
view, but you do have a high level view of what's going on. And metrics

121
00:07:47,854 --> 00:07:51,778
are also very easy to store in a high volume. So metrics don't present

122
00:07:51,864 --> 00:07:54,980
usually a challenge for like, hey, where are we going to keep all of these?

123
00:07:55,750 --> 00:07:59,218
If you're getting to a point where your database is struggling to contain the metrics

124
00:07:59,234 --> 00:08:02,502
that your production service is generating, either your Netflix and

125
00:08:02,556 --> 00:08:05,814
I'm sorry, or you have an issue with

126
00:08:05,852 --> 00:08:09,698
configuration, things like metric explosion, which we're

127
00:08:09,714 --> 00:08:12,634
not going to get into here but yeah, normally it's very easy to store.

128
00:08:12,832 --> 00:08:16,086
You have logs, right? Logs,

129
00:08:16,118 --> 00:08:19,466
as I say, they always have a complete and thorough explanation of

130
00:08:19,488 --> 00:08:23,006
the problem somewhere, right. But storage and management are their

131
00:08:23,028 --> 00:08:26,094
own challenge. Logs can be so

132
00:08:26,132 --> 00:08:29,360
complex, can contain so much data, that very often

133
00:08:31,090 --> 00:08:34,682
the real challenge is just sorting through them during a crisis.

134
00:08:34,746 --> 00:08:37,746
And so there are people who are of the opinion that if there is an

135
00:08:37,768 --> 00:08:41,122
outage and if something is not working, they really don't want to be starting

136
00:08:41,176 --> 00:08:44,958
with logs. They know that they're not in a good place if logs

137
00:08:44,974 --> 00:08:48,342
is where they're starting. And then finally

138
00:08:48,396 --> 00:08:51,942
is our new entrant in the last 510 years into

139
00:08:51,996 --> 00:08:55,030
the story of observing our systems, which is tracing, right.

140
00:08:55,180 --> 00:08:58,534
They're informatically a hybrid between metrics and logging.

141
00:08:58,582 --> 00:09:02,394
And they're trying to generalize observed time

142
00:09:02,432 --> 00:09:06,138
spans, which is a

143
00:09:06,144 --> 00:09:09,706
little bit obtuse. But essentially tracing is supposed to show

144
00:09:09,728 --> 00:09:13,598
us the components that are hit by a request. And because

145
00:09:13,764 --> 00:09:16,814
we use a modern architecture, those are not going to be

146
00:09:16,852 --> 00:09:20,414
sequential, right. They're going to be multiple time spans happening

147
00:09:20,532 --> 00:09:24,306
all at once or at the same time. And we have

148
00:09:24,328 --> 00:09:27,380
a few more figures here to kind of help us see that.

149
00:09:27,990 --> 00:09:31,426
So, one little side note about tracing. Tracing is

150
00:09:31,528 --> 00:09:35,378
relatively, it should be as dense as

151
00:09:35,544 --> 00:09:37,460
logging, possibly more so.

152
00:09:38,390 --> 00:09:42,214
And one of the journey secrets about tracing is that most trace data is

153
00:09:42,252 --> 00:09:45,510
never viewed. And by most we mean like three nines of data.

154
00:09:45,660 --> 00:09:48,310
The vast majority of trace data is never viewed.

155
00:09:50,270 --> 00:09:54,186
I see my little face is covering my joke there, right?

156
00:09:54,368 --> 00:09:57,050
Really, most of it is never viewed.

157
00:09:59,630 --> 00:10:03,518
That's kind of worth noting when we think about our data retention problems

158
00:10:03,604 --> 00:10:07,374
and other problems like that. Okay, thats is

159
00:10:07,412 --> 00:10:10,800
not what I wanted to do. Let's come over here.

160
00:10:11,330 --> 00:10:15,698
There we go. Okay, so from

161
00:10:15,784 --> 00:10:20,366
tracing, we came to the concept of distributed

162
00:10:20,398 --> 00:10:23,860
tracing. Fix that.

163
00:10:26,550 --> 00:10:30,194
So distributed tracing really

164
00:10:30,232 --> 00:10:33,670
is just the implementation of tracing, but that is able to track

165
00:10:33,740 --> 00:10:37,506
an event between multiple microservices. So here

166
00:10:37,548 --> 00:10:40,300
you see this request being passed around,

167
00:10:41,470 --> 00:10:45,350
which is creating multiple events which are sent to other APIs

168
00:10:45,510 --> 00:10:49,386
and getting back responses. And thats each time this is happening,

169
00:10:49,568 --> 00:10:53,322
there's some kind of persistence going out that is saying,

170
00:10:53,376 --> 00:10:57,030
hey, here's the stuff that we want to log about what's happening, and we

171
00:10:57,040 --> 00:10:59,406
want to be able to connect all those together. We don't want to just be

172
00:10:59,428 --> 00:11:02,982
filtering logs to see that connection. We want to be able to see it easily

173
00:11:03,146 --> 00:11:05,170
that this request is connected.

174
00:11:08,630 --> 00:11:12,770
So at a very high level, how does distributed tracing happen?

175
00:11:12,840 --> 00:11:16,374
Right. You add a trace header somewhere close to the start you

176
00:11:16,412 --> 00:11:20,162
pass it around with the request and then you have some collector

177
00:11:20,226 --> 00:11:24,006
side logic or some data gathering side logic to stitch those

178
00:11:24,028 --> 00:11:25,080
pieces together.

179
00:11:31,790 --> 00:11:35,862
So the goal of tracing is to get something like this waterfall chart,

180
00:11:35,926 --> 00:11:39,066
right, which is showing us here are the components that were hit by

181
00:11:39,088 --> 00:11:42,378
this request, and ideally seeing them in some kind

182
00:11:42,384 --> 00:11:45,594
of hierarchy to say, hey, general, we had a request to the API.

183
00:11:45,722 --> 00:11:49,274
It had these components that were hit. These were the ones that were running simultaneously.

184
00:11:49,322 --> 00:11:52,270
Here's how long they took. So beyond just,

185
00:11:52,340 --> 00:11:55,514
hey, this went to here, which again, as I mentioned earlier,

186
00:11:55,562 --> 00:11:59,394
is often where people come in as that's what they really need from the system

187
00:11:59,512 --> 00:12:02,786
is they just say, hey, I want to know what the

188
00:12:02,808 --> 00:12:05,730
heck is being touched by this request.

189
00:12:07,110 --> 00:12:10,430
These x widths here have a meaning. They have

190
00:12:10,440 --> 00:12:13,826
a meaning of how much time something took. So you see a lot of discussion

191
00:12:13,858 --> 00:12:17,506
when we talk about tracing of spans, which is the measurement of the amount of

192
00:12:17,548 --> 00:12:21,340
time that each of these components took.

193
00:12:22,350 --> 00:12:25,866
And then you get some kind of visual indicators of what

194
00:12:25,888 --> 00:12:30,702
was blocking what. Right. Like in this case, auth needed

195
00:12:30,756 --> 00:12:34,270
to be completed before we could get to payment gateway at dispatch.

196
00:12:35,250 --> 00:12:36,000
Okay,

197
00:12:40,450 --> 00:12:43,810
so once we start

198
00:12:43,880 --> 00:12:47,714
thinking about distributed tracing, one of the problems that we run

199
00:12:47,752 --> 00:12:51,442
into is this problem of how

200
00:12:51,496 --> 00:12:55,198
do we get these individual pieces to communicate.

201
00:12:55,374 --> 00:12:59,366
And so in the sort of closed source SaaS world,

202
00:12:59,548 --> 00:13:02,886
there were these efforts to say, okay, well, we'll create a

203
00:13:02,908 --> 00:13:06,598
library for maybe front end measurement, for measurement of your back end system,

204
00:13:06,684 --> 00:13:10,602
for measurement of your database, and then we can tie those together.

205
00:13:10,656 --> 00:13:13,878
If you use our closed source tools, use our SaaS tools,

206
00:13:13,974 --> 00:13:16,486
we'll be able to tie those together into a single trace.

207
00:13:16,678 --> 00:13:21,066
But as microservice world started to explode,

208
00:13:21,258 --> 00:13:25,130
it really got difficult to negotiate

209
00:13:25,210 --> 00:13:29,194
that trace header value to be passed successfully

210
00:13:29,242 --> 00:13:32,890
between all these things and a single company, a single

211
00:13:33,060 --> 00:13:37,022
effort, no matter how big, just could not maintain

212
00:13:37,166 --> 00:13:41,106
a system that could be installed everywhere that would successfully pick up

213
00:13:41,128 --> 00:13:44,740
this trace, report it successfully up to their system

214
00:13:45,430 --> 00:13:48,642
and give you this nice unified trace. There were always going to be

215
00:13:48,696 --> 00:13:51,974
these large black boxes within your trace where either

216
00:13:52,012 --> 00:13:55,174
the trace data was totally lost or it's just, yeah, we were waiting for something

217
00:13:55,212 --> 00:13:58,086
here we don't have observation of what happened.

218
00:13:58,268 --> 00:14:01,958
So that is how we get to this point with open telemetryhub.

219
00:14:02,054 --> 00:14:05,574
Open telemetryhub and the history of open telemetry and distributed tracing

220
00:14:05,622 --> 00:14:09,158
are intimately linked as this is a project to define

221
00:14:09,174 --> 00:14:13,006
can open standard for the communication between components that

222
00:14:13,028 --> 00:14:16,426
distributed tracing can work successfully. Open telemetry covers

223
00:14:16,458 --> 00:14:19,662
the other components of observability too, as we'll get into.

224
00:14:19,796 --> 00:14:22,000
But this is kind of where we start.

225
00:14:24,950 --> 00:14:28,674
So a big key idea with the open

226
00:14:28,712 --> 00:14:31,954
telemetry is this thing of the collector. So while open

227
00:14:31,992 --> 00:14:35,726
telemetry is in part is just a standard for the communication of metrics,

228
00:14:35,758 --> 00:14:39,586
trace and logging data, to say, hey, here's how thats should be transmitted.

229
00:14:39,618 --> 00:14:43,750
And that's supremely useful for distributed tracing because it means if you work

230
00:14:43,820 --> 00:14:47,122
on your little project for instrumenting,

231
00:14:47,266 --> 00:14:50,634
laravel symphony or a particular build of rails or

232
00:14:50,672 --> 00:14:54,522
what have you, you can follow these open standards and

233
00:14:54,576 --> 00:14:58,406
be able to get traces that you can tie together. But there's

234
00:14:58,438 --> 00:15:01,774
this kind of superpower involved there because we

235
00:15:01,812 --> 00:15:05,134
mentioned that there's these steps to creating trees. And one of the key steps is

236
00:15:05,172 --> 00:15:08,574
we have some way to tie those traces together,

237
00:15:08,772 --> 00:15:12,846
right? And that is one of the

238
00:15:12,868 --> 00:15:15,806
problems that is solved by the open telemetry collector.

239
00:15:15,998 --> 00:15:19,250
So the collector is where a lot of this magic happens.

240
00:15:19,320 --> 00:15:23,570
And let me zoom in a little bit on this chart. So you

241
00:15:23,640 --> 00:15:27,486
have these open telemetry standards and they can communicate,

242
00:15:27,518 --> 00:15:30,774
but to a third party service, as you can see up here, and I'll mention

243
00:15:30,812 --> 00:15:33,510
a little later that one of the ways to get started is to try just

244
00:15:33,580 --> 00:15:37,110
directly reporting from your service up to a Prometheus endpoint

245
00:15:37,770 --> 00:15:40,502
or up to another open telemetry endpoint.

246
00:15:40,646 --> 00:15:43,946
But one of the other ways to do it

247
00:15:43,968 --> 00:15:47,386
is to be running a service that is an open telemetry collector, where you have

248
00:15:47,408 --> 00:15:51,006
your multiple components thats are reporting over

249
00:15:51,108 --> 00:15:54,974
into the collector, and then the collector is saying, okay, let me go ahead

250
00:15:55,092 --> 00:15:58,398
and write out really nice,

251
00:15:58,564 --> 00:16:01,360
clear observability data.

252
00:16:04,210 --> 00:16:07,574
And the collector is not just a data explorer

253
00:16:07,642 --> 00:16:10,910
or a sort of data middleman. The collector

254
00:16:10,990 --> 00:16:15,258
has all of these multiple components that can do things like filtering,

255
00:16:15,454 --> 00:16:20,978
batching, attributing, and so attributing,

256
00:16:21,154 --> 00:16:24,118
adding attributes, I don't, thats doesn't feel like attributing. I don't know, it feels like

257
00:16:24,124 --> 00:16:28,074
a separate word, but whatever. So these

258
00:16:28,192 --> 00:16:31,578
processors are a key part of the story with the

259
00:16:31,584 --> 00:16:34,700
open telemetry collector where

260
00:16:35,310 --> 00:16:39,066
these questions that previously maybe from a SaaS servers were pretty hard to

261
00:16:39,088 --> 00:16:42,762
cover. Like hey, I had this very particular kind of

262
00:16:42,816 --> 00:16:46,366
PII data, like specific format of health data and I

263
00:16:46,388 --> 00:16:48,894
need to filter that out and make sure it's never sent, even if it got

264
00:16:48,932 --> 00:16:52,426
observed accidentally. Instead of waiting on a SaaS

265
00:16:52,458 --> 00:16:56,138
company to say, oh well, don't worry, we'll implement a filter for that, the collector,

266
00:16:56,234 --> 00:16:59,586
you could just go ahead and grab a processor component and do that

267
00:16:59,608 --> 00:17:02,866
filtering. And since a collector can be run within your own cloud,

268
00:17:03,048 --> 00:17:06,694
you can say, hey, I want to do this filtering before it's ever sent

269
00:17:06,732 --> 00:17:07,830
along the network.

270
00:17:10,810 --> 00:17:14,614
Along with these three pillars, there is this concept in open

271
00:17:14,652 --> 00:17:18,278
telemetry of baggage where you're able to add

272
00:17:18,364 --> 00:17:21,846
a little bit of information that gets passed along. So an example might

273
00:17:21,868 --> 00:17:25,434
be client id. It's kind of a classic one is that all

274
00:17:25,472 --> 00:17:28,586
of these microservices are maybe seeing this thing, but only right at the start did

275
00:17:28,608 --> 00:17:32,134
we see what their client id was, and we'd say, yeah, because that's useful

276
00:17:32,182 --> 00:17:35,854
to us. To tie this together to add filtering data later, we're going to add

277
00:17:35,892 --> 00:17:40,234
this baggage that is this client id. Now, baggage is not reported automatically.

278
00:17:40,282 --> 00:17:43,466
It's not like an attribute on a trace, but it can be useful.

279
00:17:43,498 --> 00:17:46,286
You can explicitly say, hey, I want to go ahead and check this baggage here.

280
00:17:46,308 --> 00:17:49,090
And if we got a client id, I want to write that to this trace.

281
00:17:49,670 --> 00:17:52,882
So yeah, that's kind of this. The idea of

282
00:17:52,936 --> 00:17:56,238
baggage is right, is just sort of something that contains a little something else thats

283
00:17:56,264 --> 00:17:59,586
comes along with you. And so it's very nonspecific

284
00:17:59,618 --> 00:18:02,726
about what it may contain, but it can be a useful concept as you're getting

285
00:18:02,748 --> 00:18:04,070
a little bit more advanced.

286
00:18:08,410 --> 00:18:12,202
And support for open

287
00:18:12,256 --> 00:18:17,226
telemetry is a lot better than you think. And I say that because I

288
00:18:17,248 --> 00:18:20,462
was actually writing one of the write ups of hey, here's the state of open

289
00:18:20,516 --> 00:18:24,478
telemetryhub support. And I commented, oh, hey,

290
00:18:24,564 --> 00:18:30,206
maybe Ruby is kind of not ready for use. And this is because I

291
00:18:30,228 --> 00:18:33,886
was looking on the opentelemetry IO page and

292
00:18:33,908 --> 00:18:37,602
just seeing like, hey, in know a couple of these things are listed as not

293
00:18:37,656 --> 00:18:40,898
yet implemented, but small. But of

294
00:18:40,904 --> 00:18:44,434
the way shops like Shopify use the

295
00:18:44,472 --> 00:18:48,482
Ruby open telemetry project. So pretty

296
00:18:48,536 --> 00:18:52,194
advanced actually. Even though metrics right on this table at the top

297
00:18:52,232 --> 00:18:55,158
level are listed as not implemented. You can actually, if you click in, you see,

298
00:18:55,164 --> 00:18:58,662
oh, they're experimental, but a lot of people are using lemon production now.

299
00:18:58,796 --> 00:19:02,154
So it is great that there is this sort of top level list of like,

300
00:19:02,192 --> 00:19:05,286
here's the level of support. And obviously for obvious reasons,

301
00:19:05,318 --> 00:19:09,562
like traces are kind of the first thing that's implemented. But I

302
00:19:09,616 --> 00:19:14,082
really think it's worth a look. And especially because so many of these languages,

303
00:19:14,166 --> 00:19:18,126
it's only logs that are missing. And the fact is you've had a

304
00:19:18,148 --> 00:19:21,870
way to report up logs and filter logs for a long time, almost certainly.

305
00:19:21,940 --> 00:19:24,660
So that's not really going to be the missing piece for you.

306
00:19:28,470 --> 00:19:31,726
So what are we talking about when we say hey, how's this language

307
00:19:31,758 --> 00:19:35,354
support? This means what is the state of the open telemetryhub SDK

308
00:19:35,422 --> 00:19:39,458
for this language, including automated instrumentation.

309
00:19:39,554 --> 00:19:43,080
So in languages like Java and Net,

310
00:19:43,450 --> 00:19:46,966
you should be able to get a ton of metrics out from this

311
00:19:46,988 --> 00:19:50,454
project, automatically doing instrumentation for you and automatically writing

312
00:19:50,502 --> 00:19:54,762
it to whatever data point you want to send it to. So getting

313
00:19:54,816 --> 00:19:58,874
back into that just for a moment, ways to get started

314
00:19:59,072 --> 00:20:02,714
this is from the AWS blog, but one of the things to

315
00:20:02,752 --> 00:20:05,518
remember is that you do have this option about whether or not something is going

316
00:20:05,524 --> 00:20:08,766
to go to a collector or go to some other data

317
00:20:08,948 --> 00:20:12,446
endpoint. And so what's so cool about the collector is lets you

318
00:20:12,468 --> 00:20:14,926
decide how the data is going to be batched and how it's going to be

319
00:20:14,948 --> 00:20:18,414
filtered again, removing Pii and doing other kind of clever

320
00:20:18,462 --> 00:20:22,066
stuff with your data. But if you want to have things work just from day

321
00:20:22,088 --> 00:20:26,014
one, if you want to just try things out, having stuff report directly to Prometheus

322
00:20:26,062 --> 00:20:27,800
is totally an option that you have.

323
00:20:28,730 --> 00:20:32,294
And if you're doing stuff like you want to report metrics every

324
00:20:32,332 --> 00:20:36,018
few seconds or you want report individual spans for a trace,

325
00:20:36,114 --> 00:20:39,786
yeah, that's going to result in a lot of network requests if you're just

326
00:20:39,808 --> 00:20:43,894
reporting directly and you dont have batching and stuff with the collector, but that's

327
00:20:43,942 --> 00:20:46,890
fine for a beta project or a proof of concept.

328
00:20:47,710 --> 00:20:51,078
And then obviously once you do implement a collector,

329
00:20:51,174 --> 00:20:54,286
it's very easy to change over. Also if your data is

330
00:20:54,308 --> 00:20:58,106
quite predictable, if you know what you're going to be doing, if you're using handwritten

331
00:20:58,138 --> 00:21:01,774
calls to report up data. So maybe you're managing pretty well,

332
00:21:01,812 --> 00:21:05,358
you're matching without having to define that on the collector's

333
00:21:05,454 --> 00:21:10,226
side. These are all really good reasons to say,

334
00:21:10,248 --> 00:21:13,220
hey, I'm not going to implement the open telemetry collector quite yet.

335
00:21:15,590 --> 00:21:19,238
Okay folks, that's been my time. I want to thank you so much for

336
00:21:19,244 --> 00:21:22,822
joining me again. Go check out telemetryhub.com for a

337
00:21:22,876 --> 00:21:26,758
really nice, cheap, efficient way to go ahead and report up

338
00:21:26,924 --> 00:21:30,258
open telemetry data. So that's an open telemetry endpoint. So that's

339
00:21:30,354 --> 00:21:33,894
the collector and endpoint get a little bit disclarified there, right?

340
00:21:33,932 --> 00:21:37,654
Your endpoint is where the collector is going to report its data

341
00:21:37,692 --> 00:21:41,018
for users to be able to go and see it. I'm Nocnica mellifera.

342
00:21:41,034 --> 00:21:44,190
You can find me almost every place at serverless mom and

343
00:21:44,260 --> 00:21:47,374
I want to thank you so much for joining me. Okay. Have a great

344
00:21:47,412 --> 00:21:47,690
conference.

