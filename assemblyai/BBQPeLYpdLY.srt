1
00:00:23,290 --> 00:00:27,522
Hi, let's talk about polars. So again, I'm Matt Harrison.

2
00:00:27,586 --> 00:00:31,442
I'm the author of effective pandas Machine Learning, Pocket reference

3
00:00:31,506 --> 00:00:35,138
and illustrate guide to Python three among other books.

4
00:00:35,234 --> 00:00:38,774
I'm an advisor to a company called Ponder that creates an

5
00:00:38,812 --> 00:00:42,098
enterprise version of pandas that lets you run pandas

6
00:00:42,194 --> 00:00:45,446
on your bigquery or

7
00:00:45,548 --> 00:00:49,046
on your snowflake data sets. And I

8
00:00:49,068 --> 00:00:53,194
also do corporate training. So I teach people Python and teach people data science.

9
00:00:53,242 --> 00:01:01,070
So I'm excited to talk about Polars today's.

10
00:01:02,050 --> 00:01:05,286
Okay, quick history of polars or relevant background.

11
00:01:05,338 --> 00:01:08,770
So my background is I've worked with data since

12
00:01:08,840 --> 00:01:12,222
1999. In 2006, I created my own OLAP

13
00:01:12,286 --> 00:01:15,826
engine in Python. Later I heard about pandas and I

14
00:01:15,848 --> 00:01:19,094
started using pandas. In 2016, I wrote a book

15
00:01:19,132 --> 00:01:23,110
about pandas. 2019 did some spark training.

16
00:01:23,260 --> 00:01:26,802
2020, wrote the second edition of the Pandas cookbook.

17
00:01:26,866 --> 00:01:29,942
In 2021, I wrote a book called Effective Pandas.

18
00:01:30,086 --> 00:01:34,054
And recently I've done a bunch of training around QDF

19
00:01:34,102 --> 00:01:36,220
mode and pullers as well.

20
00:01:38,270 --> 00:01:41,558
So I've got a bunch of opinions here and I'm going to just walk you

21
00:01:41,584 --> 00:01:45,022
through a data set and we're going to look at the data

22
00:01:45,076 --> 00:01:48,494
set and look at the types, talk about this thing called

23
00:01:48,532 --> 00:01:54,754
chaining, we'll talk about function application and

24
00:01:54,792 --> 00:01:56,820
we'll talk about aggregation as well.

25
00:02:00,470 --> 00:02:03,474
Okay, so let's run our data. Okay,

26
00:02:03,512 --> 00:02:05,880
so we're going to load our libraries here.

27
00:02:06,810 --> 00:02:11,026
I'm running Polar's version zero point 17

28
00:02:11,138 --> 00:02:14,470
and I'm going to download my data. This is vehicle

29
00:02:14,890 --> 00:02:18,326
data from the US government about cars and

30
00:02:18,348 --> 00:02:21,778
their mileage. So it looks something like this. We've got 41,000 rows

31
00:02:21,794 --> 00:02:25,446
and 83 columns. So we're going to look and there's a bunch of

32
00:02:25,468 --> 00:02:29,142
columns in here. I'm not concerned with all of those, but our initial data

33
00:02:29,196 --> 00:02:32,378
size of the, this is like 33 megs. Okay, so I'm

34
00:02:32,394 --> 00:02:35,118
going to walk through this data set and one of the things you want to

35
00:02:35,124 --> 00:02:38,666
do is get the types right. I'm going to use a subset of the columns

36
00:02:38,698 --> 00:02:41,806
here. So instead of all 83 columns, I'm just going to use this subset here.

37
00:02:41,828 --> 00:02:45,190
And you can see that we've got various types of columns in this polar's

38
00:02:45,210 --> 00:02:48,846
data frame. If you're familiar with pandas, this looks pretty familiar.

39
00:02:48,958 --> 00:02:51,554
We've got Int 64s, float 64s,

40
00:02:51,592 --> 00:02:55,234
UTF eight. And one thing to note is

41
00:02:55,272 --> 00:02:58,194
that polars does have a native stream type,

42
00:02:58,312 --> 00:03:01,462
which in pandas one wasn't the case. We look at the size

43
00:03:01,516 --> 00:03:04,614
of this, we're looking at around eight megs for this. So what I'd do is

44
00:03:04,652 --> 00:03:07,942
I'd just go through this data. Let's pull

45
00:03:07,996 --> 00:03:11,574
out the integers. So I'll do something like this. I'm going to say pull out

46
00:03:11,612 --> 00:03:15,482
the columns I'm interested in and then select the columns that are in 64.

47
00:03:15,536 --> 00:03:18,982
So these are the columns that are in 64. We've got city mileage,

48
00:03:19,046 --> 00:03:22,806
combined mileage, highway mileage, cylinders, the fuel

49
00:03:22,838 --> 00:03:26,574
cost per year, the range, that's how far an electric car will go and the

50
00:03:26,612 --> 00:03:30,334
year. I might want to do a describe on this. And this looks very

51
00:03:30,372 --> 00:03:33,966
similar to what we'd see in pandas. Here we get summary statistics of this.

52
00:03:34,068 --> 00:03:37,294
Now one thing to note about polars is if you're familiar with pandas,

53
00:03:37,342 --> 00:03:40,610
pandas has an index. We don't have an index here, we just have

54
00:03:40,680 --> 00:03:43,998
column names and we have a bunch of columns.

55
00:03:44,094 --> 00:03:47,254
Now I wouldn't write this like this. I would write it like this.

56
00:03:47,292 --> 00:03:50,966
Second option here, it's the same code, but I put parentheses around it

57
00:03:51,068 --> 00:03:55,494
and the parentheses allow us to not

58
00:03:55,532 --> 00:03:59,174
worry about white space. So what I can do is put each operation on

59
00:03:59,212 --> 00:04:03,034
its own line. You can see that that gives me the same result.

60
00:04:03,152 --> 00:04:06,266
Now one things to be aware of is you can see that the types for

61
00:04:06,288 --> 00:04:10,730
each of these. And I probably

62
00:04:10,800 --> 00:04:14,874
don't need to use a floating point, a 64

63
00:04:14,912 --> 00:04:19,022
bit floating point for city mileage that goes up to, it looks like 150.

64
00:04:19,156 --> 00:04:23,066
Combined mileage goes up to 136. So I could probably reduce

65
00:04:23,098 --> 00:04:26,238
these cylinders goes up to 16. And I'm going to

66
00:04:26,244 --> 00:04:29,506
just see if I could use like an int eight,

67
00:04:29,608 --> 00:04:33,314
an eight bit integer. And you can see that that goes from like negative 128

68
00:04:33,352 --> 00:04:36,946
to 127. There's also an unsigned eight bit integer which

69
00:04:36,968 --> 00:04:40,834
goes up to 255, which might be more appropriate. So I'm going to say let's

70
00:04:40,882 --> 00:04:43,798
take my data frame, pull out the columns I want, and then we're going to

71
00:04:43,804 --> 00:04:47,186
use this with columns method and we're going to try and convert

72
00:04:47,218 --> 00:04:50,754
the combined mileage to an int eight. And then we'll do a describe

73
00:04:50,802 --> 00:04:54,202
on that. When I do that I get an error and the error here

74
00:04:54,256 --> 00:04:58,154
says that this strict conversion failed. We've got values like

75
00:04:58,192 --> 00:05:01,498
131, 31 and that did not work.

76
00:05:01,584 --> 00:05:05,466
So instead of doing that with an int eight, let's try unsigned

77
00:05:05,498 --> 00:05:09,390
integer eight. And looks like that does indeed work. So this is kind of nice.

78
00:05:09,540 --> 00:05:12,734
Pandas one didn't check those types, it would just allow you to do

79
00:05:12,772 --> 00:05:16,166
those. We can use other types as well. So int 16.

80
00:05:16,218 --> 00:05:19,326
So I might do something like this where I say let's take the city mileage,

81
00:05:19,358 --> 00:05:22,894
the combined mileage, the highway mileage, the cylinders and displacement,

82
00:05:22,942 --> 00:05:26,142
convert those to unsigned eight and then

83
00:05:26,216 --> 00:05:29,462
range, fuel, cost and year. We'll convert that to

84
00:05:29,516 --> 00:05:32,822
unsigned 16 bit integers and we'll look at the size.

85
00:05:32,876 --> 00:05:36,546
We're now down to about 5.8 megabytes

86
00:05:36,578 --> 00:05:40,106
just by doing those operations without losing any of

87
00:05:40,128 --> 00:05:43,702
our data. Okay, let's look at strings

88
00:05:43,766 --> 00:05:48,250
here and

89
00:05:48,320 --> 00:05:53,294
I'm going to just select the string types. So again

90
00:05:53,332 --> 00:05:56,942
you can see that we've got a native string type here. We've got

91
00:05:56,996 --> 00:06:00,542
drive, which appears to be categorical. We've got this engine

92
00:06:00,596 --> 00:06:04,298
description column which looks like it's pseudocategorical.

93
00:06:04,394 --> 00:06:08,130
It's got a bunch of different entries in there and parentheses and commas,

94
00:06:08,470 --> 00:06:11,566
probably free form text make looks categorical.

95
00:06:11,758 --> 00:06:14,366
Model is probably categorical.

96
00:06:14,478 --> 00:06:18,466
Tranny actually looks like it has two pieces of information, whether it's manual or

97
00:06:18,488 --> 00:06:21,766
automatic, and the number of speeds. And then this created on actually looks

98
00:06:21,788 --> 00:06:24,966
like a date. So we'll look at these and try and deal with these.

99
00:06:25,068 --> 00:06:28,998
Okay, so let's just look at our size.

100
00:06:29,164 --> 00:06:32,374
Remember we have 5.8 megs and I'm going to convert

101
00:06:32,422 --> 00:06:35,562
drive, make and model to categoricals and look at our size

102
00:06:35,616 --> 00:06:39,382
after that. We're down to four megs by doing that. So categorical

103
00:06:39,446 --> 00:06:43,114
is just a string value that doesn't

104
00:06:43,162 --> 00:06:46,720
repeat itself a lot. That looks pretty good.

105
00:06:47,090 --> 00:06:51,022
Let's look at the FFS column here.

106
00:06:51,156 --> 00:06:54,954
I'm going to look at these remaining ones. So I've got engine description

107
00:06:55,002 --> 00:06:58,114
that has FFS, I've got transmission. We want to pull that

108
00:06:58,152 --> 00:07:01,986
into two, the manual and the speeds and then create it on. We'll deal with

109
00:07:02,008 --> 00:07:05,906
that when we talk about dates. So I'm just

110
00:07:05,928 --> 00:07:08,998
going to make a column here. This is an expression so I

111
00:07:09,004 --> 00:07:12,902
can pull up the documentation and you can see that we have like

112
00:07:13,036 --> 00:07:16,918
off of a column expression, we have this Str and off of Str

113
00:07:17,004 --> 00:07:20,310
we have various things that we can do. You can see that we can extract

114
00:07:21,370 --> 00:07:24,394
values from that. So I'm going to do an extraction here.

115
00:07:24,432 --> 00:07:28,058
I'm going to say, let's extract. Then I'm using a raw stream to say in

116
00:07:28,064 --> 00:07:31,226
parentheses, backslash, d plus, that will match all the

117
00:07:31,408 --> 00:07:34,666
numeric values inside of the transmission. And then I'm

118
00:07:34,698 --> 00:07:37,962
also saying alias that as the speeds column.

119
00:07:38,026 --> 00:07:41,454
And then from that same tranny column I'm going to see whether

120
00:07:41,492 --> 00:07:45,102
it contains the manual string and alias that as the manual

121
00:07:45,166 --> 00:07:48,594
column. And it looks like that does indeed work.

122
00:07:48,632 --> 00:07:51,860
If I do that okay,

123
00:07:52,630 --> 00:07:55,506
let's look at the columns after I do that. These are the columns that I

124
00:07:55,528 --> 00:07:57,826
have. And what I'm going to do is I'm going to change my code a

125
00:07:57,848 --> 00:08:01,526
little bit here. I'm going to use that columns I

126
00:08:01,548 --> 00:08:05,014
just printed out here and put it in the select method down below that

127
00:08:05,052 --> 00:08:08,374
to select those columns. Let's look at our estimated size. We're now

128
00:08:08,412 --> 00:08:11,906
down to 2.5 megabytes. Now we might be missing

129
00:08:11,938 --> 00:08:14,746
some values. So I'm going to use a filter method here and I'm going to

130
00:08:14,768 --> 00:08:18,538
say where is drive null? Let's run that. And you can

131
00:08:18,544 --> 00:08:21,882
see here are the values where it's null. Looks like a lot of those are

132
00:08:21,936 --> 00:08:25,454
electric vehicles. So what am I going to do here?

133
00:08:25,572 --> 00:08:29,246
I'm going to say, okay, in drive fill null with other

134
00:08:29,348 --> 00:08:32,858
that's in this value right here. You can see this.

135
00:08:33,044 --> 00:08:37,460
And we're also going to say and put

136
00:08:38,150 --> 00:08:41,298
in engine description. You can

137
00:08:41,304 --> 00:08:44,210
see the engine description is whether we contain it in ffs.

138
00:08:44,890 --> 00:08:47,400
Let's just run that and see if that works.

139
00:08:48,650 --> 00:08:51,240
So do we have a drive here?

140
00:08:52,890 --> 00:08:56,422
And it looks like drive worked there.

141
00:08:56,476 --> 00:09:00,166
It's not complaining. And we've got some missing cylinders as

142
00:09:00,188 --> 00:09:03,626
well. So let's just find out where those are missing. And it

143
00:09:03,648 --> 00:09:06,746
looks like those are electric vehicles as well. So I'm just going to come in

144
00:09:06,768 --> 00:09:10,006
here and save cylinders. If we have missing values,

145
00:09:10,038 --> 00:09:14,046
fill those in with zero and cast that to a un eight. And that

146
00:09:14,068 --> 00:09:17,486
looks like that works as well. Okay, let's look

147
00:09:17,508 --> 00:09:21,534
at our dates. To do this, I'm going to have

148
00:09:21,572 --> 00:09:24,542
to clean up this date column a little bit here.

149
00:09:24,596 --> 00:09:28,194
And I need to know about this replace method here.

150
00:09:28,312 --> 00:09:32,146
So I'm going to use this replace method on created on. If I try

151
00:09:32,168 --> 00:09:35,346
and convert the dates, it's not going to like those EDT and EST.

152
00:09:35,448 --> 00:09:38,610
So I'm just going to replace those with the time zone offsets there.

153
00:09:38,680 --> 00:09:42,246
And then I'm going to call the stir stir p time to convert it

154
00:09:42,268 --> 00:09:45,654
to a date time and say UTC is equal to true.

155
00:09:45,852 --> 00:09:49,030
You can see that created on now says that it is a date time.

156
00:09:49,100 --> 00:09:53,690
So that's pretty cool. If I want to convert this to

157
00:09:53,760 --> 00:09:57,114
a New York City time zone, after I've converted it to a date time,

158
00:09:57,152 --> 00:10:00,634
I can say DT convert time zone. And that looks like that

159
00:10:00,672 --> 00:10:04,442
works as well. Okay. At this point my data

160
00:10:04,496 --> 00:10:08,094
is looking pretty good. I'm going to convert this into a function here. And here

161
00:10:08,132 --> 00:10:11,454
is my function. So one of the things I like to do is work

162
00:10:11,492 --> 00:10:15,278
with the raw data here you can see that I've chained up my

163
00:10:15,364 --> 00:10:18,722
operations starting from autos and then doing all these

164
00:10:18,776 --> 00:10:22,066
operations as I've been creating these. I've been checking them to see that

165
00:10:22,088 --> 00:10:25,586
it works along the way. This is my recommended way and

166
00:10:25,608 --> 00:10:28,370
the polar's recommended way. We call this chaining,

167
00:10:28,710 --> 00:10:32,486
and it's also called flow programming. And you

168
00:10:32,508 --> 00:10:35,466
can see how I created this chain as I was going through. Now, one of

169
00:10:35,468 --> 00:10:39,346
the things that Polars gives us, that pandas doesn't give us is polar's

170
00:10:39,378 --> 00:10:43,050
gives us the ability to be lazy. And it actually has

171
00:10:43,120 --> 00:10:46,426
a query engine that will optimize what's going on there.

172
00:10:46,528 --> 00:10:50,102
So here I'm saying make a function called tweak autos lazy.

173
00:10:50,166 --> 00:10:53,418
It's actually taking in a path. This is

174
00:10:53,504 --> 00:10:57,326
a path to the file. And then you can see I'm saying scan CSV and

175
00:10:57,348 --> 00:11:00,730
then I'm saying lazy. And then down here I'm saying collect.

176
00:11:00,810 --> 00:11:03,454
So what this is going to do is it's not going to execute anything until

177
00:11:03,492 --> 00:11:06,866
I run collect. And then it's going to look at

178
00:11:06,888 --> 00:11:10,066
the columns that I use down here and it's only going to read from the

179
00:11:10,088 --> 00:11:13,954
CSV, the column that I specified down

180
00:11:13,992 --> 00:11:17,586
here. So even though they're 83 columns, it's not going to read everything. And it

181
00:11:17,608 --> 00:11:21,014
can do other things like predicate, push down, et cetera. But you can

182
00:11:21,052 --> 00:11:24,946
run this and see that it does work. So this is one of the reasons

183
00:11:24,978 --> 00:11:28,710
you'll want to chain with polars, because it can actually

184
00:11:28,780 --> 00:11:32,306
make your queries run faster when you do that. Now, a lot of people

185
00:11:32,348 --> 00:11:35,514
say this isn't easy to debug. I disagree with that. I mean,

186
00:11:35,552 --> 00:11:39,098
if I want to debug this, I can, I can stick in a pipe here.

187
00:11:39,184 --> 00:11:41,994
Pipe allows you to take the current state of the data frame and just return

188
00:11:42,032 --> 00:11:44,798
whatever you want. So here I'm just printing out the shape and then returning the

189
00:11:44,804 --> 00:11:48,174
data frame using a short circuit. You can see that I'm doing that before this

190
00:11:48,212 --> 00:11:51,758
operation. I'm doing that down here and I'm doing this at the end.

191
00:11:51,844 --> 00:11:55,102
So I should be able to see how big each of these are.

192
00:11:55,156 --> 00:11:58,386
I'm also piping in the git bar here if you want the

193
00:11:58,408 --> 00:12:02,146
intermediate state. This is just using the globals from Python to

194
00:12:02,168 --> 00:12:05,602
make a variable called DF two with the intermediate state right there.

195
00:12:05,656 --> 00:12:07,460
Let's run that and see what happens.

196
00:12:08,550 --> 00:12:11,942
Okay, you can see that we printed out the size as we're going through

197
00:12:11,996 --> 00:12:15,622
here. And if we check DF two, this is that intermediate state

198
00:12:15,676 --> 00:12:19,186
if we wanted to. So I don't think that chaining

199
00:12:19,218 --> 00:12:23,180
is necessarily hard to debug. It's just something that

200
00:12:23,550 --> 00:12:26,906
as you're building up your code, you will debug it as you're going. And then

201
00:12:26,928 --> 00:12:30,380
if you put it into a function, that makes it really easy to use.

202
00:12:30,750 --> 00:12:34,060
Now another thing you want to be aware of is function application.

203
00:12:34,370 --> 00:12:37,946
So I'm going to make this variable called autos two, which is our cleaned

204
00:12:37,978 --> 00:12:41,806
up data set. And this is again pretty us centric here. Let's try

205
00:12:41,828 --> 00:12:45,314
and convert this to liters per 100. We could

206
00:12:45,352 --> 00:12:49,554
do this by saying apply on that column and that looks like that

207
00:12:49,592 --> 00:12:52,878
is working. Our city is converted to liters

208
00:12:52,894 --> 00:12:56,674
per 100. However, we can get the same result by doing this

209
00:12:56,712 --> 00:13:00,114
instead by just saying 235 divided by that column.

210
00:13:00,242 --> 00:13:03,094
Now when you're doing function application,

211
00:13:03,212 --> 00:13:06,738
you need to be aware that that can be a slow process because what you're

212
00:13:06,754 --> 00:13:10,474
doing is you're taking your data out of the backing store.

213
00:13:10,592 --> 00:13:14,394
The case of polars, it's using the

214
00:13:14,432 --> 00:13:17,990
arrow has a rust implementation of the arrow memory

215
00:13:18,070 --> 00:13:21,962
representation of this. And you're crossing that boundary, converting each

216
00:13:22,016 --> 00:13:25,866
individual value to a python value, running the function and sticking

217
00:13:25,898 --> 00:13:29,342
it back in. So you can see that this

218
00:13:29,396 --> 00:13:32,446
took in this case four milliseconds to run the slow code

219
00:13:32,548 --> 00:13:35,886
and 150 microseconds

220
00:13:35,918 --> 00:13:40,002
to run the fast code. The last time I run this, it's similar order

221
00:13:40,056 --> 00:13:44,114
of magnitude there, almost 300 times slower to do

222
00:13:44,152 --> 00:13:47,526
the function application. So just be aware of that. You're going to

223
00:13:47,548 --> 00:13:51,014
want to stay away from that function application if

224
00:13:51,052 --> 00:13:51,880
you can.

225
00:13:56,490 --> 00:14:00,582
Okay. Another thing you'll want to do is master this thing called aggregation.

226
00:14:00,726 --> 00:14:04,394
And that's like a pivot table or a grouping if

227
00:14:04,432 --> 00:14:07,206
you're familiar with Excel or SQL.

228
00:14:07,398 --> 00:14:10,810
And so here we go. We're going to say let's get the mean

229
00:14:10,880 --> 00:14:13,990
by year and what you want to do is group by the

230
00:14:14,000 --> 00:14:17,246
year column and then we can do the mean of that. You can see this

231
00:14:17,268 --> 00:14:20,894
makes a column called year that has each year. And then

232
00:14:20,932 --> 00:14:24,318
for each value we have the numeric value for that.

233
00:14:24,404 --> 00:14:27,474
This is super powerful and it's basically one line

234
00:14:27,512 --> 00:14:31,442
of code, but I've written it as this chain. Now one of the things

235
00:14:31,496 --> 00:14:34,914
I do like to do is visualize that. So I'm going to

236
00:14:35,032 --> 00:14:38,662
load some plotting code here. Now, another way to do this, if I don't want

237
00:14:38,716 --> 00:14:42,006
to get the mean of everything, I can specify the

238
00:14:42,028 --> 00:14:45,366
columns that I want using this Ag method here. So here

239
00:14:45,388 --> 00:14:48,834
I'm saying pull off the combined mileage column

240
00:14:48,882 --> 00:14:52,294
and the speeds column and take the mean of both of those.

241
00:14:52,412 --> 00:14:55,706
Now, one of the things that pullers doesn't give you that pandas does is the

242
00:14:55,728 --> 00:14:58,886
ability to plot. So if I want to plot this, I'm generally

243
00:14:58,918 --> 00:15:02,894
going to stick it back into pandas. So here I call two pandas, which actually

244
00:15:02,932 --> 00:15:04,510
gives me a data frame.

245
00:15:06,930 --> 00:15:10,222
And because I don't have an index there,

246
00:15:10,276 --> 00:15:13,838
I'm going to stick the year into the index and then I'm going

247
00:15:13,844 --> 00:15:16,738
to call plot on that. And when I do this, the plot is kind of

248
00:15:16,744 --> 00:15:19,778
ugly. The issue here is that if you look at the data,

249
00:15:19,944 --> 00:15:23,854
the index is not sorted because of the optimizations that polars

250
00:15:23,902 --> 00:15:27,298
makes. It tries to run as fast as possible. So I'm going

251
00:15:27,304 --> 00:15:30,118
to just stick in a pandas sort index there. And you can see that we

252
00:15:30,124 --> 00:15:33,558
get a pretty plot coming out of this, allowing us to quickly see around the

253
00:15:33,564 --> 00:15:37,174
year 2010 or so, the combined mileage shot up

254
00:15:37,212 --> 00:15:39,880
quite a bit for the average value there.

255
00:15:40,570 --> 00:15:43,882
Okay, here I'm going to group by year and I'm going to,

256
00:15:43,936 --> 00:15:47,014
instead of taking the mean, I'm going to take the standard deviation.

257
00:15:47,062 --> 00:15:50,586
So once you've got these figured out, it's really easy to change mean for

258
00:15:50,608 --> 00:15:54,014
standard deviation or for other operations that you

259
00:15:54,052 --> 00:15:57,486
might want to do. Now in this case I'm going to try and

260
00:15:57,508 --> 00:16:01,278
add a country and so we might want to know about this win

261
00:16:01,364 --> 00:16:05,646
operation and we're

262
00:16:05,678 --> 00:16:09,614
going to use the is in method

263
00:16:09,662 --> 00:16:13,442
on a column as well. So here I'm going to try and add

264
00:16:13,496 --> 00:16:16,738
a country column and I'm going to say with columns and I'm going

265
00:16:16,744 --> 00:16:19,306
to say PL win. This is how you do an if statement and we're going

266
00:16:19,308 --> 00:16:23,074
to say take that make column. And if it's in Chevy,

267
00:16:23,122 --> 00:16:26,278
Ford, Dodge, GMC or Tesla, then I want a

268
00:16:26,284 --> 00:16:29,414
value of us, otherwise I want a value of other and

269
00:16:29,452 --> 00:16:33,450
we're going to call that country. And when I run this, I get an error

270
00:16:33,950 --> 00:16:37,702
and the error is that it didn't like that I used a categorical

271
00:16:37,766 --> 00:16:41,246
and did this on a categorical. So to get around this, I'm going to

272
00:16:41,268 --> 00:16:45,520
actually cast this back to UTF eight and run this. I should get

273
00:16:49,010 --> 00:16:52,398
a country column now. And then I'm going to say group by. We're going to

274
00:16:52,404 --> 00:16:56,146
group by both year and country, then we're going to aggregate that and here's the

275
00:16:56,168 --> 00:17:00,114
aggregation for that. Now in this case, if I wanted to

276
00:17:00,152 --> 00:17:04,034
plot this by year, I would have to basically pull

277
00:17:04,072 --> 00:17:07,870
out that country and I could do that in pandas.

278
00:17:07,950 --> 00:17:11,826
But if I wanted to do that in polars,

279
00:17:11,938 --> 00:17:15,058
I actually want to use what's called a pivot to do that. So here I'm

280
00:17:15,074 --> 00:17:18,678
going to say let's make our column, our country column and then we're going to

281
00:17:18,684 --> 00:17:22,878
call pivot. We're going to stick year into the index even though polars

282
00:17:22,914 --> 00:17:26,634
doesn't have an index. When we say pivot, we can specify what we want in

283
00:17:26,672 --> 00:17:29,974
the year. And here we're going to say the values are the combined

284
00:17:30,022 --> 00:17:33,430
mileage and the speeds and the columns is the country column

285
00:17:33,510 --> 00:17:37,390
and we're going to aggregate that with the mean function.

286
00:17:37,460 --> 00:17:40,766
So this columns here is going to take the values of country and stick them

287
00:17:40,788 --> 00:17:44,094
up into the columns. Let me just maybe run this for you by

288
00:17:44,132 --> 00:17:47,860
commenting out these other values so you can see what's going on here.

289
00:17:48,950 --> 00:17:52,658
And then we'll just step through the chain. Okay, so this is the result

290
00:17:52,744 --> 00:17:57,506
that we get from doing this. And then

291
00:17:57,528 --> 00:18:01,046
if we convert this to pandas, it looks pretty similar. We need an index here.

292
00:18:01,068 --> 00:18:05,266
So we're going to stick the year into the index. It looks semi

293
00:18:05,298 --> 00:18:09,062
sorted, but it's not. So we're going to sort it and then we'll call

294
00:18:09,116 --> 00:18:12,358
plot on this. That looks pretty good, but the legends in the middle. So we'll

295
00:18:12,374 --> 00:18:14,780
just stick the legend off onto the side there.

296
00:18:15,950 --> 00:18:19,660
Okay, so this lets us look and see that around

297
00:18:21,230 --> 00:18:24,894
2008 or so, we did see a bump up in the

298
00:18:24,932 --> 00:18:28,974
combined mileage. Also looks like speeds has

299
00:18:29,012 --> 00:18:32,000
an upward trend or bend at that point as well.

300
00:18:35,650 --> 00:18:38,990
Okay, so we've looked at the polar's library.

301
00:18:39,330 --> 00:18:42,562
A couple of things to note about this. If you

302
00:18:42,616 --> 00:18:46,098
change your types, the correct types will allow you to save space,

303
00:18:46,264 --> 00:18:50,198
which can allow you to load more data and run things

304
00:18:50,284 --> 00:18:53,970
at a quicker clip. Chaining operations

305
00:18:54,050 --> 00:18:57,366
is going to make your code more readable. It's going to make it look like

306
00:18:57,388 --> 00:19:00,858
a recipe of operations. I also think it removes bugs and makes it

307
00:19:00,864 --> 00:19:02,250
easier to debug,

308
00:19:03,630 --> 00:19:06,826
unlike pandas. Pandas, I recommend chaining as

309
00:19:06,848 --> 00:19:10,778
well. But polars, you actually get an additional benefit from chaining because

310
00:19:10,864 --> 00:19:14,734
it does have a query planner and it can do things like predicate, push down,

311
00:19:14,772 --> 00:19:18,170
et cetera. So you will want to chain

312
00:19:18,330 --> 00:19:22,078
in polars to make the most of it. And then remember

313
00:19:22,164 --> 00:19:25,614
that that function application is slow for math

314
00:19:25,662 --> 00:19:29,442
operations. You want to try and avoid crossing that,

315
00:19:29,576 --> 00:19:32,820
what I would call the rust to python boundary there.

316
00:19:33,190 --> 00:19:36,866
And aggregations are super powerful. If you're not familiar with

317
00:19:36,888 --> 00:19:40,326
them, play around with them. Once you get down the pattern of

318
00:19:40,348 --> 00:19:44,390
them. They're going to be super powerful and help you answer a lot of questions

319
00:19:44,460 --> 00:19:48,466
that you might have. And then I do like to visualize

320
00:19:48,578 --> 00:19:52,346
polars does not have visualization, so I'm going to jump back to pandas to

321
00:19:52,368 --> 00:19:55,930
visualize that. Well, I hope you've enjoyed this

322
00:19:56,000 --> 00:19:59,558
quick introduction to polars. It's a super powerful library

323
00:19:59,654 --> 00:20:03,520
and progressing fast. It has a lot of

324
00:20:04,130 --> 00:20:08,154
capabilities and is a lot faster than pandas

325
00:20:08,202 --> 00:20:12,080
for a lot of operations. So check this out if you need

326
00:20:13,010 --> 00:20:16,606
an option for pandas. Pandas isn't working for you. Polar's, I think,

327
00:20:16,628 --> 00:20:20,334
is in a great place. If you're interested in more content like this,

328
00:20:20,372 --> 00:20:24,030
you can follow me on Twitter Dunder M. Harrison, where I talk a lot

329
00:20:24,100 --> 00:20:27,558
about Python and data science science. Have a

330
00:20:27,564 --> 00:20:29,860
great rest of your day and enjoy the conference. Thanks everyone.

