1
00:00:24,810 --> 00:00:28,662
Hi all and welcome to this Conf42

2
00:00:28,716 --> 00:00:32,098
session called hacking opentelemetry.

3
00:00:32,194 --> 00:00:36,162
My name is Andrea Caretta and I am a senior consultant for liquid

4
00:00:36,226 --> 00:00:39,906
reply it company and I'm here with my colleague

5
00:00:39,938 --> 00:00:43,830
Alberto Gastaldello as observability expert.

6
00:00:44,170 --> 00:00:48,194
Our job consists in searching and or designing monitoring

7
00:00:48,242 --> 00:00:51,578
solution for system at full stack levels,

8
00:00:51,674 --> 00:00:55,066
infrastructure applications Andor front end user behavior,

9
00:00:55,258 --> 00:00:58,554
adopting both enterprise platform and open source

10
00:00:58,602 --> 00:01:02,286
tools reason why our path collided with opentelemetry and we

11
00:01:02,308 --> 00:01:05,806
never left that way. Just a quick summary about

12
00:01:05,908 --> 00:01:09,406
what we're going to show you today. We would like to start with a brief

13
00:01:09,438 --> 00:01:12,706
introduction about how we handle observability and we consider it

14
00:01:12,728 --> 00:01:16,434
so important. Proceeding with the clarification of what open telemetry

15
00:01:16,482 --> 00:01:19,746
is, we will mainly focus on distributed

16
00:01:19,778 --> 00:01:22,546
traces, one of the three observability pillars,

17
00:01:22,658 --> 00:01:25,590
in order to explain how we managed to hack the tool.

18
00:01:25,660 --> 00:01:29,162
But be careful, it's not properly only

19
00:01:29,216 --> 00:01:32,538
a tool. We'll see later and transform the collected data,

20
00:01:32,624 --> 00:01:34,970
showing also a little demonstration.

21
00:01:36,030 --> 00:01:39,206
So let's start with few observability tips.

22
00:01:39,398 --> 00:01:42,606
In system theory, observability is a property able to

23
00:01:42,628 --> 00:01:46,250
measure the level of internal states, determination and interpretation

24
00:01:46,410 --> 00:01:49,918
given input data and external output. It's a

25
00:01:49,924 --> 00:01:52,958
system attribute, of course, not an activity or a tool.

26
00:01:53,044 --> 00:01:56,020
I could adopt tools to reach observability, for example.

27
00:01:56,550 --> 00:02:00,260
It's a kind of mindset instead of a result,

28
00:02:00,630 --> 00:02:04,258
the disposition to realize a system, not only to work,

29
00:02:04,344 --> 00:02:07,240
but in addition to be observed, to be seen.

30
00:02:07,850 --> 00:02:11,202
Let's represent my system as an iceberg where the visible

31
00:02:11,266 --> 00:02:15,350
part it's a kind of a black box where I'm able to understand information

32
00:02:15,500 --> 00:02:19,158
only through outputs, through symptoms. Explain with the

33
00:02:19,164 --> 00:02:22,010
red matrix request rate, error and duration.

34
00:02:22,350 --> 00:02:25,686
I have to open the black box in order to understand the causes

35
00:02:25,718 --> 00:02:28,854
of the outputs. Adopt the use metric utilization,

36
00:02:28,902 --> 00:02:32,858
saturation and errors to perform the root cause analysis logs

37
00:02:32,874 --> 00:02:35,930
are important, the same as a diary of the system events

38
00:02:36,010 --> 00:02:40,190
and many additional pieces of information. Traces instead

39
00:02:40,260 --> 00:02:43,938
are able to correlate user behavior with the last query performed on

40
00:02:43,944 --> 00:02:48,174
the DB, introducing a cause effect concept in a single occurrence

41
00:02:48,302 --> 00:02:51,326
compared to the aggregated values of the matrix.

42
00:02:51,518 --> 00:02:55,494
All of them contribute to fully understand

43
00:02:55,612 --> 00:02:59,202
the system behavior, both in fully operational and anomaly

44
00:02:59,266 --> 00:03:02,742
situation. So what is open

45
00:03:02,796 --> 00:03:06,242
telemetry or better, why Opentelemetry?

46
00:03:06,386 --> 00:03:10,726
Open telemetry was stated as a standard de facto for observability.

47
00:03:10,838 --> 00:03:15,142
During the last years, many leaders, cloud providers

48
00:03:15,286 --> 00:03:19,846
or enterprise observability platform companies contributed

49
00:03:19,878 --> 00:03:23,790
to its development, starting as a framework, spread it in different

50
00:03:23,860 --> 00:03:27,438
programming languages, then it was included in the

51
00:03:27,604 --> 00:03:31,550
CNCF as an incubating project where

52
00:03:31,620 --> 00:03:35,534
every pillar for which opentelemetry defines semantic

53
00:03:35,582 --> 00:03:38,260
rules and references as a maturity level.

54
00:03:38,790 --> 00:03:42,526
Only Tracy specs was released as a stable version,

55
00:03:42,638 --> 00:03:46,246
and this is one of the reasons we're mainly focusing on this pillar instead of

56
00:03:46,268 --> 00:03:49,574
the others. Finally, new softwares with

57
00:03:49,612 --> 00:03:52,822
different purposes than monitoring started to adopt open

58
00:03:52,876 --> 00:03:56,054
telemetry standards to generate valuable telemetry data

59
00:03:56,172 --> 00:03:59,980
and provide them to external tools out of the box.

60
00:04:00,590 --> 00:04:04,154
These kind of solutions are really important to understand where the market is

61
00:04:04,192 --> 00:04:07,462
going. Auto is vendor agnostic,

62
00:04:07,526 --> 00:04:11,690
since new companies are not always so confident

63
00:04:11,770 --> 00:04:14,894
to let agents to be installed on their products.

64
00:04:15,092 --> 00:04:18,750
On the opposite, they prevent every kind of external monitoring.

65
00:04:20,290 --> 00:04:23,682
As I said before, opentelemetry defined data

66
00:04:23,736 --> 00:04:27,506
types, operations and semantic conventions in

67
00:04:27,528 --> 00:04:31,458
the beginning and officially born as a framework composed by different

68
00:04:31,544 --> 00:04:35,006
sdks in different languages to be included in an

69
00:04:35,048 --> 00:04:38,786
applications project. Then monitoring

70
00:04:38,818 --> 00:04:42,886
capabilities were synthesized in agents able to

71
00:04:42,908 --> 00:04:47,362
instrument scripts and virtual machines, j in Java

72
00:04:47,506 --> 00:04:50,922
node js, Python net and many

73
00:04:50,976 --> 00:04:54,678
other technologies. And after a while a kubernetes

74
00:04:54,774 --> 00:04:58,378
operator was created in order to instrument pods with agents

75
00:04:58,464 --> 00:05:01,886
in microservices clusters. So we currently have

76
00:05:01,908 --> 00:05:05,326
the chance for supported technologies to obtain data out of

77
00:05:05,348 --> 00:05:08,400
the box without touching any line of code.

78
00:05:09,010 --> 00:05:12,394
Last but not least, the open telemetry collector

79
00:05:12,442 --> 00:05:16,260
is one of the most important artifacts in the project.

80
00:05:17,590 --> 00:05:20,994
Those is a kind of proxy able to receive opentelemetry data

81
00:05:21,032 --> 00:05:24,306
in different formats, editing and or filtering it,

82
00:05:24,408 --> 00:05:28,338
and sending it in different formats to the desired backend.

83
00:05:28,514 --> 00:05:32,338
Taking a look to the dataflow, it's clear how it's really fundamental

84
00:05:32,434 --> 00:05:36,162
in environments with heterogeneous data sources

85
00:05:36,226 --> 00:05:39,546
to translate in

86
00:05:39,648 --> 00:05:43,670
expected format the information and convey them to target

87
00:05:43,750 --> 00:05:47,082
in different protocols, different compressions and

88
00:05:47,136 --> 00:05:50,686
different serializations. The most

89
00:05:50,708 --> 00:05:54,554
important thing to understand from here on out is that opentelemetry collector

90
00:05:54,602 --> 00:05:58,746
is the tool we hacked to perfectly andor fully handle

91
00:05:58,858 --> 00:06:02,634
telemetry data. So let's go deeper

92
00:06:02,682 --> 00:06:05,862
inside the distributed tracing world. Aderto,

93
00:06:05,946 --> 00:06:09,762
tell us your point. Thank you Andrea. Let's start

94
00:06:09,816 --> 00:06:14,370
with the definition of the free fundamentals concept in distributed tracing.

95
00:06:14,870 --> 00:06:18,386
The trace is the word request record that follows those path

96
00:06:18,418 --> 00:06:22,322
top down. Through the architecture of a system. A trace

97
00:06:22,386 --> 00:06:25,634
is composed of spans that are like a measure unit

98
00:06:25,682 --> 00:06:29,354
in this field. Each span contains all the

99
00:06:29,392 --> 00:06:32,860
details of operation that are made within a service,

100
00:06:33,870 --> 00:06:37,386
allowing us to know how much time the step

101
00:06:37,488 --> 00:06:38,380
has taken.

102
00:06:40,370 --> 00:06:43,978
But how can we correlate all spans

103
00:06:44,074 --> 00:06:46,430
that are related to a single request?

104
00:06:48,290 --> 00:06:52,442
Those tracing context defined by w

105
00:06:52,516 --> 00:06:56,686
three C in 2021 allows to propagate

106
00:06:56,798 --> 00:07:00,626
span data with the trace parent and the trace state

107
00:07:00,728 --> 00:07:04,450
headers. I'd like to point out three more

108
00:07:04,520 --> 00:07:07,970
important definition that I use throughout the webinar.

109
00:07:08,130 --> 00:07:11,906
The trace id identifies the wall distributed

110
00:07:11,938 --> 00:07:15,762
trace. When a span is created,

111
00:07:15,906 --> 00:07:19,690
a unique id is generated for it and that is

112
00:07:19,760 --> 00:07:23,542
the span id. As the request propagates

113
00:07:23,606 --> 00:07:27,210
through services, the caller service passes its

114
00:07:27,280 --> 00:07:30,574
own id to the next service as

115
00:07:30,612 --> 00:07:33,870
the parent span id. Trace Id

116
00:07:33,940 --> 00:07:38,170
and parent span id are then included in the traceparent

117
00:07:38,250 --> 00:07:42,078
header. Let's see together an example

118
00:07:42,164 --> 00:07:46,450
of distributed application this is sockshop,

119
00:07:47,110 --> 00:07:49,460
an open source demo application.

120
00:07:50,470 --> 00:07:53,534
It is the usual website architecture.

121
00:07:53,662 --> 00:07:57,986
We have a front end various other microservices in those backend

122
00:07:58,178 --> 00:08:02,902
with two databases andor a queue manager order,

123
00:08:03,036 --> 00:08:07,122
shipping queue master and cut services are auto instrumented

124
00:08:07,186 --> 00:08:10,914
using the opentelemetry operator. When a request

125
00:08:10,962 --> 00:08:14,358
is received by the backend, a tracer is generated

126
00:08:14,534 --> 00:08:18,394
as the request propagates, then through components, spans are

127
00:08:18,432 --> 00:08:22,302
added to the tree. All those data is sent

128
00:08:22,356 --> 00:08:25,854
to the collector that eventually processes it and

129
00:08:25,892 --> 00:08:29,390
then forwards to dedicated storage backends.

130
00:08:31,650 --> 00:08:35,598
Let's see how those works. We open

131
00:08:35,684 --> 00:08:39,330
sockshop web page. We choose an article

132
00:08:40,150 --> 00:08:42,850
and then add it to the cart.

133
00:08:43,990 --> 00:08:47,922
Now we are opening the cart and proceeding

134
00:08:47,986 --> 00:08:52,310
with the checkout. Let's see what this

135
00:08:52,380 --> 00:08:56,130
last click generated in our observability tracing

136
00:08:56,210 --> 00:08:57,030
backend.

137
00:08:59,790 --> 00:09:03,450
Each span represents a step in the request timeline.

138
00:09:03,870 --> 00:09:07,546
We can reconstruct the path followed by

139
00:09:07,568 --> 00:09:10,826
the request, for example, orders,

140
00:09:10,938 --> 00:09:14,794
call chart services and then the shipping

141
00:09:14,842 --> 00:09:18,286
service. I'm showing you two tools to

142
00:09:18,308 --> 00:09:21,886
highlight the fact that both an enterprise offered like dynatrace and an

143
00:09:21,908 --> 00:09:25,842
open source one like Grafana show traces in the same way

144
00:09:25,976 --> 00:09:28,610
thanks to the standard data format.

145
00:09:30,710 --> 00:09:34,338
Okay, all nice and clear, but what happens

146
00:09:34,424 --> 00:09:37,730
when we do not have those possibility to export

147
00:09:37,810 --> 00:09:41,794
tracing data? Can we make the system observable

148
00:09:41,922 --> 00:09:45,542
in some way? Fortunately, the answer is

149
00:09:45,596 --> 00:09:48,906
yes. Let's see how in

150
00:09:48,928 --> 00:09:52,726
many situations we encounter application that are not instrumentable

151
00:09:52,838 --> 00:09:57,050
due to restriction imposed by software providers.

152
00:09:58,110 --> 00:10:02,362
They just send out logs that are then stored

153
00:10:02,426 --> 00:10:06,666
in a database, eventually passing through a telemetry

154
00:10:06,698 --> 00:10:10,206
processing layer that decouples the application and

155
00:10:10,228 --> 00:10:13,354
the backend. Unfortunately,

156
00:10:13,482 --> 00:10:17,086
in this way we only deliver data for

157
00:10:17,108 --> 00:10:20,610
the second of observability. Pillar logs.

158
00:10:21,510 --> 00:10:24,686
For the sake of simplicity, we leave out the metrics.

159
00:10:24,718 --> 00:10:28,280
Pillar Andor focus on the traces from now on.

160
00:10:29,770 --> 00:10:33,190
When application logs contain tracing data that is

161
00:10:33,260 --> 00:10:36,786
as said trace id span id andor parent span

162
00:10:36,818 --> 00:10:40,620
id. This is all we need to correlate them

163
00:10:41,550 --> 00:10:45,066
in order to transform a log into a trace pan. We can

164
00:10:45,088 --> 00:10:48,554
leverage the processing layer. We use

165
00:10:48,752 --> 00:10:52,266
those Otel collector for those Andor now let's

166
00:10:52,298 --> 00:10:55,614
see how it works Andor how it can be manipulated to

167
00:10:55,652 --> 00:10:59,210
reach our goal. The collector

168
00:10:59,290 --> 00:11:01,870
is composed of three main modules,

169
00:11:02,870 --> 00:11:05,646
receivers, processors,

170
00:11:05,838 --> 00:11:09,390
exporters. The receivers accept

171
00:11:09,470 --> 00:11:12,290
telemetry data with different protocols.

172
00:11:12,950 --> 00:11:17,590
Processors allow data filtering modification or transformation.

173
00:11:18,410 --> 00:11:22,322
Exporters send that elaborated data to endpoint

174
00:11:22,386 --> 00:11:26,674
storage. There are two collector versions,

175
00:11:26,802 --> 00:11:30,694
those youre and the contrib. This last one includes

176
00:11:30,742 --> 00:11:34,742
the youre modules and all the additional modules developed

177
00:11:34,806 --> 00:11:38,570
by contributors for their purposes.

178
00:11:40,930 --> 00:11:44,330
As you can see, collector components are defined

179
00:11:44,410 --> 00:11:47,610
in a YAML config file listed

180
00:11:47,690 --> 00:11:51,722
in their own sections, with the chance to customize

181
00:11:51,786 --> 00:11:55,662
them with many comments. The highlighted collection

182
00:11:55,806 --> 00:11:59,374
is the service section where pipelines

183
00:11:59,422 --> 00:12:02,686
can be configured and each component

184
00:12:02,798 --> 00:12:06,342
can be enabled only in the step it is made

185
00:12:06,396 --> 00:12:10,114
for. We have to be careful about the pipelines

186
00:12:10,162 --> 00:12:13,794
in the auto collector releases because they are fundamental

187
00:12:13,842 --> 00:12:17,410
to understand. From here they arise from pillars division

188
00:12:17,490 --> 00:12:21,290
and they are really independent from each other. A trace information

189
00:12:21,440 --> 00:12:25,274
received could be only handled as a trace andor sent anywhere as

190
00:12:25,312 --> 00:12:29,020
a trace, same as metrics and log at every step.

191
00:12:29,970 --> 00:12:33,502
What we found out breaks this model. We paid

192
00:12:33,556 --> 00:12:37,246
our attention on logs pipeline andor. We detect a

193
00:12:37,268 --> 00:12:40,890
point in which information could be manipulated

194
00:12:40,970 --> 00:12:44,734
to switch from a structure dedicated to logs to another structure

195
00:12:44,862 --> 00:12:48,594
related to another pillar. In this case, we were interested

196
00:12:48,712 --> 00:12:52,226
in trace structure. We built

197
00:12:52,328 --> 00:12:56,242
an exported able to retrieve trace id and span id values

198
00:12:56,306 --> 00:13:00,182
contained in the log and represent them as

199
00:13:00,236 --> 00:13:04,050
key values for spans before sending them to a target.

200
00:13:04,210 --> 00:13:11,094
Able to represent distributed traces with

201
00:13:11,132 --> 00:13:14,210
an application able to produce only logs,

202
00:13:14,370 --> 00:13:18,378
meaningful logs with tracing ids we have in

203
00:13:18,384 --> 00:13:22,050
the end all we need to create correlated spans

204
00:13:22,230 --> 00:13:26,238
and so distributed trace the

205
00:13:26,324 --> 00:13:29,310
translation takes place in those collector.

206
00:13:29,890 --> 00:13:33,198
It leads to a well formatted trace pan that

207
00:13:33,284 --> 00:13:36,914
becomes a fundamental piece across the end to end

208
00:13:36,952 --> 00:13:40,850
path where a trace before could have been broken.

209
00:13:41,670 --> 00:13:44,866
Then the modified exporter forwards the

210
00:13:44,888 --> 00:13:48,434
log to the log storage in our case locky,

211
00:13:48,562 --> 00:13:51,954
and then the trace in the tracing backends

212
00:13:52,082 --> 00:13:56,070
for our case Grafana, tempo and Dynatrace.

213
00:13:57,610 --> 00:14:00,620
Let's now see how this works with a demo.

214
00:14:01,550 --> 00:14:05,638
We compiled our modified version of those opentelemetry

215
00:14:05,654 --> 00:14:09,446
collector and now we run those executable

216
00:14:09,558 --> 00:14:13,360
in order to have the service listening on a local port,

217
00:14:15,250 --> 00:14:19,134
we created a python script that simulates an application

218
00:14:19,332 --> 00:14:22,986
sending event logs. It sends

219
00:14:23,098 --> 00:14:26,834
logs in the syslog format to the local port of

220
00:14:26,872 --> 00:14:29,730
the collector that is running locally.

221
00:14:31,110 --> 00:14:34,510
We decided to focus on modifying the exporter

222
00:14:34,590 --> 00:14:39,110
at the chain level is the last point to modify data before transmission

223
00:14:40,090 --> 00:14:43,654
the collector takes each log and transforms it

224
00:14:43,692 --> 00:14:47,586
into a trace, then sends the original

225
00:14:47,618 --> 00:14:51,226
log to locky and the trace to both Grafana tempo and

226
00:14:51,248 --> 00:14:55,382
Dynatrace. Let's open our tools

227
00:14:55,446 --> 00:14:59,226
to see how this can be visualized in

228
00:14:59,248 --> 00:15:02,974
Grafana. We have those possibility to see logs and traces in the same

229
00:15:03,012 --> 00:15:07,482
dashboard. With a logkey query. We can find our generated

230
00:15:07,546 --> 00:15:11,790
logs pretty formatted in JSON for better visualization.

231
00:15:12,850 --> 00:15:16,782
This highlights the switch from the logs pillar to the traces

232
00:15:16,846 --> 00:15:20,082
pillar. From here we can

233
00:15:20,136 --> 00:15:23,394
directly find the corresponding trace thanks to

234
00:15:23,432 --> 00:15:26,858
the integration that queries Grafana tempo.

235
00:15:26,974 --> 00:15:28,760
Looking for the trace Id.

236
00:15:32,170 --> 00:15:35,030
The visualization is pretty straightforward.

237
00:15:35,370 --> 00:15:38,102
Those same can be seen in the other tool,

238
00:15:38,236 --> 00:15:42,326
Dynatrace. We navigate into distributed

239
00:15:42,358 --> 00:15:46,810
traces and visualize the trace generated by our Python script.

240
00:15:48,190 --> 00:15:52,634
Today you saw how to manipulate and transform telemetry

241
00:15:52,682 --> 00:15:56,682
data for your purposes. When complex distributed

242
00:15:56,746 --> 00:16:00,650
systems handle data adopting different formats,

243
00:16:00,810 --> 00:16:04,442
opentelemetry allows you to define pipelines

244
00:16:04,506 --> 00:16:08,698
and move data wherever you prefer. Become agnostic

245
00:16:08,794 --> 00:16:13,006
from every type of youre thank

246
00:16:13,028 --> 00:16:16,726
you for watching. For any question about this session or

247
00:16:16,788 --> 00:16:20,454
our offer, feel free to reach us through socials or

248
00:16:20,492 --> 00:16:21,480
via email.

