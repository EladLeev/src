1
00:00:23,210 --> 00:00:27,090
Hi, everybody. Welcome to my vic talk. I'm Daniel.

2
00:00:27,170 --> 00:00:30,246
I used to be a tech lead at YouTube for seven years, and now I

3
00:00:30,268 --> 00:00:34,402
work in the vector ops space with superlinked.

4
00:00:34,546 --> 00:00:38,358
Let's just dive right in. So,

5
00:00:38,444 --> 00:00:41,960
in today's talk, we'll cover the three most important questions.

6
00:00:42,570 --> 00:00:46,280
Why are vectors useful? Why use them to

7
00:00:46,650 --> 00:00:50,222
make your app better? We'll talk about which

8
00:00:50,276 --> 00:00:54,650
features of your app benefit from being vector powered,

9
00:00:54,810 --> 00:00:58,686
and then finally, we'll talk about actually

10
00:00:58,788 --> 00:01:02,206
building a vector powered feature of your app in a

11
00:01:02,228 --> 00:01:06,610
way that makes it easy to put it into production

12
00:01:07,190 --> 00:01:09,700
in a reliable way. And finally,

13
00:01:11,510 --> 00:01:15,154
you unlock the benefit for your end users, which is why

14
00:01:15,192 --> 00:01:18,598
we are doing this. We are using new technology to make your app

15
00:01:18,684 --> 00:01:22,390
better. So let's go into the why first,

16
00:01:22,460 --> 00:01:26,658
though. Because I think vectors can be not so intuitive,

17
00:01:26,754 --> 00:01:30,634
and it helps to have a mental model for why

18
00:01:30,672 --> 00:01:34,426
they are different from perhaps other representations of data that you

19
00:01:34,448 --> 00:01:35,740
have used before.

20
00:01:36,910 --> 00:01:40,622
So we have to kind of start from,

21
00:01:40,676 --> 00:01:45,214
really the basement here, and first look at when

22
00:01:45,252 --> 00:01:49,520
humans adopted language. We kind of lost something in that process.

23
00:01:50,690 --> 00:01:53,678
And I'll try to illustrate that with a few pictures here.

24
00:01:53,764 --> 00:01:57,314
So, if you look at this slide, you see, okay, on the left side,

25
00:01:57,352 --> 00:02:01,246
we have kind of a famous landscape picture for the Windows

26
00:02:01,278 --> 00:02:05,038
fans, but there and then on the right side, we have a

27
00:02:05,064 --> 00:02:09,106
representation of that picture in the RGB

28
00:02:09,138 --> 00:02:13,110
values of the individual pixels of that

29
00:02:13,260 --> 00:02:16,914
photo, right? And we have, let's say, about 1 million pixels

30
00:02:16,962 --> 00:02:20,634
in there, and each is defined with these three values. So we have a lot

31
00:02:20,672 --> 00:02:25,114
of data. We have a book worth of

32
00:02:25,152 --> 00:02:28,806
data in there that represents what each of these pixels

33
00:02:28,918 --> 00:02:32,080
looks like. Right? Now,

34
00:02:33,010 --> 00:02:36,574
a human would translate this picture into

35
00:02:36,692 --> 00:02:40,334
words, right, in order to communicate with others or kind of

36
00:02:40,372 --> 00:02:44,282
remember it. And so this is very efficient

37
00:02:44,346 --> 00:02:48,446
on the surface, right. We would say, let's say field of grass to describe

38
00:02:48,558 --> 00:02:52,194
what this picture contains. And then this is just

39
00:02:52,232 --> 00:02:56,182
three words, right? So we went from 1 into something

40
00:02:56,236 --> 00:02:58,930
very short. So this looks pretty efficient,

41
00:02:59,010 --> 00:03:02,198
right. However, the problem

42
00:03:02,364 --> 00:03:05,766
is that when

43
00:03:05,788 --> 00:03:09,114
you say field of grass, you may mean lots of

44
00:03:09,152 --> 00:03:13,980
different things. We have lost the resolution of

45
00:03:14,590 --> 00:03:18,074
similar pictures being called the same thing.

46
00:03:18,192 --> 00:03:21,694
All of these different photos kind of collapse to those three words,

47
00:03:21,812 --> 00:03:26,126
right? So let's say I did this experiment with

48
00:03:26,228 --> 00:03:29,854
mid journey, and I took field of grass and put

49
00:03:29,892 --> 00:03:33,294
it in as an input, right? And I got this field of

50
00:03:33,332 --> 00:03:36,810
grass, right. Looks very different from where we started.

51
00:03:36,980 --> 00:03:40,162
So then you might want to sort of start refining that, right?

52
00:03:40,296 --> 00:03:43,634
To kind of regain back the resolution that we lost by going

53
00:03:43,672 --> 00:03:47,522
from 1 words. So we start to specify more words,

54
00:03:47,576 --> 00:03:51,206
right? So we say field of grass on a summer day that kind

55
00:03:51,228 --> 00:03:54,818
of looks a bit closer to what we had before, but still worlds apart.

56
00:03:54,914 --> 00:03:58,266
And so we can kind of keep adding more and more words to try to

57
00:03:58,288 --> 00:04:02,058
regain that resolution. And after a

58
00:04:02,064 --> 00:04:05,786
while we get to a field of

59
00:04:05,808 --> 00:04:09,046
grass on a summer day, a few clouds in the blue sky,

60
00:04:09,078 --> 00:04:12,362
and it's slightly hilly, right? And it sort of like gets somewhere,

61
00:04:12,426 --> 00:04:15,150
but it's still a very different picture.

62
00:04:15,810 --> 00:04:19,886
So on the left we have the original, and on the right we

63
00:04:19,908 --> 00:04:24,260
have the kind of approximate reconstruction from those

64
00:04:24,710 --> 00:04:28,434
16 words or whatever that was. And in this

65
00:04:28,472 --> 00:04:32,354
demonstration we see how the natural language is a bottleneck of

66
00:04:32,392 --> 00:04:35,994
communication, right? It's a very imperfect

67
00:04:36,062 --> 00:04:39,218
way to describe stuff, and it misses the nuance,

68
00:04:39,314 --> 00:04:43,526
right? It's very hard to describe small changes

69
00:04:43,708 --> 00:04:47,138
in the input in a way that is preserved in the output.

70
00:04:47,234 --> 00:04:50,620
So this is a problem of language, right?

71
00:04:53,230 --> 00:04:56,710
It's not only hard to reconstruct what we described,

72
00:04:56,790 --> 00:04:59,842
but also the descriptions are ambiguous,

73
00:04:59,926 --> 00:05:03,486
right? All of these three images are

74
00:05:03,588 --> 00:05:06,080
constructed from those same 16 words,

75
00:05:06,690 --> 00:05:09,920
field of grass, hilly, summer day and so on.

76
00:05:11,090 --> 00:05:14,546
So basically what I'm saying is words are

77
00:05:14,568 --> 00:05:19,266
not very useful to represent information in

78
00:05:19,288 --> 00:05:22,674
terms of, for computers. And it's not just

79
00:05:22,712 --> 00:05:26,454
the words, it's any sort of structured data that you

80
00:05:26,492 --> 00:05:29,670
are using to represent stuff out there in the real world,

81
00:05:29,740 --> 00:05:33,442
right? Once you go from audio recordings that are analog

82
00:05:33,506 --> 00:05:36,662
recordings and pictures, and you try

83
00:05:36,716 --> 00:05:40,906
to represent it with some structured information, you'll just basically

84
00:05:41,008 --> 00:05:43,420
lose a bunch of the resolution there.

85
00:05:45,790 --> 00:05:50,034
So that makes those kind of structured representations not very useful

86
00:05:50,182 --> 00:05:54,106
for actually exploring the different concepts

87
00:05:54,218 --> 00:05:56,990
and for example, training machine learning models.

88
00:05:58,450 --> 00:06:02,910
And here is where we introduce vectors, basically because

89
00:06:02,980 --> 00:06:06,482
vectors give us the resolution that we need, right? So just

90
00:06:06,536 --> 00:06:09,970
imagine we have a set of two dimensional points

91
00:06:10,040 --> 00:06:13,586
on the xy axis, and we have some sort of

92
00:06:13,688 --> 00:06:16,594
function that given a picture,

93
00:06:16,722 --> 00:06:20,326
outputs xy coordinate, right?

94
00:06:20,508 --> 00:06:23,846
And we feed the pictures that we have been

95
00:06:23,868 --> 00:06:28,042
looking at into that function. And that function can

96
00:06:28,096 --> 00:06:31,850
represent similarities and differences in that space

97
00:06:31,920 --> 00:06:35,178
of different pictures of a grassy field.

98
00:06:35,344 --> 00:06:39,002
And this kind of resolution and

99
00:06:39,056 --> 00:06:42,686
nuance is what words are, or any

100
00:06:42,708 --> 00:06:46,090
sort of structured data is basically lacking.

101
00:06:46,250 --> 00:06:49,866
And this is why a vector representation is kind of closer

102
00:06:49,898 --> 00:06:53,426
to the truth in a way. And so,

103
00:06:53,608 --> 00:06:57,198
just to summarize, the reason vectors are useful

104
00:06:57,294 --> 00:07:01,102
is they are expressive, right? They allow you to capture

105
00:07:01,166 --> 00:07:04,610
differences between things that you cannot otherwise express.

106
00:07:05,370 --> 00:07:09,400
They're smooth. So things that are close

107
00:07:09,930 --> 00:07:13,750
by in the vector space are kind of gradual,

108
00:07:18,090 --> 00:07:21,498
they are similar items that gradually get less and less

109
00:07:21,584 --> 00:07:25,110
similar as youll go further in that space. And so it's smooth,

110
00:07:25,190 --> 00:07:29,194
which means you can explore it, right? You can kind of navigate, you can get

111
00:07:29,232 --> 00:07:32,494
closer to certain parts of the space,

112
00:07:32,612 --> 00:07:36,254
and this makes it very useful for any sort of machine learning that

113
00:07:36,292 --> 00:07:39,120
is basically doing the exploration, right.

114
00:07:40,450 --> 00:07:45,122
The downside is that vectors are very difficult to work with because

115
00:07:45,176 --> 00:07:49,234
they are just basically a list of

116
00:07:49,272 --> 00:07:52,370
numbers, right? And when you print out a vector,

117
00:07:53,030 --> 00:07:55,300
it doesn't really tell youll anything, right?

118
00:07:56,170 --> 00:08:00,214
So that

119
00:08:00,252 --> 00:08:04,818
sort of answers the question of why vectors, right? They're useful representations

120
00:08:04,994 --> 00:08:08,806
of data because of those properties.

121
00:08:08,998 --> 00:08:12,970
Now, in terms of what youll can build with vectors,

122
00:08:13,710 --> 00:08:18,118
obviously the world of surgeon recommendations existed

123
00:08:18,214 --> 00:08:23,306
before this kind of latest discovery

124
00:08:23,338 --> 00:08:26,682
of the transformer models that help us generate these vectors

125
00:08:26,826 --> 00:08:30,862
and these whole new hyped with vector databases. Obviously the search

126
00:08:30,916 --> 00:08:34,750
and recommendation space has existed for decades.

127
00:08:34,910 --> 00:08:38,546
So let's just have a look at how that space is doing sort

128
00:08:38,568 --> 00:08:41,140
of pre this type of technology,

129
00:08:42,070 --> 00:08:47,270
and let's look at search and recommendations specifically. So before

130
00:08:47,340 --> 00:08:50,870
using vector representations to build search interfaces,

131
00:08:52,490 --> 00:08:56,534
it was all about keywords, right? So you

132
00:08:56,572 --> 00:09:00,170
had some sort of natural language processing pipeline that tried

133
00:09:00,240 --> 00:09:03,642
to normalize the keywords in your documents and

134
00:09:03,696 --> 00:09:07,002
in your search queries, try to perhaps expand them to

135
00:09:07,056 --> 00:09:11,370
synonyms and do all kinds of precomputation

136
00:09:11,530 --> 00:09:15,226
and expansion. So you had a bunch of natural

137
00:09:15,258 --> 00:09:18,400
language processing functionality in there.

138
00:09:18,850 --> 00:09:22,658
Then you had some sort of index that, given a

139
00:09:22,664 --> 00:09:26,494
set of keywords or a keyword, pointed you to documents

140
00:09:26,542 --> 00:09:30,434
that contain those, right? So that

141
00:09:30,472 --> 00:09:33,300
was the core aspect that powered your search.

142
00:09:33,910 --> 00:09:37,446
So that's why it's really important to get those keywords right and

143
00:09:37,468 --> 00:09:41,160
normalized and processed. And then finally,

144
00:09:42,170 --> 00:09:45,634
these types of systems are very much fine tuned

145
00:09:45,682 --> 00:09:49,242
by hand, right? Observing queries that you haven't seen

146
00:09:49,296 --> 00:09:53,290
before, trying to figure out how to tweak your keyword based

147
00:09:53,360 --> 00:09:56,906
rules to get good results. And this has been

148
00:09:56,928 --> 00:10:00,534
happening for decades. And the outcome

149
00:10:00,582 --> 00:10:03,600
is that it still kind of doesn't really work that well.

150
00:10:04,130 --> 00:10:08,080
And here I have a Walgreens I could have used any example.

151
00:10:09,170 --> 00:10:12,710
So this is an online pharmacy basically, and I type in splitting headache,

152
00:10:12,730 --> 00:10:16,830
and I'm basically getting one result, which is essential oil

153
00:10:16,910 --> 00:10:21,058
and no pain medication, nothing for

154
00:10:21,224 --> 00:10:24,546
migraines, right? Because I used keywords that

155
00:10:24,568 --> 00:10:27,878
are not exactly matching the results. And so

156
00:10:27,964 --> 00:10:31,286
a keyword based system gets it

157
00:10:31,308 --> 00:10:33,800
wrong. Right? Now,

158
00:10:35,610 --> 00:10:39,526
how is this looking in the recommendation side of the problem? Because surgeon

159
00:10:39,558 --> 00:10:43,638
recommendations are very much linked, right? This is kind of two sides

160
00:10:43,654 --> 00:10:47,194
of the same coin. So I'm sure you have been

161
00:10:47,232 --> 00:10:51,082
to LinkedIn and you have seen your jobs recommendations,

162
00:10:51,226 --> 00:10:55,258
and this is a screenshot of mine and they're particularly funny.

163
00:10:55,354 --> 00:10:59,034
So I not so rarely

164
00:10:59,082 --> 00:11:02,614
get recommendations for intern jobs after being in the industry

165
00:11:02,682 --> 00:11:05,300
for 15 plus years.

166
00:11:07,190 --> 00:11:11,810
And then also some sort of stealth, like this stealth

167
00:11:12,150 --> 00:11:15,766
company. It's kind of a meme. If you search for

168
00:11:15,788 --> 00:11:19,510
stealth on LinkedIn, you'll see that there are tens of thousands of people

169
00:11:19,580 --> 00:11:22,946
working at stealth and this entity shouldn't

170
00:11:22,978 --> 00:11:26,262
really be recommended. So basically we still

171
00:11:26,316 --> 00:11:29,946
get recommendations wrong, same as

172
00:11:29,968 --> 00:11:33,574
the search. The problems are somewhat

173
00:11:33,622 --> 00:11:37,386
related, but actually recommendations have their own pile of

174
00:11:37,408 --> 00:11:40,954
problems. Let's just look at a few of them. So how would you build

175
00:11:40,992 --> 00:11:44,894
a recommender system? Before vectors, you would

176
00:11:45,092 --> 00:11:48,526
try to combine two aspects of your data, right? You would have

177
00:11:48,628 --> 00:11:52,414
content features. So you try to have metadata for your content that

178
00:11:52,452 --> 00:11:56,306
help you understand what that content is about. You would

179
00:11:56,328 --> 00:11:59,794
also have that for your users, right, stuff they tell you during sign

180
00:11:59,832 --> 00:12:03,074
up or any

181
00:12:03,112 --> 00:12:06,386
metadata that they create about themselves. So that's

182
00:12:06,418 --> 00:12:09,800
the content features. Then you would have collaborative features, which is

183
00:12:10,330 --> 00:12:14,678
features based on users behavior. So kind of similar,

184
00:12:14,764 --> 00:12:18,258
users liking similar things, type of signal,

185
00:12:18,354 --> 00:12:21,882
right? And you would sort of build those features and you would build the content

186
00:12:21,936 --> 00:12:25,546
features and you would try to marry them together in

187
00:12:25,568 --> 00:12:29,418
a model that responds well if you

188
00:12:29,424 --> 00:12:32,906
don't have enough behavioral data for the user responds well if you don't

189
00:12:32,938 --> 00:12:36,222
have enough content features. And kind of lets best of

190
00:12:36,276 --> 00:12:39,470
both worlds. And usually this work is quite custom,

191
00:12:39,540 --> 00:12:42,994
right? So companies do this in house with a lot of

192
00:12:43,112 --> 00:12:47,554
effort and time after

193
00:12:47,592 --> 00:12:51,502
being able to marry those two aspects. Then a recommender engine typically

194
00:12:51,566 --> 00:12:55,054
has kind of two steps, right? So first you

195
00:12:55,192 --> 00:12:59,042
use some sort of feature

196
00:12:59,106 --> 00:13:02,962
store, some sort of database to retrieve candidates that roughly

197
00:13:03,026 --> 00:13:06,098
match what you think would be a good recommendation,

198
00:13:06,194 --> 00:13:09,974
right? But because it's a RaF kind of search,

199
00:13:10,172 --> 00:13:13,574
kind of broad strokes, type constraints,

200
00:13:13,702 --> 00:13:17,334
you have to retrieve a lot of candidates, right, because youll know that the candidates

201
00:13:17,382 --> 00:13:21,238
will be low quality. So for the content you want to recommend, you kind of

202
00:13:21,264 --> 00:13:25,178
take a broad brush and somehow fetch 10,000 candidates

203
00:13:25,274 --> 00:13:28,366
for that recommendation that you hope that

204
00:13:28,388 --> 00:13:31,962
some of them actually will make sense. So that's the retrieval typically

205
00:13:32,026 --> 00:13:35,342
then, and then you have the ranking or the scoring,

206
00:13:35,406 --> 00:13:39,234
right? So you have a model that given a candidate piece

207
00:13:39,272 --> 00:13:42,866
of content and the user predicts the

208
00:13:42,888 --> 00:13:46,546
probability that this user will actually like the content or will

209
00:13:46,568 --> 00:13:49,640
engage with it, or let's say if it's a job that they'll apply,

210
00:13:50,730 --> 00:13:53,810
you have this model that you run over all the candidates,

211
00:13:53,890 --> 00:13:58,006
right, 10,000 times. And then you sort the candidates by

212
00:13:58,108 --> 00:14:01,914
this score and then finally find three

213
00:14:01,952 --> 00:14:05,542
jobs, let's say, out of those 10,000 that have the highest score,

214
00:14:05,606 --> 00:14:09,146
and then those you return to the user, right. This is

215
00:14:09,168 --> 00:14:12,734
a project that this whole system takes months to build,

216
00:14:12,852 --> 00:14:17,662
typically is done in a custom way, and even

217
00:14:17,716 --> 00:14:21,886
for a very large company, it can still miss

218
00:14:21,988 --> 00:14:25,682
terribly. Right. So basically,

219
00:14:25,816 --> 00:14:31,906
surgeon recommendations still an open problem. And this

220
00:14:31,928 --> 00:14:35,474
is the before vectors world. Obviously, I'm not saying with

221
00:14:35,512 --> 00:14:38,706
the vectors it's a solved problem, but we have some new approaches

222
00:14:38,738 --> 00:14:41,894
to tackle it and see what we can do

223
00:14:41,932 --> 00:14:45,126
there. Right. So let's look at the kind

224
00:14:45,148 --> 00:14:49,210
of new world. How do you build certain recommendations with

225
00:14:49,280 --> 00:14:50,250
vectors?

226
00:14:52,430 --> 00:14:56,006
If, let's say you want to treat your vectors as just features

227
00:14:56,038 --> 00:14:59,546
that you extracted, and then build the

228
00:14:59,568 --> 00:15:02,350
old school stack with retrieval and ranking on top,

229
00:15:02,500 --> 00:15:06,640
that's fine, that works, right. What I want to look at here is

230
00:15:08,290 --> 00:15:11,710
new ways that you can use the fact that we can actually

231
00:15:11,780 --> 00:15:15,154
index these vectors and find similar vectors really

232
00:15:15,272 --> 00:15:19,138
fast. Right. So basically, we'll kind

233
00:15:19,144 --> 00:15:21,634
of focus on that aspect of the problem.

234
00:15:21,832 --> 00:15:25,410
And if you look at the stack of this kind of

235
00:15:25,480 --> 00:15:28,326
new generation surgeon recommender system,

236
00:15:28,508 --> 00:15:32,600
we'll look at three aspects of it. So how to basically

237
00:15:33,450 --> 00:15:36,866
strengthen the retrieval part, given that we can do the nearest

238
00:15:36,898 --> 00:15:40,474
neighbor search. We'll look at representations of

239
00:15:40,512 --> 00:15:44,534
content and also users with vectors

240
00:15:44,582 --> 00:15:48,330
that allow us to do that. And then we'll talk about maybe

241
00:15:48,400 --> 00:15:52,578
having a component on top of this that will be much simpler

242
00:15:52,614 --> 00:15:56,206
and faster than the normal kind of ranking that

243
00:15:56,228 --> 00:15:59,614
sits on top of the retrieval. We'll talk about probably still

244
00:15:59,652 --> 00:16:02,430
a need of having that component, but it's much simpler.

245
00:16:03,350 --> 00:16:07,426
The benefits of this is that the

246
00:16:07,448 --> 00:16:11,570
whole stack is much simplified because you basically just have the retrieval.

247
00:16:12,870 --> 00:16:16,614
The results are, you don't rely on this

248
00:16:16,652 --> 00:16:20,438
kind of candidate retrieval up front, which probably misses a

249
00:16:20,444 --> 00:16:24,002
lot of the stuff you actually wanted to recommend. And then ranking

250
00:16:24,146 --> 00:16:27,830
this actually is, in terms of recall,

251
00:16:28,270 --> 00:16:32,010
not ideal, right, because what if you missed something out there,

252
00:16:32,160 --> 00:16:36,060
even though you retrieved the 10,000 items? So in that sense,

253
00:16:37,150 --> 00:16:40,566
the approximate nearest neighbor search is closer

254
00:16:40,598 --> 00:16:44,494
to a global optimum. Right, because you are basically indexing the whole

255
00:16:44,532 --> 00:16:47,822
database. You are not just running your

256
00:16:47,876 --> 00:16:50,730
ranking on some 10,000 candidates.

257
00:16:50,890 --> 00:16:54,474
So in that sense it's better and then finally faster,

258
00:16:54,522 --> 00:16:57,618
right, because you are not doing ranking, you are not running some big model on

259
00:16:57,624 --> 00:17:01,730
top of the candidates, you are not fetching 10,000 candidates from a database.

260
00:17:02,310 --> 00:17:06,514
So you push everything into the approximate nearest neighbor index.

261
00:17:06,642 --> 00:17:10,070
And this is then much faster to

262
00:17:10,140 --> 00:17:14,114
actually generate the recommendations, which if there is a user waiting

263
00:17:14,162 --> 00:17:17,554
for the feed to load or an ad to serve,

264
00:17:17,602 --> 00:17:21,994
or maybe you have a bot detection use case where if

265
00:17:22,032 --> 00:17:25,850
there is a misbehaving user, you want to catch them as fast as possible

266
00:17:25,920 --> 00:17:29,414
before they do damage. The speed really matters.

267
00:17:29,462 --> 00:17:32,826
Right? So this is kind of the basic setup.

268
00:17:32,858 --> 00:17:37,066
And let's dive into each of these three aspects

269
00:17:37,098 --> 00:17:40,400
of a vector powered surgeon recommendation system.

270
00:17:41,810 --> 00:17:45,620
All right, so first of all, I mentioned

271
00:17:48,150 --> 00:17:51,714
approximate nearest neighbor search. So what the hell is that?

272
00:17:51,832 --> 00:17:55,102
Right? We have seen on some of the previous

273
00:17:55,166 --> 00:17:58,834
slides this sort of two dimensional space with the pictures

274
00:17:58,882 --> 00:18:02,886
and similar pictures were closer to each other. And then kind

275
00:18:02,908 --> 00:18:06,274
of an autumn looking grass field was a bit further

276
00:18:06,322 --> 00:18:10,022
apart. Right? So the purpose

277
00:18:10,086 --> 00:18:13,738
of the nearest neighbor index is to

278
00:18:13,824 --> 00:18:17,894
help us find similar pictures or similar vectors

279
00:18:17,942 --> 00:18:21,054
in the space really fast, right?

280
00:18:21,252 --> 00:18:24,782
And we have to talk about what nearest means and what fast means,

281
00:18:24,836 --> 00:18:27,294
right? Those are the important aspects here.

282
00:18:27,412 --> 00:18:31,706
So in terms of quantifying

283
00:18:31,738 --> 00:18:35,570
the difference between vectors, normally what we use

284
00:18:35,640 --> 00:18:39,198
is actually we just look at the angle between the vectors,

285
00:18:39,294 --> 00:18:42,974
right, because that's kind of the distinguishing feature.

286
00:18:43,102 --> 00:18:46,674
This obviously depends on the model that you use to produce,

287
00:18:46,802 --> 00:18:50,680
given a piece of content, the vector representation, right? This model

288
00:18:51,290 --> 00:18:54,760
has certain properties that, for example, say that

289
00:18:56,810 --> 00:19:00,266
the distance should be scale independent, right? So if there are two

290
00:19:00,288 --> 00:19:04,154
vectors pointing the same way, they just have different lengths, they should

291
00:19:04,192 --> 00:19:07,738
be basically considered the same. And then this

292
00:19:07,824 --> 00:19:11,662
translates into us using the angle between vectors as

293
00:19:11,716 --> 00:19:16,110
the distance measure, right? So let's say a cosine similarity.

294
00:19:17,650 --> 00:19:21,022
So that's what we mean by nearest, right? And then

295
00:19:21,076 --> 00:19:24,606
fast, basically, just imagine that kind

296
00:19:24,628 --> 00:19:28,034
of the current, there is a bunch of vector databases out there.

297
00:19:28,072 --> 00:19:30,898
There are benchmarks, you can review those.

298
00:19:31,064 --> 00:19:34,686
But rule of thumb is that you can do thousands

299
00:19:34,718 --> 00:19:38,466
of queries per second per machine for tens of thousands

300
00:19:38,498 --> 00:19:42,518
of vectors in the index, right? So this is now technology

301
00:19:42,604 --> 00:19:46,258
that's maturing that can store a bunch

302
00:19:46,274 --> 00:19:49,018
of vectors on each node, and then you can kind of share this and you

303
00:19:49,024 --> 00:19:52,090
can get to millions and even billion vectors, right?

304
00:19:52,160 --> 00:19:56,394
So that's definitely possible. And there is a bunch of progress happening in

305
00:19:56,432 --> 00:20:01,622
this kind of approximate nearest neighbor index layer.

306
00:20:01,766 --> 00:20:05,486
And what we, the production builders, are interested

307
00:20:05,588 --> 00:20:09,342
in that want to build on top of the vector databases is

308
00:20:09,396 --> 00:20:13,146
the question of, okay, how do I take my problem search and recommendations

309
00:20:13,258 --> 00:20:16,626
and how do I express it as a vector search,

310
00:20:16,728 --> 00:20:19,938
right? Because then the vector search, there is a bunch of databases out

311
00:20:19,944 --> 00:20:23,806
there that can help me with that. So let's

312
00:20:23,838 --> 00:20:27,126
talk about that. But before we get there, I kind of want

313
00:20:27,148 --> 00:20:30,520
to reflect on something that's happening in the space, which is

314
00:20:31,130 --> 00:20:34,630
these benchmarks, right? So typically

315
00:20:35,210 --> 00:20:38,666
when you identify a metric that is easy to

316
00:20:38,688 --> 00:20:42,486
measure, like a query per second benchmark,

317
00:20:42,678 --> 00:20:46,346
everything will kind of coalesce around the metric, right? So right

318
00:20:46,368 --> 00:20:50,010
now there is this explosion of different vector databases,

319
00:20:50,590 --> 00:20:53,950
and somehow there is a lot of emphasis on the speed.

320
00:20:54,610 --> 00:20:58,686
When you choose a vector database to work with, I wouldn't focus

321
00:20:58,788 --> 00:21:02,222
so much on the speed, right. You just need good

322
00:21:02,276 --> 00:21:05,874
enough, basically, just to give you an

323
00:21:05,912 --> 00:21:09,714
idea, this chart basically shows

324
00:21:09,832 --> 00:21:13,314
the recall on the x axis. What do we mean by

325
00:21:13,352 --> 00:21:16,938
recall? This is approximate

326
00:21:16,974 --> 00:21:21,554
nearest neighbor search, right? So if you look at a data set of 10,000 vectors,

327
00:21:21,682 --> 00:21:25,654
then you look at the actual 100 nearest neighbors for

328
00:21:25,692 --> 00:21:29,258
each vector. The recall tells you that

329
00:21:29,424 --> 00:21:33,270
for this index, it was able to retrieve

330
00:21:33,350 --> 00:21:37,770
within the first hundred closest neighbors certain

331
00:21:37,840 --> 00:21:41,126
percentage of the actual closest nearest

332
00:21:41,158 --> 00:21:44,814
neighbors, right. Because it's doing approximation. So it's going to miss some,

333
00:21:44,932 --> 00:21:48,890
right. What you care about in surgeon recommendations use cases

334
00:21:48,970 --> 00:21:52,558
is recall around 80, 90%. It depends really on

335
00:21:52,564 --> 00:21:56,146
your use case. So you kind of look at this area here,

336
00:21:56,248 --> 00:22:00,034
and then y axis is queries per second, and here

337
00:22:00,072 --> 00:22:04,018
we have 1000 queries per second. And here we

338
00:22:04,024 --> 00:22:07,458
have 10,000 queries per second, right? So in this region,

339
00:22:07,634 --> 00:22:10,758
and these are 800 dimension big

340
00:22:10,844 --> 00:22:14,486
vectors, right? Which is kind of a

341
00:22:14,508 --> 00:22:18,220
good rule of thumb. You want your vectors to be up to, let's say 2000

342
00:22:18,590 --> 00:22:22,874
dimensions for good healthy system.

343
00:22:23,072 --> 00:22:26,362
All right, perfect. Okay, we talked

344
00:22:26,416 --> 00:22:30,654
about how a search and recommendations use

345
00:22:30,692 --> 00:22:34,618
case can make progress by representing

346
00:22:34,714 --> 00:22:37,902
your content and users as vectors and then doing

347
00:22:37,956 --> 00:22:42,426
this vector search that basically for a given user finds

348
00:22:42,618 --> 00:22:46,418
the nearest content. Now the devil is

349
00:22:46,424 --> 00:22:49,774
in the details, obviously, and it's in how you construct

350
00:22:49,822 --> 00:22:52,610
these vectors, right? So we are doing vector search,

351
00:22:52,760 --> 00:22:56,886
nearest neighbor search, but how are we constructing the vectors that we are

352
00:22:56,908 --> 00:22:58,520
doing it with, right?

353
00:22:59,850 --> 00:23:02,280
And so here is how basically, right.

354
00:23:03,450 --> 00:23:07,058
What you want to do is youll want to capture the aspects

355
00:23:07,074 --> 00:23:10,840
of the content and also of the user that

356
00:23:11,450 --> 00:23:15,174
are relevant for this trade off of what the user

357
00:23:15,222 --> 00:23:17,946
then sees in your app, for example, right?

358
00:23:18,048 --> 00:23:21,334
So let's talk about a use case with social

359
00:23:21,392 --> 00:23:24,320
network. In this use case,

360
00:23:25,330 --> 00:23:28,810
there is a feed of content and you care about certain properties.

361
00:23:28,890 --> 00:23:32,206
As a user, you have certain expectations of what you'll see

362
00:23:32,228 --> 00:23:36,814
in your feed, right? You expect it to be sort of roughly chronological.

363
00:23:36,942 --> 00:23:40,690
Let's just imagine LinkedIn, right? I already gave the example

364
00:23:40,760 --> 00:23:44,574
with the jobs recommendation. So let's talk about the LinkedIn

365
00:23:44,622 --> 00:23:48,630
feed, right? You kind of expect it to be roughly

366
00:23:49,050 --> 00:23:52,434
recency or kind of age of the content ordered,

367
00:23:52,482 --> 00:23:56,546
right. You expect that it's going to contain

368
00:23:56,738 --> 00:23:59,882
content that is topically relevant for you, right?

369
00:23:59,936 --> 00:24:03,830
So it's some sort of combination of the content that you liked

370
00:24:03,910 --> 00:24:07,740
or engaged with before. You also expect that

371
00:24:09,310 --> 00:24:13,022
the platform will recommend you content that

372
00:24:13,076 --> 00:24:16,526
is interesting for users that behave similar to you. This is the

373
00:24:16,548 --> 00:24:20,014
collaborative aspect of it, right? And there is maybe

374
00:24:20,052 --> 00:24:21,520
some measure of quality,

375
00:24:22,930 --> 00:24:26,194
maybe certain users are more sensitive or less

376
00:24:26,232 --> 00:24:30,094
sensitive to quality of the content. And so youll want to capture

377
00:24:30,142 --> 00:24:33,282
this as well. Now, this is nothing new.

378
00:24:33,336 --> 00:24:37,074
Like in past, you would have these features extracted, youll would assemble

379
00:24:37,122 --> 00:24:40,946
them, you would do some filtering on them to generate the candidates,

380
00:24:41,058 --> 00:24:44,520
then ship it to the ranker. There you go. Right?

381
00:24:44,970 --> 00:24:48,606
All build custom in house recommender engine staff,

382
00:24:48,658 --> 00:24:52,746
right? This is normal. The new thing with vectors is that

383
00:24:52,768 --> 00:24:55,946
you can actually take all these features and

384
00:24:55,968 --> 00:24:59,370
literally just concatenate them into the content vector,

385
00:25:00,030 --> 00:25:03,598
right? So you kind of normalize each of these vector parts and

386
00:25:03,604 --> 00:25:05,920
then you literally just concatenate them together.

387
00:25:07,170 --> 00:25:10,750
And this allows you to do cool stuff later.

388
00:25:10,820 --> 00:25:15,102
Right. It allows you to do a search that actually balances

389
00:25:15,166 --> 00:25:18,546
between these different objectives and it completely

390
00:25:18,648 --> 00:25:22,466
offloads that navigation of the trade off to

391
00:25:22,488 --> 00:25:25,986
the nearest neighbor search, right. Because in the end of the day, you just

392
00:25:26,008 --> 00:25:29,906
have one vector and you just do nearest neighbor search on it. But it's assembled

393
00:25:29,938 --> 00:25:33,446
from these different parts that are important

394
00:25:33,548 --> 00:25:37,442
for your specific use case. So that's the content

395
00:25:37,516 --> 00:25:41,126
vector construction. And then similarly,

396
00:25:41,318 --> 00:25:43,980
in the same vector space,

397
00:25:45,230 --> 00:25:49,206
you do that for the user as well. Right. So the user

398
00:25:49,318 --> 00:25:53,034
will have some topical preference. Right. So some representation

399
00:25:53,082 --> 00:25:56,106
of the content they liked before in terms of topics,

400
00:25:56,298 --> 00:25:59,466
they'll have some popularity preference, right? Is this user

401
00:25:59,578 --> 00:26:03,534
mostly interested in sort of the highest popularity

402
00:26:03,582 --> 00:26:07,602
content or are they kind of venturing into not

403
00:26:07,656 --> 00:26:11,730
just the most sort of hyped content, but also other parts,

404
00:26:12,550 --> 00:26:15,300
how well they tolerate quality,

405
00:26:16,330 --> 00:26:19,794
let's say degradation, right? And this might come from moderation

406
00:26:19,842 --> 00:26:23,506
signals and so on. And then also the recency preference,

407
00:26:23,538 --> 00:26:26,998
right? Are they after sending only the

408
00:26:27,004 --> 00:26:30,586
most recent kind of news type of stuff, or are they happy to

409
00:26:30,608 --> 00:26:34,074
venture broader into the catalog? And they are

410
00:26:34,112 --> 00:26:37,766
kind of more driven by the topic, lower popularity measures

411
00:26:37,878 --> 00:26:41,246
than recency, right? So basically you

412
00:26:41,268 --> 00:26:44,890
can again represent all these different preferences

413
00:26:44,970 --> 00:26:48,542
of the user into one vector that

414
00:26:48,596 --> 00:26:52,078
also has those parts like we did for the content,

415
00:26:52,164 --> 00:26:55,346
right? And that means that they are aligned. And now you can

416
00:26:55,368 --> 00:26:59,042
basically just take the user vector and search with it into the

417
00:26:59,096 --> 00:27:02,846
content space, right. And then that's

418
00:27:02,878 --> 00:27:06,626
your retrieval. Basically what you can also do

419
00:27:06,648 --> 00:27:10,306
with this, I have this node here in the bottom right corner is user

420
00:27:10,338 --> 00:27:14,630
to user search, right. You can discover users that behave similarly.

421
00:27:14,970 --> 00:27:18,902
This is useful for lookalike audiences

422
00:27:18,966 --> 00:27:22,806
for bot detection, for recommending

423
00:27:22,838 --> 00:27:25,866
people to each other to engage. Right? I mean,

424
00:27:25,888 --> 00:27:29,914
dating apps, obvious use case. So you have a bunch of

425
00:27:29,952 --> 00:27:33,870
opportunities on that front. But the core idea is that I'm

426
00:27:34,450 --> 00:27:38,126
extracting different features of both the user and the content and

427
00:27:38,148 --> 00:27:42,062
I'm staffing them all together into one vector, I'm putting different

428
00:27:42,116 --> 00:27:45,906
weights on them, I'm normalizing them so they can be combined in

429
00:27:45,928 --> 00:27:49,746
this way. And that gives me the basis for my

430
00:27:49,848 --> 00:27:52,930
content and user vector representations.

431
00:27:56,150 --> 00:28:00,018
Here is just a representation of how then we use these vectors.

432
00:28:00,114 --> 00:28:03,958
So if then I have a user coming in and I

433
00:28:03,964 --> 00:28:07,074
want to generate a feed of content for them, I literally

434
00:28:07,122 --> 00:28:10,698
just take the user vector and I search in

435
00:28:10,704 --> 00:28:14,220
the content space and then this is the content that will come up.

436
00:28:14,750 --> 00:28:18,074
And the key thing to understand that's kind of

437
00:28:18,112 --> 00:28:22,186
different from just these very basic overviews of vector

438
00:28:22,298 --> 00:28:24,640
powered retrieval that you see online.

439
00:28:25,250 --> 00:28:29,166
Typically people just use one model,

440
00:28:29,268 --> 00:28:32,914
right? They would just have the relevance part, for example,

441
00:28:33,032 --> 00:28:36,334
right? So you use some semantic embedding

442
00:28:36,382 --> 00:28:39,826
model and you just do the

443
00:28:39,848 --> 00:28:43,394
semantic embedding and then if you visualize it,

444
00:28:43,432 --> 00:28:46,610
you'll have kind of topically

445
00:28:46,690 --> 00:28:49,878
similar content clustered together, right. This is

446
00:28:49,964 --> 00:28:51,960
what you see out there.

447
00:28:53,050 --> 00:28:56,726
But what people are kind of realizing now is that you

448
00:28:56,748 --> 00:28:59,946
can blend all those other features in there, right? So this

449
00:28:59,968 --> 00:29:03,450
is not just kind of topically relevant content

450
00:29:03,520 --> 00:29:06,762
for user two, but it's also very

451
00:29:06,816 --> 00:29:10,966
recent content. And then the user two likes that. And so that's

452
00:29:10,998 --> 00:29:14,542
why they are closer together, right? So there are all these other things

453
00:29:14,596 --> 00:29:16,800
that can be expressed in the space.

454
00:29:17,650 --> 00:29:20,862
Obviously here is kind of a projection of that space into two d,

455
00:29:20,916 --> 00:29:23,778
so it's hard to visualize that,

456
00:29:23,944 --> 00:29:27,650
but that's kind of what it's doing in its original 1000

457
00:29:27,800 --> 00:29:31,186
dimensional space, is kind of navigating all

458
00:29:31,208 --> 00:29:34,914
those different trade offs. And then you can let

459
00:29:34,952 --> 00:29:38,610
the approximate nearest neighbor index actually do the heavy lifting,

460
00:29:38,690 --> 00:29:42,386
right. All right, cool. So that's the retrieval

461
00:29:42,418 --> 00:29:46,150
step, right. That covers the okay for a given user.

462
00:29:46,570 --> 00:29:50,294
Here is a bunch of content that sort of really matches

463
00:29:50,342 --> 00:29:54,010
their preference. Now there are still

464
00:29:54,080 --> 00:29:57,898
some aspects of this recommendation and search problem

465
00:29:58,064 --> 00:30:01,930
which can't quite be represented

466
00:30:02,010 --> 00:30:06,394
or as easily represented with the vector

467
00:30:06,442 --> 00:30:09,534
embedding. And for this, you might want a

468
00:30:09,572 --> 00:30:13,134
module on top of your retrieval that is managing the queries.

469
00:30:13,182 --> 00:30:15,650
Right. And it's sort of managing,

470
00:30:16,870 --> 00:30:19,826
it's potentially manipulating the search vector, right.

471
00:30:19,848 --> 00:30:22,766
So user is coming in, I grab their user vector,

472
00:30:22,878 --> 00:30:26,390
I tweak it and then use it to search into the content

473
00:30:26,460 --> 00:30:28,918
space. This tweak could, for example,

474
00:30:29,084 --> 00:30:33,142
turn a recommender engine into a search engine because

475
00:30:33,196 --> 00:30:37,090
I will basically just get a search query for the user.

476
00:30:37,250 --> 00:30:40,534
I will create a semantic embedding of the query

477
00:30:40,662 --> 00:30:44,170
and I will use it to overwrite or augment

478
00:30:44,910 --> 00:30:48,358
the topical preference of the user. Right. So I'm

479
00:30:48,374 --> 00:30:51,790
doing personalized search out of the box because all the other

480
00:30:51,940 --> 00:30:55,514
preference aspect of the user are still in the user vector.

481
00:30:55,642 --> 00:30:59,278
And I'm now also putting in the current context of,

482
00:30:59,364 --> 00:31:03,006
all right, this user is searching for splitting

483
00:31:03,038 --> 00:31:06,340
headache or whatever example we saw before.

484
00:31:08,310 --> 00:31:11,938
So that's kind of search vector manipulation. That's how we can build the

485
00:31:11,944 --> 00:31:15,878
search on top of the same idea. And then

486
00:31:16,044 --> 00:31:19,874
other aspect of this is, let's say you want to improve

487
00:31:19,922 --> 00:31:23,960
the diversity of authors for the content that you recommend.

488
00:31:25,130 --> 00:31:28,934
This is sort of difficult to express

489
00:31:29,062 --> 00:31:32,474
as a set of linear constraints. And so you might want

490
00:31:32,512 --> 00:31:36,362
to have this query manager on top issue multiple queries, for example,

491
00:31:36,416 --> 00:31:39,674
into different clusters of the

492
00:31:39,712 --> 00:31:43,838
author users for the content, and then assemble the result together.

493
00:31:44,004 --> 00:31:47,440
Right. So this would be sort of kind of like

494
00:31:48,290 --> 00:31:52,662
creating multiple searches from that one initial search to satisfy

495
00:31:52,826 --> 00:31:56,702
some additional constraint around recommendation

496
00:31:56,766 --> 00:31:59,666
variation, diversity, that sort of thing.

497
00:31:59,768 --> 00:32:02,962
And then finally it's an approximate system,

498
00:32:03,016 --> 00:32:06,654
it's approximate nearest neighbor retrieval. So you can't

499
00:32:06,782 --> 00:32:10,850
right away give a guarantee that there won't be something weird

500
00:32:10,930 --> 00:32:14,694
in the result set. To actually have guarantees, you need to

501
00:32:14,732 --> 00:32:17,810
post filter the results. Right. You should do this minimally.

502
00:32:17,890 --> 00:32:21,462
Right. This part shouldn't do the heavy lifting,

503
00:32:21,526 --> 00:32:24,806
but you might still want to combine some of the results,

504
00:32:24,918 --> 00:32:28,202
filter a small percentage of them out, that kind of slip through

505
00:32:28,256 --> 00:32:31,870
the nearest neighbor approximation. And for that,

506
00:32:31,940 --> 00:32:35,658
again, the query manager on top of your vector based retrieval

507
00:32:35,834 --> 00:32:39,038
is useful. Okay,

508
00:32:39,124 --> 00:32:42,254
so now we talked about why vectors. We talked

509
00:32:42,292 --> 00:32:45,426
about the types of things you can build.

510
00:32:45,528 --> 00:32:48,210
We focused on the surgeon recommendations.

511
00:32:49,270 --> 00:32:52,622
We will actually touch on the generative

512
00:32:52,686 --> 00:32:56,340
AI in a minute, but let's talk about

513
00:32:57,030 --> 00:33:00,422
how you will actually get this done. Right. This is the interesting part.

514
00:33:00,476 --> 00:33:04,440
So we are motivated. We want to build a vector powered system.

515
00:33:05,050 --> 00:33:09,078
I split it into three parts, this section. So first,

516
00:33:09,164 --> 00:33:12,698
just what do you need to get started? Right, this is

517
00:33:12,704 --> 00:33:15,606
some basic demo. You are just playing with vector embeddings.

518
00:33:15,718 --> 00:33:19,180
You want to experience how they work. Right? For this,

519
00:33:20,690 --> 00:33:24,606
youll need these four items, basically, very simple. Youll need your data,

520
00:33:24,708 --> 00:33:28,110
right? Ideally unstructured text or images.

521
00:33:30,370 --> 00:33:34,226
Basically you need to load this data from wherever you

522
00:33:34,248 --> 00:33:38,066
have it right now, maybe on your computer or in a database you need to

523
00:33:38,088 --> 00:33:41,298
load it into. Ideally the simplest is a

524
00:33:41,304 --> 00:33:45,026
Python notebook collaboratory or

525
00:33:45,048 --> 00:33:48,514
one of the online providers of Python notebooks will totally

526
00:33:48,562 --> 00:33:52,342
work. Once you load the data, you need to

527
00:33:52,396 --> 00:33:55,798
turn it into vectors. Obviously for this youll need a

528
00:33:55,804 --> 00:33:58,994
vector embedding model, right? There is a bunch of open source

529
00:33:59,042 --> 00:34:02,634
models out there. You can, for example, go to hugging phase and find

530
00:34:02,832 --> 00:34:06,266
something that's popular. And from that you kind of

531
00:34:06,288 --> 00:34:09,466
know that maybe it's an interesting place to

532
00:34:09,488 --> 00:34:12,606
start. There are

533
00:34:12,628 --> 00:34:16,074
also APIs, right? So for example, the famous OpenAI

534
00:34:16,122 --> 00:34:19,390
API that can for a given piece of content,

535
00:34:19,460 --> 00:34:23,410
generate the vector embedding. Of course there you'll have to pay.

536
00:34:23,480 --> 00:34:26,994
But you can embed thousands and thousands of

537
00:34:27,112 --> 00:34:30,866
pieces of content for cents or tens of

538
00:34:30,888 --> 00:34:34,578
cents of dollars. The cost is

539
00:34:34,664 --> 00:34:38,438
minimum until you start to

540
00:34:38,604 --> 00:34:41,942
work with millions or tens of millions of pieces of data.

541
00:34:42,076 --> 00:34:45,814
And then finally, okay, so you have your content, you have the

542
00:34:45,852 --> 00:34:49,706
vectors that represent the content and you want

543
00:34:49,728 --> 00:34:53,546
to find similar vectors to just sort of see. Okay, for this

544
00:34:53,568 --> 00:34:58,166
image, which ones are similar? You don't need any extra infrastructure

545
00:34:58,198 --> 00:35:02,094
for this. There's a bunch of libraries in Python like

546
00:35:02,132 --> 00:35:05,754
Sklearn that has built in cosine similarity.

547
00:35:05,882 --> 00:35:09,246
So you just literally have vectors as numpy arrays and you

548
00:35:09,268 --> 00:35:12,958
do a similarity calculation with

549
00:35:13,044 --> 00:35:16,174
kind of all your content. Right. So there is no indexing

550
00:35:16,222 --> 00:35:19,922
going on. This is brute force. This totally works for even

551
00:35:19,976 --> 00:35:22,610
thousands of data vectors.

552
00:35:23,670 --> 00:35:26,566
And this is a great way to get started and get a feel for how

553
00:35:26,588 --> 00:35:30,086
this works. You'll have access to the slides and I

554
00:35:30,108 --> 00:35:34,002
have these examples linked. So there is a collaboratory

555
00:35:34,146 --> 00:35:37,986
jupyter notebook that basically contains

556
00:35:38,018 --> 00:35:41,642
an example like that. Okay, so this is the getting started, the first

557
00:35:41,696 --> 00:35:45,706
steps right now, the second part. What will you need to add to

558
00:35:45,728 --> 00:35:49,706
this to build an MVP? And I would kind of

559
00:35:49,728 --> 00:35:52,906
advocate for two parts. Sometimes people just kind of

560
00:35:52,928 --> 00:35:56,474
go with the vector database and call it a day. So a vector

561
00:35:56,522 --> 00:36:00,394
database is a place to, once you create your vectors,

562
00:36:00,442 --> 00:36:04,730
you store them there, you index them there, and then in

563
00:36:04,740 --> 00:36:08,354
youll product you can do a query into the vector database instead

564
00:36:08,472 --> 00:36:12,446
of the cosine similarity directly in the notebook.

565
00:36:12,558 --> 00:36:15,890
Right? So that's kind of the basic setup for the MVP.

566
00:36:16,490 --> 00:36:20,306
But what I would want to also add to that basic package

567
00:36:20,418 --> 00:36:23,090
is some approach to evaluation,

568
00:36:23,250 --> 00:36:26,546
right. We are working with vectors. We are experimenting

569
00:36:26,578 --> 00:36:30,018
with vectors because we want to improve the end user experience, right.

570
00:36:30,044 --> 00:36:33,626
This is the whole point. And so you need some way to keep an

571
00:36:33,648 --> 00:36:37,190
eye on the quality and on what also the users

572
00:36:37,270 --> 00:36:40,574
think about this, right. So obviously the first step

573
00:36:40,612 --> 00:36:44,046
will be just eyeballing the results. Sanity checking, do they

574
00:36:44,068 --> 00:36:47,086
make sense? Let's look at 20 random inputs. Let's look

575
00:36:47,108 --> 00:36:49,950
at the top ten results.

576
00:36:51,090 --> 00:36:54,202
This sanity checking is priceless,

577
00:36:54,266 --> 00:36:57,282
right? You definitely need to start there. Youll find a bunch of issues.

578
00:36:57,336 --> 00:37:01,220
You'll need to tweak your models, choose different embedding models and so on.

579
00:37:02,710 --> 00:37:06,114
Then the second step is kind of quantitative evaluation,

580
00:37:06,242 --> 00:37:09,990
right? If you have some kind of data

581
00:37:10,060 --> 00:37:13,414
labels or some annotations from before, let's say for

582
00:37:13,452 --> 00:37:17,334
searches, which results people are actually clicking on, or for

583
00:37:17,372 --> 00:37:21,434
recommendations, a similar kind of data set, you can back

584
00:37:21,472 --> 00:37:25,110
test vector powered implementation

585
00:37:25,190 --> 00:37:28,874
of your search or of your recommendations and then kind of see, okay,

586
00:37:28,992 --> 00:37:33,370
are the things that people tend to click on appearing

587
00:37:33,450 --> 00:37:36,842
high up often enough in the vector

588
00:37:36,906 --> 00:37:40,880
powered results? Right. Is there a chance that this will actually work?

589
00:37:41,490 --> 00:37:45,298
And then once you get into the product, once you have the MVP out there,

590
00:37:45,464 --> 00:37:48,850
it's useful to collect user feedback, right. Are people

591
00:37:48,920 --> 00:37:52,274
interested in the results? Do they think they make sense?

592
00:37:52,392 --> 00:37:56,270
And then finally, and probably most importantly, the analytics,

593
00:37:56,430 --> 00:37:59,238
are people actually interacting more with the content?

594
00:37:59,324 --> 00:38:02,374
Are you achieving your goal? Are you getting more people to

595
00:38:02,412 --> 00:38:06,102
apply for a job or whatever the recommendation, use case

596
00:38:06,156 --> 00:38:09,286
or search use case that you have, right? The analytics is kind of the be

597
00:38:09,308 --> 00:38:12,634
all, end all. That's where a b testing comes in and all of that other

598
00:38:12,672 --> 00:38:16,486
stuff, but maybe not for the MVP. Some basic analytics

599
00:38:16,518 --> 00:38:19,866
setup, however, is something that I would definitely recommend because

600
00:38:19,888 --> 00:38:23,998
that's how you learn if your mvp should

601
00:38:24,084 --> 00:38:28,110
transition into the next stage. And the next stage is where this all

602
00:38:28,180 --> 00:38:33,314
gets very complicated, right? Because you

603
00:38:33,352 --> 00:38:37,758
are not just vectorizing pieces of content and letting

604
00:38:37,774 --> 00:38:41,330
those vectors be, you are now assembling all those different

605
00:38:41,400 --> 00:38:45,342
constraints. So previously we might have just worked with

606
00:38:45,496 --> 00:38:48,630
one embedding model, right? The semantic embeddings,

607
00:38:49,530 --> 00:38:53,718
vectorizing the content, vectorizing the queries, matching the two simple,

608
00:38:53,804 --> 00:38:57,302
right? But if you guys want to do

609
00:38:57,356 --> 00:39:01,302
the stuff that I described before, where you assemble

610
00:39:01,366 --> 00:39:04,394
content signals and collaborative signals and all these other

611
00:39:04,432 --> 00:39:08,620
stuff into one vector and then have a state of the art system,

612
00:39:09,710 --> 00:39:13,422
you need a few more components, right? So I'll just quickly

613
00:39:13,476 --> 00:39:14,560
run through this.

614
00:39:17,090 --> 00:39:20,810
You'll need a sort of a serving system that has APIs,

615
00:39:20,970 --> 00:39:24,462
and on one side you'll push the data

616
00:39:24,516 --> 00:39:28,142
into the API, right. You'll push youll content and users metadata

617
00:39:28,206 --> 00:39:31,362
and you'll push in your events. So this is what the users are actually

618
00:39:31,416 --> 00:39:34,866
doing in the app, right. So we are actually using the user history as

619
00:39:34,888 --> 00:39:38,470
well, not just the semantic embedding of the content and the query.

620
00:39:38,890 --> 00:39:43,366
And we are also using the user metadata. Right. So that's why the

621
00:39:43,388 --> 00:39:47,126
user data is also on the input. So this should all be coming into

622
00:39:47,228 --> 00:39:50,794
the kind of vector powered system that you have

623
00:39:50,832 --> 00:39:54,586
built. And then the

624
00:39:54,608 --> 00:39:57,818
next step from there is the transformation. So all of this data

625
00:39:57,904 --> 00:40:01,454
will have properties, will have different parts of it that

626
00:40:01,572 --> 00:40:04,522
ultimately should make it into the vectorization.

627
00:40:04,666 --> 00:40:08,094
Right. Different aspects. Let's say you have a job, you want to

628
00:40:08,212 --> 00:40:11,374
classify seniority required for that position.

629
00:40:11,492 --> 00:40:15,346
So the job title is probably pretty important for that. And the transform is

630
00:40:15,368 --> 00:40:19,506
just about pulling that seniority key from

631
00:40:19,528 --> 00:40:22,594
the content object and making sure that it comes into

632
00:40:22,632 --> 00:40:25,966
the classifier vectorizer that resolves.

633
00:40:25,998 --> 00:40:30,374
Okay, what is the seniority required for this job? Right. So that's the transform step.

634
00:40:30,572 --> 00:40:34,310
Then finally you have the vectorization, which is where youll be loading

635
00:40:34,650 --> 00:40:38,274
the embedding models from hugging phase. But also you'll

636
00:40:38,322 --> 00:40:41,500
probably have some of your own models because your data,

637
00:40:42,270 --> 00:40:46,666
there is always that aspect that's kind of special for your product and

638
00:40:46,768 --> 00:40:49,994
your kind of embeddings model needs to be able to

639
00:40:50,112 --> 00:40:54,734
support that, right? Sometimes this is just

640
00:40:54,772 --> 00:40:58,106
loading a pretrained large language model, but sometimes it's

641
00:40:58,138 --> 00:41:01,246
just vectorizing recency like we have

642
00:41:01,268 --> 00:41:04,554
shown in one of the previous slides, right, because your users care about fresh

643
00:41:04,602 --> 00:41:08,146
content, so you want to add that as one of the features. And now you

644
00:41:08,168 --> 00:41:11,906
have a problem. How do I do that? How do I express recency in

645
00:41:11,928 --> 00:41:15,300
a way that I will not have to recompute the age of the content?

646
00:41:15,670 --> 00:41:19,110
I'll not be putting the age of the content

647
00:41:19,180 --> 00:41:22,774
into the content vector and then having to run a pipeline every

648
00:41:22,812 --> 00:41:26,166
night to update that, right. That's not good. So you need to be

649
00:41:26,188 --> 00:41:30,218
really mindful about how each of these data

650
00:41:30,304 --> 00:41:33,706
properties actually makes it into that vector, right? So that

651
00:41:33,728 --> 00:41:37,226
it is available during the nearest neighbor lookup to guide the

652
00:41:37,248 --> 00:41:40,060
recommendation. So that's the vectorization step.

653
00:41:40,430 --> 00:41:43,690
And then finally, okay, we have the database.

654
00:41:43,770 --> 00:41:47,630
It has the user and content vectors. So that's the thing that

655
00:41:47,700 --> 00:41:51,258
obviously you are building this on top of the vector database.

656
00:41:51,354 --> 00:41:55,054
You have your user and content vectors in there. They're up to date. Ideally,

657
00:41:55,182 --> 00:41:58,478
if you want TikTok level recommendation performance,

658
00:41:58,574 --> 00:42:02,098
then these vectors are updated in the real time so that when the

659
00:42:02,104 --> 00:42:05,934
user clicks on something or does something, you immediately use that

660
00:42:06,072 --> 00:42:09,506
signal on the input the event into recomputing

661
00:42:09,538 --> 00:42:12,534
your vectors. And then finally you do the retrieval, right.

662
00:42:12,572 --> 00:42:15,910
You do the nearest neighbor search to power your recommendations.

663
00:42:16,890 --> 00:42:20,186
And then through an API layer again, you surface it

664
00:42:20,208 --> 00:42:24,022
into your product search recommendations. We talked about bot detection

665
00:42:24,086 --> 00:42:26,170
through the user to user similarity,

666
00:42:27,150 --> 00:42:30,918
user segmentation, and other use cases, right? Sometimes when

667
00:42:30,944 --> 00:42:34,366
the user is, for example, searching, you might have to kind

668
00:42:34,388 --> 00:42:37,818
of feed the query back through the vectorization

669
00:42:37,914 --> 00:42:39,710
before youll can do the retrieval.

670
00:42:40,450 --> 00:42:43,986
But this is kind of the anatomy of a

671
00:42:44,008 --> 00:42:47,906
system that you would need to

672
00:42:47,928 --> 00:42:50,290
really pull this off in production at scale,

673
00:42:52,630 --> 00:42:56,040
you can expect something looking like that.

674
00:42:58,090 --> 00:43:02,338
All right, and finally, let's look at the generative.

675
00:43:02,514 --> 00:43:05,154
What does all this have to do with generative AI?

676
00:43:05,202 --> 00:43:08,460
Right, this is the current hype. That's actually

677
00:43:09,070 --> 00:43:12,662
for a large part driving the vector hype.

678
00:43:12,726 --> 00:43:15,994
So let's kind of connect the dots here, right?

679
00:43:16,112 --> 00:43:19,514
So what is generative AI? And I'll just talk about

680
00:43:19,552 --> 00:43:23,434
the text, but this obviously applies to other modalities

681
00:43:23,562 --> 00:43:27,214
like images. But if you think about these large

682
00:43:27,252 --> 00:43:30,910
language models, basically they are function that

683
00:43:30,980 --> 00:43:35,166
on one side takes text, a prompt, and then outputs

684
00:43:35,358 --> 00:43:38,690
some sort of response to that prompt.

685
00:43:41,510 --> 00:43:44,946
Now, the next kind of

686
00:43:44,968 --> 00:43:48,774
step that happened after people kind of played with this very

687
00:43:48,812 --> 00:43:51,730
basic setup, right? I prompt you, you give me a response.

688
00:43:51,810 --> 00:43:55,526
Cool. The next thing you can do is kind of take that response and

689
00:43:55,548 --> 00:43:58,306
feed it back into the model,

690
00:43:58,428 --> 00:44:02,202
right? And that's where, for example, Chat GPT really

691
00:44:02,256 --> 00:44:05,546
took off, right. Because you had that sustained conversation back and

692
00:44:05,568 --> 00:44:08,854
forth, the model saw its own previous responses

693
00:44:08,902 --> 00:44:12,654
to you at every step of the way and therefore was

694
00:44:12,692 --> 00:44:16,286
able to build a response that feels like you are

695
00:44:16,308 --> 00:44:19,754
actually talking to the model. But it's critical

696
00:44:19,802 --> 00:44:23,466
to understand that the model itself doesn't have any memory,

697
00:44:23,578 --> 00:44:26,866
right. It only responds to the

698
00:44:26,888 --> 00:44:30,786
text that it receives on the input. And so in the chat use case,

699
00:44:30,808 --> 00:44:34,206
you are kind of constantly feeding in the whole chat

700
00:44:34,238 --> 00:44:37,574
history or a part of it, right. If the chat is too long,

701
00:44:37,612 --> 00:44:41,430
then it's just the recent part. This is why, for example, in Chat GPT,

702
00:44:42,970 --> 00:44:46,706
if the conversation is long enough, it forgets

703
00:44:46,818 --> 00:44:49,642
the stuff that was said a while ago, right?

704
00:44:49,696 --> 00:44:53,914
Because the context window can only take so

705
00:44:53,952 --> 00:44:57,514
much of the history and it has no other memory, right? So it

706
00:44:57,552 --> 00:44:59,740
has no other place to store,

707
00:45:01,150 --> 00:45:04,318
let's say, things that it learned about youll, right, so this

708
00:45:04,324 --> 00:45:06,480
is the Chat GPT situation.

709
00:45:07,250 --> 00:45:11,120
There are libraries that are useful for working with,

710
00:45:13,010 --> 00:45:16,590
building on top of generative AI use cases.

711
00:45:16,750 --> 00:45:20,654
I youll recommend checking out lank chain. There is a 13 minutes

712
00:45:20,702 --> 00:45:24,290
explainer video that I found really useful. So again,

713
00:45:24,360 --> 00:45:28,162
check out the link that I added to most slides.

714
00:45:28,306 --> 00:45:31,958
There is something useful there. All right,

715
00:45:32,044 --> 00:45:35,478
but what's up with this stuff? So here

716
00:45:35,644 --> 00:45:39,526
I show, I demonstrate that the system kind

717
00:45:39,548 --> 00:45:43,418
of pretends that it has memory, just to illustrate how, when I

718
00:45:43,424 --> 00:45:46,794
was saying that for it to sort of feel like

719
00:45:46,832 --> 00:45:50,586
a chat, it has to constantly feed back the whole chat history and

720
00:45:50,608 --> 00:45:53,270
then generate the next response.

721
00:45:53,430 --> 00:45:57,006
However, you can sort of play with it a game where you

722
00:45:57,028 --> 00:46:00,526
ask it to make up a number and not tell you, and then you

723
00:46:00,548 --> 00:46:04,622
guess it and it gives you feedback if the secret number is higher or lower.

724
00:46:04,756 --> 00:46:08,194
And you can actually play through a game like that, right? You can

725
00:46:08,232 --> 00:46:12,130
guess. It tells you if you are too high or too low.

726
00:46:12,200 --> 00:46:15,460
And then finally, eventually you get the result.

727
00:46:15,850 --> 00:46:19,414
It's just that technically it couldn't have

728
00:46:19,452 --> 00:46:23,346
made up a number and then kept it somewhere

729
00:46:23,378 --> 00:46:26,370
in memory, because the chat is its only memory,

730
00:46:26,450 --> 00:46:30,734
right? So this is an interesting case of the model sort of pretending

731
00:46:30,802 --> 00:46:34,474
that it thought about the number and didn't tell you, but again,

732
00:46:34,512 --> 00:46:38,266
it has no place to store that number, right? So you can

733
00:46:38,288 --> 00:46:41,120
try this yourself. It's actually quite interesting.

734
00:46:42,770 --> 00:46:46,160
And so, okay, obviously I kind of set it up.

735
00:46:47,330 --> 00:46:51,274
The touch point between what we talked about before, vector representations

736
00:46:51,322 --> 00:46:54,974
of data and generative AI is this aspect

737
00:46:55,022 --> 00:46:59,250
of memory, right? Is this aspect of being able to

738
00:46:59,400 --> 00:47:03,266
use the large language model to generate a

739
00:47:03,288 --> 00:47:07,038
vector representation? Because that can also be a byproduct. It doesn't

740
00:47:07,054 --> 00:47:10,486
have to be text, it can be a vector and then

741
00:47:10,668 --> 00:47:14,226
store, let's say you are processing a document, a large document

742
00:47:14,258 --> 00:47:17,078
that doesn't fit into the input text all at once,

743
00:47:17,164 --> 00:47:20,746
right? You might want to chunk it up by chapter or

744
00:47:20,768 --> 00:47:24,346
paragraph and then generate vectors that then

745
00:47:24,368 --> 00:47:27,734
you store in some vector database, right? So for each paragraph

746
00:47:27,782 --> 00:47:30,570
you'll have a vector that represents its meaning.

747
00:47:30,990 --> 00:47:35,034
And then you can do clustering, do similarity

748
00:47:35,082 --> 00:47:38,906
search, all kinds of different things on this kind of corpus

749
00:47:38,938 --> 00:47:42,014
of memory that you build out with the large language model.

750
00:47:42,132 --> 00:47:46,066
And there has been this word floating around

751
00:47:46,248 --> 00:47:49,602
agents. Sometimes the agents

752
00:47:49,656 --> 00:47:53,858
can be autonomous and they kind of feed the output text

753
00:47:54,024 --> 00:47:57,566
inside of the large language model and also use the large language

754
00:47:57,598 --> 00:48:01,014
model output to actually decide what the next action should be. Should I retrieve something

755
00:48:01,052 --> 00:48:04,280
from memory? Should I feed another new input to the model?

756
00:48:04,730 --> 00:48:08,534
So those would be autonomous agents, but any sort of agent or

757
00:48:08,572 --> 00:48:11,978
kind of controller module that you run on top of

758
00:48:12,064 --> 00:48:15,674
the large language model sort of use case can just

759
00:48:15,712 --> 00:48:19,366
have manually controlled logic of. All right, first I'll chunk

760
00:48:19,398 --> 00:48:22,846
up a document, I'll generate those paragraph vectors, and then I'll use

761
00:48:22,868 --> 00:48:26,142
that for search. Right? And that's the touch

762
00:48:26,196 --> 00:48:29,562
point between vector embeddings and generative

763
00:48:29,626 --> 00:48:31,840
AI use cases. Right?

764
00:48:33,590 --> 00:48:37,138
Again here I recommend checking, but auto GPT if you haven't yet.

765
00:48:37,224 --> 00:48:40,820
That's the most famous example of the autonomous agent.

766
00:48:42,070 --> 00:48:45,890
All right, so today we covered why vector

767
00:48:45,970 --> 00:48:49,238
embeddings, vector representations of your content,

768
00:48:49,324 --> 00:48:52,514
but also users are useful,

769
00:48:52,642 --> 00:48:56,642
how you can use them to build surgeon recommendation

770
00:48:56,786 --> 00:49:00,586
features for your product, the different levels of

771
00:49:00,688 --> 00:49:04,140
software stack that you need to pull this off.

772
00:49:04,590 --> 00:49:08,566
And then finally, we also looked at the connection

773
00:49:08,598 --> 00:49:12,334
of vector embeddings and generative AI. Thanks a lot for

774
00:49:12,372 --> 00:49:15,966
joining me. Obviously I like

775
00:49:15,988 --> 00:49:19,166
to talk about this topic and would love

776
00:49:19,188 --> 00:49:22,922
to learn about your use cases for vector embeddings

777
00:49:22,986 --> 00:49:26,158
and vector ops and learn about the things you

778
00:49:26,164 --> 00:49:28,702
are struggling with in the space. As I mentioned,

779
00:49:28,836 --> 00:49:32,602
I'm a founder at a company called Superlinked.

780
00:49:32,746 --> 00:49:36,230
We are building a vector ops platform that makes

781
00:49:36,380 --> 00:49:39,826
building surgeon recommendations on top of vector embeddings

782
00:49:39,938 --> 00:49:43,142
easier. Right. And so we are interested in talking

783
00:49:43,196 --> 00:49:47,058
to people who are in the space. They're experimenting with surgeon

784
00:49:47,074 --> 00:49:50,294
recommendations and we would love to learn from

785
00:49:50,332 --> 00:49:54,022
each other and deliver something useful. So let's connect

786
00:49:54,076 --> 00:49:56,520
on LinkedIn and take it from there.

