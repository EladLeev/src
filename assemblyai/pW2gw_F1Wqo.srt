1
00:01:54,250 --> 00:01:57,966
Hello and welcome to our session. Today we will

2
00:01:57,988 --> 00:02:01,310
be talking about the efficiency and the resiliency of large

3
00:02:01,380 --> 00:02:03,390
scale Kubernetes environments.

4
00:02:04,370 --> 00:02:07,666
My name is Eli Birger and I'm a co founder and chief

5
00:02:07,698 --> 00:02:11,346
technology officer of Perfectscale. Prior to establishing

6
00:02:11,378 --> 00:02:14,614
perfect scale, I have managed DevOps and infrastructure team for

7
00:02:14,652 --> 00:02:18,286
many years. I have built multiple large scale SaaS

8
00:02:18,338 --> 00:02:21,420
systems, mainly based on the Kubernetes in the recent years.

9
00:02:23,230 --> 00:02:27,402
My talk today will focus on the second day operation challenges and

10
00:02:27,456 --> 00:02:30,730
specifically on the right sizing of Kubernetes environments.

11
00:02:31,070 --> 00:02:34,154
The second day operation basically starts when your environment

12
00:02:34,202 --> 00:02:37,280
goes live and you starting to serve real customers.

13
00:02:38,290 --> 00:02:42,366
The second day operation is not a single milestone, but actually this

14
00:02:42,388 --> 00:02:45,666
is the beginning of a long journey, the journey of

15
00:02:45,688 --> 00:02:49,330
the day to day development and operations across the environment.

16
00:02:50,630 --> 00:02:54,078
The entire day to day operations has a single purpose,

17
00:02:54,174 --> 00:02:58,294
to provide the customers with the best possible experience using

18
00:02:58,412 --> 00:03:02,466
the system and from the executive perspective,

19
00:03:02,578 --> 00:03:06,946
the best possible experience but also with the lowest

20
00:03:07,058 --> 00:03:10,486
possible cost. To achieve

21
00:03:10,518 --> 00:03:13,654
this, Kubernetes ecosystem provide us with two types

22
00:03:13,702 --> 00:03:17,334
of tools, the horizontal pole autoscaler. I personally

23
00:03:17,382 --> 00:03:21,134
prefer Keda here and the cluster autoscaler. Some may

24
00:03:21,172 --> 00:03:24,986
prefer carpenter. The combination of horizontal pole

25
00:03:25,018 --> 00:03:28,910
autoscaler and the cluster autoscaler allows us to dynamically

26
00:03:29,330 --> 00:03:33,134
change the entire environment. Environments scales up

27
00:03:33,172 --> 00:03:36,866
and down horizontally according to the demands and the needs.

28
00:03:37,048 --> 00:03:40,446
So it seems we just need to set up an HPA

29
00:03:40,478 --> 00:03:44,274
and the cluster autoscaler and start enjoying the best possible

30
00:03:44,392 --> 00:03:47,400
experience at the lowest possible cost.

31
00:03:49,210 --> 00:03:52,546
So now when both the horizontal port autoscaler

32
00:03:52,578 --> 00:03:56,914
and the cluster autoscaler are installed and configured, we are expecting

33
00:03:56,962 --> 00:04:00,826
our environments to have a high resilience level combined with a

34
00:04:00,848 --> 00:04:05,158
steady cost pattern following the demand fluctuations.

35
00:04:05,334 --> 00:04:09,114
But when we look at the real data we will find something like

36
00:04:09,152 --> 00:04:12,714
this, not always satisfying resilience

37
00:04:12,762 --> 00:04:16,606
level and constantly growing cost. This is

38
00:04:16,628 --> 00:04:20,298
a good sign that our system is not properly right sized.

39
00:04:20,394 --> 00:04:23,730
Despite the presence of HPA and cluster autoscaler,

40
00:04:28,070 --> 00:04:31,614
there is no magic here. Kubernetes horizontal scalability heavily

41
00:04:31,662 --> 00:04:35,122
relays on the proper vertical sizing definitions of

42
00:04:35,176 --> 00:04:38,534
podes and nodes. Let's see how

43
00:04:38,572 --> 00:04:42,486
it works in details. Here is

44
00:04:42,508 --> 00:04:46,418
a pod with the request of four cores of cpu and eight gigabyte

45
00:04:46,434 --> 00:04:50,202
of memory. Those request values are defining how much

46
00:04:50,256 --> 00:04:53,466
resources the node should allocate for the specific

47
00:04:53,568 --> 00:04:56,886
pod when pod is assigned

48
00:04:56,918 --> 00:05:00,246
to the node. Here we are looking now at the example of node

49
00:05:00,278 --> 00:05:04,602
with eight cores of cpu and 16 gigabyte of memory. The relevant

50
00:05:04,666 --> 00:05:08,670
fraction of node resources is reserved for our pod.

51
00:05:10,130 --> 00:05:13,754
Now when kubernetes need to schedule additional pod,

52
00:05:13,882 --> 00:05:17,854
it will place them on the same node only if remaining allocatable

53
00:05:17,902 --> 00:05:21,122
of the node fits the pod request. For example,

54
00:05:21,256 --> 00:05:25,038
this red pod with twelve gigabyte of memory request

55
00:05:25,134 --> 00:05:27,990
cannot be assigned to the node.

56
00:05:30,890 --> 00:05:35,270
Instead, this pod will go to the unschedulable queue

57
00:05:35,690 --> 00:05:39,766
and cluster autoscaler, constantly monitoring

58
00:05:39,798 --> 00:05:43,242
this unschedulable queue. And once there is a pod, it will

59
00:05:43,296 --> 00:05:47,130
simply add a node to our cluster.

60
00:05:52,670 --> 00:05:56,106
So both the cluster autoscaler and the HPA are tightly

61
00:05:56,138 --> 00:06:00,334
coupled to the pod requests. Let's see how as

62
00:06:00,372 --> 00:06:03,694
we saw in a previous slide, the cluster autoscaler will scale up amount

63
00:06:03,732 --> 00:06:07,146
of nodes only when pod can't be scheduled

64
00:06:07,178 --> 00:06:10,546
on existing nodes, and it will

65
00:06:10,568 --> 00:06:14,094
scale down the particular node only if the sum of request

66
00:06:14,142 --> 00:06:18,134
of the node is less than a threshold. By the way, the default threshold is

67
00:06:18,172 --> 00:06:22,054
50% of allocation. So if the total allocations of

68
00:06:22,092 --> 00:06:26,054
your node are more

69
00:06:26,092 --> 00:06:29,546
than 50%, this node will

70
00:06:29,568 --> 00:06:33,530
not be removed from the cluster even if there is enough space

71
00:06:33,680 --> 00:06:37,034
for the podes running on this node to be

72
00:06:37,072 --> 00:06:39,130
hosted on other nodes.

73
00:06:40,110 --> 00:06:43,774
The same goes with the HPA or

74
00:06:43,892 --> 00:06:47,530
specifically with the resources based HPA. New replicas

75
00:06:47,610 --> 00:06:51,150
will start when the utilization of current pods

76
00:06:51,890 --> 00:06:56,450
exceeds some percentage of pod requests,

77
00:06:58,150 --> 00:07:02,334
and I would like to stress this thing again, the utilization

78
00:07:02,462 --> 00:07:06,420
will exceed the request amount.

79
00:07:08,970 --> 00:07:12,466
So now when we are understanding the importance of pod

80
00:07:12,498 --> 00:07:16,178
requests, how do we actually right size our pods

81
00:07:16,274 --> 00:07:19,770
and what are the correct values for the request and limit?

82
00:07:20,110 --> 00:07:24,006
Here is a simple answer. We need to provision as few resources

83
00:07:24,038 --> 00:07:28,010
as possible, but without compromising our performance.

84
00:07:30,030 --> 00:07:34,234
The request should guarantee enough resources for a proper operation

85
00:07:34,362 --> 00:07:38,990
and limit should protect our nodes from overutilization.

86
00:07:42,370 --> 00:07:45,838
So let's see what happens in the misprovisioning

87
00:07:45,934 --> 00:07:49,570
scenarios. If pod requests are too big, we will cause

88
00:07:49,640 --> 00:07:53,634
waste and excessive co2 emission. If the requests are

89
00:07:53,672 --> 00:07:56,982
under provisioned, kubernetes will not guarantee that pod will have enough

90
00:07:57,036 --> 00:08:00,770
resources to run if we forgot to provision

91
00:08:00,850 --> 00:08:04,422
requests at all. The Kubernetes will not allocate enough

92
00:08:04,476 --> 00:08:08,150
resources for a pod on the node during the assignment.

93
00:08:08,650 --> 00:08:12,486
This same as under provisioning may probably

94
00:08:12,588 --> 00:08:16,614
cause unexpected pod eviction on the memory pressure

95
00:08:16,662 --> 00:08:18,090
or cpu pressure.

96
00:08:20,030 --> 00:08:23,246
As for the limit, under provisioning, limits will

97
00:08:23,268 --> 00:08:26,398
cause cpu throttling or out of memory service

98
00:08:26,484 --> 00:08:29,898
will fail on lack of resources during load bursts,

99
00:08:29,994 --> 00:08:33,826
even if there is bunch of free resources available in entire cluster or

100
00:08:33,848 --> 00:08:35,650
even in a particular node.

101
00:08:37,510 --> 00:08:40,686
Over provisioned limits will set a wrong cutoff

102
00:08:40,718 --> 00:08:44,690
threshold, ending up with the failure of the entire node.

103
00:08:45,030 --> 00:08:49,186
Failure of the node under load spike can easily end up with a domino

104
00:08:49,218 --> 00:08:52,600
effect and cause complete outage for our system.

105
00:08:53,530 --> 00:08:57,318
Specifically for the cpu limit. In some situation it

106
00:08:57,324 --> 00:09:01,162
is okay to remove cpu limit completely and

107
00:09:01,216 --> 00:09:04,426
only cpu limit. We are not talking here about the memory limit at

108
00:09:04,448 --> 00:09:08,250
all. This is because of compressible nature of cpu.

109
00:09:09,550 --> 00:09:13,358
The complete fair scheduler of operating system

110
00:09:13,444 --> 00:09:16,906
will figure out how to distribute cpu

111
00:09:16,938 --> 00:09:19,550
time between different containers.

112
00:09:21,890 --> 00:09:25,566
So finally our mission of right sizing

113
00:09:25,598 --> 00:09:29,154
is clear. Let's roll our sleeves and set each

114
00:09:29,192 --> 00:09:33,326
and every pod with few resources as possible without compromising

115
00:09:33,358 --> 00:09:36,962
the performance. But how do we actually decide

116
00:09:37,026 --> 00:09:38,950
what is the right amount of values?

117
00:09:40,650 --> 00:09:44,322
Is it a half core or four cores? Is it 100 megabytes

118
00:09:44,386 --> 00:09:49,926
or 1gb? Intuitively we

119
00:09:49,948 --> 00:09:53,226
can try to calculate it based on the metrics. Or maybe we will just have

120
00:09:53,248 --> 00:09:56,140
a VPA recommending some values for us.

121
00:09:57,230 --> 00:10:00,918
It seems like an easy task. We just need all the service

122
00:10:01,024 --> 00:10:03,834
owners to go workload by workload.

123
00:10:03,962 --> 00:10:08,026
Look at all the metrics and adjust them accordingly for hundreds

124
00:10:08,058 --> 00:10:12,030
of workloads in multiple clusters. And we also

125
00:10:12,100 --> 00:10:16,002
will ask those service owners to keep going and

126
00:10:16,056 --> 00:10:19,106
do it every time when there is a code change,

127
00:10:19,288 --> 00:10:22,530
change in architecture or traffic patterns.

128
00:10:22,950 --> 00:10:26,186
Unfortunately, it does not sound like a realistic

129
00:10:26,238 --> 00:10:29,734
plan and this

130
00:10:29,772 --> 00:10:32,790
level of complexity definitely requires a solution.

131
00:10:33,130 --> 00:10:37,042
From my personal experience, good DevOps solutions are consist

132
00:10:37,106 --> 00:10:40,380
70% of philosophy and 30% of technology.

133
00:10:40,990 --> 00:10:44,362
The philosophy part of such solution for our

134
00:10:44,416 --> 00:10:48,534
problem is to establish an effective feedback loop to pinpoint,

135
00:10:48,662 --> 00:10:52,526
quantify and address relevant problems on the

136
00:10:52,548 --> 00:10:56,270
technology part. The shift from data to intelligence

137
00:10:56,850 --> 00:11:00,270
what is the difference between data and intelligence?

138
00:11:01,010 --> 00:11:04,610
Data is not considered intelligence until it is something

139
00:11:04,680 --> 00:11:07,730
that can be applied or acted upon.

140
00:11:08,710 --> 00:11:12,174
In other words, human are not good in analyzing massive

141
00:11:12,222 --> 00:11:15,834
amounts of data. It is boring and time consuming.

142
00:11:15,982 --> 00:11:19,894
Switching from data to actionable intelligence will streamline the

143
00:11:19,932 --> 00:11:21,560
decision making process.

144
00:11:24,010 --> 00:11:27,734
This approach will allow to shift from continuous firefighting to

145
00:11:27,772 --> 00:11:31,914
proactively pinpoint and predict and fix

146
00:11:32,032 --> 00:11:35,542
problems to switch from guesstimation

147
00:11:35,606 --> 00:11:38,060
mode to data driven decision making.

148
00:11:39,550 --> 00:11:43,222
The end result of such approach will be improved

149
00:11:43,286 --> 00:11:47,498
resilience, less SLA and SLO breaches, reduced waste

150
00:11:47,514 --> 00:11:51,760
and carbon footprint, and effective governance of the platform.

151
00:11:52,370 --> 00:11:55,490
Now let's see it in action.

152
00:12:00,390 --> 00:12:03,470
So let's see how perfect scale

153
00:12:03,630 --> 00:12:07,750
approach can help with right sizing kubernetes

154
00:12:08,410 --> 00:12:12,486
here we see a cluster. This cluster contains 240

155
00:12:12,588 --> 00:12:15,970
different workloads. Here they are deployment,

156
00:12:16,050 --> 00:12:19,350
stateful set applications,

157
00:12:20,350 --> 00:12:24,614
demon sets jobs the total cluster

158
00:12:24,662 --> 00:12:30,300
cost for last one months is $3,687.

159
00:12:31,070 --> 00:12:33,930
Let's see the big picture of our cluster.

160
00:12:34,430 --> 00:12:37,694
Our cluster combined combined for the last

161
00:12:37,732 --> 00:12:41,230
one months utilizes in 99%

162
00:12:41,300 --> 00:12:44,850
of the time, 61 cores of cpu

163
00:12:45,190 --> 00:12:48,638
or less. 261 gig memory

164
00:12:48,734 --> 00:12:52,270
or less. The combined

165
00:12:52,430 --> 00:12:56,002
number of the requests that are set together

166
00:12:56,136 --> 00:12:59,526
for all the workloads in 99% of the

167
00:12:59,548 --> 00:13:03,282
times is 156 cores

168
00:13:03,346 --> 00:13:07,302
of cpu or less. Same goes for the memory. Four hundreds

169
00:13:07,356 --> 00:13:10,300
and seven gig of memory or less.

170
00:13:11,710 --> 00:13:14,090
Let's see the total allocated.

171
00:13:15,310 --> 00:13:18,534
This is the size of our cluster and what we can easily

172
00:13:18,582 --> 00:13:22,254
see that our cluster is nearly four times bigger than we

173
00:13:22,292 --> 00:13:25,760
actually would need 99% of the times.

174
00:13:27,090 --> 00:13:30,622
However, this picture shows

175
00:13:30,676 --> 00:13:34,770
us that we have enough resources to run any workload

176
00:13:35,350 --> 00:13:39,074
in this cluster. So we

177
00:13:39,112 --> 00:13:43,070
detected 131 different resilience

178
00:13:43,150 --> 00:13:47,042
issues related to the missing or misconfigured

179
00:13:47,186 --> 00:13:50,550
resources such as requests or limits.

180
00:13:51,770 --> 00:13:55,320
Let's see an example. This is a couch base.

181
00:13:55,850 --> 00:13:59,446
It is a stateful set. It's running in a namespace

182
00:13:59,478 --> 00:14:03,450
of prode. It's running

183
00:14:03,520 --> 00:14:07,242
for 924 hours

184
00:14:07,376 --> 00:14:11,494
within last one months. This number represents

185
00:14:11,622 --> 00:14:15,694
the total uptime for all the replicas that this workload have.

186
00:14:15,812 --> 00:14:19,486
For example, if we would observe 1 hour time frame and we

187
00:14:19,508 --> 00:14:23,246
would have one replica, the number will be one. And if

188
00:14:23,268 --> 00:14:26,834
we would have three replicas at the same hour, number for this hour

189
00:14:26,872 --> 00:14:30,914
will be three. Then we understand on top

190
00:14:30,952 --> 00:14:34,562
of which node this workload is running. We also

191
00:14:34,616 --> 00:14:38,070
understand what fraction of the node is actually

192
00:14:38,140 --> 00:14:42,146
optimized or allocated toward this workload.

193
00:14:42,338 --> 00:14:46,280
So we eventually know how much the workload cost.

194
00:14:48,750 --> 00:14:52,554
We are indicating a high resilience risk for

195
00:14:52,592 --> 00:14:55,420
this workload. Let's see what the risk is.

196
00:14:56,430 --> 00:15:00,650
Let's see what do we know about this workload? This workload

197
00:15:01,010 --> 00:15:04,574
have somewhere between two to four replicas with average of three

198
00:15:04,612 --> 00:15:08,366
replicas during last one months. And we

199
00:15:08,388 --> 00:15:13,040
see a high throttling happening on the cpu and

200
00:15:13,970 --> 00:15:17,194
why this trotting is happening. This rotoring is having

201
00:15:17,252 --> 00:15:21,118
because this particular workload defined with 1000 milli

202
00:15:21,134 --> 00:15:24,450
cores as a request, 3000 millicores as a limit.

203
00:15:24,790 --> 00:15:28,966
In 95% of the time our utilization was two cores of

204
00:15:29,068 --> 00:15:32,614
CPU and the highest spike that we observed is very

205
00:15:32,652 --> 00:15:35,000
very close to the limit that we set.

206
00:15:36,410 --> 00:15:40,546
This is why the struggling happens. Those values

207
00:15:40,578 --> 00:15:44,202
might be correct at the moment they were set, but since

208
00:15:44,256 --> 00:15:47,546
then many things changed. Maybe you have more customers, maybe you

209
00:15:47,568 --> 00:15:51,126
have more data in the database. Maybe you have less efficient query or

210
00:15:51,168 --> 00:15:54,266
more microservices pulling from the same database,

211
00:15:54,458 --> 00:15:57,150
pulling data from the same database.

212
00:15:58,050 --> 00:16:02,010
So perfect scale coming. Analyzing the behavior

213
00:16:02,090 --> 00:16:05,682
of the workload of all the replicas of this workload and coming with

214
00:16:05,736 --> 00:16:09,906
recommendations how much resources you would need to set in

215
00:16:09,928 --> 00:16:12,770
order to run this workload smoothly.

216
00:16:13,430 --> 00:16:17,234
Those recommendations are also combined into the

217
00:16:17,272 --> 00:16:21,014
convenient YaML file that you can simply copy paste into

218
00:16:21,052 --> 00:16:24,870
the infrastructure as a code and run the CI CD in order to

219
00:16:24,940 --> 00:16:28,514
actually fix the problem. But in some situations

220
00:16:28,562 --> 00:16:32,186
you are not the person to make the actual fix. There is

221
00:16:32,208 --> 00:16:35,260
a service owner and he need to address the issue.

222
00:16:35,950 --> 00:16:39,766
So we can simply create a task. This task will go directly

223
00:16:39,798 --> 00:16:43,326
to the JIRA and later on can be assigned to the

224
00:16:43,348 --> 00:16:47,246
relevant stakeholder and actually fit into the

225
00:16:47,268 --> 00:16:51,150
normal workflow of the development lifecycle.

226
00:16:53,510 --> 00:16:57,602
Additional perk we can set different resilience levels for

227
00:16:57,656 --> 00:17:01,218
our workload. For example, if we running

228
00:17:01,304 --> 00:17:04,702
production database, we would like to set much wider

229
00:17:04,766 --> 00:17:08,454
boundaries for the workload. And if we

230
00:17:08,492 --> 00:17:12,950
set it to the resilience of highest level, our recommendations

231
00:17:14,170 --> 00:17:18,498
would be much bigger and we also

232
00:17:18,684 --> 00:17:21,786
will calculate the impact of the change.

233
00:17:21,888 --> 00:17:25,226
So this particular database in the highest level

234
00:17:25,248 --> 00:17:30,060
of resilience would increase the monthly cost about

235
00:17:30,590 --> 00:17:32,240
70 80%.

236
00:17:34,210 --> 00:17:38,170
In the same way we're detecting the under provisioned workloads.

237
00:17:38,250 --> 00:17:41,822
For example, this collector catcher is a deployment running in

238
00:17:41,876 --> 00:17:44,894
pro namespace and we

239
00:17:44,932 --> 00:17:48,866
spent $94 for this workload during last month's, out of

240
00:17:48,888 --> 00:17:51,970
which $76 were completely wasted.

241
00:17:53,270 --> 00:17:57,454
Let's see how. So this workload contains two different containers,

242
00:17:57,502 --> 00:18:01,346
the Yeager agent that collected traces and the actual business logic

243
00:18:01,378 --> 00:18:05,266
container. This business logic container is provisioned

244
00:18:05,458 --> 00:18:08,746
with ten gig of memory for each replica. This one

245
00:18:08,768 --> 00:18:12,246
is running from somewhere between one to six replicas

246
00:18:12,278 --> 00:18:16,506
with average of three replicas. And the utilization is

247
00:18:16,608 --> 00:18:20,314
somewhere around 2gb of memory. So we basically

248
00:18:20,432 --> 00:18:23,918
throw in 8gb of memory for each replica that we

249
00:18:23,924 --> 00:18:27,790
are running. Again, we have a handy yaml to fix the problem

250
00:18:27,860 --> 00:18:32,846
and we can create a task in

251
00:18:32,868 --> 00:18:36,498
a similar way. We are pinpointing all the different problems

252
00:18:36,584 --> 00:18:40,402
that you have in your cluster or categorizing those problems by

253
00:18:40,456 --> 00:18:43,954
risk. So you can either focus on

254
00:18:43,992 --> 00:18:48,550
the highest risk in particular namespace

255
00:18:49,610 --> 00:18:53,126
or you can go and dive into particular type

256
00:18:53,148 --> 00:18:57,030
of the problem. For example, under provision memory limit.

257
00:18:58,330 --> 00:19:01,698
Let's see again. Let's see it in action.

258
00:19:01,794 --> 00:19:06,246
So we have the workload here. This workload suffers

259
00:19:06,358 --> 00:19:10,554
from very low request in

260
00:19:10,592 --> 00:19:14,430
95% of the times. We need three times

261
00:19:14,500 --> 00:19:17,982
more resources and the

262
00:19:18,036 --> 00:19:22,110
limit is very very close to the actual utilization.

263
00:19:22,530 --> 00:19:25,858
Also, we observed the trend going

264
00:19:25,944 --> 00:19:29,486
up with memory utilization. So we are basically predicting

265
00:19:29,518 --> 00:19:33,794
here that at some point in time out

266
00:19:33,832 --> 00:19:37,102
of memory will occur and we are suggesting

267
00:19:37,166 --> 00:19:41,010
to fix the problem by increasing the amount of allocated resources

268
00:19:41,090 --> 00:19:45,122
and increasing the limit. This is going to be our impact

269
00:19:45,186 --> 00:19:49,022
of the change, but we will have this workload

270
00:19:49,186 --> 00:19:50,890
running smoothly.

271
00:19:54,510 --> 00:19:57,834
Now let's see the multicluster multicloud view.

272
00:19:57,952 --> 00:20:01,510
In this view, we see each and every cluster

273
00:20:01,670 --> 00:20:05,134
running in different clouds. We see all the problems that

274
00:20:05,172 --> 00:20:08,814
this particular cluster have, all the waste and all the total cost,

275
00:20:08,932 --> 00:20:13,322
and even the carbon footprint that this particular cluster generates.

276
00:20:13,466 --> 00:20:16,942
We see how those numbers are summing up

277
00:20:16,996 --> 00:20:20,734
in the organization level of view. How much

278
00:20:20,772 --> 00:20:24,266
is the cost, how much is the waste, how much savings

279
00:20:24,298 --> 00:20:28,326
we generated, and how much existing risks out there.

280
00:20:28,508 --> 00:20:32,166
So I hope you are enjoyed our session today

281
00:20:32,348 --> 00:20:36,306
and you learned something new about the right sizing and right scaling

282
00:20:36,338 --> 00:20:39,574
of kubernetes. Feel free to ping me on

283
00:20:39,612 --> 00:20:42,280
LinkedIn or contact me with your website.

284
00:20:42,650 --> 00:20:44,326
Thank you very much for your time.

