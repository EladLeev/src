1
00:00:22,250 --> 00:00:26,018
Hi, I'm Timothy Span. I'm a develop or advocate.

2
00:00:26,114 --> 00:00:29,970
I work mostly in streaming and in open source.

3
00:00:30,130 --> 00:00:33,970
That might seem a little strange in the machine learning track, but I'll

4
00:00:34,050 --> 00:00:37,414
show you a lot of different things you might not have thought of

5
00:00:37,452 --> 00:00:40,166
before on how to get data,

6
00:00:40,348 --> 00:00:44,034
manipulate data, run different processes

7
00:00:44,082 --> 00:00:47,442
that you needed to run to be able to do your machine

8
00:00:47,506 --> 00:00:50,714
learning and your training, classification.

9
00:00:50,842 --> 00:00:54,366
Lots of different things there. So this is for data engineers, this is

10
00:00:54,388 --> 00:00:57,902
for programmer, this is for data scientists. Anyone who

11
00:00:57,956 --> 00:01:01,598
wants to work with data and do it fast, not have

12
00:01:01,604 --> 00:01:04,794
to wait for a batch or something to load.

13
00:01:04,922 --> 00:01:08,290
Let's just get that data, run it as fast as possible,

14
00:01:08,440 --> 00:01:11,806
and do it in a way that's scalable, open source,

15
00:01:11,838 --> 00:01:15,418
and easy to use. Welcome to my talk here. This is hale

16
00:01:15,454 --> 00:01:19,094
hydrate from stream to lake. Now, I'm going to show you

17
00:01:19,132 --> 00:01:22,790
different ways that you could work with data lakes.

18
00:01:23,210 --> 00:01:26,598
Also any other source or sync of data.

19
00:01:26,764 --> 00:01:30,566
So just a could of quick flink on me. You'll have these slides

20
00:01:30,678 --> 00:01:35,398
so you can go through all these sites and see sources, code articles,

21
00:01:35,574 --> 00:01:39,874
deep dives into different technologies, whether it's Apache Nifi,

22
00:01:40,022 --> 00:01:43,290
Apache Spark, Apache OpenNLP,

23
00:01:43,370 --> 00:01:47,050
Apache Pulsar, Apache Kafka, Apache Flink.

24
00:01:47,210 --> 00:01:50,526
I also have some things on Tensorflow. I do

25
00:01:50,548 --> 00:01:55,050
a lot of different devices. I've got Nvidia Jetsons, raspberry Pis,

26
00:01:55,210 --> 00:01:58,754
tons of different content. And if you're missing something or going,

27
00:01:58,792 --> 00:02:02,542
I wish Tim would do this or that, drop me a line, whether on Twitter,

28
00:02:02,606 --> 00:02:06,830
LinkedIn, wherever you see me, comment in this conference,

29
00:02:06,990 --> 00:02:10,498
I am always looking for more things to work on, more apps

30
00:02:10,514 --> 00:02:13,858
to build more real time data pipelines.

31
00:02:14,034 --> 00:02:17,446
Show me something you're missing and let's do it. One thing you

32
00:02:17,468 --> 00:02:20,634
might notice is, I like cats. I have some cats, so you might see

33
00:02:20,672 --> 00:02:24,422
some in the pictures. Those pictures are perhaps unrelated.

34
00:02:24,566 --> 00:02:28,266
So the agents today, in this 40 or so minutes that

35
00:02:28,288 --> 00:02:32,442
I'm going to do is as follows. My use case, the primary

36
00:02:32,506 --> 00:02:36,366
one is to populate a data lake. This could be in any cloud,

37
00:02:36,468 --> 00:02:39,882
this could be from any vendor, whether it's Snowflake,

38
00:02:39,946 --> 00:02:43,474
Cloudera, Amazon, Microsoft. You know, you could build your

39
00:02:43,512 --> 00:02:46,674
own open source and put it wherever you need it to be. It could be

40
00:02:46,712 --> 00:02:50,210
in a cluster you've built of raspberry pis,

41
00:02:51,110 --> 00:02:54,270
some obscure cloud, a vm,

42
00:02:54,350 --> 00:02:58,082
whatever. We'll go over some of the challenges. What are the different

43
00:02:58,136 --> 00:03:02,018
impacts they have on what you're trying to do. Give you a solution,

44
00:03:02,194 --> 00:03:05,606
show you what the final outcome of that is. Talk to you

45
00:03:05,628 --> 00:03:09,394
about a couple of different streaming technologies in the open source,

46
00:03:09,522 --> 00:03:13,046
which means it's a great community behind it and you don't

47
00:03:13,078 --> 00:03:16,518
have to pay. But once you get into production,

48
00:03:16,694 --> 00:03:21,558
probably want some people to support you, but show you my successful architecture,

49
00:03:21,734 --> 00:03:25,006
spend as much time in that demo as possible and have a couple of

50
00:03:25,028 --> 00:03:28,478
next steps so you know where to go next. So use case

51
00:03:28,644 --> 00:03:31,774
primary one, I'm going to show you a couple because it's that easy.

52
00:03:31,892 --> 00:03:35,586
We're going to do Iot ingestion. So I've got a

53
00:03:35,608 --> 00:03:38,834
device running here on my desk here. If we were

54
00:03:38,872 --> 00:03:41,982
in person, it'd be sitting in front of me in the podium,

55
00:03:42,126 --> 00:03:45,682
which is pretty cool. So we've got a high volume

56
00:03:45,746 --> 00:03:49,190
streaming sources, different types of messages,

57
00:03:49,530 --> 00:03:52,998
different protocols, many vendors. So there could be

58
00:03:53,004 --> 00:03:56,146
a lot of challenges around that. There's no cookie

59
00:03:56,178 --> 00:03:59,910
cutter solution there. Things are different. Sometimes you're in a big hand,

60
00:03:59,980 --> 00:04:03,594
sometimes you're in a little door, you never know. So first thing

61
00:04:03,632 --> 00:04:06,902
that comes up, like we mentioned, lets of different protocols,

62
00:04:06,966 --> 00:04:10,538
different vendors, people like to do things their own way, but I want

63
00:04:10,544 --> 00:04:14,022
to be able to see what's going on right away. I don't want to wait

64
00:04:14,096 --> 00:04:16,910
for an hour, I don't want to wait for the end of the day.

65
00:04:17,060 --> 00:04:20,478
I need this data now because I'm going to be continually training

66
00:04:20,564 --> 00:04:24,594
my machine learning models, or I need to feed data

67
00:04:24,792 --> 00:04:28,482
into something that maybe is using that model,

68
00:04:28,616 --> 00:04:32,574
doing some classification and giving you some analysis

69
00:04:32,622 --> 00:04:36,190
to say, hey, there's a fraud condition, there's an alert,

70
00:04:36,270 --> 00:04:39,330
something's too high, something's been damaged,

71
00:04:39,490 --> 00:04:43,142
you should buy a stock, the weather's changing, you better bring things

72
00:04:43,196 --> 00:04:46,486
inside, you better start selling more ice cream because

73
00:04:46,508 --> 00:04:49,978
it's 100 degrees fahrenheit, those sort of things.

74
00:04:50,144 --> 00:04:53,174
What's nice as well, we'll show you. With the open sources,

75
00:04:53,302 --> 00:04:57,098
I can have visibility, which is often really

76
00:04:57,184 --> 00:05:00,198
hard on everything going on in the stream,

77
00:05:00,294 --> 00:05:03,582
because think about it, there's devices on the edge. I could

78
00:05:03,636 --> 00:05:06,766
be using various cloud servers, maybe I have

79
00:05:06,788 --> 00:05:10,682
a server that's sitting in a gateway in the back of a moving truck.

80
00:05:10,826 --> 00:05:14,226
Lots of things can go wrong. There could be bottlenecks, things could

81
00:05:14,248 --> 00:05:17,986
slow down, messages can get lost. I'll show you how. We could keep

82
00:05:18,088 --> 00:05:21,666
an eye on all that, automate that, make that simple.

83
00:05:21,848 --> 00:05:25,718
You could start today taking real time

84
00:05:25,884 --> 00:05:29,206
data, marrying it with machine learning, and doing that

85
00:05:29,228 --> 00:05:33,174
all in real time. Pretty easy. Now, one thing

86
00:05:33,212 --> 00:05:36,886
that's difficult here is sure, maybe you could

87
00:05:36,908 --> 00:05:40,294
find a solution. Maybe you wrote a ton of python scripts

88
00:05:40,342 --> 00:05:43,818
maybe there's a shell script. You've got code all over the place.

89
00:05:43,904 --> 00:05:47,498
That sprawl is hard to handle. Could also be difficult.

90
00:05:47,664 --> 00:05:51,334
You're hiring maybe consultants, maybe a bunch of different developers.

91
00:05:51,382 --> 00:05:54,314
You have to maintain that. Lots of different tools,

92
00:05:54,362 --> 00:05:57,806
nothing standardized. Can't find all the source code, don't know

93
00:05:57,828 --> 00:06:01,280
where it is running. Is it running over here? I shut down

94
00:06:01,810 --> 00:06:04,694
a vm, I shut down Kubernetes pod.

95
00:06:04,762 --> 00:06:08,050
Now things aren't working. Now I got to learn another tool,

96
00:06:08,120 --> 00:06:11,406
another language, another package. That's painful.

97
00:06:11,518 --> 00:06:14,978
It takes you forever to write this code. Those delays are not going

98
00:06:14,984 --> 00:06:18,454
to make anyone happy. Whether it's who's going to use the final

99
00:06:18,652 --> 00:06:21,954
code, or apps, or just your developers,

100
00:06:22,002 --> 00:06:25,958
your data scientists, your data analysts can't develop things quickly

101
00:06:26,044 --> 00:06:29,574
if they don't have the data, if they can't put their models out there

102
00:06:29,612 --> 00:06:33,222
quickly, if I can't get things to run my classifiers,

103
00:06:33,366 --> 00:06:36,522
you're losing money. You're losing who knows how many things.

104
00:06:36,656 --> 00:06:39,882
You need data fast. You need to be able to develop fast.

105
00:06:40,016 --> 00:06:41,680
Things need to be open,

106
00:06:42,050 --> 00:06:45,886
accessible, and have a community. So if you get stuck, you could

107
00:06:45,908 --> 00:06:49,902
keep moving on quickly. So the main solution here

108
00:06:49,956 --> 00:06:53,662
is using something like Apache Nifi. That is an open

109
00:06:53,716 --> 00:06:57,070
sources tool that does a ton of different sources.

110
00:06:57,230 --> 00:07:00,414
You don't have to learn a new tool for a new source.

111
00:07:00,542 --> 00:07:03,954
Supports lots of different message formats and protocols and

112
00:07:03,992 --> 00:07:07,106
vendors, easy to use, drag and drop,

113
00:07:07,218 --> 00:07:10,502
huge community and fits in well with everything else

114
00:07:10,556 --> 00:07:14,086
in streaming. Supports any type of data. It doesn't matter if

115
00:07:14,108 --> 00:07:17,422
it's xml, if it's binary,

116
00:07:17,586 --> 00:07:22,134
a word, document, PDF, CSV, tab delimited,

117
00:07:22,262 --> 00:07:25,402
JSON Avro, parquet I could say

118
00:07:25,456 --> 00:07:28,586
all kinds of words. Some of them you may know, some of you may not.

119
00:07:28,688 --> 00:07:32,522
Doesn't matter. Tons of different data. Sometimes the data is big,

120
00:07:32,576 --> 00:07:35,866
sometimes it's small, sometimes it's fast, sometimes it's slow.

121
00:07:35,978 --> 00:07:39,838
Doesn't matter. We could do a lot of that with little to no code.

122
00:07:40,004 --> 00:07:43,534
That makes it awesome. I can clean up my data, get it into a

123
00:07:43,572 --> 00:07:47,106
format. So if I have to do something more complex, I could do

124
00:07:47,128 --> 00:07:51,470
that with a pulsar function. I could do that with Flink, flink SQl.

125
00:07:51,630 --> 00:07:55,726
Really great. Nifi also gives me really rich visibility.

126
00:07:55,838 --> 00:07:59,302
It's got something called data provenance, which is a

127
00:07:59,356 --> 00:08:03,014
really robust lineage. And you could see everything about your

128
00:08:03,052 --> 00:08:06,614
data, all the metadata, all the metrics. You could see everything end

129
00:08:06,652 --> 00:08:10,486
to end. This is awesome. And you'll see how powerful

130
00:08:10,518 --> 00:08:14,058
that is in the demo. So now, instead of spending time trying to

131
00:08:14,064 --> 00:08:18,214
write one input app, I could build new use cases,

132
00:08:18,262 --> 00:08:22,286
new apps, get that data to the analysts and the data scientists who could

133
00:08:22,308 --> 00:08:26,670
do more models, more apps, more solutions.

134
00:08:27,010 --> 00:08:30,542
Things are cheaper now. I could develop more things

135
00:08:30,676 --> 00:08:34,320
for lets cost smaller teams to do this part.

136
00:08:34,710 --> 00:08:38,382
Let people do the fun part of building apps, building modules,

137
00:08:38,526 --> 00:08:42,034
solving problems with machine learning and deep learning makes

138
00:08:42,072 --> 00:08:45,490
you a lot more agile. I'm not spending weeks deploying things.

139
00:08:45,560 --> 00:08:49,094
I'm not spending weeks trying to figure out how to deal with

140
00:08:49,132 --> 00:08:52,342
new data when it comes in. To do things in a matter of minutes

141
00:08:52,396 --> 00:08:55,654
or hours makes you very agile when you need to change.

142
00:08:55,772 --> 00:08:59,206
Because tomorrow I have to use a different provider

143
00:08:59,238 --> 00:09:02,842
for weather. I have a new IoT device. Instead of

144
00:09:02,896 --> 00:09:06,454
spending weeks or months trying to customize

145
00:09:06,502 --> 00:09:09,546
it to whatever that format is, I could adapt on

146
00:09:09,568 --> 00:09:12,794
the fly. Makes it very powerful. Now I

147
00:09:12,832 --> 00:09:15,790
call my staff a couple of different things.

148
00:09:15,860 --> 00:09:19,614
This stack of open source Apache streaming tools out there,

149
00:09:19,732 --> 00:09:22,746
sometimes I'll call it flip, sometimes I'll call it flank.

150
00:09:22,858 --> 00:09:26,462
Flip is when I'm using Flink and pulsar,

151
00:09:26,606 --> 00:09:29,746
usually Nifi and a couple other tools as well.

152
00:09:29,848 --> 00:09:33,554
Focus on cloud data engineers, especially around the in

153
00:09:33,592 --> 00:09:37,462
machine learning to help your data scientists. But this could be in many

154
00:09:37,516 --> 00:09:40,982
other platforms. Cloud tends to be the way we're going now,

155
00:09:41,036 --> 00:09:44,294
but it could be in any environment, whether it's on

156
00:09:44,332 --> 00:09:47,486
premise, vms, docker containers, kubernetes, pods,

157
00:09:47,538 --> 00:09:50,826
wherever. With this stack I could

158
00:09:50,848 --> 00:09:54,422
support as many users as you have different frameworks,

159
00:09:54,486 --> 00:09:58,118
different languages, any of those clouds, you have lots

160
00:09:58,134 --> 00:10:01,834
of different data sources, lots of clusters. So if you have any

161
00:10:01,872 --> 00:10:05,678
experience with Python or Java, maybe a little SQL, maybe at

162
00:10:05,684 --> 00:10:09,546
least you have the concept of streaming, maybe a little ETL,

163
00:10:09,658 --> 00:10:12,560
you're ready to go using this. If you're a can,

164
00:10:13,090 --> 00:10:16,618
you could be involved here. Even you could use Nifi.

165
00:10:16,714 --> 00:10:20,526
Cat's going to question how much you're spending on cloud, but who doesn't?

166
00:10:20,638 --> 00:10:24,318
And if you happen to be cognizant

167
00:10:24,414 --> 00:10:28,086
machine learning code there, I could run you whether I'm in

168
00:10:28,108 --> 00:10:30,946
Nifi. Pulsar functions, flink,

169
00:10:31,138 --> 00:10:34,454
minify agents at the edge, wherever you need to run

170
00:10:34,492 --> 00:10:38,838
that code, whether it's sentient or not. We could do that.

171
00:10:39,004 --> 00:10:43,002
My flip stack, just to show you what it is. It uses this really

172
00:10:43,056 --> 00:10:46,854
cool pulsar flink connector that's been developed

173
00:10:46,902 --> 00:10:50,634
by stream native in the open sources. This makes it very easy for

174
00:10:50,672 --> 00:10:54,042
me to connect between flink

175
00:10:54,106 --> 00:10:57,760
code and pulsar, whether it's a source or a sync or both.

176
00:10:58,210 --> 00:11:01,450
Both tends to be common because I put something into a queue

177
00:11:01,530 --> 00:11:05,022
or a topic, I want to process it and send

178
00:11:05,076 --> 00:11:09,502
it down its way. Maybe it's cleaned up, maybe it's joined together, maybe it's aggregated.

179
00:11:09,566 --> 00:11:13,378
We show you a little bit there. I touched on Nifi as a

180
00:11:13,384 --> 00:11:16,130
big part of this. Why scalably?

181
00:11:16,470 --> 00:11:19,362
Real time streaming platform. You can collect,

182
00:11:19,426 --> 00:11:23,186
curate, use it as a universal gateway, could run my analytics

183
00:11:23,218 --> 00:11:26,678
there, I could run my machine learning models there. Does a

184
00:11:26,684 --> 00:11:29,994
lot of things. It does it very easily. And it could

185
00:11:30,032 --> 00:11:33,382
be so many different sources of data, whether they're

186
00:11:33,446 --> 00:11:36,874
cloud based, relational database, NoSQL stores like

187
00:11:36,912 --> 00:11:41,286
Mongo, elastic solar logs,

188
00:11:41,478 --> 00:11:45,262
text files, pdf, zip files, pull things

189
00:11:45,316 --> 00:11:48,800
off of government sources from a slack channel,

190
00:11:49,250 --> 00:11:53,294
hash it up, encrypt it, split text files

191
00:11:53,342 --> 00:11:56,626
apart, put them together, take apart data out of a

192
00:11:56,648 --> 00:12:00,990
zip file, look at the tail end of a log file,

193
00:12:01,150 --> 00:12:04,046
route things different way, add metadata,

194
00:12:04,158 --> 00:12:08,154
use metadata, all that while being very scalable

195
00:12:08,222 --> 00:12:11,462
out to millions of events. A second run on

196
00:12:11,516 --> 00:12:15,046
as big a cluster as you need, as many nodes, whether you're running it in

197
00:12:15,068 --> 00:12:18,566
kubernetes, whether you're running it in vms, in any

198
00:12:18,588 --> 00:12:21,782
of the clouds, on your laptop, on a desktop,

199
00:12:21,846 --> 00:12:25,686
wherever that is, hundreds of pre built components for you to drag

200
00:12:25,718 --> 00:12:28,998
and drop and build your flows very easily

201
00:12:29,174 --> 00:12:31,520
with full back pressure in there,

202
00:12:32,050 --> 00:12:35,680
security, all those features that you'd expect,

203
00:12:36,450 --> 00:12:39,434
guaranteed delivery. This is stable,

204
00:12:39,562 --> 00:12:43,294
scalable and a great solution regardless of how

205
00:12:43,332 --> 00:12:46,962
big the application is, how big your data streams are,

206
00:12:47,016 --> 00:12:50,738
what the data looks like, wherever it's running. You could do that here.

207
00:12:50,904 --> 00:12:54,546
Pulsar Pulsar is a great way to

208
00:12:54,568 --> 00:12:58,774
do distributed messaging regardless of what cloud it's running on or

209
00:12:58,812 --> 00:13:02,210
on premise. Again, the same idea with Nifi,

210
00:13:02,370 --> 00:13:05,800
open source Apache, huge community,

211
00:13:06,410 --> 00:13:09,650
lets of different options for sources and syncs.

212
00:13:09,810 --> 00:13:13,642
Lots of capabilities here. It's a no brainer for putting

213
00:13:13,696 --> 00:13:16,858
your different event data and ML data in there,

214
00:13:17,024 --> 00:13:19,862
stream your data around, do that in a secure,

215
00:13:19,926 --> 00:13:23,786
durable manner. It's georeplicated, it's great.

216
00:13:23,888 --> 00:13:25,760
Even better than my fluffy cat,

217
00:13:26,930 --> 00:13:32,026
any kind of pub sub. So you don't just have to replace workloads

218
00:13:32,058 --> 00:13:35,762
that are common in big data. This can also be in different things

219
00:13:35,816 --> 00:13:39,426
where you might be doing JMS or MQTT, which we see

220
00:13:39,448 --> 00:13:42,706
a lot in IoT, JMS, Kafka, any of

221
00:13:42,728 --> 00:13:46,530
those different protocols supported by this one messaging system

222
00:13:46,600 --> 00:13:50,390
makes it pretty easy. You can run functions very well

223
00:13:50,460 --> 00:13:53,654
integrated here. That makes it easy for you to write

224
00:13:53,772 --> 00:13:57,186
and run some of your machine learning against it like you'd

225
00:13:57,218 --> 00:14:00,714
expect. Very scalable. One thing that's really cool here.

226
00:14:00,752 --> 00:14:04,950
That's unusual is this tiered persistent storage.

227
00:14:05,110 --> 00:14:08,854
So as you put in data into this messaging

228
00:14:08,902 --> 00:14:12,494
system, it could store it out into the cloud and say

229
00:14:12,532 --> 00:14:15,982
s three buckets without you having to write special code,

230
00:14:16,116 --> 00:14:19,566
without having to do the heavy lifting that you might

231
00:14:19,588 --> 00:14:23,114
have to do elsewhere. Full rest API for managing,

232
00:14:23,162 --> 00:14:26,706
monitoring everything you need command flink interface so you make

233
00:14:26,728 --> 00:14:30,306
this easy, hook it up to your DevOps tools and there's all

234
00:14:30,328 --> 00:14:33,694
the clients you'd expect. Whether you write Python

235
00:14:33,742 --> 00:14:37,626
or you write Java, whatever it is, there's a client

236
00:14:37,678 --> 00:14:41,206
out there for you. So you could easily consume and

237
00:14:41,228 --> 00:14:44,486
produce messages. This makes this pretty great.

238
00:14:44,668 --> 00:14:47,810
I touched on Flink. I'm going to show you that in the application.

239
00:14:47,980 --> 00:14:51,082
If you haven't used Flink before, it's something

240
00:14:51,136 --> 00:14:54,982
you really need to start looking at. Flink scales out tremendously

241
00:14:55,046 --> 00:14:58,970
well, integrates with all the different frameworks you have

242
00:14:59,120 --> 00:15:02,798
typically written in Java, but with the

243
00:15:02,884 --> 00:15:06,042
enhancements that's in there. Now for Flink SQl,

244
00:15:06,186 --> 00:15:10,334
instead of having to write these complex distributed apps by hand,

245
00:15:10,532 --> 00:15:13,886
compile them, test them locally, deploy it out

246
00:15:13,908 --> 00:15:17,106
to my clusters to write a SQL statement like I

247
00:15:17,128 --> 00:15:21,102
mentioned before, if you know a little python, maybe a little Java, a little SQL,

248
00:15:21,246 --> 00:15:25,302
all of a sudden now I'm writing massively distributed real

249
00:15:25,356 --> 00:15:28,310
time pipelines for machine learning.

250
00:15:28,460 --> 00:15:31,810
Pretty straightforward. This is amazingly scalable,

251
00:15:31,890 --> 00:15:35,894
used by some of the largest companies in the world. Has everything you need

252
00:15:35,932 --> 00:15:39,286
for fault tolerance, resiliency, high availability,

253
00:15:39,478 --> 00:15:42,954
runs in yarn, runs in kubernetes, all those

254
00:15:42,992 --> 00:15:46,220
features that you want. Pretty awesome there.

255
00:15:46,590 --> 00:15:49,020
Now this is a machine learning talk.

256
00:15:50,030 --> 00:15:53,374
Get back to this. For some reason it's not showing my image here.

257
00:15:53,412 --> 00:15:57,166
Hopefully I lost my image. We'll show you that later.

258
00:15:57,268 --> 00:16:00,554
But basically what I have here is I've

259
00:16:00,602 --> 00:16:04,082
included in my Nifi distribution a couple

260
00:16:04,136 --> 00:16:07,586
of open sources components I've written so that I

261
00:16:07,608 --> 00:16:10,654
could do deep learning as part of that stream.

262
00:16:10,782 --> 00:16:14,610
And these are built using Apache, Mxnet, which is Java

263
00:16:14,970 --> 00:16:18,742
and DL for J. This is a deep learning

264
00:16:18,796 --> 00:16:22,790
Java library that lets me write

265
00:16:22,940 --> 00:16:26,422
in Java and then deploy it as a final model

266
00:16:26,476 --> 00:16:30,278
and say tensorflow or Mxnet or Pytorch.

267
00:16:30,374 --> 00:16:34,454
Really powerful. And sometimes I need to do some natural language

268
00:16:34,502 --> 00:16:37,782
processing. Patchy provides a great library

269
00:16:37,846 --> 00:16:40,938
for that. I put that into Nifi, so you

270
00:16:40,944 --> 00:16:44,862
could use that as part of your flow very easily. You don't have to call

271
00:16:44,916 --> 00:16:48,190
third party services, don't have to call another

272
00:16:48,260 --> 00:16:51,374
library. Pretty easy. This is

273
00:16:51,412 --> 00:16:55,082
my solution here. This is our architecture. Got a lot of different files.

274
00:16:55,146 --> 00:16:58,462
Lets me show you a couple different ones. XMl and JSON

275
00:16:58,606 --> 00:17:02,882
come in from different sources. If I get some, validates them,

276
00:17:03,016 --> 00:17:06,434
cleans them up, routes them where they need to go, we get that through our

277
00:17:06,472 --> 00:17:10,470
messaging system and then landed in the cloud. And then you can write

278
00:17:10,620 --> 00:17:14,278
analytics on it. And I'll show you some analytics there.

279
00:17:14,364 --> 00:17:17,974
And I'll show you some continuous queries in flink just

280
00:17:18,012 --> 00:17:21,450
to show you the ideas of what you can do with data while

281
00:17:21,520 --> 00:17:25,946
it's happening. Real time events, event happens,

282
00:17:26,128 --> 00:17:29,786
you're working on at the same time, there's no delays there. As soon as

283
00:17:29,808 --> 00:17:33,418
that network can get it to you, you're doing something with it. Whether it's

284
00:17:33,434 --> 00:17:36,926
a continuous sql, some kind of decisions, whatever it

285
00:17:36,948 --> 00:17:40,494
may be. This is that IoT data I was

286
00:17:40,532 --> 00:17:44,834
talking about. You got things like temperature, you got things like different

287
00:17:44,952 --> 00:17:48,500
sensor readings, important things. If you're running

288
00:17:49,110 --> 00:17:52,626
any kind of edge application, real time

289
00:17:52,808 --> 00:17:56,514
vehicles, maintenance, all kinds of devices out there

290
00:17:56,632 --> 00:18:00,066
also have ones for weather. And I've got links to

291
00:18:00,088 --> 00:18:03,382
some more contents you can go into that. This is.

292
00:18:03,436 --> 00:18:06,774
Thank you. We are done with the slides. I think

293
00:18:06,812 --> 00:18:09,914
everyone's happy to be done with the slides. I'm happy

294
00:18:09,952 --> 00:18:13,130
to be done with the slides. Let's show you real code

295
00:18:13,200 --> 00:18:17,340
running. That's probably more interesting than what you've seen before.

296
00:18:17,870 --> 00:18:21,134
So right now I am in Apache Nifi. We talked

297
00:18:21,172 --> 00:18:25,626
about it enough. This is it. This is not the flow

298
00:18:25,738 --> 00:18:29,742
diagram for it. This is not some

299
00:18:29,796 --> 00:18:33,870
documentation for it. This is my running system.

300
00:18:34,020 --> 00:18:37,054
There is a GUI environment that lets you build,

301
00:18:37,172 --> 00:18:40,546
monitor, deploy this. This could be locked down and

302
00:18:40,568 --> 00:18:44,274
secure. We could hide this UI if you don't want anyone ever to see

303
00:18:44,312 --> 00:18:48,262
it. But this is running. And if you see here, I've got a real time

304
00:18:48,316 --> 00:18:51,958
stream of data coming in. I have edge devices that

305
00:18:51,964 --> 00:18:55,334
are making HTTPs calls in, sending me

306
00:18:55,372 --> 00:18:59,938
data, and I have it queued because I didn't

307
00:18:59,954 --> 00:19:03,078
want to run it until I could show it to you. So right now I'm

308
00:19:03,094 --> 00:19:07,130
going to start something. This is pretty amazing. I could start and stop,

309
00:19:07,280 --> 00:19:10,634
and I could do this either through the UI, through a rest call

310
00:19:10,672 --> 00:19:13,902
or command flink interface, any part of the system.

311
00:19:14,036 --> 00:19:17,422
So if I want to pause something, maybe I

312
00:19:17,476 --> 00:19:21,406
ran out of cloud money this month. I'm going to pause it here, queue it

313
00:19:21,428 --> 00:19:25,230
up until I can get myself some more cloud availability,

314
00:19:25,390 --> 00:19:28,802
or maybe spin up some new unstructured, or maybe

315
00:19:28,856 --> 00:19:32,034
something's having problems downstream. I can pause it,

316
00:19:32,072 --> 00:19:36,022
never lose data and have no issues there.

317
00:19:36,156 --> 00:19:39,654
So I'm getting data, I'm routing energy data in

318
00:19:39,852 --> 00:19:43,142
sending that to my messaging system. I'm running

319
00:19:43,196 --> 00:19:46,374
some real time queries on some of this

320
00:19:46,412 --> 00:19:49,974
data as it comes in this query I have in

321
00:19:50,012 --> 00:19:53,766
my parameters so I could isolate that out when I deploy

322
00:19:53,798 --> 00:19:57,418
my code using DevOps tools. This is

323
00:19:57,504 --> 00:20:01,066
my current query. If the temperature in fahrenheit is

324
00:20:01,088 --> 00:20:04,734
over 60, we may be getting tools hot. Obviously you

325
00:20:04,772 --> 00:20:07,822
set these yourself. Maybe I could have machine learning figure

326
00:20:07,876 --> 00:20:11,934
out what's the new normal for temperature for that sensor reading or

327
00:20:11,972 --> 00:20:15,670
the current weather. That's up to you. Maybe I compare it against weather,

328
00:20:15,770 --> 00:20:19,138
maybe I compare it against other sensors of the same class.

329
00:20:19,304 --> 00:20:23,250
You get the idea of various things you could do there, stream the data

330
00:20:23,320 --> 00:20:27,038
in and then when I'm ready I split it up

331
00:20:27,064 --> 00:20:30,626
into individual records so I'm not sending

332
00:20:30,658 --> 00:20:34,642
too big a file out, pushing it to a couple different messaging

333
00:20:34,706 --> 00:20:38,138
systems so that I can push that up to the cloud.

334
00:20:38,304 --> 00:20:42,598
If you look here now, I'm in an Amazon hosted cluster,

335
00:20:42,774 --> 00:20:46,090
doesn't look much different to you. I have permissions in both.

336
00:20:46,240 --> 00:20:50,234
Here I've got this router stopped so no

337
00:20:50,272 --> 00:20:53,774
data was processing. I do a refresh and that's all

338
00:20:53,812 --> 00:20:57,934
been processed. Hundreds of records on a single code. Very easy

339
00:20:57,972 --> 00:21:01,134
to do. I add some metadata here,

340
00:21:01,252 --> 00:21:04,346
things like table name, what I'm doing with

341
00:21:04,388 --> 00:21:07,554
it and I'm going to send it to another messaging queue for some

342
00:21:07,592 --> 00:21:11,006
further analytics with flink here. I'm sending

343
00:21:11,038 --> 00:21:14,898
this to a cloud data store and you want

344
00:21:14,904 --> 00:21:18,134
to see how hard it was to write that code, point it to the name

345
00:21:18,172 --> 00:21:22,034
of the server, give it a table. And I'm defining that dynamically

346
00:21:22,162 --> 00:21:26,146
so that nothing's hard coded. And I'm saying it's JSOn.

347
00:21:26,258 --> 00:21:29,934
That's it. Oh, and I want to do up lets this one supports upset,

348
00:21:30,002 --> 00:21:33,574
could have been update or insert whatever. So if you notice

349
00:21:33,622 --> 00:21:36,682
something, where's the fields? I don't have to write any,

350
00:21:36,736 --> 00:21:39,974
handwrite any SQL. I don't have to write any mapping

351
00:21:40,022 --> 00:21:44,910
code. My record reader technology looks

352
00:21:44,980 --> 00:21:48,634
at it, sees if you have a schema, if you've got a schema defined

353
00:21:48,682 --> 00:21:52,074
somewhere, I'll use that. So I know exactly field names,

354
00:21:52,122 --> 00:21:55,874
field types, exactly what you want it to have and then I'll just

355
00:21:55,912 --> 00:21:59,570
match that up to the table. Very easy. Now sometimes

356
00:21:59,640 --> 00:22:03,394
your data might change a lot and you might have it, just have

357
00:22:03,432 --> 00:22:06,822
the code infer it for you. So it'll look at a

358
00:22:06,876 --> 00:22:10,374
number of records and say okay, these are the fields I see in this

359
00:22:10,412 --> 00:22:14,434
JSON or XML or Avro

360
00:22:14,562 --> 00:22:18,182
or different types of files. Let me

361
00:22:18,236 --> 00:22:22,314
show you how those readers work. So I could just pick a different one.

362
00:22:22,352 --> 00:22:26,074
I could do avro comma separated value. Grok will

363
00:22:26,112 --> 00:22:29,718
read logs or any kind of semi structured text.

364
00:22:29,904 --> 00:22:33,834
IP fix files, parquet syslogs

365
00:22:33,882 --> 00:22:36,894
window events XML, get the idea?

366
00:22:37,012 --> 00:22:40,414
Pretty easy to do that. Don't have to learn anything else.

367
00:22:40,452 --> 00:22:43,482
If you have any custom weird formats, open sources,

368
00:22:43,546 --> 00:22:46,818
take a look. Someone else might have written it, you write it yourself

369
00:22:46,904 --> 00:22:49,922
or get a consultant to do it. Pretty easy to do.

370
00:22:50,056 --> 00:22:53,746
These little boxes, I've written about 50 of them.

371
00:22:53,848 --> 00:22:57,246
It's Java. Really simple kind of. If you've ever done spring

372
00:22:57,278 --> 00:23:00,642
code like that, you write it once, build a jar

373
00:23:00,706 --> 00:23:03,702
file, put it on a cluster, it's ready to go.

374
00:23:03,836 --> 00:23:06,886
And I've got links to some of mine that you can download and use in

375
00:23:06,908 --> 00:23:10,758
your system. So that was Iot data.

376
00:23:10,924 --> 00:23:14,218
That's a great type of data. I want to show you a different type of

377
00:23:14,224 --> 00:23:17,802
data since we've got some time here. This is weather data.

378
00:23:17,936 --> 00:23:22,094
If you're in the United States, there's an organization for

379
00:23:22,132 --> 00:23:25,294
the US government called the NOAA and they

380
00:23:25,332 --> 00:23:28,862
put out forecasts from different weather stations and

381
00:23:28,916 --> 00:23:31,854
current readings from weather stations all around the country.

382
00:23:32,052 --> 00:23:35,540
And amazingly enough, every 15 minutes,

383
00:23:36,150 --> 00:23:39,874
not really streaming, but pretty quick batch, they put

384
00:23:39,912 --> 00:23:43,426
out a zip file of every reading in

385
00:23:43,448 --> 00:23:47,206
the country. That's pretty awesome. So I'm going to download that

386
00:23:47,228 --> 00:23:50,486
zip file so I could run this once just to give you

387
00:23:50,508 --> 00:23:54,882
an idea. Get that flowing. Already have it. I'll uncompress

388
00:23:54,946 --> 00:23:58,742
that zip file, pull out all the individual files

389
00:23:58,806 --> 00:24:02,454
in there and then route them. Make sure that they're an airport.

390
00:24:02,582 --> 00:24:06,042
There's some that aren't airports for my customers.

391
00:24:06,176 --> 00:24:09,942
They care about weather conditions around airports.

392
00:24:10,086 --> 00:24:13,520
That's really the main parts of the US.

393
00:24:13,970 --> 00:24:17,358
So I have all these XML files coming in.

394
00:24:17,444 --> 00:24:21,262
I want to convert them to JSON and I'll run a little query on there,

395
00:24:21,396 --> 00:24:24,702
make sure they have location data. I don't want junk coming out.

396
00:24:24,836 --> 00:24:28,078
So at the end result of this is a whole bunch

397
00:24:28,094 --> 00:24:31,406
of JSON, very easy to read. Could have kept

398
00:24:31,438 --> 00:24:34,738
it in XML. I don't really like XML. I'm going to get rid of

399
00:24:34,744 --> 00:24:38,534
that as soon as possible. So here we take a look.

400
00:24:38,652 --> 00:24:41,670
I could see all that provenance and lineage data.

401
00:24:41,820 --> 00:24:45,640
Telling me what server it's run on gives it a unique id,

402
00:24:46,010 --> 00:24:49,530
what's the size, all the different attributes,

403
00:24:50,110 --> 00:24:54,038
where it ran, what type it is. Here's that flink.

404
00:24:54,214 --> 00:24:58,170
So that's the airport ko five. It's an XML

405
00:24:58,670 --> 00:25:02,606
all kinds of metadata, how it downloaded that from

406
00:25:02,628 --> 00:25:05,678
the website and then the actual content.

407
00:25:05,844 --> 00:25:09,742
And I could see, oh yeah, here it is. There's all that JSON data.

408
00:25:09,876 --> 00:25:13,214
That was XML data before.

409
00:25:13,412 --> 00:25:16,594
Now it's JSON data. So I took that

410
00:25:16,632 --> 00:25:20,686
data in, parsed it apart, I put it in Kafka.

411
00:25:20,878 --> 00:25:25,006
And we could show you that in a minute because I wanted to distribute

412
00:25:25,038 --> 00:25:28,834
it. I could usually put it in pulsar. This ones I put in Kafka.

413
00:25:28,962 --> 00:25:32,550
Sometimes I'll put it in jms. Lots of different messaging options.

414
00:25:32,700 --> 00:25:36,230
Probably want to put that in Pulsar is probably your best

415
00:25:36,300 --> 00:25:39,766
option there. But I could do that from, I could do that from any of

416
00:25:39,788 --> 00:25:43,382
those different clients out there. So I'm consuming those messages

417
00:25:43,446 --> 00:25:46,618
back. And let's show you that whole thing we talked about,

418
00:25:46,704 --> 00:25:50,206
stream to lake. This is the stream coming from Pulsar or

419
00:25:50,228 --> 00:25:53,866
Kafka. Here is my lake.

420
00:25:54,058 --> 00:25:57,886
I'm creating Orc files, which is a

421
00:25:57,908 --> 00:26:01,518
type of file that's used by Hive. And I'm putting that in

422
00:26:01,524 --> 00:26:04,910
a directory on s three automatically.

423
00:26:05,270 --> 00:26:08,674
Don't have to do anything. This has, again, something that reads those

424
00:26:08,712 --> 00:26:12,222
JSON files, writes them there. I do the same with parquet,

425
00:26:12,366 --> 00:26:16,066
same with Kudu. That's as easy as it is

426
00:26:16,168 --> 00:26:19,686
to take that stream and get that into my data lake as fast as I

427
00:26:19,708 --> 00:26:23,442
need it to be. And that same data is also flowing

428
00:26:23,506 --> 00:26:27,094
through my messaging system so that I

429
00:26:27,132 --> 00:26:30,650
can write some more advanced analytics on here.

430
00:26:30,800 --> 00:26:34,314
So before I send all that data out, I run some

431
00:26:34,352 --> 00:26:38,234
validation on here. I check it against the schema that I have

432
00:26:38,352 --> 00:26:42,042
to make sure that nothing's weird, because sometimes they give us bad

433
00:26:42,096 --> 00:26:45,994
data, government data, you don't always get good data. I don't want that broken

434
00:26:46,042 --> 00:26:49,566
data. I could store it, I could put it in

435
00:26:49,588 --> 00:26:53,246
a directory, maybe. It's usually junk. That's up to you.

436
00:26:53,268 --> 00:26:56,466
If you see value in it, maybe you could data mine that later. And then

437
00:26:56,488 --> 00:27:00,642
I just put it into messaging system to be doing

438
00:27:00,696 --> 00:27:04,834
some more queries. Now, I mentioned I had some custom

439
00:27:04,952 --> 00:27:08,502
processors to do deep learning. This is one right

440
00:27:08,556 --> 00:27:12,440
here. This is as hard as it is for you. You pick your data set.

441
00:27:12,810 --> 00:27:16,982
There's a couple of parameters there. This is a Resnet 50 for

442
00:27:17,036 --> 00:27:21,946
doing this. And here I'm just running a couple at a time just

443
00:27:21,968 --> 00:27:25,466
to show you the results. And if

444
00:27:25,488 --> 00:27:29,100
we take a look here, you can see here by the name

445
00:27:29,550 --> 00:27:33,434
that these are images. Nifi works on

446
00:27:33,472 --> 00:27:37,614
images. See 700K versus those little files we had before.

447
00:27:37,812 --> 00:27:41,226
And inside the metadata, I put the results

448
00:27:41,258 --> 00:27:44,554
of my deep learning classification. Here's a bounding

449
00:27:44,602 --> 00:27:47,954
box that I could draw around. What I found in the

450
00:27:47,992 --> 00:27:51,330
image, which you see here, is a person.

451
00:27:51,480 --> 00:27:55,206
I could have up to five, I could do more, but I limit it to

452
00:27:55,228 --> 00:27:59,234
five because gets a little hectic for my processing

453
00:27:59,282 --> 00:28:02,840
here. So I found results. It was a person.

454
00:28:04,650 --> 00:28:08,262
There is the images, height, min, max, those sort of things.

455
00:28:08,316 --> 00:28:10,780
What's the probability that it's actually a person?

456
00:28:11,310 --> 00:28:14,970
They have pretty good confidence there. So let's see what that image actually

457
00:28:15,040 --> 00:28:18,826
is. It's the side of my head. I'm a person. I'm very happy.

458
00:28:18,928 --> 00:28:22,266
Sometimes I'm not a person. Today deep learning says I'm

459
00:28:22,298 --> 00:28:24,910
a person. Everyone rejoice.

460
00:28:25,810 --> 00:28:29,550
Yeah. AI has not taken over yet because sometimes I'm not a person.

461
00:28:29,620 --> 00:28:31,920
Sometimes the cat's labeled a person.

462
00:28:32,850 --> 00:28:35,922
You never know what you're getting. So we have all those messages here.

463
00:28:36,056 --> 00:28:39,218
I've got over 25,000 in there already

464
00:28:39,384 --> 00:28:42,974
and these are just going through my message queue

465
00:28:43,102 --> 00:28:46,638
and I'm going to read them with Flink SQl

466
00:28:46,734 --> 00:28:50,018
and do some different analytics on them. That IoT

467
00:28:50,114 --> 00:28:53,446
stream and this weather stream, pretty straightforward, but gives

468
00:28:53,468 --> 00:28:56,614
you an idea what you could do with different data as

469
00:28:56,652 --> 00:29:00,250
it's coming in. You watch it over time.

470
00:29:00,320 --> 00:29:04,218
Here's that IoT sensor data. Got a lot of data has

471
00:29:04,224 --> 00:29:07,958
gone through there. Same with the weather. So this is how I'm

472
00:29:07,974 --> 00:29:11,126
running my Flink SQL. There's lots of different consoles

473
00:29:11,158 --> 00:29:14,414
out there. Stream native has one, Verberica has one,

474
00:29:14,532 --> 00:29:17,738
Cloudera has one. There's one with Apache, Flink.

475
00:29:17,914 --> 00:29:21,758
That one's a command line one. Whatever. You're writing lots of

476
00:29:21,764 --> 00:29:25,370
different ways to run Flink SQl and it's very scalable

477
00:29:25,450 --> 00:29:28,802
now. You could also wrap it in your own Java code if you want to

478
00:29:28,856 --> 00:29:32,002
manage the deployment yourself. Maybe you're doing it all open

479
00:29:32,056 --> 00:29:35,266
source and you don't have advanced environment to do

480
00:29:35,288 --> 00:29:38,626
that. Here I'm showing some of the results

481
00:29:38,658 --> 00:29:42,086
of my continuous SQL. So when I'm building this query, I could

482
00:29:42,108 --> 00:29:46,226
see the results. And this one, if you notice, if you've worked with SQl

483
00:29:46,258 --> 00:29:50,006
before, looks pretty familiar. I'm grabbing a location.

484
00:29:50,118 --> 00:29:53,754
This is wherever that airport was that they

485
00:29:53,792 --> 00:29:58,310
took the weather. And I want the max temperature in fahrenheit,

486
00:29:58,390 --> 00:30:01,926
average temperature, minimum temperature, and I'm just displaying

487
00:30:01,958 --> 00:30:05,686
them here. This is over a short period

488
00:30:05,718 --> 00:30:09,546
of time. We can set up windows of time with these streaming systems.

489
00:30:09,578 --> 00:30:13,506
So maybe I look at all the forecasts in the last 6 hours,

490
00:30:13,688 --> 00:30:17,634
take the minute max, give you ideas there because remember this

491
00:30:17,672 --> 00:30:21,790
is not in my final data store. This is in stream.

492
00:30:21,870 --> 00:30:25,566
While this happens, another record shows up. It's added

493
00:30:25,598 --> 00:30:29,382
to this sql, this is continuously running and

494
00:30:29,436 --> 00:30:33,206
it just keeps going. We got another one here that

495
00:30:33,228 --> 00:30:37,094
I'm not doing any aggregates, I'm just looking at every record where

496
00:30:37,132 --> 00:30:40,102
the location is not null. I mean we did the validation,

497
00:30:40,166 --> 00:30:45,126
but sometimes something doesn't show up. Part of these reads

498
00:30:45,158 --> 00:30:48,614
at some of these weather stations are done manually. Someone's typing

499
00:30:48,662 --> 00:30:52,198
in the weather forecast or the

500
00:30:52,224 --> 00:30:56,382
current conditions. So there's sometimes a little bit of human

501
00:30:56,436 --> 00:31:00,302
error gets in there. But you can see some of the fields here.

502
00:31:00,436 --> 00:31:04,218
And nice thing is we wrap this in a materialized

503
00:31:04,314 --> 00:31:07,602
view so that now I have a rest endpoint that people

504
00:31:07,656 --> 00:31:11,042
can query and this is what it looks like. You get a

505
00:31:11,176 --> 00:31:14,466
JSON array of all that weather data. I could

506
00:31:14,488 --> 00:31:17,990
pull that into Jupyter notebook, pull that into an application,

507
00:31:18,140 --> 00:31:21,506
do what you need to do there. I've got another Flink SQl

508
00:31:21,538 --> 00:31:24,854
here that's joining together two streams. These are

509
00:31:24,892 --> 00:31:28,150
two different topics. I've got one for energy data,

510
00:31:28,300 --> 00:31:31,754
one for my sensor data. I join them together.

511
00:31:31,872 --> 00:31:35,334
This could have been a full outer join, left outer join,

512
00:31:35,382 --> 00:31:39,866
right outer join. Again, if you've done any ANSI SQL 92,

513
00:31:40,048 --> 00:31:42,822
Flink SQl is going to be pretty familiar.

514
00:31:42,966 --> 00:31:46,394
Uses Apache Calcite, which is used in a ton of open source

515
00:31:46,442 --> 00:31:49,754
projects like Phoenix. So you get used to this SQL

516
00:31:49,802 --> 00:31:53,282
once and you pretty much get the syntax. There's some

517
00:31:53,336 --> 00:31:57,502
extra things for doing really interesting complex

518
00:31:57,566 --> 00:32:01,406
event processing and windowing, but it's pretty much SQL.

519
00:32:01,518 --> 00:32:04,706
You don't have to write any Jav or any custom apps here,

520
00:32:04,808 --> 00:32:08,390
but if you've got operators and functions running within

521
00:32:08,460 --> 00:32:12,246
your queuing system, those can be executed before they get here

522
00:32:12,268 --> 00:32:16,130
or after. Gives you some power. Again, another materialized

523
00:32:16,210 --> 00:32:19,802
view here, so that people who don't have the

524
00:32:19,856 --> 00:32:23,450
libraries can just call this rest endpoint,

525
00:32:23,790 --> 00:32:26,902
get a bunch of JSON

526
00:32:26,966 --> 00:32:30,742
and process it as they want to. And I've got a whole bunch

527
00:32:30,806 --> 00:32:34,030
of different jobs running here. I've got

528
00:32:34,180 --> 00:32:38,362
four different flink applications that I could see in the dashboard

529
00:32:38,506 --> 00:32:41,742
and I could dive into them and see what's going on, see how many

530
00:32:41,796 --> 00:32:45,134
records, different things going on.

531
00:32:45,172 --> 00:32:48,994
This one's interesting because you've got two different source tables and a

532
00:32:49,032 --> 00:32:52,898
join, and you can see the number of records processing through there.

533
00:32:53,064 --> 00:32:56,406
Pretty basic, but gives you the idea sometimes you want to

534
00:32:56,428 --> 00:32:59,378
see data stored in the cloud. We said lake.

535
00:32:59,474 --> 00:33:02,946
Well, here's my lake. This is a table

536
00:33:03,138 --> 00:33:06,850
on top of Amazon

537
00:33:06,930 --> 00:33:10,210
s three. And it looks like a

538
00:33:10,220 --> 00:33:14,102
regular table. I could see the location and the details,

539
00:33:14,166 --> 00:33:17,546
where it's stored. And it makes it pretty easy for

540
00:33:17,568 --> 00:33:21,478
me to do what I need to do here. And it just acts

541
00:33:21,494 --> 00:33:25,182
like a database for me. And I have all that permanent data,

542
00:33:25,236 --> 00:33:28,654
so I have all the readings that have ever happened here. I could

543
00:33:28,692 --> 00:33:32,334
store them, do whatever I need to do with that. Same thing

544
00:33:32,372 --> 00:33:35,986
with the sensor readings. Pretty straightforward, regardless of

545
00:33:36,008 --> 00:33:39,458
where I want to store that, whatever my lake is, like I said, it could

546
00:33:39,464 --> 00:33:42,510
be cloud era, it could be Amazon, Microsoft,

547
00:33:42,590 --> 00:33:46,062
Redshift, snowflake, whatever's that next

548
00:33:46,136 --> 00:33:48,790
dremio, whatever's your next data lake,

549
00:33:49,210 --> 00:33:52,758
I could put it there. I create a little dashboard on it.

550
00:33:52,844 --> 00:33:57,686
You can see there's a lot of different readings across the country.

551
00:33:57,868 --> 00:34:01,646
These are some that are close to me. This is a local airport

552
00:34:01,698 --> 00:34:05,654
by me, and I could see all the data there and download

553
00:34:05,702 --> 00:34:09,386
it if I wanted to. And that has things like Latin long, so I could

554
00:34:09,408 --> 00:34:12,830
put it on a map here. And if you look, there's a lot of different

555
00:34:12,900 --> 00:34:16,990
airports where they're doing weather data in the United States.

556
00:34:17,140 --> 00:34:20,542
You zoom out enough and it's just a nice,

557
00:34:20,596 --> 00:34:24,206
pretty design there because there's thousands of records there.

558
00:34:24,308 --> 00:34:28,046
Makes it very easy. Something else I can do is I

559
00:34:28,068 --> 00:34:31,886
can send real time alerts to slack. This is great for DevOps,

560
00:34:31,918 --> 00:34:35,266
but this also may be helpful for your data scientists and

561
00:34:35,288 --> 00:34:38,966
analysts to see. Okay, there's some new data coming in. Maybe we do

562
00:34:38,988 --> 00:34:42,566
a sampling of it. Someone says, okay, there hasn't been data in a

563
00:34:42,588 --> 00:34:46,166
while. Now I see temperatures at can airport. Maybe that gives

564
00:34:46,188 --> 00:34:50,298
me an idea for what I can do next. Lots of options there,

565
00:34:50,464 --> 00:34:54,134
pretty straightforward, I think now we're at the end. Hopefully there's

566
00:34:54,182 --> 00:34:57,674
time during the real time event for questions.

567
00:34:57,792 --> 00:35:00,926
If there isn't, please reach out to me.

568
00:35:01,028 --> 00:35:04,494
I've got all my contact information here. Whether you

569
00:35:04,532 --> 00:35:08,400
contact me at pazdev on Twitter, see me on my website,

570
00:35:10,370 --> 00:35:13,886
open a pull request in know however you

571
00:35:13,908 --> 00:35:17,070
want to contact me. I'm always interested in talking

572
00:35:17,140 --> 00:35:20,746
about streaming and getting data to a data lake.

573
00:35:20,858 --> 00:35:24,734
Really easy, even if it's for machine learning or deep learning or whatever

574
00:35:24,772 --> 00:35:28,262
you need it to be for. Straightforward thing there.

575
00:35:28,396 --> 00:35:31,960
Thanks for coming to my talk. Hope you learned something.

576
00:35:32,650 --> 00:35:35,862
If you're looking to learn a little more,

577
00:35:35,996 --> 00:35:38,658
definitely follow me. We have deeper dives.

578
00:35:38,834 --> 00:35:42,406
We could do whole day events. So reach out.

579
00:35:42,508 --> 00:35:43,060
Thanks a lot.

