1
00:00:27,970 --> 00:00:32,066
Hi, I'm Piyush Verma. I am CTO

2
00:00:32,098 --> 00:00:35,894
and co founder at Lastio IO. The talk is about

3
00:00:36,012 --> 00:00:40,066
an SRE diary where as steps,

4
00:00:40,178 --> 00:00:43,958
we are trained to be on pager all the time. I mean, I've held a

5
00:00:43,964 --> 00:00:47,606
pager for almost a decade now, in fact, earlier than when it used to be

6
00:00:47,628 --> 00:00:51,422
called an SRE as well. But this talk is slightly the

7
00:00:51,476 --> 00:00:55,006
opposite end of it, where we talk about all the times when

8
00:00:55,028 --> 00:00:59,386
the pager did not ring. And that's

9
00:00:59,418 --> 00:01:03,454
where I say it went. Make a sound when it breaks. Why do

10
00:01:03,492 --> 00:01:06,674
I say that? If you look at these photos, most of these

11
00:01:06,712 --> 00:01:10,558
things, the upper half, right, they would make a sound when it breaks.

12
00:01:10,654 --> 00:01:14,386
Starting from a sonic bubble of an aircraft to a balloon, to a

13
00:01:14,408 --> 00:01:18,150
heart, and then the ram and the bios.

14
00:01:18,730 --> 00:01:22,450
Not sure if many of us would associate with a ram breaking

15
00:01:22,530 --> 00:01:25,846
and making a sound, because I haven't seen modern computers do

16
00:01:25,868 --> 00:01:29,186
that. I've not even seen a BIOS screen in ages.

17
00:01:29,298 --> 00:01:33,146
So that could also be a case were people don't associate with

18
00:01:33,168 --> 00:01:36,522
it. But from an era were, which was

19
00:01:36,576 --> 00:01:39,946
prior to this cloud computing, when these things would break, they would make a

20
00:01:39,968 --> 00:01:43,680
sound. So it was very easy to diagnose that something has gone wrong.

21
00:01:44,290 --> 00:01:46,670
Probably not the case anymore.

22
00:01:48,130 --> 00:01:51,514
Most of the failures, the flavors,

23
00:01:51,562 --> 00:01:54,594
come in form of software, human network, process and

24
00:01:54,632 --> 00:01:58,382
culture. I'm going to talk about everything but a software failures.

25
00:01:58,526 --> 00:02:02,386
A software failure is the easiest to identify. There will be a

26
00:02:02,408 --> 00:02:06,290
500 error somewhere which will be caught using

27
00:02:06,360 --> 00:02:09,426
some sort of regular expression, some sort of forwarding rule.

28
00:02:09,538 --> 00:02:12,902
It will reach a page of duty, a victor ops or some other

29
00:02:12,956 --> 00:02:16,646
tool of this manner, an incident management system, which has been set up.

30
00:02:16,748 --> 00:02:20,714
And that would ring your phone, ring your pager, ring your email or

31
00:02:20,752 --> 00:02:24,442
something like that, which works pretty well. All the other

32
00:02:24,576 --> 00:02:27,974
failures, the flavors of it starting from human, a network,

33
00:02:28,022 --> 00:02:32,090
a process or a culture, are the ones which make really

34
00:02:32,240 --> 00:02:35,546
interesting rcas. A software

35
00:02:35,578 --> 00:02:39,054
failure is very easy to identify. You find something in a log line.

36
00:02:39,252 --> 00:02:43,102
A culture failure, on the other hand, is something that you identify pretty

37
00:02:43,156 --> 00:02:46,850
late. And the good part about is, as you go from top to bottom,

38
00:02:46,920 --> 00:02:50,274
you realize that the error that actually shows up, or a

39
00:02:50,312 --> 00:02:54,014
failure that eventually shows up is very latent.

40
00:02:54,142 --> 00:02:58,254
A culture failure is the slowest

41
00:02:58,302 --> 00:03:01,282
to identify. A software failure is the easiest to identify.

42
00:03:01,346 --> 00:03:04,946
A human failure, yet slower than a software failure, but way faster than a culture

43
00:03:04,978 --> 00:03:08,502
failure. So my talk, I'm going to split this into a few

44
00:03:08,556 --> 00:03:11,914
parts where I speak of outages, a few

45
00:03:11,952 --> 00:03:14,922
of them, some of them may have heard this talk,

46
00:03:14,976 --> 00:03:18,378
but this is slightly different variation of my earlier talk.

47
00:03:18,544 --> 00:03:22,494
And these outages, they traverse from very

48
00:03:22,532 --> 00:03:25,178
simple scenarios which otherwise,

49
00:03:25,274 --> 00:03:29,134
as sres, we would bypass thinking, hey, I could

50
00:03:29,172 --> 00:03:32,366
use x took for it, I could use y took for it, I could use

51
00:03:32,388 --> 00:03:35,810
z tool for it. But they do not talk about the real

52
00:03:35,880 --> 00:03:40,030
underlying cause of a failure. I want to dissect

53
00:03:40,110 --> 00:03:43,762
these outages and go one level further to understand

54
00:03:43,896 --> 00:03:48,146
why did it fail in first place. Because as steps,

55
00:03:48,338 --> 00:03:50,966
we have three principles that we follow.

56
00:03:51,148 --> 00:03:55,158
The very first thing is we rush towards a fire. We say,

57
00:03:55,324 --> 00:03:58,706
we got to bring this down as we're going to mitigate

58
00:03:58,738 --> 00:04:01,706
this fixes as fast as possible. That's the first one, obviously.

59
00:04:01,888 --> 00:04:05,270
The second question, a very important one that we have to ask ourselves

60
00:04:05,350 --> 00:04:08,042
is where else is this breaking cause?

61
00:04:08,096 --> 00:04:12,458
Chances are, our software, our situation, our framework

62
00:04:12,554 --> 00:04:16,000
is a byproduct of practices and cultures that we follow.

63
00:04:16,370 --> 00:04:19,630
So if something is failing at some place,

64
00:04:19,780 --> 00:04:23,566
there's a very high likelihood that it fails in another place as well.

65
00:04:23,748 --> 00:04:27,774
That's the second most question we have to ask because the intent

66
00:04:27,822 --> 00:04:30,914
is to prevent another fire from happening. And the

67
00:04:30,952 --> 00:04:34,434
third ones, the real important one that we mostly fail to

68
00:04:34,472 --> 00:04:37,894
answer is, how do I prevent this from happening

69
00:04:38,012 --> 00:04:41,240
one more time? Unlike product,

70
00:04:41,770 --> 00:04:45,654
reliability cannot be improved as one feature at

71
00:04:45,692 --> 00:04:49,546
a time. It cannot be done as one single failure at a

72
00:04:49,568 --> 00:04:53,082
time. Failure happens, I fix something, then another

73
00:04:53,136 --> 00:04:56,438
failure happens, I fix something. Sadly, actionability doesn't

74
00:04:56,454 --> 00:04:59,610
work that way because customers do not give us that many change.

75
00:05:01,410 --> 00:05:04,960
The first outage that I want to speak about is

76
00:05:06,210 --> 00:05:10,400
a customer service is reporting login to be down.

77
00:05:10,930 --> 00:05:14,190
We check datadog, paper trail, neuralink cloud, watch.

78
00:05:14,260 --> 00:05:17,810
Everything looks perfectly okay. Most of these

79
00:05:17,880 --> 00:05:20,930
trying to hint towards the fact that breaking looks okay,

80
00:05:21,000 --> 00:05:24,386
servers look okay, load looks okay, errors look okay. But we

81
00:05:24,408 --> 00:05:28,018
can't figure out why login is down. Now here's

82
00:05:28,034 --> 00:05:31,266
an interesting fact. Login, if it's

83
00:05:31,298 --> 00:05:34,934
down, means that it is not accessible. So requests that

84
00:05:34,972 --> 00:05:39,026
doesn't reach us are inside out monitoring

85
00:05:39,058 --> 00:05:42,642
systems which sit in form of a setting of a sophisticated

86
00:05:42,706 --> 00:05:46,166
Prometheus chain, a datadog chain, a new relic chain is not going to buzz

87
00:05:46,198 --> 00:05:49,978
as well because something that doesn't hit you, it doesn't create an alarm on a

88
00:05:49,984 --> 00:05:52,320
failures. You don't know what you don't know.

89
00:05:53,730 --> 00:05:57,598
So what works, what gives? And meanwhile, on Twitter, there's a lot

90
00:05:57,604 --> 00:06:01,598
of sound being made. I mean, surprisingly that I've really seen that

91
00:06:01,684 --> 00:06:05,134
my automated system alerts are usually slower

92
00:06:05,182 --> 00:06:08,946
than people manually identifying something is broken. So on

93
00:06:08,968 --> 00:06:12,562
Twitter, there's a lot of sound happening. What was the real

94
00:06:12,616 --> 00:06:16,338
cause? After a bit of debugging, we identified

95
00:06:16,434 --> 00:06:20,386
that a DevOps person had manually altered

96
00:06:20,418 --> 00:06:24,166
a security group, accidentally deleting the 443 as

97
00:06:24,188 --> 00:06:28,054
well. Quite possible to happen because ever since the

98
00:06:28,092 --> 00:06:31,834
COVID lockdown started, people started working from home. And when they start

99
00:06:31,872 --> 00:06:35,514
working from home, you are always whitelisting some IP address or the other.

100
00:06:35,632 --> 00:06:39,130
And those cloud security group operation tabs usually

101
00:06:39,200 --> 00:06:43,062
allows you to make just one single press which you can accidentally

102
00:06:43,126 --> 00:06:46,558
end up deleting a rule that you should not have, which results in a

103
00:06:46,564 --> 00:06:49,918
failure. What was the real root cause here?

104
00:06:50,004 --> 00:06:53,486
How do we prevent this from happening again? Well, the obvious second question that

105
00:06:53,508 --> 00:06:57,362
we asked was, okay, where else is this happening? We may have deleted other

106
00:06:57,416 --> 00:07:01,054
rules as well, but what's the real root

107
00:07:01,102 --> 00:07:04,482
cause? If we have to dissect this, it's not

108
00:07:04,536 --> 00:07:08,066
setting up another tool. It's certainly not setting up an

109
00:07:08,088 --> 00:07:10,926
audit trail policy because, well,

110
00:07:10,968 --> 00:07:13,458
that could be one of the changes as well, that you set up an audit

111
00:07:13,474 --> 00:07:17,126
trail policy. But then somebody may miss to have a look at it as

112
00:07:17,148 --> 00:07:21,158
well. I mean, the answer to solve a human problem cannot be another human

113
00:07:21,244 --> 00:07:24,362
reviewing it every time. Because if one human has missed it, another human

114
00:07:24,416 --> 00:07:27,498
is going to miss something as well. Then what is those real root cause?

115
00:07:27,584 --> 00:07:31,006
The real root cause here is that we do

116
00:07:31,028 --> 00:07:34,410
not have this culture, or we allow exceptions

117
00:07:34,490 --> 00:07:38,270
of things to be edited manually. Now these cloud

118
00:07:38,340 --> 00:07:41,742
states, if they were being maintained religiously using a simple

119
00:07:41,796 --> 00:07:45,934
terraform script or a pilumi script, it is highly likely

120
00:07:45,982 --> 00:07:49,794
that a change would have been recorded somewhere and a rollback was

121
00:07:49,832 --> 00:07:53,982
also possible. But because that wasn't the case, and we had these exceptions

122
00:07:54,046 --> 00:07:57,682
of being able to manually go in and change something, even if it was.

123
00:07:57,736 --> 00:08:00,998
Hey, I just have one small change to make. Why do I really need to

124
00:08:01,004 --> 00:08:05,014
go via the entire terraform route? Because it takes longer. Because every

125
00:08:05,052 --> 00:08:08,534
time I have to run an apply operation, it takes around ten minutes to just

126
00:08:08,572 --> 00:08:12,074
sync all the data providers and fetch the real state. And then

127
00:08:12,112 --> 00:08:15,450
tells me that here's a diff. So it comes in the way of my

128
00:08:15,520 --> 00:08:19,418
fast application of a change. Now the side

129
00:08:19,504 --> 00:08:22,538
effect of that is that every once in a while were going to make a

130
00:08:22,544 --> 00:08:25,994
mistake like this, which ends up resulting in a higher outage.

131
00:08:26,042 --> 00:08:30,506
In those particular case, it wasn't just that the slash login is down. Imagine deleting

132
00:08:30,538 --> 00:08:34,066
a 443 rule from an inbound of a security group of

133
00:08:34,088 --> 00:08:37,874
a load balancer. It's not just login is down, everything was down.

134
00:08:37,992 --> 00:08:41,554
Just that login was reported at

135
00:08:41,592 --> 00:08:45,266
that point of time was one of the endpoints because one of the

136
00:08:45,288 --> 00:08:49,186
customers had complained about it. So real impact was larger

137
00:08:49,298 --> 00:08:52,790
CTO, a degree where we can't even tell how much the outage was,

138
00:08:52,940 --> 00:08:56,518
unless now we start auditing the request logs from

139
00:08:56,684 --> 00:09:00,470
those client side agents, et cetera, as well, which itself

140
00:09:00,540 --> 00:09:03,866
at times are missing. So the real impact, we don't even know how

141
00:09:03,888 --> 00:09:07,514
much business you lose. We don't know. We just don't know anything about

142
00:09:07,552 --> 00:09:11,086
it. So the only way to overcome the situation is

143
00:09:11,108 --> 00:09:14,800
to built a practice where we say that no matter what happens,

144
00:09:15,170 --> 00:09:19,150
we are not going to allow ourselves to make these manual short circuits.

145
00:09:22,210 --> 00:09:25,426
This still looks pretty simple and easy. I want to

146
00:09:25,448 --> 00:09:28,340
cover another outage, which is outage number two.

147
00:09:29,830 --> 00:09:33,090
This is when we were dealing with

148
00:09:33,160 --> 00:09:36,482
data centers. There weren't really these cloud providers here.

149
00:09:36,536 --> 00:09:39,874
Not that it changes the problem definition, but it's an important addition

150
00:09:39,922 --> 00:09:43,062
to the scope of the problem. Around 25

151
00:09:43,116 --> 00:09:46,934
hours, just before a new country launch that we're supposed to do, payer duty goes

152
00:09:46,972 --> 00:09:50,474
off. We check in our servers and we see that

153
00:09:50,512 --> 00:09:53,882
there are certain 500 which do report on

154
00:09:53,936 --> 00:09:57,450
elasticsearch. Now, because we are in a tight data center

155
00:09:57,520 --> 00:10:02,042
environment, default log

156
00:10:02,096 --> 00:10:05,674
analysis isn't really a luxury cause you go over a firewall,

157
00:10:05,722 --> 00:10:09,054
hop, et cetera. So what we do is one of us just decides to actually

158
00:10:09,092 --> 00:10:12,238
start copying the logs. Comes in really handy a few minutes later in

159
00:10:12,244 --> 00:10:16,002
the script, a few minutes later, what happens is that 500

160
00:10:16,056 --> 00:10:19,518
just stop coming in. Pager duty is auto resolved.

161
00:10:19,694 --> 00:10:23,314
Everything looks good. Though to our curious mind,

162
00:10:23,352 --> 00:10:27,174
we are still wondering that why did something stop working? And how did that

163
00:10:27,212 --> 00:10:31,094
thing auto resolve? So we are still debugging into it.

164
00:10:31,212 --> 00:10:34,982
Five minutes later, pager duty goes off again and again.

165
00:10:35,036 --> 00:10:38,246
The public API is unreachable. We do

166
00:10:38,268 --> 00:10:41,674
get our pingdom alerts, everything that we had set

167
00:10:41,712 --> 00:10:45,354
up alerts go in. Looks like something fishy because

168
00:10:45,392 --> 00:10:48,842
this is clearly before a public launch that is happening. So we

169
00:10:48,896 --> 00:10:52,138
start checking rundeck, because what is the most important question that

170
00:10:52,144 --> 00:10:56,014
we ask when something fails? The very most important question that I have asked when

171
00:10:56,052 --> 00:10:58,880
something stops working is that, hey, who change what?

172
00:10:59,330 --> 00:11:02,902
Because a system in its stable stationary

173
00:11:02,986 --> 00:11:06,754
state doesn't really break that often. It's only when

174
00:11:06,792 --> 00:11:10,050
it is subject to a change is when it starts breaking.

175
00:11:10,390 --> 00:11:15,234
The change could be for a

176
00:11:15,272 --> 00:11:19,270
time of a change, could be something

177
00:11:19,340 --> 00:11:23,046
was altered, a change could be something

178
00:11:23,148 --> 00:11:26,838
was a new traffic source was added. But it's one of these change

179
00:11:26,924 --> 00:11:30,646
that actually break a system. We also ask ourselves this

180
00:11:30,668 --> 00:11:34,250
important question. Hey, is my firewall down? Why is firewall important

181
00:11:34,320 --> 00:11:37,578
here? Well, because it's a data center deployment. So one of the changes could have

182
00:11:37,584 --> 00:11:40,666
happened on a firewall as well. The inbound connection could have gone up,

183
00:11:40,688 --> 00:11:44,362
but that wasn't down either. Okay, standard checklist,

184
00:11:44,426 --> 00:11:47,934
check Grafana. Nothing wrong. Check stack driver because we were

185
00:11:47,972 --> 00:11:51,130
still able to send some data there. Nothing wrong. Check all servers.

186
00:11:51,210 --> 00:11:55,250
Nothing wrong. Check load. Nothing wrong. Check docker.

187
00:11:55,990 --> 00:11:59,726
Has anything restarted? Nothing restarted. Check APM.

188
00:11:59,918 --> 00:12:03,186
Everything looks okay now. While we

189
00:12:03,208 --> 00:12:07,138
had copied the logs, we start realizing that some of the database

190
00:12:07,234 --> 00:12:10,566
errors, there was only some database errors on some

191
00:12:10,588 --> 00:12:16,086
of the requests, not everything. So which looked really suspicious if

192
00:12:16,108 --> 00:12:19,062
we take a look at it. We had all the tools that we wanted.

193
00:12:19,116 --> 00:12:22,010
We had elasticsearch available, we had Stackdriver available,

194
00:12:22,080 --> 00:12:24,858
we had sentry available, we had Prometheus available,

195
00:12:24,944 --> 00:12:28,378
we had steps available. One of the ways to solve a problem is that we

196
00:12:28,384 --> 00:12:31,998
throw more bodies at a problem. We had really all the

197
00:12:32,084 --> 00:12:35,758
sres that we wanted. We were team of ten people,

198
00:12:35,924 --> 00:12:39,358
but we couldn't find the problem. 20 hours later,

199
00:12:39,524 --> 00:12:43,454
after a lot of toil, we found that the mount

200
00:12:43,502 --> 00:12:46,654
command hadn't run on one of the DB

201
00:12:46,702 --> 00:12:49,780
shards. Now why did this happen?

202
00:12:50,230 --> 00:12:54,718
There were machines, there were data center machines that we actually provisioned.

203
00:12:54,894 --> 00:12:58,680
And one of the machines earlier night was coming in from a fix.

204
00:12:59,210 --> 00:13:03,170
We had an issue where a mount command

205
00:13:03,250 --> 00:13:07,378
used to run a temporary file system and it wasn't

206
00:13:07,474 --> 00:13:10,978
really persisting it on a reboot. We applied a

207
00:13:11,004 --> 00:13:14,662
fix across ten machines, but the machine that was broken

208
00:13:14,726 --> 00:13:17,706
at that point of time did not have that ansible check on it.

209
00:13:17,728 --> 00:13:21,366
That did not run ansible on it. Just before the country launch,

210
00:13:21,398 --> 00:13:24,734
we decided okay, let's insert this machine and we will

211
00:13:24,772 --> 00:13:27,838
have the machine available in case a load arrives as well.

212
00:13:27,924 --> 00:13:31,694
So when the machine went in, it did not have that particular

213
00:13:31,812 --> 00:13:34,770
fix, that just one fix that we had applied.

214
00:13:35,590 --> 00:13:38,420
So data goes onto a shard as well.

215
00:13:38,870 --> 00:13:42,514
The machine wasn't fixed properly, so it rebooted again.

216
00:13:42,632 --> 00:13:45,986
And when it rebooted, that data got failed. It was just

217
00:13:46,008 --> 00:13:49,302
a slice of data. So every time request would hit that

218
00:13:49,356 --> 00:13:53,510
slice of data, they would result in an error which would result in

219
00:13:53,580 --> 00:13:56,934
load raising of too many errors. Load balancer would cut

220
00:13:56,972 --> 00:14:00,570
it off. Send a pager duty breaks the traffic.

221
00:14:00,990 --> 00:14:04,090
Health comes in. Health obviously is not checking that data.

222
00:14:04,160 --> 00:14:07,980
So the machine would come back in circulation and this cycle goes on and on.

223
00:14:09,150 --> 00:14:12,240
Pretty interesting story, but what's the real root cause here?

224
00:14:12,770 --> 00:14:16,126
Those real root cause, if you understand,

225
00:14:16,308 --> 00:14:19,742
if you try to dissect this further in this particular case

226
00:14:19,796 --> 00:14:23,394
was not that we were not running any automation, we had

227
00:14:23,432 --> 00:14:27,330
state of the art ansible configuration.

228
00:14:27,830 --> 00:14:31,698
What we could have avoided this was, if I ask myself,

229
00:14:31,864 --> 00:14:35,380
could have I avoided this outage? Probably not.

230
00:14:35,750 --> 00:14:39,346
What did that outage teach me so that it doesn't happen the

231
00:14:39,368 --> 00:14:43,218
second time around? This is what is important. Most of the times you won't

232
00:14:43,234 --> 00:14:46,658
be able to avoid it or avert that outage from the first time because they're

233
00:14:46,674 --> 00:14:48,760
so unique in how they happen.

234
00:14:49,770 --> 00:14:52,874
It's going to happen. But how can we prevent this from

235
00:14:52,912 --> 00:14:56,678
happening again? When I say this, I don't mean this particular errors,

236
00:14:56,774 --> 00:15:00,154
this class of errors from happening again. And the

237
00:15:00,192 --> 00:15:04,142
only way to solve that is we realize a simple

238
00:15:04,196 --> 00:15:07,594
script could have done the job only if we had a startup

239
00:15:07,642 --> 00:15:11,006
script of an OS query which would just check the

240
00:15:11,028 --> 00:15:14,526
configuration drift of the machine. Probably we could have saved

241
00:15:14,558 --> 00:15:18,226
this. Now one of

242
00:15:18,248 --> 00:15:22,226
the big questions that we ask while something is failing is

243
00:15:22,328 --> 00:15:25,826
how was it working so far? This is

244
00:15:25,848 --> 00:15:29,830
one of the most interesting question. Then the next question is what else is breaking

245
00:15:30,650 --> 00:15:34,630
on the lines of these two outages? I want to cover another one, which is

246
00:15:34,700 --> 00:15:38,398
an outage number three. This outage

247
00:15:38,434 --> 00:15:42,374
number three is typically about a distributed

248
00:15:42,422 --> 00:15:46,710
lock which was going wrong in production.

249
00:15:46,870 --> 00:15:50,514
We used to have sort of a hosted and amounted

250
00:15:50,582 --> 00:15:54,494
terraform which would allow multiple jobs to run at the same time.

251
00:15:54,692 --> 00:15:58,206
And there was a lock in place.

252
00:15:58,388 --> 00:16:02,062
But despite the clock, multiple jobs would come in

253
00:16:02,196 --> 00:16:05,310
and the contention of a lock was really failing

254
00:16:05,470 --> 00:16:08,866
almost to a degree that it looked like this. There was a lock, but was

255
00:16:08,888 --> 00:16:12,674
pretty useless. So if I'm to

256
00:16:12,872 --> 00:16:17,026
define the ideal behavior of a lock, we were using ETCD for

257
00:16:17,128 --> 00:16:20,994
clustered clock maintenance, and the standard compare and swap

258
00:16:21,042 --> 00:16:24,386
algorithm is being used. How does those compare and swap algorithm

259
00:16:24,418 --> 00:16:27,606
works? If I just quickly walk through, right, I set in a

260
00:16:27,628 --> 00:16:30,902
value which is one. Using the previous value

261
00:16:30,956 --> 00:16:34,458
of one I set in a value of two. Everything works well.

262
00:16:34,624 --> 00:16:37,802
If I now try to put in a value of three, it will say that

263
00:16:37,856 --> 00:16:41,802
key already exists because it already exists. Or if I try to set

264
00:16:41,856 --> 00:16:45,306
a three on a previous value of two, it would say the compare operation failed.

265
00:16:45,338 --> 00:16:48,720
So it's a pretty standard API of compare and swap which works.

266
00:16:50,210 --> 00:16:52,320
But what was really happening here?

267
00:16:53,490 --> 00:16:58,370
We started with a default key which is stopped, and both

268
00:16:58,440 --> 00:17:02,146
processes are trying to set a key of started with those

269
00:17:02,168 --> 00:17:05,666
previous value of stopped. The behavior is that only one should

270
00:17:05,688 --> 00:17:09,414
win that previous value is stopped. I set a new value of

271
00:17:09,452 --> 00:17:12,440
started. Only one process should go through.

272
00:17:13,370 --> 00:17:17,334
But what's really happening, what's happening is that

273
00:17:17,452 --> 00:17:21,574
run a acquires a lock, and with a TTL,

274
00:17:21,702 --> 00:17:25,514
TTL is very important. Cause every time you have a lock based system,

275
00:17:25,632 --> 00:17:28,790
in absence of a time to live expiry,

276
00:17:28,950 --> 00:17:33,226
what ends up happening is that the process which locked

277
00:17:33,258 --> 00:17:37,214
or acquired the clock may die. So it's extremely important

278
00:17:37,412 --> 00:17:40,846
that the process which has set the

279
00:17:40,868 --> 00:17:44,574
lock also sets an expiry with it, so that

280
00:17:44,612 --> 00:17:47,954
in case the process dies, the lock is freed up and is available for others

281
00:17:47,992 --> 00:17:51,554
to use, which works well. Those only

282
00:17:51,592 --> 00:17:54,622
difference being that when a tries to update those status,

283
00:17:54,766 --> 00:17:58,446
we get keynote found, b tries to update those status

284
00:17:58,558 --> 00:18:02,274
again, then we get keynote found, which is very weird,

285
00:18:02,322 --> 00:18:06,006
because the process which had set the lock should be

286
00:18:06,028 --> 00:18:10,310
able to unset it and free it, or either

287
00:18:10,380 --> 00:18:14,486
case that both of them should be able to go ahead and find the contention.

288
00:18:14,518 --> 00:18:17,466
But this is very unique here, because both of the processes say that I've not

289
00:18:17,488 --> 00:18:20,220
found the key. Well,

290
00:18:20,830 --> 00:18:25,086
keynote found, b says keynot found. So this

291
00:18:25,108 --> 00:18:28,234
is where we all become full stack overflow developers.

292
00:18:28,362 --> 00:18:31,514
We start hunting our errors, we start hunting

293
00:18:31,562 --> 00:18:34,814
the diagnosis, and we land on a GitHub

294
00:18:34,862 --> 00:18:39,662
page, which is a very interesting one, where it mentions that ETCD

295
00:18:39,726 --> 00:18:43,774
has a problem, that ttls are expiring too soon, which is related

296
00:18:43,822 --> 00:18:47,286
to an ETCD election. It looks like a fairly technical and

297
00:18:47,308 --> 00:18:50,854
a sophisticated application, and we say, all right,

298
00:18:50,972 --> 00:18:54,680
those makes sense, and this must be it.

299
00:18:55,050 --> 00:18:58,678
So a GitHub ticket, open source

300
00:18:58,774 --> 00:19:01,914
on a system and fairly tells us that hey,

301
00:19:01,952 --> 00:19:05,162
eTCD is a problem. Our solution is,

302
00:19:05,296 --> 00:19:08,966
well, we have to replace eTCD with a console

303
00:19:09,078 --> 00:19:13,774
for reasons of a better API or a better it

304
00:19:13,812 --> 00:19:17,626
has better state maintenance, et cetera. And we almost convince

305
00:19:17,658 --> 00:19:20,880
ourselves that this is the right way to go. We make a change,

306
00:19:21,330 --> 00:19:24,814
it's a week long sprint. We spend a lot of time at it and effort

307
00:19:24,862 --> 00:19:28,050
at it and replacing it, and things go back to normal.

308
00:19:29,670 --> 00:19:31,620
Is that the real reason though?

309
00:19:32,710 --> 00:19:36,626
One of us decides that, look, if this was a

310
00:19:36,648 --> 00:19:40,374
reason, how was it working earlier? That's the most important question,

311
00:19:40,412 --> 00:19:44,406
right? That we ask, how else was it working earlier? We're not convinced that,

312
00:19:44,428 --> 00:19:47,618
look, this could really be the way to go about it. So we dig it

313
00:19:47,644 --> 00:19:51,014
deeper, and one of us really dug it deep

314
00:19:51,062 --> 00:19:54,282
to actually find out that the only problem that

315
00:19:54,336 --> 00:19:58,810
existed on those servers was the clock was adrift.

316
00:19:59,470 --> 00:20:03,662
One of the machines and the other machine of the cluster were

317
00:20:03,716 --> 00:20:07,102
running behind in time. Now why

318
00:20:07,236 --> 00:20:11,134
this would fail is because the leader at that point

319
00:20:11,172 --> 00:20:14,674
of time, which was actually setting a TTL because

320
00:20:14,712 --> 00:20:17,874
a TTL was very short and it was shorter than the

321
00:20:17,912 --> 00:20:21,250
drift. By the time those cluster would get that value,

322
00:20:21,400 --> 00:20:25,434
the time had expired. So the subsequent

323
00:20:25,502 --> 00:20:28,774
operations that went into the cluster would say that the key is

324
00:20:28,812 --> 00:20:33,286
not found something as elementary as or

325
00:20:33,308 --> 00:20:38,634
a basic checklist. This is something that we

326
00:20:38,672 --> 00:20:42,586
all know, clocks are important, NTBD is extremely important.

327
00:20:42,688 --> 00:20:46,620
But this is where we forget to have basic checklists in place.

328
00:20:47,390 --> 00:20:50,902
And we almost ended up convincing ourselves

329
00:20:50,966 --> 00:20:54,574
that hey, etcd was a problem. We should actually replace it with an

330
00:20:54,612 --> 00:20:58,366
entirely new cluster solution which is based on a

331
00:20:58,388 --> 00:21:01,594
new consensus algorithm. Worked well, but it just wasted

332
00:21:01,642 --> 00:21:05,220
so many hours of the entire team. So simple

333
00:21:05,830 --> 00:21:08,994
thing, which is as simple as,

334
00:21:09,192 --> 00:21:13,134
I mean, it just baffles me today that a very elementary

335
00:21:13,182 --> 00:21:16,434
thing, that is the first thing that we learn

336
00:21:16,472 --> 00:21:20,002
when we start debugging servers is that a time drift could actually

337
00:21:20,056 --> 00:21:23,078
cause a lot of issues. And this is the first time that we really experience

338
00:21:23,164 --> 00:21:26,566
it. Those is one of those bugs that you hit every five years and

339
00:21:26,588 --> 00:21:29,866
then you forget about it. But what was those real reason here?

340
00:21:29,968 --> 00:21:34,026
The real reason was not automation. The real reason

341
00:21:34,128 --> 00:21:37,130
was not any SRE tooling.

342
00:21:39,790 --> 00:21:43,214
It wasn't obviously for sure running on Kubernetes as many

343
00:21:43,252 --> 00:21:46,910
a times. I find that as a solution, it was

344
00:21:47,060 --> 00:21:50,494
a simple checklist of our basic first principles could

345
00:21:50,532 --> 00:21:54,606
have done the job, a very simple one which says that is

346
00:21:54,788 --> 00:21:57,714
an NTP check installed or not,

347
00:21:57,912 --> 00:21:59,780
and that could have saved this.

348
00:22:01,510 --> 00:22:04,706
And that's the real root cause behind it. And this

349
00:22:04,728 --> 00:22:07,974
is what I want to highlight here,

350
00:22:08,092 --> 00:22:12,200
that most of the time our SRE journey is assumed that

351
00:22:13,850 --> 00:22:16,886
another monitoring system, another charting system,

352
00:22:16,988 --> 00:22:20,826
another alerting system, could have saved the problem, either of

353
00:22:20,848 --> 00:22:24,154
these problems that we just described, right, and these

354
00:22:24,192 --> 00:22:27,642
are the real business cause failures here, because each

355
00:22:27,696 --> 00:22:31,082
one of this led CTO either a delay

356
00:22:31,226 --> 00:22:35,434
in our project or it led to a downtime,

357
00:22:35,562 --> 00:22:39,482
which really affected the business. And this isn't about slos.

358
00:22:39,546 --> 00:22:42,926
This isn't about SLis, because that's what typically what

359
00:22:42,948 --> 00:22:46,798
we find was the material when we talk about SRE on the Internet.

360
00:22:46,894 --> 00:22:50,162
This is about getting the first principles right. The first

361
00:22:50,216 --> 00:22:53,378
principles of, hey, were not going

362
00:22:53,384 --> 00:22:57,522
to make any changes which are manually done to a server no matter what.

363
00:22:57,656 --> 00:23:00,994
And if it is because of a reason that we are being too slow

364
00:23:01,122 --> 00:23:04,854
in application, if terraform is too slow, we need to learn

365
00:23:04,892 --> 00:23:08,198
how to make that faster. But we are not going to violate our own policy

366
00:23:08,284 --> 00:23:12,086
of making manual changes. It's not about anomaly detection

367
00:23:12,198 --> 00:23:15,882
or any fancy algorithms. There it is just the second

368
00:23:15,936 --> 00:23:19,850
outage is probably a derivative of those fact that we did not have a simple

369
00:23:19,920 --> 00:23:23,774
configuration manager validator. A simple basic tool

370
00:23:23,812 --> 00:23:27,914
like OS query which runs at the start of a system checks

371
00:23:27,962 --> 00:23:31,440
if my configuration is adrift. Could have been the answer.

372
00:23:31,810 --> 00:23:35,540
The third one is the simplest one, but the toughest of all.

373
00:23:36,630 --> 00:23:39,854
We spent our time checking timestamps

374
00:23:39,902 --> 00:23:43,554
in raft and paxos, but we didn't even have to go that

375
00:23:43,592 --> 00:23:47,214
far. The answer was our own server's timestamp

376
00:23:47,262 --> 00:23:50,982
was off. Could have been saved if there was a simple check

377
00:23:51,036 --> 00:23:55,414
which says hey, is NTP installed and active on a system which

378
00:23:55,452 --> 00:23:59,154
could have been done by using a very simple elementary

379
00:23:59,202 --> 00:24:02,874
bash script as well. The fact that we don't push ourselves to ask

380
00:24:02,912 --> 00:24:06,602
these questions and we look for answers in these tools is

381
00:24:06,656 --> 00:24:10,714
where most of the time were. We'll forget is that what

382
00:24:10,752 --> 00:24:14,714
we see as errors are actually a byproduct

383
00:24:14,762 --> 00:24:18,426
of things that we have failed to do or failed

384
00:24:18,538 --> 00:24:22,126
is a wrong word or strong word or we have ignored to take care of

385
00:24:22,228 --> 00:24:25,602
at the start of our scaffolding on how we

386
00:24:25,656 --> 00:24:29,026
build this entire thing. Which takes me to the

387
00:24:29,048 --> 00:24:32,254
outage number four, and this isn't a real outage.

388
00:24:32,302 --> 00:24:35,746
This is the one that you are going to face tomorrow. What will be the

389
00:24:35,768 --> 00:24:39,654
real root cause behind it? The real root cause behind that one

390
00:24:39,772 --> 00:24:44,402
is that we didn't learn anything from the previous one. A very profound

391
00:24:44,466 --> 00:24:47,734
line that Henry Ford said, the only real mistake is the one that we'll learn

392
00:24:47,772 --> 00:24:51,690
nothing from and that would be the reason for our next outage.

393
00:24:52,510 --> 00:24:55,834
Why is those important? And using all these principles is

394
00:24:55,872 --> 00:24:59,674
what we cause at last nine. What we have built is a lastio nine

395
00:24:59,712 --> 00:25:03,440
graph. Our theory and hours thesis says that hey,

396
00:25:04,290 --> 00:25:07,386
all these systems are actually connected,

397
00:25:07,578 --> 00:25:09,760
just like the World Wide web is.

398
00:25:10,770 --> 00:25:14,914
If you try diagnosing a problem after a situation

399
00:25:15,032 --> 00:25:18,642
has happened, and if there is no way of understanding these

400
00:25:18,696 --> 00:25:21,870
relations, it's almost impossible.

401
00:25:21,950 --> 00:25:25,300
CTo tell the impact of it. Example,

402
00:25:26,470 --> 00:25:30,022
my s three bucket permissions are public right now

403
00:25:30,156 --> 00:25:33,190
and this is something that I want to fix. Yes,

404
00:25:33,260 --> 00:25:36,390
while I know the fix is very simple, but I do not know

405
00:25:36,460 --> 00:25:39,546
what the cascading impact of that is going to be on the rest

406
00:25:39,568 --> 00:25:43,590
of my system. This is extremely

407
00:25:43,670 --> 00:25:47,114
hard to predict right now because

408
00:25:47,232 --> 00:25:50,698
we don't look at it as a graph system, we do not look at it

409
00:25:50,784 --> 00:25:53,898
as a connected set of things. And this

410
00:25:53,904 --> 00:25:57,134
is exactly what we're trying to solve at last night. And that's what

411
00:25:57,172 --> 00:26:00,414
we have built the knowledge graph for, which basically just

412
00:26:00,452 --> 00:26:04,586
goes through the system and build these components and those relationships so that any cascading

413
00:26:04,618 --> 00:26:06,910
impact is understood very clearly.

414
00:26:07,490 --> 00:26:10,434
To give an example of how do we put it into use,

415
00:26:10,632 --> 00:26:14,130
we basically load a graph and we try to find out hey,

416
00:26:14,200 --> 00:26:18,222
what does the spread look like for my particular instance

417
00:26:18,286 --> 00:26:21,286
right now? And if I run that query I realize that hey,

418
00:26:21,308 --> 00:26:24,614
quickly it tells me that hey, the split is four four three.

419
00:26:24,812 --> 00:26:28,406
Clearly there is an uneven split happening across

420
00:26:28,508 --> 00:26:32,586
one of the availability ones which may result in a failure if it

421
00:26:32,608 --> 00:26:36,234
was supposed to failed. Well, in this particular case because I wanted odd number

422
00:26:36,272 --> 00:26:39,994
of machines so this is a perfect scenario. But was

423
00:26:40,032 --> 00:26:43,374
this the case a few days back? Well, clearly not. What did

424
00:26:43,412 --> 00:26:47,070
it look like? It was two those and three, which is an even number and

425
00:26:47,140 --> 00:26:50,800
probably not the right way to actually split my application.

426
00:26:51,970 --> 00:26:55,726
Answers like these are what the questions like

427
00:26:55,748 --> 00:26:58,640
those, if I can answer them quickly,

428
00:26:59,330 --> 00:27:02,030
is what allows us to make these systems reliable.

429
00:27:02,370 --> 00:27:06,294
That's what we do. At last, lines a, we haven't yet

430
00:27:06,412 --> 00:27:09,366
open source this thing but we are more than happy to,

431
00:27:09,468 --> 00:27:13,334
if you want to give it a shout out, you want to try this

432
00:27:13,372 --> 00:27:18,070
out, just drop us a line. And we are extremely, extremely happy to

433
00:27:18,220 --> 00:27:21,926
set that up. And obviously there's a desire to actually put this out in

434
00:27:21,948 --> 00:27:24,440
all open source domain once it matures enough.

435
00:27:25,930 --> 00:27:27,110
That's all for me,

