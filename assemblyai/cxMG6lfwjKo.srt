1
00:02:14,840 --> 00:02:18,368
Well helps and welcome to my talk. LMAO helps

2
00:02:18,384 --> 00:02:21,936
during outages I'm Richard Lewis, I'm a senior DevOps

3
00:02:21,968 --> 00:02:25,732
consultant with three cloud solutions and I've had the pleasure of working with

4
00:02:25,786 --> 00:02:28,992
both software development teams and operations teams

5
00:02:29,056 --> 00:02:32,356
with. I currently have over 20 years experience of working in those

6
00:02:32,378 --> 00:02:36,152
industry. I'm also the co organizer for the Chicago

7
00:02:36,256 --> 00:02:39,628
Honorary Enthusiast User group and as you

8
00:02:39,634 --> 00:02:42,860
can tell from everything around me, I'm a diehard White Sox fan.

9
00:02:43,600 --> 00:02:47,432
So a little bit about my company three cloud is the largest peer play Azure

10
00:02:47,496 --> 00:02:51,136
service partner in those world. We have about 600 people

11
00:02:51,318 --> 00:02:54,956
who are dedicated Azure professionals focused

12
00:02:54,988 --> 00:02:59,148
on data and analytics, app innovation

13
00:02:59,244 --> 00:03:02,272
and helping clients provide a modern

14
00:03:02,416 --> 00:03:03,510
cloud platform.

15
00:03:05,560 --> 00:03:08,916
So LMao, as you could

16
00:03:08,938 --> 00:03:12,532
probably guess by now, it is not just

17
00:03:12,586 --> 00:03:16,104
laughing your way through the outage, but it's actually

18
00:03:16,142 --> 00:03:19,764
an acronym for something else. It's an acronym

19
00:03:19,812 --> 00:03:21,796
for logs,

20
00:03:21,988 --> 00:03:25,940
Metrics, Alerts and an observability

21
00:03:26,020 --> 00:03:29,836
tool. And these are the things that you need to have to make up

22
00:03:29,858 --> 00:03:33,276
a LMAO strategy. So what

23
00:03:33,298 --> 00:03:36,412
am I actually talking about today? I'm actually talking

24
00:03:36,466 --> 00:03:40,336
about providing platform support strategies for

25
00:03:40,358 --> 00:03:43,632
your team members, creating a standard for

26
00:03:43,686 --> 00:03:47,312
knowledge sharing, helping you

27
00:03:47,446 --> 00:03:51,072
reduce the meantime resolution, and building

28
00:03:51,126 --> 00:03:54,884
the psychological safety of your team members so that way they're able

29
00:03:54,922 --> 00:03:58,020
to be able to lmao their way during LMAO outages.

30
00:03:59,560 --> 00:04:03,140
So if you're not familiar with logs and metrics,

31
00:04:04,040 --> 00:04:07,544
those are the things that's going to provide you the insight as to what and

32
00:04:07,582 --> 00:04:10,936
when it's happening. It's going to

33
00:04:10,958 --> 00:04:14,340
help you figure out how many errors occurred,

34
00:04:14,500 --> 00:04:17,836
how many requests you got, and it's going to help you figure out

35
00:04:17,858 --> 00:04:20,540
the duration of those errors and request.

36
00:04:21,760 --> 00:04:25,452
Now as for alerts, they come in two

37
00:04:25,586 --> 00:04:29,440
main ways. Pages, those are critical things

38
00:04:29,510 --> 00:04:33,376
like hey, our whole network is down, or hey,

39
00:04:33,558 --> 00:04:36,844
our website's offline. And tickets,

40
00:04:36,972 --> 00:04:40,400
those would be things like hey, the hard drive

41
00:04:40,470 --> 00:04:44,468
that runs our website is at 80% full

42
00:04:44,634 --> 00:04:47,110
or 85 or 90% full.

43
00:04:47,640 --> 00:04:51,680
Or we're seeing a slowness

44
00:04:51,760 --> 00:04:55,988
in our ecommerce website. Not outage,

45
00:04:56,084 --> 00:04:58,970
just some general slowness, just some pain.

46
00:05:00,940 --> 00:05:04,840
And like most of you, I've experienced alert trauma.

47
00:05:06,240 --> 00:05:10,536
This is me, my first IT job circa

48
00:05:10,728 --> 00:05:14,312
2011 and I was supporting an Azure

49
00:05:14,376 --> 00:05:18,044
97 application. I has the only

50
00:05:18,082 --> 00:05:21,516
one doing this. I was on an on call rotation 365

51
00:05:21,538 --> 00:05:25,728
days a year for 16 hours a day. Because it was also a call center

52
00:05:25,814 --> 00:05:29,376
and they worked two shifts, I had no way to connect to

53
00:05:29,398 --> 00:05:33,168
the office remotely. Luckily I lived within 10 miles of

54
00:05:33,174 --> 00:05:36,900
the office, so whenever there was an outage I would get paged

55
00:05:37,320 --> 00:05:40,436
using actually this pager right here that you see

56
00:05:40,458 --> 00:05:43,910
on the left side of your screen and I would respond to it.

57
00:05:44,280 --> 00:05:47,636
My company had no logging framework, no concept

58
00:05:47,668 --> 00:05:51,316
of the logging framework. And within six months, you guessed

59
00:05:51,348 --> 00:05:54,500
it, I burned out. But I didn't quit.

60
00:05:54,660 --> 00:05:57,864
I let my boss know what was going on, my thoughts

61
00:05:57,912 --> 00:06:01,116
and opinions. My boss was very receptive to these thoughts and

62
00:06:01,138 --> 00:06:05,660
opinions and we worked on putting together a LMAO strategy.

63
00:06:06,880 --> 00:06:10,188
We tackled the issue of logging framework first.

64
00:06:10,354 --> 00:06:13,756
We used log for net. We tackled the

65
00:06:13,778 --> 00:06:17,760
on call rotation issue by hiring additional people

66
00:06:17,910 --> 00:06:21,452
and spreading that load out. So we created a little schedule

67
00:06:21,516 --> 00:06:26,044
when we're going to do these things. So managing

68
00:06:26,092 --> 00:06:29,156
your alerts effectively requires certain things. And as

69
00:06:29,178 --> 00:06:32,596
you guessed it, as I just said before, scheduling your team

70
00:06:32,618 --> 00:06:35,936
members appropriately. That's an important part of it. You can't

71
00:06:35,968 --> 00:06:39,304
have a person on call 24 hours a day,

72
00:06:39,342 --> 00:06:42,788
365 days a year. It's just not effective

73
00:06:42,964 --> 00:06:46,436
and they're going to leave. Then you take with them their tribal

74
00:06:46,468 --> 00:06:49,230
knowledge and now you're stuck in that position all over again.

75
00:06:49,600 --> 00:06:52,876
Avoiding alert would take wherever possible. If they

76
00:06:52,898 --> 00:06:56,572
don't need to be woken up or paged about it, let's not page

77
00:06:56,626 --> 00:07:00,464
them. Collecting those data on the alerts that are actually

78
00:07:00,502 --> 00:07:03,904
going out, look into ways to reduce those alerts so

79
00:07:03,942 --> 00:07:07,170
those would be some kind of continual improvement opportunity.

80
00:07:08,820 --> 00:07:11,890
If the alerts that are going out is because

81
00:07:12,260 --> 00:07:15,604
the servers have reached a high number

82
00:07:15,642 --> 00:07:18,960
of cpu, look at things like auto scaling.

83
00:07:19,120 --> 00:07:22,448
Or if it's an ecommerce website, all of a sudden the traffic

84
00:07:22,464 --> 00:07:27,924
has gone through the roof, say like every

85
00:07:27,962 --> 00:07:31,400
day between certain hours a website increases in traffic.

86
00:07:31,740 --> 00:07:35,528
Then maybe look at having auto scaling there as well.

87
00:07:35,694 --> 00:07:39,272
Or if it's a hard drive, as I said earlier,

88
00:07:39,336 --> 00:07:42,556
the hard drive hit a certain capacity threshold, then look

89
00:07:42,578 --> 00:07:46,104
at some kind of run hooks or run books that could automatically heal

90
00:07:46,152 --> 00:07:49,528
those kind of things. And be cautious when you're

91
00:07:49,544 --> 00:07:53,484
introducing new alerts. Every alert has a purpose.

92
00:07:53,612 --> 00:07:57,392
That's two. That's right, alert you. So be very

93
00:07:57,446 --> 00:08:01,136
cautious about those alerts that you're introducing and the impact that they're going to

94
00:08:01,158 --> 00:08:02,230
have on your team.

95
00:08:05,160 --> 00:08:08,230
Observability on the other hand, is a little bit different.

96
00:08:08,840 --> 00:08:15,200
Observability is more of the voyeurism

97
00:08:15,280 --> 00:08:18,920
of it. So you get to see what's actually

98
00:08:18,990 --> 00:08:22,424
going on on a nice pretty screen or dashboard or something like

99
00:08:22,462 --> 00:08:26,292
that. It's going to be helpful for youll to understand your KPIs

100
00:08:26,356 --> 00:08:30,284
or SLAs. You can actually monitor your usage of different

101
00:08:30,322 --> 00:08:34,108
systems the same way. And spot trends dashboards definitely

102
00:08:34,194 --> 00:08:38,190
help spot trends, and that's where you see products like power, Bi and

103
00:08:38,960 --> 00:08:41,836
tableau and things like that. So you can spot trends.

104
00:08:42,028 --> 00:08:44,690
Observability dashboards do the same thing.

105
00:08:45,700 --> 00:08:49,212
And when it comes to observability tools to help you with your observability

106
00:08:49,276 --> 00:08:52,724
dashboards, there are hundreds of thousands of products out

107
00:08:52,762 --> 00:08:56,452
there. I actually took

108
00:08:56,506 --> 00:09:00,272
this screenshot here from the cloud Native

109
00:09:00,336 --> 00:09:03,824
Computing Foundation's website. Under their monitoring

110
00:09:03,872 --> 00:09:07,216
section, they highlight a ton

111
00:09:07,258 --> 00:09:10,392
of different products. This is not exclusively all the products

112
00:09:10,446 --> 00:09:13,864
out there, but this is just a wide range of different

113
00:09:13,902 --> 00:09:16,520
types of products that you can use for observability.

114
00:09:18,560 --> 00:09:22,060
I'm sure that there are at least one or two products

115
00:09:22,130 --> 00:09:25,260
on this screen, if not more, that you're probably using

116
00:09:25,330 --> 00:09:29,584
in your current place. Looking at this screen,

117
00:09:29,702 --> 00:09:31,970
I counted at least five or six.

118
00:09:35,380 --> 00:09:39,504
Okay, more like nine products that I regularly recommend to

119
00:09:39,542 --> 00:09:43,680
clients that are helpful for their different situations.

120
00:09:44,360 --> 00:09:47,444
I don't think that there's one size that fits all. I think

121
00:09:47,482 --> 00:09:51,636
that you want to use the appropriate tool for the appropriate cost

122
00:09:51,738 --> 00:09:55,284
at the appropriate time. They all fluctuate on costs and

123
00:09:55,322 --> 00:09:59,096
trade offs. This is actually a

124
00:09:59,118 --> 00:10:02,250
dashboard here from

125
00:10:02,780 --> 00:10:06,556
new relic, and this

126
00:10:06,578 --> 00:10:09,964
dashboard actually highlights. In the

127
00:10:10,002 --> 00:10:13,532
left corner, you're seeing synthetic failures and

128
00:10:13,586 --> 00:10:16,588
violations of policies that they have in place.

129
00:10:16,754 --> 00:10:20,400
And I believe it's just a coincidence that it's working out where you have 13

130
00:10:20,900 --> 00:10:24,876
policy violations and 13 synthetic

131
00:10:24,908 --> 00:10:28,224
failures. And that violation could be just

132
00:10:28,262 --> 00:10:31,772
because every time that synthetic failure policy is violated,

133
00:10:31,836 --> 00:10:35,156
it registers as a new violation. So the numbers are correlating right

134
00:10:35,178 --> 00:10:39,060
there at that time. But the top one

135
00:10:39,130 --> 00:10:42,692
is those violations of policies. The one below it is

136
00:10:42,746 --> 00:10:47,844
the synthetic failures. And synthetic failures

137
00:10:47,892 --> 00:10:52,490
are usually generated by something third party tool monitoring or

138
00:10:53,100 --> 00:10:56,490
testing your system. So that would be just doing like

139
00:10:56,940 --> 00:11:00,396
if it's alive or dead, call to see if your service is up

140
00:11:00,418 --> 00:11:03,960
or down, or how long does it take for your website to load

141
00:11:04,120 --> 00:11:07,516
using some kind of framework like selenium or

142
00:11:07,538 --> 00:11:10,864
something like that. The next thing we

143
00:11:10,902 --> 00:11:14,044
see is the errors that are occurring

144
00:11:14,092 --> 00:11:17,964
per minute. So now we know how many failures

145
00:11:18,012 --> 00:11:22,168
have happened and a duration of how many failures we're seeing per minute.

146
00:11:22,364 --> 00:11:25,616
And this makes me wonder, what did we change recently

147
00:11:25,648 --> 00:11:28,230
in our system that caused this problem?

148
00:11:29,080 --> 00:11:32,852
And that's where you see this here. This is actually the

149
00:11:32,906 --> 00:11:36,496
deployment notes, and luckily they're writing good

150
00:11:36,538 --> 00:11:41,476
release notes, I'm assuming. And that's how you know what's

151
00:11:41,508 --> 00:11:45,208
in those most recent release or has a possible impact to what's going on here.

152
00:11:45,294 --> 00:11:49,608
I doubt that those readme with endpoints

153
00:11:49,784 --> 00:11:54,364
is causing the problem, but it could be things

154
00:11:54,402 --> 00:11:58,860
is another dashboard here, and this one's actually made by a company called Grafana.

155
00:12:00,420 --> 00:12:04,720
This here actually, I really like this dashboard. It's a good example of

156
00:12:04,870 --> 00:12:08,812
being able to embed a wiki and documentation

157
00:12:08,876 --> 00:12:12,724
directly into the dashboard. So that way who's ever on call, they're able

158
00:12:12,762 --> 00:12:17,332
to just pull up this dashboard related to that application and

159
00:12:17,466 --> 00:12:20,676
click on a link to get from here to wherever they need to

160
00:12:20,698 --> 00:12:24,596
go to to see a true source

161
00:12:24,628 --> 00:12:28,996
of truth, or to go access details

162
00:12:29,028 --> 00:12:32,212
about who to contact or details

163
00:12:32,276 --> 00:12:35,210
related to the third party service or something like that.

164
00:12:35,900 --> 00:12:39,336
There's also a diagram in here and it's showing how you can load

165
00:12:39,368 --> 00:12:42,700
images into it of what that service

166
00:12:42,770 --> 00:12:46,236
is actually looking like. Like that service's path and

167
00:12:46,338 --> 00:12:47,820
architectural diagram.

168
00:12:50,660 --> 00:12:54,192
And we're highlighting here up

169
00:12:54,246 --> 00:12:57,648
and down of the service. So now we know the frequency of

170
00:12:57,654 --> 00:13:01,356
the service going up or down, followed by again we're

171
00:13:01,388 --> 00:13:04,604
highlighting over aimed those status

172
00:13:04,652 --> 00:13:08,004
codes that are being received by those service are sending out from this

173
00:13:08,042 --> 00:13:11,892
service. So we see a combination of 500 errors and 200

174
00:13:11,946 --> 00:13:12,820
errors.

175
00:13:15,420 --> 00:13:18,996
So the next thing that's really important to think about is preparing

176
00:13:19,028 --> 00:13:22,484
your team for those outages. Having a playbook

177
00:13:22,532 --> 00:13:25,050
is one of those critical and key things,

178
00:13:26,960 --> 00:13:31,230
practicing for those outages as well.

179
00:13:32,720 --> 00:13:36,380
And having a playbook comes down to what you actually

180
00:13:36,450 --> 00:13:40,236
put in it. So I

181
00:13:40,258 --> 00:13:43,488
noted on here, it's important about having it in a location where

182
00:13:43,494 --> 00:13:46,976
it could be quickly accessible. Sometimes you may

183
00:13:46,998 --> 00:13:50,768
want to put your playbooks internally, and I kind of like,

184
00:13:50,854 --> 00:13:53,700
I'm 50 50 on those kind of things situations.

185
00:13:55,240 --> 00:13:59,012
If your internal system has to be accessed through a third party system

186
00:13:59,066 --> 00:14:03,968
that may be possibly having an outage, then you're

187
00:14:03,984 --> 00:14:07,912
going to delay yourself to get into that, your playbook. So if you're using something

188
00:14:07,966 --> 00:14:11,544
like Azure ad to authenticate to get into

189
00:14:11,582 --> 00:14:15,176
your company network, then if there's a

190
00:14:15,198 --> 00:14:18,876
problem with Azure ad, then it's going to delay you

191
00:14:18,898 --> 00:14:21,260
getting to your playbook.

192
00:14:23,520 --> 00:14:27,292
I like other systems as well, also tied together

193
00:14:27,426 --> 00:14:31,330
still using SSO, so single sign on,

194
00:14:31,780 --> 00:14:36,956
but also tie those together with other systems like Atlassian's

195
00:14:36,988 --> 00:14:40,850
confluence or a third party wiki system

196
00:14:41,400 --> 00:14:44,916
or SharePoint. So back

197
00:14:44,938 --> 00:14:48,788
to those ad again. Or SharePoint or

198
00:14:48,954 --> 00:14:52,608
Microsoft Teams has a wiki system built into that. If you're a user

199
00:14:52,624 --> 00:14:56,250
of that as well, somewhere where you can keep it

200
00:14:56,700 --> 00:15:00,170
outside of your network but still accessible, but kind of

201
00:15:01,660 --> 00:15:04,890
lessens the likelihood of having an issue there.

202
00:15:05,740 --> 00:15:09,736
But inside those playbooks, though, you want to put things like links

203
00:15:09,768 --> 00:15:13,212
to your application that may be

204
00:15:13,266 --> 00:15:15,710
related to those observability tools that you're using,

205
00:15:16,480 --> 00:15:20,076
as well as details about the golden signals of that application.

206
00:15:20,178 --> 00:15:23,792
So that way when a person is actually looking at what's going on

207
00:15:23,846 --> 00:15:27,424
and hearing from the users of what's going on, they're able to

208
00:15:27,462 --> 00:15:30,960
say this is within those normal range. This is not within the normal range.

209
00:15:31,720 --> 00:15:35,636
Any relevant notes or information from previous outages, those kind of

210
00:15:35,658 --> 00:15:38,388
situations help you title things back together.

211
00:15:38,474 --> 00:15:42,052
So you're doing like a post mortem after

212
00:15:42,106 --> 00:15:45,336
the outage and putting a link to

213
00:15:45,358 --> 00:15:49,156
the post mortem notes are quite helpful contacts

214
00:15:49,188 --> 00:15:52,840
for those application owner or any third party services

215
00:15:52,910 --> 00:15:56,724
that's owners of it as well. So say if your applications run

216
00:15:56,782 --> 00:16:00,072
something like Azure or AWS,

217
00:16:00,136 --> 00:16:04,664
then links to the premier

218
00:16:04,712 --> 00:16:08,268
support contact information. So that way who's ever on call knows how

219
00:16:08,274 --> 00:16:11,648
to get a hold of them to escalate and get the right people on the

220
00:16:11,654 --> 00:16:15,744
call properly. Or that

221
00:16:15,782 --> 00:16:18,704
way they don't have to call a manager or something like that and say,

222
00:16:18,742 --> 00:16:22,880
who do I call about this? Or so forth,

223
00:16:23,040 --> 00:16:26,788
or links to

224
00:16:26,874 --> 00:16:30,596
things like Stripe's website, if you have a payment service that may

225
00:16:30,618 --> 00:16:34,644
be causing a failure or something like that as

226
00:16:34,682 --> 00:16:37,816
well. And anything else that youll may think there's a ton

227
00:16:37,838 --> 00:16:41,064
of things you may want to put in your playbook around that application. I do

228
00:16:41,102 --> 00:16:45,764
suggest, though, dedicate a playbook per application opposed

229
00:16:45,812 --> 00:16:50,116
to doing just a single playbook of hundreds

230
00:16:50,148 --> 00:16:53,452
of thousands of things. You can put all of your playbooks together in one

231
00:16:53,506 --> 00:16:56,764
same system, but you kind of want to have them breaking out

232
00:16:56,802 --> 00:17:00,384
by section at least. And as I was talking about

233
00:17:00,422 --> 00:17:04,370
their preparation and training, I know I mentioned before

234
00:17:05,540 --> 00:17:09,696
the importance of this, but I come from the midwest of

235
00:17:09,718 --> 00:17:12,696
the United States, where we have a lot of tornadoes.

236
00:17:12,828 --> 00:17:16,276
And so as kids, we are trained, like you see on your

237
00:17:16,298 --> 00:17:19,604
screen here, when we hear that

238
00:17:19,642 --> 00:17:24,596
siren, to go into the hallway and put

239
00:17:24,618 --> 00:17:28,264
our hands over our head and curl up into a little ball in

240
00:17:28,302 --> 00:17:32,232
preparation of a tornado coming through those area if one was to happen.

241
00:17:32,366 --> 00:17:35,496
But we trained for it and we knew what

242
00:17:35,518 --> 00:17:39,404
to do as muscle memory. So the

243
00:17:39,442 --> 00:17:42,590
practice of chaos engineering is something

244
00:17:43,760 --> 00:17:47,548
still being worked out. It's been around for a while.

245
00:17:47,714 --> 00:17:51,120
The concept has been, it was created by Netflix.

246
00:17:52,340 --> 00:17:55,708
It helps you increase resiliency,

247
00:17:55,804 --> 00:17:59,296
is the goal there, really? And you're able to identify and

248
00:17:59,318 --> 00:18:02,884
address signal points of failure early. What you're doing

249
00:18:02,922 --> 00:18:06,870
is you're running controlled experiments against your system

250
00:18:07,320 --> 00:18:10,816
and you're predicting

251
00:18:10,848 --> 00:18:14,596
those possible outcome that outcome could actually happen

252
00:18:14,698 --> 00:18:17,976
or the outcome could not. And that's where the chaos engineering comes

253
00:18:17,998 --> 00:18:21,304
into play. You don't really know if it's going to happen or not, but the

254
00:18:21,342 --> 00:18:25,336
goal is, in the end, to identify your failure points,

255
00:18:25,518 --> 00:18:29,116
address those failure points. So that way, if something was

256
00:18:29,138 --> 00:18:33,070
to happen on those failure points, you youll be able to sustain it.

257
00:18:34,720 --> 00:18:38,712
There's a great article about how Netflix

258
00:18:38,776 --> 00:18:42,496
practices their castle engineering. I put a link below for

259
00:18:42,518 --> 00:18:46,080
you if you want to go take a look at it. And after

260
00:18:46,150 --> 00:18:49,712
those outages, though, you want to take the time to do a post

261
00:18:49,766 --> 00:18:53,540
mortem, usually within a day or so of the outage,

262
00:18:53,960 --> 00:18:57,510
while it's still fresh in everyone's mind. You want to get everyone together

263
00:18:59,400 --> 00:19:03,284
around the conference table and just talk through what

264
00:19:03,322 --> 00:19:07,480
went right, what went wrong, where did you get lucky?

265
00:19:09,420 --> 00:19:12,996
And just figure out what needs to be documented

266
00:19:13,028 --> 00:19:16,840
in terms of preparation for a possible future

267
00:19:16,910 --> 00:19:20,520
outage. I really like this quote here from Devon

268
00:19:20,880 --> 00:19:24,430
with Google, and it is the cost of failure is

269
00:19:27,280 --> 00:19:31,120
so talking about back to my first on call

270
00:19:31,190 --> 00:19:34,924
rotation job. One of the things I was required to do regularly

271
00:19:34,972 --> 00:19:39,132
were to do write alongs with technicians for appliances.

272
00:19:39,276 --> 00:19:42,560
And when we would go out to customers houses,

273
00:19:44,360 --> 00:19:47,140
the customer would have something broken,

274
00:19:47,480 --> 00:19:50,612
but they may have tried to fix themselves, but may have

275
00:19:50,666 --> 00:19:54,592
done it wrong, not fully followed the instructions that they got from the manufacturer,

276
00:19:54,656 --> 00:19:58,884
or weren't fully listening to a YouTube video that they were following

277
00:19:59,012 --> 00:20:02,360
and missed some key details. So we would be able to

278
00:20:02,430 --> 00:20:05,290
quickly resolve those issues within a matter of minutes.

279
00:20:05,740 --> 00:20:09,644
And the cost of education in that case was our

280
00:20:09,682 --> 00:20:13,084
service call fee. And so the

281
00:20:13,122 --> 00:20:16,348
customer, youll learn something new. They would learn how to fix that problem in the

282
00:20:16,354 --> 00:20:19,310
future, but at the same time, no,

283
00:20:19,940 --> 00:20:24,064
that gives them the ability to get their system back up online really

284
00:20:24,102 --> 00:20:27,184
quickly. So the cost of failure is

285
00:20:27,222 --> 00:20:29,440
education. It's a good quote.

286
00:20:30,660 --> 00:20:34,004
So my takeaways from my talk today are pretty

287
00:20:34,042 --> 00:20:37,636
simple. Have an lmao strategy in

288
00:20:37,658 --> 00:20:41,284
place, have that documented and ready to

289
00:20:41,322 --> 00:20:43,590
go. Everyone knows where it's at.

290
00:20:44,460 --> 00:20:48,488
Update those documents regularly after

291
00:20:48,574 --> 00:20:51,370
your outages, go back, update them,

292
00:20:52,220 --> 00:20:55,720
have a revision date on those documents.

293
00:20:56,140 --> 00:20:59,292
So that way, you know in the last time it was updated. And if they

294
00:20:59,346 --> 00:21:03,116
haven't been updated in six months, either you're not

295
00:21:03,138 --> 00:21:06,408
having allergies around that system or you're

296
00:21:06,424 --> 00:21:10,176
not documenting what's happening with that system. So good or

297
00:21:10,198 --> 00:21:13,280
bad there, avoid alert fatigue.

298
00:21:14,340 --> 00:21:17,984
The less alert fatigue you have, the more psychological safety you're building

299
00:21:18,022 --> 00:21:21,184
into youll people, and the more comfortable they are,

300
00:21:21,222 --> 00:21:24,310
they know where their documents are and they're able to go forward from there,

301
00:21:25,000 --> 00:21:28,676
the less likely that they're going to want to leave your organization and take that

302
00:21:28,698 --> 00:21:32,870
tribal knowledge with them. It'll cut down turnover and everything

303
00:21:33,800 --> 00:21:37,220
and run readiness preparation drills regularly.

304
00:21:37,960 --> 00:21:41,408
Chaos engineering again, it's a newer thing, but there's

305
00:21:41,424 --> 00:21:45,188
a lot of tools out there that can help you. Gremlin makes some great products,

306
00:21:45,274 --> 00:21:47,550
great documentation out there from them.

307
00:21:47,920 --> 00:21:51,420
Microsoft has a great has engineering product as well,

308
00:21:51,570 --> 00:21:55,230
great documentation to help you think about ways to do these things.

309
00:21:56,160 --> 00:21:59,976
And thank you so much for listening

310
00:22:00,008 --> 00:22:03,500
to me today. Thank you for your time and

311
00:22:03,570 --> 00:22:04,840
enjoy the rest of the conference.

