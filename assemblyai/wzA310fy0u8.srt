1
00:00:23,290 --> 00:00:26,406
Hi and welcome to everyone who decided to watch this talk.

2
00:00:26,508 --> 00:00:30,274
I am very excited to talk to you about orchestrating data and ML

3
00:00:30,322 --> 00:00:33,606
workflows with Apache Airflow because there are quite a few

4
00:00:33,628 --> 00:00:36,834
topics that I want to talk about. I thought I'd give you a quick overview

5
00:00:36,882 --> 00:00:40,086
over what you can expect in this talk. First, I'm going to talk about

6
00:00:40,108 --> 00:00:43,606
the term ML orchestration. Next, just in case you've never used

7
00:00:43,628 --> 00:00:46,838
Airflow before, I'm going to give you a little crash course with everything you need

8
00:00:46,844 --> 00:00:50,906
to get started. Then I will talk about the data that I was analyzing

9
00:00:50,938 --> 00:00:54,478
that I built my pipeline around, and the main part of

10
00:00:54,484 --> 00:00:57,834
the talk will be a walkthrough this data driven ML

11
00:00:57,882 --> 00:01:01,338
pipeline in airflow, both with screenshots of the Airflow UI

12
00:01:01,434 --> 00:01:04,990
and with little code snippets highlighting certain airflow features

13
00:01:05,070 --> 00:01:08,850
that I think will be useful for you if you build your own ML pipelines.

14
00:01:09,350 --> 00:01:12,366
The features I'm going to highlight will be data sets,

15
00:01:12,558 --> 00:01:15,886
asynchronous tasks, and deferrable operators.

16
00:01:15,998 --> 00:01:19,554
I will talk about dynamic tasks, the Astro SDK, my own

17
00:01:19,592 --> 00:01:23,634
custom huggingface operators, and lastly, about how I enabled

18
00:01:23,682 --> 00:01:26,786
slack alerting for certain events in my pipeline.

19
00:01:26,978 --> 00:01:30,594
After all that, I will hop over in the actual airflow environment.

20
00:01:30,642 --> 00:01:34,378
So you have a short demo of what this looks like when it's running,

21
00:01:34,544 --> 00:01:37,802
and I will talk to you about the results that I got for the data

22
00:01:37,856 --> 00:01:41,642
that I was analyzing. Lastly, we will talk or

23
00:01:41,696 --> 00:01:45,226
think about what could be next for this pipeline and for ML

24
00:01:45,258 --> 00:01:48,702
and Airflow in particular. And I will close with a few

25
00:01:48,756 --> 00:01:52,270
ML and airflow resources in case you want to get started

26
00:01:52,340 --> 00:01:55,986
on your own. All right, what is

27
00:01:56,008 --> 00:01:59,906
ML orchestration? It's a fairly new term. It is part or

28
00:01:59,928 --> 00:02:02,914
a subset of ML ops. Now,

29
00:02:02,952 --> 00:02:06,626
if you work in any capacity in the data sphere, you know

30
00:02:06,648 --> 00:02:10,418
that there are many different job titles and responsibilities. There are machine

31
00:02:10,434 --> 00:02:14,562
learning engineers, data engineers, DevOps engineers, data scientists,

32
00:02:14,706 --> 00:02:18,406
and a lot of times they do overlapping things. And this is the same

33
00:02:18,588 --> 00:02:22,362
situation here with the different terms. You do have some overlap between

34
00:02:22,416 --> 00:02:26,038
machine learning and DevOps. You have overlap between machine learning, DevOps,

35
00:02:26,054 --> 00:02:30,058
and data engineering, which is commonly what we call ML Ops. And then

36
00:02:30,144 --> 00:02:34,362
over all of that, you have orchestration. And an orchestrator

37
00:02:34,426 --> 00:02:38,046
is a tool that sits on top of your stack that

38
00:02:38,068 --> 00:02:42,026
you're using for a certain action, your ML stack or your data engineering

39
00:02:42,058 --> 00:02:45,394
stack, and then goes to each tool and

40
00:02:45,432 --> 00:02:48,994
tells it what to do at what time. So an

41
00:02:49,032 --> 00:02:53,150
orchestrator is kind of like the conductor in an orchestra

42
00:02:53,230 --> 00:02:56,750
that makes sure that all of the instruments are playing in tune on the

43
00:02:56,760 --> 00:03:00,280
right schedules, and that everything happens at the right time.

44
00:03:00,650 --> 00:03:04,566
And airflow is one of those orchestration tools. And airflow is

45
00:03:04,588 --> 00:03:08,386
very multipurpose in that you can see that little orchestrator

46
00:03:08,578 --> 00:03:12,026
sphere overlaps all of the other spheres, and there

47
00:03:12,048 --> 00:03:16,310
is an overlap between mlops and the orchestration

48
00:03:16,390 --> 00:03:19,622
or airflow. And this is what we call ML orchestration

49
00:03:19,686 --> 00:03:23,374
for this talk, this will be very

50
00:03:23,412 --> 00:03:27,066
familiar for you. If you are working in the ML sphere,

51
00:03:27,178 --> 00:03:31,854
you have your typical data science machine learning process.

52
00:03:32,052 --> 00:03:35,726
Usually you get some data, and the first rule of data is it's

53
00:03:35,758 --> 00:03:38,910
never in the format that you need and it always needs cleaning.

54
00:03:38,990 --> 00:03:41,390
So the first steps are usually data extraction,

55
00:03:41,470 --> 00:03:45,678
validation, what is classically part of the data engineering sphere.

56
00:03:45,774 --> 00:03:49,586
Then usually the data scientist comes in, does some manual exploration,

57
00:03:49,698 --> 00:03:53,474
engineers the features, builds the model, and lastly, the machine learning engineer,

58
00:03:53,522 --> 00:03:56,498
most of the time puts all of this in production.

59
00:03:56,674 --> 00:04:00,006
And along the way here you have

60
00:04:00,028 --> 00:04:03,402
a lot of steps that could be automated. Not all of them,

61
00:04:03,456 --> 00:04:07,066
of course. The exploratory analysis of the data scientist is

62
00:04:07,088 --> 00:04:10,426
really hard to automate. But a lot of things, once you know what

63
00:04:10,448 --> 00:04:14,206
the data is, what it looks like, and you know your model can really

64
00:04:14,228 --> 00:04:17,786
be automated. This is where airflow

65
00:04:17,818 --> 00:04:21,066
comes in, because you know that from any workflow

66
00:04:21,098 --> 00:04:23,966
in your life, the automated steps are very quick.

67
00:04:24,068 --> 00:04:28,146
But as soon as someone comes in, that's the

68
00:04:28,248 --> 00:04:31,650
cause of your latency. If someone has to copy paste something

69
00:04:31,720 --> 00:04:34,846
or if someone has to manually kick off the next script,

70
00:04:34,958 --> 00:04:38,418
that will really pull your process and it

71
00:04:38,424 --> 00:04:42,066
will make it much longer. And that's what we are trying to combat with airflow.

72
00:04:42,098 --> 00:04:45,718
We try to automate everything that is somehow possible

73
00:04:45,804 --> 00:04:49,014
to automate. All right,

74
00:04:49,132 --> 00:04:52,838
that's ML orchestration. Now, if you've never used Airflow

75
00:04:52,854 --> 00:04:56,186
before, I'm going to give you a very short crash course. What is

76
00:04:56,208 --> 00:04:59,974
Apache Airflow? It is a tool to schedule and monitor

77
00:05:00,022 --> 00:05:03,566
your data pipelines or any other of your Python code.

78
00:05:03,668 --> 00:05:07,326
Airflow is very versatile, so you can schedule anything that

79
00:05:07,428 --> 00:05:11,434
you can define in Python code, and that's why it's a multipurpose

80
00:05:11,482 --> 00:05:15,290
orchestrator. It's the most popular open source choice

81
00:05:15,450 --> 00:05:19,458
for this use case with 12 million downloads per month. And everything

82
00:05:19,544 --> 00:05:23,106
in Airflow is written as Python code. You're going to see what that

83
00:05:23,128 --> 00:05:27,342
code looks like. If you're using Python for machine learning, it will be very familiar

84
00:05:27,406 --> 00:05:30,886
to you. And that's also some of the big strengths of

85
00:05:30,908 --> 00:05:34,354
airflow, because it's defined in Python code, you can have CI CD

86
00:05:34,402 --> 00:05:38,374
on your pipelines process and you can infinitely scale and

87
00:05:38,572 --> 00:05:42,154
extend what you're doing with airflow. And of course,

88
00:05:42,352 --> 00:05:45,594
if after this talk you want to get started, there is a large and

89
00:05:45,632 --> 00:05:48,954
vibrant open source software community. We have our own

90
00:05:48,992 --> 00:05:52,382
slack. There are a lot of alters on stack overflow. There's the whole

91
00:05:52,436 --> 00:05:56,590
open source GitHub. Please get in touch if you have any issues or

92
00:05:56,660 --> 00:05:58,000
want to learn more.

93
00:06:00,450 --> 00:06:03,822
Now, apart from being defined in Python code,

94
00:06:03,876 --> 00:06:07,234
Airflow comes with a very functional and very

95
00:06:07,272 --> 00:06:10,702
pretty UI. What you see here is the overview.

96
00:06:10,766 --> 00:06:13,986
Over the Airflow UI you can see all of the dags. This is what

97
00:06:14,008 --> 00:06:17,326
we call one pipeline in Airflow stands for directed

98
00:06:17,358 --> 00:06:21,602
acyclic graph. And here you can see we have nine dags.

99
00:06:21,746 --> 00:06:24,850
I have some of them paused, some of them are activated,

100
00:06:25,010 --> 00:06:28,358
and I will not go through everything that you see here, but there's a lot

101
00:06:28,364 --> 00:06:31,526
of observability. You know exactly how your

102
00:06:31,548 --> 00:06:34,794
workflows are scheduled, when it ran the last

103
00:06:34,832 --> 00:06:38,378
time, when the next one will be, and what happened to the tasks? You can

104
00:06:38,384 --> 00:06:42,038
see green and red here on the screen. Green means something was successful,

105
00:06:42,134 --> 00:06:45,518
red means something failed. And there are other colors. So there's a

106
00:06:45,524 --> 00:06:48,714
lot of things you can see at one glance.

107
00:06:48,762 --> 00:06:52,106
And of course you can drill down here and look at individual dags,

108
00:06:52,138 --> 00:06:55,214
individual tasks, down to the very logs

109
00:06:55,262 --> 00:06:57,890
of the python script that you are executing.

110
00:06:59,830 --> 00:07:04,334
Okay, I already said what a dag is. A dag is comprised

111
00:07:04,382 --> 00:07:07,586
of the different tasks. So on the right hand side here we have

112
00:07:07,608 --> 00:07:11,126
a very small dag, just with two tasks. And you can see it's also a

113
00:07:11,148 --> 00:07:14,646
graph. It has two nodes. The nodes are the tasks and the edge is the

114
00:07:14,668 --> 00:07:18,594
dependency between them. It's a little arrow here. This just means the extract

115
00:07:18,642 --> 00:07:21,686
task needs to happen before the write to minio task,

116
00:07:21,798 --> 00:07:25,066
and by default the upstream task needs

117
00:07:25,088 --> 00:07:28,460
to be successful. But there's a lot of customizability here.

118
00:07:28,990 --> 00:07:33,454
On the left hand side you can see all of the code that went into

119
00:07:33,652 --> 00:07:37,166
creating this tiny dag. Of course, in production and in

120
00:07:37,188 --> 00:07:40,814
real life use cases, your dags are often much longer. But here

121
00:07:40,852 --> 00:07:44,510
you can see really everything that is needed to create one of these dags.

122
00:07:45,250 --> 00:07:48,390
On line eight, you can see where we define the DAC.

123
00:07:48,490 --> 00:07:51,842
So to tell airflow, hey, I want to make a new dag, you just

124
00:07:51,896 --> 00:07:55,698
add a decorator. The add Dac decorator on top of a

125
00:07:55,704 --> 00:07:59,302
python function and that will tell airflow. Everything that's in this function

126
00:07:59,356 --> 00:08:03,014
is a DAC. I only have to give it free parameters. Here I

127
00:08:03,052 --> 00:08:06,966
say this should happen every day, and I want it to start at the

128
00:08:06,988 --> 00:08:10,550
1 January this year. Then within that

129
00:08:10,620 --> 00:08:13,706
python function on line 13 we have two tasks that

130
00:08:13,728 --> 00:08:17,114
are defined, and I on purpose am showing the two

131
00:08:17,152 --> 00:08:20,714
different ways that you can define airflow tasks. So on line

132
00:08:20,752 --> 00:08:24,382
14 you can see I have an add task decorator and

133
00:08:24,436 --> 00:08:27,886
I put this on a Python function. And you might be thinking

134
00:08:27,988 --> 00:08:32,254
that just looks like a normal, like any regular python function. It's very true.

135
00:08:32,372 --> 00:08:35,486
You could take your current ML script that you

136
00:08:35,508 --> 00:08:38,610
have, put the add task decorator on top of it,

137
00:08:38,680 --> 00:08:41,682
and then it's a task in airflow. And if that's all you need,

138
00:08:41,736 --> 00:08:45,494
if you have several scripts and you want to run them in a certain order

139
00:08:45,532 --> 00:08:49,046
or with certain dependencies, then you know now

140
00:08:49,148 --> 00:08:51,910
everything you need to know to make that happen in airflow.

141
00:08:52,570 --> 00:08:56,214
Of course, sometimes there's a lot more customizability here.

142
00:08:56,332 --> 00:08:59,606
And the other great strength of airflow

143
00:08:59,638 --> 00:09:03,894
are predefined operators. An operator is simply

144
00:09:03,942 --> 00:09:07,318
a class that we use to create a task.

145
00:09:07,414 --> 00:09:10,922
So on line 19 you can see I have an operator that is called

146
00:09:10,976 --> 00:09:14,206
local file system to minio operator. This is actually

147
00:09:14,228 --> 00:09:17,646
an operator that I import locally on line free. You can see I

148
00:09:17,668 --> 00:09:21,838
import it from my own include folder because I made this for

149
00:09:21,924 --> 00:09:25,326
a use case and it didn't exist. And you can see I

150
00:09:25,348 --> 00:09:29,426
have a class and I only give it a few parameters. I give

151
00:09:29,448 --> 00:09:32,606
it a task id, which is what airflow will call the task in the UI

152
00:09:32,638 --> 00:09:36,098
and internally in the database. And then I give it some information,

153
00:09:36,264 --> 00:09:39,974
a bucket and an object name. And everything else happens under

154
00:09:40,012 --> 00:09:43,318
the hood. So the whole call to the minio API happens

155
00:09:43,404 --> 00:09:47,206
in this class that has been defined anywhere. And this is

156
00:09:47,228 --> 00:09:51,002
a strength because you have these predefined operators for many,

157
00:09:51,056 --> 00:09:54,646
many data tools. And if someone has created

158
00:09:54,678 --> 00:09:58,474
an operator for what you need, then you can simply use that,

159
00:09:58,512 --> 00:10:01,100
and you don't have to redefine the wheel every time.

160
00:10:03,790 --> 00:10:07,726
All right, that was a very, very simple dag. But of course dags can

161
00:10:07,748 --> 00:10:11,354
be as complex as you want them to be. Here you can see a DAC

162
00:10:11,402 --> 00:10:15,060
that is a little more complex. Of course these can get very big,

163
00:10:15,430 --> 00:10:19,170
but you can see here we have more complex dependencies. We can group

164
00:10:19,240 --> 00:10:22,898
tasks. So there's a lot of flexibility here as long

165
00:10:23,064 --> 00:10:25,650
as it stays directed and asyclic.

166
00:10:27,450 --> 00:10:31,794
Why should you use airflow? You can orchestrates data pipelines

167
00:10:31,842 --> 00:10:35,346
and ML pipelines in the same place, because airflow

168
00:10:35,378 --> 00:10:38,774
is tool agnostic. That means you can talk

169
00:10:38,812 --> 00:10:42,342
to any tool that has an API. Anything that you can do in python

170
00:10:42,406 --> 00:10:45,722
you can do in airflow so you don't have to use different

171
00:10:45,776 --> 00:10:49,926
tools anymore, and you can start creating complex dependencies.

172
00:10:50,038 --> 00:10:53,502
A lot of times you have an ML pipeline that you want to start once

173
00:10:53,556 --> 00:10:57,150
certain data is available, but maybe you want data quality

174
00:10:57,220 --> 00:11:00,878
checking to happen on that data. And the ML part of the

175
00:11:00,884 --> 00:11:04,580
pipelines should only start if the data quality check has a certain result

176
00:11:05,110 --> 00:11:08,494
and all sorts of complex dependencies

177
00:11:08,542 --> 00:11:12,098
like that. There's also fully functional API for airflow, so you

178
00:11:12,104 --> 00:11:16,020
can even start kicking off an ML pipeline by a button press on a website.

179
00:11:16,710 --> 00:11:20,086
I already said it's tool agnostic, and this is very important.

180
00:11:20,188 --> 00:11:23,446
And I realize every day more and more how important that

181
00:11:23,468 --> 00:11:27,014
is in the current data field. I feel like I wake up every day

182
00:11:27,052 --> 00:11:30,490
and there's a new ML tool and a new model and a new website

183
00:11:30,560 --> 00:11:34,198
springing up that I might want to use. And if you're using airflow

184
00:11:34,214 --> 00:11:37,900
for your orchestration, that will never be a problem

185
00:11:38,270 --> 00:11:42,090
because as long as it has some API, some way to call it,

186
00:11:42,240 --> 00:11:46,154
you can connect it to your airflow instance. And you don't have to redefine

187
00:11:46,202 --> 00:11:50,234
your whole pipeline just because a new tool doesn't natively integrate

188
00:11:50,282 --> 00:11:53,726
with another tool that you're already using. So this plug and play style

189
00:11:53,758 --> 00:11:57,506
of data orchestration is really important in the current data sphere now.

190
00:11:57,688 --> 00:12:00,450
And the third point, it is event driven.

191
00:12:03,110 --> 00:12:06,258
You can schedule dags to happen or tasks to happen

192
00:12:06,344 --> 00:12:09,606
based on events that are happening in your other data tools. I will

193
00:12:09,628 --> 00:12:13,190
show you what I mean by that in the pipeline walkthrough

194
00:12:13,770 --> 00:12:16,934
I said it's all just Python code. You can customize it,

195
00:12:16,972 --> 00:12:20,998
you can plug and play operators, create your own, you can extend upon airflow,

196
00:12:21,014 --> 00:12:24,266
it's open source. If you do something useful, please share it

197
00:12:24,288 --> 00:12:27,980
back to the community. And for ML, really important.

198
00:12:28,350 --> 00:12:32,334
There are already pre built operators that help you to

199
00:12:32,372 --> 00:12:35,338
run dedicated tasks in Kubernetes pods.

200
00:12:35,434 --> 00:12:39,646
So if you have a Kubernetes cluster like

201
00:12:39,668 --> 00:12:42,986
you saw, I put the at task decorator on top of a python

202
00:12:43,018 --> 00:12:46,318
function. I could also put at task Kubernetes. I have

203
00:12:46,324 --> 00:12:49,694
to give it a few more parameters in that case to define

204
00:12:49,742 --> 00:12:53,486
which Kubernetes cluster I'm sending the task to. But that way you can spin

205
00:12:53,518 --> 00:12:56,518
up your task in a dedicated pod. For example,

206
00:12:56,604 --> 00:13:00,310
if you need heavy gpu just for one task in your pipeline.

207
00:13:02,010 --> 00:13:05,106
Okay, that was the crash course for Apache

208
00:13:05,138 --> 00:13:08,614
Airflow. Now I'm going to jump back a little

209
00:13:08,652 --> 00:13:11,370
in my life and talk a little bit about medicine.

210
00:13:12,190 --> 00:13:16,054
So the data I was using, I was thinking, what is something I'm familiar

211
00:13:16,102 --> 00:13:19,370
with? And I've worked in neurosurgery before, so I decided to go

212
00:13:19,440 --> 00:13:22,814
with brain tumors and there was a small little data set for that

213
00:13:22,852 --> 00:13:27,370
on Kaggle. I've looked at gliomas versus meningiomas.

214
00:13:27,450 --> 00:13:30,878
You don't have to know anything about that, really. A glioma is a

215
00:13:30,884 --> 00:13:34,286
brain tumor coming from glial cells, so from the structural

216
00:13:34,318 --> 00:13:37,714
cells in your brain. And a meningioma is a brain tumor coming from

217
00:13:37,752 --> 00:13:42,466
arachnoidal cap cells. The arachnoid is a membrane that

218
00:13:42,488 --> 00:13:45,986
is surrounding your brain. So those are two different tumors that

219
00:13:46,008 --> 00:13:49,506
are coming from different cells. And of course they have all subclasses,

220
00:13:49,538 --> 00:13:53,590
but I was just looking at those two large buckets of kinds of tumour.

221
00:13:53,930 --> 00:13:57,474
I had a little more over 300 t two weighted images

222
00:13:57,522 --> 00:14:01,530
of both. T two weighted is just one type of MRI image.

223
00:14:02,030 --> 00:14:05,706
You can tell it's probably t two weighted if fluids are bright in the

224
00:14:05,728 --> 00:14:09,722
image. I did a train test, split of about a quarter being

225
00:14:09,776 --> 00:14:13,322
test, which left me with a test set with around 80

226
00:14:13,386 --> 00:14:17,434
pictures of each and a train set with 260 gliomas

227
00:14:17,482 --> 00:14:21,534
and 247 meningiomas. Now, just so

228
00:14:21,572 --> 00:14:25,322
you can imagine what my data looked like, I only had slices,

229
00:14:25,386 --> 00:14:28,594
so only pictures, not a full MRI. And here

230
00:14:28,632 --> 00:14:31,838
you can see. Sometimes the differentiation between the two types

231
00:14:31,854 --> 00:14:35,326
of brain tumors is fairly easy. This was one of the reason why I picked

232
00:14:35,358 --> 00:14:38,694
these two types, because I was thinking this is something that you might

233
00:14:38,732 --> 00:14:41,830
be able to tell from one picture. Of course,

234
00:14:41,900 --> 00:14:45,538
never conclusively. You would never make a conclusive diagnosis

235
00:14:45,634 --> 00:14:48,838
from one picture or just from an MRI. In general, you would always wait for

236
00:14:48,844 --> 00:14:52,250
the biopsy. But it is something that I could imagine an ML model

237
00:14:52,320 --> 00:14:55,674
could be learning. Here on the left

238
00:14:55,712 --> 00:14:58,650
hand side, you can see the meningioma. If you remember,

239
00:14:58,720 --> 00:15:02,406
I said, it comes from cells that are part of the membrane surrounding

240
00:15:02,438 --> 00:15:05,534
your brain, so it has to touch that membrane at one point.

241
00:15:05,572 --> 00:15:09,182
And this is a very typical picture here. And on the right hand side,

242
00:15:09,236 --> 00:15:12,686
you can see a glioma, you can see this one started in

243
00:15:12,708 --> 00:15:16,450
the middle of the brain, you can see all the edema around it.

244
00:15:16,600 --> 00:15:20,286
It looks a little angry, what we would have said in radiology,

245
00:15:20,478 --> 00:15:23,470
because this looks quite malign, which unfortunately,

246
00:15:23,550 --> 00:15:26,910
gliomas are often malign. Meningiomas are more often benign.

247
00:15:26,990 --> 00:15:30,790
But of course, both entities have benign and malign types.

248
00:15:31,290 --> 00:15:34,886
Now, this was an example of one picture where I, on the

249
00:15:34,908 --> 00:15:38,534
first glass said, yeah, I'm pretty sure what I would classify it as,

250
00:15:38,652 --> 00:15:42,726
but it's not always that simple. There were also examples where the comparison

251
00:15:42,758 --> 00:15:46,186
was much harder. Here again, on the left hand side that

252
00:15:46,368 --> 00:15:49,834
later turned out to be a meningioma, and on the right hand

253
00:15:49,872 --> 00:15:52,270
side that turned out to be a glioma.

254
00:15:54,290 --> 00:15:57,966
Okay, this was the data I had, and with that I

255
00:15:57,988 --> 00:16:01,758
got started with the pipeline. Now I will walk through the pipeline, but of

256
00:16:01,764 --> 00:16:05,626
course, if you want to see and play with the whole code, it's on GitHub.

257
00:16:05,658 --> 00:16:08,754
I will post that link in the discord as well, and you can clone that

258
00:16:08,792 --> 00:16:12,100
and use it as a blueprint for your own pipelines if you want to.

259
00:16:13,910 --> 00:16:16,580
First, I want to cover the tools that I used.

260
00:16:17,190 --> 00:16:20,994
So obviously I used Airflow and I used the astropython SDK,

261
00:16:21,042 --> 00:16:24,230
which is an open source package on top of airflow.

262
00:16:24,570 --> 00:16:28,054
I stored all my files and images in s free, but you could use any

263
00:16:28,092 --> 00:16:31,542
blob storage, of course that you want to, or even store them just locally.

264
00:16:31,686 --> 00:16:35,670
I used a duckdB instance to store references

265
00:16:35,750 --> 00:16:39,546
to my files and store the results of my different models. And then in

266
00:16:39,568 --> 00:16:43,040
the end pick the model that performed best for a specific test set.

267
00:16:43,570 --> 00:16:47,338
I decided to go with hugging phase for the ML heavy parts,

268
00:16:47,434 --> 00:16:50,990
and specifically I decided to pick the Microsoft Resnet 50

269
00:16:51,060 --> 00:16:54,926
model and fine tune that because I was thinking that's a very general model.

270
00:16:55,028 --> 00:16:58,046
I'm curious how well it performs before it's fine tuned

271
00:16:58,078 --> 00:17:01,970
and after on a very specific data set. Of course,

272
00:17:02,040 --> 00:17:05,746
since I was working with airflow, I was doing everything in Python, and in the

273
00:17:05,768 --> 00:17:09,206
end I wanted to have some way to be alerted of what's happening

274
00:17:09,308 --> 00:17:11,240
and I picked slack for that.

275
00:17:13,210 --> 00:17:16,966
Okay, again, you can see a DAG view, but this time of

276
00:17:16,988 --> 00:17:20,746
the actual airflow environment that I was using for

277
00:17:20,768 --> 00:17:25,350
this pipelines. And what you can see here, in total I have eight dags,

278
00:17:25,430 --> 00:17:29,034
so eight separate workflows, and all of them

279
00:17:29,072 --> 00:17:32,330
have a very varying different number of tasks.

280
00:17:32,830 --> 00:17:36,382
And what you can see here is what it looks like while it's running.

281
00:17:36,516 --> 00:17:40,090
And I have this overview. I know exactly when each DAC ran

282
00:17:40,170 --> 00:17:43,620
the last time and when it's scheduled to run next.

283
00:17:45,270 --> 00:17:49,262
Talking about the schedule, you might have heard about airflow

284
00:17:49,326 --> 00:17:52,834
being an advancement of Cron, and it's still very much

285
00:17:52,872 --> 00:17:56,354
possible to schedule your DAX on a cron string. But since then,

286
00:17:56,392 --> 00:18:00,114
Airflow has come a long way and I've actually scheduled

287
00:18:00,162 --> 00:18:03,350
most of the DAX in this repository on

288
00:18:03,420 --> 00:18:07,046
data. So what I did is here you can see what we call

289
00:18:07,068 --> 00:18:10,442
the data sets view in green

290
00:18:10,576 --> 00:18:14,470
or slightly bluish. Depending on your monitor. You can see the DAX

291
00:18:14,630 --> 00:18:18,314
and you can see in orange the data set.

292
00:18:18,432 --> 00:18:22,186
And what a data set is in a nutshell is if you imagine you

293
00:18:22,208 --> 00:18:25,594
have an airflow task and you know that airflow tasks,

294
00:18:25,642 --> 00:18:28,682
for example, updates a table in a database,

295
00:18:28,826 --> 00:18:31,934
then you can tell that task, hey, I know you are updating that

296
00:18:31,972 --> 00:18:36,034
table. Please, whenever you complete successfully, raise a little

297
00:18:36,072 --> 00:18:39,794
flag that tells my whole airflow environment that

298
00:18:39,832 --> 00:18:43,362
this table just has been updated. And then on the other side,

299
00:18:43,496 --> 00:18:47,426
I can tell other DAX, hey, whenever you see that flag or whenever you

300
00:18:47,448 --> 00:18:50,886
see that combination of flags, I want you to run. I want that

301
00:18:50,908 --> 00:18:54,070
whole other part of the pipelines to start. This is exactly

302
00:18:54,140 --> 00:18:57,414
what I did. So you can see here I have two dacs that are not

303
00:18:57,452 --> 00:19:01,002
scheduled on data sets. The in new test data and in new brain

304
00:19:01,056 --> 00:19:04,394
data DAC. And each of those DAX has one

305
00:19:04,432 --> 00:19:08,234
task that updates what I call a data set. So each of those

306
00:19:08,272 --> 00:19:12,202
DAX will eventually put new data in a

307
00:19:12,256 --> 00:19:15,674
folder and that folder is my data set. So I decided,

308
00:19:15,802 --> 00:19:19,470
hey, when I have new test data, I want that task to say,

309
00:19:19,540 --> 00:19:23,146
I have new test data. Raise that little flag. And the preprocessed

310
00:19:23,178 --> 00:19:27,074
test data DAC, which is downstream, is being

311
00:19:27,112 --> 00:19:30,674
told to run every time there is new data in that s free

312
00:19:30,712 --> 00:19:34,398
bucket. And that's how I started to chain my whole DAX.

313
00:19:34,494 --> 00:19:38,322
I will not go through the whole list here, but for example, on the right

314
00:19:38,376 --> 00:19:41,814
side, on the lower part of the screen, you can see that if I

315
00:19:41,852 --> 00:19:45,426
trained my model, I did the exact same thing. I told that task that trains

316
00:19:45,458 --> 00:19:48,978
the model. Hey, whenever you're successful and when you're done, raise a little

317
00:19:49,004 --> 00:19:52,534
flag that says new model has been trained. And then the testing

318
00:19:52,582 --> 00:19:56,362
part, the part that tests my fine tuned model runs every time

319
00:19:56,416 --> 00:19:57,930
that flag has been raised.

320
00:20:00,510 --> 00:20:03,580
I said, two dacs are not scheduled on a data set.

321
00:20:03,950 --> 00:20:07,774
I actually decided for two dacs that I want them to always run.

322
00:20:07,892 --> 00:20:11,326
And I'm very excited about this. You might be able to tell because this is

323
00:20:11,348 --> 00:20:15,326
a super new feature. This came out, I think, two weeks ago with airflow

324
00:20:15,358 --> 00:20:18,318
2.6. It's called the continuous schedule.

325
00:20:18,414 --> 00:20:21,998
And what this means is if you set the schedule to add continuous,

326
00:20:22,094 --> 00:20:25,614
the DAC will always run. So you have a DAC

327
00:20:25,662 --> 00:20:29,618
that's always there, always running, and whenever it completes successfully,

328
00:20:29,714 --> 00:20:33,702
it will automatically start its next run. So now I can have

329
00:20:33,836 --> 00:20:37,014
my DAX that are always listening for something to happen and

330
00:20:37,052 --> 00:20:40,246
always running. These are

331
00:20:40,268 --> 00:20:44,186
the two Dax that are always running. They look very similar. They are

332
00:20:44,208 --> 00:20:47,578
mirrors of each other. One for the training data, one for the testing data.

333
00:20:47,664 --> 00:20:51,418
Because I was thinking in my case I would assume that

334
00:20:51,584 --> 00:20:55,114
if a new MRI file comes in that it would be filed

335
00:20:55,162 --> 00:20:58,174
either to go into the training or the testing data,

336
00:20:58,292 --> 00:21:01,630
depending on certain other properties. So I'm not doing a random split here.

337
00:21:01,700 --> 00:21:05,154
I assume that someone with domain knowledge would make that decision. Where this

338
00:21:05,192 --> 00:21:08,834
file goes and what's happening here is on the left

339
00:21:08,872 --> 00:21:12,546
hand side you can see the wait for training data, wait for new testing data

340
00:21:12,728 --> 00:21:16,338
task. And these are waiting for the new task for

341
00:21:16,344 --> 00:21:20,006
the new data to come in. Now I

342
00:21:20,028 --> 00:21:23,714
said I'm using s free. So what is happening under the hood

343
00:21:23,762 --> 00:21:27,906
is probably something all of you have done before. A call to the Bodocore API.

344
00:21:28,018 --> 00:21:31,354
But I did not have to think about bodocore a single time while

345
00:21:31,392 --> 00:21:34,682
creating this pipeline, because someone has done that for me.

346
00:21:34,736 --> 00:21:38,634
Very grateful. At this point there is a

347
00:21:38,832 --> 00:21:42,362
sensor, an operator called s free key

348
00:21:42,416 --> 00:21:46,334
sensor async, which is a deferrable operator. This means that

349
00:21:46,372 --> 00:21:50,190
this task, once it starts, it is waiting for something.

350
00:21:50,340 --> 00:21:54,014
In this case I just tell it exactly what key to

351
00:21:54,052 --> 00:21:57,342
wait for, in what bucket and in what AWS

352
00:21:57,406 --> 00:22:01,614
account. You can see AWS con id. That's where I store my credentials.

353
00:22:01,662 --> 00:22:05,446
So the credentials are hidden here and make the connection to

354
00:22:05,468 --> 00:22:08,886
my AWS environment. And as soon as this

355
00:22:08,908 --> 00:22:12,534
task starts waiting, it is actually not being run by

356
00:22:12,572 --> 00:22:16,326
an airflow worker anymore. It is being put into

357
00:22:16,428 --> 00:22:19,554
a completely different process called the triggerer.

358
00:22:19,682 --> 00:22:22,754
And this process runs in an async fashion

359
00:22:22,802 --> 00:22:25,978
and frees up your worker slot, which, as you can imagine, if you have a

360
00:22:25,984 --> 00:22:29,386
lot of these tasks running, can save you a ton of resources and make your

361
00:22:29,408 --> 00:22:33,246
pipelines much more efficient. So that was the first thing I wanted to show you.

362
00:22:33,428 --> 00:22:37,630
If you have sensors running, always check if there's an asynchronous version.

363
00:22:38,930 --> 00:22:42,766
Now let's imagine I have a new file. It comes in. What I

364
00:22:42,788 --> 00:22:45,646
want to happen next is I want to have the list of all the new

365
00:22:45,668 --> 00:22:49,442
files. And then I copy the files over. And you see here I have square

366
00:22:49,496 --> 00:22:53,074
brackets with a number in it. And this is because I

367
00:22:53,112 --> 00:22:56,834
decided I want to know for each of the individual files if my

368
00:22:56,872 --> 00:23:00,454
copy statement was successful, because I want to backtrack in case maybe

369
00:23:00,492 --> 00:23:04,294
someone had put in a file with

370
00:23:04,332 --> 00:23:08,034
a different file ending, or maybe something else went wrong with a corrupted file.

371
00:23:08,082 --> 00:23:10,834
I want to be able to backtrack that for every single file.

372
00:23:10,962 --> 00:23:14,906
And I did that by using dynamic task mapping. I did not write out

373
00:23:15,008 --> 00:23:18,746
400 tasks, especially because I never know how many new data will

374
00:23:18,768 --> 00:23:22,054
come in. This time I had 400 pictures. Maybe tomorrow

375
00:23:22,102 --> 00:23:25,982
it's just one. So I need a varying number of tasks every

376
00:23:26,036 --> 00:23:29,760
time this diagrams and I definitely don't want to change the code every time.

377
00:23:30,450 --> 00:23:33,994
This is where dynamic tasks come in and dynamic tasks

378
00:23:34,042 --> 00:23:37,518
are a way to make your a flow pipelines more dynamic.

379
00:23:37,614 --> 00:23:41,234
I will not have time to talk in depth about how to use

380
00:23:41,272 --> 00:23:44,930
dynamic task mapping. There are a lot of resources online,

381
00:23:45,080 --> 00:23:48,902
there's a full length webinar as well. But what you can see here

382
00:23:48,956 --> 00:23:52,082
is we have an operator.

383
00:23:52,146 --> 00:23:56,146
So on line 73 we have the s free copy object operator.

384
00:23:56,258 --> 00:23:59,858
Again, someone already did all the work for me. I just had to plug that

385
00:23:59,884 --> 00:24:03,306
operator from the provider package and I say,

386
00:24:03,408 --> 00:24:07,082
well, for each of the instances of this operator I

387
00:24:07,136 --> 00:24:11,018
always use the same connection, I always connect to the same AWS account.

388
00:24:11,184 --> 00:24:15,034
But for each instance I have a different source bucket

389
00:24:15,082 --> 00:24:18,942
and a different destination bucket key that's on line 70 and

390
00:24:18,996 --> 00:24:22,494
I pass in a list of dictionaries and for each

391
00:24:22,532 --> 00:24:25,778
of those dictionaries a copied task will be made.

392
00:24:25,864 --> 00:24:29,554
And this happens at runtime. So the decision how

393
00:24:29,592 --> 00:24:33,474
many tasks will happen is always done once this

394
00:24:33,512 --> 00:24:37,042
task actually has to run, and that's how

395
00:24:37,096 --> 00:24:40,486
it can have one task instance maybe one day if there

396
00:24:40,508 --> 00:24:44,022
was only one new file, or it can have 100 or even 1000

397
00:24:44,076 --> 00:24:47,958
the next day without me having to change any code. If you're interested in that,

398
00:24:48,044 --> 00:24:51,210
I encourage you to look more into dynamic tasks.

399
00:24:52,750 --> 00:24:55,466
This is what it looks like in the UI you can see here. All my

400
00:24:55,488 --> 00:24:58,986
mapped tasks, all of them were success in copying my

401
00:24:59,008 --> 00:25:02,494
file. All right, we have

402
00:25:02,532 --> 00:25:05,418
our data in the right folder in the US free bucket.

403
00:25:05,514 --> 00:25:09,022
Now we do preprocessing. First little

404
00:25:09,076 --> 00:25:12,634
side note, I'm creating the task called create DuckdB.

405
00:25:12,682 --> 00:25:16,418
Pool actually doesn't create a pool in the database sense

406
00:25:16,584 --> 00:25:20,014
that you might be familiar with. This creates an airflow pool,

407
00:25:20,142 --> 00:25:23,874
which is an airflow specific feature that is a collection of

408
00:25:23,912 --> 00:25:28,190
worker slots. And I can say I have a certain number of tasks

409
00:25:28,270 --> 00:25:31,686
and those tasks can be anywhere. In my whole airflow instance they

410
00:25:31,708 --> 00:25:34,978
can be in separate DAX and I can tell all of those tasks,

411
00:25:35,074 --> 00:25:37,938
you only get to use these worker slots.

412
00:25:38,114 --> 00:25:42,390
This is used to better handle resources to, for example group tasks

413
00:25:42,470 --> 00:25:45,994
that you never want to use too many worker slots at the same

414
00:25:46,032 --> 00:25:49,494
time. But it's also super handy if you have tasks

415
00:25:49,542 --> 00:25:52,800
that you never want to have run at the same time

416
00:25:53,170 --> 00:25:56,526
because in this case I'm writing to duckdb a few

417
00:25:56,548 --> 00:26:00,186
times, and I cannot do that synchronously.

418
00:26:00,218 --> 00:26:03,454
I cannot have two processes handling writing to the same

419
00:26:03,492 --> 00:26:07,226
duckdb instances at the same time. So what I'm doing is I'm

420
00:26:07,258 --> 00:26:10,846
creating a duckdb pool with one slot, and throughout

421
00:26:10,878 --> 00:26:14,274
the whole airflow instance, all the tasks that are writing some

422
00:26:14,312 --> 00:26:17,894
information to DuckdB, I tell them you can only use that one pool, only that

423
00:26:17,932 --> 00:26:21,510
one slot, and my parallelism issues are solved.

424
00:26:22,250 --> 00:26:25,778
Next, I list all the files in my training bucket

425
00:26:25,874 --> 00:26:29,286
and then I create a table in duckdb and I load both

426
00:26:29,308 --> 00:26:32,634
the references to the files. So the key to the file name

427
00:26:32,752 --> 00:26:36,650
and the labels that in this case I'm fetching it from the file name.

428
00:26:36,720 --> 00:26:40,518
But of course in a use case you probably have a different way that you're

429
00:26:40,534 --> 00:26:44,154
getting your labels. Maybe it's stored in a different database,

430
00:26:44,202 --> 00:26:47,646
maybe it's something that someone does manually. But there's another task that

431
00:26:47,668 --> 00:26:51,486
fetches the labels, and I store both the labels and the references to the

432
00:26:51,508 --> 00:26:54,210
file together in the same table in duckdb.

433
00:26:55,270 --> 00:26:59,442
Here I want to quickly zoom into the list train files task because again,

434
00:26:59,496 --> 00:27:03,262
that's something that you would need to call the AWS API

435
00:27:03,326 --> 00:27:07,042
for. But here there's actually an even handier thing,

436
00:27:07,176 --> 00:27:11,098
and this is part of the Astro. This is the first part of the astro

437
00:27:11,134 --> 00:27:14,626
that I want to show you in this talk. In general, the Astro

438
00:27:14,658 --> 00:27:18,950
is an open source tool for next generation DAC offering, and it tries

439
00:27:19,030 --> 00:27:23,286
to create tasks or utility functions

440
00:27:23,398 --> 00:27:27,574
that are tool agnostic in itself. So if you remember earlier,

441
00:27:27,622 --> 00:27:31,526
I was using an s free copy object operator. So an operator

442
00:27:31,558 --> 00:27:35,182
that is specific to s free, if I would switch to a different

443
00:27:35,236 --> 00:27:38,510
kind of blob storage, I would have to switch out the operator.

444
00:27:38,850 --> 00:27:42,270
But in this case here I'm using the get file list function

445
00:27:42,340 --> 00:27:45,938
from the astropython SDK and I just point it towards

446
00:27:46,024 --> 00:27:49,406
the connection to my blob storage. But if tomorrow

447
00:27:49,598 --> 00:27:53,118
someone decides that we are moving from AWS to Azure,

448
00:27:53,294 --> 00:27:56,222
the only thing that I would need to do is give it a new path

449
00:27:56,286 --> 00:27:59,862
and give it a new connection id to Azure. I wouldn't have to change

450
00:27:59,916 --> 00:28:03,382
any other thing. And that is very plug and play

451
00:28:03,436 --> 00:28:07,414
because as you see, I use a variable for my connection Id. So with

452
00:28:07,452 --> 00:28:10,826
one change of a variable I could change any task that is

453
00:28:10,848 --> 00:28:14,070
using the astropython SDK from one to another blob storage,

454
00:28:14,150 --> 00:28:17,562
and also from one to another relational storage. If those

455
00:28:17,616 --> 00:28:20,954
storages are supported by the SDK. If that is something

456
00:28:20,992 --> 00:28:25,210
that sounds interesting to you now, mainly speaking to all the data engineers listening

457
00:28:25,290 --> 00:28:28,954
because this solves so much pain and it has a ton more features and functions,

458
00:28:29,002 --> 00:28:31,680
like merging in tables together, for example.

459
00:28:32,050 --> 00:28:35,970
Then I encourage you to check out the Astro Python SDK docs.

460
00:28:37,830 --> 00:28:41,394
All right, we arrived finally at training the model. So far,

461
00:28:41,432 --> 00:28:44,946
everything was kind of data engineering and not really mlops yet, but now

462
00:28:44,968 --> 00:28:48,674
we are training a model. I'm getting my image keys from my duckdb

463
00:28:48,722 --> 00:28:52,614
storage. So this is also something that could be modified if you had

464
00:28:52,652 --> 00:28:56,166
any other process that decides which image to use in

465
00:28:56,188 --> 00:28:59,738
a given training session. But here, I'm just getting all of them.

466
00:28:59,824 --> 00:29:03,062
I'm loading all the training images into a temporary folder.

467
00:29:03,126 --> 00:29:07,210
I get my labels. I train my classifier using a lot of huggingface

468
00:29:07,790 --> 00:29:11,086
technology, and then I delete the files again from

469
00:29:11,108 --> 00:29:12,590
my temporary folder.

470
00:29:14,770 --> 00:29:18,366
And this is where the custom operator came

471
00:29:18,388 --> 00:29:22,522
in. So what I first did was just put the add task decorator

472
00:29:22,586 --> 00:29:25,778
on top of a script and had it running and it worked fine. But I

473
00:29:25,784 --> 00:29:29,874
was thinking there's actually more I can do here because maybe I

474
00:29:29,912 --> 00:29:33,106
want to reuse, maybe I have another project where

475
00:29:33,128 --> 00:29:36,754
I want to have a binary image classification and

476
00:29:36,792 --> 00:29:39,938
use houging face. And I want this to be more modular.

477
00:29:40,034 --> 00:29:43,814
So what I did here is I wrapped the whole model fine tuning in

478
00:29:43,852 --> 00:29:47,414
a custom operator. This operator doesn't exist publicly yet,

479
00:29:47,612 --> 00:29:50,760
but you can just copy paste the code from my repo, of course.

480
00:29:51,310 --> 00:29:54,966
But what I did here is I wrapped all of the code that you're familiar

481
00:29:54,998 --> 00:29:58,538
with and created this class that I just have to instantiate in

482
00:29:58,544 --> 00:30:01,722
my DaC code. So I have the model name

483
00:30:01,776 --> 00:30:05,630
here, I give it the file paths, I give it the labels. Learning rate,

484
00:30:05,700 --> 00:30:09,226
and all of the other hyperparameters tell it where to save the fine tuned

485
00:30:09,258 --> 00:30:12,814
model, give it the transformation functions, and this is good to go.

486
00:30:12,932 --> 00:30:16,594
And here, one thing that you can actually see is the very last line

487
00:30:16,632 --> 00:30:20,020
that says outlets to an airflow data set.

488
00:30:20,550 --> 00:30:24,002
This is what creates that little flag that I was talking about of the data

489
00:30:24,056 --> 00:30:27,634
set. Whenever this task completes successfully, the flag

490
00:30:27,682 --> 00:30:30,866
will go up to the airflow environment. Hey, we have a new trained

491
00:30:30,898 --> 00:30:34,294
model, and everything that I decide needs to happen in that case

492
00:30:34,412 --> 00:30:35,560
can start happening.

493
00:30:38,090 --> 00:30:41,466
Okay, I have another dag because I thought,

494
00:30:41,568 --> 00:30:45,126
I'm really curious how the model performs if it's not fine tuned.

495
00:30:45,158 --> 00:30:48,886
And I want to have a robust baseline, very similar structure loading

496
00:30:48,918 --> 00:30:53,166
in the images from s three, then running a test operator that

497
00:30:53,188 --> 00:30:56,686
I call test classifier. And in the end, I write the

498
00:30:56,708 --> 00:31:00,206
results to my duckdb table. So I created another table in my

499
00:31:00,228 --> 00:31:03,966
duckdb database and in this table. I want to store all the

500
00:31:03,988 --> 00:31:07,314
models with this result so I can very easily figure out

501
00:31:07,352 --> 00:31:10,180
which was the best model for a given test run.

502
00:31:12,390 --> 00:31:15,774
Here I want to zoom in into the testing operator.

503
00:31:15,902 --> 00:31:19,302
I did the same thing again. I took all my test code and

504
00:31:19,356 --> 00:31:22,774
wrapped it into a custom operator. So I simply have to provide

505
00:31:22,892 --> 00:31:26,486
some parameters. There are more parameters that I just set

506
00:31:26,508 --> 00:31:30,126
a default to here. But now my Dax

507
00:31:30,178 --> 00:31:33,514
file stays clean and I have all my testing code in

508
00:31:33,552 --> 00:31:37,354
a separate file doing

509
00:31:37,392 --> 00:31:40,458
the same thing to the fine tuned model. Of course, that's the

510
00:31:40,464 --> 00:31:44,080
core thing that we're all always waiting for and watching the logs for.

511
00:31:44,450 --> 00:31:47,998
I get the latest fine tuned model so I'm storing all my

512
00:31:48,004 --> 00:31:51,518
fine tuned model in a local directory, but I want to

513
00:31:51,524 --> 00:31:55,006
get the latest for each test run again, loading the test images,

514
00:31:55,118 --> 00:31:58,798
testing my model and then write the results to duct

515
00:31:58,814 --> 00:32:01,970
Db and deleting the images from the temporary folder.

516
00:32:03,270 --> 00:32:06,594
The code here you can see. That's why it's so great to have things

517
00:32:06,632 --> 00:32:09,990
in a modular way. I used the same operator, same import,

518
00:32:10,140 --> 00:32:13,974
and this time I just gave it a few different parameters. Namely I

519
00:32:14,012 --> 00:32:17,586
gave it the result of the get latest fine tuned model task

520
00:32:17,618 --> 00:32:21,194
that was upstream. So I'm using a at

521
00:32:21,232 --> 00:32:26,186
task decorated function and return the

522
00:32:26,208 --> 00:32:29,366
name of the model that I want to use here and I can just plug

523
00:32:29,398 --> 00:32:32,634
that into model name and this will also automatically set

524
00:32:32,672 --> 00:32:36,814
the dependency correctly. Now what I wanted to highlight here is I

525
00:32:36,852 --> 00:32:40,160
talked about slack notifications and I was thinking,

526
00:32:40,770 --> 00:32:44,590
well, the thing I'm interested in is every time a

527
00:32:44,660 --> 00:32:47,758
model has been fine tuned and that model has been tested,

528
00:32:47,854 --> 00:32:51,522
I want to know how it's doing because maybe if it's not doing

529
00:32:51,576 --> 00:32:55,150
well, I have to go back and I have to change some habit parameters

530
00:32:55,230 --> 00:32:58,502
or change something on my code. So I decided to

531
00:32:58,556 --> 00:33:01,686
in this test classifier task I want

532
00:33:01,708 --> 00:33:05,106
to have a callback. It's an on success callback.

533
00:33:05,138 --> 00:33:07,910
So this runs whenever this task is successful.

534
00:33:08,490 --> 00:33:11,882
I could also have on failure callbacks. Of course, it's actually

535
00:33:11,936 --> 00:33:15,078
very common for people. We also have that in our internal

536
00:33:15,094 --> 00:33:18,730
data pipelines to have on failure callbacks that send you an email

537
00:33:18,800 --> 00:33:22,494
if something goes wrong with something you are responsible for. But in this

538
00:33:22,532 --> 00:33:26,442
case I wanted to be notified if the task was successful.

539
00:33:26,586 --> 00:33:28,800
I want to get a slack message.

540
00:33:30,210 --> 00:33:33,674
This is what that slack message looked like. This is fully customizable.

541
00:33:33,722 --> 00:33:36,740
So this is just what I decided I want to have in this message.

542
00:33:37,190 --> 00:33:41,170
And I'm pulling from upstream tasks. I'm pulling results here.

543
00:33:41,320 --> 00:33:45,166
I'm pulling the result of my fine tuned model test and started reading

544
00:33:45,198 --> 00:33:48,930
average test loss zero. Okay, that's too good. And yeah,

545
00:33:49,080 --> 00:33:52,114
I need to go back to the drawing board. That was one of the notifications

546
00:33:52,162 --> 00:33:55,302
while I was building this pipeline that sent me back to.

547
00:33:55,356 --> 00:33:59,574
Okay, hyperparameters completely out of whack. The accuracy is much

548
00:33:59,612 --> 00:34:02,954
worse than baseline and f one

549
00:34:02,992 --> 00:34:06,202
score. AUC is also much worse and average Tesla's of zero.

550
00:34:06,256 --> 00:34:09,526
I think this might be overfitting. So let's go back to the drawing

551
00:34:09,558 --> 00:34:10,140
board.

552
00:34:12,910 --> 00:34:16,334
And lastly, I have the last DAC in my whole

553
00:34:16,372 --> 00:34:19,902
pipeline. I didn't put the whole graph view because it's a very simple

554
00:34:19,956 --> 00:34:23,534
one, but it's a DaG that picks the best model and

555
00:34:23,572 --> 00:34:27,214
then deploys it. Doesn't actually deploy it yet because I know that code

556
00:34:27,252 --> 00:34:30,498
will look different for everyone. But I put in a task already that is

557
00:34:30,504 --> 00:34:34,226
called deploy model, where you could put in all your code that actually deploys to

558
00:34:34,248 --> 00:34:37,766
your solution. And that's the second place where I wanted to talk

559
00:34:37,788 --> 00:34:41,106
about the astropython SDK because here you can see I'm

560
00:34:41,138 --> 00:34:44,242
using at AQL something tasks.

561
00:34:44,386 --> 00:34:48,258
And what this signifies is that I'm using Astro

562
00:34:48,354 --> 00:34:51,730
convenience tasks or Astro SDK operators.

563
00:34:51,810 --> 00:34:55,190
The first one I'm using is at AQL transform,

564
00:34:55,350 --> 00:34:58,534
which is a way that you can turn any SQL

565
00:34:58,582 --> 00:35:01,754
query into a task that runs on any

566
00:35:01,792 --> 00:35:04,842
table that you put in. Here I have the intable,

567
00:35:04,906 --> 00:35:08,798
which is my results table, and I simply point that

568
00:35:08,884 --> 00:35:13,086
at a table in my relational storage. So on line 52 you

569
00:35:13,108 --> 00:35:16,482
can see that I'm pointing it at a table object

570
00:35:16,616 --> 00:35:19,986
and I again give it a connection id. And if I ever would

571
00:35:20,008 --> 00:35:24,306
decide I don't want to use duckdb anymore, maybe I'm using a

572
00:35:24,328 --> 00:35:28,290
snowflake database. I could simply point that at my snowflake instance

573
00:35:28,370 --> 00:35:31,046
and I wouldn't have to change any other code.

574
00:35:31,228 --> 00:35:34,886
So again, this is a way to make airflow much more modular and

575
00:35:34,988 --> 00:35:38,806
much more versatile and flexible if you ever change an underlying

576
00:35:38,838 --> 00:35:42,554
tool. And the second task takes in

577
00:35:42,672 --> 00:35:46,454
the returned table. So I'm running the SQL statement

578
00:35:46,502 --> 00:35:50,282
on my results table. This will return a temporary table in this case

579
00:35:50,336 --> 00:35:54,074
because I didn't give it an output table. And this temporary table is simply

580
00:35:54,122 --> 00:35:57,082
passed into the AQL data frame task,

581
00:35:57,146 --> 00:36:01,262
which automatically turns that relational table into

582
00:36:01,316 --> 00:36:04,698
a pandas data frame. So I can just print out the

583
00:36:04,724 --> 00:36:07,170
model name of the best model that I selected.

584
00:36:09,670 --> 00:36:13,266
All right, that was all of the pipelines. I know that was a lot,

585
00:36:13,368 --> 00:36:17,626
but I wanted to also show you what that looks like. So let's hop

586
00:36:17,678 --> 00:36:21,382
over to my airflow instance. And you can see here

587
00:36:21,436 --> 00:36:25,938
it's actually running. You can see there was one successful run of the whole pipeline

588
00:36:26,034 --> 00:36:29,046
because we have one past success run.

589
00:36:29,228 --> 00:36:32,794
And you also see I have all the Dax turned on and two

590
00:36:32,832 --> 00:36:36,346
of them are currently running. So these are the two dax that

591
00:36:36,368 --> 00:36:39,402
are using the add continuous schedule and they will

592
00:36:39,456 --> 00:36:42,974
always be running. And the first task, which was the s

593
00:36:43,012 --> 00:36:46,942
free key async operator, is in a deferred state here.

594
00:36:47,076 --> 00:36:50,586
So this task is using the triggerer

595
00:36:50,618 --> 00:36:54,306
component to wait for a file to drop in my

596
00:36:54,328 --> 00:36:57,458
s free bucket. So let's give it what it's waiting for.

597
00:36:57,624 --> 00:36:59,380
Hopping over to s free.

598
00:37:01,110 --> 00:37:03,954
And let's take a file here.

599
00:37:04,072 --> 00:37:07,222
Let's say I have a new meningioma that I want to train.

600
00:37:07,356 --> 00:37:10,502
I want to upgrade the train set, and then I want to retrain my model

601
00:37:10,556 --> 00:37:12,440
with this. Okay,

602
00:37:13,690 --> 00:37:18,374
uploaded. It takes

603
00:37:18,412 --> 00:37:21,770
a second and you can see it's not deferred anymore. Now this task is running.

604
00:37:21,840 --> 00:37:26,586
That was the center, and this will run all

605
00:37:26,608 --> 00:37:30,010
the downstream tasks. And the other thing that you saw is something

606
00:37:30,080 --> 00:37:34,142
happened here in this different dag. This is because this DAG is

607
00:37:34,196 --> 00:37:37,546
scheduled to happen as soon as this DAG

608
00:37:37,578 --> 00:37:41,566
has moved that file from the folder I dropped the

609
00:37:41,588 --> 00:37:44,794
new picture in into a different folder that is called

610
00:37:44,852 --> 00:37:48,866
train data ng. And now this part of the pipeline is

611
00:37:48,888 --> 00:37:51,938
starting. Can see here that actually has a

612
00:37:51,944 --> 00:37:55,986
lot of tasks scheduled, 450. So I'm not going to wait for

613
00:37:56,008 --> 00:37:59,746
all of this to happen. Just wanted you to see how the pipeline

614
00:37:59,778 --> 00:38:03,298
is kicked off. But once this DAG would be completed,

615
00:38:03,474 --> 00:38:06,774
the model training would start over here because again, I have

616
00:38:06,812 --> 00:38:10,298
scheduled it on a data set that is updated by a task in

617
00:38:10,304 --> 00:38:14,442
this environment. All right, we're not going to watch all of this happening,

618
00:38:14,576 --> 00:38:16,780
but we're going to jump into the code.

619
00:38:18,110 --> 00:38:21,706
So here you

620
00:38:21,728 --> 00:38:24,990
can see my visual code studio and

621
00:38:25,060 --> 00:38:28,698
you can see all the code that went into making this airflow

622
00:38:28,714 --> 00:38:31,598
environment. And this is also all the code that is going to be in the

623
00:38:31,604 --> 00:38:35,234
repository for you to download. On the left hand

624
00:38:35,272 --> 00:38:38,574
side, you can see what the airflow

625
00:38:38,702 --> 00:38:41,874
environment looks like. An Astro project here,

626
00:38:41,912 --> 00:38:45,454
because I created this airflow project with the Astro CLi.

627
00:38:45,582 --> 00:38:48,854
The Astro CLI is another open source tool that helps you use

628
00:38:48,892 --> 00:38:52,226
airflow. There will be instructions on how to use the Astro

629
00:38:52,258 --> 00:38:56,486
CLI on your computer in the readme of the repository. But in

630
00:38:56,508 --> 00:39:00,546
a nutshell, once you have the Astro CLI installed and you run astrodefinit

631
00:39:00,578 --> 00:39:04,234
in any empty folder. It will create most of these files for you.

632
00:39:04,352 --> 00:39:07,706
And what you can see here is the main thing is the docker file that

633
00:39:07,728 --> 00:39:10,950
I have open here. It pulls an image.

634
00:39:11,030 --> 00:39:14,554
It's called Astro runtime, but this is almost completely equivalent

635
00:39:14,602 --> 00:39:18,202
to open source airflow and it just has a few additional

636
00:39:18,266 --> 00:39:22,154
features. I'm also setting an environment variable here that is necessary

637
00:39:22,202 --> 00:39:25,614
for running the Astro SDK. But that is all that's needed to start

638
00:39:25,652 --> 00:39:29,694
airflow. And once you have run Astro in it, you can run Astro

639
00:39:29,742 --> 00:39:33,234
start next and it will start up an airflow environment with two

640
00:39:33,272 --> 00:39:36,514
example dags for you ready to use now. Next,

641
00:39:36,552 --> 00:39:40,054
I wanted to show you where the dags live. So I have this folder here

642
00:39:40,092 --> 00:39:43,606
called Dax, and this contains all of the dags that are in my

643
00:39:43,628 --> 00:39:47,362
environment. The one I want to show you is the training DAC.

644
00:39:47,506 --> 00:39:50,742
You can see here, I have some documentation import everything

645
00:39:50,876 --> 00:39:54,566
can see. I tried to modularize it as much as possible. So I'm importing

646
00:39:54,598 --> 00:39:57,414
a lot of my variables from a local config file.

647
00:39:57,462 --> 00:40:01,766
I'm importing my transform function that is running on my images.

648
00:40:01,958 --> 00:40:06,394
And of course I'm importing my fine tuned hugging face binary image classifier

649
00:40:06,442 --> 00:40:10,206
operator. I'm bad at naming things. And here

650
00:40:10,308 --> 00:40:13,966
the dag is defined. We scroll down, we can see

651
00:40:13,988 --> 00:40:17,474
all of the tasks. Each of these pieces of code is

652
00:40:17,512 --> 00:40:21,394
a task that is generated in my DAC. You can see some use

653
00:40:21,432 --> 00:40:24,942
the astro decay here. This one is using the add task decorator

654
00:40:25,006 --> 00:40:28,658
because I wanted to be able to configure

655
00:40:28,674 --> 00:40:32,694
this a little more. And here we are using the fine

656
00:40:32,732 --> 00:40:36,214
tune hugging face binary image classifier operator with all

657
00:40:36,252 --> 00:40:39,766
of the parameters. Now here

658
00:40:39,868 --> 00:40:43,414
I told you I'm importing this from a local file. So if we scroll

659
00:40:43,462 --> 00:40:47,386
back up, can see I'm importing this from include custom operator hugging face.

660
00:40:47,488 --> 00:40:50,846
So let's go to that file. And this is where

661
00:40:50,868 --> 00:40:54,526
all of the magic happens that you are probably more familiar with, a lot

662
00:40:54,548 --> 00:40:57,950
more familiar with than I am. We are using torch here

663
00:40:58,020 --> 00:41:02,030
and different utilities

664
00:41:03,410 --> 00:41:06,946
from huggingface. But if we scroll down here, we have

665
00:41:06,968 --> 00:41:10,398
all of the functions, we have the custom data set that I'm creating,

666
00:41:10,494 --> 00:41:14,466
and we have the huggingface binary image classifier here. And what

667
00:41:14,488 --> 00:41:17,618
I wanted to show you here is how easy it is to create a custom

668
00:41:17,704 --> 00:41:20,854
operator. The only things you need to do are you have

669
00:41:20,892 --> 00:41:24,930
to inherit from the base operator that you import from airflow,

670
00:41:25,090 --> 00:41:28,326
and then you have to define all of your parameters. You can

671
00:41:28,348 --> 00:41:30,860
give it a color, you don't have to, but I always do.

672
00:41:31,630 --> 00:41:34,902
And initialize it. And you just have to initialize

673
00:41:35,046 --> 00:41:38,426
the parent class as well and then you're good to go. You can

674
00:41:38,448 --> 00:41:42,778
have an execute method, and everything that is in this execute method

675
00:41:42,874 --> 00:41:46,080
will run as soon as this airflow task is running.

676
00:41:46,450 --> 00:41:50,110
And here is all that you are probably very familiar with.

677
00:41:50,180 --> 00:41:54,082
So here I'm pulling the actual model and

678
00:41:54,136 --> 00:41:57,198
I'm doing all my training in this execute method.

679
00:41:57,374 --> 00:42:01,006
And of course same thing happens with the test hugging face binary

680
00:42:01,038 --> 00:42:02,610
image classifier operator.

681
00:42:03,830 --> 00:42:07,254
All right, this is still running,

682
00:42:07,452 --> 00:42:13,222
but let's hop back into the presentation because

683
00:42:13,276 --> 00:42:17,126
I wanted to talk about the results. Now, I haven't done

684
00:42:17,148 --> 00:42:20,710
this before. This was my first time fine tuning an image classification

685
00:42:20,790 --> 00:42:24,460
model, so I went in blind. But it was very interesting

686
00:42:24,830 --> 00:42:28,714
before I fine tuned it, just running the all purpose Resnet 50

687
00:42:28,832 --> 00:42:32,258
on my classification glioma versus meningioma.

688
00:42:32,374 --> 00:42:35,978
It has not seen many brain tumors before, I'm assuming.

689
00:42:36,074 --> 00:42:39,982
But yeah, the model did not perform very well. And if you remember, I had

690
00:42:40,036 --> 00:42:43,646
a pretty balanced test set. So the split is worse

691
00:42:43,678 --> 00:42:47,458
than guessing or about

692
00:42:47,624 --> 00:42:51,442
as good as guessing. And after I fine tuned the model,

693
00:42:51,576 --> 00:42:55,134
I got something that I was pretty happy with. I tried to aim

694
00:42:55,182 --> 00:42:59,126
for highest AUC and I got 0844,

695
00:42:59,228 --> 00:43:02,342
which is something especially looking at the data set.

696
00:43:02,396 --> 00:43:05,794
The images were often cut at very different slices in the brain.

697
00:43:05,922 --> 00:43:09,138
There were other inconsistencies. I'm impressed.

698
00:43:09,234 --> 00:43:12,554
And that was only for the app pox. I only trained it for 1

699
00:43:12,592 --> 00:43:15,882
hour on my local computer. So yeah, this is actually

700
00:43:15,936 --> 00:43:17,100
working pretty well.

701
00:43:19,150 --> 00:43:22,334
All right, what's next? Of course, this is just

702
00:43:22,372 --> 00:43:25,946
a very simple ML and data orchestrating pipeline.

703
00:43:26,058 --> 00:43:29,406
There is much more that you can do here. For example, you could

704
00:43:29,428 --> 00:43:33,374
use dynamic task mapping to fine tune your

705
00:43:33,412 --> 00:43:36,958
models, to tune the hyperparameters. If you remember, I showed

706
00:43:36,974 --> 00:43:40,754
you that you can have one task, one operator, and then for

707
00:43:40,792 --> 00:43:44,066
different parameters you can simply provide it with sets and it

708
00:43:44,088 --> 00:43:47,606
will create a mapped task instance for each of these sets. You could

709
00:43:47,628 --> 00:43:51,634
do that with your hyperparameter parameters. So this is ideal

710
00:43:51,682 --> 00:43:55,080
for hyperparameter finetuning with full observability. If you go back,

711
00:43:55,690 --> 00:43:59,538
you could also, if you're running this in production, use the Kubernetes executor

712
00:43:59,554 --> 00:44:03,270
or the Kubernetes pod operator to run the heavy tasks in a dedicated

713
00:44:03,350 --> 00:44:07,050
pod and specify the resources you need, like GPU.

714
00:44:07,710 --> 00:44:10,550
And of course, one thing that I was thinking while I was doing this,

715
00:44:10,640 --> 00:44:14,570
I think at scale I would try to use a relational database

716
00:44:14,650 --> 00:44:16,670
with the option of parallel writing,

717
00:44:17,330 --> 00:44:19,950
especially for writing the file references,

718
00:44:20,290 --> 00:44:23,778
for writing the model results. Duckdb was really great,

719
00:44:23,864 --> 00:44:26,994
but writing the file references, I found

720
00:44:27,032 --> 00:44:30,514
myself wishing I could have parallel writing. And of course,

721
00:44:30,552 --> 00:44:33,582
maybe you're thinking, hey, I'm using huggingface,

722
00:44:33,726 --> 00:44:37,826
I am doing NLP and is there an operator for me? Not yet,

723
00:44:37,928 --> 00:44:40,600
but there could be, and it's pretty easy to write it.

724
00:44:41,770 --> 00:44:45,286
If that's something you're interested in, feel free to create your own and share it

725
00:44:45,308 --> 00:44:48,662
with the whole airflow community. One of the best places to get started

726
00:44:48,716 --> 00:44:52,166
with that, and also get help if you run into any issues is the airflow

727
00:44:52,198 --> 00:44:55,340
slack. I'm also going to put that link into the discord chat later.

728
00:44:57,230 --> 00:45:00,814
All right, I hope you got interested in learning more

729
00:45:00,852 --> 00:45:04,366
about airflow in seeing how Airflow and ML can

730
00:45:04,388 --> 00:45:08,074
work together. There are a lot of resources

731
00:45:08,122 --> 00:45:11,390
that are specific to ML use cases.

732
00:45:11,730 --> 00:45:15,758
For ML Flow, there's a whole provider, so there are existing operators

733
00:45:15,774 --> 00:45:19,538
that interact with ML Flow. I link the GitHub repository here.

734
00:45:19,624 --> 00:45:23,090
There's a whole demo on how to use airflow with weights and biases.

735
00:45:23,670 --> 00:45:27,458
There is a very great webinar on how to orchestrate machine learning workflows

736
00:45:27,474 --> 00:45:30,920
with airflow, and a blog post about

737
00:45:31,530 --> 00:45:35,906
what you might want to know before you get started. Both of these resources,

738
00:45:35,938 --> 00:45:39,026
the webinar and the blog post, were created by Jeff Fletcher.

739
00:45:39,058 --> 00:45:42,342
And I say that because he helped me a lot while creating this pipeline.

740
00:45:42,406 --> 00:45:45,926
So big shout out to him. He's the director of field engineering

741
00:45:45,958 --> 00:45:49,354
machine learning at Astronomer. And yeah, big thanks and

742
00:45:49,392 --> 00:45:53,386
I can really recommend his resources. And of course, if you're using Sagemaker,

743
00:45:53,498 --> 00:45:57,450
we already have a tutorial on how to connect airflow and Sagemaker,

744
00:45:57,530 --> 00:46:02,030
and there are existing operators for running things in Sagemaker.

745
00:46:02,610 --> 00:46:06,138
And yeah, the last thing I wanted to say here on the slide for resources,

746
00:46:06,234 --> 00:46:09,534
if you're very new to airflow, probably very experienced with ML,

747
00:46:09,582 --> 00:46:12,894
but want to get started with airflow, we have a quick start that you simply

748
00:46:12,942 --> 00:46:15,966
can clone and run that is more data engineering focused,

749
00:46:15,998 --> 00:46:19,314
but highlights a lot of interesting airflow features that

750
00:46:19,352 --> 00:46:23,446
also were part of this talk like dynamic task mapping and data sets. And of

751
00:46:23,468 --> 00:46:26,806
course, if there is an ML tool that you're using and that you want

752
00:46:26,828 --> 00:46:30,042
to see an integration with, or there's content that you would like to see around

753
00:46:30,096 --> 00:46:34,150
ML and Airflow, please tell me, please tell us. At astronomer,

754
00:46:34,230 --> 00:46:38,060
we are always looking for how we can help the airflow and greater data community.

755
00:46:40,270 --> 00:46:43,966
That was it. For content. Take home messages. I have to

756
00:46:43,988 --> 00:46:47,898
say one thing. As someone who formerly worked clinically. Was this clinically useful?

757
00:46:47,994 --> 00:46:51,946
Absolutely not. In reality, you would always have a radiologist

758
00:46:51,978 --> 00:46:55,486
look at these pictures, would look at the whole MRI. You never just look at

759
00:46:55,508 --> 00:46:59,534
one slice of an MRI. So this isn't really clinically useful

760
00:46:59,582 --> 00:47:02,994
in any way. It was just something that was fun and interesting to do.

761
00:47:03,112 --> 00:47:06,946
But it shows that even with these very constricted resources,

762
00:47:06,978 --> 00:47:09,110
I only had a handful of pictures,

763
00:47:10,730 --> 00:47:13,858
I only ran my whole training locally. I didn't

764
00:47:13,874 --> 00:47:17,446
do that much fine parameter tuning. You can get interesting improvements on

765
00:47:17,468 --> 00:47:21,098
an existing model, and I think in part this will be the feature future.

766
00:47:21,184 --> 00:47:24,698
Maybe not so much for brain mris, because they are always

767
00:47:24,784 --> 00:47:28,074
looked at very closely anyway, but maybe for other

768
00:47:28,112 --> 00:47:31,546
images that are, of which there are a lot taken. And maybe you

769
00:47:31,568 --> 00:47:34,974
want to prioritize which to look at first. Or one use

770
00:47:35,012 --> 00:47:38,986
case that I've seen actually already happening, or being trialed

771
00:47:39,178 --> 00:47:43,262
a few years ago in a radiology department, was that they had a model that

772
00:47:43,316 --> 00:47:47,278
flagged potentially cancerous small lesions in lung cts.

773
00:47:47,374 --> 00:47:51,106
So the radiologists, after they read the whole image normally, they would

774
00:47:51,128 --> 00:47:54,642
always go back and look at those flagged images and

775
00:47:54,696 --> 00:47:58,182
just look through these little lesions to make sure none of them

776
00:47:58,316 --> 00:48:02,162
might have been missed. So that's something that how ML

777
00:48:02,226 --> 00:48:05,800
could augment the work of physicians in many ways.

778
00:48:06,250 --> 00:48:09,478
And yeah, the next thing, can the pipeline be easily adjusted for

779
00:48:09,484 --> 00:48:12,938
other use cases? Yes, definitely. That was my goal here,

780
00:48:13,104 --> 00:48:16,762
that you can adjust it for different things. You might have to change

781
00:48:16,816 --> 00:48:20,038
a little bit about maybe the transform function for your images,

782
00:48:20,134 --> 00:48:24,266
but the whole idea of airflow is to make it as modular and adjustable

783
00:48:24,298 --> 00:48:27,566
and flexible as possible. So the main take home message here

784
00:48:27,588 --> 00:48:31,374
is airflow is the one central place that can orchestrates both

785
00:48:31,492 --> 00:48:35,530
your data and your ML. Pipelines can have complex interdependencies

786
00:48:35,610 --> 00:48:37,630
and it is fully tool agnostic.

787
00:48:39,330 --> 00:48:42,480
And with that, I thank you so much for listening to this talk.

788
00:48:42,850 --> 00:48:46,454
I hope you got interested in checking out airflow and if you have

789
00:48:46,492 --> 00:48:49,670
any questions, please ask me in the Discord channel.

790
00:48:49,820 --> 00:48:52,326
And with that, I hope you have an awesome day.

