1
00:02:00,980 --> 00:02:04,592
Hello everyone. Thanks for joining me on this

2
00:02:04,646 --> 00:02:08,496
session today. It's a pleasure to have you all. Today I'm going to

3
00:02:08,518 --> 00:02:12,356
be talking about cloud KUs Engineering with AWS fault injection

4
00:02:12,388 --> 00:02:15,572
simulator. My name fis Samuel Baruffi, can can

5
00:02:15,636 --> 00:02:19,690
solutions architect here at Amazon Web Services.

6
00:02:20,140 --> 00:02:24,056
I do help support global financial services in

7
00:02:24,078 --> 00:02:27,640
their cloud journey from architecturing,

8
00:02:27,800 --> 00:02:31,244
best practice, and so forth. So today I want to talk

9
00:02:31,282 --> 00:02:35,432
to you about how you can improve your resiliency and performance

10
00:02:35,576 --> 00:02:39,372
with controlled chaos engineer. You might be wondering,

11
00:02:39,436 --> 00:02:42,464
what is chaos engineer? Or maybe you're already very familiar with

12
00:02:42,502 --> 00:02:46,464
chaos engineering. What I want to show you

13
00:02:46,502 --> 00:02:50,164
is how AWS and AWS ecosystem can

14
00:02:50,202 --> 00:02:53,220
help you on your chaos engineer journey.

15
00:02:54,280 --> 00:02:57,636
So let's talk about the agenda for a few

16
00:02:57,738 --> 00:03:00,230
seconds of what we're going to cover today.

17
00:03:01,020 --> 00:03:05,076
So we are going to start talking about some challenges with distributed

18
00:03:05,108 --> 00:03:09,044
systems. We want to explain how distributed

19
00:03:09,092 --> 00:03:12,520
systems work and why they are complex by nature.

20
00:03:13,100 --> 00:03:17,004
Then we want to jump into why chaos engineering fis hard what

21
00:03:17,042 --> 00:03:20,348
have we heard as a company from customers that have tried to

22
00:03:20,354 --> 00:03:24,524
do by themselves kus engineering and some of the lessons and

23
00:03:24,562 --> 00:03:28,176
requirements we heard from them. Then of

24
00:03:28,198 --> 00:03:32,124
course you're going to introduce AWS fault injection simulator.

25
00:03:32,252 --> 00:03:36,160
You seem that I will be interchainable using

26
00:03:36,230 --> 00:03:39,916
the FIS acronym as fault injection

27
00:03:39,948 --> 00:03:42,980
simulator, the name of the service on AWS.

28
00:03:43,480 --> 00:03:47,136
After that, we're going to dive deep into some of the key features

29
00:03:47,168 --> 00:03:50,036
that fault injection simulator brings to us.

30
00:03:50,138 --> 00:03:53,736
Some of the use cases that you should be using or could be using the

31
00:03:53,758 --> 00:03:57,368
service. And in the end, I will spend a

32
00:03:57,374 --> 00:04:01,044
little bit of time just doing a simple demo, showing the console

33
00:04:01,092 --> 00:04:04,428
and demonstrating how you can use the service itself.

34
00:04:04,594 --> 00:04:07,692
And hopefully before I do the demo,

35
00:04:07,746 --> 00:04:12,156
I'll share some resources that if you are interested, you can take your

36
00:04:12,338 --> 00:04:15,550
learning to the next level

37
00:04:15,940 --> 00:04:18,530
by checking some of the resources we have available.

38
00:04:21,060 --> 00:04:25,196
So let's move forward. So let's talk about challenges

39
00:04:25,228 --> 00:04:28,976
with distributed systems. While most of us

40
00:04:29,158 --> 00:04:32,660
understand that distributed systems are undeniable,

41
00:04:33,080 --> 00:04:36,756
have revolutionized it industry in the last

42
00:04:36,938 --> 00:04:40,340
decade or so, they do have some challenges.

43
00:04:41,020 --> 00:04:46,376
Some of the challenges that distributed systems brings to us can

44
00:04:46,398 --> 00:04:49,860
be a combination of multiple things like latency,

45
00:04:50,020 --> 00:04:53,060
scalability, reliability, resiliency,

46
00:04:53,220 --> 00:04:57,144
concurrency, and many more. As systems

47
00:04:57,192 --> 00:05:00,924
grows larger and more distributed, what is

48
00:05:00,962 --> 00:05:04,540
often a theoretical edge case can actually

49
00:05:04,610 --> 00:05:07,040
sometimes become real occurrences.

50
00:05:08,420 --> 00:05:11,728
So it's common mistake a lot of us,

51
00:05:11,814 --> 00:05:15,424
and maybe I've been at

52
00:05:15,462 --> 00:05:18,992
fault in the past at that, that it's very

53
00:05:19,046 --> 00:05:22,756
easy to think that distributed systems are very complex when

54
00:05:22,778 --> 00:05:26,576
they become just bigger. When you're talking about hundreds or thousands

55
00:05:26,608 --> 00:05:30,196
of microservice. That is of course not the case.

56
00:05:30,378 --> 00:05:32,890
And let me explain what I mean by that.

57
00:05:33,740 --> 00:05:37,690
So even in a very simple application

58
00:05:38,780 --> 00:05:42,872
that we just want to send a message from a client to

59
00:05:42,926 --> 00:05:46,636
a server, there are a lot

60
00:05:46,658 --> 00:05:50,300
of steps that are involved into this communication.

61
00:05:50,880 --> 00:05:54,684
So let's just look at this. If a client wants

62
00:05:54,722 --> 00:05:58,336
to send a message to a server, the first thing that will happen in

63
00:05:58,358 --> 00:06:02,144
this scenario will be the client will put the message into

64
00:06:02,182 --> 00:06:05,836
the network. The network will be responsible for delivering

65
00:06:05,868 --> 00:06:09,360
the message to the server. The server then validates

66
00:06:09,440 --> 00:06:13,284
the message. Once it validates the message, the server will

67
00:06:13,322 --> 00:06:17,616
update its estate. Once that estate is updated,

68
00:06:17,728 --> 00:06:21,416
the server will reply, will put

69
00:06:21,438 --> 00:06:24,952
a reply into the network. The network will

70
00:06:25,006 --> 00:06:28,404
actually be responsible for delivering the reply

71
00:06:28,452 --> 00:06:32,184
to the client. Once the client received the reply, you actually

72
00:06:32,222 --> 00:06:35,844
validates the reply, and then finally the client will update

73
00:06:35,892 --> 00:06:39,468
its estate. So it's mind blowing just to understand

74
00:06:39,634 --> 00:06:45,100
how many steps in this very simple situation

75
00:06:45,250 --> 00:06:49,410
by sending a message from a client to a server, how many

76
00:06:50,500 --> 00:06:54,464
steps behind the scenes happens and how

77
00:06:54,502 --> 00:06:58,664
many steps you can have failures. So now let's just multiply

78
00:06:58,732 --> 00:07:01,924
that by hundreds, thousands, millions or

79
00:07:01,962 --> 00:07:06,192
even billions of occurrences across many of microservice,

80
00:07:06,256 --> 00:07:08,020
many of our microservices.

81
00:07:09,320 --> 00:07:12,708
So we only look to implement tests

82
00:07:12,884 --> 00:07:16,888
after we have outages. So it's very common

83
00:07:16,974 --> 00:07:20,584
that there is an issue on the network because we

84
00:07:20,622 --> 00:07:23,904
might not have redundance network gear.

85
00:07:24,052 --> 00:07:27,992
Only after that we go, after the occurrence

86
00:07:28,056 --> 00:07:31,580
happened, that we go and we improve. We want to change

87
00:07:31,650 --> 00:07:36,876
that. Right? So one

88
00:07:36,898 --> 00:07:40,128
of the things that has been done a lot

89
00:07:40,214 --> 00:07:44,000
is just traditional testing. Of course you need.

90
00:07:44,070 --> 00:07:47,820
So the message on this slide, fis, please don't stop doing your traditional

91
00:07:47,900 --> 00:07:51,620
testing. You should never stop doing. It's just that

92
00:07:51,690 --> 00:07:55,764
traditional test doesn't cover all the unknowns that

93
00:07:55,962 --> 00:07:59,124
distributed systems bring to the table and

94
00:07:59,162 --> 00:08:02,016
all the complexity that you have in production environments.

95
00:08:02,208 --> 00:08:06,168
So what traditional tests are good at

96
00:08:06,254 --> 00:08:09,672
are verifying known conditions and

97
00:08:09,726 --> 00:08:14,188
answer the questions like is this specific function

98
00:08:14,274 --> 00:08:18,344
or this specific action, returning the specific expected

99
00:08:18,392 --> 00:08:21,724
behavior that they are really good at it with

100
00:08:21,762 --> 00:08:25,212
you both using unit tests or functional testing of

101
00:08:25,266 --> 00:08:28,496
integration. But what about, let me pose you the

102
00:08:28,518 --> 00:08:32,236
question, what about failures that have weird errors

103
00:08:32,268 --> 00:08:36,108
that would happen on the network, like that goes over the Internet?

104
00:08:36,204 --> 00:08:39,820
What about some configuration limits on cloud providers?

105
00:08:39,980 --> 00:08:43,380
How about some drifts of your infrastructure? And what about

106
00:08:43,450 --> 00:08:46,896
all the unknowns that you are not familiar and you are not testing

107
00:08:46,928 --> 00:08:50,950
for? How can you test for something that you don't know yet?

108
00:08:53,660 --> 00:08:56,040
And it can get even more complicated.

109
00:08:56,700 --> 00:09:00,250
Some things are just really hard to test. I'll give you an example.

110
00:09:00,620 --> 00:09:04,004
In a system where you have multiple instances that

111
00:09:04,062 --> 00:09:07,756
start and stop dynamically, what happens if one of

112
00:09:07,778 --> 00:09:11,356
those instance runs out of space? I'm pretty sure the

113
00:09:11,378 --> 00:09:14,588
majority of you have been in a similar

114
00:09:14,674 --> 00:09:19,152
situation where you have to perform some maintenance in

115
00:09:19,206 --> 00:09:23,152
servers that have run out of space. Debugging apps that run

116
00:09:23,206 --> 00:09:26,864
out of space are really, really complex and

117
00:09:26,902 --> 00:09:30,404
would look something like this. You just see a bunch of

118
00:09:30,442 --> 00:09:33,924
errors and no actions you try to take on the machine actually go

119
00:09:33,962 --> 00:09:37,508
through. And that is relative common issue

120
00:09:37,594 --> 00:09:41,624
that often takes just

121
00:09:41,742 --> 00:09:46,036
a misconfiguration of log rotation or not being monitoring

122
00:09:46,228 --> 00:09:49,896
the specific configurations of your space on the

123
00:09:49,918 --> 00:09:53,256
disk from your monitoring systems. What that can be,

124
00:09:53,358 --> 00:09:57,420
you can use a third party vendor or solutions on AWS,

125
00:09:58,640 --> 00:10:02,124
but some of the solutions that you should potentially have

126
00:10:02,162 --> 00:10:05,150
written is, well, you should have a log rotation in.

127
00:10:06,240 --> 00:10:09,884
You know, if you don't have a log rotation in place, then another solution

128
00:10:09,932 --> 00:10:13,296
that you could have put in place fis, you maybe have some

129
00:10:13,318 --> 00:10:16,976
monitoring solutions that look for your storage on

130
00:10:16,998 --> 00:10:20,692
this specific instance, once they reach close

131
00:10:20,746 --> 00:10:24,032
to 90% of storage utilization,

132
00:10:24,176 --> 00:10:27,732
then you send a message so you can reactively make

133
00:10:27,786 --> 00:10:31,392
improvements. Of course, you always want to potentially make sure

134
00:10:31,546 --> 00:10:35,716
you are having automation that will solve

135
00:10:35,748 --> 00:10:39,016
those problems, rather than having to page someone in the

136
00:10:39,038 --> 00:10:41,770
middle of the night to make those changes for you.

137
00:10:42,940 --> 00:10:46,552
But you can see this is just an example of unknowns that potentially

138
00:10:46,616 --> 00:10:50,348
you haven't covered on your unit test or integration test.

139
00:10:50,514 --> 00:10:54,300
So the question that is in the industry is how can you

140
00:10:54,450 --> 00:10:58,304
be more prepared for the unknowns? And luckily there

141
00:10:58,342 --> 00:11:01,712
is a kind of engineering, and that's all

142
00:11:01,766 --> 00:11:05,330
this talk is about today that helps you with that.

143
00:11:05,860 --> 00:11:09,792
And the name of that. As all of you already know,

144
00:11:09,926 --> 00:11:13,180
it's chaos engineer. So chaos engineer

145
00:11:13,260 --> 00:11:17,332
focused on three main things. Three main pillars I'll call you have

146
00:11:17,386 --> 00:11:21,376
our three main phases. You have the stress phase, you have the observed

147
00:11:21,408 --> 00:11:25,124
phase, and you have the improve phase. So what the stress

148
00:11:25,172 --> 00:11:28,344
phase means is that you are stressing an

149
00:11:28,382 --> 00:11:31,940
application either in testing or in production environment,

150
00:11:32,100 --> 00:11:36,220
by creating disruptions, by injecting

151
00:11:36,640 --> 00:11:39,608
failure events such as server outages,

152
00:11:39,704 --> 00:11:43,436
API throttling, network disruptions in

153
00:11:43,458 --> 00:11:46,744
your environment. After you have injected for a period

154
00:11:46,792 --> 00:11:51,036
of time, you observe what those means. So you observe

155
00:11:51,068 --> 00:11:54,688
the systems and how the system responds. And this is a really important

156
00:11:54,774 --> 00:11:58,144
part of chaos engineering, because kios engineers can

157
00:11:58,182 --> 00:12:01,616
only exist if you have a really good observability

158
00:12:01,728 --> 00:12:04,944
system in place. Once you observe,

159
00:12:05,072 --> 00:12:08,644
by checking if your system is completely healthy. Or if you have

160
00:12:08,682 --> 00:12:13,052
had some sort of occurrences that are not expected,

161
00:12:13,216 --> 00:12:16,776
then you analyze what those occurrences are, and then you go to

162
00:12:16,798 --> 00:12:20,488
the last phase, which fis the improved phase. You make changes

163
00:12:20,574 --> 00:12:24,760
for your application to be more resilient or performant.

164
00:12:25,760 --> 00:12:29,468
And we do want to prove or disprove some of

165
00:12:29,474 --> 00:12:32,680
the assumptions that we have about our system capabilities

166
00:12:32,840 --> 00:12:36,716
that can potentially handle or not handle those disruptive

167
00:12:36,748 --> 00:12:41,296
events. So the

168
00:12:41,318 --> 00:12:44,976
chaos engineer, focus on improving the resiliency and

169
00:12:45,078 --> 00:12:48,576
performance of your workloads, but also focus on

170
00:12:48,598 --> 00:12:52,068
uncover those hidden issues. And that's one of the main benefits

171
00:12:52,154 --> 00:12:55,332
of chaos engineering, is those hidden issues are really hard

172
00:12:55,386 --> 00:12:59,252
to know ahead of time. And also

173
00:12:59,306 --> 00:13:02,936
you want to expose your blind spots. And this is actually the example that

174
00:13:02,958 --> 00:13:06,760
I mentioned before about having a very good observability story.

175
00:13:06,910 --> 00:13:10,970
If you don't have proper monitoring, observability and alarm, your application

176
00:13:11,500 --> 00:13:14,936
might fail. But you don't have data, good data,

177
00:13:15,038 --> 00:13:18,364
to understand what happened. So that is another aspect that is

178
00:13:18,402 --> 00:13:21,916
part of what we call continuous resiliency that you test.

179
00:13:22,018 --> 00:13:25,324
If there is any sort of failure that you

180
00:13:25,362 --> 00:13:28,716
were not expected or haven't been uncovered by a metric or

181
00:13:28,738 --> 00:13:32,448
your observability system, you need to improve that story. Aws well,

182
00:13:32,534 --> 00:13:36,096
but there is much more to that. You also want to improve the recovery time.

183
00:13:36,198 --> 00:13:41,156
If there is some major issues that you

184
00:13:41,178 --> 00:13:45,028
haven't been able to protect your application, how do you improve your recovery time?

185
00:13:45,114 --> 00:13:48,308
How do you improve your operational skills? And also, how do

186
00:13:48,314 --> 00:13:51,684
you increase or not? How do you increase increases

187
00:13:51,732 --> 00:13:54,852
the wrong word. How do you implement the culture of chaos

188
00:13:54,916 --> 00:13:58,664
engineering? And when we

189
00:13:58,702 --> 00:14:02,504
look at chaos engineering, we can look at different phases of

190
00:14:02,542 --> 00:14:05,964
chaos engineering. One thing that is important to mention is

191
00:14:06,002 --> 00:14:10,220
chaos engineering is not about breaking things randomly without

192
00:14:10,290 --> 00:14:14,424
a purpose. Chaos engineering is about breaking

193
00:14:14,472 --> 00:14:18,040
things in a controlled environment through a well planned

194
00:14:18,120 --> 00:14:22,412
experiment in order to build confidence in your application and tools

195
00:14:22,556 --> 00:14:26,348
so you can sustain turbulence and potentially

196
00:14:26,444 --> 00:14:29,636
issues on your application. To do that, you have

197
00:14:29,658 --> 00:14:33,328
to follow a well defined scientific method

198
00:14:33,424 --> 00:14:37,092
that will take you from hypothesis to running,

199
00:14:37,146 --> 00:14:40,400
can experiment to verifying their experiment,

200
00:14:40,560 --> 00:14:44,412
improving, and then going back again to statistic.

201
00:14:44,576 --> 00:14:48,104
Chaos engineers shouldn't be just one thing you

202
00:14:48,142 --> 00:14:51,448
run once every year. It should be a

203
00:14:51,534 --> 00:14:55,344
practice that you motivated and you innovated,

204
00:14:55,412 --> 00:14:59,656
and you keep your engineers applying

205
00:14:59,688 --> 00:15:03,724
this innovation on your workloads. That way

206
00:15:03,842 --> 00:15:07,712
you can sustain failures and

207
00:15:07,766 --> 00:15:11,340
always keeping your business outcomes

208
00:15:11,500 --> 00:15:15,596
intact, not getting disrupted by random,

209
00:15:15,708 --> 00:15:18,770
unknown failures that your application might face.

210
00:15:19,700 --> 00:15:22,980
But let's talk about why Kus engineering is difficult.

211
00:15:23,130 --> 00:15:26,404
And in the beginning of the presentation, I mentioned that

212
00:15:26,442 --> 00:15:30,484
AWS have actually talked to a lot of customers that have tried

213
00:15:30,522 --> 00:15:33,656
to do that by themselves. And we

214
00:15:33,678 --> 00:15:36,916
have collected four main feedbacks

215
00:15:36,948 --> 00:15:40,344
from the customers from a variety of industries and

216
00:15:40,382 --> 00:15:43,876
different company sizes. The first one is really hard to stitch

217
00:15:43,908 --> 00:15:46,872
together different tools and homemade scripts.

218
00:15:47,016 --> 00:15:50,252
You might have some open source tooling, you might just building

219
00:15:50,306 --> 00:15:54,108
some python scripts, some batches to actually implement those

220
00:15:54,194 --> 00:15:57,868
injections of failures, or even the observability piece.

221
00:15:57,954 --> 00:16:01,648
It's really hard to have that story by yourself,

222
00:16:01,814 --> 00:16:05,116
but also you require a lot of agents and libraries

223
00:16:05,228 --> 00:16:08,384
to get it started. So you might need to put a lot of infrastructure and

224
00:16:08,422 --> 00:16:11,844
configuration in place, and it's not

225
00:16:11,882 --> 00:16:15,204
very easy to get started. And then probably the most

226
00:16:15,242 --> 00:16:18,404
important one, in my opinion, is it's really difficult to

227
00:16:18,442 --> 00:16:22,108
ensure safety, because if you're doing case engineering in production

228
00:16:22,144 --> 00:16:25,784
environments, the goal is to find the

229
00:16:25,822 --> 00:16:29,464
unhidden problems, but at the same time you don't want to bring

230
00:16:29,502 --> 00:16:33,240
your whole application. So how do you create guard rails

231
00:16:33,680 --> 00:16:37,320
to stop your chaos engineering in production,

232
00:16:37,400 --> 00:16:40,940
or even in tests for that matter, to bring down whole application

233
00:16:41,090 --> 00:16:43,420
and affecting your business outcomes.

234
00:16:44,320 --> 00:16:48,012
And the last is it's really difficult to reproduce real

235
00:16:48,066 --> 00:16:51,708
world events because reward events

236
00:16:51,804 --> 00:16:55,404
are not just as simple as an API has failed.

237
00:16:55,532 --> 00:16:58,792
Normally it's a combination of scenarios that will potentially

238
00:16:58,876 --> 00:17:02,470
run in sequence or in parallel, and that is really

239
00:17:03,640 --> 00:17:07,684
very hard to reproduce with

240
00:17:07,722 --> 00:17:11,992
that. What AWS have

241
00:17:12,046 --> 00:17:15,540
introduced a couple of years ago at reinvent,

242
00:17:15,700 --> 00:17:19,412
which is AWS global conference, the yearly

243
00:17:19,476 --> 00:17:22,840
conference is AWS fault injection simulator.

244
00:17:23,340 --> 00:17:27,092
AWS fault injection simulator is a fully managed chaos

245
00:17:27,156 --> 00:17:30,972
engineering as a service. It is really easy to get started

246
00:17:31,026 --> 00:17:34,700
with a service that allows you to reproduce real world

247
00:17:34,770 --> 00:17:38,716
failures, whether the failure is as simple as stopping

248
00:17:38,748 --> 00:17:42,428
an instance or more complex, like throttling APIs,

249
00:17:42,604 --> 00:17:46,060
AWS fault injection simulator fully embraces

250
00:17:46,140 --> 00:17:49,664
the idea of safeguards, which is one of the things we've heard

251
00:17:49,702 --> 00:17:53,840
from our customers, that they want to do these chaos engineering tests,

252
00:17:53,920 --> 00:17:57,572
but at the same time they want to be protected to potentially full

253
00:17:57,626 --> 00:18:01,392
outages on their applications. And far injection simulator

254
00:18:01,456 --> 00:18:05,336
brings that capability that it

255
00:18:05,358 --> 00:18:09,364
is a way to monitor the blast radio and control the blast

256
00:18:09,412 --> 00:18:12,884
radiant of your experiment and stop it automatically

257
00:18:12,932 --> 00:18:16,204
if alarms set offs. So you have

258
00:18:16,322 --> 00:18:20,012
three main pillars here that we're going to talk a little bit

259
00:18:20,066 --> 00:18:21,150
more in detail.

260
00:18:23,760 --> 00:18:27,564
So let's talk about why is it easy to get started with fault

261
00:18:27,612 --> 00:18:30,976
injection simulator? So first, you do not need

262
00:18:31,078 --> 00:18:34,108
to integrate multiple tools and homemade scripts.

263
00:18:34,204 --> 00:18:37,404
Fault injection simulators will manage all the tests

264
00:18:37,452 --> 00:18:40,976
and experiments for you. You can also use the

265
00:18:41,078 --> 00:18:44,580
AWS managed console that you are familiar with it,

266
00:18:44,650 --> 00:18:48,340
or the AWS CLI to run those experiments.

267
00:18:48,760 --> 00:18:52,912
The interesting part here, FIS, you can use pre existing experiments

268
00:18:52,976 --> 00:18:56,616
templates, and we're going to talk what experiment templates are in a moment,

269
00:18:56,718 --> 00:19:01,108
and you can get started in minutes. And it's

270
00:19:01,124 --> 00:19:05,064
really easy to share your experiment templates with other folks

271
00:19:05,112 --> 00:19:08,796
within your organization. Or if you prefer and you want to

272
00:19:08,898 --> 00:19:12,750
open source, you can actually do that and made available to the community.

273
00:19:14,400 --> 00:19:17,488
The templates are JSon or Yaml fis that you

274
00:19:17,494 --> 00:19:21,680
can share with your team and you can version control. So you can benefit from

275
00:19:21,750 --> 00:19:24,720
the best practice associated with code reviews.

276
00:19:28,660 --> 00:19:32,144
And then let's move to the next topic, which is

277
00:19:32,182 --> 00:19:36,080
reward conditions so you can run experiments

278
00:19:36,160 --> 00:19:40,180
both in sequence and in parallel. And I made a mention

279
00:19:40,250 --> 00:19:44,184
before that real world failures are

280
00:19:44,222 --> 00:19:47,748
not just as simple as one event. Sometimes they're a combination of events.

281
00:19:47,844 --> 00:19:52,088
And the fault injection simulator allows you to combine different

282
00:19:52,174 --> 00:19:56,392
actions that will inject

283
00:19:56,456 --> 00:20:00,348
failure both in parallel or in sequence. And you can choose.

284
00:20:00,514 --> 00:20:04,316
You can also target all levels of the systems, both the

285
00:20:04,338 --> 00:20:07,630
host, the infrastructure, the network, and many more.

286
00:20:08,500 --> 00:20:12,272
And you can select maybe just a few of them. You have full control

287
00:20:12,326 --> 00:20:16,016
and flexibility in that aspect and real faults. And this

288
00:20:16,038 --> 00:20:19,492
is an important r1 faults injected at the service control

289
00:20:19,546 --> 00:20:22,976
plane level. So the faults and the injection

290
00:20:23,168 --> 00:20:26,724
that are being implemented into your

291
00:20:26,762 --> 00:20:30,544
environment are actually real world faults

292
00:20:30,592 --> 00:20:34,772
are not just makeups of potentially

293
00:20:34,836 --> 00:20:38,424
APIs and whatnot, those are actually failures that

294
00:20:38,462 --> 00:20:41,450
are happening in real time.

295
00:20:41,980 --> 00:20:43,770
So as an example,

296
00:20:45,580 --> 00:20:49,644
if you configure can experiment template to

297
00:20:49,682 --> 00:20:52,936
terminate an instance, actually the instance

298
00:20:53,048 --> 00:20:56,904
will be terminated on AWS. So you got to be careful

299
00:20:57,032 --> 00:21:00,688
because it's not faking with any metric manipulation. So you

300
00:21:00,694 --> 00:21:04,176
have to pay a little bit attention to not do something that

301
00:21:04,198 --> 00:21:06,530
you don't expect it to do.

302
00:21:07,860 --> 00:21:13,840
And then the safeguards are where you create those guardrails

303
00:21:13,920 --> 00:21:17,840
which are stop condition alarms, so you can configure alarms

304
00:21:18,000 --> 00:21:21,284
on Cloudwatch, or potentially third parties, that if

305
00:21:21,322 --> 00:21:24,808
those alarms are triggered, you can send a notification to

306
00:21:24,894 --> 00:21:28,440
thought injection simulator service to say please stop

307
00:21:28,510 --> 00:21:32,072
what you're doing because it's impacting heavily my service into

308
00:21:32,126 --> 00:21:35,788
a level that you don't want to pass that threshold. So like I

309
00:21:35,794 --> 00:21:39,976
said, it integrates natively with AWS Cloudwatch,

310
00:21:40,088 --> 00:21:43,944
and it has built in rollbacks as well. So you can redo

311
00:21:43,992 --> 00:21:47,600
what you actually have created and done until that point.

312
00:21:47,750 --> 00:21:51,132
And because fault injection simulator

313
00:21:51,196 --> 00:21:55,024
can be very dangerous in the sense that you

314
00:21:55,062 --> 00:21:59,252
want to make sure only the right people within your organization have access

315
00:21:59,386 --> 00:22:03,364
to run those experiments. As most

316
00:22:03,402 --> 00:22:06,752
of AWS services you can control with fine grained IAM

317
00:22:06,816 --> 00:22:10,512
control. So you can say this specific IAM

318
00:22:10,576 --> 00:22:14,616
principle can only do these actions on these resources and

319
00:22:14,798 --> 00:22:18,376
only these folks can start experiments. So you

320
00:22:18,398 --> 00:22:22,296
can also control what type of faults can be used and what type of

321
00:22:22,318 --> 00:22:25,800
resources can be affected using tag policies.

322
00:22:25,880 --> 00:22:29,304
So for example, instance only with environment tests

323
00:22:29,352 --> 00:22:31,470
can be affected, nothing else.

324
00:22:33,120 --> 00:22:36,684
You don't allow anything else to be affected

325
00:22:36,732 --> 00:22:38,640
by fault injection simulator.

326
00:22:40,740 --> 00:22:44,284
So here you can see an architecture

327
00:22:44,332 --> 00:22:48,352
diagram of the service. So what you see here, you can

328
00:22:48,406 --> 00:22:52,016
see that in the middle you have the AWS fault injection

329
00:22:52,048 --> 00:22:55,396
simulator which is controlled by IAM where you

330
00:22:55,418 --> 00:22:59,412
have all the policies and permissions of who can actually do what in the service.

331
00:22:59,546 --> 00:23:02,664
And you can access the service either via console or

332
00:23:02,702 --> 00:23:06,836
CLI or a combination. Once you start can experiment.

333
00:23:07,028 --> 00:23:10,692
The experiment will actually inject

334
00:23:10,756 --> 00:23:14,440
those faults into AWS resources, compute databases,

335
00:23:14,520 --> 00:23:18,216
network and storage. Those resources

336
00:23:18,248 --> 00:23:22,796
are being monitored by cloud alert alarms or

337
00:23:22,898 --> 00:23:26,850
potentially third party. You can choose and if

338
00:23:27,700 --> 00:23:31,456
we recommend you to create stop conditions and if

339
00:23:31,478 --> 00:23:35,616
those stop conditions are can, Eventbridge will

340
00:23:35,638 --> 00:23:39,204
send a notification to fault injection simulator engine

341
00:23:39,322 --> 00:23:43,044
to stop those experiments and roll back what chaos done.

342
00:23:43,162 --> 00:23:47,012
And again, it's the best practice to have cloud watch

343
00:23:47,066 --> 00:23:50,924
alarms monitoring your AWS and workloads,

344
00:23:51,072 --> 00:23:54,628
AWS accounts and workloads so you can define stop conditions

345
00:23:54,644 --> 00:23:57,240
that will automatically stop the experiments.

346
00:24:01,220 --> 00:24:04,832
Let's talk about some of the components that are part

347
00:24:04,886 --> 00:24:08,852
of the fault injection simulator service.

348
00:24:08,986 --> 00:24:12,192
You have actions, you have targets, you have experiment

349
00:24:12,256 --> 00:24:15,072
templates, and then you have experiment.

350
00:24:15,136 --> 00:24:18,732
So let's look at each one of them individually.

351
00:24:18,896 --> 00:24:22,964
So actions are default injection actions executing

352
00:24:23,012 --> 00:24:27,064
during the experiment. They are defined using

353
00:24:27,102 --> 00:24:30,672
a namespace, so you can see that you'll be AWS colon,

354
00:24:30,756 --> 00:24:33,310
the service name column action type.

355
00:24:34,000 --> 00:24:36,552
The action types can include fault types,

356
00:24:36,616 --> 00:24:40,252
target resources, the timing relative to

357
00:24:40,306 --> 00:24:43,848
any other actions, and fault injections parameters such as

358
00:24:43,874 --> 00:24:48,320
duration, the rollback behavior, or the portion of the request to throttle.

359
00:24:48,900 --> 00:24:53,804
As an example of an action, you can see here on this JSON representation

360
00:24:53,932 --> 00:24:57,284
you have two actions defined. One is a

361
00:24:57,322 --> 00:25:01,060
stop instance action and the other one is a wait action.

362
00:25:01,400 --> 00:25:04,932
Notice that the wait is ordered to execute only

363
00:25:04,986 --> 00:25:09,380
after the stop instance action has sequentially

364
00:25:09,540 --> 00:25:12,984
executed. It's also worth noting that some

365
00:25:13,022 --> 00:25:17,076
host level actions on EC two instance are performed

366
00:25:17,108 --> 00:25:20,440
through system manage agent. The system manage

367
00:25:20,510 --> 00:25:23,884
agent is a software that is installed by default on some

368
00:25:24,002 --> 00:25:28,584
versions of operating systems such as Amazon Linux Ubuntu images.

369
00:25:28,712 --> 00:25:32,680
And you can just find that information on SSM

370
00:25:32,760 --> 00:25:35,520
or fault injection simulator documentation.

371
00:25:38,100 --> 00:25:41,164
Now when we look at targets, so we talked about actions.

372
00:25:41,292 --> 00:25:45,104
When you look at targets, targets define one or more

373
00:25:45,142 --> 00:25:48,912
AWS resources on which to query

374
00:25:48,976 --> 00:25:52,656
an action. So you have an action which you do something but targets

375
00:25:52,688 --> 00:25:56,224
is where that action will actually be executed.

376
00:25:56,352 --> 00:25:59,876
You can define targets when you create an experiments template

377
00:25:59,988 --> 00:26:04,120
and you can use the same target for multiple actions in your experiment.

378
00:26:04,460 --> 00:26:08,500
So targets include the resource type, resource ids,

379
00:26:08,580 --> 00:26:11,784
tags and filters, and also a selection mode

380
00:26:11,832 --> 00:26:15,656
if you want off, then if you want random, if you just want a percentage

381
00:26:15,768 --> 00:26:17,020
and so forth.

382
00:26:19,120 --> 00:26:22,860
Here is an example JSON representation of targets.

383
00:26:23,280 --> 00:26:26,512
We are using in this example to filter target only

384
00:26:26,566 --> 00:26:30,460
by instances that are running on one specific availability zone.

385
00:26:30,540 --> 00:26:34,336
You can see here us east one a but that's not

386
00:26:34,438 --> 00:26:37,872
only filter here, there is also another filter for tags

387
00:26:37,936 --> 00:26:42,624
to refine the selection. So only EC

388
00:26:42,672 --> 00:26:46,324
two instances that are running on us fis to can a with

389
00:26:46,362 --> 00:26:50,228
the tag environment equals test that actually

390
00:26:50,314 --> 00:26:53,800
are going to be impacted. And there is also another

391
00:26:53,870 --> 00:26:57,944
filter that only instances that are running and only instances that are

392
00:26:57,982 --> 00:27:01,556
within a specific VPC. And you can see that selection mode

393
00:27:01,588 --> 00:27:05,336
is we just want two instances. So if there are more instances,

394
00:27:05,448 --> 00:27:08,584
those instance will not be affected. We're just selecting

395
00:27:08,632 --> 00:27:12,540
two. And there are other combinations that you can do like percentages

396
00:27:13,220 --> 00:27:15,890
or random as you saw in the example before.

397
00:27:17,700 --> 00:27:21,344
And experiment templates define can experiment and

398
00:27:21,382 --> 00:27:24,300
are used in the start experiment request.

399
00:27:24,460 --> 00:27:27,780
So think of an experiments template where you put

400
00:27:27,930 --> 00:27:31,748
an action target and everything else together.

401
00:27:31,914 --> 00:27:35,712
So all that information will be put together into an experiment

402
00:27:35,776 --> 00:27:38,996
template. And an experiment template include an

403
00:27:39,018 --> 00:27:42,328
action that we talk about it, a target. Then you

404
00:27:42,334 --> 00:27:45,896
have some optional information like stop condition alarms that we

405
00:27:45,918 --> 00:27:49,304
highly recommend you to always have. Stop condition alarms. So if

406
00:27:49,342 --> 00:27:52,924
something goes south that you're not expecting to do, you can

407
00:27:52,962 --> 00:27:57,096
actually automatically stop the experiment.

408
00:27:57,208 --> 00:28:00,492
You also have an IM row which will be associated to

409
00:28:00,626 --> 00:28:03,952
executing those experiments, and description and some

410
00:28:04,006 --> 00:28:07,168
text. When you look at

411
00:28:07,174 --> 00:28:10,588
the nature of the template,

412
00:28:10,684 --> 00:28:15,788
you look something like this. You can look at the right adjacent demonstration

413
00:28:15,884 --> 00:28:19,124
of a stop and restart instance. That's the name

414
00:28:19,162 --> 00:28:22,436
of the experiment template. You have a description there. You have

415
00:28:22,458 --> 00:28:25,940
a row arm that will actually be used to

416
00:28:26,010 --> 00:28:31,284
assume the row and then execute the

417
00:28:31,322 --> 00:28:35,124
specific action. So you need to make sure that row has the specific permissions

418
00:28:35,172 --> 00:28:39,208
to execute that. Then you look at the target section where we talk about

419
00:28:39,294 --> 00:28:42,990
and then we look at the actions which we also talked about.

420
00:28:44,080 --> 00:28:47,564
So let's look at two experiment templates that

421
00:28:47,602 --> 00:28:51,404
are very different, but two ideas that you can create with

422
00:28:51,442 --> 00:28:54,880
fault injection simulator as explained earlier, you can

423
00:28:54,950 --> 00:28:58,176
run a simple experiment like the one in the left, which is just

424
00:28:58,198 --> 00:29:01,344
a sequential experiment with two

425
00:29:01,382 --> 00:29:04,160
actions across three different targets,

426
00:29:04,500 --> 00:29:08,470
and you have a target group there without maybe

427
00:29:09,720 --> 00:29:14,752
not a lot of filtering and a specific target

428
00:29:14,816 --> 00:29:17,876
group, filtering of tags and so forth. And then you

429
00:29:17,898 --> 00:29:21,704
have a stop condition there. But you can also do something like

430
00:29:21,742 --> 00:29:25,064
on the right where you have a target which you

431
00:29:25,102 --> 00:29:28,132
are filtering of all Ec two instance with chaos

432
00:29:28,196 --> 00:29:31,592
ready tag. And then you have actions that

433
00:29:31,646 --> 00:29:34,844
you have a combination of sequential actions. So action one will

434
00:29:34,882 --> 00:29:38,012
happen, then action two will happen, and once action two

435
00:29:38,066 --> 00:29:41,404
happens, action three will

436
00:29:41,442 --> 00:29:45,900
happen as well. So you can do both parallel and sequential

437
00:29:46,060 --> 00:29:49,712
actions, and you can configure multiple stop

438
00:29:49,766 --> 00:29:53,244
conditions like we highly recommend in your production environment,

439
00:29:53,292 --> 00:29:57,012
but also in test, especially on production of course, that you have those

440
00:29:57,066 --> 00:30:00,420
stop conditions. And in this example you can see they have two stop

441
00:30:00,490 --> 00:30:01,380
conditions.

442
00:30:03,400 --> 00:30:07,264
And finally, experiments are simply a snapshot

443
00:30:07,312 --> 00:30:10,588
of the experiment template when it was first launched.

444
00:30:10,704 --> 00:30:14,264
So you can see on the system an execution list of all the

445
00:30:14,302 --> 00:30:18,104
experiments. So every time you click to launch an

446
00:30:18,142 --> 00:30:21,348
experiment template, you automatically create an experiment

447
00:30:21,444 --> 00:30:25,416
that you can look at who actually initiated that experiments,

448
00:30:25,528 --> 00:30:28,924
what was the result of the experiment, and all the data is there.

449
00:30:28,962 --> 00:30:32,932
So experiments will include fis next dropped of the experiment

450
00:30:33,096 --> 00:30:36,128
template that you're using. What is the creation and start time,

451
00:30:36,214 --> 00:30:39,330
what is the status of the experiments, the execution id,

452
00:30:39,940 --> 00:30:43,424
the IAM row arm and

453
00:30:43,462 --> 00:30:44,770
few other information.

454
00:30:46,760 --> 00:30:50,144
So when we look currently, what are the supported

455
00:30:50,192 --> 00:30:54,164
fault injections that FIS supports? You can see

456
00:30:54,202 --> 00:30:57,412
that you have a lot of things

457
00:30:57,466 --> 00:31:01,300
on this list and this list will keep growing. You can do

458
00:31:01,370 --> 00:31:05,524
server errors on ECQ, you can do API throttling

459
00:31:05,572 --> 00:31:09,144
on IAM, you can queue process on ECQ, you can

460
00:31:09,262 --> 00:31:12,636
add latency injections on ECQ, you can

461
00:31:12,658 --> 00:31:15,848
queue container instances on ecs,

462
00:31:15,944 --> 00:31:19,976
and you can do that on eks as well by terminating nodes. And recently

463
00:31:20,008 --> 00:31:23,020
uvi just announced network disruption,

464
00:31:23,360 --> 00:31:27,004
EbS I o pause and few others. And you see

465
00:31:27,042 --> 00:31:29,090
these lists growing with time.

466
00:31:29,860 --> 00:31:33,564
So let's look at some use cases for default injection

467
00:31:33,612 --> 00:31:36,912
simulator service. Let's look

468
00:31:36,966 --> 00:31:40,596
how we see some customers adopting chaos engineering, both in the sense of

469
00:31:40,618 --> 00:31:43,620
getting started and some of the more advanced practice.

470
00:31:45,000 --> 00:31:48,368
So we are first going to talk about one off experiments.

471
00:31:48,464 --> 00:31:52,532
And this is perhaps one of the most common ways of doing KS engineerings.

472
00:31:52,676 --> 00:31:56,004
This can for instance be experiments where you want to verify

473
00:31:56,052 --> 00:31:59,416
a new service with your system or a specific part of your

474
00:31:59,438 --> 00:32:02,824
architecture, or maybe expose monitoring blind spots.

475
00:32:02,952 --> 00:32:06,316
You create a one off experiment, you go through all the phases of

476
00:32:06,338 --> 00:32:09,100
chaos engineer from understanding the steady state,

477
00:32:09,250 --> 00:32:12,684
forming an hypothesis, designing and running that

478
00:32:12,722 --> 00:32:16,064
experiment, and so on. And this is really a

479
00:32:16,102 --> 00:32:19,580
great starting point of chaos engineer. You do a one off experiment,

480
00:32:19,660 --> 00:32:23,136
and you prove your hypothesis. Nothing broke, and you

481
00:32:23,158 --> 00:32:26,370
verified something within your system success.

482
00:32:26,900 --> 00:32:30,756
Or perhaps you have disapproved your hypothesis and something chaos happened,

483
00:32:30,858 --> 00:32:34,336
and you're going to improve. The goal is that you have learned

484
00:32:34,368 --> 00:32:37,968
something about your system and you were able to implement

485
00:32:38,064 --> 00:32:42,760
improvements. But those are just one off experiments.

486
00:32:43,100 --> 00:32:46,452
You have another common use case for chaos engineering,

487
00:32:46,596 --> 00:32:50,104
which is a part of a game day. A game day fis, a process of

488
00:32:50,142 --> 00:32:54,376
rehearsing ahead of an event by creating

489
00:32:54,408 --> 00:32:58,460
an anticipated conditions and then observing how effective

490
00:32:58,880 --> 00:33:02,236
the teams and system respond. An event could be

491
00:33:02,258 --> 00:33:05,470
an unusually high traffic day, maybe, let's say during

492
00:33:06,080 --> 00:33:09,580
a promotion day of your ecommerce, or a new launch,

493
00:33:09,660 --> 00:33:13,788
or a failure or something else. So you grab things together, you prepare

494
00:33:13,804 --> 00:33:17,548
for that game day, and you can use chaos engineering experiments to

495
00:33:17,574 --> 00:33:20,432
run a game day. By creating those event conditions,

496
00:33:20,496 --> 00:33:24,000
you monitor the system, you see how your organization behave,

497
00:33:24,080 --> 00:33:25,940
and you make new improvements.

498
00:33:27,240 --> 00:33:30,680
Another use case is automated experiments.

499
00:33:31,340 --> 00:33:34,820
Doing an automated experiment really goes back to scientific

500
00:33:34,900 --> 00:33:38,468
part of the Chaos engineering. Repeating experiments

501
00:33:38,564 --> 00:33:42,300
is a standard scientific practice for most fields.

502
00:33:43,280 --> 00:33:45,630
Automated experiments help us,

503
00:33:47,520 --> 00:33:51,192
help us cover a large set of experiments

504
00:33:51,336 --> 00:33:54,628
that we can knock over manually,

505
00:33:54,824 --> 00:33:59,232
and it verifies our assumption over time as

506
00:33:59,286 --> 00:34:03,392
unknown parts of the systems are changed. So instead of just running one

507
00:34:03,446 --> 00:34:07,204
off and maybe every couple of every six

508
00:34:07,242 --> 00:34:10,784
months or so, you have those automated experiments

509
00:34:10,832 --> 00:34:13,536
that as your architecture is changing,

510
00:34:13,648 --> 00:34:16,964
you're also doing those automated experiments, so you

511
00:34:17,002 --> 00:34:20,660
don't rely into a lot of people in a lot of organization.

512
00:34:20,820 --> 00:34:24,664
You have that automated experiments that are repeating itself

513
00:34:24,782 --> 00:34:28,088
in place. Let's talk a

514
00:34:28,094 --> 00:34:32,300
little bit more about some examples of automated experiments.

515
00:34:33,920 --> 00:34:38,072
The first automated experiment is recurring sketch experiments.

516
00:34:38,136 --> 00:34:42,552
So this is a great way to start with automated experiments,

517
00:34:42,696 --> 00:34:46,544
which is just to verify your assumption over time. Take for an

518
00:34:46,582 --> 00:34:49,792
instance, let's give an example where different teams build

519
00:34:49,846 --> 00:34:53,424
and deploy their own services within the system. So it's very common

520
00:34:53,462 --> 00:34:57,692
in distributed systems where you have multiple dozens

521
00:34:57,756 --> 00:35:01,664
or thousands or hundreds of microservices that are managed by

522
00:35:01,702 --> 00:35:05,700
different systems. How do I know that the behavior that I verify

523
00:35:06,120 --> 00:35:10,140
through chaos engineer experiment today is still valid tomorrow?

524
00:35:10,320 --> 00:35:13,896
So recurring schedule is a way that you can run maybe

525
00:35:13,998 --> 00:35:17,784
every hour, every day, or every week, and you can keep monitoring those

526
00:35:17,822 --> 00:35:21,596
conditions by adding and injecting fault into

527
00:35:21,698 --> 00:35:24,380
your evolution of your architecture.

528
00:35:25,200 --> 00:35:28,590
So that is one way. Now, let's look at

529
00:35:28,960 --> 00:35:32,156
automated experiments based on an

530
00:35:32,178 --> 00:35:35,936
event trigger idea. So an event is something

531
00:35:36,038 --> 00:35:39,024
that happens or is regarded AWS happening.

532
00:35:39,222 --> 00:35:43,020
So can assistant, could be, let's say an order FIS placed, a user login,

533
00:35:43,100 --> 00:35:46,852
or even an outscaling event. So let's say

534
00:35:46,986 --> 00:35:50,884
what if we get latency to our downstream service when

535
00:35:50,922 --> 00:35:54,372
there fis an autoscaling event? Does that affect our

536
00:35:54,426 --> 00:35:57,940
users? Well, using event driven

537
00:35:58,280 --> 00:36:02,004
trigger experiments, we can verify the behavior.

538
00:36:02,132 --> 00:36:05,544
You can create an experiment template and trigger an

539
00:36:05,582 --> 00:36:08,756
experiments based on what an autoscale event occurs.

540
00:36:08,788 --> 00:36:12,612
So you can say, well, when I see an upscale of traffic

541
00:36:12,756 --> 00:36:16,688
or some specific user action, please trigger chaos

542
00:36:16,884 --> 00:36:20,840
experiment and let's verify how the infrastructure

543
00:36:20,920 --> 00:36:24,384
and the workload behaves during that specific time.

544
00:36:24,502 --> 00:36:28,220
So with time you can think about those event triggering

545
00:36:28,300 --> 00:36:31,740
and within AWS you can use Eventbridge to automate

546
00:36:31,820 --> 00:36:35,730
a lot of that. Then of course

547
00:36:36,200 --> 00:36:40,224
you have chaos engineering, part of your CI CD

548
00:36:40,272 --> 00:36:44,000
pipeline, continuous integration,

549
00:36:44,080 --> 00:36:47,136
continuous delivery, continuous deployment pipeline.

550
00:36:47,328 --> 00:36:51,012
You can add a stage in your pipeline that automatically

551
00:36:51,156 --> 00:36:55,076
starts one or multiple experiments against your newly

552
00:36:55,108 --> 00:36:58,356
deployed application. So for instance, in the stage environment

553
00:36:58,468 --> 00:37:02,084
before you push into production, you start multiple experiments

554
00:37:02,212 --> 00:37:06,108
by triggering fault injection simulator services and the specific

555
00:37:06,194 --> 00:37:09,676
experiments templates. By doing so every time there

556
00:37:09,698 --> 00:37:13,080
is a new push of code into a specific environment.

557
00:37:13,160 --> 00:37:16,696
In my example in staging, you're very fine that the output

558
00:37:16,728 --> 00:37:20,668
of your system's each deployment. So you need a lot of observability

559
00:37:20,764 --> 00:37:24,464
tools to collect all the data and analyze. And this

560
00:37:24,502 --> 00:37:28,196
again will help us verify our assumptions because you

561
00:37:28,218 --> 00:37:31,636
created the experiment template based on the assumptions and hypothesis that

562
00:37:31,658 --> 00:37:35,380
you have, and the unknown parts of the systems are changed.

563
00:37:38,360 --> 00:37:41,800
So I think it goes without saying, but it is still worth pointing out that

564
00:37:41,870 --> 00:37:45,796
to do an automated experiment, you do need to embrace safeguards.

565
00:37:45,908 --> 00:37:50,040
So it's really important that you use the guard rails and the stop conditions within

566
00:37:50,110 --> 00:37:53,548
fault injection simulator. So if something

567
00:37:53,634 --> 00:37:57,500
happens, especially when you are doing a lot of those events,

568
00:37:58,480 --> 00:38:02,168
that you are automating a lot of those events, you don't know potentially

569
00:38:02,264 --> 00:38:05,776
you're not just clicking a button, looking those events happened like in

570
00:38:05,798 --> 00:38:10,428
one off experiments. You want to make sure you can automatically

571
00:38:10,524 --> 00:38:14,192
pause an experiment that has brought your

572
00:38:14,246 --> 00:38:18,260
potentially application into a degraded

573
00:38:18,760 --> 00:38:22,196
situation. So it's really important. And I highlight again,

574
00:38:22,378 --> 00:38:24,470
please be very careful with that.

575
00:38:26,200 --> 00:38:29,584
So I mentioned that aim for automation,

576
00:38:29,632 --> 00:38:32,740
but the journey of chaos engineering,

577
00:38:32,820 --> 00:38:36,424
you should start with one off experiments, potentially create

578
00:38:36,462 --> 00:38:39,800
some schedule. Once you are comfortable with those, run some game days

579
00:38:39,870 --> 00:38:43,640
from game days. You can start with some schedule

580
00:38:43,800 --> 00:38:46,632
chaos engineering experiments,

581
00:38:46,696 --> 00:38:50,220
invocation, and then you can go through more automated ones,

582
00:38:50,290 --> 00:38:53,420
potentially with event driven or your CI CD.

583
00:38:55,140 --> 00:38:58,530
What I want you just before we pause here for demo.

584
00:38:59,140 --> 00:39:03,276
If you are interested and you would like to use fault injection simulator

585
00:39:03,388 --> 00:39:07,184
within your organizations, please check out these resources.

586
00:39:07,312 --> 00:39:11,104
You have five links here. The first one is the AWS well architected

587
00:39:11,152 --> 00:39:14,356
framework that provides best practice and guidance on how

588
00:39:14,378 --> 00:39:17,552
to build workloads that are fault resilience.

589
00:39:17,616 --> 00:39:21,140
They are fault protective resiliency,

590
00:39:21,220 --> 00:39:25,770
highly available, cost optimized, secure and

591
00:39:26,140 --> 00:39:29,624
operationalized for production. You can click on the link

592
00:39:29,662 --> 00:39:33,536
to check more on our website for the fault injection simulator

593
00:39:33,588 --> 00:39:37,180
service. If you're interested, I highly recommend you go

594
00:39:37,250 --> 00:39:41,432
on your own time, on your own AWS account. The chaos Engineer workshop.

595
00:39:41,496 --> 00:39:46,316
You guide you step by step on how to do some of those experiments

596
00:39:46,348 --> 00:39:49,936
that are common across multiple companies. You can

597
00:39:49,958 --> 00:39:53,600
check the file injection simulator documentation and also there are

598
00:39:53,670 --> 00:39:57,636
a GitHub repository that is publicly that has a lot

599
00:39:57,658 --> 00:40:00,800
of examples. So you can just copy the JSON files

600
00:40:00,880 --> 00:40:04,768
and you can run those examples, build on top of those examples,

601
00:40:04,784 --> 00:40:06,820
or just reuse those examples.

602
00:40:08,680 --> 00:40:11,988
So now I'll do a very simple demo

603
00:40:12,074 --> 00:40:14,296
just for the sake of time. I don't have a lot of time. I'll do

604
00:40:14,318 --> 00:40:18,184
a simple demo just showing the console and how

605
00:40:18,222 --> 00:40:21,108
you can get started with fault injection simulator.

606
00:40:21,204 --> 00:40:25,244
So I'll see you in a moment as I transition to my screen

607
00:40:25,362 --> 00:40:28,636
on the AWS console. Okay, so let's jump into the

608
00:40:28,658 --> 00:40:31,964
demo. What do I want to show you on the demo will be

609
00:40:32,002 --> 00:40:35,680
a simple example and I'm just sharing.

610
00:40:36,180 --> 00:40:39,952
As you can see, this diagram will be an application that

611
00:40:40,006 --> 00:40:43,664
has two EC, two instances being managed by

612
00:40:43,702 --> 00:40:47,740
an auto scaling group and they're just running nginx

613
00:40:47,820 --> 00:40:51,284
as a web server. What I'm going to demonstrate to

614
00:40:51,322 --> 00:40:54,544
you, I'm going to create some load synthetic

615
00:40:54,592 --> 00:40:57,684
cloud. Just because this is a demo, I don't have people using this

616
00:40:57,722 --> 00:41:01,428
web server. So I'm going to create some synthetic cloud and we're going

617
00:41:01,434 --> 00:41:06,200
to create an experiment where we are going to terminate one of those instances

618
00:41:06,700 --> 00:41:10,136
that are part of my autoscaling group. So I have two instance as part

619
00:41:10,158 --> 00:41:13,516
of my autoscaling group I want to terminate one and

620
00:41:13,538 --> 00:41:17,432
then I want to use my monitoring dashboard

621
00:41:17,496 --> 00:41:20,876
and the observability data that I have collected to understand

622
00:41:20,978 --> 00:41:24,636
what type of behavior my application have. It's very simple, but I will

623
00:41:24,658 --> 00:41:28,064
create it. I'll show you step by step how

624
00:41:28,102 --> 00:41:31,376
you can create that using fault injection simulator. And then

625
00:41:31,398 --> 00:41:34,896
we're going to look at some of the results. So the

626
00:41:34,918 --> 00:41:38,084
hypothesis there is I have an application,

627
00:41:38,282 --> 00:41:41,264
they have two EC,

628
00:41:41,312 --> 00:41:44,790
two instances managed by an auto scaling group.

629
00:41:45,240 --> 00:41:49,104
What happens? The assumption and hypothesis

630
00:41:49,152 --> 00:41:53,016
there is my application shouldn't suffer a outage because

631
00:41:53,118 --> 00:41:56,328
I have two instances and one will still be

632
00:41:56,334 --> 00:41:59,844
serving traffic through the load balancer. So I have a load balancer

633
00:41:59,892 --> 00:42:03,548
endpoint, as you can see here. I'll just refresh. This is

634
00:42:03,554 --> 00:42:07,548
the load balancer endpoint that is just providing data.

635
00:42:07,634 --> 00:42:12,456
And as you can see I'm going to be paging the phpinfo

636
00:42:12,568 --> 00:42:15,816
PHp web page and I'm

637
00:42:15,848 --> 00:42:19,404
going to use that. So before we jump

638
00:42:19,452 --> 00:42:22,528
there, I want to show you a quick dashboard. So this is

639
00:42:22,534 --> 00:42:26,124
something you need to be in place in order to have the observe

640
00:42:26,252 --> 00:42:29,524
of part of the chaos engineering. So I have this

641
00:42:29,562 --> 00:42:33,056
dashboard on Cloudwatch. So this is Cloudwatch is a monitoring

642
00:42:33,088 --> 00:42:36,548
tool, a managed service on AWS that supports a lot

643
00:42:36,554 --> 00:42:40,240
of the monitoring metrics, logging and observability.

644
00:42:40,400 --> 00:42:43,812
So here I have a dashboard that collects a lot of the graphics.

645
00:42:43,956 --> 00:42:47,124
So the first one will be the customer load connection status.

646
00:42:47,172 --> 00:42:51,128
So I can see if there are any error status, any 500,

647
00:42:51,214 --> 00:42:55,404
400 errors or 200. In this case you don't see any data just

648
00:42:55,442 --> 00:42:59,308
because there is nothing really there. Right now. On the other side you

649
00:42:59,314 --> 00:43:02,444
can see the server NgInX connection status. You can see that has

650
00:43:02,482 --> 00:43:06,240
been just one because my load balancer is just pinging

651
00:43:07,620 --> 00:43:11,356
those Ec two to see if they're healthy in order to redirect

652
00:43:11,388 --> 00:43:15,056
traffic to them. Then you can see response times. Of course there is

653
00:43:15,078 --> 00:43:19,060
nothing there because there is no traffic being generated by the load test

654
00:43:19,130 --> 00:43:23,076
that I'm going to do. You can see the response time as well. There is

655
00:43:23,098 --> 00:43:27,312
pretty much nothing. Then I collect cpu utilization.

656
00:43:27,456 --> 00:43:30,760
You can see 99% fis idle, so there is nothing

657
00:43:30,830 --> 00:43:34,068
running there. You can see some of the network status.

658
00:43:34,244 --> 00:43:38,228
So tcp time, weight very little and tcp

659
00:43:38,244 --> 00:43:42,404
established over here. So currently no network connection.

660
00:43:42,532 --> 00:43:45,644
And down below here you can see two more graphics. Let me move

661
00:43:45,682 --> 00:43:48,030
this here so it's easier for us to see.

662
00:43:49,200 --> 00:43:53,196
You can see that I have the number of number

663
00:43:53,218 --> 00:43:56,704
of instance on my altiscating group, and then I can see the number of

664
00:43:56,742 --> 00:44:00,112
healthy versus unhealthy on my alti scaling group.

665
00:44:00,246 --> 00:44:03,424
So I have one healthy count on one

666
00:44:03,462 --> 00:44:07,032
availability zone and another healthy count in another availability zone.

667
00:44:07,196 --> 00:44:10,640
And the health check for my cloud balancer,

668
00:44:10,720 --> 00:44:13,872
I can see that both instance are healthy.

669
00:44:13,936 --> 00:44:17,248
And finally the instance check of my autoscaling

670
00:44:17,264 --> 00:44:20,932
group, they're both healthy. So let's

671
00:44:20,996 --> 00:44:24,676
first jump into the dashboard, the console, and let's

672
00:44:24,708 --> 00:44:29,224
create a fault injection simulator. So first let's check.

673
00:44:29,422 --> 00:44:33,416
Let's just go for fault injection simulator fis.

674
00:44:33,528 --> 00:44:36,872
Let's go into the service and let's create an experiment

675
00:44:36,936 --> 00:44:40,632
template. As I demonstrated and explained before, an experiments

676
00:44:40,696 --> 00:44:45,648
template to be a combination of things that we want to test

677
00:44:45,734 --> 00:44:49,932
can hypothesis. So in this hypothesis

678
00:44:50,076 --> 00:44:53,920
we're just going to call the description terminate

679
00:44:54,340 --> 00:44:58,768
half of the instances in a altiscaling group.

680
00:44:58,934 --> 00:45:02,832
And the name let's just call terminate half of

681
00:45:02,886 --> 00:45:06,868
instances. So you just give a description, you give a name.

682
00:45:07,034 --> 00:45:10,296
Now you have an action, if you remember an action fis, something that

683
00:45:10,318 --> 00:45:13,752
you want Fis to go and do. So this

684
00:45:13,806 --> 00:45:17,370
we're going to call terminate instance as the name.

685
00:45:18,540 --> 00:45:22,296
You can do any like the name is optional, the description is optional,

686
00:45:22,328 --> 00:45:26,376
sorry. And the action type here, you can just type terminate instance.

687
00:45:26,488 --> 00:45:30,140
We are going to use the pre built action called

688
00:45:30,290 --> 00:45:33,644
EC two terminate instance. What this actually does

689
00:45:33,682 --> 00:45:37,024
behind the scenes, you actually terminate an instance and I'll show

690
00:45:37,062 --> 00:45:40,912
you how it actually works. I don't need to start after

691
00:45:40,966 --> 00:45:45,164
because we're just doing a simple experiment and it automatically

692
00:45:45,212 --> 00:45:48,676
creates a target. And I will show in a moment what the

693
00:45:48,698 --> 00:45:52,324
target has and how we can program to be more on

694
00:45:52,362 --> 00:45:56,356
what we actually need. So I'm going to click save so

695
00:45:56,378 --> 00:45:59,732
it automatically create a target for me. Let's click

696
00:45:59,786 --> 00:46:03,076
edit because right now it just gives the

697
00:46:03,098 --> 00:46:06,804
name and the resource which they are correctly

698
00:46:06,852 --> 00:46:10,488
here I can just call a ski target just so he has a

699
00:46:10,494 --> 00:46:13,852
better idea. But I don't want to

700
00:46:13,986 --> 00:46:17,416
manually select the EC two instance because remember my auto

701
00:46:17,448 --> 00:46:20,604
scaling group is managing this. So I want the

702
00:46:20,642 --> 00:46:24,168
target to be selected by resource tags

703
00:46:24,184 --> 00:46:27,584
and I'll show in a moment what are those tags? So I'm just

704
00:46:27,622 --> 00:46:31,404
looking here to make sure I have those correctly. So the resource tag

705
00:46:31,452 --> 00:46:35,676
will be, I'll give a name and

706
00:46:35,718 --> 00:46:39,236
I want the target to

707
00:46:39,258 --> 00:46:45,136
be filtered by tag name fis Stackasgashg.

708
00:46:45,328 --> 00:46:49,192
But I also want a filter. I only want

709
00:46:49,246 --> 00:46:52,584
the fault injection simulator to look for

710
00:46:52,702 --> 00:46:57,064
instances that are running. So the state of

711
00:46:57,102 --> 00:47:01,252
the instance needs to be running and then the selection mode.

712
00:47:01,396 --> 00:47:04,824
I don't want all the instance because I know if I terminate

713
00:47:04,872 --> 00:47:09,390
all the instance my application be down. I want to say 50%

714
00:47:09,760 --> 00:47:13,336
of my instances. In this case I only have two instances.

715
00:47:13,368 --> 00:47:16,972
So one instance needs to be a new random

716
00:47:17,036 --> 00:47:19,756
select. One of those two needs to be terminated.

717
00:47:19,868 --> 00:47:22,130
So I'll go and I click save.

718
00:47:23,380 --> 00:47:27,504
In this case I have already configured an IM role that has permission to

719
00:47:27,542 --> 00:47:30,820
do those actions like terminate instance on my autoscaling group.

720
00:47:30,890 --> 00:47:34,756
So I'm just going to use the FIS workshop service role, but you

721
00:47:34,778 --> 00:47:37,812
would need to do that. And here, for the sake of the demo,

722
00:47:37,866 --> 00:47:41,124
I'm not going to create any stop act conditions, but I highly

723
00:47:41,172 --> 00:47:44,376
recommend every single time you create a stop condition. So if

724
00:47:44,398 --> 00:47:47,032
something happens that is outside your control,

725
00:47:47,166 --> 00:47:50,424
a metric gets triggered on Cloudwatch and stop

726
00:47:50,542 --> 00:47:54,044
your experiments. And we want

727
00:47:54,082 --> 00:47:57,800
to send logs to Cloudwatch logs. So I'm just going to browse

728
00:47:57,960 --> 00:48:01,500
and we have a bucket called fis workshop.

729
00:48:05,920 --> 00:48:09,916
1 second, I think it's here. Yes, fis logs.

730
00:48:10,028 --> 00:48:13,344
Here is where I want to save the logs. So all the logs of the

731
00:48:13,382 --> 00:48:16,592
experiment on the things it's going to be doing is going to be saved on

732
00:48:16,646 --> 00:48:20,276
s three, sorry, on cloud watch logs. And then you can

733
00:48:20,298 --> 00:48:23,588
look at the cloud watch logs. And here it's just giving a

734
00:48:23,594 --> 00:48:26,340
name. So I'm going to go create experiment,

735
00:48:27,240 --> 00:48:30,696
create experiments template here. He asked me, are you sure you want to

736
00:48:30,718 --> 00:48:34,196
create an experiment template without a stop condition?

737
00:48:34,388 --> 00:48:37,770
So this is a stop, like this is a

738
00:48:38,540 --> 00:48:41,624
warning for you. In this case, because of the demo, I'm just going to say

739
00:48:41,662 --> 00:48:45,340
create. But you should always, in your production environment,

740
00:48:45,680 --> 00:48:48,956
most definitely should have that stop condition. So I'm just going to go

741
00:48:48,978 --> 00:48:52,684
and create experiment template. So here I have

742
00:48:52,722 --> 00:48:56,464
my experiment template. You can look at the targets, targets are

743
00:48:56,502 --> 00:49:00,608
looking to terminate ec two instances that have this specific

744
00:49:00,694 --> 00:49:04,784
resource stack and having this filter. So let me just show

745
00:49:04,822 --> 00:49:08,320
you those instances. So you have an idea that I'm not

746
00:49:08,390 --> 00:49:12,160
just lying to you and there is no vaporware.

747
00:49:12,320 --> 00:49:15,412
So if we look here, we have two

748
00:49:15,466 --> 00:49:17,776
instances, fis stack,

749
00:49:17,808 --> 00:49:21,556
ASG that are running one in

750
00:49:21,578 --> 00:49:24,984
us east one b, another in us east, one a, that are

751
00:49:25,022 --> 00:49:28,872
managing by an outscaling group. So if I go on my altiscaling group

752
00:49:29,006 --> 00:49:32,296
and I show you I have this out scaling

753
00:49:32,328 --> 00:49:36,396
group that has desired capacity of two and minimum capacity of

754
00:49:36,418 --> 00:49:39,624
two. And I look at instance management,

755
00:49:39,752 --> 00:49:43,468
I have two instances with the specific

756
00:49:43,554 --> 00:49:47,104
launch template, us east one a and us east one b.

757
00:49:47,222 --> 00:49:50,608
And if I click on this instance, it just redirects to that one

758
00:49:50,774 --> 00:49:54,284
that has the specific tag that we are searching.

759
00:49:54,332 --> 00:49:58,064
So it's going to select one of those when I start experiment to

760
00:49:58,102 --> 00:50:01,556
actually go MQ. So what

761
00:50:01,578 --> 00:50:04,980
we want to do, because we don't have cloud in my, this is just a

762
00:50:05,050 --> 00:50:08,172
demo, we don't have cloud. I want to run just a simple script

763
00:50:08,256 --> 00:50:12,084
that is generating synthetic traffic to my instance.

764
00:50:12,132 --> 00:50:16,116
So I have just a script here that we call some lambda

765
00:50:16,148 --> 00:50:18,840
functions to generate load.

766
00:50:19,420 --> 00:50:23,096
So now once we have generated load, you see

767
00:50:23,118 --> 00:50:26,860
in a moment, these graphics will start picking up in a few

768
00:50:27,010 --> 00:50:30,316
seconds or minutes, start picking up load tests, but at the

769
00:50:30,338 --> 00:50:34,444
same time. So let's just maybe give a few seconds

770
00:50:34,572 --> 00:50:39,744
and then let's just watch the

771
00:50:39,782 --> 00:50:43,664
load picking up and then let's queue an

772
00:50:43,702 --> 00:50:48,036
instance and let's see what we

773
00:50:48,058 --> 00:50:51,556
can observe by that. Our hypothesis fis,

774
00:50:51,578 --> 00:50:54,628
the application should be remaining online, but will

775
00:50:54,714 --> 00:50:58,336
actually be fully online, or are we going to see any errors

776
00:50:58,368 --> 00:51:02,212
of connections or maybe too much traffic? So while we wait

777
00:51:02,266 --> 00:51:05,796
for that, we start cloudwatch. There is a little bit of delay

778
00:51:05,828 --> 00:51:09,624
to show the metrics for me because the logs are being generated and

779
00:51:09,662 --> 00:51:13,412
displayed on the dashboard. I'm just going to start my experiment.

780
00:51:13,556 --> 00:51:15,900
So I'm going to go on the console,

781
00:51:16,800 --> 00:51:20,472
I'm just going to go experiment, sorry, I'm going to go on my experiment

782
00:51:20,536 --> 00:51:24,156
template. I'm going to select the one I created which is terminate

783
00:51:24,188 --> 00:51:28,300
half the instances and I'm going to click start experiment.

784
00:51:28,460 --> 00:51:33,200
I can add a tag, I can say name forced experiments,

785
00:51:34,020 --> 00:51:37,856
you can just call whatever you want for this experiment and

786
00:51:37,878 --> 00:51:41,044
I'll click start experiment. You ask me again,

787
00:51:41,082 --> 00:51:44,452
like, are you sure you want to start this experiment? Because you have no stop

788
00:51:44,506 --> 00:51:46,950
condition. So if something goes outside your control,

789
00:51:47,960 --> 00:51:51,272
you have the ability to stop that experiment again,

790
00:51:51,326 --> 00:51:54,490
because it's a demo, we are fine, we're just going to click start.

791
00:51:55,900 --> 00:51:59,940
So click start. You can see that this is initiating

792
00:52:00,020 --> 00:52:03,064
state. We're just going to click refresh.

793
00:52:03,112 --> 00:52:06,924
It's on running state. You'll take a few seconds to

794
00:52:06,962 --> 00:52:11,116
actually run. Let's just wait a little bit here and

795
00:52:11,138 --> 00:52:14,648
you can see on the timeline, I think if you refresh

796
00:52:14,744 --> 00:52:19,088
it running, you can look at the logs, the logs will be published here.

797
00:52:19,254 --> 00:52:22,944
Once that action and experiment has finished and

798
00:52:22,982 --> 00:52:25,200
what this is actually doing behind the scenes,

799
00:52:26,420 --> 00:52:29,504
it's terminating one of those Ec two instances. So you can see it's

800
00:52:29,552 --> 00:52:33,124
completed, you can see on the timeline and

801
00:52:33,162 --> 00:52:36,804
we refresh it just terminated instance because it's just one

802
00:52:36,842 --> 00:52:40,536
thing and you see the logs in a moment will actually be. Here they are

803
00:52:40,558 --> 00:52:45,064
actually. Now it starts the experiment and

804
00:52:45,102 --> 00:52:48,650
then it terminates the instance. So here you can see

805
00:52:49,820 --> 00:52:53,630
action has completed, it terminated the instance for me.

806
00:52:54,640 --> 00:52:58,456
So if we go and we look into the auto

807
00:52:58,488 --> 00:53:01,836
scaling group and we refresh, you can

808
00:53:01,858 --> 00:53:05,532
see that one instance. Now it's unhealthy because

809
00:53:05,586 --> 00:53:09,620
it terminated my instance and because autoscaling

810
00:53:09,640 --> 00:53:13,410
group chaos, a desire or two, it's automatically putting that,

811
00:53:13,780 --> 00:53:17,804
creating a new instance. But now if you look at the cloud watch dashboards,

812
00:53:17,932 --> 00:53:21,104
we can see now that we have cloud, right? So we can see that load

813
00:53:21,152 --> 00:53:25,044
now 2900 actually have been

814
00:53:25,082 --> 00:53:28,660
successful, but there is a lot of requests that are getting 500

815
00:53:28,730 --> 00:53:31,924
HTTP errors. So my application is still up and running.

816
00:53:31,962 --> 00:53:35,432
And if I go and I try to refresh, you can see that

817
00:53:35,566 --> 00:53:39,512
it's running. But I might get a gateway error here, I might get a 500

818
00:53:39,566 --> 00:53:43,644
error because the main reason now I only have one ec two

819
00:53:43,682 --> 00:53:47,276
instance. So you saw it took time, and now you can

820
00:53:47,298 --> 00:53:51,548
see the cpu usage. Before it was nothing,

821
00:53:51,634 --> 00:53:55,216
but now it's more than 60%, right. And you can see

822
00:53:55,238 --> 00:53:59,232
that some of the network connections are waiting. Not everything

823
00:53:59,286 --> 00:54:02,396
is waiting, but you can see that the millisecond response

824
00:54:02,428 --> 00:54:06,096
time is decreasing. So as I refresh this

825
00:54:06,118 --> 00:54:09,604
page, you saw it was not very taking

826
00:54:09,642 --> 00:54:12,710
a while. You see this is spinning, it's not doing a good job.

827
00:54:15,240 --> 00:54:19,636
If I go down below, you can see that now my

828
00:54:19,658 --> 00:54:22,760
dashboard recognized that they only have one healthy,

829
00:54:23,100 --> 00:54:26,744
and I also only see one healthy. But you can see now the outscaling group

830
00:54:26,782 --> 00:54:30,024
is spinning up another instance. So within a few seconds or

831
00:54:30,062 --> 00:54:34,344
minutes you see better connections because another instance

832
00:54:34,392 --> 00:54:40,492
will be serving traffic. But while

833
00:54:40,546 --> 00:54:43,616
these experiments is ongoing, it's very simple. What we

834
00:54:43,638 --> 00:54:47,184
were able to observe is if I have a peak of

835
00:54:47,222 --> 00:54:50,770
traffic and one of the instance goes down,

836
00:54:51,380 --> 00:54:54,960
I'm not really able to serve quality traffic

837
00:54:55,620 --> 00:54:59,024
with performance to my customers. And you can see this,

838
00:54:59,062 --> 00:55:02,752
when you look at this, you see that there are a lot of error counts.

839
00:55:02,816 --> 00:55:06,324
There is mostly almost half, actually more

840
00:55:06,362 --> 00:55:10,132
than half of those requests are errors. You're getting

841
00:55:10,186 --> 00:55:13,304
500 or some error connections. You're not even be

842
00:55:13,342 --> 00:55:17,224
able to achieve connections. So you see it's taking quite a bit of time,

843
00:55:17,342 --> 00:55:20,744
which the latency has increased. So that's what you

844
00:55:20,782 --> 00:55:23,852
obseRve. So if I were the owner of this application,

845
00:55:23,986 --> 00:55:28,888
I would potentially increase the pool of scaling instance,

846
00:55:28,984 --> 00:55:32,600
the pool of instance that I have on my altiscaling to maybe potentially

847
00:55:32,680 --> 00:55:35,528
four, maybe across three or four availability zones,

848
00:55:35,624 --> 00:55:38,736
depending on the region that this is running. So this

849
00:55:38,758 --> 00:55:41,868
is really simple. And then as you scroll down, you could see that it picked

850
00:55:41,884 --> 00:55:45,544
up the LaTENCY. So some of the requests, the duration maximum

851
00:55:45,612 --> 00:55:50,004
was 2.6 seconds, but average now is

852
00:55:50,042 --> 00:55:53,444
just 1.3. But when we look at

853
00:55:53,482 --> 00:55:57,444
potentially for 1 hour, you could see that

854
00:55:57,562 --> 00:56:01,512
it was Much lower. And because we're running the

855
00:56:01,566 --> 00:56:05,444
experiments now with a lot of load, he has increased the traffic.

856
00:56:05,572 --> 00:56:09,876
And if we scroll down, we only have one instance healthy.

857
00:56:09,988 --> 00:56:13,336
But now we have Three instances, two instances that are

858
00:56:13,358 --> 00:56:17,468
BACK at our auto scaling group. So ELB is now doing the health check

859
00:56:17,554 --> 00:56:21,484
in order to bring it up my instance. And you

860
00:56:21,522 --> 00:56:25,132
see in a moment we might not have enough time to finalize

861
00:56:25,196 --> 00:56:28,544
here. Just watch the service cpu. But you see the service

862
00:56:28,582 --> 00:56:32,304
cpu will be much better once that instance is

863
00:56:32,342 --> 00:56:36,256
in place. So this is a very simple

864
00:56:36,438 --> 00:56:40,164
example and you can look at the experiments. So once you click

865
00:56:40,202 --> 00:56:43,956
on experiments you're able to see all the experiments. You can

866
00:56:43,978 --> 00:56:47,764
click on the experiment id and you can find the timeline. In this

867
00:56:47,802 --> 00:56:51,176
case I'm just doing an action, but you can have a sequence, you can

868
00:56:51,198 --> 00:56:54,616
do many parallel and you

869
00:56:54,638 --> 00:56:58,788
can start experiments. If you remember when you were talking about automated experiments

870
00:56:58,884 --> 00:57:02,296
as part of a recurring event using Eventbridge or it

871
00:57:02,318 --> 00:57:06,156
can be part of your CI CD, you can now mix and

872
00:57:06,178 --> 00:57:09,532
match a lot of different combination and the whole idea here is

873
00:57:09,586 --> 00:57:13,688
to have in mind the continuous resiliency and improve the performance

874
00:57:13,784 --> 00:57:15,870
and availability of your application.

875
00:57:16,420 --> 00:57:19,856
So that was it for the demo. I do hope you

876
00:57:19,878 --> 00:57:23,232
were able to take away some of the key learnings from thought

877
00:57:23,286 --> 00:57:27,344
injection simulator. I highly recommend you go through the workshop and feel

878
00:57:27,382 --> 00:57:31,424
free to reach out to the service team and myself and

879
00:57:31,462 --> 00:57:35,810
anyone on the AWS team if you have any feedback or

880
00:57:36,580 --> 00:57:39,784
just in general if you want to share your experience.

881
00:57:39,982 --> 00:57:44,644
Thank you so much everyone, it was a pleasure. Wish you a great remaining

882
00:57:44,692 --> 00:57:45,940
of the conference.

