1
00:00:22,010 --> 00:00:25,638
Hello, my name is Eduardo Dixo, I'm a senior data scientist at Continental and

2
00:00:25,644 --> 00:00:28,966
today I'm going to talk to you about object detectors using cnns and

3
00:00:28,988 --> 00:00:32,934
transformers applied to images recorded by datasets. First I'm going to

4
00:00:32,972 --> 00:00:36,246
introduce the task of object detectors and also the data set that

5
00:00:36,268 --> 00:00:40,226
we'll be using. Next we'll see some common CNNS based architectures

6
00:00:40,258 --> 00:00:43,734
like the faster RCNN and retinate before discussing

7
00:00:43,782 --> 00:00:47,898
the transformer and seeing how the detection transformer performs on our data set.

8
00:00:47,984 --> 00:00:51,766
So let's begin by first introducing the task of object detection taskoff.

9
00:00:51,798 --> 00:00:55,114
Object detection can be regarded as given an input

10
00:00:55,162 --> 00:00:58,714
image. We want to find all the objects that are present in that image.

11
00:00:58,842 --> 00:01:03,546
So we need to spatially locate them using bounding

12
00:01:03,578 --> 00:01:07,026
boxes and also we need to classify them into a set of

13
00:01:07,048 --> 00:01:10,962
predefined classes. If we compare the task of object detection with

14
00:01:11,096 --> 00:01:14,738
image classification, for example, in image classification we

15
00:01:14,824 --> 00:01:18,054
usually have a single main target. In object detection we may

16
00:01:18,092 --> 00:01:22,098
have different number of objects present in that image

17
00:01:22,194 --> 00:01:26,370
with different poses, with different scales. And this makes the task

18
00:01:26,530 --> 00:01:30,038
very challenging, more challenging than the image classification. For example,

19
00:01:30,124 --> 00:01:33,146
the data set that we are going to use is those visron data set that

20
00:01:33,168 --> 00:01:36,534
contains nearly 6000 training images and 500 validation

21
00:01:36,582 --> 00:01:39,850
images. It also contains ten categories from which we are only interested

22
00:01:39,920 --> 00:01:43,214
in cars. We are going to build an object detector for

23
00:01:43,332 --> 00:01:47,054
detecting only one class, which will be cars. And this

24
00:01:47,092 --> 00:01:50,142
data set is interesting because it records the

25
00:01:50,196 --> 00:01:53,950
images and the different conditions like different weather,

26
00:01:54,030 --> 00:01:57,298
different lighting, different object density of

27
00:01:57,304 --> 00:02:01,566
the scenes, different scales of the objects. We have some fast motion artifacts

28
00:02:01,678 --> 00:02:04,786
because of the movement of the cars or the movement of

29
00:02:04,808 --> 00:02:08,566
those drone during flight. And also the bounding boxes are annotated for

30
00:02:08,588 --> 00:02:12,294
occlusion and those truncation. Some applications of

31
00:02:12,332 --> 00:02:16,454
training such object detector could be interesting

32
00:02:16,572 --> 00:02:20,602
for road safety, traffic monitoring or

33
00:02:20,656 --> 00:02:24,470
even driving assistance as finding free tracking

34
00:02:24,630 --> 00:02:28,474
slots. First, we make a distinction between the one

35
00:02:28,592 --> 00:02:32,142
stage and two stage object detectors. Two stage object

36
00:02:32,196 --> 00:02:36,154
detectors contain a region proposal network

37
00:02:36,282 --> 00:02:40,990
that will output high confidence region proposals

38
00:02:41,410 --> 00:02:44,930
that should contain an object on it. So it's not concerned

39
00:02:46,230 --> 00:02:50,146
what is the class of the object in it, it's only concerned if there is

40
00:02:50,168 --> 00:02:53,582
an object or not. And then the object detector head that typically

41
00:02:53,646 --> 00:02:57,510
drones bounding box regression for finding the position of the object

42
00:02:57,580 --> 00:03:01,414
and object classification to find its class can attend to these

43
00:03:01,452 --> 00:03:05,174
proposed regions and by doing so it will

44
00:03:05,212 --> 00:03:08,634
have a much smaller set of candidate regions that might

45
00:03:08,672 --> 00:03:12,614
have an object. And this will eliminate

46
00:03:12,662 --> 00:03:16,566
many of the case positives that we would have otherwise. A one shot

47
00:03:16,598 --> 00:03:20,166
detector, on the other hand, generates a dense sampling

48
00:03:20,198 --> 00:03:24,078
of possible object locations. So it will generate lots of

49
00:03:24,164 --> 00:03:27,934
object candidate locations with different shapes and

50
00:03:27,972 --> 00:03:31,438
different aspect ratios, and it will process them directly to

51
00:03:31,444 --> 00:03:34,898
learn the class labels and bounding boxes. The first model that we are

52
00:03:34,904 --> 00:03:38,670
going to discuss is the faster RCNN. The faster RCNn

53
00:03:38,750 --> 00:03:42,014
is a two stage object detector that employs two models,

54
00:03:42,062 --> 00:03:45,986
a region proposal network, and also the classifier head

55
00:03:46,088 --> 00:03:49,910
that has the bounding box, regression and object classification. We will start

56
00:03:49,980 --> 00:03:53,494
by following those typical data flow of the image as

57
00:03:53,532 --> 00:03:56,962
it goes through the architecture. So the image goes through the backbone.

58
00:03:57,026 --> 00:04:00,430
The backbone goal is to extract eye

59
00:04:00,450 --> 00:04:03,706
level semantic feature maps from the image. That will be

60
00:04:03,728 --> 00:04:06,950
useful later for the region proposal network and for the classifier.

61
00:04:07,110 --> 00:04:10,346
This can be typically achieved by any of the

62
00:04:10,368 --> 00:04:14,106
shelf convolutional architectures like Rasnet or Vgg.

63
00:04:14,298 --> 00:04:18,138
As the image goes through these several convolutional

64
00:04:18,154 --> 00:04:21,706
layers, it gets downsampled

65
00:04:21,818 --> 00:04:25,630
so it will have smaller width and smaller height,

66
00:04:25,710 --> 00:04:29,138
but much more depth, meaning that the feature map of the

67
00:04:29,144 --> 00:04:32,770
last stage of the backbone will have

68
00:04:32,920 --> 00:04:36,722
many channels. Next we have this region proposal network.

69
00:04:36,786 --> 00:04:40,726
This region proposal network will predict the object bounds as well

70
00:04:40,828 --> 00:04:44,406
as the objectness cars. Meaning if it is

71
00:04:44,428 --> 00:04:49,110
an object or not and it's a fully convolutional network,

72
00:04:49,770 --> 00:04:52,966
it will receive as input the feature maps from the backbone.

73
00:04:53,078 --> 00:04:56,714
It will slide those window over these feature maps. At each

74
00:04:56,752 --> 00:05:00,594
point of those sliding window it will generate k anchor boxes.

75
00:05:00,662 --> 00:05:04,462
The number of anchor boxes is parameterized by this k and

76
00:05:04,516 --> 00:05:08,458
it will have two sibling networks for the outputs,

77
00:05:08,554 --> 00:05:12,190
one that is two times the number of anchor boxes

78
00:05:13,970 --> 00:05:17,134
for the score classification in foreground and

79
00:05:17,172 --> 00:05:20,526
background, and the other one will be four times the

80
00:05:20,548 --> 00:05:24,042
number of anchor boxes for the bounding box coordinates.

81
00:05:24,186 --> 00:05:28,802
Finally, now we have a set of regions proposed

82
00:05:28,866 --> 00:05:32,614
by this region proposal network module, and in a very naive way,

83
00:05:32,652 --> 00:05:36,642
we could simply crop the image using these proposal regions

84
00:05:36,706 --> 00:05:40,326
and feed it into another classifier just

85
00:05:40,348 --> 00:05:43,722
to get the object class. However, we want to make those

86
00:05:43,776 --> 00:05:46,986
end to end and to reuse the feature maps that we have

87
00:05:47,008 --> 00:05:50,266
computed from the backbone. And for doing so, we are going

88
00:05:50,288 --> 00:05:54,414
to map the feature maps to the proposals of the region proposal network using

89
00:05:54,452 --> 00:05:58,154
this region of interest pulling layer that will extract

90
00:05:58,202 --> 00:06:02,250
then fixed size features maps from each of these proposals

91
00:06:02,330 --> 00:06:05,422
from the feature map. The reason these are fixed size

92
00:06:05,476 --> 00:06:09,618
is because we are going to use fully connected layer that expects fixed size.

93
00:06:09,784 --> 00:06:13,570
Then we have this classifier that will predict the object

94
00:06:13,640 --> 00:06:17,106
class as well as the bounding box coordinates.

95
00:06:17,218 --> 00:06:21,030
We are going to use the detection tool library which is a pytorch based

96
00:06:21,100 --> 00:06:24,934
deep learning framework for object detectors and

97
00:06:24,972 --> 00:06:29,660
also semantic segmentation. And we are using to use

98
00:06:30,190 --> 00:06:34,134
faster RCNn with a Resnet 50 backbone

99
00:06:34,262 --> 00:06:38,106
using fully features pyramid networks the reason we

100
00:06:38,128 --> 00:06:41,482
are going to use these feature pyramid networks is because we have

101
00:06:41,536 --> 00:06:45,054
images in our data set that have very

102
00:06:45,092 --> 00:06:48,766
small scale so we have small cars and also large cars that we

103
00:06:48,788 --> 00:06:52,430
want to detectors depending on the altitude that the drone is

104
00:06:52,500 --> 00:06:55,986
flying. And by using these feature pyramid networks we

105
00:06:56,008 --> 00:06:59,326
can improve the multiscale object detection because those goal

106
00:06:59,358 --> 00:07:03,166
of the feature pyramid network is to build these eye level semantic

107
00:07:03,198 --> 00:07:07,302
feature maps across all the pyramid levels from

108
00:07:07,436 --> 00:07:11,078
a single image of a single

109
00:07:11,244 --> 00:07:15,186
resolution. This is done by merging

110
00:07:15,218 --> 00:07:18,994
the bottom pathway which is the feature maps

111
00:07:19,122 --> 00:07:22,986
from our CNN backbone that then are upsampled through

112
00:07:23,008 --> 00:07:26,294
those top line pathway and merged through lateral

113
00:07:26,342 --> 00:07:30,294
connections in the feature pyramid network

114
00:07:30,342 --> 00:07:34,202
architecture. For training the faster RCNN the first step is to register

115
00:07:34,266 --> 00:07:38,078
our data set. We do this so that

116
00:07:38,244 --> 00:07:41,120
the detectors two knows how to obtain it.

117
00:07:41,810 --> 00:07:46,290
If we already have the annotations in adjacent cocoa

118
00:07:47,350 --> 00:07:51,054
format, we can use the register cocoa instances directly.

119
00:07:51,182 --> 00:07:54,386
In this case we have prepared the annotations in this format so we can use

120
00:07:54,408 --> 00:07:58,102
those register cocoa instances and we also pass the

121
00:07:58,236 --> 00:08:01,602
base path images so it knows where to fetch the images

122
00:08:01,666 --> 00:08:05,320
from. Next, detectors two uses the key value

123
00:08:06,170 --> 00:08:09,734
config system based on YaML files

124
00:08:09,862 --> 00:08:13,798
that provide already some common functionality and operations.

125
00:08:13,894 --> 00:08:17,366
If we require more advanced features, we can drop down to the Python's

126
00:08:17,398 --> 00:08:20,506
API or also derive from a

127
00:08:20,528 --> 00:08:24,030
base config file and implement the attributes. And in here

128
00:08:24,100 --> 00:08:27,834
what we are going to do is first we load the default configuration

129
00:08:27,882 --> 00:08:31,054
file. We then inherit from

130
00:08:31,092 --> 00:08:33,990
the configuration file of the model that we want to fine tune.

131
00:08:34,170 --> 00:08:38,494
We specify the training and test data sets that we already registered previously.

132
00:08:38,542 --> 00:08:41,954
We specify the number of workers for the multiprocessing part

133
00:08:42,072 --> 00:08:45,874
and we load the pretrained model weights from

134
00:08:45,912 --> 00:08:49,462
the detectors two model zoo. Then we have the learning rate,

135
00:08:49,516 --> 00:08:52,726
the maximum number of iterations, the batch size, and the steps at

136
00:08:52,748 --> 00:08:56,642
which to decay the learning rate. All of these are very important parameters

137
00:08:56,706 --> 00:09:00,886
that we should tune to get the best metrics,

138
00:09:00,918 --> 00:09:04,298
but also to squeeze the best performance out

139
00:09:04,304 --> 00:09:07,674
of the GPU. And then we specify the number of classes for

140
00:09:07,792 --> 00:09:11,450
this particular architecture, which is one

141
00:09:11,600 --> 00:09:15,066
because we are only interested in detectors cars.

142
00:09:15,258 --> 00:09:18,862
Finally, we can launch the training using the default trainer class

143
00:09:18,916 --> 00:09:21,738
that provide out of the box standard training logic.

144
00:09:21,834 --> 00:09:26,090
If we require, we could also implement our own Python

145
00:09:26,250 --> 00:09:30,458
training loop or also subclass this default trainer

146
00:09:30,554 --> 00:09:33,898
in here. Since we are not loading from a checkpoint. We pass this resemb equal

147
00:09:33,914 --> 00:09:38,466
false. Now we take a look at a one stage detector.

148
00:09:38,578 --> 00:09:42,466
So retinate is a powerful one stage detector

149
00:09:42,498 --> 00:09:46,306
that employs the feature pyramid network that we have seen before that helps

150
00:09:46,338 --> 00:09:49,546
with a multiscale detection of

151
00:09:49,568 --> 00:09:53,354
the objects and also two

152
00:09:53,392 --> 00:09:56,554
civilian networks, one for classification and the other for bounding box

153
00:09:56,592 --> 00:10:00,658
regression. The one stage detectors were typically regarded

154
00:10:00,694 --> 00:10:04,906
as being faster than the two stage, but they were lagging the accuracy

155
00:10:05,018 --> 00:10:09,034
of the two stage detectors. So the authors of retinate

156
00:10:09,082 --> 00:10:12,350
attributed this to the eye class imbalance between

157
00:10:12,420 --> 00:10:15,230
foreground and background that may happen.

158
00:10:15,380 --> 00:10:20,034
And the reason is if you remember these

159
00:10:20,072 --> 00:10:23,534
one stage detectors, they will examples

160
00:10:23,662 --> 00:10:26,726
a large set of candidate regions, many of them will

161
00:10:26,748 --> 00:10:30,194
be background, will be easy negatives and they will not contribute

162
00:10:30,242 --> 00:10:33,766
with a useful learning signal for the network or they

163
00:10:33,788 --> 00:10:37,470
can even overwhelm the training loss.

164
00:10:37,570 --> 00:10:41,978
So what they propose is this nova loss called those focal loss that

165
00:10:42,144 --> 00:10:45,674
adds this modulating factor to the standard cross entropy and

166
00:10:45,712 --> 00:10:50,586
it will downweight the well classified examples so

167
00:10:50,608 --> 00:10:54,738
that the model can focus more on the other examples. For the retinate

168
00:10:54,774 --> 00:10:59,214
we are also going to use REsnet 50 backbone for

169
00:10:59,252 --> 00:11:02,698
comparison with the faster RCNN. We also use the detectors

170
00:11:02,714 --> 00:11:05,842
and two library for doing so. Registering the data

171
00:11:05,896 --> 00:11:08,994
set requires no changes. Launching the training also requires no

172
00:11:09,032 --> 00:11:12,702
changes, but we need to change the configuration file.

173
00:11:12,766 --> 00:11:16,322
So in this case we need to inherit from the appropriate model. We also

174
00:11:16,376 --> 00:11:20,294
need to load the appropriate models from the model zoo. And now

175
00:11:20,332 --> 00:11:24,278
for setting the number of classes we need to access a

176
00:11:24,284 --> 00:11:28,230
different attribute of the config which is under those retinate

177
00:11:28,830 --> 00:11:32,570
num classes. After training both models we see that

178
00:11:32,640 --> 00:11:36,502
they both have good cocoa evaluation

179
00:11:36,566 --> 00:11:40,574
metrics. We are using the average precision which

180
00:11:40,692 --> 00:11:44,474
basically penalizes missing detections

181
00:11:44,522 --> 00:11:48,266
and also detecting having too many duplicate

182
00:11:48,298 --> 00:11:51,822
detections for the same object. And we see

183
00:11:51,876 --> 00:11:55,906
that the average precision is very similar for each model. In this case the

184
00:11:55,928 --> 00:12:00,846
retina net is better at detecting larger

185
00:12:00,878 --> 00:12:04,766
objects but worse at smaller objects. But if we look at the average precision,

186
00:12:04,798 --> 00:12:08,386
they are very equally matched and also the inference results

187
00:12:08,418 --> 00:12:12,834
as well. Another thing that is commonly employed

188
00:12:12,882 --> 00:12:16,710
in computer vision is that augmentation for aiding in the generalization

189
00:12:18,170 --> 00:12:22,326
of the network. And the reason is that we want our object detection

190
00:12:22,358 --> 00:12:26,134
to work under different lighting, viewpoint scales, et cetera.

191
00:12:26,262 --> 00:12:29,814
So we can generate an augmentation policy that will bake

192
00:12:29,862 --> 00:12:33,038
these transformations there. And so

193
00:12:33,124 --> 00:12:36,542
we pass our data set through this augmentation policy, enriching our data

194
00:12:36,596 --> 00:12:40,126
set that we will then use for training our model.

195
00:12:40,308 --> 00:12:43,954
In this example we have an horizontal flip and

196
00:12:43,992 --> 00:12:47,906
also some we can see on the left the

197
00:12:47,928 --> 00:12:51,214
augmentation policy used and also some random brightness,

198
00:12:51,262 --> 00:12:53,570
some random saturation, some random contrast.

199
00:12:55,190 --> 00:12:58,754
For using this augmentation policy we use those that takes

200
00:12:58,792 --> 00:13:02,146
a data set. We use this data set mapper

201
00:13:02,178 --> 00:13:06,310
that takes a data set in detection two and then we map

202
00:13:07,130 --> 00:13:10,090
our data set into a format that will be used by the model,

203
00:13:10,160 --> 00:13:13,882
which is this dictionary with the keys it

204
00:13:13,936 --> 00:13:18,426
with image instances. So we

205
00:13:18,448 --> 00:13:21,962
read the image, we transform it using the augmentation policy

206
00:13:22,016 --> 00:13:25,966
that we have defined. We also need to be careful for transferring as

207
00:13:25,988 --> 00:13:29,934
well the bounding boxes and then of generating those

208
00:13:29,972 --> 00:13:33,294
data in the format the model expects. But we are not limited to

209
00:13:33,332 --> 00:13:36,318
use representations only from detectors tool.

210
00:13:36,404 --> 00:13:39,974
We can also integrate external libraries like algorithmations

211
00:13:40,042 --> 00:13:43,198
or cornea and these libraries

212
00:13:43,374 --> 00:13:46,862
have a very large collection of transformations

213
00:13:46,926 --> 00:13:50,786
that are not readily available in

214
00:13:50,808 --> 00:13:54,600
detectors tool like this random sandflower and that we can also use.

215
00:13:55,290 --> 00:13:58,954
One comment is that we used data

216
00:13:58,992 --> 00:14:02,650
augmentation for training the faster RCNN and the retina net,

217
00:14:02,800 --> 00:14:06,442
but we didn't see improvements even

218
00:14:06,496 --> 00:14:10,182
when training for more iteration steps.

219
00:14:10,326 --> 00:14:14,250
Now we will discuss those transformers. The transformer

220
00:14:14,330 --> 00:14:18,110
was originally proposed as a sequence to sequence model for machine translation

221
00:14:18,450 --> 00:14:21,274
and it is now a standard in natural language processing.

222
00:14:21,322 --> 00:14:25,582
But also it has found its way into computer vision and other tasks.

223
00:14:25,726 --> 00:14:29,710
It's a very general purpose architectures that lacks the inductive biases

224
00:14:29,790 --> 00:14:33,326
of cnns, for example the locality and translation invariance.

225
00:14:33,438 --> 00:14:36,758
But given large enough scale data, it can learn this from the data

226
00:14:36,844 --> 00:14:40,738
and perform on par or even surpassing the cnns.

227
00:14:40,834 --> 00:14:44,274
The vanilla transformers uses

228
00:14:44,322 --> 00:14:47,894
an encoder and a decoder. The encoder has

229
00:14:47,932 --> 00:14:51,494
two modules and the decoder, the multi head self

230
00:14:51,532 --> 00:14:55,450
attention and the feed forward network. And we employ around each models

231
00:14:55,790 --> 00:14:59,238
a residual connection and also layer normalization.

232
00:14:59,414 --> 00:15:02,734
The decoder also uses cross attentions. So in those

233
00:15:02,772 --> 00:15:06,734
cross attention the keys and values come from

234
00:15:06,772 --> 00:15:10,094
the encoder and the queries come from the decoder. And we

235
00:15:10,132 --> 00:15:14,222
also have when we talk about differences

236
00:15:14,286 --> 00:15:17,394
between applying these transformers from NLP to visions, we have

237
00:15:17,432 --> 00:15:21,314
these differences in scale and resolution scale being

238
00:15:21,352 --> 00:15:24,866
that in NLP the words serve as the basic elements of pre

239
00:15:24,888 --> 00:15:28,146
processing. But when we're talking about object detection, those objects

240
00:15:28,178 --> 00:15:31,954
may vary in scale, so they may be compared of a different number of pixels

241
00:15:32,082 --> 00:15:35,862
and resolution. If we think that, for example,

242
00:15:35,996 --> 00:15:39,958
those images are comprised of a

243
00:15:39,964 --> 00:15:43,782
big number of a large number of pixels. Since the soft

244
00:15:43,836 --> 00:15:47,174
attention is very central to the transformers, let's see what

245
00:15:47,212 --> 00:15:50,778
makes it so appealing when compared to other layers.

246
00:15:50,954 --> 00:15:54,366
We see that self attention. So in here, this table on

247
00:15:54,388 --> 00:15:58,026
the bottom left, the t stands for the sequence

248
00:15:58,058 --> 00:16:01,546
size and the d stands for the representation dimensionality

249
00:16:01,658 --> 00:16:05,086
of each part of the sequence. And we see that self attention is

250
00:16:05,108 --> 00:16:08,290
more parameter efficient and fully connected layers as well,

251
00:16:08,440 --> 00:16:12,510
better at handling arbitrary variants input sizes.

252
00:16:12,670 --> 00:16:16,130
And if we compare this to recurrent layers,

253
00:16:16,290 --> 00:16:19,666
it's also more perimeter efficient if the size of the sequence

254
00:16:19,698 --> 00:16:23,126
is smaller than the representation dimensionality when compared to

255
00:16:23,148 --> 00:16:26,482
convolutional layers. Convolutional layers

256
00:16:26,546 --> 00:16:30,722
for achieved a global receptive field, meaning that every pixel

257
00:16:30,786 --> 00:16:34,294
would interact with every pixel, we typically

258
00:16:34,342 --> 00:16:37,962
need to stack many of these convolutional layers on top of each other.

259
00:16:38,096 --> 00:16:41,566
And in self attention, all parts of those sequence interact with each

260
00:16:41,588 --> 00:16:45,166
other within a single layer. Let's take a look at how

261
00:16:45,188 --> 00:16:50,570
the self attention works. So the self attention relates

262
00:16:50,650 --> 00:16:54,206
different positions of a sequence to compute a different representation

263
00:16:54,238 --> 00:16:57,694
of that sequence. So we feed it as an input a sequence

264
00:16:57,742 --> 00:17:01,774
z, in this case of size t and dimensional

265
00:17:01,822 --> 00:17:05,150
td, and we compute three matrices, the queries,

266
00:17:05,230 --> 00:17:08,626
keys and values. We do so by multiplying the input with this

267
00:17:08,648 --> 00:17:12,374
matrix UQV and slicing along the last

268
00:17:12,412 --> 00:17:15,478
dimension, the dimension of the tree times the dimension of

269
00:17:15,484 --> 00:17:18,946
the head, and this will generate the queries, keys and values

270
00:17:18,978 --> 00:17:22,746
for us. Next, we compute the dot product between the queries and

271
00:17:22,768 --> 00:17:26,474
keys, so the queries and keys must have the same dimension and we

272
00:17:26,512 --> 00:17:29,850
divide by a scaling factor. To alleviate vanishing gradient problems,

273
00:17:30,000 --> 00:17:34,174
we apply a soft max in a row wise manner, and this will

274
00:17:34,212 --> 00:17:38,190
be our attention matrix that has size t

275
00:17:38,260 --> 00:17:41,466
by t. So it's quadratic to the size of the input

276
00:17:41,498 --> 00:17:45,326
sequence, which is one of the bottlenecks of the transformer. And then we multiply

277
00:17:45,358 --> 00:17:48,866
this by v, our matrix value to

278
00:17:48,968 --> 00:17:52,530
retrieve the final computation.

279
00:17:52,870 --> 00:17:56,526
However, the transformer

280
00:17:56,558 --> 00:17:59,766
doesn't use the regular self attention, it use a generalization of it,

281
00:17:59,868 --> 00:18:03,410
which is called a multi head self attention. Multi head self attention

282
00:18:03,490 --> 00:18:07,314
is an extension of the self attention in which we run case of attention

283
00:18:07,362 --> 00:18:11,018
operations in parallel. So we run

284
00:18:11,104 --> 00:18:14,570
many self attentions in parallel, we concatenate them,

285
00:18:14,640 --> 00:18:17,946
and then we do a linear projection again to

286
00:18:17,968 --> 00:18:22,474
the dimension d. To not explode the dimensionality.

287
00:18:22,602 --> 00:18:25,882
Let's now revisit the transformers after having seen how the self attention

288
00:18:25,946 --> 00:18:29,854
works. So in the original transformer we add

289
00:18:29,892 --> 00:18:33,122
an encoder and a recorded but we can also

290
00:18:33,176 --> 00:18:36,258
use only a part of the architecture. For example,

291
00:18:36,344 --> 00:18:39,250
architectures that only use those encoder part like vert,

292
00:18:40,870 --> 00:18:44,926
are important when we only want global representation

293
00:18:44,958 --> 00:18:48,294
of those sequence. And you want to build classification on top of it. For example

294
00:18:48,332 --> 00:18:52,262
for performing sentiment analysis. When we

295
00:18:52,316 --> 00:18:56,022
architectures that only use a decoder are used for

296
00:18:56,076 --> 00:18:59,586
language modeling like GPD two. And we also have architectures

297
00:18:59,618 --> 00:19:03,366
that use both encoder and decoder like detection transformer

298
00:19:03,398 --> 00:19:07,386
that we'll see next. A fact that is also important is that self attention is

299
00:19:07,408 --> 00:19:11,822
invariant to the position of those tokens. So it's very common to add

300
00:19:11,956 --> 00:19:15,550
these position encodings to the input so that those model can reason

301
00:19:15,620 --> 00:19:18,846
about the positions of the parts of the

302
00:19:18,868 --> 00:19:22,766
sequence during

303
00:19:22,868 --> 00:19:26,846
the self attention in the encoder and decoder

304
00:19:26,878 --> 00:19:30,066
blocks. And now we are going

305
00:19:30,088 --> 00:19:33,986
to talk about the detection transformer. The detection transformer is a

306
00:19:34,008 --> 00:19:37,366
very simple architectures that is based on a CNN and a

307
00:19:37,388 --> 00:19:40,614
transformer recorded architectures and it

308
00:19:40,652 --> 00:19:44,274
uses a CNN backbone. So we feed it an images

309
00:19:44,402 --> 00:19:48,026
and this image goes through the CNN backbone and

310
00:19:48,048 --> 00:19:52,010
generates a feature map with lower width

311
00:19:52,350 --> 00:19:55,610
and lower weights, but with a much deeper

312
00:19:56,190 --> 00:19:59,718
number of channels. And now we have this distancer

313
00:19:59,734 --> 00:20:03,546
of width, height and channels, but we want to feed it into the transformer

314
00:20:03,578 --> 00:20:07,166
encoder. But the transformer encoder is expecting a

315
00:20:07,188 --> 00:20:11,306
sequence. So the way we can do this is by flattening the spatial

316
00:20:11,338 --> 00:20:14,946
dimensions of the input, by multiplying the

317
00:20:14,968 --> 00:20:18,846
height and width and then we can feed it into the transformer encoder.

318
00:20:18,958 --> 00:20:22,498
Then we have the transformer decoder that has these object

319
00:20:22,584 --> 00:20:26,942
queries which are learned by the model as the input.

320
00:20:27,086 --> 00:20:30,854
And these object queries are the number of objects that we are trying to

321
00:20:30,892 --> 00:20:34,294
detect in an image. So it must be set to

322
00:20:34,332 --> 00:20:37,698
be larger than the largest number of objects

323
00:20:37,714 --> 00:20:41,114
that we have in an image to provide us some slack. And they will learn

324
00:20:41,152 --> 00:20:45,014
to attend to specific areas and specific bounding boxes

325
00:20:45,062 --> 00:20:48,746
sizes in an image. Then the decoder is

326
00:20:48,768 --> 00:20:52,698
also conditioned on the encoder output

327
00:20:52,794 --> 00:20:56,250
and we predict the classes,

328
00:20:56,410 --> 00:21:00,058
the object class and the bounding box through parallel decoding. So it's

329
00:21:00,074 --> 00:21:03,290
not in an autoregressive way. We output them in parallel

330
00:21:03,370 --> 00:21:07,122
and we are treating the object detectors problem as a direct set case

331
00:21:07,176 --> 00:21:11,154
prediction. So we need an appropriate loss for that. They use

332
00:21:11,192 --> 00:21:15,278
those bipartite matching loss based on the hungarian algorithm

333
00:21:15,374 --> 00:21:18,834
that is permutation invariant and also forces a unique

334
00:21:18,882 --> 00:21:22,594
assignment between the ground truth and the predicted objects.

335
00:21:22,722 --> 00:21:26,294
We are going to use the egging phase library that contains many

336
00:21:26,332 --> 00:21:30,630
transformers and they recently added those visual transformers

337
00:21:30,710 --> 00:21:33,914
for image classification like the visual transformer VIT and also

338
00:21:33,952 --> 00:21:37,706
this detection transformer for object detection. They added this

339
00:21:37,728 --> 00:21:41,310
to the library and we are going to use based on a REsnet 50

340
00:21:41,380 --> 00:21:44,842
backbone the reason we use this dilated convolutional

341
00:21:44,906 --> 00:21:48,394
is that the dilated convolution will increase

342
00:21:48,442 --> 00:21:52,590
the resolution by a factor of two at the expense of more computations,

343
00:21:53,570 --> 00:21:57,210
but it will help detecting small scale

344
00:21:57,290 --> 00:22:01,102
objects. Egin face provides a very comprehensive

345
00:22:01,166 --> 00:22:04,786
set of documentation. It also explains the internal part of those model and we also

346
00:22:04,888 --> 00:22:08,490
have these example notebooks by Niels rogue that are linked

347
00:22:08,510 --> 00:22:11,846
at the page and at the bottom of this slide. That explains how we

348
00:22:11,868 --> 00:22:15,990
can fine tune object object object object object object object object object

349
00:22:16,060 --> 00:22:19,846
object object detection transformers CNNS drone case

350
00:22:19,868 --> 00:22:23,630
feature extractor used for pre processing

351
00:22:23,810 --> 00:22:27,658
the input for the model or for post processing the output of the model in

352
00:22:27,664 --> 00:22:31,258
the cocoa notation format. For example for running the cocoa

353
00:22:31,274 --> 00:22:34,794
evaluation metrics we also have the data for object detection

354
00:22:34,842 --> 00:22:38,414
model that exposes those

355
00:22:38,452 --> 00:22:42,510
logits and the prediction boxes and also we have this data

356
00:22:42,660 --> 00:22:46,298
config that can be used for institiating

357
00:22:46,474 --> 00:22:49,794
data for object detection model. Through this

358
00:22:49,832 --> 00:22:54,114
configuration. The modifications that we do when

359
00:22:54,152 --> 00:22:58,114
compared to those notebook is that we use the REsnet with a little convolutions instead

360
00:22:58,152 --> 00:23:01,366
of the REsnet 50. We also set the maximum size of

361
00:23:01,388 --> 00:23:04,786
the image to 1100 to not eat good out of memory

362
00:23:04,818 --> 00:23:08,394
aircorse and we also use a smaller batch size of two instead

363
00:23:08,432 --> 00:23:11,946
of four because on v 100 gpu we

364
00:23:11,968 --> 00:23:16,460
use get out of memory aircraft. Otherwise, after training

365
00:23:16,830 --> 00:23:20,794
detectors transformers on our data set we see that average

366
00:23:20,922 --> 00:23:24,990
precision is very poor compared to the objects

367
00:23:25,410 --> 00:23:29,038
detections. Based on cnns we

368
00:23:29,044 --> 00:23:32,522
have seen previously, the model is able to detect large

369
00:23:32,596 --> 00:23:36,478
objects. It has a fairly good average precision

370
00:23:36,494 --> 00:23:40,210
for large objects, but it is very small for small and medium objects

371
00:23:40,710 --> 00:23:44,718
which can be attributed to the detection transformer

372
00:23:44,814 --> 00:23:48,530
not being suitable for these small scale

373
00:23:48,690 --> 00:23:52,194
object detectors problems and as feature pyramid

374
00:23:52,242 --> 00:23:56,102
networks did for cnns for

375
00:23:56,236 --> 00:24:00,074
helping addressing those multiscale object detectors problem.

376
00:24:00,272 --> 00:24:04,246
Similar approaches could also help improving the detection transformer

377
00:24:04,278 --> 00:24:10,702
further. We see in the inference results. We have some

378
00:24:10,756 --> 00:24:14,026
duplicate detections that could be probably removed

379
00:24:14,058 --> 00:24:17,982
by using non maximal suppression and we also have

380
00:24:18,036 --> 00:24:21,326
some missed detections. So how can

381
00:24:21,348 --> 00:24:24,846
we improve these results further? We can for example scale the

382
00:24:24,868 --> 00:24:28,706
backbone. In all of those experiments we use the resonate 50 but we could

383
00:24:28,728 --> 00:24:32,580
use a larger backbone like a REsnet 101.

384
00:24:33,350 --> 00:24:38,994
The results we had for the documentation didn't

385
00:24:39,042 --> 00:24:43,218
improve our results, but we could fine tune the probabilities

386
00:24:43,394 --> 00:24:47,350
or also change the augmentation transformations

387
00:24:47,710 --> 00:24:51,322
to find if we could get better results. Right now we also

388
00:24:51,376 --> 00:24:55,366
have more publicly available data sets

389
00:24:55,398 --> 00:24:58,022
recorded by drones like Miva,

390
00:24:58,086 --> 00:25:01,534
UAVDT and so on and we could use this to build

391
00:25:01,652 --> 00:25:04,782
a larger data set to see if we can get

392
00:25:04,916 --> 00:25:08,558
better results out of this. Also, we only used

393
00:25:08,724 --> 00:25:12,842
static images for the object detection part.

394
00:25:12,996 --> 00:25:17,566
But if we think about video object detection, we can exploit

395
00:25:17,678 --> 00:25:21,394
these temporal cues of the

396
00:25:21,432 --> 00:25:25,034
different frames to reduce the number of false positives.

397
00:25:25,182 --> 00:25:29,000
We also have different transformer architectures, for example

398
00:25:30,330 --> 00:25:33,986
the using transformer or the focal transformer

399
00:25:34,098 --> 00:25:37,850
that could be used and tested to see if

400
00:25:37,920 --> 00:25:41,574
they provide better results. To conclude,

401
00:25:41,702 --> 00:25:45,334
we see that CNNs make for very powerful baselines.

402
00:25:45,462 --> 00:25:49,142
We used off the shelf pre trained CNN

403
00:25:49,206 --> 00:25:53,182
architectures, the faster CNN and retinate and

404
00:25:53,236 --> 00:25:56,858
got very good average precision results in Visdron

405
00:25:56,954 --> 00:26:01,562
for detecting cars. The transformer architectures are being increasingly

406
00:26:01,626 --> 00:26:04,786
used in research and practice and we

407
00:26:04,808 --> 00:26:08,686
can see that they are being added to these mainstream libraries. Like egging

408
00:26:08,718 --> 00:26:11,822
case, for example, the detection transformer

409
00:26:11,886 --> 00:26:16,262
is better suited for medium to large to

410
00:26:16,316 --> 00:26:19,954
large objects. But developments similar to the feature pyramid

411
00:26:20,002 --> 00:26:23,382
network as it was

412
00:26:23,436 --> 00:26:27,266
used for CNNs can also help. The detection

413
00:26:27,298 --> 00:26:31,580
transformer and the transformers will continue to be used into

414
00:26:32,110 --> 00:26:35,430
downstream tasks like object detection, images classification

415
00:26:35,510 --> 00:26:39,434
and image representations. We can see many research papers coming from these

416
00:26:39,472 --> 00:26:42,606
areas and last but not least,

417
00:26:42,788 --> 00:26:47,002
transformers make from a unifying framework for different fields.

418
00:26:47,066 --> 00:26:50,650
So before we encode all of these inductive biases

419
00:26:50,730 --> 00:26:53,514
that we have for those CNNs and for the OSTMs.

420
00:26:53,642 --> 00:26:57,626
On the other hand, the transformer makes for a very general purpose architecture

421
00:26:57,658 --> 00:27:01,886
that lacks these inductive biases, but it can learn them from

422
00:27:02,068 --> 00:27:06,214
large scale data and it has

423
00:27:06,252 --> 00:27:09,346
given very good results for natural language processing

424
00:27:09,458 --> 00:27:12,850
and it's now also giving some state of the art results

425
00:27:12,930 --> 00:27:16,322
in image. And so it can maybe unify

426
00:27:16,386 --> 00:27:20,294
both fields and also unify the

427
00:27:20,332 --> 00:27:23,602
practitioners and researchers from both areas.

428
00:27:23,746 --> 00:27:27,026
So today, this concludes my presentation.

429
00:27:27,218 --> 00:27:29,170
I want to thank you for listening.

