1
00:00:38,850 --> 00:00:42,694
Hello. Today I'm going to talk about the complete handbook to

2
00:00:42,732 --> 00:00:46,374
open telemetry metrics. My name is Prathamish. I work as

3
00:00:46,412 --> 00:00:50,426
a developer evangelist. At last nine at last nine. We are building tools

4
00:00:50,458 --> 00:00:53,866
for observability and monitoring and we have a time series

5
00:00:53,898 --> 00:00:57,742
data warehouse called as levitate which you can use for

6
00:00:57,796 --> 00:01:01,422
all your high cardinality metric needs. You can store application,

7
00:01:01,556 --> 00:01:06,050
infra business and product metrics. It's an end to end monitoring solution.

8
00:01:06,630 --> 00:01:10,258
So today's agenda, we'll talk about why you should care

9
00:01:10,344 --> 00:01:13,442
about this talk. If you're already doing something else,

10
00:01:13,576 --> 00:01:17,398
why should you think about opentelemetry metrics in the first place?

11
00:01:17,564 --> 00:01:20,914
We'll discuss about Prometheus and opentelemetry metrics,

12
00:01:20,962 --> 00:01:24,678
the key differences between both of them. We'll talk about

13
00:01:24,764 --> 00:01:28,682
opentelemetry collector in detail, how it can help us and

14
00:01:28,736 --> 00:01:32,426
do lot of heavy lifting before shipping our metrics to

15
00:01:32,448 --> 00:01:36,186
the destination. We'll of course touch upon some of the

16
00:01:36,368 --> 00:01:39,542
formatting changes between otel metrics

17
00:01:39,606 --> 00:01:43,514
and then the conversion to Prometheus metrics. We'll touch upon

18
00:01:43,562 --> 00:01:47,882
some of the advanced concepts like cumulative versus delta temporality,

19
00:01:48,026 --> 00:01:51,486
and then we'll see what is coming in future. What are

20
00:01:51,508 --> 00:01:55,166
the roadmap for supporting hotel metrics

21
00:01:55,198 --> 00:01:58,498
natively in Prometheus? First of all, why should you

22
00:01:58,504 --> 00:02:02,434
care about this talk, right? Opentelemetry is getting lot

23
00:02:02,472 --> 00:02:05,606
of attention and it is getting a lot

24
00:02:05,628 --> 00:02:09,286
of adoption as well. It's the second most popular project

25
00:02:09,388 --> 00:02:13,314
in the CNCF landscape after kubernetes, and overall,

26
00:02:13,362 --> 00:02:16,738
the vendors that are supporting opentelemetry is increasing.

27
00:02:16,914 --> 00:02:20,918
The adopters who are trying to use opentelemetry

28
00:02:21,014 --> 00:02:24,486
is also increasing every day. It helps

29
00:02:24,518 --> 00:02:28,154
in certain senses. It helps us bring standardization to our

30
00:02:28,192 --> 00:02:32,014
opentelemetry. It helps us with vendor neutrality because

31
00:02:32,132 --> 00:02:35,854
we are not tied to one single vendor at all.

32
00:02:35,972 --> 00:02:39,306
Because of the API as well as spec standardization,

33
00:02:39,498 --> 00:02:42,640
we can try out different vendors for our data.

34
00:02:42,950 --> 00:02:46,162
It also has promise of signal correlation because

35
00:02:46,216 --> 00:02:49,934
it emits standard metadata for all signals

36
00:02:50,062 --> 00:02:54,062
and the naming conventions are also present so you can have correlation

37
00:02:54,126 --> 00:02:57,926
between your logs, metrics and traces. It has support for

38
00:02:57,948 --> 00:03:01,094
a lot of languages for sdks as well as

39
00:03:01,132 --> 00:03:04,342
client libraries so that you can start using them for

40
00:03:04,476 --> 00:03:08,154
hotel metrics. And then other projects are

41
00:03:08,192 --> 00:03:11,926
also considering adding native support for opentelemetry

42
00:03:11,958 --> 00:03:16,554
metrics. So there is an experimental support in Prometheus for

43
00:03:16,672 --> 00:03:20,426
consuming the Otel metrics, and there are some enhancements that

44
00:03:20,448 --> 00:03:24,506
are planned for future as well, which we will discuss. So that's

45
00:03:24,538 --> 00:03:28,174
why I think you should listen to this talk and know

46
00:03:28,212 --> 00:03:31,470
more about opentelemetry metrics. But before

47
00:03:31,540 --> 00:03:34,898
going to opentelemetry, let's talk about the standard

48
00:03:34,984 --> 00:03:38,254
of metrics that exists, which is Prometheus,

49
00:03:38,382 --> 00:03:41,426
which allows us to scrape metrics from

50
00:03:41,528 --> 00:03:44,994
applications different targets, and then store them,

51
00:03:45,032 --> 00:03:48,486
run alerting on them, create graphs and so on. You can

52
00:03:48,508 --> 00:03:51,974
also optionally remote write these metrics to a long

53
00:03:52,012 --> 00:03:55,750
term storage like levitate for better reliability and performance.

54
00:03:56,090 --> 00:03:59,106
So that is the typical Prometheus architecture.

55
00:03:59,218 --> 00:04:02,442
You basically scrape metrics. It's based on a pull based

56
00:04:02,496 --> 00:04:06,540
model. Optionally you can write it to a storage as well.

57
00:04:06,910 --> 00:04:10,582
Most importantly, data is collected as cumulative

58
00:04:10,646 --> 00:04:14,078
data. So if you collect something at t one,

59
00:04:14,164 --> 00:04:17,758
the value was one, then at t two the value became four

60
00:04:17,844 --> 00:04:21,280
because the difference between t one and t two was three.

61
00:04:21,650 --> 00:04:24,714
But the value that gets reported to Prometheus is four.

62
00:04:24,772 --> 00:04:28,594
The cumulative value very similarly, at t three, seven will

63
00:04:28,632 --> 00:04:31,678
get reported, and at t four, nine will get reported.

64
00:04:31,774 --> 00:04:35,586
And this will play an important role in our future discussion.

65
00:04:35,618 --> 00:04:39,602
I'll talk about that. How opentelemetry considers temporality

66
00:04:39,666 --> 00:04:43,666
slightly differences in terms of formats Prometheus

67
00:04:43,698 --> 00:04:47,554
supports two formats. There is a text exposition format of metrics

68
00:04:47,602 --> 00:04:51,146
which it supports, and of course it also supports the

69
00:04:51,248 --> 00:04:55,194
open metrics standard for metrics as well. It has

70
00:04:55,232 --> 00:04:58,650
float values for like the type of values is basically

71
00:04:58,720 --> 00:05:02,630
float. You can store all of them as float by default.

72
00:05:02,790 --> 00:05:06,190
It is based on a multidimensional data model using

73
00:05:06,260 --> 00:05:09,482
labels so that you can have any number of labels

74
00:05:09,546 --> 00:05:13,406
for each time series. Their values can be anything as well.

75
00:05:13,588 --> 00:05:17,026
And in terms of getting the data, it is based on the

76
00:05:17,128 --> 00:05:20,622
pull based scrape model. So those are the key characteristics

77
00:05:20,686 --> 00:05:24,874
of Prometheus. If you think about what is opentelemetry,

78
00:05:25,022 --> 00:05:28,914
it is basically a collection of APIs, sdks and tools.

79
00:05:29,042 --> 00:05:33,138
This is the standard definition that I have captured from opentelemetry

80
00:05:33,154 --> 00:05:37,058
website. You can use hotel to basically generate

81
00:05:37,154 --> 00:05:40,714
instrument, collect all your telemetry data, and even

82
00:05:40,752 --> 00:05:44,538
export it to different storage backends depending on your needs.

83
00:05:44,704 --> 00:05:48,890
The key difference here is that Otel supports all three signals and

84
00:05:48,960 --> 00:05:52,842
another key difference is that there is no storage backend. With Otel

85
00:05:52,986 --> 00:05:57,210
you have to use a vendor or open source tool like Prometheus

86
00:05:57,370 --> 00:06:01,130
for storing these metrics and other telemetry signals.

87
00:06:01,290 --> 00:06:04,766
If you talk about metrics, Prometheus is the natural backend for your

88
00:06:04,788 --> 00:06:08,894
Otel metrics. There are other hotel compatible

89
00:06:09,022 --> 00:06:12,786
metric backends as well. Levitate is one of them. There are others as

90
00:06:12,808 --> 00:06:16,086
well, datadog and new relic as well. But Prometheus is a

91
00:06:16,108 --> 00:06:20,114
natural choice because that has been the standard and pioneer

92
00:06:20,162 --> 00:06:23,480
for metric based platforms for a long time.

93
00:06:24,410 --> 00:06:27,074
So if you think about the open telemetry architecture,

94
00:06:27,202 --> 00:06:30,394
there are standards specifications, then there are

95
00:06:30,432 --> 00:06:33,814
sdks and client libraries, and then there are tools

96
00:06:33,862 --> 00:06:36,426
which can help you collect this data,

97
00:06:36,528 --> 00:06:40,374
transform this, and then export it to storage backends.

98
00:06:40,502 --> 00:06:44,494
Now we'll touch upon the standards as well as sdks as

99
00:06:44,532 --> 00:06:48,474
well, but let's look at the middleware tools because they play an extremely

100
00:06:48,522 --> 00:06:52,698
important role in the lifecycle of how the metrics are collected

101
00:06:52,794 --> 00:06:56,226
and then finally shipped to the storage. So there is

102
00:06:56,248 --> 00:06:59,998
a component called as hotel collector which basically connects

103
00:07:00,094 --> 00:07:03,550
the source of data to the destination backend.

104
00:07:03,710 --> 00:07:07,826
It does the heavy lifting of transforming processing

105
00:07:07,858 --> 00:07:11,286
the data. It can basically convert some

106
00:07:11,308 --> 00:07:15,362
parts of data for interoperability as well, and then finally it exports

107
00:07:15,426 --> 00:07:18,070
to the compatible storage backings.

108
00:07:18,430 --> 00:07:21,766
There is a hotel operator component as well which allows

109
00:07:21,798 --> 00:07:26,102
you to run hotel collectors at scale. In Kubernetes

110
00:07:26,246 --> 00:07:30,362
you can manage the service discovery. You can do auto instrumentation of

111
00:07:30,416 --> 00:07:34,234
your services in a Kubernetes environment using otel operator

112
00:07:34,362 --> 00:07:37,962
in this talk will not touch upon a lot on hotel operator,

113
00:07:38,106 --> 00:07:42,010
but will focus on hotel collector a lot because that plays

114
00:07:42,090 --> 00:07:45,246
very important role in terms of how hotel

115
00:07:45,278 --> 00:07:47,650
metrics can be used by the end user.

116
00:07:48,550 --> 00:07:52,414
So opentelemetry, as we said, does not have any storage

117
00:07:52,462 --> 00:07:56,146
backends. Prometheus is a good fit for storing

118
00:07:56,178 --> 00:07:59,666
your hotel metrics. It also supports storing

119
00:07:59,698 --> 00:08:03,254
them as well with certain charts that we will discuss.

120
00:08:03,452 --> 00:08:07,618
There are other commercial backends as well, like levitate neuralink Datadoc,

121
00:08:07,714 --> 00:08:11,450
where you can send your hotel metrics as well. If you think about the

122
00:08:11,520 --> 00:08:14,586
opentelemetry promise and what it allows you to

123
00:08:14,608 --> 00:08:18,202
do is basically being vendor neutral means you can try but

124
00:08:18,256 --> 00:08:21,914
different platforms. It helps you standardize your metrics

125
00:08:21,962 --> 00:08:25,514
by providing semantic conventions so all HTTP

126
00:08:25,562 --> 00:08:29,214
services will follow the same naming conventions for your

127
00:08:29,252 --> 00:08:33,130
metrics. There will be default metadata emitted

128
00:08:33,290 --> 00:08:36,978
for all the metrics so that you can correlate your metrics with

129
00:08:37,064 --> 00:08:40,466
other signals such as traces and logs as well.

130
00:08:40,648 --> 00:08:44,034
It also has certain design decisions which can help you

131
00:08:44,072 --> 00:08:47,926
do performance improvements in your pipelines because it can

132
00:08:47,948 --> 00:08:51,542
do some of the heavy lifting of transforming the metrics or even

133
00:08:51,596 --> 00:08:54,966
dropping running high cardinality workflows and all

134
00:08:54,988 --> 00:08:58,282
of that. If you think about the metrics project

135
00:08:58,336 --> 00:09:01,946
goals, it's important to know about it, because that

136
00:09:01,968 --> 00:09:05,594
will also help you understand why we need to talk

137
00:09:05,632 --> 00:09:09,826
about opentelemetry metrics and Prometheus metrics

138
00:09:09,958 --> 00:09:13,946
in this kind of a way. So the idea behind Opentelemetry

139
00:09:13,978 --> 00:09:17,374
metrics project was basically being able to

140
00:09:17,412 --> 00:09:21,694
connect metrics to other signals because hotel's aim was to

141
00:09:21,892 --> 00:09:25,726
support all three signals of observability metrics,

142
00:09:25,758 --> 00:09:28,946
logs and traces at the same time.

143
00:09:29,048 --> 00:09:32,194
At that time, the goal was also to give a way

144
00:09:32,232 --> 00:09:36,386
to customers who are using open census metrics so

145
00:09:36,408 --> 00:09:39,958
that they can migrate their projects to opentelemetry. And you

146
00:09:39,964 --> 00:09:43,874
will see a lot of the influence in the design spec of opentelemetry,

147
00:09:44,002 --> 00:09:47,462
which is influenced from opensensors. And then

148
00:09:47,516 --> 00:09:51,850
of course working with existing metric standards like Prometheus statsd

149
00:09:52,190 --> 00:09:55,754
so that the interoperability exists, you can move

150
00:09:55,792 --> 00:09:59,306
data from one system to another, you can push Otel metrics to

151
00:09:59,328 --> 00:10:03,310
Prometheus and so on. That was also one of the design goals.

152
00:10:03,810 --> 00:10:07,166
Now, if you think about hotel collector, and this

153
00:10:07,188 --> 00:10:10,686
is the diagram from the official website, but you have three

154
00:10:10,708 --> 00:10:13,986
layers. Basically you have receivers where you can receive data in

155
00:10:14,008 --> 00:10:17,854
different formats, including Prometheus and OTLP

156
00:10:17,902 --> 00:10:21,746
format. Then you have a pipeline of processors. So you

157
00:10:21,768 --> 00:10:25,106
can run these data streams via different

158
00:10:25,288 --> 00:10:29,666
processors. You can run it via batch processor, an aggregate processor,

159
00:10:29,778 --> 00:10:33,318
transformation processors, and at the end, after all the

160
00:10:33,324 --> 00:10:37,750
things are done, then you can export it to a storage backend of your choice.

161
00:10:38,910 --> 00:10:43,082
If you compare it with an earlier diagram that we had seen with

162
00:10:43,136 --> 00:10:46,266
Prometheus architecture model, this is how

163
00:10:46,288 --> 00:10:50,054
the Otel collector will look like. There will be some applications

164
00:10:50,102 --> 00:10:53,726
which can push the metrics in an OTLP format to

165
00:10:53,748 --> 00:10:57,706
the collector. The Otel collector also can scrape metrics

166
00:10:57,738 --> 00:11:01,566
from slash metrics. So if you have standard applications who

167
00:11:01,588 --> 00:11:05,022
are emitting metrics on slash metrics, Otel collector

168
00:11:05,086 --> 00:11:08,660
can just scrape those metrics as it is without changing anything.

169
00:11:09,190 --> 00:11:12,766
And then of course it can remote write it to a storage

170
00:11:12,798 --> 00:11:17,050
backend. Or it also exposes slash metrics endpoints.

171
00:11:17,070 --> 00:11:20,850
So you can have your own Prometheus which can scrape metrics

172
00:11:20,930 --> 00:11:24,486
from an Otel collector. Otel collector can also write

173
00:11:24,588 --> 00:11:27,894
push in OTLP format to other commercial

174
00:11:27,942 --> 00:11:30,998
storage backends as well. Recently,

175
00:11:31,094 --> 00:11:35,510
Prometheus has added the capability of pushing metrics

176
00:11:35,590 --> 00:11:38,538
to Prometheus in an OTLP format as well.

177
00:11:38,704 --> 00:11:42,202
And there are a lot of plans of supporting

178
00:11:42,266 --> 00:11:46,110
and improving this experience in future. But that option also

179
00:11:46,180 --> 00:11:49,966
exists where your hotel collector can just push now

180
00:11:50,068 --> 00:11:52,670
to Prometheus in an OTLP format.

181
00:11:53,670 --> 00:11:57,698
Let's talk about the receiver, because that is the entry point for

182
00:11:57,864 --> 00:12:01,374
basically getting the metrics into hotel collector.

183
00:12:01,502 --> 00:12:04,814
So there are two receivers that the collector supports.

184
00:12:04,942 --> 00:12:08,482
It's basically a drop in replacement for your Prometheus

185
00:12:08,546 --> 00:12:12,182
to scrape your services like if you're using hotel collector and

186
00:12:12,236 --> 00:12:15,958
Prometheus receiver, then you don't need a Prometheus to scrape the data,

187
00:12:16,044 --> 00:12:19,786
you can just get rid of it because the receiver will do the

188
00:12:19,808 --> 00:12:23,718
same job. It also supports service discovery

189
00:12:23,814 --> 00:12:27,734
the same way that you have it with your scrape config in Prometheus

190
00:12:27,782 --> 00:12:31,514
configuration, just that you have to basically copy the same YAML configuration

191
00:12:31,562 --> 00:12:35,166
and put it under the hotel collector stanza and it

192
00:12:35,188 --> 00:12:39,706
will start working. There is a simple receiver

193
00:12:39,738 --> 00:12:43,200
as well that I didn't mention, but it is used only for

194
00:12:43,570 --> 00:12:46,978
scraping limited services, so that's why it is not to

195
00:12:46,984 --> 00:12:50,974
be used in production workloads. Talking about processors

196
00:12:51,022 --> 00:12:54,622
now this is the most interesting part, because here we can literally

197
00:12:54,686 --> 00:12:58,678
do renaming of metrics, dropping of metrics, we can perform

198
00:12:58,764 --> 00:13:02,754
memory analyzations, we can do redactions, we can do attribute

199
00:13:02,802 --> 00:13:07,254
changes and all of that. So there are two Prometheus that

200
00:13:07,292 --> 00:13:10,910
are applicable to metrics and there are default Prometheus.

201
00:13:11,010 --> 00:13:14,726
The first one is batch, which basically ships these metrics

202
00:13:14,758 --> 00:13:17,942
in batches to a storage backend.

203
00:13:18,086 --> 00:13:21,606
And then there is a memory limiter processor as well,

204
00:13:21,728 --> 00:13:25,594
which does memory analysis so that it can check periodic

205
00:13:25,642 --> 00:13:28,974
checks of your memory usage of the collector. And then

206
00:13:29,012 --> 00:13:32,090
of course will begin refusing the data, forcing gcs

207
00:13:32,170 --> 00:13:35,838
to reduce memory consumption when the limits have been exceeded.

208
00:13:35,934 --> 00:13:39,726
All of this just to make sure that your collector doesn't

209
00:13:39,758 --> 00:13:43,458
go down if you send too many metrics to it.

210
00:13:43,624 --> 00:13:47,106
And of course you can change these memory limits. Some of

211
00:13:47,128 --> 00:13:50,710
the limits are soft limits, some of the limits are hard limits.

212
00:13:51,050 --> 00:13:54,982
But the cool part about processors is you can do lot of other things

213
00:13:55,036 --> 00:13:58,962
as well, not just the memory checks or sending data in batches,

214
00:13:59,106 --> 00:14:02,694
you can generate new metrics. So let's say you have two metrics.

215
00:14:02,742 --> 00:14:05,914
One is the CPU usage of a pod and then

216
00:14:05,952 --> 00:14:09,858
the other is CPU limit of a node. You can literally

217
00:14:09,974 --> 00:14:14,462
create a new metric for Pod CPU utilization by

218
00:14:14,516 --> 00:14:18,474
basically dividing these two values. There are otel mathematical

219
00:14:18,602 --> 00:14:22,522
operations that this processor also supports. You can use

220
00:14:22,596 --> 00:14:26,434
addition subtraction multiplication to create the

221
00:14:26,472 --> 00:14:30,210
new metrics. By just adding these mathematical operations.

222
00:14:31,270 --> 00:14:34,706
There are other processes as well for transformation. This is

223
00:14:34,728 --> 00:14:38,214
where things get really interesting. Your recording rules use

224
00:14:38,252 --> 00:14:42,246
cases probably will not be needed anymore because you can just create

225
00:14:42,348 --> 00:14:46,306
or transform the metrics in the collector itself. You can rename

226
00:14:46,338 --> 00:14:49,466
them, drop them, aggregate them. So even if you

227
00:14:49,488 --> 00:14:53,066
have some high cardinality metrics, you can create new

228
00:14:53,168 --> 00:14:57,014
aggregated metrics out of them using the processor

229
00:14:57,062 --> 00:15:01,226
for transformation, and then send the end result to

230
00:15:01,248 --> 00:15:04,526
the storage back end so that way you can have lot of

231
00:15:04,548 --> 00:15:07,854
heavy lifting to be done at the collector layer itself instead

232
00:15:07,892 --> 00:15:11,662
of doing it on the storage layer or after storage layer like using

233
00:15:11,716 --> 00:15:15,934
recording rules. And that is one of the key advantage

234
00:15:16,062 --> 00:15:19,010
of opentelemetry collector exporters.

235
00:15:19,430 --> 00:15:23,138
After the processors are done and you have

236
00:15:23,304 --> 00:15:26,946
completed that pipeline, the next task is of course to ship

237
00:15:26,978 --> 00:15:29,270
these metrics to the storage backend.

238
00:15:30,650 --> 00:15:34,114
There are three mechanisms basically that the collector provides.

239
00:15:34,162 --> 00:15:37,394
You can scrape metrics from last metrics endpoint

240
00:15:37,442 --> 00:15:40,794
that the collector exposes. So that way it

241
00:15:40,832 --> 00:15:45,210
works very similar to how your Prometheus can scrape metrics from the

242
00:15:45,360 --> 00:15:48,886
application targets. Instead of application targets, it will scrape

243
00:15:48,918 --> 00:15:52,174
it directly from the collector. You can also remote write from

244
00:15:52,212 --> 00:15:55,694
collector as well to long term storages like

245
00:15:55,732 --> 00:15:59,422
limited and the recently added experimental support for

246
00:15:59,476 --> 00:16:02,714
OTLP push. So you can push to Prometheus

247
00:16:02,762 --> 00:16:05,166
as well. So now as of today,

248
00:16:05,348 --> 00:16:08,806
Prometheus can accept data in OTLP

249
00:16:08,858 --> 00:16:12,222
push format as well. Not just the pull format

250
00:16:12,286 --> 00:16:16,054
that we discussed earlier, but things are

251
00:16:16,092 --> 00:16:19,426
not that straightforward when it comes to shipping

252
00:16:19,458 --> 00:16:22,710
metrics to Prometheus. And the reasons are many,

253
00:16:22,780 --> 00:16:26,470
right? The metric types are different, specifications are different,

254
00:16:26,540 --> 00:16:30,678
hotel metrics. Spec is not same as open metrics

255
00:16:30,774 --> 00:16:34,614
format. There is a use case of cumulative versus

256
00:16:34,662 --> 00:16:38,346
delta temporality as well, and both have their own pros and

257
00:16:38,368 --> 00:16:42,786
cons. Naming conventions are different. Hotel has semantic conventions

258
00:16:42,918 --> 00:16:46,334
and it has to take care of making sure that those

259
00:16:46,372 --> 00:16:50,094
conventions are same for not just metrics but for

260
00:16:50,132 --> 00:16:54,130
logs and traces as well, so that you can do the signal correlation

261
00:16:54,790 --> 00:16:58,866
out of order metrics metadata. There are a lot of things that

262
00:16:58,888 --> 00:17:02,526
are different in case of hotel metrics and Prometheus

263
00:17:02,558 --> 00:17:06,886
metrics. And all of them basically play an important

264
00:17:06,988 --> 00:17:10,690
role when you are thinking about shipping metrics to Prometheus.

265
00:17:10,850 --> 00:17:13,750
So let's start with metric types.

266
00:17:14,170 --> 00:17:18,006
There are of course different metric types in hotel and

267
00:17:18,028 --> 00:17:20,742
Prometheus. Some of them are compatible.

268
00:17:20,886 --> 00:17:25,702
So there can be a one to one translation of these metrics

269
00:17:25,846 --> 00:17:29,402
for certain data types. But then there are some other types as well.

270
00:17:29,456 --> 00:17:32,970
Like there is a gauge histogram,

271
00:17:33,130 --> 00:17:36,942
asynchronous context. In case of hotel, Prometheus has standard

272
00:17:36,996 --> 00:17:40,954
types, counter gauges, summary histogram, and then recently

273
00:17:41,002 --> 00:17:44,702
added sparse histogram as well. So the Prometheus

274
00:17:44,766 --> 00:17:48,322
receiver and the exporters, they basically can

275
00:17:48,376 --> 00:17:52,018
convert the metrics that can be converted one on one,

276
00:17:52,104 --> 00:17:55,902
but other metrics which can't be converted. They are basically not supported

277
00:17:56,046 --> 00:17:59,670
when you're shipping those metrics to Prometheus.

278
00:18:00,330 --> 00:18:04,418
Let's talk about the cumulative versus delta temporality.

279
00:18:04,594 --> 00:18:07,858
Temporality cumulative temporality, right? Like what we

280
00:18:07,884 --> 00:18:11,622
discussed earlier was in this case, the overall

281
00:18:11,686 --> 00:18:15,194
value gets submitted to Prometheus. So at

282
00:18:15,232 --> 00:18:18,854
t one the value was one, but at t two the value became

283
00:18:18,902 --> 00:18:21,446
four because it increased by three.

284
00:18:21,568 --> 00:18:25,086
And in case of cumulative temporality, four will

285
00:18:25,108 --> 00:18:29,066
be reported to the storage system. In case of delta temporality,

286
00:18:29,178 --> 00:18:32,334
three will be reported because three was the delta from

287
00:18:32,372 --> 00:18:35,794
the last value. Now, there are pros and cons of both of these

288
00:18:35,832 --> 00:18:39,746
approaches. Cumulative temporality means that you

289
00:18:39,768 --> 00:18:43,374
may have to maintain a state because you always want to increase

290
00:18:43,422 --> 00:18:46,882
the value from last time. So you have to maintain a state on client

291
00:18:46,946 --> 00:18:50,434
side before pushing out these values to the storage

292
00:18:50,482 --> 00:18:55,350
backend. Delta can be stateless because

293
00:18:55,420 --> 00:18:58,946
you're just capturing the difference, right? You don't want to worry about

294
00:18:58,988 --> 00:19:03,018
keeping the state at all. There are certain backends which support

295
00:19:03,104 --> 00:19:06,378
only cumulative, like Prometheus supports cumulative. As of now,

296
00:19:06,464 --> 00:19:10,678
there is no support for delta. The other backends like datadog

297
00:19:10,774 --> 00:19:14,362
recommends using delta because it can improve certain performance

298
00:19:14,426 --> 00:19:18,570
things on their side. So it's a choice, and hotel

299
00:19:18,730 --> 00:19:21,834
leaves it up to you how you want to do it. Cumulative is the default,

300
00:19:21,882 --> 00:19:25,586
I think, but you can configure it in the applications themselves

301
00:19:25,768 --> 00:19:29,186
and you can restart the applications if

302
00:19:29,208 --> 00:19:33,106
you change the configuration. Of course you have to restart the application so that

303
00:19:33,128 --> 00:19:37,270
it takes the effect. In case of delta temporality,

304
00:19:37,690 --> 00:19:41,702
if somehow you drop the samples, that means there is a data loss because

305
00:19:41,756 --> 00:19:44,790
you cannot really recover the values.

306
00:19:45,290 --> 00:19:48,982
If certain labels are dropped, certain samples are dropped.

307
00:19:49,126 --> 00:19:53,242
But in case of cumulative, that's not a problem because over a long

308
00:19:53,296 --> 00:19:57,606
time rate and other functions can absorb the lossy

309
00:19:57,638 --> 00:20:01,022
values, and then you don't have to worry about

310
00:20:01,156 --> 00:20:03,710
losing out on data. In case of cumulative,

311
00:20:04,690 --> 00:20:08,746
the naming conventions are also different. In case of hotel and Prometheus,

312
00:20:08,858 --> 00:20:13,654
Otel uses the dot notation, so HTTP request

313
00:20:13,802 --> 00:20:18,270
duration is something that it uses. Additionally, it enforces

314
00:20:18,350 --> 00:20:21,570
that every metric should have a unit as well.

315
00:20:21,720 --> 00:20:26,014
In case of Prometheus, it does not use dots. It uses underscores

316
00:20:26,142 --> 00:20:29,654
for metric and label names, and sometimes the unit is also

317
00:20:29,692 --> 00:20:32,726
part of the metric name itself, so that is one of the key

318
00:20:32,748 --> 00:20:36,850
difference. The receivers and exporters have the ability

319
00:20:36,930 --> 00:20:41,670
to normalize the hotel metrics into Prometheus

320
00:20:41,830 --> 00:20:45,686
metrics based on the naming conventions. It can also do it vice

321
00:20:45,718 --> 00:20:48,886
versa as well. When scraping data from Prometheus,

322
00:20:48,918 --> 00:20:52,762
it can convert prometheus metrics to hotel naming conventions

323
00:20:52,826 --> 00:20:56,494
as well. Of course, it cannot scale up the values or do

324
00:20:56,532 --> 00:20:59,786
conversion between units like it cannot convert milliseconds

325
00:20:59,818 --> 00:21:03,186
to seconds or kilograms to grams and so on. That is

326
00:21:03,208 --> 00:21:06,850
still a processor task. It basically

327
00:21:06,920 --> 00:21:10,020
does only the naming convention changes.

328
00:21:11,110 --> 00:21:14,546
By the way, the naming convention changes also mean that if

329
00:21:14,568 --> 00:21:17,826
you have standard dashboards which are using a

330
00:21:17,848 --> 00:21:21,702
certain style of naming, they may not work if you are doing these

331
00:21:21,756 --> 00:21:25,014
transformations or if you start using hotel collector to

332
00:21:25,052 --> 00:21:28,394
ship those metrics. So that is one of the trade off that you have to

333
00:21:28,432 --> 00:21:31,766
consider while choosing otel metrics

334
00:21:31,798 --> 00:21:35,066
versus Prometheus metrics. So what we discussed so

335
00:21:35,088 --> 00:21:38,346
far, open telemetry metrics as of today,

336
00:21:38,528 --> 00:21:41,822
there is a way to consume them and then ship them to

337
00:21:41,876 --> 00:21:45,690
systems like Prometheus. There are some gotchas and conventions

338
00:21:45,770 --> 00:21:49,870
rules that you have to follow. We discussed the collector

339
00:21:50,530 --> 00:21:54,222
in a very detailed way. We discussed all three aspects of the collector

340
00:21:54,366 --> 00:21:58,030
including the receivers, processors and exporters.

341
00:21:58,190 --> 00:22:01,314
We also discussed the recent support that has been

342
00:22:01,352 --> 00:22:04,674
added in Prometheus which can allow you to push metrics as

343
00:22:04,712 --> 00:22:08,546
well, Otel metrics as well in an experimental

344
00:22:08,578 --> 00:22:12,726
way apart from just the pull mechanism that Prometheus is very

345
00:22:12,828 --> 00:22:16,562
popularly known for. But Prometheus

346
00:22:16,626 --> 00:22:20,774
is also working on adding lot of other capabilities for hotel

347
00:22:20,822 --> 00:22:24,746
support and making it first class for hotel metrics so

348
00:22:24,768 --> 00:22:28,390
that you can use Prometheus as a backend,

349
00:22:28,550 --> 00:22:31,550
natural choice of backend for your Otel metrics.

350
00:22:32,050 --> 00:22:35,630
There are issues created for this in Prometheus issue

351
00:22:35,700 --> 00:22:39,806
tracker. Some of the interesting issues are around out

352
00:22:39,828 --> 00:22:43,822
of order support because Prometheus earlier

353
00:22:43,886 --> 00:22:47,966
did not have a support for getting out of order metrics. But Otel

354
00:22:48,078 --> 00:22:51,170
pushes batches of metrics so there can be a case where

355
00:22:51,320 --> 00:22:55,246
but of order metrics are reported and Prometheus

356
00:22:55,358 --> 00:22:58,626
is now working on supporting that. Then the UTF

357
00:22:58,658 --> 00:23:02,630
eight support for label and metric names. This is also something

358
00:23:02,700 --> 00:23:06,178
that might be added in Prometheus and that will solve

359
00:23:06,194 --> 00:23:09,862
the case of normalization that we discussed earlier where dots

360
00:23:09,926 --> 00:23:13,718
were converted into underscores and vice versa.

361
00:23:13,894 --> 00:23:17,078
The delta temporality support is also being discussed.

362
00:23:17,254 --> 00:23:20,274
While the delta to cumulative

363
00:23:20,422 --> 00:23:24,846
conversion is not very easy. There are some discussions around

364
00:23:24,948 --> 00:23:28,638
supporting that in native Prometheus as well.

365
00:23:28,804 --> 00:23:32,574
And then of course supporting the resource attributes metadata so that

366
00:23:32,612 --> 00:23:37,234
those can be stored in Prometheus correctly and then utilize later.

367
00:23:37,432 --> 00:23:41,106
There can be some performance improvements in all of this as well. So all

368
00:23:41,128 --> 00:23:44,610
of these issues are tracked on Prometheus issue tracker and

369
00:23:44,680 --> 00:23:48,406
you can basically check their progress. Some of these issues are

370
00:23:48,428 --> 00:23:52,834
also slated to be completed by Prometheus

371
00:23:52,882 --> 00:23:56,754
3.0 roadmap as per the last dev summit discussion

372
00:23:56,802 --> 00:24:00,346
that I saw on YouTube. But that is something that

373
00:24:00,368 --> 00:24:04,346
is very interesting update and I think that will be

374
00:24:04,368 --> 00:24:07,914
very great addition to the community if all of these

375
00:24:07,952 --> 00:24:11,514
things are supported natively in Prometheus. So,

376
00:24:11,552 --> 00:24:15,438
to recap, what we discussed was we started with why you should care about

377
00:24:15,524 --> 00:24:19,834
opentelemetry metrics. We touched upon how they are different from Prometheus.

378
00:24:19,882 --> 00:24:24,230
What are the key differences between Prometheus and opentelemetry,

379
00:24:24,330 --> 00:24:28,530
not just in terms of the specification, but also in terms of architecture?

380
00:24:28,870 --> 00:24:32,766
And we also touched upon what are some of the things that opentelemetry

381
00:24:32,798 --> 00:24:36,046
can offload from Prometheus. If we can ship

382
00:24:36,078 --> 00:24:39,738
metrics from hotel to Prometheus directly, we discussed

383
00:24:39,774 --> 00:24:44,386
about the current state of the art of opentelemetry and Prometheus interoperability,

384
00:24:44,578 --> 00:24:48,754
and then also discuss what is upcoming. What are the plans of Prometheus

385
00:24:48,802 --> 00:24:52,514
project to make the experience of using opentelemetry

386
00:24:52,562 --> 00:24:55,970
metrics in a more seamless and native

387
00:24:56,050 --> 00:24:59,934
way. So that's all mostly what I have for today,

388
00:25:00,092 --> 00:25:03,646
discussing about the open telemetry metrics and

389
00:25:03,748 --> 00:25:07,466
how you can get started with it. You can find me on Twitter,

390
00:25:07,578 --> 00:25:11,086
you can follow me or last nine, and then of course check

391
00:25:11,108 --> 00:25:14,794
out levitate, which is our hotel and Prometheus compatible

392
00:25:14,842 --> 00:25:18,058
TSDB to manage high cardinality workloads.

393
00:25:18,154 --> 00:25:18,540
Thank you.

