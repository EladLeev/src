1
00:00:41,010 --> 00:00:44,738
Hello, my name is Erez Berkner and I'm CEO and co founder of Lumigo.

2
00:00:44,754 --> 00:00:48,610
And welcome to this session about monitoring and debugging Kubernetes

3
00:00:48,690 --> 00:00:51,854
versus serverless. So just as a context, we're going to talk about

4
00:00:51,892 --> 00:00:55,514
the evolution of the cloud. We're going to talk about monitoring Kubernetes

5
00:00:55,562 --> 00:00:59,146
and how serverless changes everything. And then we're going to talk about monitoring

6
00:00:59,178 --> 00:01:02,606
servers. And hopefully we'll do a quick demo at the end. So when we

7
00:01:02,628 --> 00:01:06,420
talk about the evolution of transportation as an example,

8
00:01:06,870 --> 00:01:10,466
we can see different things that evolve over time. And the reason I'm talking

9
00:01:10,488 --> 00:01:14,926
about transportation because I think it's very relevant, very similar metaphor

10
00:01:14,958 --> 00:01:18,430
for serverless. So when we think about transportation,

11
00:01:18,510 --> 00:01:21,666
we based to ride by a car we owned, we bought it, we fueled

12
00:01:21,698 --> 00:01:24,934
it, we navigate and we got to our destination or we could

13
00:01:24,972 --> 00:01:28,006
went a car, then we don't own it. But we still need to take care

14
00:01:28,028 --> 00:01:31,706
of all the maintenance. We can navigate through trains or

15
00:01:31,728 --> 00:01:35,594
based and that means just figuring out how to get there or

16
00:01:35,632 --> 00:01:39,306
we can get an Uber and that's focusing on getting there. And this

17
00:01:39,328 --> 00:01:42,958
is very much what's happening in the cloud. So we used to work with

18
00:01:42,964 --> 00:01:46,106
the physical servers where we own the hardware,

19
00:01:46,218 --> 00:01:49,614
deploy the operating system and in charge of scale and

20
00:01:49,652 --> 00:01:53,146
our code. Virtual machines really took the hardware away. So we were renting

21
00:01:53,178 --> 00:01:56,686
servers, containers, brought this to an upper

22
00:01:56,718 --> 00:01:59,422
level of virtualization and abstraction.

23
00:01:59,566 --> 00:02:03,294
So you care mostly about runtime, the scale and the code. And serverless

24
00:02:03,342 --> 00:02:06,834
is really about upload your code to the cloud and we got you covered.

25
00:02:06,882 --> 00:02:10,886
So this is where these are similar trends at the

26
00:02:10,908 --> 00:02:14,854
same time when we talk about continue managing them is

27
00:02:14,892 --> 00:02:18,646
very popular to do with Kubernetes. And I want

28
00:02:18,668 --> 00:02:22,326
to mention five layers of Kubernetes that we need to monitor.

29
00:02:22,518 --> 00:02:25,914
We have the infrastructure, the actual hardware where we have a

30
00:02:25,952 --> 00:02:29,926
vital sign that we need to monitor. We have the cluster

31
00:02:30,038 --> 00:02:33,830
of the Kubernetes itself, we have the pod,

32
00:02:33,910 --> 00:02:37,690
we have the overall readiness of availability of the pods.

33
00:02:37,770 --> 00:02:40,970
And we have the application itself where we see application logs.

34
00:02:41,050 --> 00:02:44,478
And you need to remember that whenever you go and work with Kubernetes, you need

35
00:02:44,484 --> 00:02:48,050
to make sure that you cover all these five layers in order to really understand

36
00:02:48,120 --> 00:02:51,954
what's going on across hardware, infrastructure and

37
00:02:51,992 --> 00:02:55,380
application. I want to share with you a couple of tools that are very

38
00:02:55,770 --> 00:02:59,702
work really good with Kubernetes. I'm sure some of you heard of

39
00:02:59,756 --> 00:03:02,838
Prometheus, a great, great open source tool

40
00:03:02,924 --> 00:03:07,014
that basically allow you to connect or basically

41
00:03:07,132 --> 00:03:11,258
you have able to pull metrics from the different nodes and

42
00:03:11,344 --> 00:03:15,146
aggregate those. You can save it to a storage, you can analyze them

43
00:03:15,168 --> 00:03:18,694
and push alerts through Prometheus alert

44
00:03:18,742 --> 00:03:22,910
manager to slack to predator duty. And you can use Grafana which is also

45
00:03:22,980 --> 00:03:27,034
very popular with Prometheus in order to visualize that and have dashboards

46
00:03:27,082 --> 00:03:31,610
that show you the health of your system. This is an example of Grafana

47
00:03:31,690 --> 00:03:35,566
monitoring a Kubernetes cluster. And note

48
00:03:35,598 --> 00:03:39,122
that there are different layers over here of what we talked about

49
00:03:39,176 --> 00:03:42,846
before, like the pod usage, like the cluster's

50
00:03:42,878 --> 00:03:45,986
availability, the cpu, the hardware, so on and so

51
00:03:46,008 --> 00:03:49,478
forth in terms of getting the application logs. So you can use

52
00:03:49,564 --> 00:03:53,510
a log stash in order to log stash like basically elk in order

53
00:03:53,580 --> 00:03:57,350
to get aggregation of all the logs and make them available and

54
00:03:57,420 --> 00:04:01,226
searchable. Another great best practice to have monitoring of the

55
00:04:01,248 --> 00:04:05,434
application logs of kubernetes. This is a really cool open

56
00:04:05,472 --> 00:04:08,698
source tool that I want to share. It's not that

57
00:04:08,864 --> 00:04:12,570
known in the community, but I found it very helpful,

58
00:04:12,730 --> 00:04:16,670
especially when we need to understand how traffic is flowing across

59
00:04:16,740 --> 00:04:20,734
different containers. And this

60
00:04:20,772 --> 00:04:24,762
tool called viseral and relatively easy to connect to this

61
00:04:24,836 --> 00:04:28,434
and again, it's open source. So go check it out. It might

62
00:04:28,472 --> 00:04:32,546
be very useful for your scenario. I won't finish the Kubernetes part without

63
00:04:32,648 --> 00:04:36,274
mentioning service mesh, which basically allow you

64
00:04:36,312 --> 00:04:40,530
to, or I would say when you're monitoring service mesh based architecture,

65
00:04:40,610 --> 00:04:44,774
your life is much easier because you have a centralized point where

66
00:04:44,812 --> 00:04:48,626
you can deliver and get information and metrics and you don't need to

67
00:04:48,668 --> 00:04:51,754
go and add a layer that do that. It's actually

68
00:04:51,792 --> 00:04:54,922
integrated in your architecture. Istio has some great,

69
00:04:54,976 --> 00:04:58,762
great tools for that. So that's another point to remember

70
00:04:58,896 --> 00:05:02,314
when you're architecturing your environment and you

71
00:05:02,352 --> 00:05:05,690
care about monitoring. If you have a service mesh

72
00:05:05,770 --> 00:05:09,658
baked in or planning to use service mesh, use it also for your monitoring.

73
00:05:09,754 --> 00:05:13,362
And let's talk a bit about serverless. So when we talk about

74
00:05:13,416 --> 00:05:17,054
serverless, I just want to frame it. It's not just like AWS

75
00:05:17,102 --> 00:05:20,254
lambdas, it's a variety of services from lambdas

76
00:05:20,302 --> 00:05:24,366
to containers to manage containers. Fargate, DynamosDB's API

77
00:05:24,398 --> 00:05:27,782
gateways, stripe, Twilio, all of these are

78
00:05:27,836 --> 00:05:31,286
ways to consume functionality without

79
00:05:31,388 --> 00:05:35,426
actually maintaining a server. And this is what I define as serverless

80
00:05:35,538 --> 00:05:39,434
environment. And when we talk about this environment can understand that serverless is

81
00:05:39,472 --> 00:05:43,514
different, it's ephemeral, meaning there is no server that is always

82
00:05:43,552 --> 00:05:47,322
up and running. There are hundreds of components that work

83
00:05:47,376 --> 00:05:51,146
together, not just like three tier application that we used to have and

84
00:05:51,168 --> 00:05:54,826
there are actually no servers, so you cannot deploy agents anywhere.

85
00:05:54,858 --> 00:05:58,170
You need a new methods and in order to monitor

86
00:05:58,330 --> 00:06:01,726
serverless the right way, you need to make sure that you have the

87
00:06:01,748 --> 00:06:05,202
right tools. So that's a quick comparison of server based

88
00:06:05,256 --> 00:06:11,314
versus serverless. So in a server base or continue for

89
00:06:11,352 --> 00:06:15,738
all of them, you have many many small parts. You need distributed tracing

90
00:06:15,854 --> 00:06:19,814
in a serverless. In a continue environment you can use good old

91
00:06:19,932 --> 00:06:23,394
agent based and there are many good solution open source

92
00:06:23,442 --> 00:06:27,106
or proprietary solutions that solve that. In serverless

93
00:06:27,298 --> 00:06:30,906
alerts doesn't work anymore, so you need to use

94
00:06:31,008 --> 00:06:34,954
APIs or libraries in order to integrate and

95
00:06:35,072 --> 00:06:38,714
infer what's going on within a service. When we talk about

96
00:06:38,752 --> 00:06:41,658
costs, this is containers is per resource,

97
00:06:41,754 --> 00:06:45,726
serverless is driven per request, which really makes a difference when you

98
00:06:45,748 --> 00:06:49,006
think about what to monitor on containers. Kubernetes you

99
00:06:49,028 --> 00:06:52,794
need to monitor hardware, operating system, serverless based

100
00:06:52,852 --> 00:06:56,530
applications. On serverless it's only the application that's your responsibility.

101
00:06:56,870 --> 00:07:00,162
Service discovery again, you have the different

102
00:07:00,296 --> 00:07:03,698
tools in continue the legacy tools also

103
00:07:03,784 --> 00:07:07,318
of course they are accepted using service mesh serverless you

104
00:07:07,324 --> 00:07:10,470
can do that based on APIs from a main point,

105
00:07:10,620 --> 00:07:13,910
like AWS for example. I think the most important

106
00:07:13,980 --> 00:07:17,720
thing to remember is that you still own the

107
00:07:18,090 --> 00:07:21,154
monitoring part. Nobody will do that for you in containers

108
00:07:21,202 --> 00:07:24,966
or in serverless. And that's an important point to remember when you're offloading

109
00:07:24,998 --> 00:07:28,154
things to the cloud provider. So what do we really need when we talk

110
00:07:28,192 --> 00:07:31,430
about serverless monitoring or modern cloud monitoring?

111
00:07:31,510 --> 00:07:34,894
First of all, we need to be able to identify and fix issues in

112
00:07:34,932 --> 00:07:38,510
minutes. And for this, because we have so many different services, we need somebody

113
00:07:38,580 --> 00:07:42,174
to connect the dots and make data bugging data available for

114
00:07:42,212 --> 00:07:45,310
us on demand. I'm going to show that in a second. In a quick demo

115
00:07:45,380 --> 00:07:48,590
we have hundreds of services we need to do distributed tracing,

116
00:07:48,670 --> 00:07:52,322
but it has to be automatic. I cannot chase after every new service

117
00:07:52,376 --> 00:07:55,762
that is popping up. And in the third point, we need

118
00:07:55,816 --> 00:07:59,302
to make sure that we're able to identify bottlenecks because there are so many

119
00:07:59,356 --> 00:08:02,742
potential bottlenecks in those environments. And as I mentioned,

120
00:08:02,796 --> 00:08:06,962
all of this need to be agentless and based on APIs and code libraries.

121
00:08:07,106 --> 00:08:10,294
So this is what we do at Lumigo.

122
00:08:10,342 --> 00:08:13,978
We basically take metrics, tracing and logs, and we

123
00:08:14,064 --> 00:08:17,606
connect the dots in order to make sure that you're

124
00:08:17,638 --> 00:08:21,774
able to understand when things are going wrong and be able to fix it.

125
00:08:21,812 --> 00:08:25,006
And I want to show you this in a very quick demo.

126
00:08:25,188 --> 00:08:28,138
So I have over here our demo environment,

127
00:08:28,234 --> 00:08:32,746
sorry, 1 second. So this is a Lumigo

128
00:08:32,778 --> 00:08:36,314
environment. Basically, this is a demo environment that is connected

129
00:08:36,362 --> 00:08:39,438
to AWS Wildride, a serverless environment.

130
00:08:39,534 --> 00:08:43,714
And I want to take you to one scenario that is very popular with

131
00:08:43,752 --> 00:08:47,486
our customers. So just refreshing the dashboard to make sure we will

132
00:08:47,608 --> 00:08:50,886
observe the last seven days instead of the

133
00:08:50,908 --> 00:08:54,674
last hour. That's basically taking us from like live monitoring

134
00:08:54,722 --> 00:08:58,614
when I want to have this as a dashboard that is kind of always open

135
00:08:58,732 --> 00:09:02,698
to something that is more of can investigation. What I

136
00:09:02,704 --> 00:09:06,106
want to show you over here is this is an example of

137
00:09:06,208 --> 00:09:10,086
what we have in terms of environment like invocation.

138
00:09:10,198 --> 00:09:13,214
What is the number of failures? What are the functions that fail the most?

139
00:09:13,252 --> 00:09:16,366
Where do I have latencies? Do I have call start? What are the

140
00:09:16,388 --> 00:09:20,090
main issues with call start? Same goes to cost analysis,

141
00:09:20,250 --> 00:09:24,126
slow APIs, timeouts, dashboard, showing you the out

142
00:09:24,148 --> 00:09:27,582
of the box, the main thing that you should care about in a serverless environment.

143
00:09:27,646 --> 00:09:31,186
And let's suppose we got an alert to petroduty from Lumigo about

144
00:09:31,288 --> 00:09:35,026
this failure. So if we want to understand what's going on over here

145
00:09:35,048 --> 00:09:38,806
or over here, we click on that specific service. This is a

146
00:09:38,828 --> 00:09:42,742
lambda, in order to start investigating what actually happens and

147
00:09:42,796 --> 00:09:46,646
what are the cases where this failed. So we can see

148
00:09:46,668 --> 00:09:49,866
that this lambda ran 7000 time in the

149
00:09:49,888 --> 00:09:53,718
last seven days, three almost 50% failures.

150
00:09:53,814 --> 00:09:57,430
And over here we have the actual invocations and the results

151
00:09:57,510 --> 00:10:00,826
we want to drill down into a specific failure to understand what happened.

152
00:10:00,928 --> 00:10:04,954
And this is where we move from just monitoring to debugging. And Lumigo

153
00:10:05,002 --> 00:10:08,254
builds the end to end story of this

154
00:10:08,292 --> 00:10:11,706
request, of the request that failed. A specific invocation failed

155
00:10:11,738 --> 00:10:14,750
within that request. And now Lumigo will show me

156
00:10:14,820 --> 00:10:18,754
the story of that request across all the different services.

157
00:10:18,872 --> 00:10:22,962
What we see over here is that this is the actual failure, the reason

158
00:10:23,016 --> 00:10:26,770
that we started, we got here because this lambda failed. At the same time,

159
00:10:26,840 --> 00:10:30,502
I can understand what is the customer facing API and decide whether

160
00:10:30,556 --> 00:10:33,798
this is critical to fix now or not. When I want to understand

161
00:10:33,884 --> 00:10:37,266
what happened, I can click on a specific service. And then Lumigo

162
00:10:37,298 --> 00:10:40,262
generates a lot of debugging information, post mortem,

163
00:10:40,326 --> 00:10:44,106
things like what was the stacked race, what was the parameters of the

164
00:10:44,128 --> 00:10:47,834
stacked race when it failed, what was the event that triggers this

165
00:10:47,872 --> 00:10:51,386
lambda environment? Variables, the logs, a lot of. I call

166
00:10:51,408 --> 00:10:55,006
it debugging heaven, because it's always there with all the information that you

167
00:10:55,028 --> 00:10:58,174
need in order to understand what happened and solve the

168
00:10:58,212 --> 00:11:01,950
issue. And you don't need to go across thousands of logs and try to find

169
00:11:02,020 --> 00:11:05,646
what you want. This is without any code changes, without the need to

170
00:11:05,668 --> 00:11:09,474
issue logs. Logs. You basically can go into every service and

171
00:11:09,512 --> 00:11:12,882
see what are the inputs and outputs of that service. So this is an event

172
00:11:12,936 --> 00:11:16,430
bridge. I can see the message that went to Eventbridge,

173
00:11:16,510 --> 00:11:19,766
I can look at Dynamodb and see the actual

174
00:11:19,868 --> 00:11:23,586
data. This is a query to Dynamodb. And this was the response,

175
00:11:23,618 --> 00:11:27,526
no response. And this is also true to external services like Twilio. So I

176
00:11:27,548 --> 00:11:31,590
can click on Twilio and see the request for Twilio and see the response

177
00:11:31,670 --> 00:11:35,258
coming from Twilio, sent a successful sms, this number,

178
00:11:35,344 --> 00:11:38,698
so forth, so on and so forth. And I can also see the specific

179
00:11:38,864 --> 00:11:42,646
logs of that specific request. Maybe I have million requests,

180
00:11:42,678 --> 00:11:45,662
but this one request is the one I want to see. It's like a story

181
00:11:45,716 --> 00:11:49,358
with 62 logs over here, starting from the very basic first

182
00:11:49,444 --> 00:11:53,102
authentication that was done over here, all the way to going

183
00:11:53,156 --> 00:11:56,594
across the different services. And I can look at this

184
00:11:56,632 --> 00:12:00,242
and read what's actually happening like a story across

185
00:12:00,296 --> 00:12:04,126
all the different services. Great. So going back to summarize

186
00:12:04,238 --> 00:12:08,166
where we are, a couple of takeaways. We talked about the five layers of

187
00:12:08,188 --> 00:12:10,646
kubernetes. You need to monitor all of them.

188
00:12:10,748 --> 00:12:14,322
Microservices requires distributed tracing, whether it's containers,

189
00:12:14,386 --> 00:12:18,386
whether it's cloud native, whether it's serverless. Emerging monitoring.

190
00:12:18,498 --> 00:12:22,058
There's an emerging monitoring challenge around tracing of

191
00:12:22,144 --> 00:12:25,590
managed services. We saw the DynamodB, the event bridge.

192
00:12:25,670 --> 00:12:28,346
How do I know, how do I trace across them? How do I know the

193
00:12:28,368 --> 00:12:31,626
messages that go through them? When I need to investigate. This is growing and

194
00:12:31,648 --> 00:12:35,354
you need to make sure this is covered in your environment. Use existing frameworks,

195
00:12:35,402 --> 00:12:39,146
open source. We talk about a couple of tools that are available, commercial or non

196
00:12:39,178 --> 00:12:43,834
commercial, but make sure you bake it in there. And serverless requires

197
00:12:43,882 --> 00:12:48,014
you to also be able to manage to understand managed distributed

198
00:12:48,062 --> 00:12:51,214
tracing. So serverless is not just lambdas, it's DynamoDB,

199
00:12:51,262 --> 00:12:54,802
it's API gateways, it's eventbridge, it's stripe, it's Twilio. Make sure

200
00:12:54,856 --> 00:12:58,774
that you are able to distribute to trace across those services.

201
00:12:58,972 --> 00:13:02,582
I want to thank you and if

202
00:13:02,636 --> 00:13:05,766
you have any questions, please feel free to reach out.

203
00:13:05,868 --> 00:13:09,446
This is all my details, my mail and my twitter. If you want

204
00:13:09,468 --> 00:13:13,094
to try out Lumigo, we have a free trial and a free

205
00:13:13,132 --> 00:13:16,518
tier. You're welcome to just go to Lumigo IO, click start a

206
00:13:16,524 --> 00:13:20,254
free trial and it's five minutes to connect the system. No code

207
00:13:20,292 --> 00:13:23,694
changes and everything is automated so you can have a full view of

208
00:13:23,732 --> 00:13:26,878
your environment with this. I thank you very much.

209
00:13:26,964 --> 00:13:28,542
And wish you a great week.

