1
00:00:34,530 --> 00:00:38,294
Hi, thank you so much for joining me here today. It'd be great

2
00:00:38,332 --> 00:00:42,002
to hear where you're all from, so please do leave a comment in the chat

3
00:00:42,066 --> 00:00:45,474
and introduce yourself where you're coming from. Likewise,

4
00:00:45,522 --> 00:00:48,966
please use the q and a in the comments if you've got any questions

5
00:00:49,068 --> 00:00:53,230
throughout this webinar, and I'll do my best to get to them at the end.

6
00:00:53,380 --> 00:00:56,894
I'm also joined by some of my friends at Learn kits, so shout out

7
00:00:56,932 --> 00:01:00,206
to Salman and some of my friends at Linode who will also be kind

8
00:01:00,228 --> 00:01:03,918
of assisting in the chat, who may get to your questions

9
00:01:04,004 --> 00:01:08,550
before I do. So to kick things off, my name's Chris Nesbittsmith.

10
00:01:08,650 --> 00:01:11,874
I'm based in London and currently an instructor for Learn

11
00:01:11,912 --> 00:01:15,746
KX and a consultant to various bits of UK government and

12
00:01:15,768 --> 00:01:19,382
a tinkerer of open source stuff. I've been using and

13
00:01:19,436 --> 00:01:22,726
abusing Kubernetes in production since it was 0.4,

14
00:01:22,828 --> 00:01:26,646
so believe me when I say it's been a journey. I've certainly got the

15
00:01:26,668 --> 00:01:29,994
scars and the war wounds to show for it. So you

16
00:01:30,032 --> 00:01:34,118
believe the hype? The Kubernetes lets you scale infinitely,

17
00:01:34,294 --> 00:01:37,914
auto heal your cluster, and so on. So your cluster is self

18
00:01:37,952 --> 00:01:41,322
monitoring. It's scaling up instances of your cloud native,

19
00:01:41,386 --> 00:01:44,240
stateless applications on demand when you need more,

20
00:01:45,730 --> 00:01:49,722
but all of a sudden your nodes are full, you can scale

21
00:01:49,786 --> 00:01:53,626
no more. Well, enter the cluster autoscaler

22
00:01:53,658 --> 00:01:57,060
and of course a splash of yaml in order to save the day,

23
00:01:57,670 --> 00:02:01,438
and that can integrate with your cloud vendor to provision

24
00:02:01,534 --> 00:02:04,834
more necessary nodes. And the good news is that

25
00:02:04,872 --> 00:02:07,030
the autoscaler is configurable.

26
00:02:07,770 --> 00:02:11,714
Though sadly, as we'll see, it's not quite as configurable

27
00:02:11,762 --> 00:02:15,142
as you might hope or expect. There are

28
00:02:15,196 --> 00:02:18,626
alternatives, but the official cluster autoscaler only scales

29
00:02:18,658 --> 00:02:22,294
up when there's pending nodes in order to satisfy

30
00:02:22,342 --> 00:02:25,446
the demand. Which is probably a good idea, since there's

31
00:02:25,478 --> 00:02:28,826
little point adding more nodes unless you have the workload that

32
00:02:28,848 --> 00:02:32,186
needs them. Okay, so first let's

33
00:02:32,218 --> 00:02:36,062
refresh ourselves on how the Kubernetes kind of scheduler works.

34
00:02:36,196 --> 00:02:39,866
So if I create a deployment with two replicas,

35
00:02:40,058 --> 00:02:44,106
I do this by submitting some Yaml to the API server, which then writes

36
00:02:44,138 --> 00:02:48,114
it to etcd. The controller is watching for this type of

37
00:02:48,152 --> 00:02:51,278
event, recognizes it needs to go and create some pods,

38
00:02:51,374 --> 00:02:53,810
which it does, and then these are now pending.

39
00:02:54,230 --> 00:02:58,018
The scheduler is a component that is looking for pending pods,

40
00:02:58,114 --> 00:03:02,230
sees these, and then schedules or assigns them to a node.

41
00:03:02,650 --> 00:03:06,342
The scheduling, however, is broken down into a few steps from

42
00:03:06,396 --> 00:03:10,134
the initial queue through filtering viable nodes

43
00:03:10,182 --> 00:03:13,770
to then scoring them before actually finally creating the binding.

44
00:03:14,510 --> 00:03:18,006
But how does the scheduler know how much memory and cpu

45
00:03:18,038 --> 00:03:21,354
a pod uses? Well, it doesn't strictly

46
00:03:21,402 --> 00:03:25,454
speaking. So you need to spoon feed this with requests and

47
00:03:25,492 --> 00:03:29,034
limits. So if you don't specify requests and limits,

48
00:03:29,082 --> 00:03:32,286
Kubernetes will play completely blind. Your cluster will

49
00:03:32,308 --> 00:03:35,690
inevitably become overloaded, nodes will become oversubscribed,

50
00:03:35,770 --> 00:03:39,154
and you'll be consultant fighting by's. So if your only

51
00:03:39,192 --> 00:03:42,802
takeaway from any of this is that all your containers should definitely have

52
00:03:42,856 --> 00:03:46,346
requests and limits defined, then we've at least achieved something useful

53
00:03:46,398 --> 00:03:50,086
here. So requests are the initial ask and limits are the point where

54
00:03:50,108 --> 00:03:54,310
the container will be throttled at cpu or killed if it exceeds the memory.

55
00:03:55,770 --> 00:03:59,398
Okay, so applications come in all sorts of shapes and

56
00:03:59,404 --> 00:04:02,914
sizes. So you may have some applications that are more cpu

57
00:04:02,962 --> 00:04:06,166
intensive and don't require much memory, while on the other hand you

58
00:04:06,188 --> 00:04:09,930
may have others that have a greater memory than a cpu footprint

59
00:04:09,970 --> 00:04:13,162
it. So those applications have to be deployed

60
00:04:13,226 --> 00:04:16,846
inside computing units which have

61
00:04:16,948 --> 00:04:20,750
again their own cpu and memory kind of characteristics.

62
00:04:21,250 --> 00:04:24,074
So for every application deployed in a cluster,

63
00:04:24,122 --> 00:04:28,126
Kubernetes makes a note of the memory and cpu requirements.

64
00:04:28,318 --> 00:04:31,954
It then decides where to place the application in the cluster. In this

65
00:04:31,992 --> 00:04:35,586
case, it's on the far left nodes. If another application of

66
00:04:35,608 --> 00:04:38,806
the same size is deployed well, Kubernetes goes through that same process and

67
00:04:38,828 --> 00:04:42,614
finds the best node to run the app. In this case, it picks the right

68
00:04:42,652 --> 00:04:46,566
hand nodes. As more applications are submitted to

69
00:04:46,588 --> 00:04:49,946
the cluster, Kubernetes keeps making notes of

70
00:04:49,968 --> 00:04:53,222
the cpu and memory requirements and allocating

71
00:04:53,286 --> 00:04:57,114
these applications in the cluster. If you play this game long

72
00:04:57,152 --> 00:05:01,046
enough, you might notice that Kubernetes appears at least to be a reasonably

73
00:05:01,078 --> 00:05:05,086
skilled Tetris player. So your service of the board, your apps of the blocks and

74
00:05:05,108 --> 00:05:09,200
Kubernetes is trying to fit as many blocks as efficiently as possible.

75
00:05:10,610 --> 00:05:14,522
But what about the size of the worker nodes? Well, what kind of instance

76
00:05:14,586 --> 00:05:17,794
types can you use to build your cluster? Well, nowadays the cloud

77
00:05:17,832 --> 00:05:21,154
vendors make almost every instance type available to be

78
00:05:21,192 --> 00:05:24,450
part of the cluster, so you've got pretty much free choice.

79
00:05:25,030 --> 00:05:28,710
There is a catch though, so you'd be forgiven for thinking

80
00:05:28,780 --> 00:05:32,246
that if you get an eight gig ram and two cpu node from

81
00:05:32,268 --> 00:05:35,570
your cloud vendor, you could deploy four pods

82
00:05:35,650 --> 00:05:39,930
that are one and a half gig ram and need a quarter of a cpu.

83
00:05:40,270 --> 00:05:43,594
However, it's not quite so one of those

84
00:05:43,632 --> 00:05:47,130
pods would remain pending, which if configured, will of course

85
00:05:47,280 --> 00:05:50,474
cause the cluster autoscaling to go and create a new

86
00:05:50,512 --> 00:05:53,390
node and then eventually your workload becomes scheduled.

87
00:05:54,130 --> 00:05:58,186
But why is this so? When you provision a managed

88
00:05:58,218 --> 00:06:01,598
instance, you might think that the memory and cpu available

89
00:06:01,684 --> 00:06:05,300
can be used for running pods. And you are right.

90
00:06:06,070 --> 00:06:09,374
However, some memory and cpu should be reserved

91
00:06:09,422 --> 00:06:13,614
for the operating system and you should also reserve memory

92
00:06:13,662 --> 00:06:17,086
and cpu for the Kubelet. But surely

93
00:06:17,118 --> 00:06:20,710
the rest is available to my pods, right? Well, not quite yet.

94
00:06:20,780 --> 00:06:24,018
So you also need to reserve some memory for the eviction threshold.

95
00:06:24,114 --> 00:06:27,862
So if the Kubernetes notices that the memory usage is going over

96
00:06:27,916 --> 00:06:30,650
that point, it will start evicting nodes.

97
00:06:31,870 --> 00:06:35,274
Your cloud vendor will usually choose these numbers for

98
00:06:35,312 --> 00:06:38,906
you. For example, say AWS typically reserves 255

99
00:06:38,928 --> 00:06:42,314
meg of memory for kubernetes, eleven meg for each

100
00:06:42,352 --> 00:06:44,750
pod that you can deploy on that instance.

101
00:06:45,250 --> 00:06:49,294
So this is the reserved memory for the cubelet. So the cpu reserved is

102
00:06:49,332 --> 00:06:52,282
usually around zero three to zero four of a cpu.

103
00:06:52,426 --> 00:06:55,886
For the operating system. They reserve 100 meg of memory

104
00:06:55,918 --> 00:06:59,074
and zero one cpu for the eviction threshold and

105
00:06:59,112 --> 00:07:02,942
another 100 meg. So in AWS

106
00:07:03,006 --> 00:07:06,246
if you select an m five large, here's a visual recap of how

107
00:07:06,268 --> 00:07:09,990
the resources are subdivided. With this particular instance you can deploy

108
00:07:10,490 --> 00:07:13,926
27 pods. The other thing to consider

109
00:07:14,028 --> 00:07:17,918
is all the time that this takes. So let's

110
00:07:17,954 --> 00:07:21,654
assume that you've configured your horizontal pod autoscaler or HPA

111
00:07:21,702 --> 00:07:25,766
in order to scale up your pods dynamically. So well that's

112
00:07:25,798 --> 00:07:28,250
where the journey probably starts.

113
00:07:28,830 --> 00:07:32,634
So to start with, about 90 seconds is what's

114
00:07:32,682 --> 00:07:36,522
needed for your horizontal pod autoscaler to react and decide

115
00:07:36,586 --> 00:07:38,560
to scale up your application.

116
00:07:40,050 --> 00:07:44,010
Then the cluster autoscaler then takes around say 30 seconds

117
00:07:44,090 --> 00:07:46,850
to request a new node from the cloud vendor,

118
00:07:47,270 --> 00:07:50,530
then around three to four minutes for the machine to boot,

119
00:07:51,030 --> 00:07:54,434
and then around another 30 seconds for it to join the cluster and then be

120
00:07:54,472 --> 00:07:57,894
ready to run workloads. Then you can of course add on

121
00:07:57,932 --> 00:08:01,286
time for pulling your container image that won't be cached on this

122
00:08:01,308 --> 00:08:04,758
brand new machine as well. So to help visualize the

123
00:08:04,764 --> 00:08:08,554
impact of this this can have I did made a library last

124
00:08:08,592 --> 00:08:11,926
year that actually fakes a Kubernetes scheduler. It allows

125
00:08:11,958 --> 00:08:15,834
you to specify many different types of nodes and model their

126
00:08:15,872 --> 00:08:19,658
scaling dynamics, tracking container startup time, so on,

127
00:08:19,824 --> 00:08:23,610
and define your node properties. So it takes a lot of shortcuts

128
00:08:23,690 --> 00:08:27,166
in order to provide hundreds of thousands of intervals representing kind

129
00:08:27,188 --> 00:08:30,926
of days, and do that in tens of milliseconds. It's not

130
00:08:30,948 --> 00:08:34,666
the real Kubernetes scheduler pull requests are very welcome if you'd like to improve

131
00:08:34,698 --> 00:08:38,146
it. So to give you a way to play with that.

132
00:08:38,168 --> 00:08:41,474
I also made a game as a novelty for Kubecon last year

133
00:08:41,512 --> 00:08:45,382
called Black Friday. The scenario is that you're an SRE team

134
00:08:45,436 --> 00:08:49,314
supporting a retailer facing a spike in traffic

135
00:08:49,362 --> 00:08:53,090
on Black Friday, and then again on Cyber Monday, with a lull

136
00:08:53,170 --> 00:08:58,038
between and a calm before and after she

137
00:08:58,044 --> 00:08:58,940
sees some things.

138
00:09:01,070 --> 00:09:04,634
It's a three tier service of a front end, back end,

139
00:09:04,672 --> 00:09:08,566
and database, all of which have different kind of scaling

140
00:09:08,598 --> 00:09:11,990
properties, startup times, et cetera.

141
00:09:12,150 --> 00:09:15,758
So we can see some of the details here in the hints. So we

142
00:09:15,764 --> 00:09:19,774
can see the properties on our node or different

143
00:09:19,812 --> 00:09:23,962
node types, and we can see the profile

144
00:09:24,026 --> 00:09:27,566
of the front end and similar for the back end. It's a bit zoomed

145
00:09:27,598 --> 00:09:31,934
in at the minute on this presentation. So you can see here. Here's my Friday

146
00:09:31,982 --> 00:09:36,034
spike, and here's my cyber Monday spike and how

147
00:09:36,072 --> 00:09:38,280
the heuristics of how that trials off.

148
00:09:39,690 --> 00:09:43,574
So the goal is to configure your cluster to

149
00:09:43,612 --> 00:09:47,902
as closely follow the spike with just enough infrastructure.

150
00:09:48,066 --> 00:09:51,914
So failing some requests and getting a few SLA penalties might

151
00:09:51,952 --> 00:09:55,770
actually result in a greater profit ultimately.

152
00:09:56,350 --> 00:09:59,962
So let's have a kind of play on this.

153
00:10:00,016 --> 00:10:03,950
So if I, for example, change my, say, minimum node count

154
00:10:04,020 --> 00:10:06,938
down to one, and similarly,

155
00:10:06,954 --> 00:10:10,750
just let's, let's see what this looks like. If I drop everything

156
00:10:10,820 --> 00:10:16,740
down to start at one instance of everything 1015,

157
00:10:20,790 --> 00:10:24,338
you can see I can change the point on when I scale my,

158
00:10:24,504 --> 00:10:27,510
when I scale up these, these pods.

159
00:10:27,930 --> 00:10:32,162
So let's see. Hopefully that should schedule out and we'll see some failed requests.

160
00:10:32,226 --> 00:10:35,346
Yeah, love you. Maybe live demos.

161
00:10:35,378 --> 00:10:35,960
Hey,

162
00:10:42,170 --> 00:10:46,010
yeah, that's live demo failing.

163
00:10:46,910 --> 00:10:49,500
Maybe let's go with. Go with two.

164
00:10:54,410 --> 00:10:58,600
Failing to schedule my first interval, I should have.

165
00:11:00,830 --> 00:11:03,830
Okay, so we can see that in this,

166
00:11:04,000 --> 00:11:08,014
we've reached a point where

167
00:11:08,052 --> 00:11:11,918
we've got some failed requests right here and

168
00:11:12,004 --> 00:11:15,322
some more failed requests on cyber Monday. So this is where my infrastructure

169
00:11:15,386 --> 00:11:19,874
scaling and my pod auto scaling failed to follow the

170
00:11:19,912 --> 00:11:23,140
spike up of demand as closely as it needed to.

171
00:11:25,270 --> 00:11:28,766
But that has actually only caused me a penalty

172
00:11:28,798 --> 00:11:32,502
in this scenario of $1,154. So,

173
00:11:32,556 --> 00:11:36,006
in effect, my company is in profit based on

174
00:11:36,028 --> 00:11:40,662
where it presumed it would be if it didn't have any autoscaling of about $15,000.

175
00:11:40,716 --> 00:11:43,900
So maybe not a bad idea, perhaps. And you can

176
00:11:44,590 --> 00:11:48,026
tune and tweak these numbers. So please do feel free to

177
00:11:48,048 --> 00:11:51,210
play with this. May the nodes ever be in your favor.

178
00:11:51,280 --> 00:11:54,926
Ultimately, this is an experiment to kind of play with and try

179
00:11:54,948 --> 00:11:56,510
and get your name on the leaderboard.

180
00:11:58,450 --> 00:12:01,360
Cool. Okay, first live demo out the way,

181
00:12:03,970 --> 00:12:07,700
what do we do in order to stack some of the odds in our favor?

182
00:12:08,790 --> 00:12:13,058
Well, we can not scale at all. That is always an option

183
00:12:13,224 --> 00:12:15,170
and often overlooked.

184
00:12:16,870 --> 00:12:20,210
Or what if we could get a head start on the scaling?

185
00:12:20,630 --> 00:12:24,438
So maybe not scaling at all? Sounds a bit flippant, but what do

186
00:12:24,444 --> 00:12:27,974
I really mean by that? Well, going back to our scenario of fitting our

187
00:12:28,012 --> 00:12:31,446
pods on a machine, taking into account the reserves for

188
00:12:31,468 --> 00:12:34,650
the kubernetes, if we sized our machine correctly,

189
00:12:35,550 --> 00:12:39,546
we might be able to fit all of our workload in a node. So this

190
00:12:39,568 --> 00:12:43,254
isn't easy given the vast array of possible machine sizes.

191
00:12:43,382 --> 00:12:47,066
So we've done some of the hard work for you in this space and created

192
00:12:47,098 --> 00:12:49,982
an instance calculator. Next live demo.

193
00:12:50,036 --> 00:12:53,630
So with this I can tune and tweak the

194
00:12:53,700 --> 00:12:57,134
configuration of my pod so I can say that I want to look

195
00:12:57,172 --> 00:13:00,746
at the efficiency and how many pods

196
00:13:00,778 --> 00:13:04,466
I can fit on it, so I can size my pods up in memory and

197
00:13:04,488 --> 00:13:07,774
see that dynamically move around. So I can see that I've not got a particularly

198
00:13:07,822 --> 00:13:11,142
efficient node size. Now I can flip around,

199
00:13:11,196 --> 00:13:15,602
I can see that other nodes offer a bearing

200
00:13:15,666 --> 00:13:19,974
level of efficiency if my pod looks like it requires these

201
00:13:20,012 --> 00:13:23,446
sorts of kind of properties, and I can densely

202
00:13:23,478 --> 00:13:27,782
stack my things so I can hunt around for different cloud vendors

203
00:13:27,926 --> 00:13:31,626
and pick the right tool, pick the right node for the

204
00:13:31,648 --> 00:13:35,134
type, and I can optimize for either cpu or

205
00:13:35,172 --> 00:13:38,526
indeed for memory as well, and kind of change some

206
00:13:38,548 --> 00:13:42,080
of the properties around the demon sets and agents and things like that.

207
00:13:44,210 --> 00:13:47,518
Cool. Okay, so finally onto the topic of

208
00:13:47,524 --> 00:13:49,200
this webinar. All the waits over,

209
00:13:50,770 --> 00:13:54,690
he says his clicker stops working. So what if we could always

210
00:13:54,760 --> 00:13:57,954
have at least one node that's ready for when you need it.

211
00:13:57,992 --> 00:14:01,478
So removing that three and a bit minute wait.

212
00:14:01,644 --> 00:14:05,058
Well, to do this, we can create a placeholder pod

213
00:14:05,154 --> 00:14:08,546
that as soon as your workload comes along needing the resource,

214
00:14:08,658 --> 00:14:12,038
the placeholder pod becomes evicted, causing your

215
00:14:12,044 --> 00:14:15,546
cluster autoscaling to boot a new machine in order to

216
00:14:15,568 --> 00:14:18,746
host the new replacement placeholder. And this will continue

217
00:14:18,848 --> 00:14:22,634
as you scale into further nodes, keeping you always one

218
00:14:22,672 --> 00:14:25,814
step ahead. Okay, now to praise

219
00:14:25,862 --> 00:14:29,246
the demo gods again, where I do a real live demo and hope that

220
00:14:29,268 --> 00:14:32,506
everything works with a real Kubernetes cluster.

221
00:14:32,618 --> 00:14:36,106
I have backup plans of videos if not. So let's

222
00:14:36,138 --> 00:14:39,454
have a look. So behind the scenes here,

223
00:14:39,492 --> 00:14:43,218
there is a real Kubernetes cluster, and I can prove that in

224
00:14:43,224 --> 00:14:46,606
a minute. So we've got a simple application where we can see the effects

225
00:14:46,638 --> 00:14:50,322
of clicking on the scale buttons. So if I say to scale

226
00:14:50,386 --> 00:14:53,446
to five. We start with

227
00:14:53,468 --> 00:14:57,558
one replica. Okay. And clicking the

228
00:14:57,644 --> 00:15:00,942
five. Hopefully that should start little huck.

229
00:15:01,106 --> 00:15:04,140
So I cross my fingers and hope that the thing that I built works.

230
00:15:09,070 --> 00:15:12,010
Cool. Hopefully. So they're pending.

231
00:15:13,730 --> 00:15:16,080
Never works. We have a backup plan.

232
00:15:28,130 --> 00:15:31,406
They are all there for the browser. Not loading

233
00:15:31,438 --> 00:15:35,330
the timer. Right. So what you would see is this timer starting to could

234
00:15:35,400 --> 00:15:39,140
up. We'll ignore it for the minute, but it will add a.

235
00:15:40,630 --> 00:15:44,354
We'll check back in a sec. But what you'll see is this timer should

236
00:15:44,392 --> 00:15:47,106
elapse for three minutes. So if you look at the time of how long we've

237
00:15:47,138 --> 00:15:50,486
been talking here, it will be about three minutes in

238
00:15:50,508 --> 00:15:54,106
order to scale up to have two nodes. So the minute the

239
00:15:54,128 --> 00:15:57,050
cluster auto scaler is going off and asking for a new node,

240
00:15:57,550 --> 00:16:01,034
we can do that in a separate instance over here. So I've got two

241
00:16:01,072 --> 00:16:05,750
clusters to

242
00:16:05,760 --> 00:16:09,360
actually at least show you the timer that actually works. It did.

243
00:16:12,290 --> 00:16:15,434
Okay, fine. Live demos.

244
00:16:15,482 --> 00:16:19,166
Okay, similarly. So assume that this is like plus 10 seconds

245
00:16:19,198 --> 00:16:22,020
and the other one's plus how many minutes, fine, we'll come back to them.

246
00:16:24,710 --> 00:16:28,286
We started one replica. We scaled to five. So the current node

247
00:16:28,398 --> 00:16:32,078
gets saturated with the four pods, and one is left pending behind the scenes.

248
00:16:32,174 --> 00:16:36,006
So now the cluster auto scaler is going to request a new node from

249
00:16:36,028 --> 00:16:39,398
Linode. So, while I stall for about three minutes of

250
00:16:39,404 --> 00:16:42,746
what would otherwise be kind of radio silence of me praying for it to kind

251
00:16:42,768 --> 00:16:46,186
of work properly, are there any questions I

252
00:16:46,208 --> 00:16:49,738
can come to? So,

253
00:16:49,824 --> 00:16:54,098
Sam, if we create a prescaled node for each node

254
00:16:54,134 --> 00:16:57,854
pool, would that then cost high? Right,

255
00:16:57,892 --> 00:17:00,240
so how to turn them off when we're not in use?

256
00:17:01,490 --> 00:17:05,550
So, yeah, if you've got a node that's hanging around that's

257
00:17:05,910 --> 00:17:09,540
basically just running a placeholder, then, yeah.

258
00:17:10,070 --> 00:17:14,014
Your plan is to ultimately

259
00:17:14,062 --> 00:17:17,474
waste some money. Ultimately. So your plan is to keep

260
00:17:17,592 --> 00:17:21,054
that spare. This isn't fundamentally an uncommon

261
00:17:21,102 --> 00:17:25,046
pattern that you might be used to seeing in

262
00:17:25,068 --> 00:17:28,294
other worlds where you might have a raid array, where you might have a hot

263
00:17:28,332 --> 00:17:31,686
spare in that, because, sure, you can go and get a

264
00:17:31,708 --> 00:17:35,158
new drive or something or a new blade unit to throw in the chassis,

265
00:17:35,254 --> 00:17:38,106
but if you've got one already wrapped up, it saves you a trip to the

266
00:17:38,128 --> 00:17:41,654
data center. So likewise, albeit that's a timeline

267
00:17:41,702 --> 00:17:45,020
that normally is measured in best hours, probably days.

268
00:17:45,550 --> 00:17:49,182
This is. We're trying to now shave minutes off so we're at a better place

269
00:17:49,236 --> 00:17:52,350
than we might have been with more traditional infrastructure and hardware.

270
00:17:52,690 --> 00:17:56,138
But I'll come to some of that point at the end how you might

271
00:17:56,164 --> 00:18:00,594
be able to lessen the financial impact of

272
00:18:00,632 --> 00:18:02,130
this sort of pattern.

273
00:18:04,870 --> 00:18:08,446
So we mentioned oversubscribing the nodes

274
00:18:08,478 --> 00:18:12,486
with requests and limits. How does

275
00:18:12,508 --> 00:18:17,974
that work when we oversubscribe our vms in VMware? So if

276
00:18:18,012 --> 00:18:21,946
you oversubscribe your nodes in the

277
00:18:21,968 --> 00:18:26,186
hypervisor, then you are in for a

278
00:18:26,288 --> 00:18:29,740
bad day. I guess it's a very short answer.

279
00:18:30,590 --> 00:18:33,786
Kubernetes is not how it's designed to work in the

280
00:18:33,808 --> 00:18:39,002
same way as you'll notice that Kubernetes does not play well with a swap

281
00:18:39,066 --> 00:18:42,366
for memory. It relies on kind of real resource and you need

282
00:18:42,388 --> 00:18:46,494
to really kind of give kubernetes as

283
00:18:46,532 --> 00:18:49,966
much as it can get in terms of empowerment

284
00:18:49,998 --> 00:18:53,298
to kind of know what's going on.

285
00:18:53,464 --> 00:18:56,718
I think on the first node I can see, so I've

286
00:18:56,734 --> 00:19:00,114
got two clusters here, there's two tabs. This first

287
00:19:00,152 --> 00:19:04,390
cluster has now got its second nodes, so we should see a second

288
00:19:04,460 --> 00:19:08,018
pod. So once that's pending at the minute, I want the other nodes.

289
00:19:08,034 --> 00:19:11,960
There we go. Fine. Okay, so that took about

290
00:19:12,490 --> 00:19:15,100
three minutes. So I'll just finish off that question.

291
00:19:15,950 --> 00:19:19,450
So yeah, you really need to give Kubernetes as much information

292
00:19:19,520 --> 00:19:23,146
as you can in order for it to make the best decision. So like

293
00:19:23,168 --> 00:19:26,334
help it, help you ultimately tell it what's going

294
00:19:26,372 --> 00:19:30,206
on. That's okay. So as

295
00:19:30,228 --> 00:19:33,982
you can see, that took about three minutes for my pod to scale from one

296
00:19:34,036 --> 00:19:37,934
to five. The first bit of it was done within about 10 seconds

297
00:19:38,062 --> 00:19:41,540
and then the last kind of pending pod took a bit longer.

298
00:19:42,390 --> 00:19:45,634
So let's see what

299
00:19:45,672 --> 00:19:49,214
the difference is if I use my placeholder pattern.

300
00:19:49,262 --> 00:19:51,800
So what I'm going to do is I'm going to scale back down to one

301
00:19:52,650 --> 00:19:55,720
should hopefully go and kill my pods. Cool.

302
00:19:56,170 --> 00:19:59,718
And then I'm going to deploy my placeholder. So I'll show you in

303
00:19:59,724 --> 00:20:02,070
a minute. This is a real Kubernetes cluster.

304
00:20:02,570 --> 00:20:05,658
I've just got some Javascript and a web page that

305
00:20:05,744 --> 00:20:09,206
makes like submitting some yaml, just so you got a visual view of what's

306
00:20:09,238 --> 00:20:13,450
going on. On my other screen, I am panically watching

307
00:20:13,520 --> 00:20:16,082
the actual kind of the logs and the events.

308
00:20:16,246 --> 00:20:19,646
So now I've got a node here. So I'm back down to

309
00:20:19,668 --> 00:20:23,342
my original scenario where I have one instance of the application running

310
00:20:23,476 --> 00:20:27,054
and I have my placeholder nodes keeping the

311
00:20:27,092 --> 00:20:30,978
other node present. So it's a kind of hack on the auto scaler just to

312
00:20:30,984 --> 00:20:35,090
keep like a node available. So now

313
00:20:35,160 --> 00:20:37,700
if I click scale to five,

314
00:20:39,130 --> 00:20:42,230
I'll quite quickly see that it saturated the first node and

315
00:20:42,300 --> 00:20:45,714
in a few seconds,

316
00:20:45,842 --> 00:20:49,750
7 seconds. There we go. My container has now booted

317
00:20:50,090 --> 00:20:53,226
in the second node. So we've gone from kind of near on

318
00:20:53,248 --> 00:20:56,714
four minutes down to about 7 seconds. Pretty good,

319
00:20:56,752 --> 00:21:00,060
right? Cool. Okay,

320
00:21:02,670 --> 00:21:05,866
we can turn the placeholder back on and what that

321
00:21:05,888 --> 00:21:09,818
will do in the background is that will go off and start another node.

322
00:21:09,914 --> 00:21:13,038
Sorry, the placeholder was already on. Sorry, I didn't need to do it. So the

323
00:21:13,044 --> 00:21:17,442
placeholder was there. So in the background or what's already happening is that the

324
00:21:17,576 --> 00:21:21,250
scheduler has started booting another node.

325
00:21:21,670 --> 00:21:24,930
So we can do that again in my second cluster,

326
00:21:26,790 --> 00:21:29,842
which has stalled, showing the number of pods. Fine.

327
00:21:29,896 --> 00:21:32,294
Okay, maybe we won't do it in the second cluster. Something weird is going on

328
00:21:32,332 --> 00:21:32,920
there.

329
00:21:35,370 --> 00:21:39,062
Fine. At least I had two plans. Cool.

330
00:21:39,116 --> 00:21:41,560
Okay. That was the stressful bit out of the way,

331
00:21:43,790 --> 00:21:45,500
and then the slides don't change.

332
00:21:50,430 --> 00:21:53,654
Could. So to prove that this is a real cluster,

333
00:21:53,702 --> 00:21:57,180
this is what was just going on just a second ago,

334
00:21:57,650 --> 00:22:01,086
looking at the events that were kind of

335
00:22:01,108 --> 00:22:04,446
streaming out the cluster, demonstrating the nodes coming up and down and the pods being

336
00:22:04,468 --> 00:22:07,854
scaled up and down as I went. We can

337
00:22:07,892 --> 00:22:10,930
skip past these because this is the backup videos that I had of it working.

338
00:22:11,000 --> 00:22:14,322
Cool. So how did I make this all

339
00:22:14,376 --> 00:22:18,254
happen though? So firstly, we need a placeholder,

340
00:22:18,302 --> 00:22:22,402
and that needs to be big enough to know that it will never be schedulable

341
00:22:22,546 --> 00:22:26,342
alongside any real workload on a node. So it should

342
00:22:26,396 --> 00:22:30,054
be sized big enough in order to fill the whole

343
00:22:30,092 --> 00:22:33,698
compute node. Then you need to specify a

344
00:22:33,724 --> 00:22:36,986
low priority class in order to make sure that it's evicted as

345
00:22:37,008 --> 00:22:40,678
soon as there's a real workload that comes along and needs that capacity.

346
00:22:40,854 --> 00:22:44,294
So the placeholder pod competes for resources.

347
00:22:44,342 --> 00:22:47,614
So we need to define that. We want it to have that low

348
00:22:47,652 --> 00:22:51,290
priority with a priority of minutes one. All other pods,

349
00:22:51,450 --> 00:22:55,162
even by default ones, will have precedents

350
00:22:55,226 --> 00:22:58,960
and the placeholder is then evicted as soon as the cluster runs out of space.

351
00:23:01,090 --> 00:23:04,338
If I flip back, we should hopefully almost,

352
00:23:04,424 --> 00:23:08,580
maybe. There we go. Bingo. So we've now got our

353
00:23:09,830 --> 00:23:13,894
over provisioned nodes, and if I scale back down to one

354
00:23:13,932 --> 00:23:17,910
and turn the placeholder off, we can come back to that at the end and

355
00:23:17,980 --> 00:23:21,070
hopefully see that the auto scaler

356
00:23:21,090 --> 00:23:24,858
has reduced us back down to zero. That will take a minute or so

357
00:23:24,944 --> 00:23:27,900
for the cluster auto scaler to recognize that.

358
00:23:31,230 --> 00:23:34,682
Okay, so now for a demo of how this works

359
00:23:34,736 --> 00:23:38,026
with auto scaling, however, though. So just now we demoed

360
00:23:38,058 --> 00:23:41,626
it with kind of point and click to change the replica

361
00:23:41,658 --> 00:23:45,694
count. How does this work in a more real worldish type scenario where

362
00:23:45,732 --> 00:23:48,750
you may have your kind of horizontal port auto scaling.

363
00:23:49,330 --> 00:23:52,766
I'll be honest with you, I have definitely exhausted

364
00:23:52,798 --> 00:23:56,194
my credit with the demo gods. So I'm going to be playing some videos here

365
00:23:56,232 --> 00:23:59,778
and provide a bit of narration and also save us all hanging around for kind

366
00:23:59,784 --> 00:24:01,720
of the three minutes or so that it takes.

367
00:24:03,370 --> 00:24:08,182
So before

368
00:24:08,236 --> 00:24:11,414
I start to provide a little orientation on the left hand side,

369
00:24:11,452 --> 00:24:15,130
we can see the request per second that we're serving. The bottom

370
00:24:15,200 --> 00:24:18,458
left, you can see the nodes and the pods on them. And you

371
00:24:18,464 --> 00:24:22,122
can see my nodes in here again, can take up to four

372
00:24:22,176 --> 00:24:25,926
workload pods. I've got a simple application that can handle

373
00:24:25,958 --> 00:24:29,610
a fixed number of requests, and I'm ramping up traffic.

374
00:24:29,690 --> 00:24:33,134
As you can see, we start with two nodes and we're able to kind of

375
00:24:33,172 --> 00:24:37,070
sustainably handle the traffic as it increases in until

376
00:24:37,140 --> 00:24:41,006
we fill both nodes. And at that point that the HPA

377
00:24:41,038 --> 00:24:44,974
has decided to scale up and then we finally see the cluster auto

378
00:24:45,022 --> 00:24:48,660
scaler in a minutes or so of time that I can fast forward

379
00:24:48,970 --> 00:24:52,582
start to actually provide the more nodes that will be

380
00:24:52,636 --> 00:24:53,430
required.

381
00:24:55,450 --> 00:24:57,160
Skip forward a bit. There we go.

382
00:24:58,730 --> 00:25:02,050
The cluster also scale has now come in and provided the extra nodes.

383
00:25:02,130 --> 00:25:05,754
So I've manipulated these results a little so as not to leave you waiting too

384
00:25:05,792 --> 00:25:09,418
long, which beat about around four minutes for the

385
00:25:09,424 --> 00:25:13,066
nodes to be available. So in that time, while we

386
00:25:13,088 --> 00:25:16,682
were waiting for the nodes to be available, well, the traffic that we were able

387
00:25:16,736 --> 00:25:20,526
to kind of service as an application really kind

388
00:25:20,548 --> 00:25:23,786
of flattens out. But as soon as we've kind of got the resources,

389
00:25:23,898 --> 00:25:27,154
it kind of goes up again and we can see the recovery curve coming

390
00:25:27,192 --> 00:25:30,194
in. Okay,

391
00:25:30,392 --> 00:25:33,090
so next round.

392
00:25:37,750 --> 00:25:41,730
Yeah, cool. So now we can compare that to our more proactive pattern

393
00:25:41,810 --> 00:25:45,830
where we have a placeholder pod that's keeping us a spare node ready

394
00:25:45,900 --> 00:25:50,146
at all times. As the traffic builds up, we can see that that placeholder

395
00:25:50,178 --> 00:25:53,866
quickly becomes evicted and our workload pods become scheduled on

396
00:25:53,888 --> 00:25:57,334
that node, a new placeholder pod is created as pending,

397
00:25:57,382 --> 00:26:01,498
causing the cluster autoscaler to go off and create a new node. So sometimes,

398
00:26:01,584 --> 00:26:04,938
however, as happens here, we'll see the traffic buildup in the

399
00:26:04,944 --> 00:26:08,142
HPA outpace, the speed at which we were able to stand

400
00:26:08,196 --> 00:26:09,470
up the new nodes.

401
00:26:13,010 --> 00:26:16,278
The placeholder pod didn't actually get landed

402
00:26:16,314 --> 00:26:18,340
on the node until sometime later.

403
00:26:24,900 --> 00:26:28,608
As you can see, we're adding nodes, and immediately we're not even getting the

404
00:26:28,614 --> 00:26:31,984
chance to schedule the over provisioning pod in them.

405
00:26:32,102 --> 00:26:35,524
Though the over provisioning pod has always made sure that we've always had

406
00:26:35,562 --> 00:26:38,304
a request in flight to get a new node,

407
00:26:38,432 --> 00:26:41,876
but the result of this ultimately is, as you can see here, is that the

408
00:26:41,898 --> 00:26:45,780
traffic steadily kind of paces up. There's some bumps,

409
00:26:45,860 --> 00:26:49,396
but we can see that there's no kind of flattening

410
00:26:49,428 --> 00:26:53,156
out. So there's no point where we actually are failing requests

411
00:26:53,188 --> 00:26:54,090
at any point.

412
00:26:57,200 --> 00:27:00,540
So this all comes at an inevitable cost. Your plan

413
00:27:00,610 --> 00:27:04,568
is, in this case, to always have extra capacity ready and waiting

414
00:27:04,584 --> 00:27:08,472
for your workload to require it. What might therefore

415
00:27:08,536 --> 00:27:12,316
be some better answers, though? Well, you can tune your workload

416
00:27:12,428 --> 00:27:15,360
in order to make sure that you're not leaving gaps.

417
00:27:15,940 --> 00:27:18,992
Or better yet, remember that pod priority thing that we used?

418
00:27:19,046 --> 00:27:22,804
Well, if you've got workload that suits it on your cluster, that could like

419
00:27:22,842 --> 00:27:26,832
to run and would give you more return on investment than just a placeholder.

420
00:27:26,896 --> 00:27:30,756
So stuff that can handle stopping and starting when

421
00:27:30,778 --> 00:27:33,856
it's appropriate to. So perhaps some housekeeping,

422
00:27:33,888 --> 00:27:37,448
some analytics and machine learning, or maybe just less important

423
00:27:37,534 --> 00:27:41,156
services. So you might want to say proactive. The shopping cart

424
00:27:41,268 --> 00:27:45,080
supporting applications and pods over, say, the customer service

425
00:27:45,150 --> 00:27:48,360
desk. Ones that you can structure,

426
00:27:48,940 --> 00:27:52,408
allowing you to structure your cluster workload to be more aligned to

427
00:27:52,414 --> 00:27:54,280
your business benefits and goals.

428
00:27:55,660 --> 00:27:59,056
I've been Chris Nesbittsmith thank you again for joining me today.

429
00:27:59,158 --> 00:28:02,896
Like subscribe, whatever the kids do on LinkedIn, GitHub, whatever you can be.

430
00:28:02,918 --> 00:28:06,736
Rest assured there'll be little to no spam since I'm much content at all since

431
00:28:06,758 --> 00:28:09,856
I'm awful at self promotion, especially on social media.

432
00:28:09,958 --> 00:28:13,836
CNS me just points at my LinkedIn talks. CNS me contains

433
00:28:13,868 --> 00:28:17,948
this and some other talks, and they're all open, so that's

434
00:28:17,964 --> 00:28:21,550
the end. Questions are very welcome on this or pretty much anything else.

