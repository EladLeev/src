1
00:00:41,490 --> 00:00:45,186
Hi everyone, welcome to this session on global active

2
00:00:45,218 --> 00:00:48,914
active serverless architectures. My name is George Fonseca.

3
00:00:48,962 --> 00:00:52,190
I'm a senior solution architect at Amazon Web Services.

4
00:00:52,340 --> 00:00:56,334
Whatever the industry, organizations today are looking to become more

5
00:00:56,372 --> 00:00:59,834
agile so that they can innovate and respond

6
00:00:59,882 --> 00:01:04,206
to changes faster. To do that, organizations need to build applications

7
00:01:04,238 --> 00:01:08,046
that can scale to millions of users while still having global

8
00:01:08,078 --> 00:01:11,938
availability, respond in milliseconds and manage

9
00:01:12,024 --> 00:01:15,206
petabytes, if not exabytes, of data. We call

10
00:01:15,228 --> 00:01:18,786
them modern applications and they cover use cases

11
00:01:18,898 --> 00:01:23,506
from web mobile, IoT to machine learning applications,

12
00:01:23,618 --> 00:01:27,922
but also shared service platforms and microservice backends

13
00:01:27,986 --> 00:01:32,358
and more. One of the main factors impacting global applications

14
00:01:32,454 --> 00:01:35,850
is the end user network latency. In the past,

15
00:01:36,000 --> 00:01:39,226
Amazon has reported a drop of 1% in sales

16
00:01:39,328 --> 00:01:42,734
for each additional 100 milliseconds in load time.

17
00:01:42,852 --> 00:01:47,006
Content delivery networks, or cdns, have successfully been used

18
00:01:47,108 --> 00:01:50,126
to speed up the global delivery of static content,

19
00:01:50,228 --> 00:01:53,710
and these include images, videos and JavaScript

20
00:01:53,790 --> 00:01:57,154
libraries. But the dynamic calls those still

21
00:01:57,192 --> 00:02:01,202
need to be sent back to the back reads. For example,

22
00:02:01,336 --> 00:02:05,466
if you have users in Europe accessing a backend in Australia,

23
00:02:05,598 --> 00:02:09,234
those users will notice an additional 300 milliseconds

24
00:02:09,282 --> 00:02:13,282
in latency, and this isn't acceptable for most popular games,

25
00:02:13,346 --> 00:02:17,822
but also banking requirements or interactive applications.

26
00:02:17,986 --> 00:02:21,690
Therefore, having locally available applications and

27
00:02:21,760 --> 00:02:25,258
content is becoming more and more important these days.

28
00:02:25,424 --> 00:02:29,954
But bear in mind, building and successfully running a multiregion

29
00:02:30,022 --> 00:02:33,406
active active architecture is hard. In this session we

30
00:02:33,428 --> 00:02:36,890
will address one common use case, an HTTP

31
00:02:36,970 --> 00:02:41,082
API processing relational data with heavy reads

32
00:02:41,146 --> 00:02:45,026
and fewer writes. Also, these writes need

33
00:02:45,048 --> 00:02:49,106
to be transactional across multiple microservices as

34
00:02:49,128 --> 00:02:53,198
well as third party services and on premise components.

35
00:02:53,374 --> 00:02:56,818
Other use cases for multiregion deployment include

36
00:02:56,914 --> 00:03:00,774
disaster recovery, where multiregion is a standard practice to

37
00:03:00,812 --> 00:03:03,970
keep your disaster recovery environment on a different region.

38
00:03:04,050 --> 00:03:07,474
Also, data residency, where multiregion is a solution

39
00:03:07,522 --> 00:03:10,906
for compliancy and regulation when you need to keep the

40
00:03:10,928 --> 00:03:14,074
data of your users within the regions of those

41
00:03:14,112 --> 00:03:17,798
users. There's also software as a service applications,

42
00:03:17,894 --> 00:03:21,462
where multiregion is a standard practice for tenant isolation.

43
00:03:21,606 --> 00:03:25,182
And then there are the antipatterns. When should

44
00:03:25,236 --> 00:03:28,974
you avoid using multiregion deployment? Well,

45
00:03:29,092 --> 00:03:32,334
there is the high availability insider region. For these

46
00:03:32,372 --> 00:03:35,434
scenarios you should leverage availability zones,

47
00:03:35,562 --> 00:03:40,542
not multiregion deployment. Then there is the comparison between multiregion

48
00:03:40,606 --> 00:03:44,526
and AWS edge service solutions, and often edge

49
00:03:44,558 --> 00:03:48,322
service is enough to address the latency without complicating

50
00:03:48,386 --> 00:03:52,066
your solution. But I am assuming that you have already analyzed

51
00:03:52,098 --> 00:03:55,842
the pros and cons of these solutions and you have decided

52
00:03:55,906 --> 00:04:00,042
for the strategy of multiregion. So let's move on. The final

53
00:04:00,096 --> 00:04:03,434
goal of the session is to have our applications deployed to

54
00:04:03,472 --> 00:04:07,462
several regions across the globe. These regions will communicate

55
00:04:07,526 --> 00:04:11,246
through interregional secure connections, and the

56
00:04:11,268 --> 00:04:15,242
on premise data centers will have a high throughput, low latency

57
00:04:15,306 --> 00:04:19,214
connectivity to the closest AWS region. Finally,

58
00:04:19,332 --> 00:04:22,898
our users will interact via the Internet with the

59
00:04:22,904 --> 00:04:26,670
application stack at the region with the lowest latency

60
00:04:26,830 --> 00:04:30,034
relative to those users. The first topic to

61
00:04:30,072 --> 00:04:33,566
approach is the connectivity between the AWS regions.

62
00:04:33,758 --> 00:04:37,558
For that, customers can use a managed network service

63
00:04:37,644 --> 00:04:41,058
called AWS Transit Gateway. Transit gateway

64
00:04:41,154 --> 00:04:44,966
connects vpcs to on prem networks through a

65
00:04:44,988 --> 00:04:48,762
central hub. This simplifies networking peering and

66
00:04:48,816 --> 00:04:52,566
acts as a cloud router both within the regions

67
00:04:52,678 --> 00:04:56,086
and across the regions. This interregion

68
00:04:56,118 --> 00:04:59,638
peering uses the AWS global network

69
00:04:59,734 --> 00:05:03,434
to connect the transit gateways together, and this is a fully

70
00:05:03,482 --> 00:05:07,466
redundant fiber network backbone providing many terabytes

71
00:05:07,498 --> 00:05:11,086
of capacity between the regions at speeds of up to

72
00:05:11,108 --> 00:05:14,894
100 gigs per second. Furthermore, data is automatically

73
00:05:14,942 --> 00:05:17,790
encrypted and never travels over the public Internet.

74
00:05:17,870 --> 00:05:21,586
For our use based customers can deploying transit gateway on

75
00:05:21,608 --> 00:05:25,166
each target region and then connect pairs of regions with

76
00:05:25,208 --> 00:05:29,202
the transit gateways. Customers will then connect each transit gateway

77
00:05:29,266 --> 00:05:33,030
with all other transit gateways and the result will be

78
00:05:33,100 --> 00:05:36,674
this full mesh of transit gateways across the globe.

79
00:05:36,802 --> 00:05:40,314
But how about connectivity from on premise data

80
00:05:40,352 --> 00:05:43,718
centers and third party networks to AWS?

81
00:05:43,894 --> 00:05:47,450
For that, we recommend customers to use AWS direct

82
00:05:47,520 --> 00:05:51,134
connect for all production workloads. Direct Connect is

83
00:05:51,172 --> 00:05:54,634
a cloud service solution that establishes a private

84
00:05:54,682 --> 00:05:58,670
connectivity between AWS and your data center or

85
00:05:58,740 --> 00:06:02,094
your office or your colocation environment with

86
00:06:02,132 --> 00:06:04,690
speeds up to 100 gigs per second.

87
00:06:04,840 --> 00:06:08,242
Alternatively, you may use AWS site to site

88
00:06:08,296 --> 00:06:11,358
VPN and AWS client VPN

89
00:06:11,454 --> 00:06:15,082
to establish that secure connection over the public Internet.

90
00:06:15,166 --> 00:06:18,854
To improve the end user experience, customers can also

91
00:06:18,972 --> 00:06:22,966
expose the APIs using Amazon Cloudfront. This is a

92
00:06:22,988 --> 00:06:26,854
fast, secure and programmable CDN. It is used by

93
00:06:26,892 --> 00:06:30,838
customers like Tinder and Slack to secure and accelerate

94
00:06:30,934 --> 00:06:35,078
the dynamic API calls as well as the websocket connections.

95
00:06:35,254 --> 00:06:39,478
It accelerates traffic by routing it from the edge locations

96
00:06:39,574 --> 00:06:43,550
where the users are to the AWS origins using

97
00:06:43,620 --> 00:06:46,698
AWS's dedicated network backbone.

98
00:06:46,874 --> 00:06:49,690
But when you deploy to multiregions,

99
00:06:49,770 --> 00:06:53,514
cloud front might not be enough. Users will have to be routed

100
00:06:53,562 --> 00:06:56,994
to the nearest AWS region and the question is how

101
00:06:57,032 --> 00:07:00,530
do you do that while keeping the same URL? The answer

102
00:07:00,600 --> 00:07:04,322
is Amazon route 53. This is a highly available and

103
00:07:04,376 --> 00:07:07,942
scalable cloud DNS web service. In particular,

104
00:07:08,076 --> 00:07:11,782
we want to look at its geolocation, geoproximity and

105
00:07:11,836 --> 00:07:15,634
latency routing policies so that we can route end users

106
00:07:15,682 --> 00:07:19,894
to the best application endpoints for our multiregion

107
00:07:19,942 --> 00:07:23,866
active active use case customers can combine cloud from

108
00:07:23,968 --> 00:07:27,606
and route 53 to achieve this domain

109
00:07:27,638 --> 00:07:31,322
name translation, but also dynamic content security

110
00:07:31,456 --> 00:07:35,126
and acceleration. Also path based functional

111
00:07:35,158 --> 00:07:39,422
routing and finally latency based routing across

112
00:07:39,476 --> 00:07:43,114
the regions. So let's recap the network architectures

113
00:07:43,162 --> 00:07:46,602
for our use case customers can use transit gateway

114
00:07:46,666 --> 00:07:50,414
to establish the secure low latency connectivity between the AWS

115
00:07:50,462 --> 00:07:53,666
regions, use direct connect to establish a

116
00:07:53,688 --> 00:07:57,294
secure private low latency connectivity between the data centers

117
00:07:57,342 --> 00:08:00,850
and the nearest AWS region, and then use cloud

118
00:08:00,920 --> 00:08:04,562
from with route 53 to route end users

119
00:08:04,626 --> 00:08:08,974
from the edge locations to the best AWS region based on network

120
00:08:09,042 --> 00:08:12,474
latency and with the network connectivity sorted, the next

121
00:08:12,512 --> 00:08:16,102
step is to keep the data in sync between the AWS

122
00:08:16,166 --> 00:08:20,394
regions and for that there are at least two approaches for

123
00:08:20,432 --> 00:08:24,554
this cross regional data replication, the synchronous

124
00:08:24,602 --> 00:08:28,874
solution and the asynchronous solution. With synchronous replication,

125
00:08:29,002 --> 00:08:32,914
write requests need to successfully replicate across regions so

126
00:08:32,952 --> 00:08:36,862
that they can be acknowledged back to the application. This ensures

127
00:08:36,926 --> 00:08:40,594
consistency across regions, but creates a dependency on

128
00:08:40,632 --> 00:08:44,402
other regions, so if one region fails, they all

129
00:08:44,456 --> 00:08:47,894
fail. With a synchronous replication, write requests are

130
00:08:47,932 --> 00:08:51,382
successful if they are persisted locally only

131
00:08:51,516 --> 00:08:55,830
and the cross regions replication is deferred by milliseconds or

132
00:08:55,900 --> 00:08:58,618
until the target regions are available.

133
00:08:58,784 --> 00:09:02,838
So with this we overcome the regional outages,

134
00:09:03,014 --> 00:09:07,270
but we also cause writing conflicts between regions.

135
00:09:07,430 --> 00:09:10,894
Nonetheless, we will follow the async approach for

136
00:09:10,932 --> 00:09:14,526
our use case and we will see how to solve these issues.

137
00:09:14,708 --> 00:09:18,622
The first crossregional database engine to consider is

138
00:09:18,676 --> 00:09:22,282
Amazon DynamoDB Global Tables DynamoDB

139
00:09:22,346 --> 00:09:25,714
is a serverless key value and document pathbased that

140
00:09:25,752 --> 00:09:29,090
can handle up to 10 trillion requests per day and support

141
00:09:29,160 --> 00:09:32,082
peaks of up to 20 million requests per second.

142
00:09:32,216 --> 00:09:35,922
The global tables feature provides a fully managed

143
00:09:35,986 --> 00:09:39,538
automatic table replication across AWS regions

144
00:09:39,634 --> 00:09:43,686
with single digit millisecond latency. Unfortunately, it does

145
00:09:43,708 --> 00:09:47,910
not fit our use based because we have the requirement for

146
00:09:47,980 --> 00:09:51,830
relational and transactional data. The second

147
00:09:51,900 --> 00:09:55,926
cross regional database engine to consider is Amazon Aurora

148
00:09:56,038 --> 00:10:00,098
Global database. Aurora is a MySQL and PostgreSQL

149
00:10:00,134 --> 00:10:03,626
compatible relational database built for the cloud. The global

150
00:10:03,658 --> 00:10:07,642
database feature, which is available with Amazon Aurora serverless

151
00:10:07,706 --> 00:10:11,194
version two, allows a single database to span

152
00:10:11,242 --> 00:10:15,558
multiple AWS regions with subsecond latency to readonly

153
00:10:15,594 --> 00:10:19,438
replicas on those regions. In the case of a regional outage,

154
00:10:19,534 --> 00:10:23,118
a secondary region can be promoted to primary region

155
00:10:23,214 --> 00:10:26,658
in less than 1 minute. This results in an RTo

156
00:10:26,754 --> 00:10:30,166
of 1 minute, which is recovery time objective and

157
00:10:30,188 --> 00:10:33,346
an RPo of 1 second, which is recovery

158
00:10:33,378 --> 00:10:37,506
point objective. Finally, customers can consider Amazon

159
00:10:37,538 --> 00:10:41,094
s three replication. Amazon S three is an object storage

160
00:10:41,142 --> 00:10:44,538
service used to store and protect any amount of data for a

161
00:10:44,544 --> 00:10:47,958
range of use cases, and these use cases include

162
00:10:48,054 --> 00:10:51,394
data lakes, websites, enterprise applications,

163
00:10:51,462 --> 00:10:54,974
IoT, and big data analytics, just to name a few.

164
00:10:55,092 --> 00:10:59,054
The S three replication feature allows data to be replicated from

165
00:10:59,092 --> 00:11:02,522
one sourced bucket to multiple destination buckets, with most

166
00:11:02,596 --> 00:11:06,722
objects being replicated in seconds and 99.99%

167
00:11:06,776 --> 00:11:10,834
of objects being replicated within the first 15 minutes.

168
00:11:11,032 --> 00:11:14,622
So let's recap the considerations for cross regions

169
00:11:14,686 --> 00:11:18,978
data replication. We looked at Amazon DynamodB, Amazon Aurora,

170
00:11:19,074 --> 00:11:23,986
and Amazon S three. Each of these services offers crossregional

171
00:11:24,098 --> 00:11:27,570
automatic replication features using serverless technology,

172
00:11:27,740 --> 00:11:30,986
so now it's time to put them to work. For that,

173
00:11:31,088 --> 00:11:34,902
I will introduce the command and query responsibility segregation

174
00:11:34,966 --> 00:11:39,062
pattern, or cqrs for short. This architectures pattern involves

175
00:11:39,126 --> 00:11:43,294
separating the data mutation part of a system from the query part

176
00:11:43,332 --> 00:11:46,782
of that system. In traditional architectures, the same data

177
00:11:46,836 --> 00:11:50,170
model is used to query and update the pathbased.

178
00:11:50,250 --> 00:11:53,726
That is simple and works well for basic crude operations.

179
00:11:53,838 --> 00:11:57,454
But our use case is asymmetrical with heavy reads

180
00:11:57,502 --> 00:12:01,374
and fewer writes. Therefore, it has very different performance

181
00:12:01,422 --> 00:12:05,258
and scalability requirements between reads and writes.

182
00:12:05,374 --> 00:12:09,442
Customers can use cqrs to perform writes onto normalized

183
00:12:09,506 --> 00:12:13,158
modules in relational databases and then perform

184
00:12:13,244 --> 00:12:16,966
queries against a normalized database that stores the

185
00:12:16,988 --> 00:12:20,646
data in the same format required by the APIs.

186
00:12:20,758 --> 00:12:24,202
This will reduce the processing overhead of the reads while

187
00:12:24,256 --> 00:12:27,946
increasing the maintainability of complex business logic on

188
00:12:27,968 --> 00:12:31,722
the writes in AWS. The CQRS pattern is typically

189
00:12:31,786 --> 00:12:35,482
implemented with Amazon API gateway. In this scenario,

190
00:12:35,626 --> 00:12:39,514
mutations are post requests processed by an AWS

191
00:12:39,562 --> 00:12:43,418
lambda function that calls domain specific services.

192
00:12:43,514 --> 00:12:46,750
A denormalized version of the data is then mirrored

193
00:12:46,830 --> 00:12:50,274
onto DynamoDB so that subsequent queries are

194
00:12:50,312 --> 00:12:54,254
performed by get requests reading the normalized objects

195
00:12:54,302 --> 00:12:57,890
directly from DynamoDB tables. The asynchronous version

196
00:12:57,970 --> 00:13:02,166
of CQRS pattern implementation adds a queue to

197
00:13:02,188 --> 00:13:06,306
the writing operation. This will allow for long running writes

198
00:13:06,418 --> 00:13:09,874
with the immediate API responses back to the clients,

199
00:13:10,002 --> 00:13:13,802
but now reads have become more complexity because the write

200
00:13:13,856 --> 00:13:17,638
duration of the requests is unknown. The solution

201
00:13:17,734 --> 00:13:21,498
is to notify the clients using the websockets feature

202
00:13:21,594 --> 00:13:25,214
of API gateway, which is invoked by a lambda function when

203
00:13:25,252 --> 00:13:28,686
the dynamodB tables are updated. Another way

204
00:13:28,708 --> 00:13:32,174
to implement CQRs is to use AWS app

205
00:13:32,212 --> 00:13:35,294
sync, exposing the API as a GraphQL

206
00:13:35,342 --> 00:13:39,758
API. This definitely simplifies real time updates

207
00:13:39,854 --> 00:13:43,134
because it allows for no code data subscriptions

208
00:13:43,262 --> 00:13:46,706
directly from DynamoDB, and it also reduces the

209
00:13:46,728 --> 00:13:50,326
implementation complexity by allowing front end developers to

210
00:13:50,348 --> 00:13:53,586
query multiple data entities with a single graphic

211
00:13:53,618 --> 00:13:56,662
QL endpoint. When applied to our use case,

212
00:13:56,796 --> 00:14:00,578
this results in a multiregional architecture where

213
00:14:00,764 --> 00:14:04,246
route 53 routes requests to a regional

214
00:14:04,278 --> 00:14:07,834
API endpoint based on latency, and the data is

215
00:14:07,872 --> 00:14:11,626
then queried or subscribed to from DynamoDB global

216
00:14:11,658 --> 00:14:15,482
Tables in sync between or across the regions

217
00:14:15,626 --> 00:14:19,294
AWS. For the migrations, they are limited to the

218
00:14:19,332 --> 00:14:23,026
primary region only and mutation requests on

219
00:14:23,048 --> 00:14:26,958
the secondary regions are forwarded to the primary regions.

220
00:14:27,054 --> 00:14:30,862
This pattern is called read local, write global

221
00:14:30,926 --> 00:14:35,122
and the advantage is that it removes the data conflicts across

222
00:14:35,176 --> 00:14:38,726
the regions at a cost of added write latency on the

223
00:14:38,748 --> 00:14:41,926
secondary regions, but that has minor relevance to

224
00:14:41,948 --> 00:14:45,382
our use case. Our last step is to ensure data

225
00:14:45,436 --> 00:14:48,826
consistency across the data persistence layers of

226
00:14:48,848 --> 00:14:52,234
the application's microservices, considering our use

227
00:14:52,272 --> 00:14:56,054
based dependency on third party services and on premise

228
00:14:56,102 --> 00:14:59,890
components for that, I will introduce the saga pattern.

229
00:14:59,990 --> 00:15:04,094
This is a failure management pattern to coordinate transactions between

230
00:15:04,212 --> 00:15:07,706
multiple microservices. In AWS. The saga

231
00:15:07,738 --> 00:15:10,842
pattern is typically implemented with AWS

232
00:15:10,906 --> 00:15:14,434
step functions. This is a serverless orchestration service

233
00:15:14,552 --> 00:15:18,066
that lets you combine and sequence lambda functions and

234
00:15:18,088 --> 00:15:21,918
other AWS services to build business critical applications.

235
00:15:22,014 --> 00:15:25,874
For example, for a shopping cart checkout operation, the application will

236
00:15:25,912 --> 00:15:29,858
first need to pre authorize the credit card that is successful.

237
00:15:29,954 --> 00:15:33,718
Then the application will actually charge the credit card,

238
00:15:33,804 --> 00:15:37,494
and only if that is successful will the customer information be

239
00:15:37,532 --> 00:15:41,226
updated. So by using step functions, each step can

240
00:15:41,248 --> 00:15:45,030
follow the single responsibility principle. While all of the plumbing

241
00:15:45,110 --> 00:15:48,634
and the failure management and the orchestration logic are

242
00:15:48,672 --> 00:15:52,298
kept separated, going back to our use based architecture

243
00:15:52,394 --> 00:15:56,350
where we have the lambda functions performing the mutations step

244
00:15:56,420 --> 00:16:00,234
functions will now take their place. As for asynchronous migrations,

245
00:16:00,362 --> 00:16:04,562
customers can leverage Amazon Eventdriven. This is a serverless event

246
00:16:04,616 --> 00:16:08,158
bus for building event driven applications at scale,

247
00:16:08,254 --> 00:16:12,110
directly integrating with over 130 event sources

248
00:16:12,190 --> 00:16:15,966
and over 35 targets, and it's now time to bring

249
00:16:16,008 --> 00:16:19,734
it all together into our final architectures. So in this

250
00:16:19,772 --> 00:16:23,654
application, route 53 plus cloud from will

251
00:16:23,692 --> 00:16:27,506
route the requests from the edge locations to the regions based on

252
00:16:27,548 --> 00:16:31,014
latency. Appsync will then query and subscribe

253
00:16:31,142 --> 00:16:33,638
the normalized data from Dynamodb.

254
00:16:33,734 --> 00:16:37,942
Applying will also orchestrate the mutations, but only on the primary

255
00:16:38,006 --> 00:16:42,554
regions. An event bridge will serve the event driven asynchronous

256
00:16:42,602 --> 00:16:46,282
mutations. Finally, the data layer will be automatically

257
00:16:46,346 --> 00:16:50,206
synchronized between the regions. Upon a

258
00:16:50,308 --> 00:16:54,190
secondary region failure route 53, health checks

259
00:16:54,270 --> 00:16:58,114
will divert traffic automatically to other regions, so the

260
00:16:58,152 --> 00:17:01,374
affected users within the lost secondary

261
00:17:01,422 --> 00:17:04,830
region will only notice an additional network latency

262
00:17:04,910 --> 00:17:08,502
and when that affected secondary region recovers, data will be

263
00:17:08,556 --> 00:17:11,938
automatically resynced and the secondary regions

264
00:17:12,034 --> 00:17:14,950
will then be able to resume services the users.

265
00:17:15,290 --> 00:17:18,806
On the other hand, upon a primary regions failure,

266
00:17:18,918 --> 00:17:22,854
the application will enter a readonly mode. If the expected

267
00:17:22,902 --> 00:17:26,506
duration of the outage is acceptable by the business, then the

268
00:17:26,528 --> 00:17:30,370
application may continue to run with limited functionalities

269
00:17:30,470 --> 00:17:34,762
until the primary region recovers. Otherwise, a secondary region

270
00:17:34,826 --> 00:17:38,570
should be promoted to primary at this point by activating

271
00:17:38,650 --> 00:17:42,682
the failover or the disaster recovery procedures. This reference

272
00:17:42,746 --> 00:17:46,586
architecture is publicly available on the AWS Architecture

273
00:17:46,618 --> 00:17:49,754
center. The PDF version includes further details

274
00:17:49,802 --> 00:17:53,470
and you can download it by scanning the QR code on the bottom

275
00:17:53,540 --> 00:17:57,282
left corner of this. Slide it finally, there are some additional

276
00:17:57,346 --> 00:18:00,690
resources around multi regional architectures at AWS

277
00:18:00,770 --> 00:18:04,086
for you to explore. Take a look at these links and I hope you

278
00:18:04,108 --> 00:18:06,180
have enjoyed this session. Thank you for your time.

