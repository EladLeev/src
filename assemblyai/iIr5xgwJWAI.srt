1
00:00:00,250 --> 00:00:01,630
Are you an SRE,

2
00:00:03,570 --> 00:00:07,358
a developer, a quality

3
00:00:07,444 --> 00:00:11,162
engineer who wants to tackle the challenge of improving reliability

4
00:00:11,226 --> 00:00:14,970
in your DevOps? You can enable your DevOps for reliability

5
00:00:15,050 --> 00:00:19,114
with chaos native. Create your free account at Chaos

6
00:00:19,162 --> 00:01:17,270
native Litmus Cloud hello

7
00:01:17,340 --> 00:01:21,010
and welcome to this talk about shift left, your performance testing.

8
00:01:21,170 --> 00:01:25,046
My name is Hari Krishnan. I'm a consultant and a coach. I help companies with

9
00:01:25,068 --> 00:01:28,390
cloud transformation, exchange programming, agile and lean.

10
00:01:28,810 --> 00:01:32,058
My interests include distributed systems and high performance application

11
00:01:32,144 --> 00:01:35,674
architecture. Below are some of the conferences that I've spoken at.

12
00:01:35,792 --> 00:01:39,434
So let's quickly get started with the talk. Before we

13
00:01:39,472 --> 00:01:43,386
jump into the topic about why we need to shift left in performance

14
00:01:43,418 --> 00:01:46,960
testing, let's understand the context. So, a quick show of hands

15
00:01:47,330 --> 00:01:50,638
in which environment you identify most of your performance issues.

16
00:01:50,804 --> 00:01:54,510
Would that be a local machine? Not likely. Right?

17
00:01:54,660 --> 00:01:58,270
What about development environment? I haven't seen many teams identify,

18
00:01:58,350 --> 00:02:01,842
but if you are, that's great. Most of the performance issues,

19
00:02:01,976 --> 00:02:05,134
at least what we start identifying within the staging environment,

20
00:02:05,182 --> 00:02:08,998
because that's where we can at least generate some load against the particular

21
00:02:09,084 --> 00:02:12,214
environment. And lastly, for the teams that I have worked

22
00:02:12,252 --> 00:02:15,574
with, a big part of the performance issues are

23
00:02:15,612 --> 00:02:19,446
identified in the production replica because that's where you have the infrastructure

24
00:02:19,478 --> 00:02:23,206
that's close to production. And whatever we are able to reproduce

25
00:02:23,238 --> 00:02:26,906
or generate as a load is something that

26
00:02:26,928 --> 00:02:30,410
is representative of the real environment. And for those issues

27
00:02:30,480 --> 00:02:34,426
which we cannot identify up till this point, our users

28
00:02:34,458 --> 00:02:37,998
identify it for us and then we have to fix them. The intensity of

29
00:02:38,004 --> 00:02:41,674
the red color in this particular diagram here kind of represents

30
00:02:41,722 --> 00:02:45,266
like the darker the shade of red, the longer it takes to fix the

31
00:02:45,288 --> 00:02:47,970
issue, and that's not desirable.

32
00:02:48,310 --> 00:02:51,886
So let's understand first what is the current setup

33
00:02:51,918 --> 00:02:56,418
of performance testing and why we have that issue.

34
00:02:56,584 --> 00:02:59,874
So usually the developer starts writing code on the machine

35
00:02:59,922 --> 00:03:03,650
and then starts pushing it to the development environment and eventually

36
00:03:03,730 --> 00:03:07,462
makes its way into staging. And that's probably the first point in time

37
00:03:07,516 --> 00:03:10,874
when the performance testing comes in and has

38
00:03:10,912 --> 00:03:13,830
to set up the performance testing environment,

39
00:03:13,990 --> 00:03:17,466
write the test script, set up the load generator, the agents and

40
00:03:17,488 --> 00:03:20,874
whatnot, send the load to the staging environment and maybe

41
00:03:20,912 --> 00:03:24,366
even to the production environment, generate the report

42
00:03:24,468 --> 00:03:27,886
out of the test run and then ultimately share it back with

43
00:03:27,908 --> 00:03:32,154
the developer. Developer makes sense out of it and then incorporates

44
00:03:32,202 --> 00:03:36,318
the feedback into the code base. So that's the usual cycle.

45
00:03:36,494 --> 00:03:40,626
So what's wrong with this setup? The first problem itself is

46
00:03:40,648 --> 00:03:43,986
the fact that if we are identifying the issues as late as staging of

47
00:03:44,008 --> 00:03:47,086
production, it's already quite late in the cycle.

48
00:03:47,198 --> 00:03:50,754
Right? And what's worse is if we

49
00:03:50,792 --> 00:03:54,406
identify the issue as staging of production, then it

50
00:03:54,428 --> 00:03:57,302
takes an equal amount of time to again fix the issue,

51
00:03:57,356 --> 00:04:00,866
iterate over it, and then bring it all the way from local machine

52
00:04:00,898 --> 00:04:03,482
to development to staging. And that's not it,

53
00:04:03,536 --> 00:04:07,306
right? The higher environments are highly contested. I'm not the

54
00:04:07,328 --> 00:04:10,730
only developer who's trying to fix

55
00:04:10,800 --> 00:04:14,714
performance issues or working on trying to see

56
00:04:14,752 --> 00:04:18,126
if Pyfix is even working. There are other developers who would like to verify it

57
00:04:18,148 --> 00:04:20,480
also. So the higher environments get.

58
00:04:21,330 --> 00:04:24,938
There's a lot of timesharing going on there,

59
00:04:24,964 --> 00:04:28,702
and we tend to have a difficult time trying to identify

60
00:04:28,846 --> 00:04:32,066
which feature we need to test or which issue can we put it

61
00:04:32,088 --> 00:04:35,060
through its basis? So that's the difficulty there.

62
00:04:36,070 --> 00:04:39,714
And that's what leads to this kind of a graph

63
00:04:39,762 --> 00:04:42,834
where we identify most of the issues very late in the cycle,

64
00:04:42,962 --> 00:04:46,214
like leave it up to the users to find it, what should it look like?

65
00:04:46,252 --> 00:04:49,654
Instead, we'd like to identify most of the issues

66
00:04:49,692 --> 00:04:52,946
on our local machine. And then there are those issues which we cannot

67
00:04:52,978 --> 00:04:57,146
discount. Some of them have to be identified, or we

68
00:04:57,168 --> 00:05:00,794
can only test them or figure them out in the higher environment because

69
00:05:00,832 --> 00:05:04,266
of the nature of the issue itself. But largely we'd like to identify as

70
00:05:04,288 --> 00:05:07,566
much as possible on our local machine or towards the left hand side. That's what

71
00:05:07,588 --> 00:05:10,942
we mean by shift. Left here should be easy,

72
00:05:10,996 --> 00:05:14,338
right? The performance testing already has a

73
00:05:14,424 --> 00:05:18,306
robust setup, and as a developer, all one needs to

74
00:05:18,328 --> 00:05:21,806
do is wear the performance testing hat, borrow the performance testing

75
00:05:21,838 --> 00:05:25,362
setup from them, and start sending the load to

76
00:05:25,416 --> 00:05:29,682
local machine or development environment. And we can start depending less

77
00:05:29,736 --> 00:05:33,270
on the performance system and make their lives also a little easier.

78
00:05:33,850 --> 00:05:37,382
This is easier said than done. There are multiple issues,

79
00:05:37,516 --> 00:05:41,198
or rather challenges, in applying shiftlift

80
00:05:41,314 --> 00:05:45,500
to performance testing. Let's understand those.

81
00:05:45,950 --> 00:05:49,226
The first issue itself is a scale challenge. So how

82
00:05:49,248 --> 00:05:53,610
do you truly create a truly representative environment

83
00:05:53,770 --> 00:05:57,598
on your local machine or even development environment for

84
00:05:57,764 --> 00:06:00,270
something that's going to be running in production?

85
00:06:01,010 --> 00:06:04,610
Production environments are usually like sophisticated.

86
00:06:05,430 --> 00:06:10,222
You have a hybrid cloud, you have sophisticated

87
00:06:10,286 --> 00:06:14,254
distributed systems and microservice architectures and load balancing and whatnot.

88
00:06:14,382 --> 00:06:18,034
Production replica also seems to mimic a lot of it as

89
00:06:18,072 --> 00:06:21,494
you come down. The environments and staging has a little

90
00:06:21,532 --> 00:06:25,314
lesser complexity, and ultimately on the local machine, it's just a humble

91
00:06:25,362 --> 00:06:29,186
laptop. Now, how do I say that I have verified the performance

92
00:06:29,218 --> 00:06:32,554
characteristics of an application on my local machine, and so

93
00:06:32,592 --> 00:06:35,850
I'm confident that it's going to work on production? Or how do I even

94
00:06:36,000 --> 00:06:40,186
take a problem that I'm identifying on the production environment and bring it down to

95
00:06:40,288 --> 00:06:43,646
my local machine and even replicate it. Quite hard to

96
00:06:43,668 --> 00:06:46,874
do that, right? Second is obviously the network

97
00:06:46,922 --> 00:06:51,194
topology itself. Production environments might have a lot more complicated

98
00:06:51,322 --> 00:06:55,070
firewalls and whatnot. The latency levels, the proxies

99
00:06:55,150 --> 00:06:58,482
and practically everything that's involved there is

100
00:06:58,536 --> 00:07:01,934
a lot more complex and there's not much you can replicate.

101
00:07:01,982 --> 00:07:05,366
I mean you can try, but there's only to an extent that we

102
00:07:05,388 --> 00:07:09,350
can do all of that fancy networking on a local machine.

103
00:07:10,970 --> 00:07:14,870
And let's leaving the application aside, the performance setup itself

104
00:07:14,940 --> 00:07:17,826
tend to become quite complicated.

105
00:07:17,938 --> 00:07:21,394
The higher environments have like you need multiple machines to generate

106
00:07:21,442 --> 00:07:25,206
a sufficient load, and it's a fairly sophisticated setup,

107
00:07:25,238 --> 00:07:29,226
and you start stuffing that into lower environments, that is hard in

108
00:07:29,248 --> 00:07:32,866
itself, and you try to push that into my local machine

109
00:07:32,918 --> 00:07:36,750
as a developer, and this is what I have to deal with.

110
00:07:36,820 --> 00:07:40,266
This is my eightgB MacBook Pro and the memory pressure I'm dealing

111
00:07:40,298 --> 00:07:43,860
with while running Catlink test, that's not desirable now.

112
00:07:45,430 --> 00:07:49,010
All right, so how do we solve this? So, shift left

113
00:07:49,080 --> 00:07:52,482
equals scale down. You can't go to C trial for every

114
00:07:52,536 --> 00:07:56,254
single design change, right? That wouldn't be practical.

115
00:07:56,382 --> 00:08:00,726
We have to figure out a way to scale down the problem and try

116
00:08:00,748 --> 00:08:04,120
to figure it out on our local machines or on the left hand side.

117
00:08:04,970 --> 00:08:08,454
But that's not fairly straightforward, is it? So let's take

118
00:08:08,492 --> 00:08:12,550
two parameters and see if we can scale down a performance

119
00:08:12,630 --> 00:08:16,214
issue. So let's say I just want to deal with request

120
00:08:16,262 --> 00:08:19,654
per second and response time, throughput and latency.

121
00:08:19,782 --> 00:08:23,406
Very popular two metrics that we would like to figure out and

122
00:08:23,428 --> 00:08:27,342
how to scale it down. So if I have x and y as the two

123
00:08:27,396 --> 00:08:31,230
KPIs for my production, now do I scale it down

124
00:08:31,380 --> 00:08:35,430
with aspect ratio, or do I just say I'll maintain

125
00:08:35,450 --> 00:08:38,962
my RPS, but I will probably discount a little bit on response time?

126
00:08:39,096 --> 00:08:42,434
Or do I do something else? Like what is the accurate way

127
00:08:42,472 --> 00:08:46,790
to scale down and still validate the performance KPIs

128
00:08:47,450 --> 00:08:49,590
in a truly representative manner?

129
00:08:50,410 --> 00:08:54,840
The reality is we cannot. So what do we do about it?

130
00:08:55,370 --> 00:08:58,586
Even if we cannot scale down the real problem, we can scale down

131
00:08:58,608 --> 00:09:01,530
the trend and invalidate hypotheses.

132
00:09:03,150 --> 00:09:06,330
So since I spoken about hypotheses, let's understand

133
00:09:06,400 --> 00:09:09,958
what hypotheses invalidation is. So hypotheses,

134
00:09:10,054 --> 00:09:13,822
as our old school lessons have taught us, like you

135
00:09:13,876 --> 00:09:17,786
have to make a statement, and then there is a verifiability aspect

136
00:09:17,818 --> 00:09:21,054
to it and a falsifiability aspect to it. So for example, I could

137
00:09:21,092 --> 00:09:25,694
say all green apples are sour so

138
00:09:25,732 --> 00:09:29,442
in order to prove that statement, there are two ways. One, I can go eat

139
00:09:29,496 --> 00:09:32,674
all the green apples in this world and say, not even one of them

140
00:09:32,712 --> 00:09:36,558
is sweet. So all of them are Sar. So that's the verifiability

141
00:09:36,654 --> 00:09:40,326
aspect of it. Right now, that's not very practical for me to eat

142
00:09:40,348 --> 00:09:44,022
all the green apples in this world. So the falsifiability aspect is,

143
00:09:44,076 --> 00:09:47,862
even if I identify one green apple, that's sweet. I've potentially

144
00:09:47,926 --> 00:09:51,866
proven like the entire hypotheses wrong. So that's the

145
00:09:52,048 --> 00:09:55,706
hypotheses invalidation aspect. Now how

146
00:09:55,728 --> 00:09:59,574
does that apply to software engineering and particularly to performance

147
00:09:59,622 --> 00:10:02,320
testing? Let's take a look.

148
00:10:03,970 --> 00:10:07,626
So recently I was tasked with a somewhat

149
00:10:07,658 --> 00:10:11,454
similar problem wherein as you can see on the x axis, I have

150
00:10:11,572 --> 00:10:15,822
the throughput. As the throughput increases exponentially,

151
00:10:15,966 --> 00:10:19,726
I want my response time to degrade only at a logarithmic scale.

152
00:10:19,758 --> 00:10:23,470
So this is what was expected of the system behavior,

153
00:10:23,630 --> 00:10:27,250
and they wanted me to figure out if this

154
00:10:27,320 --> 00:10:30,786
has been satisfied by the application. Now, how do I scale

155
00:10:30,818 --> 00:10:34,326
this problem down? It's very hard, right? Because 10,000 RpS is

156
00:10:34,348 --> 00:10:37,618
not something I can figure out on my local machine. Not practical.

157
00:10:37,794 --> 00:10:41,174
But what can I do? Like I said, I can scale

158
00:10:41,222 --> 00:10:44,698
down the trend. The trend here, the clue itself

159
00:10:44,784 --> 00:10:49,110
was in the problem, which is the exponential increase in throughput

160
00:10:49,190 --> 00:10:53,026
and a degradation of the response time. Only in logarithmics

161
00:10:53,078 --> 00:10:57,054
case I can take that and scale that entire problem down and

162
00:10:57,092 --> 00:10:59,886
I just figure it out with one RPS ten,

163
00:10:59,908 --> 00:11:02,842
900. Now when I tried it on my machine,

164
00:11:02,906 --> 00:11:05,780
I saw that that's not possibly true.

165
00:11:06,310 --> 00:11:09,698
The latency was in fact degrading at an

166
00:11:09,704 --> 00:11:13,154
exponential scale. This is not desirable. So what I can then do is very

167
00:11:13,192 --> 00:11:16,786
quickly realize that I have experiments, so I have disproved the

168
00:11:16,808 --> 00:11:20,470
hypothesis does not fit with the current application code,

169
00:11:20,620 --> 00:11:23,986
and I don't need to go testing on the higher environment.

170
00:11:24,098 --> 00:11:27,446
I can quickly apply a fix on my local machine, see if that's working,

171
00:11:27,628 --> 00:11:30,986
and then only then, if that works,

172
00:11:31,088 --> 00:11:34,890
then I can push it to the higher environment. Let's say I made that fix

173
00:11:35,040 --> 00:11:38,406
and I even got a better performance than what was expected.

174
00:11:38,518 --> 00:11:42,234
Maybe I'm even tracking very much under

175
00:11:42,272 --> 00:11:45,738
the logarithmic scale. But is this sufficient for me to say that it's

176
00:11:45,754 --> 00:11:49,562
going to work on the higher environment? No, it's inconclusive,

177
00:11:49,626 --> 00:11:53,354
because on my local machine I have proven it at a lower scale.

178
00:11:53,482 --> 00:11:57,202
But that does not mean it potentially will

179
00:11:57,256 --> 00:12:00,754
work in the higher environment since it is inconclusive. Now this

180
00:12:00,792 --> 00:12:04,718
part of it, I have to go with the verifiability experimentation.

181
00:12:04,814 --> 00:12:08,030
So I validate it in the higher environment. And I realize

182
00:12:08,110 --> 00:12:11,366
in fact what I found on my local machine does told good for some more

183
00:12:11,388 --> 00:12:15,254
time. But after 1000 rps it tends to fall off. So now

184
00:12:15,292 --> 00:12:18,886
I need to again identify what's going on beyond 1000 rps and why

185
00:12:18,908 --> 00:12:22,246
is it falling off. And for that I come back to my local machine,

186
00:12:22,278 --> 00:12:25,834
I figure out a way to replicate the issue, and then I can

187
00:12:25,872 --> 00:12:29,258
fix it on my local machine, verify it in the left hand side,

188
00:12:29,344 --> 00:12:32,122
and then move to the right. Now this is not all bad,

189
00:12:32,176 --> 00:12:35,758
right? The first part, like wherein I identified the issue at

190
00:12:35,764 --> 00:12:39,198
the very lower scale itself. The one fix, I could do it

191
00:12:39,204 --> 00:12:41,946
on my local machine itself. I didn't have to go to the higher environment,

192
00:12:42,058 --> 00:12:45,390
I only had to go to the higher environment where it was absolutely necessary.

193
00:12:45,470 --> 00:12:49,042
So I did save that one point

194
00:12:49,096 --> 00:12:51,650
where I had to depend on the higher environment.

195
00:12:52,470 --> 00:12:56,562
Now, as you can see on the left hand side we're learning through falsifiability,

196
00:12:56,706 --> 00:13:00,594
and on the right we are confirming our learning through verifiability.

197
00:13:00,722 --> 00:13:03,160
So that's the beauty of this whole idea.

198
00:13:04,250 --> 00:13:08,226
Now there is one more example of this scaling

199
00:13:08,258 --> 00:13:11,498
down I'd like to share. This is one of my favorites. I call it a

200
00:13:11,504 --> 00:13:14,422
with and without experiment, or the AB experiment.

201
00:13:14,566 --> 00:13:17,658
So again, the current characteristics of

202
00:13:17,664 --> 00:13:20,942
a system I was dealing with was, I told, hit 10,000

203
00:13:20,996 --> 00:13:24,794
rps at 80% cpu utilization.

204
00:13:24,922 --> 00:13:28,526
What was expected is once we introduce a cache, I'd be able

205
00:13:28,548 --> 00:13:31,994
to hit 1 million rps at the same cpu utilization.

206
00:13:32,122 --> 00:13:35,338
Fairly ambitious, but let's see if we can scale

207
00:13:35,354 --> 00:13:39,006
this down. Now, 10,000 rps itself was

208
00:13:39,028 --> 00:13:42,706
out of bounds for me, 1 million rps is really, really hard.

209
00:13:42,888 --> 00:13:45,246
How do I figure this out on my local machine?

210
00:13:45,438 --> 00:13:48,886
Now the interesting piece here is I

211
00:13:48,908 --> 00:13:52,562
don't really need to worry about 10,000, 1 million and whatnot.

212
00:13:52,706 --> 00:13:56,006
What can I do? On my left hand side, let's say I

213
00:13:56,108 --> 00:13:59,894
at some max CPU, let's not even say 80% at some max

214
00:13:59,942 --> 00:14:03,366
CPU I have XRPs without cache.

215
00:14:03,478 --> 00:14:07,606
And at the same cpu level on my local machine

216
00:14:07,718 --> 00:14:11,766
with the cache, I have yrps. The only pattern

217
00:14:11,798 --> 00:14:15,374
that matters to me is the fact that X is less than y.

218
00:14:15,492 --> 00:14:19,326
It doesn't matter what xrps is, it doesn't matter what Y RPS is. But the

219
00:14:19,348 --> 00:14:22,718
fact that the cash does make a difference is what is important.

220
00:14:22,884 --> 00:14:26,270
Because if we don't do this, I've seen many times that

221
00:14:26,420 --> 00:14:29,666
the cash is just there for moral support really doesn't make

222
00:14:29,768 --> 00:14:33,858
any discernible difference. So for me, this was really

223
00:14:34,024 --> 00:14:37,586
an important eye opening experiment to

224
00:14:37,608 --> 00:14:41,174
run on my local machine. To say the cache makes

225
00:14:41,212 --> 00:14:44,406
a difference, in fact, it's working and then I can take it to

226
00:14:44,428 --> 00:14:47,270
the higher environment. I don't need to go to the higher environment to realize that

227
00:14:47,340 --> 00:14:50,602
my cache setting sensors are not working. So I could save that

228
00:14:50,656 --> 00:14:53,930
one aspect. So that's the with and without experiment.

229
00:14:55,470 --> 00:14:58,778
Let's get to the next challenge now with shifting left.

230
00:14:58,944 --> 00:15:02,446
Now, the problem with trying to run performance tests within your

231
00:15:02,468 --> 00:15:05,694
sprints cycle, take the typical sprint right,

232
00:15:05,732 --> 00:15:09,086
like the developer starts writing code for feature one and she's done

233
00:15:09,108 --> 00:15:12,106
with it. And based on what we've already spoken,

234
00:15:12,218 --> 00:15:15,658
we know that performance testing is a time consuming activity. So we don't

235
00:15:15,674 --> 00:15:19,586
want to do it right within the sprint, so it becomes difficult. So we

236
00:15:19,688 --> 00:15:23,330
create a task out of it and we hand it over to the performance system

237
00:15:23,400 --> 00:15:27,362
for the person to look into it and figure out,

238
00:15:27,416 --> 00:15:30,886
put the issue through, put the feature through its paces and come back with some

239
00:15:30,908 --> 00:15:34,854
feedback. Meanwhile, we churn out feature number two and

240
00:15:34,892 --> 00:15:38,066
then hand that also to the performance tester.

241
00:15:38,178 --> 00:15:41,466
Now, based on what we have done in the previous sprint two,

242
00:15:41,488 --> 00:15:44,774
we start churning out feature three. And that's

243
00:15:44,902 --> 00:15:48,406
the first point when the performance tester comes back saying, hey, that feature

244
00:15:48,438 --> 00:15:52,560
one which you developed, I found a few performance issues. It's not really

245
00:15:52,930 --> 00:15:57,166
confirming with the NFRs that were expected of it.

246
00:15:57,348 --> 00:16:01,502
However, we are in the middle of the sprint, we cannot change stuff. So we

247
00:16:01,636 --> 00:16:05,730
press on with feature number four and by then the performance system

248
00:16:05,800 --> 00:16:08,660
comes back with more feedback for feature two.

249
00:16:09,110 --> 00:16:13,330
Now what happens? Print number three, a developer,

250
00:16:13,670 --> 00:16:17,346
she has to sit through and look at the issues

251
00:16:17,528 --> 00:16:21,206
that have been identified, performance issues that have been identified on

252
00:16:21,228 --> 00:16:24,626
feature one, feature two, and meanwhile a performance

253
00:16:24,658 --> 00:16:28,954
tester is putting feature through three

254
00:16:29,072 --> 00:16:33,226
through its faces. So practically what has happened now is

255
00:16:33,408 --> 00:16:37,514
sprints three is a complete washout. And this is a

256
00:16:37,552 --> 00:16:41,050
very typical anti pattern which we call the hardening sprint,

257
00:16:41,390 --> 00:16:45,598
wherein practically we have not done any meaningful work. We're just fixing issues

258
00:16:45,764 --> 00:16:49,626
from some of the previous sprints. So this is not desirable.

259
00:16:49,738 --> 00:16:52,480
And this is not something new either.

260
00:16:52,930 --> 00:16:56,382
Every time we call a feature done before it's actually

261
00:16:56,436 --> 00:16:59,726
fully done, this is what happens. We don't complete the testing

262
00:16:59,758 --> 00:17:02,066
and then we call the feature done and move on to the next one,

263
00:17:02,168 --> 00:17:05,970
whereas the performance testing actually came back with some issues

264
00:17:06,040 --> 00:17:09,558
that were identified and we had to fix it. How do

265
00:17:09,564 --> 00:17:13,446
we solve this? The better way for

266
00:17:13,468 --> 00:17:17,094
us to do this is to collaboratively look

267
00:17:17,132 --> 00:17:21,030
into the problem. The developer and the performance system work together, finish the feature,

268
00:17:21,190 --> 00:17:24,858
pull us through its spaces, and then identify the issues,

269
00:17:24,944 --> 00:17:28,758
fix those issues and then fix the performance issues. And that's

270
00:17:28,854 --> 00:17:32,266
like we have to come to terms with the reality of it. It does

271
00:17:32,288 --> 00:17:35,830
take two sprints, and practically this is a

272
00:17:35,840 --> 00:17:39,278
best case scenario that we identified issues and we fixed it in the

273
00:17:39,284 --> 00:17:42,218
very first iteration and that itself took two sprints.

274
00:17:42,314 --> 00:17:47,202
Now, obviously this is not desirable. We'd like to become

275
00:17:47,256 --> 00:17:50,642
a lot more efficient at churning our features, right? So the better

276
00:17:50,696 --> 00:17:54,846
way to look at this problem, instead of siphoning off performance

277
00:17:54,878 --> 00:17:58,046
testing, is to reduce the longest testing

278
00:17:58,078 --> 00:18:01,234
itself. How do we do that? We reduce effort, we reduce

279
00:18:01,282 --> 00:18:04,706
complexity, we reduce repetition, and we have to automate

280
00:18:04,738 --> 00:18:08,806
all the way. And that's the only way we can solve this problem. So let's

281
00:18:08,838 --> 00:18:12,380
look at how some of those aspects we took and

282
00:18:13,070 --> 00:18:16,906
incorporated into our daily activities. So let's look

283
00:18:16,928 --> 00:18:20,778
at repetition. So developer writes

284
00:18:20,794 --> 00:18:24,938
application code. Performance testing is responsible for the performance testing setup.

285
00:18:25,114 --> 00:18:29,146
Developer writes API tests and Performance tester writes

286
00:18:29,338 --> 00:18:33,290
performance test script. API tests

287
00:18:33,450 --> 00:18:36,474
generate requests. Performance testing also generate requests.

288
00:18:36,602 --> 00:18:40,974
Very similar. API test assert response performance

289
00:18:41,022 --> 00:18:44,114
is not so much. They assert more

290
00:18:44,152 --> 00:18:48,066
on the response time. As long as the request itself is

291
00:18:48,248 --> 00:18:52,326
coming back positively, that's all we care about in performance testing.

292
00:18:52,348 --> 00:18:55,778
If you think about it, the API tests and performance test scripts

293
00:18:55,794 --> 00:18:59,000
are practically duplicated across every time.

294
00:19:00,090 --> 00:19:03,206
One of the examples I'd like to take here is one of the teams

295
00:19:03,238 --> 00:19:06,662
I was working with, a developer was writing API tests in karate,

296
00:19:06,726 --> 00:19:10,886
and the performance test was. Tester was writing the perf test script in Kala

297
00:19:10,918 --> 00:19:13,674
with Katlin. So obviously that was repeat effort.

298
00:19:13,802 --> 00:19:17,966
And wherever there is duplication, there is inconsistency because

299
00:19:17,988 --> 00:19:21,806
we are repeating it and we don't know whether the performance test script is in

300
00:19:21,828 --> 00:19:24,734
line with what the API tests are doing and what the feature really is.

301
00:19:24,852 --> 00:19:28,866
So there's that inconsistency, and then there is a disconnect. The developer is

302
00:19:28,888 --> 00:19:32,062
developing the feature and the API test, and the processor

303
00:19:32,126 --> 00:19:36,038
probably is not privy to all the details of it. So maybe

304
00:19:36,204 --> 00:19:39,490
if we do not understand the intricacies of the architecture,

305
00:19:39,570 --> 00:19:43,314
a performance system may not necessarily identify loopholes

306
00:19:43,362 --> 00:19:47,000
which the developer could have. So how do we solve this?

307
00:19:47,930 --> 00:19:51,194
Shift left equals repurpose. Take a perfectly good road.

308
00:19:51,232 --> 00:19:54,762
Can you chop it up? You remove all the unnecessary stuff,

309
00:19:54,896 --> 00:19:58,474
put in a big engine and go rallying. Very much

310
00:19:58,512 --> 00:20:02,510
the same with API test, right? We can reuse that as per test.

311
00:20:02,660 --> 00:20:04,960
That's what essentially we ended up doing.

312
00:20:05,890 --> 00:20:09,294
This helps with reduced maintenance because there is

313
00:20:09,332 --> 00:20:12,618
no inconsistency. The perf test will not go

314
00:20:12,804 --> 00:20:16,658
out of line with the API test.

315
00:20:16,824 --> 00:20:20,254
And also from the point of view of collaboration,

316
00:20:20,302 --> 00:20:24,034
it promotes better working style within

317
00:20:24,072 --> 00:20:28,306
the team because the first tester is now leveraging the API test that the developer

318
00:20:28,338 --> 00:20:32,166
has written and they both can collaborate on the system design for the

319
00:20:32,188 --> 00:20:36,754
purpose itself, reducing complexity.

320
00:20:36,882 --> 00:20:40,814
Now this is a hard one. Performance testing told

321
00:20:40,962 --> 00:20:44,586
have come a long way in the recent past, and there are a

322
00:20:44,608 --> 00:20:48,346
ton of tools out there, all of which are almost great in

323
00:20:48,368 --> 00:20:52,154
their own way, and we are at a loss for often choosing which

324
00:20:52,192 --> 00:20:55,774
is the best tool for the job, right? So that itself is like quite

325
00:20:55,812 --> 00:20:59,582
the choice among so many great told. And then once you have

326
00:20:59,636 --> 00:21:03,386
chosen your tool, then comes the metrics. It's not useful

327
00:21:03,418 --> 00:21:07,186
to just have a test performance test report

328
00:21:07,288 --> 00:21:11,086
without the application metrics, right? So we'd like to put the application metrics

329
00:21:11,118 --> 00:21:15,138
and the performance test results in one metric store and visualize it.

330
00:21:15,224 --> 00:21:18,446
And talking of visualization, we have Kibana,

331
00:21:18,478 --> 00:21:21,686
Grafana and the likes. So that's a lot more tools in

332
00:21:21,708 --> 00:21:25,542
the mix. And ultimately the custom code

333
00:21:25,676 --> 00:21:30,154
which has to come in to put all this tooling infrastructure and orchestration together.

334
00:21:30,352 --> 00:21:33,850
Now this is a fairly sizable piece of work and

335
00:21:33,920 --> 00:21:37,750
add to the costing, licensing and preferences

336
00:21:37,830 --> 00:21:40,630
of the team and the performancerelated whatnot.

337
00:21:40,790 --> 00:21:43,770
Now, based on multiple other factors,

338
00:21:43,850 --> 00:21:47,294
ultimately we end up arriving at one stack and

339
00:21:47,332 --> 00:21:50,894
the stack is fairly unique for almost each individual team and even

340
00:21:50,932 --> 00:21:54,686
within a company. So that's a lot of complexity and

341
00:21:54,788 --> 00:21:58,174
repeat investment into getting a post testing

342
00:21:58,222 --> 00:22:01,938
going. So that's hard enough for

343
00:22:01,944 --> 00:22:05,598
the performance testing to handle and set it up on the staging environment.

344
00:22:05,694 --> 00:22:09,490
If you hand it to a developer and say hey, stuff that into your laptop,

345
00:22:09,570 --> 00:22:12,630
that's not going to be very easy,

346
00:22:12,700 --> 00:22:16,706
right? Because as a developer I'd be at loss. I don't

347
00:22:16,738 --> 00:22:20,150
understand many of these tools. I'm a newbie to these tools.

348
00:22:20,230 --> 00:22:23,260
How do I install these? It's going to be quite difficult.

349
00:22:24,510 --> 00:22:27,740
So whenever there is this kind of fragmentation situation,

350
00:22:28,190 --> 00:22:31,130
a good answer is containerizing the problem,

351
00:22:31,200 --> 00:22:34,878
right? That's exactly what we ended up doing. So we

352
00:22:34,964 --> 00:22:38,826
containerized the performance testing setup itself and developed

353
00:22:38,858 --> 00:22:42,478
it more like code like, instead of creating the performance test setup on

354
00:22:42,484 --> 00:22:45,860
the higher environment and then bringing it to the lower environment and

355
00:22:46,470 --> 00:22:49,746
it's not fitting. Rather, we develop the perf test setup on

356
00:22:49,768 --> 00:22:53,522
the local machine and containerize it so that now it

357
00:22:53,576 --> 00:22:57,706
works well on the local machine as well as on the higher environment.

358
00:22:57,838 --> 00:23:01,414
That's the objective. So with

359
00:23:01,452 --> 00:23:05,410
that, I'd like to jump into a quick demo of perfies.

360
00:23:05,570 --> 00:23:09,434
Perfumes is a tool that we came up with

361
00:23:09,472 --> 00:23:13,066
based on some of these principles that we've been speaking about for

362
00:23:13,088 --> 00:23:17,210
some time now. It helps processors and developers collaborate,

363
00:23:17,550 --> 00:23:21,306
wherein the developer can write API tests in karate and the

364
00:23:21,328 --> 00:23:25,450
thing can be leveraged as process through gatling, karate gatling integration

365
00:23:25,610 --> 00:23:29,086
without actually writing scala code. I'll get to that in a bit

366
00:23:29,188 --> 00:23:31,754
and ultimately gather the metrics in prometheus,

367
00:23:31,882 --> 00:23:35,466
visualize the results in grafana

368
00:23:35,498 --> 00:23:38,766
in real time, and the entire orchestration is

369
00:23:38,788 --> 00:23:42,482
handled by coffee and it's dockerized, which means I can

370
00:23:42,536 --> 00:23:45,626
install it on my local machines or higher environment alike.

371
00:23:45,678 --> 00:23:49,558
So that's the whole idea. Now with shift left me jump right into the

372
00:23:49,644 --> 00:23:50,280
demo.

373
00:23:53,210 --> 00:23:56,440
So prose as a tool

374
00:23:57,450 --> 00:24:01,322
and then extracting it to a location of your choice. So I have already

375
00:24:01,376 --> 00:24:05,020
done that. So now all I need to do is

376
00:24:05,470 --> 00:24:09,274
set up the force home environment variable. So I'll do

377
00:24:09,312 --> 00:24:12,278
that. And with that pretty much called forfeit.

378
00:24:12,374 --> 00:24:15,894
Now I need a project which already has a karate

379
00:24:15,942 --> 00:24:19,134
API test which I can leverage as performance test. So for this

380
00:24:19,172 --> 00:24:22,670
purpose, what I'm going to do is I'm going to leverage

381
00:24:23,010 --> 00:24:27,170
the karate demo project which is on karate

382
00:24:27,750 --> 00:24:31,182
GitHub account and I'm going to take one of their feature

383
00:24:31,246 --> 00:24:34,482
files, the karate feature files, and run it as a perf test.

384
00:24:34,616 --> 00:24:37,000
So let's take a look how we can do that.

385
00:24:38,330 --> 00:24:41,542
So I already cloned this application, the karate demo

386
00:24:41,596 --> 00:24:44,694
project. So let me boot it up,

387
00:24:44,812 --> 00:24:46,280
the springboard application.

388
00:24:51,880 --> 00:24:55,592
So while that application is boating up, I'm going to quickly

389
00:24:55,726 --> 00:24:59,192
init our project. So what I'm going to do here is

390
00:24:59,246 --> 00:25:01,720
perfume sh. Init.

391
00:25:04,160 --> 00:25:09,100
So what init has done is essentially created

392
00:25:13,200 --> 00:25:17,112
two extra files, which is the perfume Yaml configuration

393
00:25:17,256 --> 00:25:18,910
which you will see right here.

394
00:25:24,430 --> 00:25:27,758
And there's a folder called perfume which I'll get to in

395
00:25:27,764 --> 00:25:31,338
a bit in terms of what the details are. Now what I'm

396
00:25:31,354 --> 00:25:34,914
going to do is the

397
00:25:34,952 --> 00:25:38,420
feature file. I'm going to replace the contents of it,

398
00:25:40,470 --> 00:25:43,806
sorry, not the feature file, I'm going to replace the content of the puffy Yaml

399
00:25:43,838 --> 00:25:47,918
file. So this is a template which perhaps

400
00:25:47,934 --> 00:25:51,030
has dropped in for us. I'm going to get rid of whatever there is here.

401
00:25:51,180 --> 00:25:54,630
Instead I'm going to copy paste something that I already have and

402
00:25:54,700 --> 00:25:58,102
let me walk you through from the top. So the first

403
00:25:58,156 --> 00:26:01,426
point we have karate features directory. So I need to let

404
00:26:01,468 --> 00:26:04,918
perfume know where the feature file is. Sitting the greeting feature which I'd

405
00:26:04,934 --> 00:26:08,374
like to run as performance testing. So the first parameter,

406
00:26:08,422 --> 00:26:11,834
the feature directory and the location, both of these put together help

407
00:26:11,872 --> 00:26:15,530
perfume locate where the feature file is sitting.

408
00:26:15,610 --> 00:26:19,534
And then I also have this simulation name which

409
00:26:19,572 --> 00:26:23,346
I'm going to call reading because

410
00:26:23,368 --> 00:26:26,706
that's what I'm testing. And ultimately there

411
00:26:26,728 --> 00:26:30,350
is this one other variable called karate mv, which is perfume.

412
00:26:30,510 --> 00:26:34,114
Now why do I need this? Because now the

413
00:26:34,152 --> 00:26:37,262
application, the springboard application is running on my local machine,

414
00:26:37,406 --> 00:26:41,382
whereas perfume is going to be running on Docker, which means

415
00:26:41,436 --> 00:26:44,642
I need to tell perfies to point to my local machine.

416
00:26:44,786 --> 00:26:48,326
That's why I have said whenever the environment is

417
00:26:48,348 --> 00:26:51,878
perfume, the application is running on post docker intern.

418
00:26:51,964 --> 00:26:55,802
So that's the only bit. It could be any other URL. Now with that

419
00:26:55,936 --> 00:26:59,610
pretty much we are ready to get started with the performance register.

420
00:27:00,210 --> 00:27:04,202
The rest of the lines here in this file practically

421
00:27:04,266 --> 00:27:07,710
has a load pattern. And this load pattern is very similar

422
00:27:07,860 --> 00:27:11,626
to the DSL that Gatling exposes through scala.

423
00:27:11,738 --> 00:27:15,306
And like I said, you don't have to write any color code

424
00:27:15,348 --> 00:27:18,578
at all to get this going that you can just

425
00:27:18,664 --> 00:27:21,742
configure the Yaml file and it will practically

426
00:27:21,806 --> 00:27:25,518
do it for you. And the last bit is the URi patterns. I don't need

427
00:27:25,544 --> 00:27:29,094
this for the test, so I'm going to get rid of it. So with

428
00:27:29,132 --> 00:27:33,186
that. So let's meanwhile look at whether the application has booted

429
00:27:33,218 --> 00:27:36,534
up. Yeah, it has.

430
00:27:36,652 --> 00:27:41,050
I'm going to say the springboard application grading

431
00:27:41,470 --> 00:27:45,610
and hello world. So the springboard application is indeed running now.

432
00:27:45,680 --> 00:27:49,354
So now I need to get started with perfume to

433
00:27:49,392 --> 00:27:52,926
run the performance testing. So I have the perfumes yaml. I've done

434
00:27:52,948 --> 00:27:56,526
my init. So what do I do next? Now I

435
00:27:56,548 --> 00:28:01,098
need to start perfuse perfume.

436
00:28:01,194 --> 00:28:04,882
Sh message start. What do I need to do?

437
00:28:04,936 --> 00:28:08,706
Start here, because remember the stack which

438
00:28:08,728 --> 00:28:12,786
I have shown you in the slide deck, this entire stack is

439
00:28:12,808 --> 00:28:16,398
now going to get booted up on my docker.

440
00:28:16,574 --> 00:28:20,170
And for that I need to run profit

441
00:28:20,190 --> 00:28:23,638
start for it to clear up everything

442
00:28:23,724 --> 00:28:26,600
and put the entire startup here. As you can see,

443
00:28:27,450 --> 00:28:31,198
I have my influx three v promise here. Grafana,

444
00:28:31,314 --> 00:28:33,740
the whole Brit is running right here.

445
00:28:34,750 --> 00:28:38,426
Okay, so now that means I am good to

446
00:28:38,448 --> 00:28:41,646
get started with the testing itself. And after it

447
00:28:41,668 --> 00:28:44,846
has booted, it's saying that Grafana is running a local of

448
00:28:44,868 --> 00:28:47,840
3000. Let's go take a look.

449
00:28:51,310 --> 00:28:53,100
Yeah, and it is up.

450
00:28:55,010 --> 00:28:58,378
The default username and password is admin. Admin.

451
00:28:58,474 --> 00:29:01,966
We're not going to change it now. I'll just continue

452
00:29:02,068 --> 00:29:03,600
with that,

453
00:29:10,160 --> 00:29:13,596
and you will see that there is already a default dashboard that Buffy has

454
00:29:13,618 --> 00:29:17,072
dropped in for us. She's obviously empty. There's no data

455
00:29:17,126 --> 00:29:20,396
in there. So I need to run a test in order for it to populate

456
00:29:20,428 --> 00:29:23,940
some data. Now, I'll say puffy sh test.

457
00:29:24,090 --> 00:29:27,828
So when I say test by default, the first parameter it's going

458
00:29:27,834 --> 00:29:32,144
to take is the puffy yaml file. And it's

459
00:29:32,192 --> 00:29:35,536
going to take this file, interpret the load

460
00:29:35,568 --> 00:29:39,768
pattern that I have given here, and also understand

461
00:29:39,854 --> 00:29:43,188
the aspect that I need to run creating

462
00:29:43,284 --> 00:29:46,792
feature as a performance test. And it's going to create a

463
00:29:46,846 --> 00:29:50,296
Gatling simulation out of this and then run that as

464
00:29:50,318 --> 00:29:53,596
a pop test against the springboard application that

465
00:29:53,618 --> 00:29:57,550
we earlier saw. So right now, this whole thing is coating up inside

466
00:29:58,000 --> 00:30:01,250
of Docker. Let me just show you that

467
00:30:01,940 --> 00:30:05,456
you have the docker instance, which is running the

468
00:30:05,478 --> 00:30:08,896
Gatling test inside, and eventually you'll end up seeing all of

469
00:30:08,918 --> 00:30:12,576
this data in

470
00:30:12,598 --> 00:30:16,404
Grafana. So, as you can see already, the bottom two panels have

471
00:30:16,442 --> 00:30:19,984
started showing some data. Now, why these panels

472
00:30:20,032 --> 00:30:23,552
have cpu and memory. This is practically just a demonstration,

473
00:30:23,616 --> 00:30:27,696
the fact that I'm monitoring the perfect infrastructure

474
00:30:27,728 --> 00:30:31,556
here through the advisor, and I am plotting

475
00:30:31,588 --> 00:30:35,592
it as a way to represent how my

476
00:30:35,726 --> 00:30:39,336
performance test setup is behaving in terms of cpu and

477
00:30:39,358 --> 00:30:42,200
memory as the test itself is progressing.

478
00:30:42,280 --> 00:30:46,408
So I'll be able to see that side by side, and eventually you'll

479
00:30:46,424 --> 00:30:49,624
be able to see some data at the top as the test boots

480
00:30:49,672 --> 00:30:50,270
up.

481
00:30:59,840 --> 00:31:03,244
Now, as you can see, Grafana is now showing us

482
00:31:03,282 --> 00:31:07,276
the graph itself in terms of what the load pattern was.

483
00:31:07,458 --> 00:31:10,868
This is very much in line with the test ephemera,

484
00:31:10,904 --> 00:31:14,204
which is one constant user for the first 30 seconds

485
00:31:14,252 --> 00:31:17,596
and then three constant users for the next 15 seconds. That's what's

486
00:31:17,628 --> 00:31:21,216
happening here. They can also get a sense of how we can correlate

487
00:31:21,408 --> 00:31:24,964
this load pattern, what we're seeing on Grafana, along with

488
00:31:25,082 --> 00:31:28,784
the application metrics such as cpu,

489
00:31:28,832 --> 00:31:32,244
memory, et cetera. So that's like a very quick performance

490
00:31:32,292 --> 00:31:35,976
test. In a matter of just 5 minutes, we were able

491
00:31:35,998 --> 00:31:40,356
to convert our karate API test into Gatlink

492
00:31:40,388 --> 00:31:44,396
performance test with perfume. All right, so let's jump back

493
00:31:44,418 --> 00:31:48,236
into our deck. So how does all this help in

494
00:31:48,258 --> 00:31:51,868
terms of shift left of performance testing? Let's understand

495
00:31:51,954 --> 00:31:55,464
that in a bit. So, quick recap

496
00:31:55,512 --> 00:31:58,896
of what we just saw. You have your laptop and you have your

497
00:31:58,918 --> 00:32:02,656
API code and the karate test corresponding to that. You deploy your

498
00:32:02,678 --> 00:32:05,680
code to your local environment, and that could be dockerized,

499
00:32:06,180 --> 00:32:10,304
and that's your application. Now you install puffy's on your machine

500
00:32:10,352 --> 00:32:13,572
which is again dockerized. All you need to do now is

501
00:32:13,626 --> 00:32:17,364
create the puffy's configuration file and this

502
00:32:17,402 --> 00:32:20,612
config file is read by the Buffy CLI and

503
00:32:20,666 --> 00:32:24,936
it's going to convert the karate API test into the

504
00:32:24,958 --> 00:32:28,872
Gatling performance test. Gather metrics both from Gatling and your

505
00:32:28,926 --> 00:32:33,160
application and visualize that on Grafana

506
00:32:34,060 --> 00:32:37,736
so that you can analyze the results and based on

507
00:32:37,758 --> 00:32:41,084
that, make changes to your code. Once you're satisfied with the code

508
00:32:41,122 --> 00:32:45,496
changes that you made, and you know that it's working fairly

509
00:32:45,608 --> 00:32:49,772
to your satisfaction on your local machine, you promote your code to higher environment.

510
00:32:49,916 --> 00:32:53,596
Likewise you promote your performance test setup also to the higher environment.

511
00:32:53,708 --> 00:32:57,184
Perf is being dockerized. It doesn't matter, it can just be deployed into

512
00:32:57,222 --> 00:33:00,556
the higher environment as well. And you could use a perf

513
00:33:00,588 --> 00:33:03,984
test pipeline if you wish. Even for the higher environment,

514
00:33:04,032 --> 00:33:08,004
it uses pretty much a similar configuration file. It's able to

515
00:33:08,202 --> 00:33:12,336
generate load to the higher environment based on your karate EPA

516
00:33:12,368 --> 00:33:16,056
test. Gather metrics from that environment into

517
00:33:16,078 --> 00:33:19,348
the metric store, make it available to you for analysis

518
00:33:19,444 --> 00:33:22,904
through Grafana dashboard, and then you can make the change. So on the left hand

519
00:33:22,942 --> 00:33:26,716
side, with your local machine, perfus is able to enable. Perfies can

520
00:33:26,738 --> 00:33:29,948
enable you to shift left and test very much on your

521
00:33:30,034 --> 00:33:33,948
lower environment, and the same setup can pretty much be promoted to

522
00:33:33,954 --> 00:33:37,964
the higher environment and you don't have to deal with additional complexity.

523
00:33:38,092 --> 00:33:40,450
So that's the whole idea here.

524
00:33:42,740 --> 00:33:46,256
Well, with all the topics that

525
00:33:46,278 --> 00:33:50,084
we've spoken about today and all the tooling that we've seen, I think

526
00:33:50,122 --> 00:33:53,584
the biggest challenge of it all in terms of shifting

527
00:33:53,632 --> 00:33:57,524
left and performance testing is the mindset itself for

528
00:33:57,562 --> 00:34:00,948
performance testing versus performance engineering. Let's understand

529
00:34:01,034 --> 00:34:04,552
this in a little bit more detail. Because of the word

530
00:34:04,606 --> 00:34:08,232
testing. Performance testing is usually something that we

531
00:34:08,286 --> 00:34:12,200
may tend to believe that it's more like a verification activity that does

532
00:34:12,270 --> 00:34:16,012
towards the tail end of development cycle just to see if everything is

533
00:34:16,066 --> 00:34:19,660
working fine. But rather, it'd be nice if we could

534
00:34:19,810 --> 00:34:23,740
use burp testing more like a learning activity through multiple spikes,

535
00:34:24,480 --> 00:34:28,008
which helps us avoid guesswork in our system architecture

536
00:34:28,104 --> 00:34:31,760
by helping us learn quickly in terms of what we are

537
00:34:31,910 --> 00:34:34,912
trying to design. How do we do that?

538
00:34:35,046 --> 00:34:39,068
The only way to avoid guesswork is to bring in some scientific rigor

539
00:34:39,164 --> 00:34:42,608
into our day to day activities. So in

540
00:34:42,694 --> 00:34:46,416
line with this, I was just wondering if all the principles that we've spoken

541
00:34:46,448 --> 00:34:50,308
about now, can we sort of put that into, so to speak, which will

542
00:34:50,394 --> 00:34:54,008
help us think through shifting left

543
00:34:54,094 --> 00:34:57,672
more consciously. And that's where I came up with this idea called

544
00:34:57,806 --> 00:35:00,984
continuous evaluation template. On the left

545
00:35:01,022 --> 00:35:05,084
hand side you have some basic information like

546
00:35:05,122 --> 00:35:08,488
what is the problem statement, the baseline KPI, the target KPI

547
00:35:08,504 --> 00:35:12,204
you'd like to achieve, and the hypotheses that you're trying

548
00:35:12,242 --> 00:35:16,076
to learn or verify. On the right are the

549
00:35:16,098 --> 00:35:19,408
more interesting piece, which is the experiment design itself,

550
00:35:19,574 --> 00:35:22,992
which is for every hypothesis we have to design one

551
00:35:23,046 --> 00:35:26,316
falsifiability experiment and one verifiability experiment.

552
00:35:26,428 --> 00:35:29,924
I'll get to that in a bit. And ultimately the validated learning

553
00:35:29,962 --> 00:35:32,630
that we get out of this experiment itself.

554
00:35:33,080 --> 00:35:36,516
So let's look at this template in

555
00:35:36,538 --> 00:35:39,920
the context of a more concrete example. I'm going to leverage

556
00:35:40,000 --> 00:35:43,264
the example that we've seen earlier itself,

557
00:35:43,322 --> 00:35:46,468
which is to increase the rps with a certain cpu.

558
00:35:46,564 --> 00:35:50,504
So let's say at 80% cpu I'm currently able to achieve ten

559
00:35:50,542 --> 00:35:54,132
k rps, and I'd like to have a ten fold

560
00:35:54,196 --> 00:35:57,404
increase to 100 krp, as any

561
00:35:57,522 --> 00:36:01,324
developer would immediately jump to. I also came

562
00:36:01,362 --> 00:36:04,808
up with the same idea, why don't we just drop in a cache

563
00:36:04,824 --> 00:36:09,504
and things should become better? Now obviously the

564
00:36:09,542 --> 00:36:13,596
first aspect is I need to have a falsifiability experiment for this hypothesis

565
00:36:13,628 --> 00:36:18,252
that the cache will indeed help reduce repetitive computations

566
00:36:18,316 --> 00:36:21,472
and thereby reduce cpu. So what can I do on my local

567
00:36:21,526 --> 00:36:24,560
machine? Remember the with and without experiment,

568
00:36:24,640 --> 00:36:28,336
or rather the thought process that we saw earlier.

569
00:36:28,448 --> 00:36:31,616
So first I'll verify if adding a cache

570
00:36:31,728 --> 00:36:35,168
makes any discernible change at all to the cpu usage,

571
00:36:35,264 --> 00:36:39,256
and that would be my first check. If that doesn't work, there's no point in

572
00:36:39,278 --> 00:36:42,872
me even taking it to the higher environment and verifying if the cache is actually

573
00:36:43,006 --> 00:36:46,924
helping me achieve that ten k increase. Sorry, tenfold increase in the

574
00:36:47,042 --> 00:36:50,236
RpM. Now since it does

575
00:36:50,258 --> 00:36:54,076
not work, I need to understand what went wrong and I realized that

576
00:36:54,098 --> 00:36:57,788
the cache is not working because of the miss rate. If it's almost

577
00:36:57,874 --> 00:37:01,008
100% now for this 100%,

578
00:37:01,094 --> 00:37:04,464
I hypothesize that the reason the miss rate is so high is

579
00:37:04,502 --> 00:37:08,544
because the TTL on the cache key is too small, so thereby before

580
00:37:08,662 --> 00:37:12,404
even can be accessed, the keys are expiring, so thereby I'm not

581
00:37:12,442 --> 00:37:16,516
able to hit the cache. So there's 100%.

582
00:37:16,698 --> 00:37:20,544
Now I fix the TTL and then try the same falsifiability experiment.

583
00:37:20,672 --> 00:37:24,628
And this time I see that there is some discernible difference between having a

584
00:37:24,634 --> 00:37:27,716
cache and not having a cache. So now I go to the staging environment,

585
00:37:27,748 --> 00:37:31,096
I can go to the higher environment and figure out now again, staging is

586
00:37:31,118 --> 00:37:34,836
not equivalent to production, so I cannot probably hit 100 krp.

587
00:37:34,948 --> 00:37:38,216
But what can I do? It's a Tenfold increase,

588
00:37:38,248 --> 00:37:41,944
right? Remember the trend scaling example that I've shown you earlier?

589
00:37:42,072 --> 00:37:45,896
Very much on those lines. What I'm going to do now is say whatever staging

590
00:37:45,928 --> 00:37:49,488
is able to achieve today, can I achieve ten x of that

591
00:37:49,654 --> 00:37:53,232
by leveraging the cash? That's the experiment I'd like to know.

592
00:37:53,366 --> 00:37:57,436
Say, suppose even that doesn't work. I'm able to only achieve

593
00:37:57,468 --> 00:38:01,560
about four x improvement. And I realize the issues. The missed

594
00:38:01,580 --> 00:38:05,140
rate is still about 40%. I still have high eviction rate.

595
00:38:05,290 --> 00:38:09,284
Now what do I do about that? Further hypothesize, I have high

596
00:38:09,322 --> 00:38:12,596
eviction rate because the cache size is too

597
00:38:12,618 --> 00:38:16,196
small. So as the cash is growing, keys are getting pushed

598
00:38:16,228 --> 00:38:19,988
out, and thereby again, I'm having high misflate and whatnot.

599
00:38:20,164 --> 00:38:23,576
Now, how do I design a falsifiability experiment for

600
00:38:23,598 --> 00:38:27,320
you? Obviously I do not have as much memory on my machine,

601
00:38:27,400 --> 00:38:31,276
so I cannot increase the cache size and experiment with that. But if I

602
00:38:31,298 --> 00:38:34,588
cannot increase the memory and increase the cache size,

603
00:38:34,674 --> 00:38:38,444
can I decrease the cache size and see if the eviction rate becomes higher?

604
00:38:38,642 --> 00:38:42,688
That should prove that there is correlation between cache size and eviction rate,

605
00:38:42,774 --> 00:38:45,872
right? At least I will know that this is the problem.

606
00:38:45,926 --> 00:38:49,410
And the hypothesis is indeed at least

607
00:38:49,860 --> 00:38:52,940
not falsifiable. Bet.

608
00:38:53,030 --> 00:38:56,096
Now if that works, then I can take it to the higher environment,

609
00:38:56,208 --> 00:39:00,452
increase the capsize of Teju, and see if I can achieve my

610
00:39:00,586 --> 00:39:04,296
number right. And yes, this time I'm able to achieve ten x of

611
00:39:04,318 --> 00:39:09,332
paging. So great. Now all this experimentation

612
00:39:09,396 --> 00:39:13,144
has now put me in a position to suggest what

613
00:39:13,182 --> 00:39:16,968
should be the TTL, what should be the cache on production.

614
00:39:17,064 --> 00:39:20,456
A production replica for us to achieve the 100 krp,

615
00:39:20,568 --> 00:39:24,456
right? And I can don't jump into a conclusion.

616
00:39:24,488 --> 00:39:27,924
I first verify that if I just deploy this setting,

617
00:39:28,072 --> 00:39:32,240
there should be some change in the cpu. At least it should drop

618
00:39:33,220 --> 00:39:36,816
a little bit. Which means like the deployment is successful on itself,

619
00:39:36,918 --> 00:39:40,764
and only when that is done will I go for a full blown

620
00:39:40,812 --> 00:39:44,244
test with 100 krps test itself,

621
00:39:44,362 --> 00:39:47,716
and if that is done, then I can ship the code. Now,

622
00:39:47,738 --> 00:39:51,540
as you can see, this template sort of forced me to think hard

623
00:39:51,610 --> 00:39:55,496
about solving the problems through falsifiability. And on

624
00:39:55,518 --> 00:39:58,984
the left hand side, rather than depending very much on the right. And for

625
00:39:59,022 --> 00:40:02,276
every single piece, like taking the cache, deploying it to production,

626
00:40:02,308 --> 00:40:06,156
then realizing the settings are incorrect and then coming back and making change.

627
00:40:06,258 --> 00:40:10,488
Now by shifting left, I've reduced my dependency on the higher environment,

628
00:40:10,584 --> 00:40:14,456
and that's where I find most value in this template. Now, obviously, for the purpose

629
00:40:14,488 --> 00:40:18,316
of demonstration, I've reduced this in real life. There was a

630
00:40:18,338 --> 00:40:21,076
lot more steps which I had to go through before I could shift.

631
00:40:21,208 --> 00:40:26,192
But you get the idea in terms of how this could be valuable and think

632
00:40:26,246 --> 00:40:30,108
through a problem in a lot more detail and have scientific

633
00:40:30,204 --> 00:40:33,744
rigor about it. Now, with that, thank you very much

634
00:40:33,782 --> 00:40:37,484
for being a very kind audience. And these are my handles,

635
00:40:37,532 --> 00:40:41,176
and I'd love to keep in touch and answer any questions that my

636
00:40:41,198 --> 00:40:41,270
time.

