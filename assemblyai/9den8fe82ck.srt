1
00:00:00,410 --> 00:00:06,126
Jamaica make up real

2
00:00:06,148 --> 00:00:09,530
time feedback into the behavior of your distributed systems

3
00:00:09,610 --> 00:00:12,490
and observing changes exceptions.

4
00:00:12,570 --> 00:00:15,722
Errors in real time allows you to not only experiment

5
00:00:15,786 --> 00:00:19,822
with confidence, but respond instantly to get things working

6
00:00:19,876 --> 00:00:20,480
again.

7
00:00:24,610 --> 00:01:04,666
Close thanks

8
00:01:04,708 --> 00:01:08,770
for joining my talk on disaster recovery preparedness using chaos engineering.

9
00:01:09,110 --> 00:01:12,642
In this session, I will go over disaster recovery context and the difference

10
00:01:12,696 --> 00:01:16,118
between high availability and disasterlike recovery. We'll talk about how

11
00:01:16,124 --> 00:01:20,102
to approach Dr. From a cost risk perspective, and then we'll talk about

12
00:01:20,156 --> 00:01:23,526
resiliency with chaos engineering. So let's talk

13
00:01:23,548 --> 00:01:27,346
about the difference between high availability and disasterlike recovery.

14
00:01:27,458 --> 00:01:31,306
High availability is when you improve your uptime and resiliency by

15
00:01:31,328 --> 00:01:35,462
removing single points of failure and redundancy, whereas disaster recovery

16
00:01:35,526 --> 00:01:39,626
is your set of plans and policies that recover your workloads when things go down.

17
00:01:39,808 --> 00:01:43,214
Backups are not a Dr. Plan and disaster recovery needs

18
00:01:43,252 --> 00:01:47,354
to be clearly defined and practiced often in order to prove confidence

19
00:01:47,402 --> 00:01:51,466
in your distributed systems. When thinking about disaster recovery,

20
00:01:51,498 --> 00:01:54,994
we're always going to focus on resiliency. Resiliency is being

21
00:01:55,032 --> 00:01:58,670
prepared for that black swan event. Increasing availability

22
00:01:58,750 --> 00:02:02,178
and practicing restoration consistency helps you build

23
00:02:02,264 --> 00:02:05,458
resiliency. Cloud native companies expect

24
00:02:05,544 --> 00:02:09,874
failure and are constantly improving their resiliency. Everything breaks

25
00:02:09,922 --> 00:02:13,266
all the time and you need to be prepared for things to fail,

26
00:02:13,378 --> 00:02:15,894
especially in a shared responsibility model.

27
00:02:16,092 --> 00:02:19,782
When thinking about resiliency, you have to understand that resiliency

28
00:02:19,846 --> 00:02:23,258
is critical and affects your user experience

29
00:02:23,344 --> 00:02:26,662
for your customers. Resiliency is also complex

30
00:02:26,726 --> 00:02:30,454
and grows in complexity over time as your applications

31
00:02:30,502 --> 00:02:33,302
grow, whether it be integrations, features,

32
00:02:33,366 --> 00:02:35,978
mergers and acquisitions, and et cetera.

33
00:02:36,074 --> 00:02:39,854
Resiliency is a key cost driver on your recovery point

34
00:02:39,892 --> 00:02:43,230
in time, objectives and the criticality of your workloads.

35
00:02:43,310 --> 00:02:47,026
You might have safety related workloads that if

36
00:02:47,048 --> 00:02:51,666
they go down, people's safety is in

37
00:02:51,688 --> 00:02:55,170
place. So the criticality of the workload really

38
00:02:55,240 --> 00:02:59,430
can help determine how you have to build in your resiliency.

39
00:02:59,770 --> 00:03:03,206
Resiliency is completely different in a cloud than it is with

40
00:03:03,228 --> 00:03:07,154
on Prem applications. I can remember working at Verizon

41
00:03:07,202 --> 00:03:10,522
and we would have to build out our applications in a 40

42
00:03:10,576 --> 00:03:13,594
40 distributed model so that we

43
00:03:13,632 --> 00:03:17,610
never ran over 40% capacity or we would have to add more.

44
00:03:17,760 --> 00:03:22,010
So in the cloud, you build in capacity

45
00:03:22,170 --> 00:03:25,514
by knowing what to do when certain things fail,

46
00:03:25,562 --> 00:03:28,522
whether it be an instance stop start, shutdown,

47
00:03:28,666 --> 00:03:31,930
instance degradation, availability zone,

48
00:03:32,090 --> 00:03:35,150
service event, or even like we had in December,

49
00:03:35,310 --> 00:03:39,122
the regional event. Building in resiliency in the cloud means

50
00:03:39,176 --> 00:03:42,210
being prepared for the unknown failure events.

51
00:03:42,630 --> 00:03:46,182
So when we're talking about Dr. We're going to

52
00:03:46,236 --> 00:03:49,766
start with defining our recovery point objectives, right? And your

53
00:03:49,788 --> 00:03:53,960
recovery time objective. Your recovery time objective is the

54
00:03:55,450 --> 00:03:58,970
acceptable delay between service interruption and service

55
00:03:59,040 --> 00:04:02,154
restoration. This determines what is considered can

56
00:04:02,192 --> 00:04:05,594
acceptable time window when the service becomes unavailable. Right.

57
00:04:05,712 --> 00:04:10,182
And your recovery point objective is the maximum acceptable

58
00:04:10,246 --> 00:04:13,962
time since the last data recovery point. This determines

59
00:04:14,026 --> 00:04:17,870
what is considered an acceptable loss of data between the last recovery point

60
00:04:17,940 --> 00:04:21,742
and the service outage. Now that we understand that, we can

61
00:04:21,796 --> 00:04:25,006
build our Dr. Strategy based on these factors.

62
00:04:25,118 --> 00:04:28,814
Right. So backup and restore is using to backup

63
00:04:28,862 --> 00:04:32,094
your data applications in the Dr. Region and restore

64
00:04:32,142 --> 00:04:35,646
this data when it's needed to recover from the disaster.

65
00:04:35,838 --> 00:04:39,254
If your time is in the hours and up to 24

66
00:04:39,292 --> 00:04:42,726
hours or less, this is going to be your best option. Whereas if

67
00:04:42,748 --> 00:04:46,422
your recovery point time is in the minutes pilot light, that might

68
00:04:46,476 --> 00:04:50,602
work for you, and it keeps a minimal version of the environment. By always

69
00:04:50,656 --> 00:04:53,930
running the most critical core elements of your system in the Dr.

70
00:04:54,000 --> 00:04:57,562
Region at the time of performing the recovery, you can quickly

71
00:04:57,616 --> 00:05:01,910
provision a full scale production environment that includes the critical core.

72
00:05:01,990 --> 00:05:05,306
So you'll have your pilot light just sitting there waiting to be flipped

73
00:05:05,338 --> 00:05:07,840
on whenever disaster happens.

74
00:05:08,690 --> 00:05:12,454
Your warm standby is a little bit different. It's a little bit more expensive

75
00:05:12,522 --> 00:05:15,886
in that it keeps a reduced version

76
00:05:15,998 --> 00:05:19,682
of a fully functional environment that's always running

77
00:05:19,736 --> 00:05:23,374
in your Dr. Region. Business critical systems are fully

78
00:05:23,422 --> 00:05:26,590
duplicated and are always on, but with a reduced fleet.

79
00:05:26,670 --> 00:05:30,434
So when the time comes for recovery, the system scales quickly to

80
00:05:30,472 --> 00:05:33,842
process that production low. And then your most expensive

81
00:05:33,906 --> 00:05:36,946
is your active active. And this is how we build things in the on prem

82
00:05:36,978 --> 00:05:40,086
world where your RPO is basically none

83
00:05:40,118 --> 00:05:43,542
or seconds, and your RTO is in seconds. So your workload

84
00:05:43,606 --> 00:05:47,462
is deployed and actively serving traffic in multiple AWS

85
00:05:47,526 --> 00:05:52,118
regions. Right. So the strategy requires you to synchronize users

86
00:05:52,134 --> 00:05:55,626
and data between the regions that you're using. And so there's

87
00:05:55,658 --> 00:05:59,246
a lot of data transfer going back and forth and a lot of databases that

88
00:05:59,268 --> 00:06:02,926
need to be in sync. And when recovery

89
00:06:02,958 --> 00:06:06,798
time comes, you can use services such as AWS

90
00:06:06,894 --> 00:06:10,494
route 53, or the global accelerator to route that user

91
00:06:10,542 --> 00:06:13,874
traffic to an entirely different workload application.

92
00:06:13,992 --> 00:06:17,606
Right? So in that system, those are going to be

93
00:06:17,628 --> 00:06:21,030
your highest priority critical workloads. Right.

94
00:06:21,100 --> 00:06:24,658
And one thing you're going to want to do is avoid recovery

95
00:06:24,754 --> 00:06:28,618
mechanisms that are not often tested. You want

96
00:06:28,624 --> 00:06:32,422
to define these regular tests for failover to ensure

97
00:06:32,486 --> 00:06:35,994
that your expected recovery point in time objectives are met.

98
00:06:36,112 --> 00:06:39,462
So always avoid creating recovery

99
00:06:39,526 --> 00:06:42,000
mechanisms, but never practicing them.

100
00:06:42,690 --> 00:06:46,510
It's important to practice. In the navy, we were constantly

101
00:06:47,010 --> 00:06:50,782
going through firefighting exercises and everything because

102
00:06:50,836 --> 00:06:54,866
you're always practicing for that event when you need

103
00:06:54,888 --> 00:06:58,194
to put out a fire. So take those same context and

104
00:06:58,232 --> 00:07:02,194
utilize them. Now let's talk about resiliency and

105
00:07:02,232 --> 00:07:06,902
using chaos engineering to prepare better

106
00:07:06,956 --> 00:07:10,162
your resiliency posture, right? So chaos

107
00:07:10,226 --> 00:07:13,746
engineering, as you know, is the discipline of experimenting

108
00:07:13,778 --> 00:07:17,286
with the system with the aim of increasing confidence in

109
00:07:17,308 --> 00:07:21,030
its ability to withstand problems in your environment.

110
00:07:23,130 --> 00:07:26,882
My philosophy and our philosophy is testing in a non production

111
00:07:26,946 --> 00:07:31,802
environment should always be performed regularly and be part of your integration

112
00:07:31,866 --> 00:07:33,470
and deployment lifecycle.

113
00:07:35,890 --> 00:07:39,454
In production teams must perform these tests in such a way to

114
00:07:39,492 --> 00:07:43,090
not cause the service to unavailable. The last thing you want to do

115
00:07:43,240 --> 00:07:46,434
is cause problems with your customers while

116
00:07:46,472 --> 00:07:50,334
you're testing out hypothesis. So always run these tests in non

117
00:07:50,382 --> 00:07:54,974
production or development environments and then make

118
00:07:55,032 --> 00:07:58,694
sure that test results are measured and then compared with

119
00:07:58,732 --> 00:08:02,566
availability objectives to understand whether the application

120
00:08:02,668 --> 00:08:05,846
is running in that particular environment is

121
00:08:05,868 --> 00:08:08,410
able to meet those defined objectives.

122
00:08:11,150 --> 00:08:14,198
When you start first experimenting with chaos engineering,

123
00:08:14,294 --> 00:08:17,946
start small and build confidence. Don't go straight to

124
00:08:17,968 --> 00:08:21,598
regional failures. Start by stopping instances or doing things at a

125
00:08:21,604 --> 00:08:24,954
host level that you can build confidence

126
00:08:25,002 --> 00:08:28,286
and form your hypothesis, and then work your way up

127
00:08:28,388 --> 00:08:32,062
to availability zone failures or even into

128
00:08:32,116 --> 00:08:35,694
the regional failures. But try to build auto recovery

129
00:08:35,742 --> 00:08:39,826
mechanisms into your systems. After you perform

130
00:08:39,928 --> 00:08:43,346
these experiments. Always assess your risk for

131
00:08:43,368 --> 00:08:46,626
appetite and make sure to isolate failures at all

132
00:08:46,648 --> 00:08:50,582
times. Like we talked about on the last slide. Never do things in production that

133
00:08:50,636 --> 00:08:54,326
could have an effect on your customers and always have a backout and

134
00:08:54,348 --> 00:08:57,874
a rollback plan. And when we're

135
00:08:57,922 --> 00:09:01,238
quantifying the results of the experiments you're

136
00:09:01,254 --> 00:09:04,634
using to want to think about how long does it take to

137
00:09:04,672 --> 00:09:08,410
detect these failures and how long does it take to get notified?

138
00:09:08,750 --> 00:09:12,560
Should a status page be updated, right? Or should you notify your customers?

139
00:09:13,250 --> 00:09:16,654
How long does your auto recovery happen? That's a big

140
00:09:16,692 --> 00:09:20,682
factor, right? Because if you have a recovery objective

141
00:09:20,746 --> 00:09:23,822
of ten minutes and your auto recovery takes 20,

142
00:09:23,876 --> 00:09:27,042
then you're going to have to go back to the drawing board.

143
00:09:27,096 --> 00:09:31,026
And is it a partial or full auto recovery or

144
00:09:31,208 --> 00:09:34,322
how long does it take to really get back to that steady state?

145
00:09:34,456 --> 00:09:38,054
That's going to be one of the key quantifiable results of

146
00:09:38,092 --> 00:09:41,240
the experiment and what you're going to be looking for.

147
00:09:42,330 --> 00:09:45,686
You also want to do reviews of the

148
00:09:45,708 --> 00:09:49,066
incident. So having a blameless culture is something that really needs to be

149
00:09:49,088 --> 00:09:52,566
in place for this to work, right? So you'll

150
00:09:52,598 --> 00:09:57,062
talk about the event, the impact, and go over the five whys.

151
00:09:57,206 --> 00:10:01,158
Make sure all your data and monitoring and observability metrics

152
00:10:01,174 --> 00:10:04,254
are there that tell you this. We have

153
00:10:04,292 --> 00:10:07,626
a saying at one of my previous places that charts and graphs didn't

154
00:10:07,658 --> 00:10:11,870
happen. So make sure you have your proper eyes and observability

155
00:10:12,390 --> 00:10:16,306
metrics there so that you can learn

156
00:10:16,488 --> 00:10:19,634
from what happened. You want to make sure that

157
00:10:19,672 --> 00:10:24,162
you take corrective actions and they're followed through upon. Right. So in

158
00:10:24,216 --> 00:10:27,506
these post mortems, we call them coes or correction

159
00:10:27,538 --> 00:10:31,126
of errors. Have a defined list and

160
00:10:31,148 --> 00:10:34,946
have a structure of how these meetings go and clearly

161
00:10:34,978 --> 00:10:38,150
define the lessons learned and what to take out of it.

162
00:10:38,300 --> 00:10:42,074
If you're not going to learn from these failures, and if you're not going to

163
00:10:42,272 --> 00:10:45,434
take the results and learn from them, then you're never going to

164
00:10:45,472 --> 00:10:49,274
be able to improve the resiliency. And finally,

165
00:10:49,392 --> 00:10:53,550
as you're going through these, continually audit

166
00:10:53,890 --> 00:10:57,550
these meetings or these post mortems,

167
00:10:58,690 --> 00:11:03,306
try to get to a way to where you're having a weekly cadence and constantly

168
00:11:03,418 --> 00:11:07,490
improving on things. At one of my stops, AWS, an SRE,

169
00:11:08,470 --> 00:11:11,934
we met with our knock engineers on a bi weekly basis,

170
00:11:11,982 --> 00:11:15,818
and we went over every single escalation, and then we created

171
00:11:15,854 --> 00:11:19,590
a runbook every time. And so any new escalation

172
00:11:20,010 --> 00:11:23,426
shouldn't have a runbook. Right. So if we're getting escalated

173
00:11:23,538 --> 00:11:27,126
for things on a repeatable basis, then that to

174
00:11:27,148 --> 00:11:28,810
me, is considered toil,

175
00:11:31,070 --> 00:11:34,694
especially if there's human interaction. Try to automate

176
00:11:34,742 --> 00:11:38,778
those processes, but have those weekly operational overviews and

177
00:11:38,864 --> 00:11:42,842
go over your planning metrics. Make sure that you're

178
00:11:42,906 --> 00:11:45,406
continuously improving. There's a saying,

179
00:11:45,508 --> 00:11:49,454
kaizen, which is continuous improvement. Make sure that you're always trying

180
00:11:49,492 --> 00:11:53,110
to improve and learn from these events and learn from these failures.

181
00:11:53,290 --> 00:11:57,140
When you do that, you will build a much more resilient system.

182
00:11:58,550 --> 00:12:01,220
So how do we get started?

183
00:12:01,750 --> 00:12:04,814
Well, you can run recurring experiments,

184
00:12:04,862 --> 00:12:08,194
right? And what are some good candidates

185
00:12:08,242 --> 00:12:11,554
for recurring experiments? Machine led processes

186
00:12:11,602 --> 00:12:14,070
like unit tests, regression tests,

187
00:12:14,890 --> 00:12:17,110
integration tests, and load tests.

188
00:12:18,590 --> 00:12:21,978
Remember, just like these other tests, it's important to consider

189
00:12:22,064 --> 00:12:25,174
the scope and the duration of the recurrent

190
00:12:25,222 --> 00:12:28,774
fault injection experiments, right. So, because fault

191
00:12:28,822 --> 00:12:32,618
injection experiments generally expose issues across a large

192
00:12:32,704 --> 00:12:36,714
number of link systems, they will typically require extended

193
00:12:36,762 --> 00:12:40,126
runtimes to ensure sufficient data collection. So make sure

194
00:12:40,148 --> 00:12:43,694
you put them in the later stages of your CI CD pipeline.

195
00:12:43,742 --> 00:12:47,682
That way they don't slow up your developers. And here

196
00:12:47,816 --> 00:12:51,810
is a link to one of the chaos engineering workshops where you can create

197
00:12:51,960 --> 00:12:55,634
a recurring experiment. And in this

198
00:12:55,672 --> 00:12:58,914
experiment, we focused on running it

199
00:12:58,952 --> 00:13:02,278
in a CI CD pipeline with the argument that it's easy to

200
00:13:02,284 --> 00:13:05,414
slow down the pipeline to run only once a year. But hard to speed up

201
00:13:05,452 --> 00:13:09,834
a manual process to run multiple times every day. Right? So go

202
00:13:09,872 --> 00:13:13,494
with one repo, use a single repository to host

203
00:13:13,542 --> 00:13:17,050
the definition of the pipeline, the infrastructure and the template.

204
00:13:17,870 --> 00:13:22,574
You want to do this so that you can co

205
00:13:22,612 --> 00:13:27,166
version all components of the system. Right. Whether this is a good idea kind

206
00:13:27,188 --> 00:13:30,926
of depends on your governance processes, but each of the

207
00:13:30,948 --> 00:13:34,660
parts can easily be dependent. And with

208
00:13:35,430 --> 00:13:39,314
this part of the workshop, you can create these

209
00:13:39,352 --> 00:13:42,946
so that you can integrate them easily into your pipeline. And as

210
00:13:42,968 --> 00:13:46,402
you can see, it's using the CDK to build out the infrastructure.

211
00:13:46,466 --> 00:13:50,738
And you create a code repo

212
00:13:50,914 --> 00:13:54,514
and pipeline using the CDK and then you trigger

213
00:13:54,562 --> 00:13:57,638
the pipeline to instantiate the infrastructure and

214
00:13:57,644 --> 00:14:01,762
then trigger the pipeline to update the infrastructure and perform the fault injection.

215
00:14:01,906 --> 00:14:05,430
So it's a really cool workshop. Scan it, give it a shot.

216
00:14:05,770 --> 00:14:09,998
Yeah. And then here are some other resources that,

217
00:14:10,044 --> 00:14:12,670
that are at your disposal.

218
00:14:13,330 --> 00:14:16,986
Always, if you would like to run these with your tam,

219
00:14:17,098 --> 00:14:20,906
reach out to your tam. But yeah, thanks for joining

220
00:14:20,938 --> 00:14:22,620
my talk and you all have a great day.

