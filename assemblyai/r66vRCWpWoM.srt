1
00:00:25,650 --> 00:00:29,474
Hey there, this is Ruben Lerner. Thanks for coming to my talk about dates

2
00:00:29,522 --> 00:00:32,614
and times in pandas for conf fourty two.

3
00:00:32,732 --> 00:00:35,878
I am not using to be using slides for this talk. Rather, I'm going to

4
00:00:35,884 --> 00:00:39,542
be typing directly into the Jupyter notebook. And I hope that you'll then see

5
00:00:39,596 --> 00:00:43,094
not only the sorts of cool things we can do with times and dates in

6
00:00:43,132 --> 00:00:46,838
pandas with Python, of course, but you'll also see the process that

7
00:00:46,844 --> 00:00:50,158
I can use to play with Jupyter and use it in order to do what

8
00:00:50,164 --> 00:00:52,958
I want to do here. In case you don't know who I am. As I

9
00:00:52,964 --> 00:00:56,286
said, my name is Ruben Lerner. You can learn more about me at learner Co.

10
00:00:56,388 --> 00:00:59,854
Il. I'm a full time Python trainer. I have a whole

11
00:00:59,892 --> 00:01:03,294
variety of online courses that I offer to people around

12
00:01:03,332 --> 00:01:06,894
the world, everything from intro Python to advanced Python courses.

13
00:01:06,942 --> 00:01:10,338
I also have a set of courses called weekly Python exercise, and I have a

14
00:01:10,344 --> 00:01:14,014
number of free courses as well, including one about Python

15
00:01:14,062 --> 00:01:17,810
interviews and Python for non programmers. Take a look at my site.

16
00:01:17,880 --> 00:01:20,802
It'll lead you to all the good stuff there, as well as to my blog

17
00:01:20,866 --> 00:01:24,630
and to my mailing list, the better developers list, which about 20,000

18
00:01:24,700 --> 00:01:30,070
people subscribe to this point. You can get that@betterdevelopersweekly.com

19
00:01:30,220 --> 00:01:33,138
okay, well, what are we going to talk about here? We're not here to talk

20
00:01:33,164 --> 00:01:36,058
about all my stuff. We're here to talk about Python pandas. So we're going to

21
00:01:36,064 --> 00:01:39,510
be talking about dates and times in pandas. So first of all, let's define

22
00:01:39,590 --> 00:01:43,626
dates and times, how they work in programming.

23
00:01:43,738 --> 00:01:47,678
Then we're going to talk about how we can load date time information

24
00:01:47,844 --> 00:01:52,094
into a data frame. And then what we're going to do is look at

25
00:01:52,212 --> 00:01:56,370
dates, let's say extracting date time information.

26
00:01:56,520 --> 00:01:59,454
And then we're using to talk about time intervals,

27
00:01:59,582 --> 00:02:03,230
aka deltas, and how we can get that in a data frame.

28
00:02:03,310 --> 00:02:07,766
And then we will talk about how we can use date time

29
00:02:07,868 --> 00:02:12,226
objects in a data frame index.

30
00:02:12,338 --> 00:02:16,466
And then finally we will talk about time series aggregation

31
00:02:16,578 --> 00:02:19,718
and resampling. Cool. That's a lot of stuff, but we're going to

32
00:02:19,724 --> 00:02:23,142
go through it and hopefully be really cool and interesting. I find this stuff incredibly

33
00:02:23,206 --> 00:02:26,442
interesting, maybe because I'm a nerd and maybe because

34
00:02:26,576 --> 00:02:29,946
probably because it's actually interesting. So let's first start off with dates and

35
00:02:29,968 --> 00:02:33,566
times in programming. So dates and times. The thing is,

36
00:02:33,668 --> 00:02:37,866
there are two different ways to measure

37
00:02:37,978 --> 00:02:41,114
time, and I'm not talking about whether we measure

38
00:02:41,162 --> 00:02:43,914
with month day, year, hour, minute,

39
00:02:43,962 --> 00:02:47,166
second, or whether measure with seconds, which is sort of the traditional Unix

40
00:02:47,198 --> 00:02:51,234
way of doing it, number of seconds. Since the 1 January 1970,

41
00:02:51,352 --> 00:02:54,590
we basically have two different things. We can talk about a timestamp,

42
00:02:54,670 --> 00:02:58,082
aka a date time, aka a point in

43
00:02:58,136 --> 00:03:01,430
time, and we can talk about a time interval, aka a time

44
00:03:01,500 --> 00:03:05,382
delta. So what are these different things? Well, basically a date time

45
00:03:05,436 --> 00:03:08,578
or a timestamp, depending on what language you use, what system you're using,

46
00:03:08,684 --> 00:03:12,714
this points to, or this is a specific point in time.

47
00:03:12,832 --> 00:03:16,010
So we can say, e g. When you were

48
00:03:16,080 --> 00:03:20,074
married or when you were born or when you got

49
00:03:20,192 --> 00:03:24,026
your first paycheck or something like that, is a specific point in time.

50
00:03:24,128 --> 00:03:26,814
There were many, many points in time before it, many, many points in time after

51
00:03:26,852 --> 00:03:30,554
it, but is a specific specific point in time and it will never repeat.

52
00:03:30,602 --> 00:03:34,394
It is unique. And so we can use all sorts of data structures

53
00:03:34,442 --> 00:03:38,058
in Python to do this. The time module provides us with all sorts

54
00:03:38,074 --> 00:03:41,026
of stuff for working with that. Also the date time module, which is sort of

55
00:03:41,048 --> 00:03:43,986
a higher level way of working with dates and times, can also do that for

56
00:03:44,008 --> 00:03:47,026
us. So that's great, right? If I have a date time, in fact, if I

57
00:03:47,048 --> 00:03:50,390
can go here, I can say here, import date time, I can say date time,

58
00:03:50,460 --> 00:03:53,846
date time, knows date time now. And you see,

59
00:03:53,868 --> 00:03:57,478
I get back. This is a pure Python date time object. It's in

60
00:03:57,484 --> 00:04:00,518
the daytime module. The datetime class knows it was a method we

61
00:04:00,524 --> 00:04:03,782
call on that and we get back now. 2021. 516,

62
00:04:03,846 --> 00:04:08,454
1222. 552. And this is the number of, I believe it's milliseconds.

63
00:04:08,502 --> 00:04:11,502
Microseconds. I guess it would be. Well, okay, what if I say like,

64
00:04:11,636 --> 00:04:14,830
start equals date time, date time now,

65
00:04:14,900 --> 00:04:16,430
and then we wait a few seconds.

66
00:04:19,250 --> 00:04:22,494
Okay, I've waited enough. End equals date time. Date time.

67
00:04:22,612 --> 00:04:25,422
Now we see that start is here and end is there.

68
00:04:25,476 --> 00:04:28,306
Okay, this is not a huge surprise. I waited, what does it look like?

69
00:04:28,328 --> 00:04:31,406
About 7 seconds. And sure enough, the second one is 7 seconds

70
00:04:31,438 --> 00:04:35,140
later. What if, though, I want to find but how much time

71
00:04:36,310 --> 00:04:39,846
pass between these two date time

72
00:04:39,948 --> 00:04:43,446
objects? Right? So that's also a measurement of time.

73
00:04:43,548 --> 00:04:46,614
But it's not a measurement of time in the same way. And I must say

74
00:04:46,652 --> 00:04:49,366
it took a long time, no pun intended, it took a long time for me

75
00:04:49,388 --> 00:04:53,306
to understand that when we talk about time as people, we are actually

76
00:04:53,328 --> 00:04:56,474
using these two different ideas. One is a specific point in time. One is

77
00:04:56,512 --> 00:04:59,706
a length of time. A span of time, or has I called it before?

78
00:04:59,808 --> 00:05:03,146
A time interval, a time delta, for example, let's say you have

79
00:05:03,168 --> 00:05:06,398
a meeting that starts at 01:00 p.m. And it will go from 01:00 p.m.

80
00:05:06,404 --> 00:05:09,950
To 02:00 p.m. So the start time will be at 01:00 p.m.

81
00:05:10,020 --> 00:05:13,422
The end time will be at 02:00 p.m. But the length of time it takes

82
00:05:13,476 --> 00:05:16,762
will be 1 hour. That's a time interval. That's a time span.

83
00:05:16,826 --> 00:05:19,406
And what if you get a call just before the meeting starts and they say,

84
00:05:19,428 --> 00:05:22,094
oh, we can't meet now, let's meet half an hour later, we'll just push it

85
00:05:22,132 --> 00:05:25,606
back. Well, everything gets pushed back to the start time, and because later the

86
00:05:25,628 --> 00:05:28,534
end time becomes later. But that time interval will actually be the same.

87
00:05:28,572 --> 00:05:31,958
It's still going to be 1 hour in length, as if we're lucky enough to

88
00:05:31,964 --> 00:05:34,946
have 1 hour meetings. Right? So how can I deal with that? Has a python.

89
00:05:34,978 --> 00:05:38,134
Well, I can actually just say end minus start, and look

90
00:05:38,172 --> 00:05:41,994
what it does. If I take one daytime object, I subtract it from

91
00:05:42,032 --> 00:05:45,162
another daytime object, I get a time delta object.

92
00:05:45,216 --> 00:05:48,438
Daytime delta. And it's kind of weird in that it's

93
00:05:48,454 --> 00:05:51,526
going to measure here seconds and microseconds. If it's

94
00:05:51,558 --> 00:05:55,722
really large, it'll measure in days, seconds, and microseconds. But those are the only components

95
00:05:55,786 --> 00:05:58,382
of a time delta that we really need to think about and worry about,

96
00:05:58,436 --> 00:06:01,450
even though it's covering any sort of time delta,

97
00:06:01,610 --> 00:06:05,134
these are the three sort of pieces that it uses to measure that. So anything

98
00:06:05,172 --> 00:06:08,482
up to a second will be in microseconds. Anything between a second and a day

99
00:06:08,536 --> 00:06:11,378
will be in seconds. Anything a day or greater will be in days. And you

100
00:06:11,384 --> 00:06:15,170
can of course mix these together as we see here on my screen. Okay,

101
00:06:15,240 --> 00:06:19,166
so we have daytime objects and we have time delta

102
00:06:19,198 --> 00:06:22,482
objects. What in the world does this have to do with pandas?

103
00:06:22,546 --> 00:06:25,526
Well, it turns out that when we're working with pandas, we actually deal with a

104
00:06:25,548 --> 00:06:28,758
lot of dates and time dates. So I'm going to load up now one of

105
00:06:28,764 --> 00:06:32,570
my favorite data sets to play with. So I'm just going to first say Pylab

106
00:06:32,990 --> 00:06:37,110
inline, I'll say import pandas as PD and from pandas,

107
00:06:37,270 --> 00:06:40,598
import series and data friendly. Okay, so I've loaded

108
00:06:40,614 --> 00:06:44,258
up pylab inline is for using Jupyter. It makes sure that numpy

109
00:06:44,294 --> 00:06:47,642
and Matplotlib are both loaded and they're in the current namespace.

110
00:06:47,706 --> 00:06:51,118
I load up pandas because it's not loaded by default. And then from the

111
00:06:51,124 --> 00:06:53,966
Pandas library from the pandas module. I'm going to load years and data frame into

112
00:06:53,988 --> 00:06:56,782
the current namespace just because it's super useful to be able to do that.

113
00:06:56,836 --> 00:07:00,238
And now I can actually create a new data frame and I can do it

114
00:07:00,324 --> 00:07:03,738
based on a file. And a whole lot of files in the world data science

115
00:07:03,754 --> 00:07:07,462
are in CSV format. It's not a surprise. We have PD read

116
00:07:07,516 --> 00:07:10,658
CSV and I can just read in a file here. Well, what is this file

117
00:07:10,674 --> 00:07:12,374
that I'm going to read in? Let's just take a look at it first.

118
00:07:12,412 --> 00:07:15,478
This csv file. So this is taxi csV. This is from a number of

119
00:07:15,484 --> 00:07:18,726
years ago. It's a little out of date, but not. We'd still do analysis on.

120
00:07:18,748 --> 00:07:22,374
It's kind of fun. These are taxi rides from

121
00:07:22,412 --> 00:07:25,858
New York City from 2015. And basically when I was growing

122
00:07:25,874 --> 00:07:28,486
up, and even though I live in Israel now, I grew up in the New

123
00:07:28,508 --> 00:07:31,386
York area, if I wanted to take taxi. So I'd get into the taxi,

124
00:07:31,418 --> 00:07:34,266
tell the driver where I wanted to go, and when we arrived there, the driver

125
00:07:34,298 --> 00:07:36,446
would tell me, well, it was written on the meter how much I had to

126
00:07:36,468 --> 00:07:39,834
pay, and then basically I would pay the driver

127
00:07:39,882 --> 00:07:43,486
however much it was, add a tip, and we would be done, and I'd get

128
00:07:43,508 --> 00:07:46,814
a receipt as well. Well, nowadays, of course, in New York, it's way more high

129
00:07:46,852 --> 00:07:50,286
tech than it was back when I was a teenager. Nowadays, when you get into

130
00:07:50,308 --> 00:07:53,726
a taxi, there's actually a computer in the backseat, and the computer, you think,

131
00:07:53,748 --> 00:07:57,478
is there just to annoy you with television ads and movies and all sorts of

132
00:07:57,484 --> 00:08:00,758
spammy stuff. But at the end of the trip, it allows you to use a

133
00:08:00,764 --> 00:08:03,346
credit card to pay, and then it asks you how much you want to tip.

134
00:08:03,378 --> 00:08:06,694
And it's, I think it's like 2025 and 30%, something like that.

135
00:08:06,812 --> 00:08:09,266
People definitely pay more for tips now than they did when I was a teenager,

136
00:08:09,298 --> 00:08:12,586
when we were told it was 15%. And of course, you can choose something else

137
00:08:12,608 --> 00:08:16,170
if you want. So basically, this is a record of

138
00:08:16,240 --> 00:08:19,498
every single taxi ride in New York, where you can go to the New

139
00:08:19,504 --> 00:08:23,526
York City open data system and download taxi rides from just about anytime

140
00:08:23,558 --> 00:08:26,366
you want. I think it lasts ten or 15 years. I don't remember exactly when

141
00:08:26,388 --> 00:08:30,094
they started recording this. So the computer is not just there to annoy you with

142
00:08:30,132 --> 00:08:33,390
ads and not just there to help you with taking your credit card,

143
00:08:33,460 --> 00:08:37,278
but it also records a lot of information about the trip so that

144
00:08:37,284 --> 00:08:40,186
you can use it in all sorts of analysis or so that the taxi limousine

145
00:08:40,218 --> 00:08:43,102
Commission in New York can use it for all sorts of analysis as well.

146
00:08:43,156 --> 00:08:45,478
So this is from a number of years ago. And you'll see they have a

147
00:08:45,484 --> 00:08:48,854
whole bunch of different columns here, the vendor id, that is to say,

148
00:08:48,972 --> 00:08:52,646
which company made this computer the pickup daytime and

149
00:08:52,668 --> 00:08:55,046
drop off daytime. We're going to be getting back to this in just a little

150
00:08:55,068 --> 00:08:58,418
bit. Number of passengers, how far did you go in miles?

151
00:08:58,514 --> 00:09:01,906
The pickup longitude and pickup latitude. And these actually have been removed

152
00:09:01,938 --> 00:09:05,210
from more recent data because I think people realize it's kind of creepy to have

153
00:09:05,280 --> 00:09:09,114
this open data, say, where you're going from where you're going to. What rate

154
00:09:09,152 --> 00:09:12,438
were you using? Storm Ford flag I've never quite understood, truth be told, where you're

155
00:09:12,454 --> 00:09:16,074
dropped off, longitude and latitude, how you paid, how much the fare

156
00:09:16,122 --> 00:09:19,706
was, how much extra tax, tips, tools, improvement,

157
00:09:19,738 --> 00:09:23,246
surcharge and then finally the total. So we can read this data in and

158
00:09:23,268 --> 00:09:25,598
if we read it in then we can do some analysis of it. And I

159
00:09:25,604 --> 00:09:28,466
love to do analysis of this in all of my data science courses that I

160
00:09:28,488 --> 00:09:31,586
do for corporations and for individuals. Well, let's read it in. If I say here

161
00:09:31,608 --> 00:09:34,974
DF equals CSV,

162
00:09:35,102 --> 00:09:38,178
it actually gets read in very, very nicely. Right? We can see that

163
00:09:38,184 --> 00:09:41,506
we have the headers here and we have the rows and there's only the has.

164
00:09:41,538 --> 00:09:44,998
That's only going to show me the first five rows in this file. But if

165
00:09:45,004 --> 00:09:48,134
we ask DF count we're going to find out that there are actually

166
00:09:48,252 --> 00:09:51,138
10,000 knows in this file and there are no nan values.

167
00:09:51,154 --> 00:09:54,774
So every single column is going to give us nine, 9910 thousand minus

168
00:09:54,822 --> 00:09:57,754
one. And so what else? We have passenger count and this and this and this

169
00:09:57,792 --> 00:10:01,590
also really great stuff. The thing is, if I now say DF

170
00:10:01,670 --> 00:10:04,974
info, show me pandas, what kind of data do we

171
00:10:05,012 --> 00:10:07,898
have for the pickup and the drop off dates?

172
00:10:07,994 --> 00:10:11,694
We have objects and DF info tells us not about the data

173
00:10:11,732 --> 00:10:14,186
in the data frame, but about the data frame itself. How is it defined,

174
00:10:14,218 --> 00:10:16,718
what does it have? And we see that we have two objects here. Now you

175
00:10:16,724 --> 00:10:19,614
might say, oh, that's fine, the objects are good, but it's not actually that good

176
00:10:19,652 --> 00:10:22,782
because what it means is pandas wasn't able to figure out what kind of data

177
00:10:22,836 --> 00:10:25,926
it was. It's usually able to figure out integers and usually able to figure out

178
00:10:25,948 --> 00:10:28,598
floats. And we see that here. But anything else it just says, oh, that must

179
00:10:28,604 --> 00:10:31,766
be an object and calls it an object that's really a string. So we really

180
00:10:31,788 --> 00:10:34,598
have here is string data. Now I'm going to reduce the number of columns we

181
00:10:34,604 --> 00:10:37,126
have because we don't really care that much but a whole bunch of them for

182
00:10:37,148 --> 00:10:40,370
our purposes today. So I'm going to say t, I think we'll read CSV.

183
00:10:40,450 --> 00:10:43,834
And I'm going to say here. Oh, yeah, I'm going to say here,

184
00:10:43,872 --> 00:10:47,750
use calls equals. And I'm going to say TPF, picked off daytime.

185
00:10:47,830 --> 00:10:50,998
And I'm going to say here, TPF drop off daytime.

186
00:10:51,094 --> 00:10:54,714
I'm going to say here passenger count. And then I'm going to say here,

187
00:10:54,832 --> 00:10:58,702
I'll do trip distance. And I'm going to say here total amount.

188
00:10:58,756 --> 00:11:01,438
These are like the columns that I enjoy playing with. And so now if I

189
00:11:01,444 --> 00:11:04,862
do a DF info, it's at least fewer columns. But we still have the problem

190
00:11:04,916 --> 00:11:07,966
we did before, that these are all objects, these are all strings. Like what are

191
00:11:07,988 --> 00:11:11,594
we going to do now? I need to somehow tell pandas, listen, pandas,

192
00:11:11,642 --> 00:11:14,174
because if we look at it now, it looks like a date and time,

193
00:11:14,212 --> 00:11:16,806
right? I mean it has dates, it has times, but these are being read as

194
00:11:16,828 --> 00:11:19,974
strings. If we want to do any sort of date time manipulation, we're really going

195
00:11:20,012 --> 00:11:23,478
to need to have it read in as date time objects. So how can we

196
00:11:23,484 --> 00:11:25,638
do that? Well, it's not going to guess on its own, so we're going to

197
00:11:25,644 --> 00:11:27,462
need to give it a little bit of a kick. We're going to need to

198
00:11:27,516 --> 00:11:29,206
give it a little bit of a hint. And what we can do is we

199
00:11:29,228 --> 00:11:32,966
can say here our dates equals and I'll say here which columns

200
00:11:32,998 --> 00:11:36,730
I want TP date time and TP

201
00:11:37,710 --> 00:11:41,402
date time. And we do that. And now I look at DF and it looks

202
00:11:41,456 --> 00:11:45,014
exactly the same. The visual part of this or the printed representation

203
00:11:45,062 --> 00:11:48,526
is exactly the same. But if I say DF info, oh, look at

204
00:11:48,548 --> 00:11:51,706
that. Now we see that we have 64 bit dates time objects,

205
00:11:51,738 --> 00:11:54,590
and it's even telling us it's down to the nanosecond, which is kind of nice

206
00:11:54,660 --> 00:11:57,438
because I hate to be late to a meeting by nanosecond. So it's good that

207
00:11:57,444 --> 00:12:00,618
pandas can keep track of this stuff for me. So this is now how we're

208
00:12:00,634 --> 00:12:04,254
going to read it in. Now you might be saying, wait, it's very nice

209
00:12:04,292 --> 00:12:06,374
that when I read the. I'm just going to scroll up here for a moment.

210
00:12:06,412 --> 00:12:09,606
So you can see it's very nice that pandas is able to read this sort

211
00:12:09,628 --> 00:12:12,838
of date format and understand that this is a date. So I

212
00:12:12,844 --> 00:12:16,182
tell parse dates and it says, oh, I see year, month, day,

213
00:12:16,236 --> 00:12:19,394
hour, minute, second. This is indeed a common date format.

214
00:12:19,442 --> 00:12:22,794
It's also one that's very easy to parse. But what

215
00:12:22,832 --> 00:12:25,434
if, just what if I have it in different format? What if I have it

216
00:12:25,472 --> 00:12:28,986
in month day year format, which is pretty common in the

217
00:12:29,008 --> 00:12:32,442
United States. Yeah. Then it might have a harder time understanding it.

218
00:12:32,496 --> 00:12:34,718
It can understand it, but might have a harder time.

219
00:12:34,804 --> 00:12:38,734
Moreover, is it month a year or day month year? Is it

220
00:12:38,772 --> 00:12:42,554
american style or rest of the world style? I should say like european

221
00:12:42,602 --> 00:12:46,786
style, shall we say? Because typically in asian countries they actually use this style of

222
00:12:46,888 --> 00:12:50,994
from biggest to smallest. So year of month, day, hour, minute, second. So if

223
00:12:51,032 --> 00:12:54,338
we say basically, how does it know whether the month come first or the

224
00:12:54,344 --> 00:12:58,370
date comes first? If I have 1220 21,

225
00:12:58,440 --> 00:13:02,086
is it the 1 February or the 2 January? So it turns out that

226
00:13:02,108 --> 00:13:05,158
when I read in a date and time, or when I read anything, if I

227
00:13:05,164 --> 00:13:08,662
do Alpha PD read CSV, there are a ton of different

228
00:13:08,716 --> 00:13:12,166
options that we can provide. One of them, of course, was parse dates. And I

229
00:13:12,188 --> 00:13:14,246
gave it, told it which ones to do. But there are a whole bunch of

230
00:13:14,268 --> 00:13:17,046
other things that have to do with dates, and one of them here is.

231
00:13:17,068 --> 00:13:20,346
Here we go. Parse dates. Okay, that's good. Equals false infer

232
00:13:20,378 --> 00:13:23,514
daytime format. Like should it assume that it knows the format?

233
00:13:23,642 --> 00:13:26,286
If it doesn't infer it, then what are we going to do? Well, we can

234
00:13:26,308 --> 00:13:29,530
play with this a little bit. Keep date, call date parser.

235
00:13:29,610 --> 00:13:33,918
And this is where you can, if you really want, to, provide a custom function

236
00:13:34,004 --> 00:13:36,846
that will read in the date and then spit out a daytime object based on

237
00:13:36,868 --> 00:13:41,066
its parsing. And this is what I was talking about here. Day first equals false

238
00:13:41,178 --> 00:13:44,950
meaning. Is the first number there the date, or is the month?

239
00:13:44,980 --> 00:13:47,938
Month. So day first equals false means? No, it's the month. So if you're reading

240
00:13:47,954 --> 00:13:50,918
in european dates, you're going to have to say day first equals true. Okay,

241
00:13:51,004 --> 00:13:53,506
fine. Then we got all sorts of other stuff there, but we're going to ignore

242
00:13:53,538 --> 00:13:56,630
those for now. So I've successfully read this data in

243
00:13:56,700 --> 00:13:59,718
and knows I can say DF info and we see that it's dates and I

244
00:13:59,724 --> 00:14:02,746
say df head ten and we see that it states and times.

245
00:14:02,848 --> 00:14:06,780
Fantastic. Well, now how does that help me?

246
00:14:07,470 --> 00:14:10,950
What have I gotten from this? That these are actually date and time objects?

247
00:14:11,030 --> 00:14:14,686
Well, I can now perform all sorts of calculations on them. For example,

248
00:14:14,788 --> 00:14:18,462
maybe I want to find out what was the hour at which

249
00:14:18,516 --> 00:14:21,710
each of these trips took place. How can I find that out?

250
00:14:21,780 --> 00:14:28,974
Well, I can say DF or TPAP date

251
00:14:29,012 --> 00:14:32,318
time, and I run that and we get here, that column, that's not

252
00:14:32,324 --> 00:14:35,446
a big surprise. Wouldn't it be nice if I could sort of go through each

253
00:14:35,468 --> 00:14:38,038
of these and say, I just want to pull out the hour, I just want

254
00:14:38,044 --> 00:14:40,546
to pull out the minute, I just want to pull out the month. All reasonable,

255
00:14:40,578 --> 00:14:43,346
right? Things that we would want to do with dates and times. So it turns

256
00:14:43,378 --> 00:14:46,006
out that there's a DT accessor object.

257
00:14:46,188 --> 00:14:49,254
This is similar to the stir accessor object that you might be familiar with.

258
00:14:49,292 --> 00:14:52,742
And this allows me to pull out date and time information. So if I say

259
00:14:52,796 --> 00:14:56,198
DT now, we're going to the daytime accessor now, it has its own attributes that

260
00:14:56,204 --> 00:14:58,966
I can use to retrieve stuff. If I say DT hour, and look at that,

261
00:14:58,988 --> 00:15:02,686
we're pulling out the hour, I can say DT day of week, and it'll tell

262
00:15:02,708 --> 00:15:05,098
you what day of the week it is. I can say DT month, and I'll

263
00:15:05,114 --> 00:15:08,286
look at that, I can say year. That'll allow me to do it. Yeah.

264
00:15:08,308 --> 00:15:11,806
Is leap year, is order end. So I

265
00:15:11,828 --> 00:15:15,998
can basically pull in all sorts of really useful, interesting information

266
00:15:16,164 --> 00:15:19,666
based on this DT accessor. All right? And so the DT accessor has a whole

267
00:15:19,688 --> 00:15:22,290
bunch of attributes that allow us to do these. Now you might be saying,

268
00:15:22,360 --> 00:15:26,206
okay, I can understand hour, I can understand minute, but what's with this is quarter

269
00:15:26,238 --> 00:15:29,330
end. Why in the world would I care if it's the quarter end or not?

270
00:15:29,400 --> 00:15:32,694
And the answer is that, well, you have to remember that pandas was developed in

271
00:15:32,732 --> 00:15:36,262
an investment firm, and so they were trying to do all sorts of, or actually

272
00:15:36,316 --> 00:15:40,070
Wes McKinney, who invented pandas, was trying to figure out, how can I use

273
00:15:40,140 --> 00:15:43,522
Python to perform these sorts of calculations that are typically,

274
00:15:43,586 --> 00:15:46,834
traditionally done, say, in Excel? And if you are working in investments,

275
00:15:46,882 --> 00:15:49,366
then you want to know, is the month end? Is the month beginning? Is it

276
00:15:49,388 --> 00:15:52,478
a holiday? Is other quarter end? And so there are all sorts of attributes you

277
00:15:52,484 --> 00:15:56,206
can use to pull information out like that from the DT accessor. And then

278
00:15:56,228 --> 00:15:58,858
you'll either get a number out, as we did for a K minute, or you'll

279
00:15:58,874 --> 00:16:01,326
get a true false value. You have to decide what you're going to do with

280
00:16:01,348 --> 00:16:04,686
that. So how can I use this? Well, maybe I

281
00:16:04,708 --> 00:16:07,218
want to know what is the distribution like? I have all this data here.

282
00:16:07,224 --> 00:16:10,558
I have 10,000 taxi rides. Is this a reasonable distribution

283
00:16:10,654 --> 00:16:13,858
of hours across the day? Is this a reasonable distribution of

284
00:16:13,864 --> 00:16:17,460
dates across the week? So let's find out. I'm going to say here DT hour.

285
00:16:18,950 --> 00:16:22,262
Let's find out here DT hour. Let's do a value count,

286
00:16:22,396 --> 00:16:26,134
because remember, what I'm getting back here is a series. So because I'm getting

287
00:16:26,172 --> 00:16:29,714
a series back, I can apply the value counts method and thus

288
00:16:29,762 --> 00:16:32,794
get back what's basically a count of how often each of these

289
00:16:32,832 --> 00:16:36,774
values appears. And you can see very clearly here that this is not a representative

290
00:16:36,822 --> 00:16:40,954
sample, that we have a whole lot of trips from 11:00 a.m. Some from 03:00

291
00:16:40,992 --> 00:16:44,490
p.m. Some from midnight and a handful from 04:00 p.m.

292
00:16:44,560 --> 00:16:47,550
So if you're trying to make any sorts of assertions based on this data,

293
00:16:47,620 --> 00:16:51,214
they're probably going to be fundamentally flawed because they don't actually have anything

294
00:16:51,252 --> 00:16:54,526
to do with, it's not a real distribution of data. So you might

295
00:16:54,548 --> 00:16:57,218
be making all sorts of mistakes. What about days of the week? Can we do

296
00:16:57,224 --> 00:17:01,074
that? Well, I'm sure I could check here day of week and we get value

297
00:17:01,112 --> 00:17:04,638
counts and we see that once again the distribution is totally wacky.

298
00:17:04,734 --> 00:17:08,866
You can't really make any sorts of serious decisions based

299
00:17:08,888 --> 00:17:12,246
on the data here. Or if you do make decisions, you'd better go check them

300
00:17:12,348 --> 00:17:16,310
based on actual more representative data so we can find out this sort of information,

301
00:17:16,380 --> 00:17:20,266
then plot it accordingly, or just make assertions accordingly. Or we can pull things

302
00:17:20,288 --> 00:17:24,358
out based on that. So I can say, for example, DF types

303
00:17:24,454 --> 00:17:27,658
pickup daytime DT day of

304
00:17:27,744 --> 00:17:31,146
week equals, equals one. That's going to give me, of course, true or

305
00:17:31,168 --> 00:17:34,366
false. I can use that as a boolean index into DF. And then I

306
00:17:34,388 --> 00:17:37,742
get back only those rows in which the day of week

307
00:17:37,796 --> 00:17:40,910
was one, meaning Monday. And then I can even use this,

308
00:17:40,980 --> 00:17:44,110
right, this is like a good start, but I can even say, let's apply

309
00:17:44,180 --> 00:17:47,338
that actually just to total amount. And then I can get

310
00:17:47,364 --> 00:17:50,340
that. So I can find, but what's the total amount for this day of week?

311
00:17:50,710 --> 00:17:54,446
How much do people pay on average on Monday versus, say Sunday?

312
00:17:54,558 --> 00:17:57,906
I can pull this out here and say day of week zero and get

313
00:17:57,928 --> 00:18:01,478
that out as well. We see that basically on Sundays they paid more. Can we

314
00:18:01,484 --> 00:18:04,822
really say that? I don't really think so, given this data, but given

315
00:18:04,876 --> 00:18:08,694
real data, you absolutely could. So that's like an initial view

316
00:18:08,732 --> 00:18:11,106
of what we can do with date and time data. And for many people that's

317
00:18:11,138 --> 00:18:14,038
great. That's enough, because basically we can use that and then we can do all

318
00:18:14,044 --> 00:18:16,918
sorts of analysis. I can find out on what day of the week, on what

319
00:18:16,924 --> 00:18:19,462
day of the month, at what hour of the day, did people buy the most

320
00:18:19,516 --> 00:18:22,846
from our online store or do they buy the lease? Or then

321
00:18:22,868 --> 00:18:25,278
we can sort of juice it with oh, let's give them some coupons during the

322
00:18:25,284 --> 00:18:28,878
dead time. Sort of like a happy hour at a bar except that it's online

323
00:18:28,964 --> 00:18:32,346
so less physical drinking. At least we're not spreading

324
00:18:32,378 --> 00:18:35,582
across the network there. But maybe I want to find

325
00:18:35,636 --> 00:18:39,234
out not at what hour did things take place or what day did things take

326
00:18:39,272 --> 00:18:42,370
place. Maybe I want to find out how long different trips were.

327
00:18:42,440 --> 00:18:45,694
Well, we saw already before that. If I have a date time and another daytime

328
00:18:45,742 --> 00:18:49,234
I can subtract them one from the other and I will get a time delta.

329
00:18:49,282 --> 00:18:52,482
Guess what? We can do that here. So I can say df of TP drop

330
00:18:52,546 --> 00:18:56,390
off date time minus DF types pickup

331
00:18:58,330 --> 00:19:01,334
date time. And if I subtract those, oops, I didn't mean to do that.

332
00:19:01,372 --> 00:19:04,874
I did an e. I meant to do minus. Let's go reload our

333
00:19:04,992 --> 00:19:08,698
file again. Read CSV. Now I'm going to go down to the bottom.

334
00:19:08,784 --> 00:19:12,426
I'm going to say minus not equals. There we go. And what do we

335
00:19:12,448 --> 00:19:16,202
see? We actually see a time delta object for each

336
00:19:16,256 --> 00:19:19,918
of our elements. And you see here the dtype is indeed time Delta. Well what

337
00:19:19,924 --> 00:19:22,202
I'm going to do now is I'm going to create a new column. Say df

338
00:19:22,266 --> 00:19:26,382
of trip time equals equal sign.

339
00:19:26,436 --> 00:19:29,726
Seems to be like really for your happy today. And now if I say df

340
00:19:29,758 --> 00:19:32,622
head look what we're going to get and what we have then is the pickup

341
00:19:32,686 --> 00:19:35,826
drop off pass, recount trip distance, total amount and then we have the trip time

342
00:19:35,928 --> 00:19:39,266
which is pretty great. So now I can find out what was the trip time.

343
00:19:39,288 --> 00:19:42,246
Well how am I going to search for that? I could say like ds of

344
00:19:42,268 --> 00:19:45,960
trip time equals equals. And we could say like what zero day

345
00:19:46,330 --> 00:19:49,778
28 23. Could I do that? I can. Right. And it'll

346
00:19:49,794 --> 00:19:52,250
find all the ones that were like that which is going to be one.

347
00:19:52,320 --> 00:19:55,114
So that's like one way to do it. But here's the thing. I can also

348
00:19:55,152 --> 00:19:58,682
do comparisons. I can say DS of trip time is less

349
00:19:58,736 --> 00:20:04,214
or let's say less than and I can say meaning

350
00:20:04,262 --> 00:20:07,278
how many trips were less than 1 hour in length. And this gives us a

351
00:20:07,284 --> 00:20:10,186
boolean index or boolean series which we can apply as an index.

352
00:20:10,218 --> 00:20:13,566
So this will give us all the rides that took less than 1 hour which

353
00:20:13,588 --> 00:20:17,090
we'd hope is quite a lot. How many rides took less than 1 minute?

354
00:20:18,950 --> 00:20:22,338
We can search for that too and we'll find you'd be surprised. 84 of the

355
00:20:22,344 --> 00:20:25,906
rides out of our 10,000 actually took less than 1 minute. Now if

356
00:20:25,928 --> 00:20:29,026
you're thinking this is kind of weird, how am I comparing this

357
00:20:29,048 --> 00:20:32,338
dates? Time object or this time delt object, I should say with a string.

358
00:20:32,434 --> 00:20:35,430
Well, that's because they're automatically doing some conversions for us.

359
00:20:35,500 --> 00:20:38,258
That's because pandas are smart enough to say, wait, if I have a time delta

360
00:20:38,274 --> 00:20:41,954
here and I have a string here, maybe, just maybe, I should do the comparison.

361
00:20:42,002 --> 00:20:45,414
It turns the string into a time delta and then it can do the comparison,

362
00:20:45,462 --> 00:20:49,082
which is pretty snazzy. I can even use some words. I can say like,

363
00:20:49,136 --> 00:20:52,906
hey, not, I can say 1 minute or I can

364
00:20:52,928 --> 00:20:55,978
say 1 second. Of course I can make, by the way,

365
00:20:56,064 --> 00:20:58,574
look at that, there are a whole bunch of them that took less than 1

366
00:20:58,612 --> 00:21:01,502
second. And you thought traffic in New York was always terrible, right?

367
00:21:01,556 --> 00:21:04,158
I can also say like how many of them were more than, oh, I don't

368
00:21:04,164 --> 00:21:06,942
know, 10 hours. Were there any taxi rides that were more than 10 hours?

369
00:21:06,996 --> 00:21:10,498
Yes, there were. There was one ride that actually took twelve and a

370
00:21:10,504 --> 00:21:14,258
half hours. So much for no traffic, right? But it's okay, they only had to

371
00:21:14,264 --> 00:21:17,734
pay $11 for the privilege and they only went 1 mile. So I guess actually

372
00:21:17,772 --> 00:21:21,058
the traffic was pretty bad. Or this is representing some data that's

373
00:21:21,074 --> 00:21:24,630
a little, shall we say, not reliable. So we can

374
00:21:24,700 --> 00:21:27,766
use timestamp objects, we can read them in,

375
00:21:27,788 --> 00:21:31,282
we can use them, extract from them, but we can also use time delta objects

376
00:21:31,346 --> 00:21:34,154
and see how they compare them on our own and look for them in different

377
00:21:34,192 --> 00:21:37,866
ways. So these are two major ways that we can use dates and times in

378
00:21:37,888 --> 00:21:40,826
pandas. But it gets better than that. As you might know,

379
00:21:40,928 --> 00:21:44,426
pandas indexes are really powerful. And right now what we have

380
00:21:44,448 --> 00:21:46,766
is just a bunch of numbers, right? There's nothing wrong with that per se.

381
00:21:46,788 --> 00:21:50,206
But if I say types head, let's say 20,

382
00:21:50,308 --> 00:21:53,118
we're going to see that the indexes are zero through 20. And that's because we

383
00:21:53,124 --> 00:21:56,814
didn't set them to be anything special. And the default index is just integers from

384
00:21:56,852 --> 00:22:00,158
zero to whatever. But one of the nice things about pandas indexes is that we

385
00:22:00,164 --> 00:22:02,878
can set them to be anything we want, more or less. They can be just

386
00:22:02,884 --> 00:22:05,958
about any data type and then we can retrieve based on those. And what we

387
00:22:05,964 --> 00:22:09,330
can do then is we can say, hey, I actually want one of these columns

388
00:22:09,410 --> 00:22:12,758
to be the index. And we call that a time series. So what

389
00:22:12,764 --> 00:22:16,434
I can do is I can say df set index, bDF tpad

390
00:22:16,482 --> 00:22:20,022
let's say pickup date time. And if I do that, look what I get back,

391
00:22:20,076 --> 00:22:23,066
I get back a data frame that's identical to what we had before, except that

392
00:22:23,088 --> 00:22:27,078
now the index is that column. And you can even see sort of this historical

393
00:22:27,174 --> 00:22:30,362
remnant here. We know that the column was originally called that,

394
00:22:30,416 --> 00:22:33,626
even though indexes aren't really called by name. There's just one problem with what

395
00:22:33,648 --> 00:22:36,186
I did here, which is that I did not actually change the data frame.

396
00:22:36,218 --> 00:22:39,406
This returned a new data frame. And so if I want to change it,

397
00:22:39,428 --> 00:22:43,066
it's really tempting to use in place equals true, but don't do that. The pandas

398
00:22:43,098 --> 00:22:46,206
core developers have told us now for a few years that's not the right way

399
00:22:46,228 --> 00:22:49,246
to do it. Instead, what you should do is say DF equals that. And what

400
00:22:49,268 --> 00:22:52,046
I'm doing is I'm then taking the new data frame that I got back and

401
00:22:52,068 --> 00:22:55,462
assigning it to DF. And now if I say DF head, sure enough,

402
00:22:55,516 --> 00:22:58,870
we have changed. Okay, that's nice. So what?

403
00:22:58,940 --> 00:23:02,518
Well, actually, remember that we can retrieve things based on an index. That's sort

404
00:23:02,524 --> 00:23:06,534
of the whole point. So I can say DF lock of 20 15

405
00:23:06,732 --> 00:23:09,910
62 11 19 29.

406
00:23:10,060 --> 00:23:13,798
And remember also that indexes and pandas are not guaranteed to be unique.

407
00:23:13,894 --> 00:23:16,970
Now, this might sound weird, but it means I can pull out all of

408
00:23:17,040 --> 00:23:20,778
the trips that took place on the 2 June 2015

409
00:23:20,864 --> 00:23:24,074
at 1119 and 29 seconds. And we see that there were three trips that

410
00:23:24,112 --> 00:23:27,374
started at exactly that time. But it gets better than that, because if I now

411
00:23:27,412 --> 00:23:31,034
use not the entire timestamp, but part of it, if I chop

412
00:23:31,082 --> 00:23:34,922
off the seconds from there, now I get everything that matches

413
00:23:34,986 --> 00:23:38,654
any seconds for those minutes. It's like having a star there. Obviously,

414
00:23:38,692 --> 00:23:41,906
we don't use a star. And so now we see it's 146 trips that took

415
00:23:41,928 --> 00:23:45,182
place on that day, at that time, at that minute, but any seconds.

416
00:23:45,246 --> 00:23:48,578
So sometimes it's 56, sometimes it's 59, sometimes it's 30th. Could be

417
00:23:48,584 --> 00:23:51,494
any of those. And we can keep doing this. I want to find out everything

418
00:23:51,532 --> 00:23:54,614
that happened at 11:00 a.m. Any minute. I want to find out anything that happened

419
00:23:54,652 --> 00:23:58,470
on that date, anytime. And so this allows me to really sort of find

420
00:23:58,540 --> 00:24:01,746
stuff based on those dates and times with a lot

421
00:24:01,788 --> 00:24:05,098
of flexibility, which is fantastic. The other thing I can do is I

422
00:24:05,104 --> 00:24:09,034
can use a slice so I can say DF lock of 2015,

423
00:24:09,152 --> 00:24:12,246
let's say 6211,

424
00:24:12,358 --> 00:24:17,322
1929, all in. And then we'll say 2015 6211,

425
00:24:17,386 --> 00:24:21,246
let's say 29, 29. Actually, let's even say like 2029.

426
00:24:21,268 --> 00:24:24,846
So just 1 minute and this will now give us all the rights that took

427
00:24:24,868 --> 00:24:28,542
place in that 1 minute interval from 1119 29 until

428
00:24:28,596 --> 00:24:32,446
11 20 29. Notice it'll also be up to and including because

429
00:24:32,548 --> 00:24:36,166
when you do a slice in, pandas don't have integers or

430
00:24:36,188 --> 00:24:39,094
you don't have numbers, I should say actually I guess it's just integers then.

431
00:24:39,132 --> 00:24:42,054
It's always up to and including as opposed to what integers is up to and

432
00:24:42,092 --> 00:24:45,206
not including. So this is actually a really powerful technique as well.

433
00:24:45,308 --> 00:24:48,454
So if you're looking to find out, hey, show me during this

434
00:24:48,492 --> 00:24:51,766
interval, what was the average trip distance? Well,

435
00:24:51,788 --> 00:24:55,466
I can do that. I can just say here dflock of that trip distance and

436
00:24:55,488 --> 00:24:58,634
I can say mean and we'll find out what was the mean trip distance during

437
00:24:58,672 --> 00:25:01,738
this little slice of time. But you know what might be fun to do,

438
00:25:01,824 --> 00:25:05,306
or even useful, not just fun, is to say I want to

439
00:25:05,328 --> 00:25:09,326
find out from the first timestamp we have to the

440
00:25:09,348 --> 00:25:11,658
last one, we know that it's not going to be continuous because we know it's

441
00:25:11,674 --> 00:25:13,454
going to be chopped up and we have some data. We don't have the data.

442
00:25:13,492 --> 00:25:16,962
So from the earliest one until the latest one in let's say 1 hour

443
00:25:17,016 --> 00:25:20,814
intervals for each of those hours, what was the average distance?

444
00:25:20,862 --> 00:25:24,114
Or what was the average total fare? Total amount. Can I do that?

445
00:25:24,152 --> 00:25:28,138
Yes, I can, because now that I've set my index

446
00:25:28,174 --> 00:25:31,526
to be a timestamp, now that we have its time series knows I

447
00:25:31,548 --> 00:25:34,978
can say df resample, and resample allows

448
00:25:34,994 --> 00:25:38,598
me to say I want to go based on 1 hour. So again, it's going

449
00:25:38,604 --> 00:25:41,846
to take the earliest time, the latest time and find all the times in between

450
00:25:41,948 --> 00:25:45,218
in 1 hour blocks. There might be no data for a bunch

451
00:25:45,234 --> 00:25:48,006
of those, and that's okay, in which case we'll just get none. So if I

452
00:25:48,028 --> 00:25:51,642
resample, look what we get back. We get the date time index resampler. Very exciting.

453
00:25:51,706 --> 00:25:53,966
And where do I know that it's one h from? Because I looked at the

454
00:25:53,988 --> 00:25:57,358
documentation and quite frankly, except for a few of them, I have to look it

455
00:25:57,364 --> 00:26:00,558
up for all of them. So okay, one h. Now what? Now I can say

456
00:26:00,644 --> 00:26:04,222
show me. Let's say trip distance mean.

457
00:26:04,356 --> 00:26:07,974
And look at that. I now have the mean trip distance

458
00:26:08,122 --> 00:26:11,438
for every hour from the earliest time to the latest time. Now it's

459
00:26:11,454 --> 00:26:14,818
not showing me everything because pandas traditionally doesn't show me everything and there

460
00:26:14,824 --> 00:26:17,218
are a lot of Nan values. So once I get back to the series I

461
00:26:17,224 --> 00:26:19,766
can actually drop na, you're going to see, it's going to go back to be

462
00:26:19,788 --> 00:26:23,398
those 4 hours that I had before. Okay, not so exciting. That's fine. But what

463
00:26:23,404 --> 00:26:27,234
if I were to say, like for example, one, I think lowercase

464
00:26:27,282 --> 00:26:30,454
M is minute. No, it's month. But you can use here

465
00:26:30,492 --> 00:26:33,370
any number you want. You can use here any of the extractors that you want.

466
00:26:33,440 --> 00:26:36,554
I know about hour, I know about day, again,

467
00:26:36,672 --> 00:26:39,818
the others, I have to just sort of look up because there's just so many.

468
00:26:39,904 --> 00:26:42,314
And so you can play, I can say here, well, what about like eight hour

469
00:26:42,352 --> 00:26:46,254
intervals or twelve hour intervals? Or I can say in each even

470
00:26:46,372 --> 00:26:49,838
twelve hour interval, how much did people go? How far did people go? Or how

471
00:26:49,844 --> 00:26:52,826
much did they pay? By the way, if I take off that trip distance,

472
00:26:52,938 --> 00:26:56,458
let's say I do it, apply to the entire data frame. And there we go.

473
00:26:56,484 --> 00:26:59,966
I'm going to for each of the columns and then for each of these timestamps.

474
00:27:00,078 --> 00:27:03,966
And so you can do this with resampling on any interval

475
00:27:03,998 --> 00:27:06,930
you want. You can pull out columns if you want, but you don't have to.

476
00:27:07,000 --> 00:27:10,358
And then you can use any aggregation function that you want as well. And the

477
00:27:10,364 --> 00:27:12,914
aggregation functions are min, max,

478
00:27:13,042 --> 00:27:16,278
mean, standard deviation count. And I'm sure

479
00:27:16,284 --> 00:27:19,446
there's some others, but those are the main ones that we would use. And so

480
00:27:19,548 --> 00:27:22,918
again, they have to aggregate, they have to give us a full answer over a

481
00:27:22,924 --> 00:27:26,102
whole bunch of things, but allows it to split up by time. And the fact

482
00:27:26,156 --> 00:27:28,646
that it doesn't care where we have data, it's just going to sort of go

483
00:27:28,668 --> 00:27:31,546
from that earliest to latest makes it especially useful.

484
00:27:31,658 --> 00:27:35,306
Okay, I hope that you enjoyed this talk. I hope it was useful, mind opening

485
00:27:35,338 --> 00:27:38,686
and so forth. Once again, I would be delighted to hear from you. If you

486
00:27:38,708 --> 00:27:42,078
have comments or questions about this talk, I can also send you

487
00:27:42,084 --> 00:27:45,518
a copy of the Jupiter notebook that I used for this. So you can just

488
00:27:45,524 --> 00:27:49,034
drop me a line. You can always catch me on Twitter as learner,

489
00:27:49,082 --> 00:27:52,766
or you can catch me via email at Reuven M. Lerner Coil. And once

490
00:27:52,788 --> 00:27:56,482
again, my newsletter, better developers, goes out every week to about 20,000 developers

491
00:27:56,546 --> 00:27:58,966
and I would love to have you join and years with me with a new

492
00:27:58,988 --> 00:28:02,406
article about Python free every Monday. Thanks so much for watching my

493
00:28:02,428 --> 00:28:05,126
talk. I hope you enjoyed and I hope to hear from you. Enjoy the rest

494
00:28:05,148 --> 00:28:05,490
of the conference.

