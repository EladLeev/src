1
00:00:21,930 --> 00:00:25,862
Hello, I'm Karl Weinmeister with Google's developer app SC team.

2
00:00:25,996 --> 00:00:29,938
Today we're going to talk about ten things that can go wrong with ML projects.

3
00:00:30,114 --> 00:00:33,366
Let's get started. So machine learning practitioners are

4
00:00:33,388 --> 00:00:36,534
solving important problems every day, running into a

5
00:00:36,572 --> 00:00:40,210
set of unique challenges. Today we're going to talk about the best practices

6
00:00:40,290 --> 00:00:44,258
and tools that can help address them. The issues we're

7
00:00:44,274 --> 00:00:47,798
going to discuss fall into four categories, building a model,

8
00:00:47,964 --> 00:00:50,890
model accuracy, transparency and fairness,

9
00:00:50,970 --> 00:00:54,400
and mlops. So let's start with our first problem.

10
00:00:55,090 --> 00:00:58,298
It's all about these business problem you're trying to solve.

11
00:00:58,394 --> 00:01:02,158
So many organizations are transforming and

12
00:01:02,324 --> 00:01:05,706
really changing how they do things with machine learning, but we see

13
00:01:05,748 --> 00:01:09,490
other companies that are really struggling to get value out of those machine learning

14
00:01:09,560 --> 00:01:12,674
projects. So it's key that your machine learning model is

15
00:01:12,712 --> 00:01:16,210
aligned with your goals. It's also important that

16
00:01:16,280 --> 00:01:19,938
when you're figuring out if you're doing well, you have a baseline

17
00:01:20,114 --> 00:01:24,134
where you can evaluate how your model is doing. You need to know

18
00:01:24,332 --> 00:01:28,114
how existing approaches are working, whether they're manual,

19
00:01:28,242 --> 00:01:32,194
whether they're implemented by traditional software development

20
00:01:32,242 --> 00:01:35,658
systems, or in a previous form of machine learning.

21
00:01:35,744 --> 00:01:38,982
You need to know what your starting point is, to know how much you're improving

22
00:01:39,046 --> 00:01:42,602
based on that. So it's key to know what that baseline is and the goals.

23
00:01:42,666 --> 00:01:46,142
So let's talk a little more about that. So I recommend watching

24
00:01:46,196 --> 00:01:50,718
this video coming from DeepMind, Google's research

25
00:01:50,884 --> 00:01:54,554
organization, where there's a great video on product management

26
00:01:54,602 --> 00:01:58,414
for AI and the speaker talks about a lot of topics.

27
00:01:58,462 --> 00:02:01,794
It's about a 30 minutes video. Some of these key takeaways that I took from

28
00:02:01,832 --> 00:02:05,358
that were staying focused on your tools

29
00:02:05,454 --> 00:02:08,370
as a project, but being flexible on the tactics.

30
00:02:08,450 --> 00:02:12,466
It's inevitable that you're going to run into things that don't

31
00:02:12,498 --> 00:02:16,274
work along the way. So you just need to keep adapting but not losing sight

32
00:02:16,322 --> 00:02:20,182
of what your ML solution is aiming to achieve

33
00:02:20,246 --> 00:02:23,782
for your users. Secondly, from a scheduling

34
00:02:23,846 --> 00:02:27,766
perspective, it can be hard with machine learning because it's

35
00:02:27,798 --> 00:02:31,542
often a large research and discovery. Part of the mission

36
00:02:31,606 --> 00:02:35,694
is where you start things, where you're not sure exactly how things

37
00:02:35,732 --> 00:02:38,238
are going to work. You might not have an answer.

38
00:02:38,404 --> 00:02:42,746
You're learning a lot as you go, and that's hard to plan around. So setting

39
00:02:42,778 --> 00:02:46,690
milestones that you can adapt and change

40
00:02:46,760 --> 00:02:50,594
as you go is fine, but it's important to plan things

41
00:02:50,632 --> 00:02:53,966
out with milestones as you go to sort of have some structure

42
00:02:53,998 --> 00:02:57,514
on your project. Finally, having users involved

43
00:02:57,582 --> 00:03:00,966
at each stage of your project is important. So you

44
00:03:00,988 --> 00:03:04,342
may still be doing a lot of research work, but having

45
00:03:04,396 --> 00:03:08,166
some early insights and lessons to share with your users to

46
00:03:08,188 --> 00:03:11,466
ensure you're on track is often very valuable. So some

47
00:03:11,488 --> 00:03:15,174
of these insights will help ensure that you're getting value out of your AI

48
00:03:15,222 --> 00:03:19,098
projects. The next thing I want to focus on here is

49
00:03:19,184 --> 00:03:22,042
that your problem needs to be a good fit for machine learning.

50
00:03:22,176 --> 00:03:25,854
This isn't an exhaustive list, but here are a few things that

51
00:03:25,892 --> 00:03:29,358
you might want to keep in mind when you're wondering, is this an

52
00:03:29,444 --> 00:03:32,654
area where machine learning can help? The first group is

53
00:03:32,692 --> 00:03:36,142
predictive analytics, and these are problems. Theyre you have historical

54
00:03:36,206 --> 00:03:39,346
data and you want to look at trends that are going to happen in the

55
00:03:39,368 --> 00:03:42,146
future. Whether it's looking at past,

56
00:03:42,328 --> 00:03:45,826
say, transactions and trying to figure out if a new one

57
00:03:45,848 --> 00:03:49,106
is fraud, looking for equipment,

58
00:03:49,138 --> 00:03:53,014
if it's going to fail over time by looking at information about

59
00:03:53,052 --> 00:03:56,294
heat or vibration, et cetera, being able to extract those

60
00:03:56,332 --> 00:04:00,130
patterns for when the systems are going to fail, being able to fix them beforehand.

61
00:04:00,210 --> 00:04:03,766
There's all kinds of situations like this. We're using historical data to predict

62
00:04:03,798 --> 00:04:07,306
some new future. There's another class of problems around unstructured data.

63
00:04:07,408 --> 00:04:11,162
This is where you have data that's not in the tabular

64
00:04:11,226 --> 00:04:14,926
format that fits into, say, databases. This is where you

65
00:04:14,948 --> 00:04:18,654
have images, text, et cetera. There's a

66
00:04:18,692 --> 00:04:22,314
variety of different use cases here, but just a few examples

67
00:04:22,362 --> 00:04:26,258
of things like maybe triaging emails if

68
00:04:26,264 --> 00:04:30,066
there's a large load for the customer service organization, being able to

69
00:04:30,248 --> 00:04:33,474
cluster and move items to the right

70
00:04:33,512 --> 00:04:36,854
place. Another area could be automation, where you have

71
00:04:36,892 --> 00:04:40,450
a manual process and you're trying to automatically fulfill

72
00:04:40,530 --> 00:04:43,782
some step of that process, and you see a few examples of that.

73
00:04:43,916 --> 00:04:47,826
Finally, personalization, where you want to understand how your

74
00:04:47,868 --> 00:04:51,658
users tick and you want to be able to provide

75
00:04:51,744 --> 00:04:55,674
them useful information. Next steps that help them

76
00:04:55,872 --> 00:04:59,574
in your application to achieve what they're trying to do faster,

77
00:04:59,622 --> 00:05:02,926
easier, et cetera. All right, so let's move on to the next

78
00:05:02,948 --> 00:05:07,022
part of building a model. A huge problem can be jumping straight into

79
00:05:07,076 --> 00:05:10,446
model development with these prototype. A machine learning project

80
00:05:10,548 --> 00:05:14,354
is an iterative process, so you start with something simple and

81
00:05:14,392 --> 00:05:17,842
you refine it as you go, and many times something

82
00:05:17,896 --> 00:05:21,918
simple if it achieves your goals, is fine, but it's

83
00:05:21,934 --> 00:05:25,774
always good to start small and expand from there. A quick prototype

84
00:05:25,822 --> 00:05:29,422
can tell you a lot about challenges. Those could be access

85
00:05:29,496 --> 00:05:33,106
to data as you start to build out that initial model. Maybe there are teams

86
00:05:33,138 --> 00:05:36,822
you need to work with to request data or integration points

87
00:05:36,876 --> 00:05:40,938
that you weren't aware of. Maybe you really struggle with getting

88
00:05:41,104 --> 00:05:44,806
a decent model accuracy. There's all kinds

89
00:05:44,838 --> 00:05:48,586
of other questions you can find out and that will help with scoping out

90
00:05:48,608 --> 00:05:53,306
the length of the project and be able to understand what you're

91
00:05:53,338 --> 00:05:56,574
getting into. Starting with that prototype, there are

92
00:05:56,612 --> 00:06:00,590
a couple of tools that can help. So let's first start with Bigquery ML.

93
00:06:00,930 --> 00:06:04,766
So Bigquery ML allows you to create machine

94
00:06:04,798 --> 00:06:07,982
learning models directly from the data warehouse in Bigquery.

95
00:06:08,046 --> 00:06:11,378
So you'll see a little bit in this animation where you can

96
00:06:11,384 --> 00:06:15,134
write some SQL statements for deep neural network

97
00:06:15,182 --> 00:06:18,638
models, logistic regression, all these different

98
00:06:18,664 --> 00:06:21,926
types of models, even time series forecasting. It allows you

99
00:06:21,948 --> 00:06:25,874
to get started quickly. By the way, this is a full fledged production

100
00:06:25,922 --> 00:06:29,078
system as well. If you want to run your models out

101
00:06:29,084 --> 00:06:32,486
of bigquery completely, a great option for that,

102
00:06:32,508 --> 00:06:35,898
but allows you to move quickly, try some things out, see what your

103
00:06:35,904 --> 00:06:38,630
baseline accuracy is. Similarly,

104
00:06:38,710 --> 00:06:42,074
AutomL is another great option. If you want to write

105
00:06:42,112 --> 00:06:46,058
a custom model, theyre you take your own data. Use AutoML to handle

106
00:06:46,074 --> 00:06:49,354
the training, deployment and serving steps, and then generating a rest API.

107
00:06:49,402 --> 00:06:52,874
It can wrap all that into one user

108
00:06:52,922 --> 00:06:56,354
interface or SDK to enable you to do all these steps quickly

109
00:06:56,472 --> 00:06:59,774
so that can serve as performance baseline.

110
00:06:59,902 --> 00:07:04,126
It also can help with explainability where you can get feature importances

111
00:07:04,158 --> 00:07:08,046
so you can know maybe where AutoML is focusing in theyre

112
00:07:08,088 --> 00:07:11,654
there's signal. Maybe that's an area where with your data engineering, you can dig

113
00:07:11,692 --> 00:07:15,318
in a little bit to extract some more features from it.

114
00:07:15,484 --> 00:07:18,614
Problem number three, model training taking a long time.

115
00:07:18,732 --> 00:07:22,234
Not sure if you've ever run into this. If you're working with a larger model

116
00:07:22,352 --> 00:07:26,154
where it can take days or even weeks in some

117
00:07:26,192 --> 00:07:30,074
cases to run, and that just really mlops down your team where

118
00:07:30,112 --> 00:07:34,010
you might reach a point where you just have to wait until tomorrow

119
00:07:34,090 --> 00:07:38,222
to find out what you can do next. So that slows down that process of

120
00:07:38,276 --> 00:07:43,326
innovation and can really harm

121
00:07:43,438 --> 00:07:47,134
the success of your project. Serverless training with Vertex

122
00:07:47,182 --> 00:07:50,974
AI can be very helpful for this because it allows you to submit

123
00:07:51,022 --> 00:07:55,560
training jobs across a distributed infrastructure using

124
00:07:56,330 --> 00:07:59,862
gpus and other custom

125
00:07:59,996 --> 00:08:03,666
chips that you'd like to use, and building models

126
00:08:03,698 --> 00:08:07,254
in all different kind of frameworks. Using a container image as your

127
00:08:07,292 --> 00:08:11,526
base allows you to do even things like hyperparameter tuning

128
00:08:11,638 --> 00:08:15,546
where you can create multiple models and find out

129
00:08:15,568 --> 00:08:18,874
the one that's working best. And this really allows you

130
00:08:18,912 --> 00:08:22,314
to speed up that training time. And as

131
00:08:22,352 --> 00:08:25,678
well as you see here in these screenshot, things like getting access

132
00:08:25,764 --> 00:08:29,040
to logs, downloading your model

133
00:08:29,490 --> 00:08:32,538
storing it in these cloud and managing on the cloud it will take care of

134
00:08:32,564 --> 00:08:36,082
for you as well. Another option as far as

135
00:08:36,216 --> 00:08:39,714
training quickly is cloud tpus or

136
00:08:39,752 --> 00:08:44,754
tensor processing units. And those are custom

137
00:08:44,872 --> 00:08:48,638
chips that are built for machine learning workloads.

138
00:08:48,734 --> 00:08:52,202
They allow you to create models

139
00:08:52,366 --> 00:08:55,862
very quickly at high scale, and can speed up that training

140
00:08:55,916 --> 00:08:59,090
even more if that's something you want to use. All right, let's move on to

141
00:08:59,100 --> 00:09:02,602
the next group of issues around model accuracy. So,

142
00:09:02,656 --> 00:09:06,090
first type of issue could happen when you have an imbalanced data set.

143
00:09:06,240 --> 00:09:09,754
So many machine learning tasks have many

144
00:09:09,792 --> 00:09:13,162
more examples that fall into one category

145
00:09:13,226 --> 00:09:16,798
than the other, right? So let's take fraud detection as an example

146
00:09:16,884 --> 00:09:20,250
where fortunately, say, most of the data is benign,

147
00:09:20,330 --> 00:09:24,254
theyre they are not fraudulent transaction, and you have a few that

148
00:09:24,292 --> 00:09:28,066
are. It's like finding a needle in the haystack. So we

149
00:09:28,088 --> 00:09:32,334
could have a trivial model that simply predicts that everything is benign

150
00:09:32,462 --> 00:09:36,854
and I would have a good accuracy, but that's not going to add

151
00:09:36,892 --> 00:09:40,326
any value, right? So what you

152
00:09:40,348 --> 00:09:44,006
want to do is apply some techniques to make sure

153
00:09:44,028 --> 00:09:47,542
that your accuracy is good across both of the

154
00:09:47,596 --> 00:09:51,014
classes, even if there's not a lot of data for them. So let's

155
00:09:51,062 --> 00:09:53,100
look at this next resource here,

156
00:09:54,270 --> 00:09:58,010
which is a tutorial for dealing with imbalanced data.

157
00:09:58,160 --> 00:10:02,334
So this is on the Tensorflow website, and it provides some of these

158
00:10:02,452 --> 00:10:06,922
different techniques. So things like weighting different classes differently,

159
00:10:06,986 --> 00:10:10,302
basically applying a greater penalty for

160
00:10:10,356 --> 00:10:13,794
mistakes on, say, the class with

161
00:10:13,832 --> 00:10:17,314
fewer examples. Other things

162
00:10:17,352 --> 00:10:21,442
you could do are oversampling and undersampling, where you take

163
00:10:21,496 --> 00:10:25,926
with the existing data you have, you basically can duplicate some

164
00:10:25,948 --> 00:10:29,426
of those records so that there's a more balanced

165
00:10:29,458 --> 00:10:33,074
number between the classes. Or conversely, under sampling,

166
00:10:33,122 --> 00:10:36,840
where you remove examples from the class with more.

167
00:10:37,210 --> 00:10:41,066
Finally, you could consider generating synthetic data. So there

168
00:10:41,088 --> 00:10:44,858
are packages in Python, like one called smote, for example,

169
00:10:45,024 --> 00:10:49,210
that can look at the distributions of your data and generate

170
00:10:49,550 --> 00:10:52,990
data that's similar to what's in your training

171
00:10:53,060 --> 00:10:57,118
sets. All things to try. Personally, I've had

172
00:10:57,284 --> 00:11:01,130
the best success with waiting classes

173
00:11:01,210 --> 00:11:03,700
to help with that issue.

174
00:11:04,150 --> 00:11:07,890
And automl has some ability to help with this

175
00:11:07,960 --> 00:11:11,490
as well, without even changing some of the class

176
00:11:11,560 --> 00:11:15,174
weighting that we discussed previously. So when you're training

177
00:11:15,212 --> 00:11:19,400
a model, there's something called an optimization objective, and this is

178
00:11:19,850 --> 00:11:23,366
what you're optimizing for. And so

179
00:11:23,388 --> 00:11:25,720
you can see here there are several different options.

180
00:11:26,490 --> 00:11:29,786
And if you switch to something called the

181
00:11:29,808 --> 00:11:33,386
area under the curve for precision recall, that is

182
00:11:33,408 --> 00:11:37,258
generally better for helping with that

183
00:11:37,344 --> 00:11:40,586
class with fewer examples in it, but you see there's

184
00:11:40,618 --> 00:11:44,522
a spectrum of possibilities depending on if you're trying to maximize

185
00:11:44,586 --> 00:11:47,950
accuracy for all of the data

186
00:11:48,100 --> 00:11:51,502
or create a more balanced result,

187
00:11:51,636 --> 00:11:54,686
et cetera. You can just customize using this.

188
00:11:54,788 --> 00:11:58,830
It's under advanced options for model evaluation.

189
00:11:59,570 --> 00:12:02,930
There is something you can do too, is where you create a model,

190
00:12:03,080 --> 00:12:06,866
and then you can review the accuracy at different thresholds.

191
00:12:06,978 --> 00:12:10,002
You can then review the confusion matrix.

192
00:12:10,066 --> 00:12:14,146
And if you see that below, this is an example of flight delays.

193
00:12:14,258 --> 00:12:18,038
And again, very good to see that most of

194
00:12:18,044 --> 00:12:21,514
the time, flights are on time, although it might not always feel that way.

195
00:12:21,712 --> 00:12:25,594
We can see here that there's definitely a difference in the

196
00:12:25,792 --> 00:12:29,526
accuracy with flights that are delayed,

197
00:12:29,638 --> 00:12:33,070
a little bit harder to pick those out with this particular model.

198
00:12:33,220 --> 00:12:36,334
So you could go through this process,

199
00:12:36,532 --> 00:12:40,266
look at the results for each of the different classes,

200
00:12:40,378 --> 00:12:44,654
and then perhaps come back and adjust that optimization objective

201
00:12:44,702 --> 00:12:48,322
if you'd like. So, model accuracy, this is a huge

202
00:12:48,376 --> 00:12:53,314
one. And this really can be where projects might

203
00:12:53,352 --> 00:12:57,074
not even succeed. They get completely stuck, right? So you are

204
00:12:57,112 --> 00:13:00,246
creating a model and you just get to a point where you can't improve the

205
00:13:00,268 --> 00:13:03,942
model accuracy anymore, and it's not good enough. It's not going to really add any

206
00:13:03,996 --> 00:13:07,894
value for the business. And sometimes

207
00:13:08,092 --> 00:13:11,142
it just is what it is, that it's a hard problem to solve,

208
00:13:11,206 --> 00:13:15,002
and there's not that signal available in the data,

209
00:13:15,136 --> 00:13:18,650
but often, with some creative thinking,

210
00:13:18,720 --> 00:13:22,638
you can move past those obstacles. So let's go back to this example

211
00:13:22,724 --> 00:13:26,480
of flight delays. So, on the left, I have a

212
00:13:26,930 --> 00:13:30,366
research paper that talks about historically, what are

213
00:13:30,388 --> 00:13:34,206
the reasons for flight delays. So I was actually looking at modeling

214
00:13:34,238 --> 00:13:38,180
this problem, and I started with data around

215
00:13:39,030 --> 00:13:42,878
the start and end times of the different flights,

216
00:13:43,054 --> 00:13:46,454
the carrier, et cetera. And that gave

217
00:13:46,492 --> 00:13:50,306
me some information. But what I did was I augmented

218
00:13:50,338 --> 00:13:53,634
my data with weather information. So I took a bigquery

219
00:13:53,682 --> 00:13:56,866
public data set of weather.

220
00:13:56,978 --> 00:14:00,614
I had Latlong coordinates, I joined

221
00:14:00,662 --> 00:14:04,166
that against the airport. So I know, okay, well, the arriving flight

222
00:14:04,198 --> 00:14:08,250
is going to have hail in it, things like that. And that definitely

223
00:14:08,320 --> 00:14:11,950
improved my model. No, it actually wasn't a huge

224
00:14:12,020 --> 00:14:15,582
increase. And this data kind of shows that

225
00:14:15,716 --> 00:14:19,466
6% of the root

226
00:14:19,498 --> 00:14:24,374
cause of flight delays and cancellations due to extreme cancellations,

227
00:14:24,442 --> 00:14:27,826
but the biggest one is aircraft arriving late. And this would be an

228
00:14:27,848 --> 00:14:31,570
example of if you have better data,

229
00:14:31,640 --> 00:14:35,542
you might be able to work on some data engineering to

230
00:14:35,596 --> 00:14:39,782
look at the whole flight graph and

231
00:14:39,916 --> 00:14:43,830
what flight is coming into the flight that you're trying to predict,

232
00:14:44,250 --> 00:14:48,026
even multiple flights back, using that as information. So the

233
00:14:48,048 --> 00:14:52,122
point here is really understanding these problem is

234
00:14:52,176 --> 00:14:55,930
key. It's not just about the algorithm and the math

235
00:14:56,350 --> 00:15:00,220
that you can often make a much bigger difference by

236
00:15:00,750 --> 00:15:04,494
understanding these domain that you're in. So that's my number one

237
00:15:04,532 --> 00:15:08,046
tip is look at improving domain expertise for the

238
00:15:08,068 --> 00:15:11,946
problem you're trying to solve. Really dig in, ensure you have the right experts

239
00:15:11,978 --> 00:15:15,570
on the team and as a data scientist learn

240
00:15:15,640 --> 00:15:18,834
the domain the best you can and you'll probably think

241
00:15:18,872 --> 00:15:22,386
of some things that are going to help you. Secondly, and sort of

242
00:15:22,408 --> 00:15:26,054
related to things, including more data, that always

243
00:15:26,092 --> 00:15:29,878
helps improve your accuracy, of course, and varied training data of

244
00:15:30,044 --> 00:15:33,494
different types for different tables that are going

245
00:15:33,532 --> 00:15:36,950
to add some diversity there for your model.

246
00:15:37,100 --> 00:15:40,682
Feature engineering of course is always useful to

247
00:15:40,816 --> 00:15:44,998
unlock information from some features. Maybe I'm

248
00:15:45,014 --> 00:15:48,886
making this up. You have a date field and you want to extract

249
00:15:48,998 --> 00:15:52,494
whether it's a weekend or a weekday. There might be

250
00:15:52,532 --> 00:15:56,174
different patterns for that, all kinds of different things

251
00:15:56,212 --> 00:15:59,694
that you can do with your data to ensure that

252
00:15:59,812 --> 00:16:02,718
your model can take advantage of it.

253
00:16:02,804 --> 00:16:06,226
So looking at some of the other things, consider removing some of the features that

254
00:16:06,248 --> 00:16:09,330
are causing overfitting where you're sort of locking into

255
00:16:09,400 --> 00:16:12,546
noise. I might suggest with what we talked

256
00:16:12,568 --> 00:16:16,034
about earlier, start with a smaller model and then incrementally

257
00:16:16,082 --> 00:16:19,762
add features back. Also try different model architectures.

258
00:16:19,826 --> 00:16:23,606
This is traditionally what we do in data science. Try a

259
00:16:23,628 --> 00:16:26,690
different model architecture, different number of layers,

260
00:16:26,770 --> 00:16:30,570
hyperparameter tuning, ensembling your model after

261
00:16:30,640 --> 00:16:34,138
you've done some of these more fundamental things. Finally,

262
00:16:34,224 --> 00:16:37,654
just for a gut check, always doesn't hurt to try automl

263
00:16:37,702 --> 00:16:41,342
to see what kind of performance is possible

264
00:16:41,396 --> 00:16:44,334
with the input data that you have. Let's move on.

265
00:16:44,452 --> 00:16:47,726
Transparency and fairness, our 6th issue,

266
00:16:47,828 --> 00:16:51,040
your model doesn't serve all of your users well.

267
00:16:51,650 --> 00:16:54,980
And so this issue

268
00:16:56,070 --> 00:16:59,550
is a very important and complex

269
00:16:59,630 --> 00:17:03,534
issue that when you look at how you're dealing with AI

270
00:17:03,662 --> 00:17:07,094
development, you want to look at a responsible AI framework to help with

271
00:17:07,132 --> 00:17:10,806
this. So here are a few questions to ask as you're building your

272
00:17:10,828 --> 00:17:14,454
model to help. So the

273
00:17:14,492 --> 00:17:18,200
first is around some of the business questions we asked before.

274
00:17:18,570 --> 00:17:21,690
What is the problem you're trying to solve? Who's your user?

275
00:17:22,270 --> 00:17:26,278
Moving on. Things like the risks, success factors. Now we're

276
00:17:26,294 --> 00:17:29,910
starting to move on to data. How was the training data

277
00:17:30,000 --> 00:17:33,738
collected, sampled, labeled. There's all kinds

278
00:17:33,754 --> 00:17:36,926
of issues that can pop up into this phase of the project.

279
00:17:37,028 --> 00:17:40,510
It's key to ensure that you're collecting data

280
00:17:40,580 --> 00:17:44,578
and those sampling and labeling processes ensure you

281
00:17:44,584 --> 00:17:47,938
have a representative sample. I'm not going to

282
00:17:47,944 --> 00:17:51,298
go through all the points here, but it's worth, as you go through the training

283
00:17:51,384 --> 00:17:54,100
problems, the evaluation process,

284
00:17:54,630 --> 00:17:58,166
you are considering all of these important questions when

285
00:17:58,188 --> 00:18:01,906
the model might have some limitations

286
00:18:02,018 --> 00:18:05,830
that you want to document and you want to document how

287
00:18:05,900 --> 00:18:09,306
you collected your data from end to end, so that you

288
00:18:09,328 --> 00:18:12,966
can continue to assess where you're

289
00:18:12,998 --> 00:18:16,314
at from a responsibility perspective and keep improving on

290
00:18:16,352 --> 00:18:19,594
it. There are a couple of tools that I'd like to mention here

291
00:18:19,632 --> 00:18:23,390
that can help. The what if tool allows you to

292
00:18:23,540 --> 00:18:27,370
slice your data by various factors

293
00:18:27,450 --> 00:18:30,574
to see why predictions happen the way that

294
00:18:30,612 --> 00:18:34,274
they did. This can give you a much more

295
00:18:34,312 --> 00:18:38,574
detailed understanding of your model accuracy versus a simple statistic

296
00:18:38,622 --> 00:18:42,254
like it was 98% precision

297
00:18:42,302 --> 00:18:46,166
or something like that. And you can also things is why

298
00:18:46,188 --> 00:18:49,734
it's called the what if tool. Actually change some values and

299
00:18:49,772 --> 00:18:53,538
see what happens. If I change a value slightly,

300
00:18:53,634 --> 00:18:58,566
does that change my prediction? So it's almost like a debugging

301
00:18:58,598 --> 00:19:02,714
tool for your model. Tensorflow model analysis is

302
00:19:02,752 --> 00:19:06,874
also helpful. So what this can help do is it

303
00:19:06,912 --> 00:19:10,714
can produce estimates of

304
00:19:10,752 --> 00:19:14,014
performance by slice of the data. So let's take a look at

305
00:19:14,052 --> 00:19:18,074
what that really means. Here is an example of Chicago

306
00:19:18,122 --> 00:19:21,774
taxi trip data, where we're estimating what

307
00:19:21,812 --> 00:19:25,106
the tip is going to be for a taxi trip. And you

308
00:19:25,128 --> 00:19:28,514
can see some statistics here. The bar graph shows

309
00:19:28,552 --> 00:19:31,874
you the number of samples at different hours of the day.

310
00:19:31,992 --> 00:19:35,202
So we're seeing that 05:00 in the morning,

311
00:19:35,256 --> 00:19:38,354
06:00 in the morning, much lower number of trips.

312
00:19:38,482 --> 00:19:41,926
Well, that might impact the accuracy. It's an example of not

313
00:19:41,948 --> 00:19:45,574
having a balanced data set, so it's actually going to slice per hour

314
00:19:45,692 --> 00:19:48,654
and give you statistics on that accuracy.

315
00:19:48,722 --> 00:19:51,946
So this will allow you to see by different

316
00:19:52,048 --> 00:19:56,406
dimensions of your data how balanced the errors

317
00:19:56,438 --> 00:19:59,594
are. So it can be a very useful tool. So we talked

318
00:19:59,632 --> 00:20:04,202
about assessing the accuracy and equitability

319
00:20:04,266 --> 00:20:07,866
of the model. Now let's look at how to document

320
00:20:07,898 --> 00:20:11,498
it. So ML models often get distributed

321
00:20:11,594 --> 00:20:14,578
as is. Here's these model, it just works. Okay?

322
00:20:14,744 --> 00:20:18,322
And think about it. With software development, they always

323
00:20:18,376 --> 00:20:20,450
have tutorials,

324
00:20:20,790 --> 00:20:24,434
documentation explaining each

325
00:20:24,552 --> 00:20:29,030
part of the user interface, and glossary, et cetera.

326
00:20:29,450 --> 00:20:32,802
Let's think about these same concept applied to ML models.

327
00:20:32,866 --> 00:20:36,998
Right? So first is being able to explain

328
00:20:37,164 --> 00:20:41,354
what's happening under the hood. So for a variety of different

329
00:20:41,552 --> 00:20:44,954
data types, it's important to look at why

330
00:20:44,992 --> 00:20:48,262
these predictions happened the way they did. And explainable

331
00:20:48,326 --> 00:20:51,962
AI can tell you which features are

332
00:20:52,016 --> 00:20:55,258
driving the decisions. So you see some examples

333
00:20:55,274 --> 00:20:58,974
here, maybe in an image. What is it about the image that was critical to

334
00:20:59,012 --> 00:21:02,410
making that determination, or in, say, in tabular data.

335
00:21:02,580 --> 00:21:06,146
We see that distance was the most important factor in our

336
00:21:06,168 --> 00:21:10,398
model's predictions. Today on Google Cloud, we support explainability

337
00:21:10,494 --> 00:21:13,826
across multiple layers of the

338
00:21:14,008 --> 00:21:17,990
platform, from automl to prediction to using

339
00:21:18,140 --> 00:21:22,210
our sdks to perform explainability.

340
00:21:22,290 --> 00:21:25,686
From your notebook and model cards allow you

341
00:21:25,708 --> 00:21:30,122
to document your model. You can specify information

342
00:21:30,256 --> 00:21:33,354
such as how you collected the data. You can

343
00:21:33,392 --> 00:21:36,730
put graphs around your performance curves,

344
00:21:38,110 --> 00:21:41,614
the model architecture. This is what we've done for the

345
00:21:41,652 --> 00:21:44,810
object detection API.

346
00:21:44,970 --> 00:21:48,634
And you can see there's a model card toolkit that allows you to generate

347
00:21:48,682 --> 00:21:52,254
these model cards based on information or model, or even

348
00:21:52,292 --> 00:21:55,778
attach it to your Tensorflow extended pipeline to generate one of

349
00:21:55,784 --> 00:21:59,410
these automatically. All right, so our final class of problems,

350
00:21:59,560 --> 00:22:02,210
ML Ops, are machine learning operations.

351
00:22:02,790 --> 00:22:06,358
So what if you built a model that just

352
00:22:06,444 --> 00:22:09,958
was a bad model? It was built on some training data

353
00:22:10,044 --> 00:22:13,254
that had just a bunch of

354
00:22:13,292 --> 00:22:17,186
data quality issues, and it somehow got into production. All kinds

355
00:22:17,218 --> 00:22:20,282
of users were impacted by that. Definitely not something

356
00:22:20,336 --> 00:22:24,426
that you want to have happen. Vertex pipelines can help with that by

357
00:22:24,528 --> 00:22:27,814
allowing you to codify the set of steps to build a machine

358
00:22:27,862 --> 00:22:31,786
learning model and providing guardrails

359
00:22:31,818 --> 00:22:35,626
around deployment so you can implement steps,

360
00:22:35,738 --> 00:22:38,762
or rather processes like continuous integration,

361
00:22:38,906 --> 00:22:42,522
continuous deployment, and include tests along

362
00:22:42,596 --> 00:22:45,694
the way for each of these different steps.

363
00:22:45,822 --> 00:22:50,050
Pipelines allow you to build a custom machine learning pipeline.

364
00:22:50,950 --> 00:22:55,282
Next issue, your model accuracy is drifting downward. So most

365
00:22:55,336 --> 00:22:59,106
software projects you can have or should have unit

366
00:22:59,138 --> 00:23:02,566
best on them, where you can evaluate whether your code is working or

367
00:23:02,588 --> 00:23:06,134
not, and the unit test will tell you if it's working or not.

368
00:23:06,172 --> 00:23:09,434
It's binary, it works, or it's broken. It's a little

369
00:23:09,472 --> 00:23:13,382
more subtle with machine learning, where things might drift

370
00:23:13,446 --> 00:23:16,170
slightly, the data distributions,

371
00:23:17,150 --> 00:23:20,090
for whatever reasons, for whatever you're modeling over time,

372
00:23:20,160 --> 00:23:22,810
may change due to outside conditions changing.

373
00:23:22,970 --> 00:23:26,094
So how do you detect and manage that?

374
00:23:26,292 --> 00:23:30,298
There are a couple of processes to consider. One is continuous evaluation,

375
00:23:30,474 --> 00:23:34,226
where you're regularly sampling your model's predictions, comparing those

376
00:23:34,248 --> 00:23:37,406
to ground truth, and these assessing

377
00:23:37,438 --> 00:23:41,374
the accuracy. Another thought is continuous

378
00:23:41,422 --> 00:23:45,026
training, where you deploy an

379
00:23:45,048 --> 00:23:48,614
NML pipeline that extracts the data, trains a new model,

380
00:23:48,732 --> 00:23:51,634
tests the model, of course, and then deploys it to production.

381
00:23:51,762 --> 00:23:55,062
And each of these different steps can

382
00:23:55,116 --> 00:23:59,018
work together to look for issues

383
00:23:59,104 --> 00:24:02,666
when you've drifted away from a certain threshold and

384
00:24:02,688 --> 00:24:05,990
then preventing the issue by training models

385
00:24:06,070 --> 00:24:09,354
on a recurring basis or when a

386
00:24:09,392 --> 00:24:12,954
certain amount of data changes. So you want to find that right sweet spot,

387
00:24:13,002 --> 00:24:16,862
not create too many models with every small change that's happening,

388
00:24:16,996 --> 00:24:20,126
but not let your models get too stale where their

389
00:24:20,148 --> 00:24:24,446
performance starts getting impacted. Vertex model monitoring

390
00:24:24,558 --> 00:24:28,866
can help around detecting these issues as

391
00:24:28,888 --> 00:24:31,934
far as drift or training serving SKU.

392
00:24:31,982 --> 00:24:35,466
So maybe you're starting to see where your users

393
00:24:35,518 --> 00:24:39,462
are making a lot of prediction with data that's much

394
00:24:39,516 --> 00:24:43,362
different from what you originally trained on. That might be a warning

395
00:24:43,426 --> 00:24:47,762
signal that your model isn't quite as applicable

396
00:24:47,826 --> 00:24:51,446
as it could be. So this gives you an additional layer of confidence

397
00:24:51,478 --> 00:24:54,506
in your model reliability. Now,

398
00:24:54,608 --> 00:24:58,086
our final issue is around model inference.

399
00:24:58,118 --> 00:25:01,514
So this is when you've built a model, it's deployed and

400
00:25:01,552 --> 00:25:04,606
people are using it, or they're making inferences or predictions on your

401
00:25:04,628 --> 00:25:07,806
model. So it's a success. You solved the problem at

402
00:25:07,828 --> 00:25:11,098
the accuracy level you were looking for. It's integrated

403
00:25:11,114 --> 00:25:14,546
into widely used application. Now, how do you handle the

404
00:25:14,648 --> 00:25:18,126
spiky workloads that may result? And how do you avoid

405
00:25:18,238 --> 00:25:23,534
over provisioning infrastructure while preventing errors

406
00:25:23,582 --> 00:25:26,974
or high latency? If you don't have enough infrastructure

407
00:25:27,022 --> 00:25:30,614
set up, vertex prediction can help with that, because that can allow you

408
00:25:30,652 --> 00:25:34,102
to set up an online endpoint where you can

409
00:25:34,236 --> 00:25:37,366
serve your model, and it will scale automatically based on

410
00:25:37,388 --> 00:25:40,646
your traffic. So you can set up, say, a minimum number of nodes,

411
00:25:40,678 --> 00:25:44,742
a maximum number of nodes. It will scale those up and down based on various

412
00:25:44,806 --> 00:25:48,806
utilization thresholds. It will help you with logging,

413
00:25:48,918 --> 00:25:52,240
it'll provide you the option of using

414
00:25:52,610 --> 00:25:55,230
some powerful GPU chips,

415
00:25:55,890 --> 00:25:59,854
and so you'll be able to ensure that you're serving the

416
00:25:59,892 --> 00:26:03,614
right amount of requests with vertex

417
00:26:03,662 --> 00:26:07,554
prediction at an optimized cost. So that

418
00:26:07,592 --> 00:26:11,426
wraps up the ten different issues. I hope that was helpful. Let's look

419
00:26:11,448 --> 00:26:15,294
at a few resources. Vertex AI is

420
00:26:15,352 --> 00:26:19,126
the AI platform that we discussed today that can help with several of

421
00:26:19,148 --> 00:26:22,838
these problems. Codelabs are a way to dive in and

422
00:26:22,924 --> 00:26:25,942
use notebooks and building

423
00:26:25,996 --> 00:26:29,938
models and basically get some training. They're free resources

424
00:26:30,034 --> 00:26:33,366
at Codelabs, at developers google.com, and if

425
00:26:33,388 --> 00:26:36,982
you're into learning via video, like this one,

426
00:26:37,116 --> 00:26:40,494
AI Adventures is one of our video series that has a lot

427
00:26:40,532 --> 00:26:44,286
of different resources around using Google Cloud for

428
00:26:44,308 --> 00:26:47,662
AI. And that concludes our

429
00:26:47,716 --> 00:26:51,134
presentation today. So I thank you for watching.

430
00:26:51,252 --> 00:26:52,380
I hope you have a great day.

