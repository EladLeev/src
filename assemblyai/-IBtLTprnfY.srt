1
00:00:41,010 --> 00:00:45,010
Hi everyone, welcome to Comp 42 cloud native confident.

2
00:00:45,170 --> 00:00:48,642
I am Adrian Teka and I'm a senior developer advocate

3
00:00:48,706 --> 00:00:52,366
for MongoDB. And today I'd like to talk to

4
00:00:52,388 --> 00:00:55,290
you about multicloud magic.

5
00:00:55,370 --> 00:00:58,910
So let's start on this journey. We're first going

6
00:00:58,980 --> 00:01:02,094
to get on the same page. We're going to say what is

7
00:01:02,132 --> 00:01:05,502
multicloud? Because you may have heard it before, you may have seen

8
00:01:05,556 --> 00:01:08,462
the buzzword flown around or seen it around the Internet,

9
00:01:08,526 --> 00:01:11,954
and it's really just a simple definition, but it's important to be

10
00:01:11,992 --> 00:01:15,682
on the same page. So we know weve we're going in the context of this

11
00:01:15,736 --> 00:01:19,590
presentation. After that we'll move on to the next likely question

12
00:01:19,660 --> 00:01:23,574
you might have, which is do we really need a multicloud option?

13
00:01:23,692 --> 00:01:27,750
Then we'll get into the bulk of my presentation, which is the

14
00:01:27,820 --> 00:01:31,118
different ways that you can use multicloud clusters

15
00:01:31,234 --> 00:01:34,646
in real world scenarios. And then finally weve

16
00:01:34,678 --> 00:01:38,202
going to see how to actually set up our own cluster and see how easy

17
00:01:38,256 --> 00:01:41,606
it is to do in MongoDB Atlas. So we'll

18
00:01:41,638 --> 00:01:45,038
start at the beginning, what is multicloud? And if you have an idea

19
00:01:45,124 --> 00:01:48,542
or if you have a different definition, I'm curious to hear. So feel

20
00:01:48,596 --> 00:01:52,794
free to let me know either in the chat or after the presentation.

21
00:01:52,922 --> 00:01:55,514
But for the context of this presentation,

22
00:01:55,642 --> 00:01:59,154
multicloud is probably the first instinct that you

23
00:01:59,192 --> 00:02:03,070
had when you thought about this question. And that's any single architectures

24
00:02:03,150 --> 00:02:06,350
that uses two or more cloud providers.

25
00:02:06,430 --> 00:02:09,798
And for the context of this, weve going to be focusing on

26
00:02:09,804 --> 00:02:14,034
the big three. So that's AWS, GCP and Azure.

27
00:02:14,162 --> 00:02:18,258
And as part of this multi cloud architecture, this can also mean that it's

28
00:02:18,274 --> 00:02:21,706
a mixture of public and private clouds. So as

29
00:02:21,728 --> 00:02:25,114
long as you're using two or more of the public providers or two

30
00:02:25,152 --> 00:02:28,826
or more of the big three, I should say, then it should be

31
00:02:28,848 --> 00:02:33,082
classified as a multicloud architectures. So two or more cloud providers

32
00:02:33,146 --> 00:02:37,210
within this same architecture is what we designate as a multicloud

33
00:02:37,290 --> 00:02:41,006
kind of solution. So next, do we really need a

34
00:02:41,028 --> 00:02:44,622
multicloud option? And I want you to think, when was the last

35
00:02:44,676 --> 00:02:48,194
time that you had to deal with an outage of some

36
00:02:48,232 --> 00:02:51,586
sort of managed service? Maybe there were a few in the last

37
00:02:51,608 --> 00:02:55,198
few months, maybe there were some really large ones that caused

38
00:02:55,214 --> 00:02:58,840
some production issues for you. Think about it for a second.

39
00:02:59,530 --> 00:03:03,378
Now, we know that outages are not really uncommon,

40
00:03:03,474 --> 00:03:07,366
but we also don't think that they happen as often as they

41
00:03:07,388 --> 00:03:10,938
do. And unfortunately that's kind of the case.

42
00:03:11,104 --> 00:03:14,890
If we go back two years, we'll kind of see a timeline of

43
00:03:14,960 --> 00:03:18,950
how many of these outages have actually occurred and how much impact

44
00:03:19,030 --> 00:03:23,134
they had. In June of 2019, there was a really large one from

45
00:03:23,172 --> 00:03:27,502
Google Cloud outage. It was so large that they called it the catch 22

46
00:03:27,556 --> 00:03:31,326
that broke the Internet. Now, what happened here was Google

47
00:03:31,428 --> 00:03:35,394
usually runs these routine configuration changes. They do these

48
00:03:35,432 --> 00:03:38,670
maintenance events often, and it's not something abnormal.

49
00:03:38,750 --> 00:03:42,622
In this particular case, they had one of those same usual

50
00:03:42,686 --> 00:03:46,134
maintenance events, and they intended for this to be applied to

51
00:03:46,172 --> 00:03:49,538
only a few servers in a specific geographic region.

52
00:03:49,634 --> 00:03:53,202
But when that happened and when this particular outage

53
00:03:53,266 --> 00:03:56,978
occurred, there was a combination of misconfigurations

54
00:03:57,074 --> 00:04:01,034
and some bugs in the software that led to the

55
00:04:01,072 --> 00:04:03,302
automation piece of the software,

56
00:04:03,446 --> 00:04:06,870
descheduling network control jobs in multiple

57
00:04:06,950 --> 00:04:10,506
locations. So what that amounted to is

58
00:04:10,608 --> 00:04:13,786
wired had a really good analogy. If you think about all the traffic

59
00:04:13,818 --> 00:04:17,262
that's running through Google and the Internet, and you think of

60
00:04:17,316 --> 00:04:20,942
all of that data running through numerous tunnels, what happened

61
00:04:20,996 --> 00:04:24,426
after that occurred was that those tunnels effectively got

62
00:04:24,468 --> 00:04:27,842
blocked. And there weve only two tunnels from maybe like

63
00:04:27,896 --> 00:04:31,458
six tunnels that they had running that allowed data to flow through.

64
00:04:31,544 --> 00:04:35,102
And effectively this resulted in an Internet wide

65
00:04:35,166 --> 00:04:38,502
gridlock. But it wasn't just Google. AWS also

66
00:04:38,556 --> 00:04:41,846
suffered their own outages in this particular case.

67
00:04:41,948 --> 00:04:45,606
Reading from the retrospective that they gave according to

68
00:04:45,628 --> 00:04:48,866
what happened in this outage, they say this event was caused

69
00:04:48,898 --> 00:04:52,506
by a failure of our data center control system, which is used to

70
00:04:52,528 --> 00:04:55,770
control and optimize the various cooling systems used

71
00:04:55,840 --> 00:04:59,498
in our data centers. So what happened here is the control

72
00:04:59,584 --> 00:05:03,114
system had some third party code that dealt

73
00:05:03,162 --> 00:05:06,670
with the interactions between the devices within the data center.

74
00:05:06,740 --> 00:05:10,586
So the things that communicated with the fans, with the chillers

75
00:05:10,698 --> 00:05:14,398
and the temperature sensors, there was a bug in this particular

76
00:05:14,564 --> 00:05:17,886
piece of third party software. And that bug caused

77
00:05:17,918 --> 00:05:21,294
the exchange of many, many interactions in the millions

78
00:05:21,422 --> 00:05:24,802
and effectively, again, this cases the control system

79
00:05:24,856 --> 00:05:28,134
to become unresponsive. And when that happened, the rest

80
00:05:28,172 --> 00:05:32,514
of the fiasco kind of unraveled, right? Raxo services started overheating,

81
00:05:32,642 --> 00:05:36,454
and that bug caused multiple redundant cooling systems to

82
00:05:36,492 --> 00:05:39,874
fail in many parts of the affected availability zones.

83
00:05:40,002 --> 00:05:43,098
And then we see Google again, right? March of 2020,

84
00:05:43,184 --> 00:05:46,554
they had a significant router failure in a data center in

85
00:05:46,592 --> 00:05:49,626
Atlanta. And Azure is not immune to

86
00:05:49,648 --> 00:05:53,342
this either. Specifically, when the pandemic hit, we really

87
00:05:53,396 --> 00:05:57,374
started to see the limits of what our cloud providers can

88
00:05:57,412 --> 00:06:01,022
offer us. As we started to all work from

89
00:06:01,076 --> 00:06:04,930
home, many of us started to use the Internet a lot more,

90
00:06:05,000 --> 00:06:08,498
conduct business meetings, team meetings, do everything

91
00:06:08,584 --> 00:06:12,050
online and started to use more of these services. We really

92
00:06:12,120 --> 00:06:15,374
saw how much of a toll that took on provider

93
00:06:15,422 --> 00:06:18,902
like Azure. As an example, they shared some

94
00:06:18,956 --> 00:06:22,326
stats for some of their services. So right at the height of

95
00:06:22,348 --> 00:06:25,622
the pandemic around April of 2020,

96
00:06:25,756 --> 00:06:29,062
they showed that the Microsoft Teams usage really,

97
00:06:29,116 --> 00:06:32,582
really spiked. So Microsoft Teams is their video conferencing software.

98
00:06:32,646 --> 00:06:36,822
And just as an example, they had about 44 million daily users,

99
00:06:36,886 --> 00:06:40,574
which generated over 900 million meeting minutes and

100
00:06:40,612 --> 00:06:44,046
calling minutes on teams in a single week.

101
00:06:44,228 --> 00:06:47,694
That is incredibly unexpected in terms

102
00:06:47,732 --> 00:06:51,406
of how much new traffic and unexpected and

103
00:06:51,508 --> 00:06:55,602
really scaled traffic that they did not expect. And we saw that with

104
00:06:55,656 --> 00:06:59,454
these outages and with the shortages that they were experiencing

105
00:06:59,582 --> 00:07:02,990
in this particular case, they really felt it in Europe

106
00:07:03,070 --> 00:07:06,514
and on and on. Right. Azure also had

107
00:07:06,552 --> 00:07:10,326
another one in terms of bottlenecks in the APAC region in June of

108
00:07:10,348 --> 00:07:14,066
2020. And the most recent one that was fairly

109
00:07:14,098 --> 00:07:17,526
large was the AWS outage that took down a

110
00:07:17,548 --> 00:07:21,494
big chunk of the Internet. And that was in November of 2020 for the Kinesis

111
00:07:21,542 --> 00:07:25,162
data streams outage. So in between all

112
00:07:25,216 --> 00:07:29,082
of these kind of larger outages that you may have seen or

113
00:07:29,136 --> 00:07:32,974
have experienced, and if you were thinking about my

114
00:07:33,012 --> 00:07:36,782
question prior to starting this kind of timeline, you may have

115
00:07:36,836 --> 00:07:40,334
even more examples of these scenarios where you had

116
00:07:40,372 --> 00:07:44,046
some outages that really caused some issues. And what

117
00:07:44,068 --> 00:07:47,230
we want to see here is that we know that, again, that outages

118
00:07:47,310 --> 00:07:50,562
are not impossible, that they never happen. We know that they

119
00:07:50,616 --> 00:07:54,350
occur. What is more important to see in this timeline

120
00:07:54,430 --> 00:07:58,534
and in the examples and events that you may have experienced yourself is

121
00:07:58,572 --> 00:08:02,034
that no cloud is spared from the outages.

122
00:08:02,162 --> 00:08:05,654
And this is a very important point, because a lot of

123
00:08:05,692 --> 00:08:09,030
companies who are kind of feeling the pain of

124
00:08:09,100 --> 00:08:12,774
vendor lock in, for example, or those who are slowly migrating

125
00:08:12,822 --> 00:08:16,826
to the cloud and are only considering a single cloud

126
00:08:17,008 --> 00:08:20,426
if they are a company that needs to be global or needs to

127
00:08:20,448 --> 00:08:24,634
be highly available, this is something that is near and dear

128
00:08:24,682 --> 00:08:28,906
to their heart. This is something that is a very big portion that can impact

129
00:08:29,018 --> 00:08:32,750
the user's perception of their application and is also something

130
00:08:32,820 --> 00:08:36,466
that they may worry about in terms of their uptime. So the

131
00:08:36,488 --> 00:08:40,514
fact that no cloud is spared from outages means that, yes, we can

132
00:08:40,552 --> 00:08:44,514
kind of confidently say that this is an issue

133
00:08:44,632 --> 00:08:47,942
if we are on a specific cloud provider or only a single

134
00:08:47,996 --> 00:08:51,974
cloud provider. And we have run into issues before where

135
00:08:52,012 --> 00:08:55,000
an outage on one has caused problems.

136
00:08:55,690 --> 00:08:59,726
So that's more on the consumer side and of us experiencing

137
00:08:59,778 --> 00:09:03,178
the outages ourselves. But also in June of 2020,

138
00:09:03,264 --> 00:09:06,506
we took a look at this report from

139
00:09:06,608 --> 00:09:10,198
this CIO think tank. So what happened here was they

140
00:09:10,304 --> 00:09:13,786
got around 30 it leaders in a variety

141
00:09:13,898 --> 00:09:17,838
of tech companies and they started to talk about

142
00:09:17,924 --> 00:09:21,598
multicloud. What does that look like? How do we actually

143
00:09:21,684 --> 00:09:25,346
implement it? Is it worth it to consider a

144
00:09:25,368 --> 00:09:28,526
multicloud architecture for the various

145
00:09:28,638 --> 00:09:31,570
architectures they have and the different industries that they have?

146
00:09:31,640 --> 00:09:35,614
And for the most part, these CIOs across the board acknowledge

147
00:09:35,662 --> 00:09:38,958
that it's not a matter of if, but a matter of when that they're

148
00:09:38,974 --> 00:09:42,358
going to use multicloud. And it would come to a

149
00:09:42,364 --> 00:09:45,878
variety of reasons of why they are not on there yet.

150
00:09:45,964 --> 00:09:49,222
But there are a few quotes that I see here that really

151
00:09:49,276 --> 00:09:53,206
set the tone, really captures the essence of why I think multicloud

152
00:09:53,238 --> 00:09:56,794
is becoming more prevalent and a more approachable solution for

153
00:09:56,832 --> 00:10:00,530
many of these companies as they grow. So the first one is Gregory Sherman,

154
00:10:00,550 --> 00:10:04,234
he's a vp of business platform technology for Fiserv,

155
00:10:04,362 --> 00:10:07,578
and that is an american multinational Fortune

156
00:10:07,594 --> 00:10:11,054
500 company and they provide financial services.

157
00:10:11,252 --> 00:10:14,826
So what he says is the main driver is

158
00:10:14,868 --> 00:10:18,146
what our clients are asking for. We have banks who have

159
00:10:18,168 --> 00:10:21,182
an Azure preference, we have banks who have an AWS

160
00:10:21,246 --> 00:10:25,086
preference, Google Cloud and on and on, we don't

161
00:10:25,118 --> 00:10:29,346
really get to choose. And that's incredibly key for multi

162
00:10:29,378 --> 00:10:32,982
cloud because there are many industries like Fiserv who may have

163
00:10:33,036 --> 00:10:36,578
clients like this. In their case, their clients include banks,

164
00:10:36,674 --> 00:10:39,270
credit unions, security brokers, dealers,

165
00:10:39,350 --> 00:10:43,082
leveraging and finance companies. And especially in very

166
00:10:43,136 --> 00:10:46,906
regulated industries like that, sometimes Pfizer does not have the

167
00:10:46,928 --> 00:10:50,298
option, they need to give the option to their clients of where

168
00:10:50,384 --> 00:10:54,234
to hold their data or where to host their applications.

169
00:10:54,362 --> 00:10:57,934
And so that's why multi cloud is something that is not just a nice

170
00:10:57,972 --> 00:11:01,534
to have, but almost a necessary thing for this particular

171
00:11:01,652 --> 00:11:04,990
company. Next is Mohan Pucha, who's the vice president

172
00:11:05,060 --> 00:11:08,802
of architecture and digital strategy at Aon. Now, Aeon is another

173
00:11:08,856 --> 00:11:12,766
multinational company that offers financial risk migration

174
00:11:12,878 --> 00:11:17,266
products. And what he says is we have to be native AWS

175
00:11:17,378 --> 00:11:21,110
because of their advanced capabilities in analytics and

176
00:11:21,180 --> 00:11:23,730
we have to be in azure because frankly,

177
00:11:23,810 --> 00:11:27,602
developers love that ecosystem and productive

178
00:11:27,666 --> 00:11:31,538
developers are probably the best thing. So I really

179
00:11:31,564 --> 00:11:35,926
like this quote, because it not only speaks to the fact that maybe multicloud

180
00:11:35,958 --> 00:11:40,138
is a decision that comes from the top down, but it's actually also coming

181
00:11:40,224 --> 00:11:44,266
from the bottom up, or from developers who are having more autonomy

182
00:11:44,298 --> 00:11:47,642
in their decisions. And as developers,

183
00:11:47,706 --> 00:11:51,562
we want to use the best tools for the job. So this is incredibly

184
00:11:51,626 --> 00:11:55,106
exciting because most of the scenarios we've seen of

185
00:11:55,128 --> 00:11:58,994
why multicloud has previously been difficult thing to kind of

186
00:11:59,032 --> 00:12:02,866
implement is being made easier with something like a

187
00:12:02,888 --> 00:12:06,818
multicloud cluster, and we'll see why. A lot of the scenarios

188
00:12:06,834 --> 00:12:10,646
I will show very soon, almost next slide is that the

189
00:12:10,668 --> 00:12:14,850
biggest bottleneck is having data portability for their applications,

190
00:12:14,930 --> 00:12:18,362
and so we'll see how those are actually used. Now in the real

191
00:12:18,416 --> 00:12:22,182
world, we will be seeing how some different multicloud

192
00:12:22,246 --> 00:12:25,990
solutions are used on the basis of multicloud

193
00:12:26,070 --> 00:12:29,802
clusters. We'll start with some very common use

194
00:12:29,856 --> 00:12:33,566
cases where multicloud is a very easy thing for

195
00:12:33,668 --> 00:12:37,642
clients to kind of choose and migrate towards, and that's

196
00:12:37,706 --> 00:12:41,086
data sovereignty and data residency issues. So in

197
00:12:41,108 --> 00:12:45,102
this particular case, Canada has this direction for electronic

198
00:12:45,166 --> 00:12:48,494
data residency. Now, the Government of Canada,

199
00:12:48,622 --> 00:12:52,542
every few years or so, they write out a strategic

200
00:12:52,606 --> 00:12:55,726
it plan that is to be dispersed throughout

201
00:12:55,758 --> 00:12:59,462
all of the government entities. What they put in this plan is just some

202
00:12:59,516 --> 00:13:03,894
best practices, some directives for how they are to implement different it

203
00:13:04,012 --> 00:13:06,966
policies. And in this particular case,

204
00:13:07,148 --> 00:13:10,550
one of them is called the direction of electronic data residency.

205
00:13:10,630 --> 00:13:14,326
So what does that mean? This stated that all sensitive electronic

206
00:13:14,358 --> 00:13:17,562
data that's under government control needed to be

207
00:13:17,696 --> 00:13:21,510
located within the geographic boundaries of Canada.

208
00:13:21,590 --> 00:13:25,182
Now, there are a few options with, you know, there cloud be some

209
00:13:25,236 --> 00:13:28,874
locations abroad that have been pre approved by the Government of Canada,

210
00:13:28,922 --> 00:13:32,446
like diplomatic or consular missions, but for the most part, those are very

211
00:13:32,548 --> 00:13:35,746
rare. And ultimately, all of the data,

212
00:13:35,848 --> 00:13:39,954
at least the sensitive data that they have classified as sensitive data,

213
00:13:40,072 --> 00:13:43,742
needs to remain within the geographic boundaries of Canada.

214
00:13:43,886 --> 00:13:48,070
So a couple provinces have actually gone a little bit further than that.

215
00:13:48,220 --> 00:13:51,798
British Columbia and Nova Scotia, they have

216
00:13:51,884 --> 00:13:55,666
said that all of the data, well, they pretty much align with the initial

217
00:13:55,698 --> 00:13:59,002
strategic plan, but they do follow it to a t.

218
00:13:59,056 --> 00:14:02,380
So that's all government data. Public schools, healthcare services,

219
00:14:02,910 --> 00:14:06,794
governmental public data, those all go and have to remain within

220
00:14:06,832 --> 00:14:10,134
Canada, whereas Ontario, they only are applying

221
00:14:10,182 --> 00:14:13,486
this to health records and health data. And that brings us to

222
00:14:13,508 --> 00:14:17,230
the first kind of scenario that I want to share with you, which is the

223
00:14:17,300 --> 00:14:21,214
story of an emergency services application and

224
00:14:21,252 --> 00:14:24,974
how they used multicloud to give them higher availability

225
00:14:25,102 --> 00:14:28,494
while still complying with this directive.

226
00:14:28,622 --> 00:14:32,782
So for the most part, most of these companies, including our emergency

227
00:14:32,846 --> 00:14:36,354
services application client, was hosting their data on

228
00:14:36,392 --> 00:14:39,746
AWS Montreal. And that's not too far fetched

229
00:14:39,778 --> 00:14:43,926
to think of, because last time I checked, the market share pretty

230
00:14:43,948 --> 00:14:47,990
much belonged to AWS for now with 34%. And most companies

231
00:14:48,060 --> 00:14:51,414
are on AWS. And in this particular case, this region,

232
00:14:51,542 --> 00:14:55,098
they all had to be on AWS Montreal, because that's the

233
00:14:55,104 --> 00:14:58,742
only one AWS offers. So in this scenario,

234
00:14:58,886 --> 00:15:02,166
that's not a great thing, because let's

235
00:15:02,198 --> 00:15:05,854
say this happens, let's say AWS Montreal goes out for one

236
00:15:05,892 --> 00:15:09,230
reason or another, even though it's rare. We have seen before

237
00:15:09,300 --> 00:15:12,926
in the last timeline that it's not as rare as we think.

238
00:15:13,028 --> 00:15:16,626
And when this occurs, and assuming that your application was

239
00:15:16,648 --> 00:15:20,322
not built to fail over properly or did not have the means to,

240
00:15:20,456 --> 00:15:24,178
then your application will also fail. It will also be

241
00:15:24,264 --> 00:15:27,926
out. And for an application like an emergency services

242
00:15:28,028 --> 00:15:31,314
application, outages are not only annoying,

243
00:15:31,362 --> 00:15:34,566
they're almost unacceptable in this case. And we

244
00:15:34,588 --> 00:15:38,198
know that this is a really negative outlook for any company,

245
00:15:38,284 --> 00:15:42,086
right? Any type of application downtime where your users can't

246
00:15:42,118 --> 00:15:45,690
access your product or your services, that always, almost always

247
00:15:45,760 --> 00:15:49,862
translates to either financial loss or some sort of reputation loss,

248
00:15:49,926 --> 00:15:53,562
because nobody likes to deal with an application that's not working.

249
00:15:53,696 --> 00:15:57,438
And what really drove it home for this emergency services application

250
00:15:57,524 --> 00:16:01,246
is that the recent AWS outage of November of 2020 did

251
00:16:01,268 --> 00:16:05,154
not help. It did not make them feel confident at all that they could

252
00:16:05,192 --> 00:16:08,606
rely on this single region on AWS.

253
00:16:08,718 --> 00:16:12,574
So what did they do? Well, they already took advantage

254
00:16:12,702 --> 00:16:16,194
of another cloud provider they had to that

255
00:16:16,232 --> 00:16:19,462
was still within Montreal, and the only other region that was

256
00:16:19,516 --> 00:16:23,414
there, or only other cloud provider that provided that same region was

257
00:16:23,452 --> 00:16:27,126
GCP. So they took advantage of that and set up a

258
00:16:27,148 --> 00:16:30,426
failover strategy that way. And because they wanted to

259
00:16:30,448 --> 00:16:34,122
be extra fault tolerant or wanted to have that built

260
00:16:34,176 --> 00:16:37,462
in, they also took advantage of two other regions that Azure

261
00:16:37,526 --> 00:16:40,874
provided. So there is an Azure Canada central region, which is based

262
00:16:40,912 --> 00:16:44,766
in Toronto, and an Azure Canada east region that is

263
00:16:44,788 --> 00:16:48,938
based in Quebec. And so now in this type of architectures,

264
00:16:49,034 --> 00:16:52,782
they're feeling very confident that no matter what kind of outages may

265
00:16:52,836 --> 00:16:56,306
occur, they would be okay. They will not have an

266
00:16:56,328 --> 00:16:59,218
outage. They will still be able to access their application.

267
00:16:59,384 --> 00:17:03,058
So again, if AWS Montreal goes out, no problem,

268
00:17:03,144 --> 00:17:06,626
GCP can step up and fill in the gap there. And in the

269
00:17:06,648 --> 00:17:10,306
even more rare but still plausible scenario that the entire region

270
00:17:10,338 --> 00:17:14,422
of Montreal just goes out, well, that's when the azure regions can step

271
00:17:14,476 --> 00:17:18,006
up and fill in the gap in those outages. So that's

272
00:17:18,038 --> 00:17:21,754
the canadian example of kind of abiding and

273
00:17:21,792 --> 00:17:25,366
still being compliant to this data residency requirement

274
00:17:25,478 --> 00:17:28,886
while still taking advantage of the other providers.

275
00:17:28,918 --> 00:17:32,346
They almost had to specifically for this client because of

276
00:17:32,368 --> 00:17:36,106
the type of application that they had, they did not want to have outages.

277
00:17:36,218 --> 00:17:40,458
And so that was a very great use case for multicloud clusters.

278
00:17:40,554 --> 00:17:44,554
Another very similar one is Australia. Now, Australia passed

279
00:17:44,602 --> 00:17:48,366
some similar legislation. This one is called the My Health Records

280
00:17:48,398 --> 00:17:52,322
act of 2012, and that pretty much states that there's the

281
00:17:52,376 --> 00:17:56,082
requirement to not hold or take records, health records

282
00:17:56,146 --> 00:17:59,894
outside of Australia. So if we take a look at the

283
00:18:00,012 --> 00:18:04,594
cloud providers and regions in the Australia

284
00:18:04,642 --> 00:18:08,150
landscape, you'll find that most of them are pretty much

285
00:18:08,220 --> 00:18:11,766
here in Sydney. There's a GCP region in Sydney and there's

286
00:18:11,798 --> 00:18:15,626
an AWS region in Sydney. But as you'll see,

287
00:18:15,728 --> 00:18:19,322
this still is prone to the same problems before that.

288
00:18:19,376 --> 00:18:22,794
If you are on either one GCP or AWS,

289
00:18:22,922 --> 00:18:26,382
if you had a regional outage, well, you're out of luck there

290
00:18:26,436 --> 00:18:30,014
because those are the only two that were available there in

291
00:18:30,052 --> 00:18:33,330
Sydney. In order to fight against that. Again,

292
00:18:33,400 --> 00:18:37,006
in this scenario, Azure with their Melbourne

293
00:18:37,118 --> 00:18:40,798
region, in this case the Australia Southeast region,

294
00:18:40,974 --> 00:18:44,178
gave these companies a different opportunity

295
00:18:44,344 --> 00:18:48,418
to be able to spread their availability

296
00:18:48,594 --> 00:18:51,862
across multiple regions. So not just the cloud

297
00:18:51,916 --> 00:18:55,574
providers, but across multiple regions. In the

298
00:18:55,612 --> 00:18:59,386
particular case of Australia, what Azure actually found was that

299
00:18:59,488 --> 00:19:03,354
they had another kind of policy that required some in

300
00:19:03,392 --> 00:19:06,554
country disaster recovery options. And so with that,

301
00:19:06,592 --> 00:19:09,846
they actually added two additional regions in Canberra,

302
00:19:09,958 --> 00:19:13,790
one Australia central and Australia central, two built

303
00:19:13,860 --> 00:19:17,358
specifically for this kind of compliance issue.

304
00:19:17,444 --> 00:19:21,214
And if you were wondering, they also do have one in Sydney, in case

305
00:19:21,252 --> 00:19:25,118
you wanted to collect them all. But again, the point of this is to show

306
00:19:25,204 --> 00:19:28,466
that Australia is a fairly large continent, and if

307
00:19:28,488 --> 00:19:31,666
you wanted to work in any of the other territories and needed to service those

308
00:19:31,688 --> 00:19:35,038
other territories, well, you're going to need to start making use of the

309
00:19:35,064 --> 00:19:39,202
other cloud providers. So those are kind of the low hanging

310
00:19:39,266 --> 00:19:42,674
fruit scenarios of these multicloud

311
00:19:42,722 --> 00:19:45,846
clusters in play. And now I want to talk

312
00:19:45,868 --> 00:19:49,670
about some actual situations where they

313
00:19:49,740 --> 00:19:53,850
use multicloud clusters and where we're seeing some different

314
00:19:53,920 --> 00:19:57,482
use cases of how multi cloud has solved different problems.

315
00:19:57,616 --> 00:20:00,974
This next one is called it the recommendation feature. In this

316
00:20:01,012 --> 00:20:04,622
particular case, we had a client who had

317
00:20:04,676 --> 00:20:08,458
some workloads and they were running on AWS.

318
00:20:08,554 --> 00:20:12,346
Now, to be transparent, they were already using MongoDB

319
00:20:12,378 --> 00:20:16,634
Atlas to host their database and host their data, and they were hosted

320
00:20:16,682 --> 00:20:20,498
on the AWS region. They were hosting an AWS and this application

321
00:20:20,584 --> 00:20:24,018
was an internal help desk software type of application.

322
00:20:24,184 --> 00:20:27,926
As they started to expand and as they started to grow, they wanted to

323
00:20:27,948 --> 00:20:31,462
add a recommendation feature. And when they spoke to their

324
00:20:31,516 --> 00:20:34,614
developers who weve about to implement this,

325
00:20:34,732 --> 00:20:38,438
they took some time, researched and found that hey,

326
00:20:38,524 --> 00:20:42,106
we want to use Automl, which is a Google service,

327
00:20:42,288 --> 00:20:46,086
and it's basically a tool that uses machine learning to reveal

328
00:20:46,118 --> 00:20:49,626
the structure and meaning of text. So they wanted to use this to

329
00:20:49,648 --> 00:20:53,534
be able to tap into not only their production data, live data

330
00:20:53,652 --> 00:20:57,262
and patterns, but also the knowledge base that they had

331
00:20:57,316 --> 00:21:00,602
so that they could recommend potentially relevant

332
00:21:00,746 --> 00:21:04,906
knowledge base articles for the help desk technicians

333
00:21:04,938 --> 00:21:08,514
when they were using this software. And so now with this kind of

334
00:21:08,552 --> 00:21:11,650
scenarios, the big glaring thing here now is,

335
00:21:11,720 --> 00:21:15,154
well, how do we get that data over there? We need to get data

336
00:21:15,272 --> 00:21:19,398
over there somehow in order to be able to use this

337
00:21:19,484 --> 00:21:22,582
tool. Or at least that was the goal, because that would make it much

338
00:21:22,636 --> 00:21:26,150
easier to integrate and use the automl tool

339
00:21:26,220 --> 00:21:29,594
for this proposed analytics application that was

340
00:21:29,632 --> 00:21:32,986
to serve as their recommendation feature. So what do

341
00:21:33,008 --> 00:21:36,380
you think were the potential options for this?

342
00:21:37,310 --> 00:21:40,542
Well, to start, one of the common

343
00:21:40,596 --> 00:21:44,094
ways we've already solved this problem is to write custom

344
00:21:44,212 --> 00:21:49,006
code. Custom code. It works and it

345
00:21:49,028 --> 00:21:53,166
will always work, but it's not necessarily the best option.

346
00:21:53,348 --> 00:21:57,086
As most of us know, if you've ever done any kind of custom scripting

347
00:21:57,118 --> 00:22:00,402
like this before, these kinds of things are always

348
00:22:00,536 --> 00:22:03,938
unique. They're too purpose built, they're only made

349
00:22:04,024 --> 00:22:07,454
and written in a specific way that is solely

350
00:22:07,502 --> 00:22:11,782
to solve this problem. And that is a big reason why yes,

351
00:22:11,836 --> 00:22:15,318
it will work, but it's not necessarily the best case because that's a

352
00:22:15,324 --> 00:22:19,138
lot of maintenance to put into something that is super custom and it's

353
00:22:19,154 --> 00:22:22,594
another thing to maintain. And even as devs,

354
00:22:22,642 --> 00:22:25,846
we know this pain, right, we know that this is something that we don't

355
00:22:25,878 --> 00:22:29,386
want to do or try to avoid at all costs. And so even if

356
00:22:29,408 --> 00:22:33,818
we try to automate that kind of pain away by say, using something like kafka,

357
00:22:33,914 --> 00:22:37,150
which is something that can stream updates from one source to another,

358
00:22:37,300 --> 00:22:40,862
for this particular client's case, it was just another piece

359
00:22:40,996 --> 00:22:44,094
that they did not want to maintain. It was an additional service

360
00:22:44,212 --> 00:22:47,602
that they just did not want to

361
00:22:47,736 --> 00:22:51,662
set up. And so this was not the option that they chose,

362
00:22:51,726 --> 00:22:55,042
nor was it the best one for their scenario. So the next

363
00:22:55,096 --> 00:22:58,626
option then is something called backup and restore.

364
00:22:58,738 --> 00:23:02,482
So in this scenario, we would be taking live snapshots

365
00:23:02,546 --> 00:23:06,454
of their live data on AWS and we would restore it

366
00:23:06,492 --> 00:23:10,234
over to GCP or wherever they had

367
00:23:10,272 --> 00:23:14,042
the other database hosted at the time and

368
00:23:14,096 --> 00:23:18,022
have it analyzed. But again, this was another costly

369
00:23:18,086 --> 00:23:21,414
ETL process that they did not want to maintain. It's another separate

370
00:23:21,462 --> 00:23:25,086
piece and too many pieces weve something that they just did

371
00:23:25,108 --> 00:23:28,126
not want. So the bigger issue with

372
00:23:28,148 --> 00:23:31,806
this option too is that when they would work with the

373
00:23:31,828 --> 00:23:35,402
data and work with the snapshots, it was usually done in batches,

374
00:23:35,466 --> 00:23:38,690
which meant the analysts that were trying to use this data

375
00:23:38,760 --> 00:23:42,114
for this application, this analytics application, they were always

376
00:23:42,152 --> 00:23:45,634
waiting for new data to be uploaded. So they were always working with somewhat stale

377
00:23:45,682 --> 00:23:49,206
data and it just was not working for them. So in

378
00:23:49,228 --> 00:23:53,506
this particular case, something like a multi cloud cluster

379
00:23:53,618 --> 00:23:56,786
really was the best option for this scenario.

380
00:23:56,898 --> 00:24:00,346
Now, again, they were already using MongoDB Atlas. This is

381
00:24:00,368 --> 00:24:03,914
part of why this option was much more appealing than the other two.

382
00:24:04,032 --> 00:24:08,490
And by having the same underlying cloud database support

383
00:24:08,640 --> 00:24:12,314
all the different kinds of workloads that they had, it made it a very

384
00:24:12,352 --> 00:24:16,014
easy decision to say, yeah, multicloud cluster makes sense in

385
00:24:16,052 --> 00:24:19,386
our case and what we want to do here. So how did they achieve

386
00:24:19,418 --> 00:24:23,150
this? Well, they did spin up an analytics node

387
00:24:23,230 --> 00:24:26,866
to be able to work with their analytics application,

388
00:24:27,048 --> 00:24:29,918
and they spun that up on GCP.

389
00:24:30,094 --> 00:24:33,250
So what this means is they used a specific

390
00:24:33,320 --> 00:24:36,626
node that is meant for analytics, it is meant

391
00:24:36,658 --> 00:24:40,530
for complex, for long running operations.

392
00:24:40,690 --> 00:24:44,662
And the better part is that this was separate from

393
00:24:44,716 --> 00:24:47,714
their production workloads, their operational workloads,

394
00:24:47,762 --> 00:24:51,706
the ones that are running on AWS. So while they can use

395
00:24:51,808 --> 00:24:54,986
the GCP node to their heart's content for any

396
00:24:55,008 --> 00:24:59,098
kind of analysis and to fuel the automl tool

397
00:24:59,184 --> 00:25:02,538
for their analytics application, their AWS

398
00:25:02,634 --> 00:25:06,778
workloads remain untouched. They're not competing for additional resources,

399
00:25:06,874 --> 00:25:11,006
and they can stay focused on servicing the

400
00:25:11,108 --> 00:25:13,860
production workloads for their application.

401
00:25:14,230 --> 00:25:18,126
So this was a very good fit for multicloud

402
00:25:18,158 --> 00:25:21,170
clusters in their very specific scenario.

403
00:25:22,950 --> 00:25:25,774
Next is another very, I will say,

404
00:25:25,832 --> 00:25:29,174
experimental, and this is still being worked.

405
00:25:29,292 --> 00:25:33,094
But it is something that is an interesting idea

406
00:25:33,292 --> 00:25:36,946
that if they're able to work out the kinks would make multicloud

407
00:25:36,978 --> 00:25:40,634
a really, really awesome options. So this client had some

408
00:25:40,672 --> 00:25:44,074
workloads, and what they wanted to do was be

409
00:25:44,112 --> 00:25:47,882
able to burst those workloads to whichever cloud

410
00:25:47,936 --> 00:25:51,274
provider had the best pricing at the time. So they wanted to take

411
00:25:51,312 --> 00:25:55,294
advantage of any pricing fluctuations and send

412
00:25:55,332 --> 00:25:58,974
their workloads that way. So, for example, if Azure had the best

413
00:25:59,012 --> 00:26:02,446
pricing for them at that point, they would love to be able to

414
00:26:02,548 --> 00:26:05,858
move that workload over there or send some that way.

415
00:26:05,944 --> 00:26:09,694
And likewise, as things fluctuated, let's say AWS

416
00:26:09,822 --> 00:26:13,026
met that kind of threshold they had of whatever pricing they

417
00:26:13,048 --> 00:26:17,090
were choosing for, they could also send some workloads that way and

418
00:26:17,160 --> 00:26:20,598
onward, right? If for whatever reason GCP also went down,

419
00:26:20,684 --> 00:26:24,134
send them there. So this is kind of the holy grail of

420
00:26:24,172 --> 00:26:27,366
this particular production, but there's still a long way

421
00:26:27,388 --> 00:26:30,506
to go, and the reason for that is even though we

422
00:26:30,528 --> 00:26:33,946
try to generalize these workloads, these tasks that we

423
00:26:33,968 --> 00:26:37,574
tried to do, there are still very slight differences

424
00:26:37,622 --> 00:26:40,954
between the cloud providers that still make it fairly difficult

425
00:26:41,072 --> 00:26:44,254
to kind of do this as seamlessly as they

426
00:26:44,292 --> 00:26:47,646
want to. So it's already a very large step in that the

427
00:26:47,668 --> 00:26:51,114
data is already available that was already a big bottleneck

428
00:26:51,162 --> 00:26:55,150
that they had that was removed by using something like a multicloud cluster.

429
00:26:55,230 --> 00:26:58,434
And having the data readily available on all three

430
00:26:58,472 --> 00:27:02,130
of those makes this, it's a step closer to

431
00:27:02,280 --> 00:27:05,986
reality and possibility of being able to do this. But they are

432
00:27:06,008 --> 00:27:09,794
still working out those kinks with the slight configuration differences

433
00:27:09,842 --> 00:27:14,006
between them. But I'll definitely update you if they get this to work.

434
00:27:14,188 --> 00:27:18,486
And finally, there's a last kind of almost very extreme

435
00:27:18,598 --> 00:27:22,262
scenario of cost optimization here. So this client

436
00:27:22,326 --> 00:27:26,106
was a major auto manufacturing company, and what

437
00:27:26,128 --> 00:27:29,974
they did was they had most of their applications primarily on AWS,

438
00:27:30,022 --> 00:27:33,534
and their workloads were running on AWS. But what they did was they had some

439
00:27:33,572 --> 00:27:36,842
conversations, and they had some conversations in tandem,

440
00:27:36,906 --> 00:27:40,078
they had some negotiations with aws and also

441
00:27:40,164 --> 00:27:44,046
with Azure. And what resulted from those conversations

442
00:27:44,158 --> 00:27:48,542
was that, let's say they had some better pricing deals

443
00:27:48,606 --> 00:27:52,286
at Azure. Well, they wanted to be able to switch

444
00:27:52,478 --> 00:27:55,586
just like that. Literally just like that. They wanted to

445
00:27:55,608 --> 00:27:58,982
be able to let their engineers know and say, hey,

446
00:27:59,036 --> 00:28:02,406
we were able to negotiate this better pricing deal for the amount

447
00:28:02,428 --> 00:28:05,730
of workloads that we're about to put onto this cloud provider.

448
00:28:05,810 --> 00:28:09,574
And so being able to migrate over that quickly and that

449
00:28:09,612 --> 00:28:12,982
seamlessly was something that was very appealing and something that

450
00:28:13,036 --> 00:28:16,282
multicloud could offer. How would it do this?

451
00:28:16,336 --> 00:28:20,026
So again, if they were based on a multi cloud cluster, which was something

452
00:28:20,128 --> 00:28:23,662
to consider, what could occur was that they could

453
00:28:23,716 --> 00:28:27,978
have their original clusters hosted in AWS,

454
00:28:28,074 --> 00:28:31,294
but the moment that they needed to switch over, they would just

455
00:28:31,332 --> 00:28:35,642
have to change the provider. They would just have to change the highest priority cloud

456
00:28:35,716 --> 00:28:39,406
provider in the cluster's configuration settings. And you can do this via

457
00:28:39,438 --> 00:28:43,026
the atlas Ui, or you could do this via the command line.

458
00:28:43,128 --> 00:28:47,026
And so this would gracefully roll over to the destination

459
00:28:47,138 --> 00:28:50,454
cloud provider that they wanted to do, and that would

460
00:28:50,492 --> 00:28:53,894
allow them to be able to make these kinds of very quick changes.

461
00:28:54,012 --> 00:28:57,766
Now, of course, caveat. Depending on the size of

462
00:28:57,788 --> 00:29:01,518
data they had, depending on if their applications were architected

463
00:29:01,554 --> 00:29:04,954
in a way that cloud take advantage of this, where it was truly just

464
00:29:04,992 --> 00:29:09,050
the data needing to be migrated over different cloud providers, then yes,

465
00:29:09,120 --> 00:29:12,474
this is a possibility. Right. So I just want to preface that

466
00:29:12,512 --> 00:29:15,918
and not to kind of just generalize and say yes, do a

467
00:29:15,924 --> 00:29:20,106
multicloud cluster and all your problems are solved. That's not the case. Your applications

468
00:29:20,138 --> 00:29:24,034
need to be architectures in a way that could take advantage of that. And if

469
00:29:24,072 --> 00:29:27,970
they are, then multicloud clusters could be a very nice

470
00:29:28,040 --> 00:29:31,986
thing to supplement that kind of architectures that you're looking for.

471
00:29:32,088 --> 00:29:35,666
So weve spoken about these different client scenarios and

472
00:29:35,688 --> 00:29:39,702
now I want to talk about how this affects developers. So as

473
00:29:39,756 --> 00:29:43,174
devs we again want to use the best

474
00:29:43,212 --> 00:29:46,566
tools for the job. If you ask anyone in your network, if you have

475
00:29:46,588 --> 00:29:50,458
looked at the Internet, if you have read anything about any part of

476
00:29:50,544 --> 00:29:54,554
cloud development, you'll know that there are preferences. You might have

477
00:29:54,592 --> 00:29:57,962
your own preferences about what to use and from

478
00:29:58,016 --> 00:30:02,090
where. You ask devs and they'll say yes, I only use AWS

479
00:30:02,170 --> 00:30:05,822
Lambda because it's the best tool for the job. Or if you

480
00:30:05,876 --> 00:30:09,658
want to do anything with machine leveraging or artificial intelligence,

481
00:30:09,754 --> 00:30:13,534
most developers will say yeah, GCP is the place to go there,

482
00:30:13,572 --> 00:30:15,860
although Azure is catching up quite quickly.

483
00:30:16,390 --> 00:30:19,826
But for the most part they did have a stronghold. And when you

484
00:30:19,848 --> 00:30:22,930
think ML and AI, you think GCP's platform.

485
00:30:23,080 --> 00:30:26,306
And then there is a very large segment of developers who

486
00:30:26,328 --> 00:30:29,974
are also on the Azure ecosystem. A lot of european companies are full

487
00:30:30,012 --> 00:30:33,366
azure ecosystems, and so in that scenario they want

488
00:30:33,388 --> 00:30:36,818
to use Azure DevOps, they want to use Azure active directory.

489
00:30:36,914 --> 00:30:40,458
But as we start to evolve and start to use more of

490
00:30:40,544 --> 00:30:43,798
these cloud services, as we start to expand,

491
00:30:43,974 --> 00:30:47,674
we find that sometimes we do want to use some of the other

492
00:30:47,712 --> 00:30:51,114
tools that may be on other cloud providers, especially if

493
00:30:51,152 --> 00:30:55,134
we are on a single cloud provider now or

494
00:30:55,172 --> 00:30:58,126
have been feeling the pains of some vendor lock in.

495
00:30:58,148 --> 00:31:01,406
Maybe the initial choice that was made when you moved to the

496
00:31:01,428 --> 00:31:04,782
cloud was not the best one, and you find that another one would actually

497
00:31:04,836 --> 00:31:09,086
serve you better. There are all kinds of reasons why we would want that flexibility

498
00:31:09,198 --> 00:31:12,354
to be able to use the best tool for the job.

499
00:31:12,472 --> 00:31:15,794
And so something like a multicloud cluster can

500
00:31:15,832 --> 00:31:19,014
help you get there. And the reason we say this is

501
00:31:19,052 --> 00:31:22,534
because for the most part, the biggest bottleneck has

502
00:31:22,572 --> 00:31:26,434
been the data. It's always been how do we move the relevant

503
00:31:26,482 --> 00:31:29,706
data that we need to use the services that we need on

504
00:31:29,728 --> 00:31:33,226
these other cloud providers? And now there's a plausible solution for

505
00:31:33,248 --> 00:31:37,222
that through multicloud clusters. The next thing as developers

506
00:31:37,286 --> 00:31:40,686
that we need to consider is that we're now pretty much

507
00:31:40,788 --> 00:31:44,922
responsible for even higher availability and even lower latency.

508
00:31:45,066 --> 00:31:48,766
So with all the outages that have occurred and as

509
00:31:48,788 --> 00:31:52,186
we expand and have to cover global markets,

510
00:31:52,298 --> 00:31:55,666
there's more of an ask for us to be sure that we

511
00:31:55,688 --> 00:31:58,706
can provide the same experience to all of our customers,

512
00:31:58,808 --> 00:32:02,402
to anyone that's using our application. And so there

513
00:32:02,456 --> 00:32:05,806
are a lot of reasons why we may need to

514
00:32:05,848 --> 00:32:09,090
take advantage of other cloud providers regions,

515
00:32:09,250 --> 00:32:12,646
either because there's a region that is only covered by

516
00:32:12,668 --> 00:32:16,198
one cloud provider, which is very common, or you

517
00:32:16,284 --> 00:32:19,898
need to have absolute availability for very specific

518
00:32:19,984 --> 00:32:23,898
applications like the emergency services application, where outages are

519
00:32:23,984 --> 00:32:27,642
not possible, they should never occur. And so in this

520
00:32:27,696 --> 00:32:31,382
kind of architecture, having a multicloud cluster

521
00:32:31,526 --> 00:32:35,454
gives us a much wider range and a much larger set

522
00:32:35,492 --> 00:32:39,166
of regions to work with to help us solve these problems

523
00:32:39,268 --> 00:32:42,914
and make sure we're able to deliver with these kinds of

524
00:32:42,952 --> 00:32:46,078
asks for availability. And lastly,

525
00:32:46,254 --> 00:32:49,474
as we start to become more comfortable with this and start to

526
00:32:49,512 --> 00:32:53,202
use multicloud clusters and start making

527
00:32:53,256 --> 00:32:56,690
it easier for our applications to take advantage of

528
00:32:56,760 --> 00:33:00,390
all of the cloud providers if it warrants it. I think the next thing that

529
00:33:00,460 --> 00:33:03,574
us as devs will have to worry about now is making sure

530
00:33:03,612 --> 00:33:07,042
that we're as cloud agnostic as possible. So wherever

531
00:33:07,106 --> 00:33:10,370
we happen to put our data, or wherever our clients

532
00:33:10,450 --> 00:33:14,106
ask us to put our data, if we're in a similar industry like

533
00:33:14,128 --> 00:33:17,898
Fiserv, where we don't get to choose, having that availability to

534
00:33:17,984 --> 00:33:21,590
serve the customers where they are is our ultimate goal.

535
00:33:21,750 --> 00:33:25,054
So now I'll quickly go through the last kind of

536
00:33:25,092 --> 00:33:29,022
scenario that we've seen where multicloud has also been a great fit, and that's future

537
00:33:29,076 --> 00:33:32,358
proofing, specifically with mergers and acquisitions.

538
00:33:32,474 --> 00:33:36,414
So another very common use case is that we have european

539
00:33:36,462 --> 00:33:40,254
companies who acquire other companies, and the european

540
00:33:40,302 --> 00:33:44,814
companies are mostly based in Azure. They're a full Azure ecosystem.

541
00:33:44,942 --> 00:33:48,642
And when they acquire these other companies, most of these acquisitions,

542
00:33:48,706 --> 00:33:51,894
they are on AWS. So what needs to happen now is

543
00:33:51,932 --> 00:33:55,510
the cross cloud migration. So how does this work?

544
00:33:55,580 --> 00:33:58,774
Well, again, custom scripting is always an option, but we

545
00:33:58,812 --> 00:34:01,946
already know about the flaws with that, so I'll already move on

546
00:34:01,968 --> 00:34:05,370
to the next one. Now, in this particular scenario,

547
00:34:05,790 --> 00:34:09,094
this acquisition that occurred, again, this client

548
00:34:09,142 --> 00:34:12,526
was already in MongoDB Atlas, and with that there

549
00:34:12,548 --> 00:34:15,934
was the option to use live migration, which is something that

550
00:34:15,972 --> 00:34:19,358
is offered through MongoDB Atlas. So what happens here

551
00:34:19,444 --> 00:34:23,078
is that they could set up a destination Atlas

552
00:34:23,114 --> 00:34:27,074
cluster in Azure, and that would then live migrate from

553
00:34:27,112 --> 00:34:30,626
the AWS clouds and the AWS nodes over to

554
00:34:30,648 --> 00:34:34,642
Azure. But the problem with this, even with this option

555
00:34:34,696 --> 00:34:38,194
in Atlas already, is that again, it's a separate service

556
00:34:38,312 --> 00:34:41,954
and it's also a very big hassle, because the connection

557
00:34:42,002 --> 00:34:45,254
string needs to be properly cut over. You would need to bounce it

558
00:34:45,292 --> 00:34:49,414
to make sure that the traffic is now going to the correct place. And even

559
00:34:49,452 --> 00:34:53,066
this little bit of manual intervention just was not a

560
00:34:53,088 --> 00:34:56,266
good thing for them. They did not want this, they wanted it

561
00:34:56,288 --> 00:34:59,818
to be more seamless than what was already possible.

562
00:34:59,984 --> 00:35:03,790
So again, in this particular case, multi cloud cluster made

563
00:35:03,860 --> 00:35:07,230
sense. And this is because what they already

564
00:35:07,300 --> 00:35:11,290
had, they already had MongoDB Atlas foundation.

565
00:35:11,450 --> 00:35:15,406
They needed to do was just change the cluster settings to not only be

566
00:35:15,428 --> 00:35:19,406
a multi cloud cluster, but to also change the highest priority region

567
00:35:19,518 --> 00:35:22,830
and cloud provider to be from AWS to Azure.

568
00:35:22,910 --> 00:35:25,986
So how does this do this? Right, the whole reason they choose this is that

569
00:35:26,008 --> 00:35:29,794
they want a graceful rollover and they want to make sure that it all properly

570
00:35:29,842 --> 00:35:33,590
cuts over. Well, by default, MongoDB always has

571
00:35:33,660 --> 00:35:37,446
a three node replica set. That's a minimum. And this is always

572
00:35:37,548 --> 00:35:41,146
so that we can ensure at least a reliable election with

573
00:35:41,168 --> 00:35:44,186
the bare minimum of three. That means there's one primary and

574
00:35:44,208 --> 00:35:47,578
there's two secondaries. Now, when we would begin

575
00:35:47,664 --> 00:35:51,738
this kind of cross cloud migration through a multi cloud cluster,

576
00:35:51,834 --> 00:35:55,850
we would first start migrating over the first two secondaries,

577
00:35:55,930 --> 00:35:59,946
so we'd migrate over the ones on AWS

578
00:36:00,058 --> 00:36:04,154
over to Azure, and then we would elect

579
00:36:04,292 --> 00:36:07,842
a new primary. So the previous primary was on

580
00:36:07,896 --> 00:36:11,714
AWS, but now we would ensure that it was now

581
00:36:11,752 --> 00:36:15,202
on the new cloud provider, which is azure, and then finally

582
00:36:15,336 --> 00:36:18,786
we would migrate over the remaining secondary.

583
00:36:18,898 --> 00:36:22,454
And what was really great about this was that of course it was

584
00:36:22,492 --> 00:36:25,874
all automated and taken care of by MongoDB Atlas.

585
00:36:25,922 --> 00:36:29,626
But the best part about it was you did not have to change the

586
00:36:29,648 --> 00:36:33,194
connection string, which was the biggest point of contention in

587
00:36:33,232 --> 00:36:37,510
this scenario. So this allowed them to do this cross cloud migration

588
00:36:37,670 --> 00:36:40,926
much more gracefully, and to also ensure that

589
00:36:41,028 --> 00:36:45,022
once this migration was finished, that it would properly move

590
00:36:45,076 --> 00:36:48,986
traffic over the right way without any downtime.

591
00:36:49,098 --> 00:36:53,114
And that's kind of the buffet of scenarios

592
00:36:53,162 --> 00:36:57,214
where multicloud is being used and is benefiting

593
00:36:57,342 --> 00:37:01,330
specific situations. And so now I will quickly

594
00:37:01,480 --> 00:37:04,866
go through how to create one. But before I

595
00:37:04,888 --> 00:37:08,610
do that, the last thing I'll leave you with is another

596
00:37:08,680 --> 00:37:12,114
quote from that think tank that I mentioned earlier was from Brad Lewis,

597
00:37:12,162 --> 00:37:15,330
and he's the vp and global lead from Dell Technologies,

598
00:37:15,410 --> 00:37:19,478
and he basically says if you want to start to have true portability

599
00:37:19,574 --> 00:37:23,114
of applications, obviously the data has to go

600
00:37:23,152 --> 00:37:27,014
with the application. And so this is why multicloud

601
00:37:27,062 --> 00:37:30,746
clusters have become a step in the right direction when it

602
00:37:30,768 --> 00:37:34,714
comes to even thinking about a multicloud solution,

603
00:37:34,842 --> 00:37:38,794
because the data has been the bottleneck for many of these scenarios

604
00:37:38,842 --> 00:37:42,814
and a lot of these customers. But by using something like a

605
00:37:42,852 --> 00:37:46,334
multicloud cluster, they're able to move closer to

606
00:37:46,372 --> 00:37:49,730
that multicloud strategy and take advantage of it.

607
00:37:49,800 --> 00:37:53,454
And so now I'm going to quickly roll

608
00:37:53,502 --> 00:37:56,906
over and show you how easy it is to set up a cluster.

609
00:37:57,038 --> 00:38:00,754
So if you've ever gone through the MongoDB Atlas UI

610
00:38:00,802 --> 00:38:04,578
and created a cluster, you'll see that this is what you are faced

611
00:38:04,594 --> 00:38:08,294
with, right? You decide which cloud provider and region you

612
00:38:08,332 --> 00:38:11,750
want to use, and that's where your initial cluster would be hosted.

613
00:38:11,830 --> 00:38:15,626
But now, if you wanted to do a multicloud cluster like

614
00:38:15,648 --> 00:38:19,290
I've been talking about, then you would just turn this on.

615
00:38:19,360 --> 00:38:23,374
And what this does is it basically shows you a couple more

616
00:38:23,412 --> 00:38:27,594
options. You now have the ability to choose from electable nodes,

617
00:38:27,642 --> 00:38:32,042
which are the ones that are the only ones that participate in elections.

618
00:38:32,186 --> 00:38:35,506
That means they're the ones that can be elected to be a

619
00:38:35,528 --> 00:38:39,310
primary in your production workloads or your operational workloads.

620
00:38:39,390 --> 00:38:42,402
And you also have read only nodes to choose from.

621
00:38:42,456 --> 00:38:46,286
So these are great for, let's say you have some markets

622
00:38:46,318 --> 00:38:49,814
that are really far away from where you are based in, but we need

623
00:38:49,852 --> 00:38:54,054
to make sure those reads for those markets are just as

624
00:38:54,172 --> 00:38:57,798
lightning fast as the ones in your local area. Well, you can

625
00:38:57,884 --> 00:39:01,178
spin up additional comes in those markets and have them point

626
00:39:01,264 --> 00:39:05,034
to those nodes and make it much nicer and

627
00:39:05,072 --> 00:39:09,226
faster and equivalent to these regions to be able to read for

628
00:39:09,328 --> 00:39:12,694
any of your service areas. And finally, there are a third

629
00:39:12,752 --> 00:39:16,110
type of node, which is the analytics node. And this is the one that the

630
00:39:16,180 --> 00:39:19,758
automl recommendation feature took advantage of.

631
00:39:19,844 --> 00:39:23,614
And this is the one that they chose when they set up an

632
00:39:23,652 --> 00:39:27,522
analytics node in GCP. But for now,

633
00:39:27,576 --> 00:39:30,914
I'll just show you how to set up a cluster really quick.

634
00:39:30,952 --> 00:39:34,174
So I'm going to choose actually GCP as my highest

635
00:39:34,222 --> 00:39:37,854
provider region because I am based in Las Vegas and they technically

636
00:39:37,902 --> 00:39:41,446
would be the closest one to me. And in this case, I'm going to set

637
00:39:41,468 --> 00:39:45,142
up what's called a two two one node distribution. So you'll see here,

638
00:39:45,276 --> 00:39:48,614
we always, always want to make sure that we have an

639
00:39:48,652 --> 00:39:52,126
odd number of comes, and that's to ensure reliable

640
00:39:52,178 --> 00:39:55,674
elections. If we had an even number, it's a possibility to

641
00:39:55,712 --> 00:39:58,874
have an elections be split down the middle and we would not be able to

642
00:39:58,912 --> 00:40:02,894
elect a proper primary. So this is why we ask for an odd number

643
00:40:02,932 --> 00:40:06,302
of nodes. So I'll add a couple more here, we'll choose the next

644
00:40:06,356 --> 00:40:10,350
closest regions to me and AWS.

645
00:40:10,690 --> 00:40:14,218
Remember, we want an odd number, so I'll do a two two one here.

646
00:40:14,324 --> 00:40:18,270
And in this kind of node distribution, this is kind of the bare node

647
00:40:18,350 --> 00:40:22,350
minimum that you would need to be able to provide equivalent

648
00:40:22,430 --> 00:40:26,274
read and write availability guarantees for your

649
00:40:26,312 --> 00:40:30,054
cluster. So this would be the multicloud cluster I'd have.

650
00:40:30,092 --> 00:40:33,554
I'd have the highest priority in Las Vegas and Azure

651
00:40:33,682 --> 00:40:37,286
and AWS as my secondaries. And because I don't want to

652
00:40:37,308 --> 00:40:41,194
make you wait and watch this cluster being generated, I've already

653
00:40:41,232 --> 00:40:44,586
done that. So this is what it would look like. You'll see that you had

654
00:40:44,608 --> 00:40:47,974
a preferred region in GCP, just as I've

655
00:40:48,022 --> 00:40:51,674
asked, with some secondaries in the AWS north

656
00:40:51,722 --> 00:40:55,514
region and the Azure California region

657
00:40:55,642 --> 00:40:58,734
as my scenarios. And if

658
00:40:58,772 --> 00:41:02,314
you remember when I said in a couple of the scenarios

659
00:41:02,362 --> 00:41:05,710
where all they had to do was change the provider,

660
00:41:06,130 --> 00:41:09,458
well, it really is just like that. You can either do this through the

661
00:41:09,464 --> 00:41:13,186
CLI, or if you needed to move over and

662
00:41:13,288 --> 00:41:17,030
change the cloud provider that you would have, you would just

663
00:41:17,180 --> 00:41:20,614
go over here and you would set the

664
00:41:20,652 --> 00:41:24,530
highest row to be your new highest priority

665
00:41:24,610 --> 00:41:27,686
cloud provider. So if I wanted to do what I just

666
00:41:27,708 --> 00:41:30,966
did here, which is set Azure to be my highest priority

667
00:41:30,998 --> 00:41:34,618
cloud provider and migrate some nodes over to

668
00:41:34,704 --> 00:41:37,818
Azure, this is all I would have to change.

669
00:41:37,984 --> 00:41:41,066
And let's go back here.

670
00:41:41,248 --> 00:41:44,474
And that's really how quickly it is to set it up.

671
00:41:44,512 --> 00:41:47,966
Obviously it'll take a little bit more time for the nodes to be deployed and

672
00:41:47,988 --> 00:41:51,758
all of that, but in terms of setting it up, that is all you

673
00:41:51,764 --> 00:41:55,126
would have to configure. If you wanted to set up a multi cloud cluster,

674
00:41:55,178 --> 00:41:58,366
this is also available to if you already had an existing

675
00:41:58,478 --> 00:42:02,334
MongoDB cluster that was not yet a multi cloud designation,

676
00:42:02,462 --> 00:42:05,934
you would also change it in the same fashion, and they are eligible

677
00:42:05,982 --> 00:42:09,926
to be changed into a multi cloud cluster in that way.

678
00:42:10,108 --> 00:42:13,622
And that's it. So, salamat, that means thank you

679
00:42:13,676 --> 00:42:17,666
in Tagalog. Thank you for taking the time to listen to me talk about multi

680
00:42:17,698 --> 00:42:21,494
cloud clusters. If you have any questions, please feel free

681
00:42:21,532 --> 00:42:25,174
to find me either on Twitter or in the chat. I will

682
00:42:25,212 --> 00:42:28,966
be here and I'll do my best to answer your questions. Thanks so

683
00:42:28,988 --> 00:42:31,660
much and I hope you enjoy. Enjoy the rest of Comp 42.

