1
00:00:00,250 --> 00:00:01,630
Are you an SRE,

2
00:00:03,570 --> 00:00:07,358
a developer, a quality

3
00:00:07,444 --> 00:00:11,162
engineer who wants to tackle the challenge of improving reliability

4
00:00:11,226 --> 00:00:14,970
in your DevOps? You can enable your DevOps for reliability

5
00:00:15,050 --> 00:00:18,286
with chaos native. Create your free account

6
00:00:18,388 --> 00:01:16,838
at chaos native. Litmus Cloud hello,

7
00:01:16,924 --> 00:01:19,862
and welcome to our talk, the elephant in the blameless war room,

8
00:01:19,916 --> 00:01:23,634
accountability. This talk started when Emily

9
00:01:23,682 --> 00:01:27,542
and I were encountering executives in Fortune 500

10
00:01:27,596 --> 00:01:31,466
companies who owned software reliability for these entire company

11
00:01:31,568 --> 00:01:35,002
that didn't really believe in blameless culture. They would ask us

12
00:01:35,056 --> 00:01:38,454
point blank, well, somebody so has to get fired,

13
00:01:38,502 --> 00:01:42,422
right? And that actually was very poignant,

14
00:01:42,566 --> 00:01:46,106
and it got me thinking, what about accountability?

15
00:01:46,298 --> 00:01:49,774
And so that's something that Emily and I spent a lot of time thinking

16
00:01:49,892 --> 00:01:53,646
and distilled the answers in this talk. We really wanted to reconcile the

17
00:01:53,668 --> 00:01:57,342
idea of being totally blameless, but still holding personal accountability

18
00:01:57,406 --> 00:02:00,274
when it was the best solution. And we're really excited to share what we found

19
00:02:00,312 --> 00:02:04,034
today. First, a round of

20
00:02:04,072 --> 00:02:07,346
intros. I'm Christina. I'm on the strategy

21
00:02:07,378 --> 00:02:11,286
team at Blameless, strategizing for executive team cohesion and

22
00:02:11,308 --> 00:02:15,362
also market positioning. I'm really passionate about making blameless

23
00:02:15,426 --> 00:02:18,614
culture work, not only for engineers, but also for

24
00:02:18,652 --> 00:02:22,086
business leaders. And I'm Emily. I'm a content writer at Blameless.

25
00:02:22,198 --> 00:02:26,154
I'm originally an outsider to the world of SRE, but I've been really excited to

26
00:02:26,192 --> 00:02:29,500
learn about the space and to start sharing my perspectives with the community.

27
00:02:30,830 --> 00:02:34,602
So we started thinking about factors that have a huge impact

28
00:02:34,666 --> 00:02:38,494
on business value. And one of the major ones agreed upon by every

29
00:02:38,532 --> 00:02:42,426
study is developer velocity. Then we found that a major factor

30
00:02:42,458 --> 00:02:45,746
in developer velocity is psychological safety. And what do you

31
00:02:45,768 --> 00:02:49,790
think is a major factor in psychological safety? Blameless culture.

32
00:02:49,870 --> 00:02:52,818
That would be correct. So it really is a big deal.

33
00:02:52,904 --> 00:02:56,066
You can't underestimate the importance of being blameless.

34
00:02:56,258 --> 00:02:59,442
Yeah. And especially when speaking with business leaders,

35
00:02:59,506 --> 00:03:03,126
it's really important to speak in their language, and that is

36
00:03:03,148 --> 00:03:06,546
the currency of communication. And so showing

37
00:03:06,578 --> 00:03:10,534
that there is business value in having a blameless culture is tremendously

38
00:03:10,582 --> 00:03:13,894
powerful. So picture

39
00:03:13,942 --> 00:03:17,546
this. We have an engineer working one night on

40
00:03:17,568 --> 00:03:20,606
the testing environment admin panel. However,

41
00:03:20,708 --> 00:03:24,446
on this dark and stormy night, they slowly realize this

42
00:03:24,468 --> 00:03:29,082
isn't the testing environment. This is the production environment.

43
00:03:29,226 --> 00:03:32,590
Of course, these changes lead to a major incident.

44
00:03:33,750 --> 00:03:37,106
And just at this time, the door opens and the

45
00:03:37,128 --> 00:03:40,450
executive walks in and asks, what happened?

46
00:03:40,600 --> 00:03:44,066
Who's responsible? So this is

47
00:03:44,088 --> 00:03:47,606
a pretty chaotic situation. A lot of things have gone wrong,

48
00:03:47,708 --> 00:03:51,826
and a lot of emotions are running high. Let's break down what happened. The shared

49
00:03:51,858 --> 00:03:55,062
reality is pretty simple. Leadership walked in they asked,

50
00:03:55,196 --> 00:03:59,162
what happened? Who's responsible? Probably their forehead was pretty

51
00:03:59,216 --> 00:04:02,550
furrowed, their voice was raised. They're a little agitated,

52
00:04:02,630 --> 00:04:06,182
speaking faster, and physically hovering around people's desks,

53
00:04:06,246 --> 00:04:09,846
really trying to get to the bottom of this. Now, as members

54
00:04:09,968 --> 00:04:14,094
of the engineering team, how would this shared reality be

55
00:04:14,132 --> 00:04:17,982
interpreted? Well, it's very natural and

56
00:04:18,036 --> 00:04:21,950
human to feel blame and frustrated or even afraid,

57
00:04:22,290 --> 00:04:26,462
imagining all different scenarios about what are the possible repercussions

58
00:04:26,526 --> 00:04:30,286
could be, and that could make it really difficult to focus on resolving

59
00:04:30,318 --> 00:04:33,810
the issue. So even if that's what the engineering team's thinking,

60
00:04:33,880 --> 00:04:37,302
let's look inside the head of the leadership. What might they be going

61
00:04:37,356 --> 00:04:41,014
through? Exactly? And heres I wanted to say that

62
00:04:41,132 --> 00:04:44,610
in psychology, there's this idea of a fundamental attribution

63
00:04:44,690 --> 00:04:48,546
error, where when we feel hurt,

64
00:04:48,658 --> 00:04:52,394
we assume that the other person has bad intentions. War is

65
00:04:52,432 --> 00:04:56,154
a bad person, whereas when we hurt someone else,

66
00:04:56,272 --> 00:04:59,946
as you can see, we would assume that that was an accident. I was

67
00:04:59,968 --> 00:05:03,470
just having a bad day. And so it's very natural and human,

68
00:05:03,540 --> 00:05:06,954
again to feel blamed and assume that that is the objective

69
00:05:07,002 --> 00:05:11,162
reality, when really the only shared reality is that leadership

70
00:05:11,226 --> 00:05:15,086
asked, who's responsible? And these lets. So let's

71
00:05:15,118 --> 00:05:18,894
see what might actually be going on with the leadership

72
00:05:19,022 --> 00:05:23,266
without assigning intent based on the

73
00:05:23,288 --> 00:05:26,694
engineering team's feelings. So their

74
00:05:26,732 --> 00:05:30,006
goals are probably very similar to the engineering teams. They're really

75
00:05:30,028 --> 00:05:34,338
just focused on resolving the incident, preventing it from reoccurring and restoring.

76
00:05:34,354 --> 00:05:37,834
True, with these stakeholders. Now, sometimes we

77
00:05:37,872 --> 00:05:42,246
could have the same objectives and get there through different paths.

78
00:05:42,358 --> 00:05:45,706
And so, given what we each know as

79
00:05:45,808 --> 00:05:49,530
the engineering team versus leadership, we might think that

80
00:05:49,680 --> 00:05:53,406
for the leader, holding someone responsible is the best path forward,

81
00:05:53,508 --> 00:05:57,070
whereas for the engineer, there might be other paths. So really

82
00:05:57,220 --> 00:06:01,194
it's easy to think, oh, leadership doesn't respect psychological

83
00:06:01,242 --> 00:06:05,118
safety. They're wrong. This is not the way to resolve incidents. I hate

84
00:06:05,134 --> 00:06:08,450
this toxic culture, but that doesn't actually solve the problem.

85
00:06:08,600 --> 00:06:12,162
And what I found in my experience is that it

86
00:06:12,216 --> 00:06:16,094
really helps when you try to step into the other person's shoes

87
00:06:16,142 --> 00:06:20,274
and see how, under their set of assumptions, their conclusion

88
00:06:20,322 --> 00:06:23,846
may be reasonable or logical. And once you uncover those

89
00:06:23,868 --> 00:06:27,458
implicit assumptions, then you can directly speak to those as a

90
00:06:27,484 --> 00:06:30,460
starting point of influencing and causing change.

91
00:06:31,230 --> 00:06:34,502
So again, this you can easily

92
00:06:34,566 --> 00:06:37,786
see as a starting source of conflict. What starts to

93
00:06:37,808 --> 00:06:41,366
dissolve conflict is when you start to see commonalities

94
00:06:41,478 --> 00:06:44,518
between the two parties. So here we can see that not only do they have

95
00:06:44,544 --> 00:06:47,774
the blame goals, but they're really feeling the same way too. Both of them are

96
00:06:47,812 --> 00:06:51,086
pretty tense. They're pretty stressed, and they're very afraid of

97
00:06:51,108 --> 00:06:54,866
the conference this incident could have. They're both very motivated to

98
00:06:54,888 --> 00:06:58,846
resolve the incident and restore the service to functionality. So let's

99
00:06:58,878 --> 00:07:02,706
start breaking down. How can we cover this bridge? How can we bring these two

100
00:07:02,728 --> 00:07:06,698
groups together? This will be the main backbone

101
00:07:06,734 --> 00:07:09,762
for our talk. We're going to establish empathy for leadership,

102
00:07:09,906 --> 00:07:13,762
understand their goals and perspective, and look at the assumptions

103
00:07:13,826 --> 00:07:16,914
and perspective differences that might be driving their blameful

104
00:07:16,962 --> 00:07:20,838
behavior. We'll address their concerns in three major areas. Looking at the incident,

105
00:07:20,934 --> 00:07:24,966
the engineers involved, and the stakeholders trust, then we'll

106
00:07:24,998 --> 00:07:28,790
cover how to be blamelessly accountable, how to incorporate accountability

107
00:07:28,870 --> 00:07:32,414
into the best solution. So what are the assumptions that could

108
00:07:32,452 --> 00:07:36,094
cause leadership wanting to hold someone accountable as a way of

109
00:07:36,132 --> 00:07:39,294
resolving the incident to be correct? That's the question

110
00:07:39,332 --> 00:07:42,926
that Emily and I asked ourselves. So they might

111
00:07:42,948 --> 00:07:46,754
assume some things about the incident, like it just straight up should never have happened,

112
00:07:46,872 --> 00:07:50,526
or that the best way to deter other people from making the same error

113
00:07:50,558 --> 00:07:54,414
is to punish someone. And some of the assumptions

114
00:07:54,462 --> 00:07:58,382
about the engineer could include a skilled engineer would

115
00:07:58,456 --> 00:08:02,054
never make this mistake. If someone made a mistake like this, it must mean

116
00:08:02,092 --> 00:08:04,578
that there's an issue with their competence or skill.

117
00:08:04,754 --> 00:08:07,560
Removing the engineer will remove the problem.

118
00:08:08,090 --> 00:08:12,570
And without punishment, these engineer won't fully understand the impact of their mistake.

119
00:08:12,990 --> 00:08:16,298
They could have some assumptions about how the stakeholders are feeling too.

120
00:08:16,464 --> 00:08:19,878
They might believe that the stakeholder wants to see someone singled

121
00:08:19,894 --> 00:08:23,406
out and perhaps fired, that this is the most persuasive way to

122
00:08:23,428 --> 00:08:26,926
convince them that these incident is resolved. They might

123
00:08:26,948 --> 00:08:30,906
also think the stakeholders are expecting some sort of fairness, that because they've

124
00:08:30,938 --> 00:08:34,798
experienced pain from the incident, they'd want to see pain experienced by the engineering

125
00:08:34,814 --> 00:08:38,194
team as well. But we know this

126
00:08:38,232 --> 00:08:41,874
isn't really how things will play out. Even though blame seems like

127
00:08:41,912 --> 00:08:45,470
a good way to achieve your goals, given these assumptions,

128
00:08:45,550 --> 00:08:48,994
we know that systemic changes are far more enduring and beneficial.

129
00:08:49,122 --> 00:08:52,646
So how do we cover this gap of understanding? Absolutely.

130
00:08:52,748 --> 00:08:55,654
And do remember that even at this point,

131
00:08:55,772 --> 00:08:59,474
both the engineering team and the leadership have the same

132
00:08:59,532 --> 00:09:03,162
shared goals. They both want to resolve the incident. It's just given

133
00:09:03,216 --> 00:09:06,746
what we know about incidents, engineer, stakeholders, we might have

134
00:09:06,768 --> 00:09:08,650
different ways of getting to that outcome.

135
00:09:10,430 --> 00:09:13,486
So first, let's understand leadership's perspective on the

136
00:09:13,508 --> 00:09:17,214
incident. If they assume that it should never

137
00:09:17,252 --> 00:09:21,034
have happened, punishment will deter others from making the same error.

138
00:09:21,162 --> 00:09:24,466
Then how can we skillfully address it so

139
00:09:24,488 --> 00:09:28,622
that we challenges their mind without triggering their defense mechanisms?

140
00:09:28,766 --> 00:09:32,702
And so Emily and I came up with a way to essentially uncover

141
00:09:32,766 --> 00:09:35,954
these perspective differences. What better way to have

142
00:09:35,992 --> 00:09:39,640
open minded conversations than to ask questions?

143
00:09:40,010 --> 00:09:43,906
And keep in mind that these are not questions that we are expecting

144
00:09:43,938 --> 00:09:47,446
you to ask during the incident. Everyone is

145
00:09:47,468 --> 00:09:51,414
still stressed and tensed and focused on the resolution during the incident.

146
00:09:51,542 --> 00:09:55,018
We highly suggest that you ask about these probing questions

147
00:09:55,184 --> 00:09:58,714
when both the leader and you are in a calm state of mind where you

148
00:09:58,752 --> 00:10:02,170
can meaningfully engage in a conversation and where you're both open

149
00:10:02,240 --> 00:10:05,520
to changing your mind. So these are some of the possible questions.

150
00:10:06,130 --> 00:10:09,982
One thing to ask is, is 100% reliability even possible?

151
00:10:10,116 --> 00:10:13,918
Is it worth the cost if it is? And what kind

152
00:10:13,924 --> 00:10:17,314
of tradeoff are you willing to make between trying to prevent incidents and

153
00:10:17,352 --> 00:10:20,754
preparing yourself to react to them, given that you only have

154
00:10:20,792 --> 00:10:24,866
so much engineering capacity? Another question to ask is, are there other ways of

155
00:10:24,888 --> 00:10:27,350
making people more blameful than using punishment?

156
00:10:29,370 --> 00:10:33,110
So let's see how we could address leadership's concerns about the incident.

157
00:10:34,810 --> 00:10:38,710
So one thing to emphasize when you're having these conversations is that systemic changes

158
00:10:38,780 --> 00:10:42,122
are more enduring and beneficial. That if you actually change

159
00:10:42,176 --> 00:10:45,674
the system at heart of the incident, then you'll do way

160
00:10:45,712 --> 00:10:48,970
more to prevent the incident from recurring than just swapping out people.

161
00:10:49,120 --> 00:10:52,878
And it helps to give a specific example of how in the past

162
00:10:52,964 --> 00:10:56,894
you've had a system change that was more effective than letting go

163
00:10:56,932 --> 00:11:00,826
of someone. Based on Aristotle's model of persuasion,

164
00:11:00,938 --> 00:11:04,834
ethos, pathos, logos, it's actually very important to

165
00:11:04,872 --> 00:11:08,542
strategically sequence your appeals based on how likely

166
00:11:08,606 --> 00:11:11,714
the person you're talking to will agree with you.

167
00:11:11,832 --> 00:11:15,406
So, for example, if you anticipate that the leader is less likely

168
00:11:15,438 --> 00:11:19,094
to agree with you, it's more important to establish your credibility first,

169
00:11:19,212 --> 00:11:23,094
then use logic, then use emotional appeal at the end. So don't start

170
00:11:23,132 --> 00:11:26,374
with, oh, the team feels really guilty. But start

171
00:11:26,412 --> 00:11:29,766
with, I've been doing this for many years, and I've

172
00:11:29,798 --> 00:11:33,414
seen this example where the systemic change was a lot more effective

173
00:11:33,462 --> 00:11:36,586
and enduring than actually letting go of someone and

174
00:11:36,608 --> 00:11:40,726
then say, from an emotional standpoint, what helping the leader

175
00:11:40,758 --> 00:11:43,230
understand how the engineer might be feeling.

176
00:11:44,450 --> 00:11:48,414
Another thing to really make clear is that there's no way that

177
00:11:48,452 --> 00:11:52,186
complex systems won't ever fail. There's no way to prevent incidents

178
00:11:52,218 --> 00:11:56,094
100% of the time, as much as you might try. Yeah. So Dave

179
00:11:56,142 --> 00:11:59,554
Renson, the former head of customer reliability engineering at

180
00:11:59,592 --> 00:12:03,362
Google, would often in his talk say to air is human,

181
00:12:03,496 --> 00:12:07,234
the sun is not 100% reliable because, well, it's unreliable

182
00:12:07,282 --> 00:12:10,790
at the very end, and then our heres are not 100%

183
00:12:10,860 --> 00:12:14,934
reliable, and complex system failures are inevitably going to

184
00:12:14,972 --> 00:12:18,146
fail. So helping the leader understand that software doesn't

185
00:12:18,178 --> 00:12:21,340
just work the way it probably did in the 90s

186
00:12:21,950 --> 00:12:24,330
is going to be very helpful.

187
00:12:25,390 --> 00:12:29,046
Another thing to really get across, and this is more kind of on the emotional

188
00:12:29,078 --> 00:12:32,618
side, maybe, as you were talking about, that engineers are not at their best when

189
00:12:32,624 --> 00:12:35,454
they're stressed. If they're in a fight or flight mode where they think, like,

190
00:12:35,492 --> 00:12:38,254
every mistake they make could lead to the end of their job,

191
00:12:38,372 --> 00:12:41,600
they're not going to be able to focus at all on actually solving the problem.

192
00:12:41,970 --> 00:12:45,554
Absolutely. While zero one and two can wait until after

193
00:12:45,592 --> 00:12:49,202
the incident is resolved, you can see that zero three is actually

194
00:12:49,256 --> 00:12:52,578
the key to helping the leader understand

195
00:12:52,744 --> 00:12:56,174
the situation, to give enough room for the engineers

196
00:12:56,222 --> 00:12:59,826
to focus on resolving the issue in the moment. So instead of letting

197
00:12:59,858 --> 00:13:03,746
leadership hover and ask what's going on, what's going to happen, who's responsible?

198
00:13:03,858 --> 00:13:07,846
You can say, hey, we're all here trying to resolve the problem as

199
00:13:07,868 --> 00:13:11,046
soon as possible, and engineers don't solve problems well in

200
00:13:11,068 --> 00:13:14,634
fight or flight mode. So it might be helpful if you could take some time

201
00:13:14,672 --> 00:13:17,766
and give the engineer team and give the engineering

202
00:13:17,798 --> 00:13:21,210
team some space to resolve the issue first, and then we'll come back

203
00:13:21,280 --> 00:13:25,118
to looking at what actually happened. Now let's take

204
00:13:25,124 --> 00:13:28,158
a look at how the leader might be feeling about the engineers involved in the

205
00:13:28,164 --> 00:13:31,726
incident. They might have some assumptions about the

206
00:13:31,748 --> 00:13:35,002
engineer that we covered before, like a skilled engineer

207
00:13:35,066 --> 00:13:38,226
would never make a mistake like this, or that if you just get

208
00:13:38,248 --> 00:13:42,100
rid of this one bad apple, then everything should be fine. Or that

209
00:13:42,630 --> 00:13:46,690
the engineers don't really understand these severity of an incident without punishment.

210
00:13:47,350 --> 00:13:50,886
So let's try to look at how we can bridge the gap in

211
00:13:50,908 --> 00:13:53,670
our perspectives that would lead them to have these assumptions.

212
00:13:55,210 --> 00:13:59,930
So ask them, do you think there are deeper causes of incidents beyond individuals?

213
00:14:00,910 --> 00:14:04,346
I love these open ended questions because it really gives

214
00:14:04,448 --> 00:14:08,330
everyone in the conversation a chance to do creative problem solvings together

215
00:14:08,480 --> 00:14:12,634
instead of fixating on things that have gone wrong. This question embeds

216
00:14:12,682 --> 00:14:15,920
some forward momentum towards solving the problem together.

217
00:14:17,090 --> 00:14:20,842
Another question is, do engineers understand the business impact of incidents?

218
00:14:20,906 --> 00:14:23,182
It could be that they don't, and there needs to be more of an open

219
00:14:23,236 --> 00:14:26,818
dialogue between leadership and engineering teams so that they can understand

220
00:14:26,904 --> 00:14:30,174
how their development choices will translate into money gained or lost

221
00:14:30,222 --> 00:14:33,826
by the company. More than likely, though, they do understand that

222
00:14:33,848 --> 00:14:37,590
the incident was severe, and they're probably already feeling plenty guilty.

223
00:14:37,930 --> 00:14:41,910
Yeah. So let's talk about addressing these leadership concerns

224
00:14:43,290 --> 00:14:47,694
about the engineer. Anyone in this position could have made that mistake.

225
00:14:47,842 --> 00:14:51,802
And so from Emily and I's conversation with the

226
00:14:51,936 --> 00:14:55,846
emergency physician at Stanford Emergency Medicine

227
00:14:56,038 --> 00:14:59,194
LA has said that there are more missed heart attacks in

228
00:14:59,232 --> 00:15:03,402
the US than what we would expect. Because doctors,

229
00:15:03,466 --> 00:15:07,360
even though they've gone through so many years of very tough training,

230
00:15:07,890 --> 00:15:11,070
will have interruptions. Sometimes their brain is holding a

231
00:15:11,140 --> 00:15:14,974
different set of contexts. Maybe these engineer was helping another engineer

232
00:15:15,022 --> 00:15:18,510
with a deploy in production and therefore coming back immediately

233
00:15:18,590 --> 00:15:22,226
under a time rush. Perhaps they didn't realize that this

234
00:15:22,248 --> 00:15:26,310
is actually not the testing environment, but these production environment.

235
00:15:28,250 --> 00:15:32,050
And another thing, again on more the emotional side of appeals,

236
00:15:32,210 --> 00:15:35,846
really try to create empathy between the leader and the engineer that

237
00:15:35,868 --> 00:15:39,414
was involved. Really emphasize that nobody wanted this outcome.

238
00:15:39,462 --> 00:15:43,018
It's very easy for leadership people to feel isolated in

239
00:15:43,024 --> 00:15:46,698
their roles, that only they can grasp the magnitude of

240
00:15:46,784 --> 00:15:50,154
all of these incidents. But the

241
00:15:50,192 --> 00:15:53,726
engineer is obviously suffering very much as well.

242
00:15:53,828 --> 00:15:57,374
And closing the gap on that empathy will allow them to understand a lot

243
00:15:57,412 --> 00:16:00,762
better. Absolutely. And it's also about building common ground,

244
00:16:00,826 --> 00:16:04,586
because for leaders who typically are people that take extremely

245
00:16:04,618 --> 00:16:07,842
high ownership of company performance, they will feel like,

246
00:16:07,896 --> 00:16:11,362
oh, I'm the only one that heres because they're not sure.

247
00:16:11,416 --> 00:16:15,454
They're also in a mode of stress when they're trying to assign

248
00:16:15,502 --> 00:16:19,014
blame, if that is the case. And so when that is happening,

249
00:16:19,132 --> 00:16:22,694
it's difficult to have the mental space to recognize that,

250
00:16:22,732 --> 00:16:26,006
no, we actually all feel bad. And it is possible that

251
00:16:26,028 --> 00:16:29,546
the engineer doesn't fully understand the business impact and that is these

252
00:16:29,568 --> 00:16:33,130
leadership's responsibility to actually help the engineering understand.

253
00:16:33,280 --> 00:16:36,282
And what I found is that it actually builds tremendous trust.

254
00:16:36,416 --> 00:16:39,862
When you can facilitate a conversation where the engineer

255
00:16:39,926 --> 00:16:43,166
does acknowledge the understanding of the

256
00:16:43,188 --> 00:16:46,686
business impact, it doesn't mean you're taking

257
00:16:46,868 --> 00:16:49,934
full responsibility of something that would have happened and could

258
00:16:49,972 --> 00:16:53,650
have happened with other people, but it means that you understand the pain

259
00:16:53,720 --> 00:16:56,660
that leadership is experiencing too.

260
00:16:58,070 --> 00:17:02,002
Another thing from a business perspective to emphasize is that it's way

261
00:17:02,056 --> 00:17:05,682
more costly to hire someone new than to train the existing team.

262
00:17:05,816 --> 00:17:09,798
Even if there are gaps in knowledge. It's way easier to bring someone up

263
00:17:09,804 --> 00:17:12,998
to speed than to hire someone brand new and teach them all

264
00:17:13,004 --> 00:17:16,694
of the intricacies of your system. As you can see, the first and

265
00:17:16,732 --> 00:17:20,134
third point heres are logical appeals and then these second one

266
00:17:20,172 --> 00:17:23,302
is an emotional appeal. So if the leader

267
00:17:23,366 --> 00:17:27,034
likely understands you, these you can start with can emotional appeal. But if

268
00:17:27,072 --> 00:17:30,570
you foresee them disagreeing with you first, start with one and three first.

269
00:17:30,720 --> 00:17:34,390
Finally, let's take a look at their perspective on stakeholder trust.

270
00:17:34,560 --> 00:17:38,238
Again, they might have some assumptions about what the stakeholders might want to see

271
00:17:38,324 --> 00:17:41,726
that they might want to see someone get fired as a way to

272
00:17:41,748 --> 00:17:45,554
be convinced that the incident is restored or to maintain some

273
00:17:45,592 --> 00:17:48,530
fairness among different teams and stakeholders.

274
00:17:49,110 --> 00:17:52,546
So again, here are some questions to ask to try to figure out where your

275
00:17:52,568 --> 00:17:56,774
perspectives are different on these situations. Like just ask open

276
00:17:56,812 --> 00:18:01,270
endedly, are there other ways to rebuild trust with stakeholders besides retribution?

277
00:18:02,650 --> 00:18:06,722
Another thing to ask is how will they respond to retribution versus

278
00:18:06,786 --> 00:18:10,262
being informed on your long term plans to resolve this incident

279
00:18:10,326 --> 00:18:14,442
and other incidents like it. As you can see, the first question heres also

280
00:18:14,576 --> 00:18:18,410
allows everyone to come together and brainstorm different options

281
00:18:18,560 --> 00:18:22,122
rather than to just say it's wrong to punish these engineer because

282
00:18:22,176 --> 00:18:25,934
that actually cuts the momentum of the conversation and

283
00:18:25,972 --> 00:18:29,230
stops it at that point. Whereas you can direct that energy towards

284
00:18:29,300 --> 00:18:32,674
a new option and see, okay, what are some other ways that we could build

285
00:18:32,712 --> 00:18:33,300
trust?

286
00:18:35,990 --> 00:18:39,266
So now that you've opened up this dialogue, how can you present your

287
00:18:39,288 --> 00:18:42,402
own perspective? One thing to really

288
00:18:42,456 --> 00:18:45,902
try to convince leadership of is that your action plan will inspire

289
00:18:45,966 --> 00:18:49,282
confidence that once it's explained to stakeholders, they'll see

290
00:18:49,336 --> 00:18:52,694
how it leads to in a more enduring solution. The other point to

291
00:18:52,732 --> 00:18:56,246
mention is to really show that you hear and acknowledge the

292
00:18:56,268 --> 00:18:59,958
pain of your customers and also all stakeholders impacted.

293
00:19:00,054 --> 00:19:03,370
So who else could be impacted by customers? Well,

294
00:19:03,520 --> 00:19:06,682
sometimes customer success team is often the team

295
00:19:06,736 --> 00:19:10,714
that is responsible for retention metrics. And so if customers are impacted

296
00:19:10,762 --> 00:19:14,170
by incidents, they may be more at risk of churn,

297
00:19:14,330 --> 00:19:18,014
which makes it harder for the customer success team to do their job.

298
00:19:18,132 --> 00:19:20,190
And so extending,

299
00:19:21,410 --> 00:19:24,834
so extending empathy and understanding not only to

300
00:19:24,872 --> 00:19:28,180
customers but to internal stakeholders as well,

301
00:19:28,710 --> 00:19:30,260
is very important.

302
00:19:31,830 --> 00:19:35,106
So let's return back to this incident where the leader has come

303
00:19:35,128 --> 00:19:38,306
in, asked what happened and who's responsible in

304
00:19:38,328 --> 00:19:41,574
the moment. How is the best way to respond to this? We think

305
00:19:41,612 --> 00:19:44,850
some of the elements it should have is to be very direct and succinct.

306
00:19:44,930 --> 00:19:48,434
No beating around the bush. Yeah, because that could actually make you seem

307
00:19:48,482 --> 00:19:52,410
suspicious and trying to hide something if you're beating around the bush.

308
00:19:53,150 --> 00:19:56,954
Another thing to really focus on is building common ground, looking for

309
00:19:56,992 --> 00:20:00,214
things that you can both agree on, things that you're both feeling and goals

310
00:20:00,262 --> 00:20:04,234
that you share. You also really want to create psychological

311
00:20:04,282 --> 00:20:07,562
safety. And if you see any rush to point fingers and blame,

312
00:20:07,626 --> 00:20:10,880
really try to alleviate that with some of the questions we mentioned before.

313
00:20:12,610 --> 00:20:16,686
Like I said, having shared goals is extremely important. So explicitly

314
00:20:16,718 --> 00:20:20,610
articulate them, make sure you're all on the same page and facing the same direction,

315
00:20:21,590 --> 00:20:25,458
and then give visibility into what these next steps will be.

316
00:20:25,624 --> 00:20:29,046
Now that you have set up the goals that you share. How are you going

317
00:20:29,068 --> 00:20:32,994
to achieve them without using blame? So let's go back to that moment where leadership

318
00:20:33,042 --> 00:20:36,742
walks in through the door. So what happened

319
00:20:36,796 --> 00:20:39,560
here? Who is responsible for this mess?

320
00:20:40,170 --> 00:20:43,686
It really could have been anyone. We're all focused on resolving

321
00:20:43,718 --> 00:20:47,562
the incident as quickly as possible. So why don't we give these team

322
00:20:47,616 --> 00:20:50,698
some time and space to focus on the resolution first?

323
00:20:50,864 --> 00:20:54,606
And I understand the impact this has on customers and we're committed to

324
00:20:54,628 --> 00:20:58,494
restoring stakeholder trust and we'll take full ownership of

325
00:20:58,532 --> 00:21:01,742
working towards preventing incidents like this in the future through

326
00:21:01,796 --> 00:21:05,326
in depth contributing, factor analysis and follow up actions over

327
00:21:05,348 --> 00:21:08,670
the next two weeks after this incident is resolved.

328
00:21:10,070 --> 00:21:13,378
That seems fair. I look forward to seeing what you find.

329
00:21:13,544 --> 00:21:16,994
Wow. That was actually a very scary and stressful

330
00:21:17,042 --> 00:21:21,190
experience for me, even hearing Emily say that because

331
00:21:21,340 --> 00:21:25,174
I could feel blamed. I felt like I needed to hold

332
00:21:25,212 --> 00:21:29,062
someone accountable in that moment. But I wanted to actually ask,

333
00:21:29,116 --> 00:21:32,874
how did you feel as you were asking those questions? I tried to really

334
00:21:32,912 --> 00:21:36,858
embody the feeling that this was a big deal and that I had the entire

335
00:21:36,944 --> 00:21:40,330
company perhaps riding on resolving this quickly.

336
00:21:40,480 --> 00:21:43,854
I really wanted to get across how passionate I felt about

337
00:21:43,892 --> 00:21:47,342
this going wrong and convey the importance to everyone

338
00:21:47,396 --> 00:21:50,846
else. So if that came across as scary, we can see now where

339
00:21:50,868 --> 00:21:55,054
these gaps start to pop up. Yeah. Wow. That's powerful.

340
00:21:55,182 --> 00:21:58,750
See, even when I was scared, I actually lost these ability

341
00:21:58,830 --> 00:22:02,274
in that moment to understand you. Heres just really

342
00:22:02,392 --> 00:22:06,018
prioritizing this issue. It came off definitely feeling like

343
00:22:06,104 --> 00:22:10,022
you're trying to hold a specific person responsible. So yeah,

344
00:22:10,076 --> 00:22:14,022
that was very powerful. Thanks. So immediate response is actually not enough.

345
00:22:14,156 --> 00:22:17,446
Let's look at what the follow up investigation could look like.

346
00:22:17,548 --> 00:22:20,166
Rather than saying this engineer screwed up,

347
00:22:20,268 --> 00:22:23,802
let's dig a little bit deeper and do the hard work and see

348
00:22:23,856 --> 00:22:27,546
what are other contributing factors. So as an example, we can

349
00:22:27,568 --> 00:22:31,198
return to our story from the start and ask a few questions

350
00:22:31,284 --> 00:22:34,730
about how this may have happened. Like why do the admin

351
00:22:34,810 --> 00:22:38,302
control panels for production and testing look

352
00:22:38,356 --> 00:22:42,286
really, really similar? Yeah. And should production have a

353
00:22:42,308 --> 00:22:45,402
big flashing banner? Production? This is production.

354
00:22:45,466 --> 00:22:49,566
This is production. Yeah. And maybe just a single person acting

355
00:22:49,598 --> 00:22:52,942
alone shouldn't be able to make these changes. Maybe there should be like an oversight

356
00:22:53,006 --> 00:22:56,674
someone has to review before they go through. And should we

357
00:22:56,712 --> 00:23:00,274
maybe be selective about the engineers who can make changes on the production

358
00:23:00,322 --> 00:23:03,478
admin panel? So just by digging into it,

359
00:23:03,644 --> 00:23:07,266
we can come up with all sorts of enduring systemic changes that can prevent

360
00:23:07,298 --> 00:23:10,682
this specific incident and all sorts of other incidents like it going forward.

361
00:23:10,816 --> 00:23:13,450
It's so much better than just getting rid of one engineer.

362
00:23:13,790 --> 00:23:17,514
Yeah. So again, you should have this follow up

363
00:23:17,552 --> 00:23:20,694
conversation a little while after the incident is resolved

364
00:23:20,822 --> 00:23:23,898
and you've uncovered these perspective differences. You've uncovered

365
00:23:23,914 --> 00:23:28,074
what assumptions they might have had, and then you can start really meaningfully

366
00:23:28,122 --> 00:23:31,738
implementing these changes. So now that you've done the investigation,

367
00:23:31,834 --> 00:23:35,426
you've come up with really great systematic changes that will

368
00:23:35,448 --> 00:23:39,234
help prevent issues like this in the future. Are you done?

369
00:23:39,432 --> 00:23:42,802
Well, as you can imagine, no. There's also follow up

370
00:23:42,856 --> 00:23:45,974
planning for reliability overall. So looking at these

371
00:23:46,012 --> 00:23:49,666
incidents, how does that inform the three pillars of planning

372
00:23:49,778 --> 00:23:53,058
people? Process tooling and process includes prioritization

373
00:23:53,154 --> 00:23:56,866
as well. So for people, how do incidents inform headcount

374
00:23:56,898 --> 00:24:00,474
planning? Do we need more people and process? How can we

375
00:24:00,512 --> 00:24:04,006
update runbooks or production readiness checklists?

376
00:24:04,198 --> 00:24:08,074
Is there things that we can do to consistently improve our

377
00:24:08,112 --> 00:24:11,914
performance? Resolving incidents in the future? And also consider investment

378
00:24:11,962 --> 00:24:15,326
in goals that will really up level the effectiveness of the

379
00:24:15,348 --> 00:24:18,926
engineering team. So just from one incident, we can dive into

380
00:24:19,028 --> 00:24:22,462
major priorities for the entire organization? Absolutely.

381
00:24:22,596 --> 00:24:25,906
Incidents can uncover issues that maybe the

382
00:24:25,928 --> 00:24:29,938
company isn't even ready to hear about yet. Because after asking

383
00:24:30,024 --> 00:24:33,634
enough whys of how something happened and not just going

384
00:24:33,672 --> 00:24:37,270
down one path of these tree, but exploring multiple options,

385
00:24:37,420 --> 00:24:40,934
it often ends up revealing something about

386
00:24:41,132 --> 00:24:44,722
leadership and also about how the team is structured.

387
00:24:44,866 --> 00:24:48,858
So there's really interesting insights when we dig deeply into

388
00:24:48,944 --> 00:24:52,106
incidents. So let's go back to

389
00:24:52,128 --> 00:24:55,674
the beginning of the talk where executives asked, someone still

390
00:24:55,712 --> 00:24:58,778
has to get fired, right? Well, sometimes,

391
00:24:58,944 --> 00:25:02,926
yes. But when is it fair to hold someone accountable in the

392
00:25:02,948 --> 00:25:06,158
traditional sense where you are actually letting someone go?

393
00:25:06,324 --> 00:25:09,566
Well, Emily and I came up with a number of prerequisite questions

394
00:25:09,668 --> 00:25:13,630
to ask as a starting point. So heres

395
00:25:13,700 --> 00:25:17,046
expectations for this person's job clear? Were they realistic?

396
00:25:17,098 --> 00:25:20,498
Were they well documented? Did they know what they were supposed to be doing?

397
00:25:20,664 --> 00:25:23,874
And were the mistakes of the incident a result of their lack of

398
00:25:23,912 --> 00:25:26,930
skill, good intentions, or honest effort?

399
00:25:27,770 --> 00:25:31,378
Have you been sharing feedback about their gaps in performance

400
00:25:31,474 --> 00:25:34,658
on a real, consistent basis, making sure they know that they're

401
00:25:34,674 --> 00:25:37,974
not up to par? And also, have you accounted for all

402
00:25:38,012 --> 00:25:41,930
other contributing factors? When it is in the context of an incident,

403
00:25:42,910 --> 00:25:46,346
holding someone accountable shouldn't be the easy way out. It shouldn't be something that you

404
00:25:46,368 --> 00:25:49,962
leap to as the simplest solution, but instead something

405
00:25:50,016 --> 00:25:54,126
that you resort to after accounting for every other circumstance that could have

406
00:25:54,148 --> 00:25:58,026
led to the mistake. And as you can see, this is a very distinct

407
00:25:58,058 --> 00:26:02,154
and separate process from incident resolution. This is performance

408
00:26:02,202 --> 00:26:05,854
management. And just because there are incidents which are normal

409
00:26:05,902 --> 00:26:09,646
and natural and happens with every company and every system. It doesn't

410
00:26:09,678 --> 00:26:13,250
mean that that can be a substitute for proper performance management.

411
00:26:15,110 --> 00:26:19,014
So let's talk about being blamelessly accountable, having your cake and eating it

412
00:26:19,052 --> 00:26:23,270
too, being both blameless in culture, but holding people accountable when necessary.

413
00:26:24,890 --> 00:26:28,700
Well, at Twitter, we learned that accountability faces forward.

414
00:26:29,070 --> 00:26:32,346
So it means that the team that is accountable will

415
00:26:32,368 --> 00:26:35,830
take full ownership of improving reliability from the incident

416
00:26:35,910 --> 00:26:39,546
point moving forward. It also means that

417
00:26:39,568 --> 00:26:43,486
you're separating these reliability outcomes from performance management. Like we

418
00:26:43,508 --> 00:26:46,714
were talking about before, performance management

419
00:26:46,762 --> 00:26:50,960
should never be a substitute for resolving the incident in the best possible way.

420
00:26:52,450 --> 00:26:55,886
And likewise, it shouldn't come at the cost of in depth contributing factor

421
00:26:55,918 --> 00:26:59,646
analysis. You shouldn't give up on trying to find other causes

422
00:26:59,678 --> 00:27:02,770
of incidents just because you've decided to hold someone accountable.

423
00:27:03,190 --> 00:27:06,750
Absolutely. So really, there's no trade off between

424
00:27:06,840 --> 00:27:10,562
blameless culture and accountability. It's not that if you are blameless,

425
00:27:10,626 --> 00:27:14,006
you sacrifice accountability. You could very much have both a

426
00:27:14,028 --> 00:27:17,706
blameless culture and also people feeling a tremendous sense of

427
00:27:17,728 --> 00:27:20,954
ownership about improving the system together as a whole.

428
00:27:21,152 --> 00:27:24,566
Leadership is critical in fostering a psychologically safe

429
00:27:24,598 --> 00:27:28,598
culture, and it takes incredible empathy, stress tolerance

430
00:27:28,694 --> 00:27:32,518
and critical thinking to get blamelessness and accountability

431
00:27:32,614 --> 00:27:36,206
working together in harmony. But it is possible, as we've seen it

432
00:27:36,228 --> 00:27:39,774
done in this example. So the example we shared is actually based

433
00:27:39,812 --> 00:27:43,226
on a true story and in real life, these engineer,

434
00:27:43,338 --> 00:27:46,786
of course, felt bad, but was not punished in any way as a result

435
00:27:46,888 --> 00:27:50,542
of the incident. And the team worked together to implement these systematic

436
00:27:50,606 --> 00:27:54,974
changes to make the distinction between testing environment and production environment

437
00:27:55,102 --> 00:27:59,346
more clear. A perfect example of blamelessness

438
00:27:59,378 --> 00:28:03,126
and accountability working in harmoniously together. So, as we worked on this

439
00:28:03,148 --> 00:28:06,610
talk, Christina and I found a wealth of valuable resources.

440
00:28:06,690 --> 00:28:10,358
If this subject interests you, we encourage you to check them out. We learned a

441
00:28:10,364 --> 00:28:13,794
lot about empathy, conflict resolution. We looked at the reliability

442
00:28:13,842 --> 00:28:16,950
journey of other companies and how they reached this point of maturity.

443
00:28:17,290 --> 00:28:20,670
And we learned a lot about just what it means to be blameless.

444
00:28:22,770 --> 00:28:26,826
So what do we do about the elephant in the blameless war room? We shouldn't

445
00:28:26,858 --> 00:28:30,430
hide it. Let's ride it. Yeah. Thanks for coming to our talk.

446
00:28:30,500 --> 00:28:30,860
Thank you.

