1
00:01:44,750 --> 00:01:48,830
Hello, everyone. My name is Tommy. I'm a technical writer at Semaphore,

2
00:01:48,910 --> 00:01:52,526
and today I'm going to talk to you about technical writing in the era

3
00:01:52,558 --> 00:01:56,434
of AI. If you have any questions, here's my contact information.

4
00:01:56,632 --> 00:02:00,114
With that said, let's jump to the talk. I'm going to try

5
00:02:00,152 --> 00:02:03,806
to answer the question, is technical writing a worthy career

6
00:02:03,838 --> 00:02:08,042
path to pursuit? I think the answer is yes. This is short answer,

7
00:02:08,176 --> 00:02:11,482
and the long answer is, it depends. It depends on

8
00:02:11,536 --> 00:02:15,386
what type of writing we do and how much we are willing to

9
00:02:15,408 --> 00:02:19,638
learn and adapt. The world has had many technological

10
00:02:19,734 --> 00:02:23,850
leaps over the centuries. At every stage there have been people who cried

11
00:02:23,930 --> 00:02:27,342
they would destroy jobs. And to some extent it's been true.

12
00:02:27,396 --> 00:02:31,054
We don't have the typesetter job anymore. But the bottom line

13
00:02:31,092 --> 00:02:34,270
is that new technologies have created more jobs

14
00:02:34,350 --> 00:02:37,602
and given voice to more people. My question

15
00:02:37,656 --> 00:02:41,234
is, is AI just another leap? I don't think

16
00:02:41,272 --> 00:02:44,722
so. I think ais and language models are a new

17
00:02:44,856 --> 00:02:48,342
class of things. They're a new type of things. What makes

18
00:02:48,396 --> 00:02:52,214
AI different is that they can produce text at scale and

19
00:02:52,252 --> 00:02:56,278
they can kind of think so. I will use the

20
00:02:56,364 --> 00:03:00,074
term AI and language model interchangeably, even though

21
00:03:00,112 --> 00:03:03,754
they are not technically the same. So technical writer is

22
00:03:03,792 --> 00:03:07,654
not immune. There are a ton of new products dedicated

23
00:03:07,702 --> 00:03:10,330
to automating that skill.

24
00:03:11,570 --> 00:03:15,930
These are a few examples. They can take code and output documentation.

25
00:03:16,090 --> 00:03:20,394
We even have Synthesia that can produce talks with a virtual

26
00:03:20,442 --> 00:03:24,046
avatar, so even that can be automated.

27
00:03:24,238 --> 00:03:28,146
And the future is kind of grim, at least

28
00:03:28,248 --> 00:03:32,242
in the numbers for forecast. This is Forester's 2023

29
00:03:32,296 --> 00:03:35,774
forecast. And in the top right corner, there's technical

30
00:03:35,822 --> 00:03:39,922
writer. So we are very much exposed to AI

31
00:03:39,986 --> 00:03:43,426
influence. So should we give into fear?

32
00:03:43,538 --> 00:03:46,902
I don't think so. Let's pause and think through. I believe

33
00:03:46,956 --> 00:03:50,186
there are many reasons why technical writers will be

34
00:03:50,208 --> 00:03:53,786
around for a while. I have come up with five of

35
00:03:53,808 --> 00:03:56,794
these reasons, and we'll talk about them now. So,

36
00:03:56,832 --> 00:03:59,934
reason one is the hype cycle. This is

37
00:03:59,972 --> 00:04:02,734
how things appear to be according to the news.

38
00:04:02,852 --> 00:04:08,734
There's like an explosion first, then there's a

39
00:04:08,772 --> 00:04:11,994
drop in the hype as things began

40
00:04:12,042 --> 00:04:15,962
to fail and technology does not fulfill

41
00:04:16,026 --> 00:04:19,890
the proposes initial promise. And finally, there's a plateau where

42
00:04:19,960 --> 00:04:22,830
things are more balanced and we know the limitations.

43
00:04:22,990 --> 00:04:26,230
I think in crypto we already passed the hype cycle.

44
00:04:27,130 --> 00:04:30,962
AI is just starting. At least language models

45
00:04:31,026 --> 00:04:34,870
are just starting. So we have to be mindful of this

46
00:04:34,940 --> 00:04:38,460
because this tends to make news to be very

47
00:04:39,230 --> 00:04:42,810
negative. They will print

48
00:04:43,230 --> 00:04:46,554
the most extreme cases or the

49
00:04:46,592 --> 00:04:50,686
most negative outcomes, because the

50
00:04:50,708 --> 00:04:53,934
truth is that years sells. So reason number two

51
00:04:53,972 --> 00:04:59,070
is that AI has already failed to replace know.

52
00:04:59,140 --> 00:05:03,498
One of the most high stakes businesses,

53
00:05:03,594 --> 00:05:07,506
I think, is Hollywood. And this year, there was one of

54
00:05:07,528 --> 00:05:11,182
the longest strikes of the Writers Guilds of America.

55
00:05:11,246 --> 00:05:14,850
These are the people that write screenplays and films.

56
00:05:16,170 --> 00:05:20,006
I don't doubt for a second that Hollywood tried to use AI to

57
00:05:20,028 --> 00:05:23,414
replace them, at least during the strike, because they already used

58
00:05:23,452 --> 00:05:27,314
AI for other things, like replacing their

59
00:05:27,372 --> 00:05:31,498
dead actors and things like that. But the fact that they

60
00:05:31,584 --> 00:05:34,426
settled with the guild, I think,

61
00:05:34,608 --> 00:05:38,300
says that AI is not as good as

62
00:05:38,670 --> 00:05:41,710
a writer, as a human writer.

63
00:05:42,210 --> 00:05:45,754
Going back to tech, a few days ago, Google released

64
00:05:45,802 --> 00:05:49,454
the state of DevOps 2023. To me,

65
00:05:49,572 --> 00:05:52,480
the most surprising fact was on page eight.

66
00:05:52,950 --> 00:05:56,338
It says that AI was the lowest contributor to team

67
00:05:56,424 --> 00:05:59,714
performance. So either companies are not

68
00:05:59,752 --> 00:06:03,342
using AI or they are not being as powerful

69
00:06:03,406 --> 00:06:07,314
as promised. So reason number three is that AI

70
00:06:07,362 --> 00:06:10,230
has, like any technology, its limitations.

71
00:06:11,290 --> 00:06:15,026
They are trained on Internet data, so there's

72
00:06:15,058 --> 00:06:19,482
a lot of misinformation there. There's a lot of bad

73
00:06:19,616 --> 00:06:23,606
text from the writer's perspective,

74
00:06:23,798 --> 00:06:27,146
text that's full of clutter, or very verbals or not

75
00:06:27,168 --> 00:06:29,530
to the point, and has a lot of faults.

76
00:06:31,010 --> 00:06:34,202
There's another big problem with these engines,

77
00:06:34,266 --> 00:06:38,238
that they do not offer the same level of privacy that some companies

78
00:06:38,324 --> 00:06:41,902
need. So maybe you cannot use AI at all for

79
00:06:41,956 --> 00:06:45,806
compliance or protecting sensitive data. And there's

80
00:06:45,838 --> 00:06:49,150
also a few technical limitations, like context

81
00:06:49,230 --> 00:06:53,026
size, and how companies make changes to

82
00:06:53,048 --> 00:06:55,990
the models without much transparency.

83
00:06:56,330 --> 00:06:59,638
But there's one problem that outshines all this. And in

84
00:06:59,644 --> 00:07:03,106
my opinion, the biggest problem with AI

85
00:07:03,298 --> 00:07:07,126
is the Ankani Valley. So, in a few

86
00:07:07,148 --> 00:07:10,850
words, this is how we respond to things that are human,

87
00:07:10,940 --> 00:07:14,810
like things that try to appear human but

88
00:07:14,880 --> 00:07:18,934
do not achieve it. They create a sense of aversion

89
00:07:18,982 --> 00:07:20,590
and negative emotion.

90
00:07:22,050 --> 00:07:25,470
So here we have the same robot in a bigger size.

91
00:07:25,540 --> 00:07:29,274
This is Hiroshi Ishiwuro, who is a robot inventor.

92
00:07:29,402 --> 00:07:32,654
And you can see Dankani Valley quite clearly here.

93
00:07:32,852 --> 00:07:37,086
So why I bring this? Because I think text

94
00:07:37,188 --> 00:07:41,300
produced by language model suffers from this.

95
00:07:42,230 --> 00:07:45,766
It's not as obvious as a picture or as a

96
00:07:45,788 --> 00:07:49,714
robot. Maybe if you are casually reading,

97
00:07:49,762 --> 00:07:53,400
you might not notice. But if you read enough content,

98
00:07:54,650 --> 00:07:58,434
you start to spot things. You have a feeling there's

99
00:07:58,482 --> 00:08:02,510
no one at the wheel, that there's no voice, there's no iness

100
00:08:02,610 --> 00:08:06,746
in the text. And as a reader, it's very difficult to

101
00:08:06,768 --> 00:08:10,634
me to make a connection with AI

102
00:08:10,682 --> 00:08:14,186
generated context that's unedited,

103
00:08:14,298 --> 00:08:17,786
that's wrong. And I feel this is a bigger

104
00:08:17,818 --> 00:08:21,502
problem, the biggest problem, because you need to connect with your

105
00:08:21,636 --> 00:08:25,714
readers, even in technical writing. So reason number

106
00:08:25,752 --> 00:08:29,970
four is that we can adapt, knowing the limitations of

107
00:08:30,120 --> 00:08:33,460
language models, we can work along them.

108
00:08:33,830 --> 00:08:38,182
Here's what I call the adoption spectrum. There's full

109
00:08:38,236 --> 00:08:42,358
adoptions on the left and zero adoption on the right.

110
00:08:42,524 --> 00:08:46,374
So to the left we have the writer. I call these

111
00:08:46,412 --> 00:08:50,298
writers the cyborgs, because they use

112
00:08:50,384 --> 00:08:53,690
the raw output of the language model.

113
00:08:53,760 --> 00:08:57,382
So they're kind of mixture through robots

114
00:08:57,446 --> 00:09:01,434
and humans. And these

115
00:09:01,472 --> 00:09:05,182
are people that usually need to write a lot of content in a short time.

116
00:09:05,236 --> 00:09:08,974
And I think it shows because the output will suffer for

117
00:09:09,012 --> 00:09:11,680
the problems language model have.

118
00:09:12,530 --> 00:09:16,500
So you can publish an accurate information or

119
00:09:17,350 --> 00:09:21,202
text that's too long, does not go to the point,

120
00:09:21,336 --> 00:09:24,754
takes too long to reach a conclusion, or makes no

121
00:09:24,792 --> 00:09:28,134
sense. On the other side, maybe we have a

122
00:09:28,172 --> 00:09:31,922
person that's not able to use AI, or don't want to use AI

123
00:09:32,066 --> 00:09:35,510
for some reason, maybe privacy reasons,

124
00:09:36,010 --> 00:09:39,642
or there's some compliance that prevents using

125
00:09:39,696 --> 00:09:43,610
these tools. I think the optimal place

126
00:09:43,680 --> 00:09:47,306
for us is in the middle whenever possible. So we

127
00:09:47,328 --> 00:09:50,910
have two more roles. One is the director.

128
00:09:51,410 --> 00:09:54,826
The director is a person that guides

129
00:09:55,018 --> 00:09:59,882
the AI very carefully, and it's

130
00:10:00,026 --> 00:10:03,730
constantly prompting

131
00:10:05,030 --> 00:10:08,110
the language model to generate one paragraph.

132
00:10:08,270 --> 00:10:11,554
Paragraph. Another paragraph is going very slowly. You're going

133
00:10:11,592 --> 00:10:15,170
to rewrite as needed, adjust, go back,

134
00:10:15,240 --> 00:10:19,046
go forth, and it will be like an actual director in

135
00:10:19,068 --> 00:10:22,806
a film, telling the actors how to show emotion, how to

136
00:10:22,908 --> 00:10:26,566
do their lines, and keep things the

137
00:10:26,588 --> 00:10:30,300
bigger picture making sense. The other

138
00:10:32,030 --> 00:10:36,022
category here is the swiss army writer.

139
00:10:36,086 --> 00:10:38,300
I feel I'm more in this camp,

140
00:10:38,990 --> 00:10:42,846
but we use the tool for things

141
00:10:43,028 --> 00:10:46,126
that we feel the tool is better than we. For example,

142
00:10:46,308 --> 00:10:49,230
maybe we use them for outlining,

143
00:10:49,890 --> 00:10:53,922
for moving past a writer's block, for research,

144
00:10:54,056 --> 00:10:57,902
for summarizing, for translation, for maybe editing

145
00:10:57,966 --> 00:11:01,182
or brainstorming. When we need ideas,

146
00:11:01,246 --> 00:11:05,022
or when we have some blogs, some difficulties,

147
00:11:05,086 --> 00:11:08,740
we rely on the AI to help us.

148
00:11:09,590 --> 00:11:12,822
We can think of AI like another writer that's next to us,

149
00:11:12,876 --> 00:11:16,854
and we can talk and interchange some

150
00:11:16,892 --> 00:11:20,314
ideas. But we rely a little

151
00:11:20,352 --> 00:11:24,458
less on the AI because we still write maybe 50%

152
00:11:24,544 --> 00:11:28,106
or more of the text by hand. I think we need to

153
00:11:28,128 --> 00:11:31,614
be in this middle zone because we don't want

154
00:11:31,652 --> 00:11:35,082
to be over dependent on these technologies

155
00:11:35,226 --> 00:11:39,230
and we don't want to lose our skills. The final reason,

156
00:11:39,380 --> 00:11:43,074
and with this I finish the talk, is that writing is

157
00:11:43,112 --> 00:11:45,300
only 20% of the job.

158
00:11:46,470 --> 00:11:50,370
I think there's so much more to editing than writing.

159
00:11:51,590 --> 00:11:55,146
You have to interview people, you have to talk to SMEs,

160
00:11:55,278 --> 00:11:58,582
to developers, to product managers. You have to

161
00:11:58,636 --> 00:12:02,726
be in the middle of all this storm. You have to put yourself in

162
00:12:02,748 --> 00:12:06,678
the shoes of the users, prioritize information,

163
00:12:06,844 --> 00:12:11,034
make all kind of decisions that maybe we're not even aware of.

164
00:12:11,152 --> 00:12:14,522
And these are things that the language models cannot do

165
00:12:14,576 --> 00:12:17,658
or struggle a lot to do. They're not good at this.

166
00:12:17,824 --> 00:12:21,406
So until we have a language model that can have a

167
00:12:21,428 --> 00:12:24,574
creating with a developer and ask the right questions,

168
00:12:24,692 --> 00:12:28,270
I think we're safe. So I started talk

169
00:12:28,340 --> 00:12:32,222
questioning if technical editing is a worthy pad. My answer was,

170
00:12:32,276 --> 00:12:35,698
it depends. It depends on the type of writing you do.

171
00:12:35,864 --> 00:12:39,170
Do you need the human touch? I think

172
00:12:39,240 --> 00:12:43,182
you do in all writing, even on technical writing.

173
00:12:43,326 --> 00:12:47,206
Do you need to connect with the audience? I think you always need to connect.

174
00:12:47,388 --> 00:12:51,426
So these are things that language models cannot

175
00:12:51,458 --> 00:12:54,918
do on its own. I think language models are good

176
00:12:55,004 --> 00:12:59,158
for a certain amount of things, and we should be

177
00:12:59,244 --> 00:13:02,486
good at the other things, the things that the language models cannot

178
00:13:02,518 --> 00:13:05,706
do on its own. So it's up to us to figure out

179
00:13:05,808 --> 00:13:09,674
how to fit in this new scheme. And if we

180
00:13:09,712 --> 00:13:13,014
do, I think technical editing is a very fulfilling

181
00:13:13,062 --> 00:13:16,858
and worthy career. Thank you so much for watching.

182
00:13:17,024 --> 00:13:20,874
Since we don't have a chance for a QA, I will

183
00:13:20,912 --> 00:13:24,526
leave my contact information. You can dm me or drop

184
00:13:24,558 --> 00:13:28,594
me an email. In my homepage you will

185
00:13:28,632 --> 00:13:31,906
find my personal blog and links to my work on

186
00:13:31,928 --> 00:13:35,106
semaphore and my YouTube videos. So if you want to

187
00:13:35,128 --> 00:13:38,498
talk, contact me. I will be happy to discuss

188
00:13:38,584 --> 00:13:41,954
and talk to you and hear from you. Thanks again and have

189
00:13:41,992 --> 00:13:43,646
a great rest of the conference.

