1
00:00:31,330 --> 00:00:34,978
Hello. My name is David Kjerrumgaard and I'm a developer advocate at Stream

2
00:00:34,994 --> 00:00:38,886
Native. I'm also a committer on the Apache Pulsar project and have over

3
00:00:38,908 --> 00:00:42,310
a decade of experience with event streaming and event driven architecture.

4
00:00:43,210 --> 00:00:46,998
I'm also the author of two books, including Pulsar in action by Manning Press,

5
00:00:47,084 --> 00:00:50,126
which is available for free download at the link shown here.

6
00:00:50,268 --> 00:00:53,498
If you like my talk and want to learn more about Apache Pulsar, I encourage

7
00:00:53,514 --> 00:00:54,910
you to download a copy.

8
00:00:57,170 --> 00:01:00,526
Let's start with a quick outline of the topics I'm going to cover during this

9
00:01:00,548 --> 00:01:04,138
talk. I will start with a quick introduction to Apache Pulsar,

10
00:01:04,234 --> 00:01:07,442
covering what it is and how it is different from other messaging and event

11
00:01:07,496 --> 00:01:10,020
streaming systems you may have encountered in the past.

12
00:01:10,470 --> 00:01:13,694
Next, I will explain why go is well suited for developing cloud native

13
00:01:13,742 --> 00:01:16,290
applications that interact with Apache Pulsar.

14
00:01:16,870 --> 00:01:20,466
After discussing the why of Go and Pulsar, I will demonstrate

15
00:01:20,498 --> 00:01:23,654
how to develop cloud native pulsar applications in Go.

16
00:01:23,852 --> 00:01:27,170
And finally, after we've developed and tested the application locally,

17
00:01:27,250 --> 00:01:29,340
I will walk you through the deployment process.

18
00:01:30,830 --> 00:01:34,694
In this section, we will explore Apache Pulsar to give you a better understanding

19
00:01:34,742 --> 00:01:37,290
of how it can be used inside your applications.

20
00:01:40,510 --> 00:01:43,934
Developed at Yahoo in 2012, Apache Pulsar is a cloud

21
00:01:43,972 --> 00:01:46,400
native messaging and event streaming platform.

22
00:01:47,570 --> 00:01:51,242
First of all, it was architected to take full advantage of the capabilities

23
00:01:51,306 --> 00:01:55,774
unique to cloud environments, including elastic scalability and high availability

24
00:01:55,902 --> 00:01:59,394
and redundant components. Secondly, it is the only

25
00:01:59,432 --> 00:02:03,374
platform that supports both traditional messaging semantics, like an ActivemQ

26
00:02:03,422 --> 00:02:07,154
or RabbitMQ, as well as event streaming semantics, like an Apache

27
00:02:07,202 --> 00:02:10,914
Kafka, Apache Pulsar

28
00:02:10,962 --> 00:02:15,218
provides a published and subscribed model for messaging that allows producers and consumers

29
00:02:15,314 --> 00:02:18,826
to exchange messages asynchronously. Producers and

30
00:02:18,848 --> 00:02:22,458
consumers are completely decoupled from one another and only interact with

31
00:02:22,464 --> 00:02:25,770
the Pulsar message broker, which acts as an intermediary.

32
00:02:26,110 --> 00:02:29,946
Producers and consumers exchange messages via topics that

33
00:02:29,968 --> 00:02:32,750
are used to store the messages until they are consumed.

34
00:02:34,610 --> 00:02:38,730
Apache Pulsar is the only messaging system with a two tiered architecture

35
00:02:38,810 --> 00:02:42,746
in which the component that serves the messages is separate from the component that stores

36
00:02:42,778 --> 00:02:45,930
the messages. Pulsar's serving layer consists of one or

37
00:02:45,940 --> 00:02:49,394
more brokers which are entirely stateless. This means that no data

38
00:02:49,432 --> 00:02:52,738
is stored on the brokers themselves. Instead, data is stored in

39
00:02:52,744 --> 00:02:56,370
a separate layer based on another Apache project called bookkeeper.

40
00:02:56,710 --> 00:03:00,486
This design has several advantages, including the ability for any broker to

41
00:03:00,508 --> 00:03:03,880
serve requests for any topic in the system at any time.

42
00:03:04,490 --> 00:03:07,998
This allows for automatic rebalancing of load across the broker

43
00:03:08,034 --> 00:03:11,578
layer, among many other things. The stateless nature of

44
00:03:11,584 --> 00:03:15,098
the brokers also allows you to dynamically increase or decrease the number of

45
00:03:15,104 --> 00:03:17,610
brokers based on your current workload.

46
00:03:19,070 --> 00:03:22,314
Similarly, the bookkeeper layer consists of one or more nodes known as

47
00:03:22,352 --> 00:03:26,238
bookies. Storage can easily be expanded simply by adding new nodes to

48
00:03:26,244 --> 00:03:29,614
the cluster. Tying these two layers together is

49
00:03:29,652 --> 00:03:33,442
a metadata storage layer which keeps track of where the data is stored for each

50
00:03:33,496 --> 00:03:35,330
topic inside bookkeeper.

51
00:03:38,070 --> 00:03:41,954
Pulsar's unique architecture enables it to provide several capabilities that

52
00:03:41,992 --> 00:03:45,458
distinguishes it from other messaging systems. By separating the

53
00:03:45,464 --> 00:03:48,946
storage of message from the serving of messages means you can offload the storage

54
00:03:48,978 --> 00:03:52,566
to other network accessible storage such as s three, which is

55
00:03:52,588 --> 00:03:56,182
not only cost effective, but also allows you to retain data

56
00:03:56,236 --> 00:03:59,818
for longer periods of time. Pulsar's two tiered design

57
00:03:59,904 --> 00:04:03,626
also enables elastic scalability on both tiers, so you can

58
00:04:03,648 --> 00:04:07,274
exploit kubernetes features such as auto scaling to take full

59
00:04:07,312 --> 00:04:11,034
advantage of those capabilities. Last but not

60
00:04:11,072 --> 00:04:14,946
least, it was also developed to support georeplication and multitenancy

61
00:04:15,078 --> 00:04:18,250
from the start and provide stronger data durability

62
00:04:18,330 --> 00:04:22,670
guarantees by flushing all messaging to disk before

63
00:04:22,740 --> 00:04:26,274
acknowledging them. Next, let's take a look at how we

64
00:04:26,312 --> 00:04:29,982
logically structure data within a pulsar cluster. As I mentioned earlier,

65
00:04:30,046 --> 00:04:34,082
pulsar supports georeplication. In order for pulsar clusters to

66
00:04:34,136 --> 00:04:38,114
georeplicate with one another, they must both belong to the same logical pulsar

67
00:04:38,162 --> 00:04:41,894
instance. Within each pulsar cluster, the first level

68
00:04:41,932 --> 00:04:45,458
of hierarchy are tenants as shown here in the green boxes.

69
00:04:45,634 --> 00:04:49,446
Each of these represents a different organizational unit and has an administrator

70
00:04:49,478 --> 00:04:53,402
separate unto itself that can control who can and cannot access data.

71
00:04:53,456 --> 00:04:57,494
Within these tenants, underneath the tenant exists

72
00:04:57,542 --> 00:05:01,538
namespaces as shown in the green here, these are logical groupings of topics

73
00:05:01,574 --> 00:05:04,698
that have similar policies or data storage requirements,

74
00:05:04,794 --> 00:05:09,210
data access requirements, things of that nature. It allows you to easily administrator

75
00:05:09,290 --> 00:05:12,750
a pulsar cluster by having the concept of namespaces.

76
00:05:13,250 --> 00:05:16,626
Within the namespaces themselves are multiple topics as shown here.

77
00:05:16,728 --> 00:05:20,066
Topics, as we mentioned earlier, are the lowest messaging channel between the

78
00:05:20,088 --> 00:05:23,394
producers and consumers that allow you to store messages and

79
00:05:23,432 --> 00:05:27,030
send them back and forth between producers and consumers.

80
00:05:29,450 --> 00:05:32,866
Now let's take a look at the messaging semantics within Apache

81
00:05:32,898 --> 00:05:35,494
Pulsar. As you can see on the left,

82
00:05:35,612 --> 00:05:39,058
Apache Pulsar supports multiple different protocols and

83
00:05:39,084 --> 00:05:42,346
is the first messaging system to do so. This means that

84
00:05:42,368 --> 00:05:45,878
you can have an MQTT client, a RabidMQ client,

85
00:05:45,974 --> 00:05:49,978
a pulsar client, and a Kafka client, all publishing or

86
00:05:50,064 --> 00:05:54,058
consuming from the same Apache pulsar topic. We achieve

87
00:05:54,074 --> 00:05:57,838
this through what's known as pluggable protocol handlers. This makes it a

88
00:05:57,844 --> 00:05:59,680
very flexible messaging system.

89
00:06:00,850 --> 00:06:04,798
On the right you can see the four subscription types supported

90
00:06:04,814 --> 00:06:08,658
by Apache Pulsar. These all support either

91
00:06:08,744 --> 00:06:12,434
streaming or messaging delivery semantics, which makes it very

92
00:06:12,472 --> 00:06:16,654
versatile. As you can see at the top there's key shared

93
00:06:16,702 --> 00:06:20,886
failover and exclusive. All that provide the streaming semantics that

94
00:06:20,908 --> 00:06:24,626
you're accustomed with when coming from an Apache Kafka system. This allows

95
00:06:24,658 --> 00:06:28,134
you to read data in order and process it in order as it was

96
00:06:28,172 --> 00:06:31,306
produced to the system. At the bottom you can see that

97
00:06:31,328 --> 00:06:34,742
there is the shared subscription type, which supports a more traditional

98
00:06:34,886 --> 00:06:38,858
messaging consumption pattern, such as a work queue in which

99
00:06:39,024 --> 00:06:42,574
all the work is handed out across a subset of the messages to each

100
00:06:42,612 --> 00:06:45,918
consumer to get higher throughput across that particular

101
00:06:46,004 --> 00:06:49,610
topic. Now that we've reviewed Apache Pulsar,

102
00:06:49,690 --> 00:06:53,602
let's explore why Go is a good fit for cloud native pulsar application

103
00:06:53,736 --> 00:06:57,474
development. It is worth

104
00:06:57,512 --> 00:07:00,814
noting that Go itself has some advantages when it comes to cloud native

105
00:07:00,862 --> 00:07:04,946
development, including its efficiency and scalability, and it

106
00:07:04,968 --> 00:07:08,134
is already a popular choice for cloud native development due to its support

107
00:07:08,172 --> 00:07:11,570
for restful APIs. Along with several third party libraries

108
00:07:11,650 --> 00:07:13,430
and development frameworks,

109
00:07:15,530 --> 00:07:19,126
Apache Pulsar provides a Golang client library that provides a

110
00:07:19,148 --> 00:07:22,570
simple and intuitive API for interacting with the Pulsar cluster.

111
00:07:23,070 --> 00:07:27,286
In addition, Go is one of the three supported languages for pulsar functions,

112
00:07:27,398 --> 00:07:31,238
which is a lightweight, serverless framework similar to AWS lambdas.

113
00:07:31,414 --> 00:07:34,634
Pulsar functions provide an easy way for you to develop stream processing

114
00:07:34,682 --> 00:07:38,510
applications with just a few lines of code and a configuration file.

115
00:07:38,850 --> 00:07:42,174
Even though I won't be covering pulsar functions in this talk, there's something

116
00:07:42,212 --> 00:07:45,470
you may want to explore in the future. As I mentioned previously,

117
00:07:45,550 --> 00:07:49,106
Apache Pulsar provides an officially supported Go client library that

118
00:07:49,128 --> 00:07:52,050
you can use to create producers, consumers, and readers.

119
00:07:53,190 --> 00:07:57,000
You can install the Pulsar library using the Go get command as shown here.

120
00:07:57,610 --> 00:08:01,394
API documents are also available on the Godoc page listed

121
00:08:01,442 --> 00:08:02,040
here.

122
00:08:12,190 --> 00:08:16,650
We will begin the development process by making a directory called let's Go Pulsar.

123
00:08:19,520 --> 00:08:23,948
Next, we will change into that directory and create a subdirectory

124
00:08:24,124 --> 00:08:29,874
for the consumer and

125
00:08:29,912 --> 00:08:32,210
another one for the producer.

126
00:08:35,840 --> 00:08:39,904
I will then change into the producer directory and

127
00:08:39,942 --> 00:08:44,944
initialize it as a Go module using

128
00:08:44,982 --> 00:08:49,168
the Go mod init command and giving it the name Pulsar Go

129
00:08:49,334 --> 00:08:50,400
producer.

130
00:08:58,580 --> 00:09:02,064
Next, we will go get the Pulsar client library using the Go

131
00:09:02,102 --> 00:09:04,960
get command we showed on the previous slide.

132
00:09:18,100 --> 00:09:21,984
This will download all the necessary binaries we need to use for the pulsar

133
00:09:22,032 --> 00:09:25,952
client. Next, we'll repeat the process for the consumer

134
00:09:26,016 --> 00:09:29,300
application. So we'll change into the consumer directory,

135
00:09:29,720 --> 00:09:33,528
initialize that module using Gomod init, and naming the

136
00:09:33,534 --> 00:09:35,720
module pulsar Go consumer.

137
00:09:37,180 --> 00:09:41,370
Finally, we'll use the go get command from our history to complete the process.

138
00:09:43,740 --> 00:09:46,540
Now let's take a closer look at the code that we're writing.

139
00:09:46,880 --> 00:09:50,460
First, we'll start with the producer code, which is in the producer folder and

140
00:09:50,530 --> 00:09:52,860
starts with the main function called producer.

141
00:09:54,320 --> 00:09:57,676
The first steps we do when connecting to a pulsar cluster is to get the

142
00:09:57,698 --> 00:10:00,880
configuration. Now we use these configuration in order to

143
00:10:00,950 --> 00:10:04,284
create what's called a pulsar client. This client

144
00:10:04,332 --> 00:10:08,352
in turn can be used to create a producer, as we see here. We'll walk

145
00:10:08,406 --> 00:10:12,304
through the process of getting the client config and how we've made it dynamically configurable

146
00:10:12,352 --> 00:10:15,636
using a util class which exists in the util module in

147
00:10:15,658 --> 00:10:19,584
order to get this information. So the pulsar util

148
00:10:19,712 --> 00:10:23,284
module here comes with a function called

149
00:10:23,322 --> 00:10:27,176
get clients which takes in a set of client options. These client options then

150
00:10:27,198 --> 00:10:30,456
in turn are passed to the constructor for the pulsar client. To create a

151
00:10:30,478 --> 00:10:31,610
new client object,

152
00:10:33,660 --> 00:10:36,804
we have pre configured a method to return the client

153
00:10:36,852 --> 00:10:40,524
configuration that is passed into the get clients method and

154
00:10:40,642 --> 00:10:44,024
made it configurable by the use of a constant called pulsar

155
00:10:44,072 --> 00:10:47,536
broker URL, which tells us the name the endpoint of the broker we're going to

156
00:10:47,558 --> 00:10:51,440
connect to, along with some various configuration options that we've hard coded.

157
00:10:52,020 --> 00:10:55,616
Similarly, there's a function called get producer config which we

158
00:10:55,638 --> 00:10:58,896
will use to get the producer configuration based on a

159
00:10:58,918 --> 00:11:02,656
constant called producer topic. These constants are

160
00:11:02,678 --> 00:11:06,464
defined in a class called constants go and are just string

161
00:11:06,512 --> 00:11:10,192
variables that map to property values that we're going to pass in in a properties

162
00:11:10,256 --> 00:11:14,036
file. So, for example, the pulsar broker URL is mapped to

163
00:11:14,058 --> 00:11:16,440
the property called client service URL.

164
00:11:18,460 --> 00:11:21,896
This property in turn is defined in our producer properties file, as you can

165
00:11:21,918 --> 00:11:25,784
see here. So the clients service URL points to a pulsar

166
00:11:25,912 --> 00:11:30,300
cluster running in my local Kubernetes cluster. And for the producer we have

167
00:11:30,450 --> 00:11:33,480
this topic called public default purchases,

168
00:11:33,640 --> 00:11:37,356
which again goes by tenant namespace and topic

169
00:11:37,388 --> 00:11:39,040
name. As we saw earlier.

170
00:11:41,060 --> 00:11:44,624
Once we've created a client, then we say create producer. Again getting

171
00:11:44,662 --> 00:11:48,596
this configuration values. In this case, we specify the

172
00:11:48,618 --> 00:11:51,796
topic name and that's it for

173
00:11:51,818 --> 00:11:55,524
our logic. We loop through randomly selecting both

174
00:11:55,562 --> 00:11:58,916
a data value for a key and an item that is

175
00:11:58,938 --> 00:12:02,120
passed through onto the pulsar topic.

176
00:12:02,780 --> 00:12:05,930
Then we use the send method producer send

177
00:12:06,700 --> 00:12:10,408
to send a message. Pulsar message is shown here.

178
00:12:10,574 --> 00:12:14,248
The properties include a payload which is a raw byte array of

179
00:12:14,254 --> 00:12:17,384
the data, which in this case is one of the items that has been purchased,

180
00:12:17,432 --> 00:12:20,716
a book, alarm clock, et cetera, and a key. We pass in the

181
00:12:20,738 --> 00:12:23,820
key which is a string containing one of the username.

182
00:12:24,400 --> 00:12:28,252
Since this is an infinite loop, we continuously publish

183
00:12:28,316 --> 00:12:31,964
messages and we sleep for 5 seconds in between publication

184
00:12:32,012 --> 00:12:35,932
of messages just so we can see the messages come in at a slower

185
00:12:35,996 --> 00:12:40,230
pace. Now let's turn our attention to the consumer application.

186
00:12:41,640 --> 00:12:44,948
Just like the producer application, first thing you can see is that

187
00:12:44,954 --> 00:12:48,564
we import the pulsar go client. The first step

188
00:12:48,602 --> 00:12:52,344
we take is to get the pulsar configuration and use that to create

189
00:12:52,382 --> 00:12:55,896
a pulsar client. This client in

190
00:12:55,918 --> 00:12:59,064
turn can be used to subscribe to a topic shown here.

191
00:12:59,262 --> 00:13:02,856
To start listening to messages coming in inbound messages on

192
00:13:02,878 --> 00:13:06,412
that topic, we then create a message channel

193
00:13:06,466 --> 00:13:09,884
that continuously listens on this channel, and as each

194
00:13:09,922 --> 00:13:14,216
new message comes in, we print out a descriptive

195
00:13:14,248 --> 00:13:17,424
message of the name the consumer that's received it,

196
00:13:17,542 --> 00:13:20,976
a unique message id, and display both the key and

197
00:13:20,998 --> 00:13:23,730
the raw payload for the message content itself.

198
00:13:24,420 --> 00:13:28,236
Finally, when we're done printing out that content, we must acknowledge

199
00:13:28,268 --> 00:13:31,936
the message to let the broker know that we've received and successfully processed

200
00:13:31,968 --> 00:13:35,588
the message so that it won't get redelived again in the

201
00:13:35,594 --> 00:13:39,712
event of an unsuccessful processing, we can also use what's called a negative acknowledgment

202
00:13:39,776 --> 00:13:43,130
here to force the broker to redeliver the message again.

203
00:13:46,860 --> 00:13:50,408
Consumer method also has a utility method shown here, similar to

204
00:13:50,414 --> 00:13:53,704
what we saw in the producer. So I won't spend much time on it other

205
00:13:53,742 --> 00:13:57,244
than to point out that we've added a separate function here called get

206
00:13:57,282 --> 00:14:00,264
consumer configuration, which includes two new constants,

207
00:14:00,312 --> 00:14:03,436
one for the consumer topic, the name of the topic with which we want to

208
00:14:03,458 --> 00:14:06,450
consume from, and a unique subscription name.

209
00:14:07,860 --> 00:14:11,884
These constants are defined here and map, as they did previously in the producer

210
00:14:12,012 --> 00:14:16,416
section, to message two properties in

211
00:14:16,438 --> 00:14:20,176
the configuration we're going to pass in and sort of make this properties

212
00:14:20,288 --> 00:14:23,636
dynamic. The consumer topic is

213
00:14:23,658 --> 00:14:27,296
again public default purchases. So again we're listening to the same topic that we're

214
00:14:27,328 --> 00:14:30,660
producing from, and we've created a unique subscription name.

215
00:14:30,810 --> 00:14:34,488
That way, if the consumer ever gets disconnected and reconnects using the

216
00:14:34,494 --> 00:14:38,104
same subscription name, it will pick up immediately where it left off without

217
00:14:38,142 --> 00:14:39,560
losing any messages.

218
00:14:40,940 --> 00:14:44,696
Now that we've reviewed the code, let's build and test the code locally.

219
00:14:44,808 --> 00:14:48,236
We'll start by changing into producer directory and using the Go

220
00:14:48,258 --> 00:14:51,980
build command to build the pulsar

221
00:14:52,320 --> 00:14:56,364
application in a different window.

222
00:14:56,412 --> 00:15:00,096
We'll change into the consumer directory and use Go build to

223
00:15:00,118 --> 00:15:02,770
build the consumer application.

224
00:15:10,000 --> 00:15:13,676
Once these binaries have been created, next test is

225
00:15:13,698 --> 00:15:17,696
to run them locally. So we'll run the command here using producer there

226
00:15:17,718 --> 00:15:21,408
and we can see it connects and starts generating messages. It generates a first message

227
00:15:21,574 --> 00:15:25,788
and another message. It's publishing of batteries and then gift card are produced.

228
00:15:25,964 --> 00:15:29,692
Now we'll go to the other window and start the consumer running it locally.

229
00:15:29,836 --> 00:15:32,736
We can see that it read previous messages that have been published to the topic,

230
00:15:32,768 --> 00:15:35,824
but it most recently read the book and batteries

231
00:15:35,872 --> 00:15:39,536
messages that were sent, and then as soon as the producer had sent the alarm

232
00:15:39,568 --> 00:15:42,516
clock message shown at the top, it showed up in the bottom.

233
00:15:42,618 --> 00:15:45,656
Similarly, the gift card message shows down here at the bottom as well.

234
00:15:45,678 --> 00:15:48,616
So we can see that the applications are in sync. They're sharing data across the

235
00:15:48,638 --> 00:15:55,080
same topic, which is greater

236
00:15:59,520 --> 00:16:02,984
in order to deploy a cloud native application, we must first containerize

237
00:16:03,032 --> 00:16:06,688
it. These containers not only bundle the application along with all

238
00:16:06,694 --> 00:16:10,272
its dependencies into a single deployable unit, they also provide

239
00:16:10,326 --> 00:16:14,400
an isolated environment for the application to run in inside your cloud native environment.

240
00:16:16,820 --> 00:16:20,368
The most common technology used for containerization is docker.

241
00:16:20,544 --> 00:16:23,744
However, this usually requires you to create and maintain a separate

242
00:16:23,792 --> 00:16:27,348
docker file inside your codebase solely for the purpose of creating the

243
00:16:27,354 --> 00:16:30,952
container itself. In this talk, I'm going to use a different

244
00:16:31,006 --> 00:16:34,632
technology known as buildpacks that simplify the container building

245
00:16:34,686 --> 00:16:38,276
process by eliminating the need for a Docker file entirely.

246
00:16:38,468 --> 00:16:41,924
Instead, buildpacks automatically detect the language and framework

247
00:16:41,972 --> 00:16:45,724
your application is using and will automatically containerize it for you with

248
00:16:45,762 --> 00:16:49,244
a single command. Before we start

249
00:16:49,282 --> 00:16:51,996
using the tool, I wanted to show you the website where you can get more

250
00:16:52,018 --> 00:16:56,124
information on this buildpacks tool itself. If you go to buildpacks IO,

251
00:16:56,172 --> 00:16:59,330
you'll see a lot of information, including getting starting videos,

252
00:17:01,220 --> 00:17:05,280
a detailed section on why you want to use cloud native buildpack specifically,

253
00:17:05,780 --> 00:17:09,712
and a little historical reference on the project itself, which was designed in 2011

254
00:17:09,776 --> 00:17:11,590
to solve a very critical problem.

255
00:17:13,880 --> 00:17:16,916
Now, in order to perform the next steps I'm going to do you have to

256
00:17:16,938 --> 00:17:20,276
first have the buildpack software installed, which you can access through

257
00:17:20,298 --> 00:17:22,330
the start tutorial link,

258
00:17:23,100 --> 00:17:26,616
and it starts with an assumption that you have docker installed. If you don't have

259
00:17:26,638 --> 00:17:29,672
Docker installed already, then please do so. Next,

260
00:17:29,726 --> 00:17:33,368
you can choose to install the pack library, which is the tool used to

261
00:17:33,454 --> 00:17:36,876
build packs. We'll use in the next steps. And as

262
00:17:36,898 --> 00:17:40,904
you can see, they have multiple different installation methods

263
00:17:40,952 --> 00:17:44,344
for your particular OS distribution. Whether you're using Linux,

264
00:17:44,472 --> 00:17:47,520
macOS, or windows. There's a process for you.

265
00:17:47,670 --> 00:17:51,424
Since I'm using macOS and I have homebrew installed, I chose that

266
00:17:51,462 --> 00:17:54,892
path. But again, please refer to the documentation for your operating

267
00:17:54,956 --> 00:17:57,760
system for details on how to get it installed.

268
00:18:01,000 --> 00:18:04,600
So let's switch back to our console and put these build packs to work.

269
00:18:04,750 --> 00:18:08,344
We'll start by moving up a directory and using the build pack tool

270
00:18:08,462 --> 00:18:11,210
to go ahead and build the pulsar application.

271
00:18:12,700 --> 00:18:16,524
The command basically takes in the pack. You're calling the build command first

272
00:18:16,562 --> 00:18:19,932
and giving it a name of the docker container you want it to build.

273
00:18:20,066 --> 00:18:23,832
In this case it's Go producer. You also have to specify

274
00:18:23,896 --> 00:18:27,296
the name of the builder you want to use. In this case it's the

275
00:18:27,318 --> 00:18:31,008
standard build version one and it is tagged as a docker image. As we

276
00:18:31,014 --> 00:18:34,556
shall see, it downloads this information to first build the container

277
00:18:34,588 --> 00:18:38,036
itself class. We specify the path of the application we want to

278
00:18:38,058 --> 00:18:41,172
build and it begins downloading the build pack

279
00:18:41,226 --> 00:18:44,416
application. Similarly, we'll switch

280
00:18:44,448 --> 00:18:48,196
down to the consumer shell and run the same command to

281
00:18:48,218 --> 00:18:51,864
build the client application. In this case, we're specifying the name

282
00:18:51,902 --> 00:18:55,064
of the docker container to be go consumer. Everything else

283
00:18:55,102 --> 00:18:58,536
remains the same, including the builder. As you can see

284
00:18:58,558 --> 00:19:02,408
in the top, progress is being made on downloading the buildpack's

285
00:19:02,424 --> 00:19:04,510
builder version docker image itself,

286
00:19:05,360 --> 00:19:08,430
which is the logical component that does all the building itself.

287
00:19:08,880 --> 00:19:12,776
Again, we specify the path for the consumer and it begins downloading

288
00:19:12,968 --> 00:19:15,410
the application as well and building it.

289
00:19:18,380 --> 00:19:22,036
This process will go on for a bit and depending on your download speed,

290
00:19:22,148 --> 00:19:26,004
will take some time to download all this information and get these binary

291
00:19:26,052 --> 00:19:27,930
packs down and ready to build.

292
00:19:31,230 --> 00:19:35,034
Can see now we're finally finishing the download of the application of the build

293
00:19:35,072 --> 00:19:38,954
pack images themselves and

294
00:19:38,992 --> 00:19:42,470
there we can see updated that the newer images have been downloaded.

295
00:19:42,630 --> 00:19:46,414
Once the build packs have been downloaded, you can see that it analyzes and

296
00:19:46,452 --> 00:19:49,806
detects the type of package that we've tried to build. In this case

297
00:19:49,828 --> 00:19:53,834
it sees that it's go and needs to download a go runtime,

298
00:19:53,962 --> 00:19:57,780
a Go build path, et cetera in order to build this application.

299
00:19:58,150 --> 00:20:01,698
So it goes out and fetches these using curl and

300
00:20:01,864 --> 00:20:05,614
internally runs the Go build command along with some caching

301
00:20:05,662 --> 00:20:08,806
information to start building the go binaries inside the

302
00:20:08,828 --> 00:20:12,502
cache itself. This process

303
00:20:12,556 --> 00:20:16,360
will go on for a little bit until it's successfully built the go application

304
00:20:16,890 --> 00:20:19,830
and when it is done building the go binary.

305
00:20:19,990 --> 00:20:23,574
It will start building a docker image or containerizing

306
00:20:23,622 --> 00:20:26,620
around this particular application as we've seen here.

307
00:20:27,630 --> 00:20:32,394
So let's wait a little bit for this to finish up there.

308
00:20:32,432 --> 00:20:35,946
It's finally done. You can see it's finished all the build and now it's

309
00:20:35,978 --> 00:20:39,418
starting to use some of this application and it's showing

310
00:20:39,434 --> 00:20:42,574
up here at the top. It's building the go producer image shown at the top

311
00:20:42,612 --> 00:20:46,080
and it's trying to build a go consumer image here at the bottom.

312
00:20:46,450 --> 00:20:50,078
So part of the catching layer it already had is built on the go runtime

313
00:20:50,254 --> 00:20:53,762
image, which it has. And so it's using that and also a Go

314
00:20:53,896 --> 00:20:57,614
mod go path library image as well. And then finally

315
00:20:57,672 --> 00:21:01,400
building the image. Go producer and go consumer are now built.

316
00:21:02,250 --> 00:21:05,698
Now we can use these docker images. We can test them locally by running docker

317
00:21:05,714 --> 00:21:09,766
to run them locally. Slight typo here.

318
00:21:09,868 --> 00:21:13,686
Let's retry it again with Docker and we can run this producer now locally.

319
00:21:13,718 --> 00:21:16,698
So now we're running the exact same application we built locally but inside as a

320
00:21:16,704 --> 00:21:19,690
docker container and confirm that it works.

321
00:21:19,760 --> 00:21:23,066
So again we see the output that has created a producer. We've connected to

322
00:21:23,088 --> 00:21:26,106
the same pulsar broker and are sending

323
00:21:26,138 --> 00:21:29,502
additional messages and we're picking up where we left off. By the messaging id

324
00:21:29,556 --> 00:21:32,480
you can see we're picking up where we left off.

325
00:21:32,850 --> 00:21:35,966
Let's also test the docker consumer image as

326
00:21:35,988 --> 00:21:39,582
well. And you can see we're in sync again. We're consuming the most recent messages

327
00:21:39,726 --> 00:21:43,458
we've gotten. Everything's up to date. We can see the information

328
00:21:43,544 --> 00:21:46,994
coming through and that they're in sync. So this is a great indication that our

329
00:21:47,112 --> 00:21:50,146
docker images have been built successfully using the build packs

330
00:21:50,178 --> 00:21:50,950
library.

331
00:21:54,120 --> 00:21:57,588
We'll let these run for a little bit longer just to confirm everything's up

332
00:21:57,594 --> 00:22:00,550
and running. Now let's kill them and move on.

333
00:22:01,160 --> 00:22:04,564
Once the build pack process has completed, you'll have two docker

334
00:22:04,612 --> 00:22:07,704
images on your local machine. In order to

335
00:22:07,742 --> 00:22:11,176
use these images outside of your desktop environment, you'll need to

336
00:22:11,198 --> 00:22:14,972
push them to a container repository. We will walk through that process.

337
00:22:15,106 --> 00:22:18,584
Next, let's return to our development

338
00:22:18,632 --> 00:22:22,904
shells. We'll go back to the shell where we created the Goproducer

339
00:22:23,032 --> 00:22:25,980
Docker image and now add a tag to it.

340
00:22:26,130 --> 00:22:29,836
We'll prepend it with my Docker account id shown

341
00:22:29,868 --> 00:22:32,800
here and give it the same name go producer.

342
00:22:33,220 --> 00:22:36,908
Once I've tagged it, the next step is to push it up to the Docker

343
00:22:36,924 --> 00:22:40,950
hub repository, which is why I've prepended my account name on there.

344
00:22:42,600 --> 00:22:46,128
Once that is done, we will repeat the process for the go consumer Docker image

345
00:22:46,144 --> 00:22:49,536
that we've created. Down below we can see that the producer

346
00:22:49,568 --> 00:22:53,176
image was successfully published to Docker Hub and

347
00:22:53,198 --> 00:22:56,280
so we're going to tag the consumer with a similar fashion,

348
00:22:56,940 --> 00:23:01,000
adding my Docker hub account id as a prefix,

349
00:23:01,900 --> 00:23:05,772
giving it the image name go consumer, and then pushing it up

350
00:23:05,826 --> 00:23:10,808
to the Docker hub repository where we'll be able to access this image

351
00:23:10,904 --> 00:23:13,500
from our Kubernetes environment.

352
00:23:24,900 --> 00:23:28,036
In order to deploy our cloud native application we will create a

353
00:23:28,058 --> 00:23:31,988
deployment manifest that specifies the container images to use the

354
00:23:31,994 --> 00:23:35,140
resources our application will require, et cetera.

355
00:23:36,280 --> 00:23:39,584
Now, since our application is configurable by properties,

356
00:23:39,712 --> 00:23:43,096
we will also use config maps and persistent volume class as

357
00:23:43,118 --> 00:23:44,840
part of the application deployment.

358
00:23:46,140 --> 00:23:49,336
Let's take a look at this deployment manifest in detail.

359
00:23:49,518 --> 00:23:53,148
You can see that it is located in a separate folder called deployment inside the

360
00:23:53,154 --> 00:23:56,540
project itself and the file is called KH deployment.

361
00:23:57,680 --> 00:24:01,292
First we can see that it specifies an application type

362
00:24:01,346 --> 00:24:05,060
as the template and uses the pulsar let's

363
00:24:05,080 --> 00:24:09,088
go metadata tag so

364
00:24:09,094 --> 00:24:11,330
we are able to identify this resource quickly.

365
00:24:13,460 --> 00:24:16,816
We'll notice that we're going to deploy the producer and consumer together. So there will

366
00:24:16,838 --> 00:24:19,600
be a total of two containers inside the single pod.

367
00:24:20,660 --> 00:24:23,664
You can see that we'll be using the images that we just tagged and pushed

368
00:24:23,712 --> 00:24:27,376
previously and we'll also be using a separate mount

369
00:24:27,408 --> 00:24:31,344
path to get the properties that are dynamically configurable. As you recall

370
00:24:31,392 --> 00:24:34,936
when we looked at the code, all of these properties match to labels and this

371
00:24:34,958 --> 00:24:38,436
allows us to dynamically change things like the broker URL

372
00:24:38,468 --> 00:24:42,500
for the pulsar cluster, the name of the topic we publish to et cetera.

373
00:24:42,660 --> 00:24:45,932
We will access this information through a config mat on a mount point.

374
00:24:45,986 --> 00:24:49,256
Here we specify some resources constraints.

375
00:24:49,288 --> 00:24:52,636
Since our application is very light, we won't need very much and we

376
00:24:52,658 --> 00:24:55,230
always want to pull the images which will not be available.

377
00:24:56,880 --> 00:24:59,856
We do a similar configuration for our consumer application.

378
00:24:59,958 --> 00:25:03,776
Again, we're going to use the image that we tagged and pushed previously and

379
00:25:03,798 --> 00:25:06,880
we're going to have a mount path for resources consumers properties.

380
00:25:07,220 --> 00:25:11,136
As you may recall, when we're going through the code for the consumer,

381
00:25:11,168 --> 00:25:14,708
for example, I showed the resource manager code. We'll go through this again.

382
00:25:14,874 --> 00:25:18,240
Let's show here. It's going to assume there's consumer properties

383
00:25:18,320 --> 00:25:21,864
in the utils. The resource manager is reading this property right,

384
00:25:21,902 --> 00:25:25,656
resources consumers properties, and we're making sure to map to

385
00:25:25,678 --> 00:25:29,464
that. This way it will automatically pick up these values and use whatever

386
00:25:29,502 --> 00:25:32,696
we want to change. Last but not

387
00:25:32,718 --> 00:25:37,032
least, we specify two different volume mounts, one for the consumer configuration,

388
00:25:37,096 --> 00:25:40,188
which will have a map to the consumer config map which we'll create in a

389
00:25:40,194 --> 00:25:44,104
minute, and a producer configuration will have a producer configuration

390
00:25:44,232 --> 00:25:48,156
with a key being the file name itself of consumer properties for the consumer config

391
00:25:48,188 --> 00:25:51,932
map and producer properties for the producer properties

392
00:25:51,996 --> 00:25:56,112
config map as I mentioned,

393
00:25:56,166 --> 00:25:59,604
our deployment will use Kubernetes objects known as config maps to store

394
00:25:59,642 --> 00:26:03,744
the application properties. This allows us to dynamically change the configuration

395
00:26:03,792 --> 00:26:06,390
of our application without modifying the code.

396
00:26:07,160 --> 00:26:11,204
We will mount the config maps as properties files in a known location so

397
00:26:11,242 --> 00:26:14,548
our application will be able to read them in the format it expects.

398
00:26:14,724 --> 00:26:18,120
Now let's walk through the process of creating the config maps from a property

399
00:26:18,190 --> 00:26:22,404
file. So let's return to our shell environment

400
00:26:22,532 --> 00:26:26,100
and use Kubecontrol to create the config maps. Be sure

401
00:26:26,110 --> 00:26:29,580
that your kubeconfig is pointing to the proper Kubernetes environment.

402
00:26:30,320 --> 00:26:33,628
We use the command create config map and give it the name of

403
00:26:33,634 --> 00:26:37,884
the config map we want to create and specify the file

404
00:26:38,012 --> 00:26:41,964
where all the properties exist for our producer. We'll get an indication

405
00:26:42,012 --> 00:26:44,560
that the config map was properly created.

406
00:26:45,220 --> 00:26:48,484
Let's switch to the consumer environment and run the same

407
00:26:48,522 --> 00:26:52,384
command. This time we'll create the config map name consumer config

408
00:26:52,432 --> 00:26:56,596
map and point it to the consumer directory where

409
00:26:56,618 --> 00:26:58,740
all the properties files exist.

410
00:27:03,970 --> 00:27:07,994
Once they have both been successfully created, we can then use cube

411
00:27:08,042 --> 00:27:11,486
control to look at the config map to

412
00:27:11,508 --> 00:27:14,750
guarantee that these configuration maps have been existed

413
00:27:15,410 --> 00:27:17,860
have been created in the environment that we want.

414
00:27:21,370 --> 00:27:24,626
So there we can see that they're listed, the consumer config map and the producer

415
00:27:24,658 --> 00:27:28,514
config map, which matches our description in our deployment manifest

416
00:27:28,562 --> 00:27:32,074
file. Let's take a look at one of these config maps to get a better

417
00:27:32,112 --> 00:27:34,620
understanding of what their contents are.

418
00:27:36,110 --> 00:27:39,862
So here we can see that the consumer properties

419
00:27:39,926 --> 00:27:43,706
is mapped as we expected, along with the pulsar client

420
00:27:43,738 --> 00:27:47,854
URL. We can see that the consumer properties came

421
00:27:47,892 --> 00:27:51,614
over, including the consumer topic and subscription name

422
00:27:51,812 --> 00:27:55,074
along with the client service URL to connect

423
00:27:55,112 --> 00:27:59,042
to the pulsar cluster. All this information will be accessible from the key

424
00:27:59,096 --> 00:28:02,894
called consumer properties at runtime when our consumer

425
00:28:02,942 --> 00:28:07,054
connects. Now that we've created the config maps,

426
00:28:07,102 --> 00:28:10,258
the final step is to deploy the application itself.

427
00:28:10,424 --> 00:28:14,518
We will do this by using the Kubecontrol apply command and specifying the

428
00:28:14,524 --> 00:28:17,766
deployment manifest we looked at earlier. You can see that

429
00:28:17,788 --> 00:28:21,122
we've got an indication that the deployment was successfully created.

430
00:28:21,266 --> 00:28:24,906
Let's start looking at what has happened underneath the covers. First, we'll list the

431
00:28:24,928 --> 00:28:28,026
pods to see that they're just being created. There will be two. One for the

432
00:28:28,048 --> 00:28:31,254
producer and one for the consumer. Well, we can verify

433
00:28:31,302 --> 00:28:34,530
that the deployment is listed by doing a cube control git.

434
00:28:34,630 --> 00:28:37,994
Deployments. Next, let's describe the deployment

435
00:28:38,042 --> 00:28:41,850
in general by specifying the Kubecontrol

436
00:28:41,930 --> 00:28:45,694
describe command, along with specifying the full name of the deployment

437
00:28:45,742 --> 00:28:49,490
itself. In this case, it's pulsar. Let's go deployment.

438
00:28:50,470 --> 00:28:53,460
This returns a lot of information that we can look at,

439
00:28:54,390 --> 00:28:58,914
including details on the number of replicas,

440
00:28:59,042 --> 00:29:00,710
the labels we specified,

441
00:29:01,850 --> 00:29:05,570
the images that are going to be used for the producer and the consumer,

442
00:29:05,650 --> 00:29:07,990
the resource limits that we've requested,

443
00:29:09,210 --> 00:29:12,466
the mount points, et cetera. The image for the consumer,

444
00:29:12,578 --> 00:29:15,526
all the config maps as expected. The consumer config map,

445
00:29:15,558 --> 00:29:18,170
the producer config map, et cetera.

446
00:29:18,510 --> 00:29:21,686
Let's go back and look at the pods again. We can see that now that

447
00:29:21,728 --> 00:29:25,566
they're both up and running. So now let's explore what's going on

448
00:29:25,588 --> 00:29:29,386
inside these pods. Let's first describe what's

449
00:29:29,418 --> 00:29:34,554
going on inside the pod

450
00:29:34,602 --> 00:29:38,146
themselves. And it should list the two different containers. So let's go ahead

451
00:29:38,168 --> 00:29:42,114
and grab this information, copy and paste this pod name

452
00:29:42,152 --> 00:29:46,306
which is dynamically assigned. Let's paste it in there and

453
00:29:46,328 --> 00:29:49,666
we can see if you ever have issues deploying the application.

454
00:29:49,768 --> 00:29:52,786
As you know, before you can look at this event logs there to indicate what's

455
00:29:52,818 --> 00:29:56,354
going on. We can see successfully deployed. The config maps

456
00:29:56,402 --> 00:30:00,226
are mounted, the images are being used. The containers

457
00:30:00,258 --> 00:30:04,330
were started. So both the producer and the consumer have been started and created.

458
00:30:04,830 --> 00:30:08,234
So that's great. Now let's look at some logs in there. Let's verify again

459
00:30:08,272 --> 00:30:11,446
through our command line that information is being displayed.

460
00:30:11,478 --> 00:30:15,598
So we'll also get the logs, but we'll specify first the

461
00:30:15,764 --> 00:30:19,386
container for the let's Go pulsar producer to confirm

462
00:30:19,418 --> 00:30:22,798
that messages are being generated. And we can see here we

463
00:30:22,804 --> 00:30:26,020
can watch again the data is picking up where it left off,

464
00:30:26,470 --> 00:30:30,386
producing some additional information. Every 5 seconds a new message is

465
00:30:30,408 --> 00:30:34,494
being published. Now let's change gears

466
00:30:34,542 --> 00:30:37,886
and look at the consumer pod or the container

467
00:30:37,918 --> 00:30:41,206
within the consumer. It's the consumer container itself. And we can see

468
00:30:41,228 --> 00:30:45,350
that it's receiving messages as well. This is a good indication that the application

469
00:30:45,420 --> 00:30:49,110
has been deployed and picked up all the configuration properties as we expected.

470
00:30:51,070 --> 00:30:54,140
So let's summarize a few points that we've covered during this talk.

471
00:30:56,030 --> 00:30:59,174
First, Apache Pulsar is a cloud native messaging and event streaming

472
00:30:59,222 --> 00:31:02,118
platform that's designed for cloud native environments,

473
00:31:02,294 --> 00:31:06,410
and go is a good fit for developing cloud native applications that use Pulsar.

474
00:31:06,570 --> 00:31:09,150
Due to Apache Pulsar's go client library.

475
00:31:09,810 --> 00:31:13,658
I also showed you build packs and are a great tool for containerizing

476
00:31:13,674 --> 00:31:17,290
your go applications without the need to maintain a separate docker file,

477
00:31:17,450 --> 00:31:20,734
and walked you through the process of packaging and deploying a cloud native go

478
00:31:20,772 --> 00:31:23,806
application that interacts with Apache Pulsar. If you want

479
00:31:23,828 --> 00:31:27,314
to learn more, all the code available for this demonstration is available at the GitHub

480
00:31:27,362 --> 00:31:28,260
repo shown here.

