1
00:00:22,730 --> 00:00:26,118
My name is Pablo Skipper. I'm a chief of officer in

2
00:00:26,124 --> 00:00:29,702
the AI investments and today with

3
00:00:29,756 --> 00:00:31,270
Anna Warno,

4
00:00:32,650 --> 00:00:36,578
data scientist from Southern Booth, we will make a preview

5
00:00:36,674 --> 00:00:39,846
of the most advanced and most

6
00:00:40,028 --> 00:00:43,606
efficient, at least in our opinion, machine learning

7
00:00:43,708 --> 00:00:47,106
time series forecasting methods.

8
00:00:47,298 --> 00:00:50,894
I will make a short introduction with slides and

9
00:00:50,932 --> 00:00:54,750
then I will make a real hands on

10
00:00:54,820 --> 00:00:58,058
session how to use some of these methods

11
00:00:58,234 --> 00:01:01,726
on the real data sets. The brief again is

12
00:01:01,748 --> 00:01:05,710
as follow. I will tell a little bit more about the time series,

13
00:01:05,790 --> 00:01:09,954
what this is, and about the statistical methods which

14
00:01:10,152 --> 00:01:13,858
were very extensively used till the two years

15
00:01:13,944 --> 00:01:17,666
ago about the M four and five competition.

16
00:01:17,858 --> 00:01:21,730
Then I briefly go through the most advanced

17
00:01:21,890 --> 00:01:25,554
and effective machine learning time time

18
00:01:25,602 --> 00:01:29,382
time time time time series forecasting methods. The real

19
00:01:29,436 --> 00:01:34,010
time real hands on session how to forecast

20
00:01:35,230 --> 00:01:39,062
different time series using these presented

21
00:01:39,126 --> 00:01:42,830
methods. So for the long time, very long

22
00:01:42,900 --> 00:01:46,330
time, the domination of the statistical

23
00:01:46,410 --> 00:01:51,070
method was obvious. The machine learning methods

24
00:01:52,310 --> 00:01:56,450
achieve much lower results and

25
00:01:56,520 --> 00:01:59,982
the most popular was to use this listed

26
00:02:00,046 --> 00:02:04,066
statistical methods. I do not go into the details, but I

27
00:02:04,088 --> 00:02:08,130
only want to highlight the fact that the most typical

28
00:02:08,210 --> 00:02:11,682
and most popular way of using the statistical method

29
00:02:11,746 --> 00:02:15,286
was CTO ensembling different methods which is

30
00:02:15,308 --> 00:02:19,222
also used very extensively by machine learning methods.

31
00:02:19,366 --> 00:02:24,326
The breakthrough was in the 2018 when the results

32
00:02:24,358 --> 00:02:27,446
of the M four competition was announced.

33
00:02:27,558 --> 00:02:32,090
M competition is probably the most prestigious scientifically

34
00:02:32,170 --> 00:02:35,294
baked packet competition time series

35
00:02:35,332 --> 00:02:38,858
forecasting methods. It is organized by the professor Sparrows

36
00:02:38,874 --> 00:02:43,106
McLiakis from the University of Nicosia and

37
00:02:43,208 --> 00:02:46,878
the fourth edition of this competition

38
00:02:46,974 --> 00:02:50,738
called M four. The first and the second place

39
00:02:50,824 --> 00:02:54,302
was won by the so called hybrid method. So method

40
00:02:54,366 --> 00:02:58,566
which uses statistical approaches and machine learning

41
00:02:58,668 --> 00:03:02,520
and machine learning part was same. Much more important

42
00:03:03,050 --> 00:03:06,966
for this methods comparing to the statistical one

43
00:03:07,068 --> 00:03:11,014
and M five which results

44
00:03:11,062 --> 00:03:14,714
was presented last year was dominated by

45
00:03:14,752 --> 00:03:18,474
the only machine learning methods as in the

46
00:03:18,512 --> 00:03:22,490
M four competition, the goal was to predict

47
00:03:22,570 --> 00:03:27,134
over 1000 100,000

48
00:03:27,332 --> 00:03:30,686
time series, so the very very big amount of

49
00:03:30,708 --> 00:03:34,322
the time series. We consider these results as a very

50
00:03:34,376 --> 00:03:37,426
comprehensive and reliable and the

51
00:03:37,448 --> 00:03:41,358
first method is the ES hybrid method by Swamik

52
00:03:41,374 --> 00:03:44,900
Smell. It is the winning method from M four.

53
00:03:45,450 --> 00:03:48,882
It uses statistical approach, exponential smoothing,

54
00:03:48,946 --> 00:03:52,498
hot winters for the data processing

55
00:03:52,674 --> 00:03:57,320
and also use very novel way of learning

56
00:03:57,630 --> 00:04:01,382
the neural network with the special architecture.

57
00:04:01,446 --> 00:04:05,946
I will tell a little bit more on the next slide together

58
00:04:06,048 --> 00:04:10,880
with the parameters of the exponential smoothing and

59
00:04:11,250 --> 00:04:14,462
this method uses model and sampling very

60
00:04:14,516 --> 00:04:18,506
extensively in a very unique and novel

61
00:04:18,698 --> 00:04:22,554
way. Another novelty of this method was to use

62
00:04:22,612 --> 00:04:26,238
LSTM network, but not a typical LSTM

63
00:04:26,414 --> 00:04:29,790
network but with the delays and residual

64
00:04:29,870 --> 00:04:33,198
connection both these concepts are very popular

65
00:04:33,294 --> 00:04:37,314
in the image recognition for the convolutional

66
00:04:37,362 --> 00:04:41,254
neural network and this application of

67
00:04:41,292 --> 00:04:44,870
that concept was the first

68
00:04:45,020 --> 00:04:48,502
one, at least on the peak scale for

69
00:04:48,556 --> 00:04:52,922
the LSTM and tiny time series forecasting

70
00:04:52,976 --> 00:04:56,326
methods results was obviously great. So Swallow

71
00:04:56,358 --> 00:05:00,750
Queen won the M four competition and

72
00:05:00,900 --> 00:05:04,522
yeah, this architecture was very heavily studied

73
00:05:04,666 --> 00:05:08,318
by other scientists and person

74
00:05:08,404 --> 00:05:11,470
working on the time series forecasting.

75
00:05:11,890 --> 00:05:15,454
One more very important thing about the eshybrid

76
00:05:15,502 --> 00:05:19,154
method is that it uses ten sampling in

77
00:05:19,192 --> 00:05:23,474
a very advanced way. So the

78
00:05:23,512 --> 00:05:27,670
models, the best models for the given time series,

79
00:05:28,970 --> 00:05:32,694
please remember that it was used for the 100,000 time

80
00:05:32,732 --> 00:05:36,774
series was collected and for

81
00:05:36,812 --> 00:05:40,442
the final predictions that these models were

82
00:05:40,496 --> 00:05:44,154
used to achieve the most robust and

83
00:05:44,192 --> 00:05:47,306
accurate results. The second method which

84
00:05:47,328 --> 00:05:51,282
is purely machine learning method is nbeats.

85
00:05:51,366 --> 00:05:55,962
It was published after the M four competition and claims

86
00:05:56,026 --> 00:06:00,110
to get the better results on the M four data

87
00:06:00,180 --> 00:06:03,934
sets comparing CTO ES hybrid this is

88
00:06:03,972 --> 00:06:07,766
purely machine learning methods. It has a unique stack

89
00:06:07,818 --> 00:06:11,778
block based architecture, has different type of

90
00:06:11,784 --> 00:06:15,490
the block transomal generic and also

91
00:06:15,560 --> 00:06:19,038
has some explainability and transfer learning

92
00:06:19,144 --> 00:06:23,122
features and as well uses advanced

93
00:06:23,186 --> 00:06:26,710
model enemy on a very big scale

94
00:06:27,390 --> 00:06:31,414
it ensembling over 100 models

95
00:06:31,542 --> 00:06:33,850
to make a predictions.

96
00:06:34,750 --> 00:06:38,598
This is the architecture of tap nbeats method

97
00:06:38,694 --> 00:06:42,010
and as you can see there are many stack.

98
00:06:42,170 --> 00:06:45,546
Each stack is built of the block

99
00:06:45,658 --> 00:06:52,030
and also these residual connections within

100
00:06:52,100 --> 00:06:56,354
the stack. So the input from

101
00:06:56,392 --> 00:07:00,066
the one layer is passed to the

102
00:07:00,088 --> 00:07:03,746
given layer and also skip this layer to go to

103
00:07:03,768 --> 00:07:07,042
the next layer. Very unique concept

104
00:07:07,106 --> 00:07:11,286
is also to pull

105
00:07:11,388 --> 00:07:14,710
the results of each block and

106
00:07:14,780 --> 00:07:18,298
combine them together and use

107
00:07:18,384 --> 00:07:22,074
them as the output from the stack and

108
00:07:22,192 --> 00:07:25,782
each stack is adding their output

109
00:07:25,846 --> 00:07:30,170
to the global forecasting which is ensampled

110
00:07:31,090 --> 00:07:34,030
inside the given model.

111
00:07:34,180 --> 00:07:37,994
So the nbeats have clearly on the end sampling

112
00:07:38,042 --> 00:07:41,390
in a very good way and

113
00:07:41,460 --> 00:07:44,578
is the fully pure machine learning

114
00:07:44,744 --> 00:07:48,274
method. The next methods which I wanted to

115
00:07:48,312 --> 00:07:51,694
mention is the complete framework called glue

116
00:07:51,742 --> 00:07:55,546
on TS. It is the complete framework for the time series

117
00:07:55,598 --> 00:07:59,650
forecasting. It includes various

118
00:07:59,730 --> 00:08:03,554
models, also the most advanced neural networks

119
00:08:03,602 --> 00:08:07,694
architecture like transformer and different method

120
00:08:07,762 --> 00:08:10,010
of data transformation.

121
00:08:10,830 --> 00:08:14,374
Also it allows for the probabilistic time series modeling

122
00:08:14,422 --> 00:08:18,426
to determine the distribution. It supports for

123
00:08:18,448 --> 00:08:22,622
the cloud computing training and inference and also

124
00:08:22,676 --> 00:08:25,920
has a very strong community support.

125
00:08:26,290 --> 00:08:29,534
And as I said this framework is ready to

126
00:08:29,572 --> 00:08:33,102
use. You can download that library

127
00:08:33,166 --> 00:08:36,466
and start using. It's not easy to use

128
00:08:36,568 --> 00:08:40,622
but it is easier to use than previous

129
00:08:40,686 --> 00:08:44,830
two methods which are available on the SS source code and

130
00:08:45,000 --> 00:08:48,518
you need to download and

131
00:08:48,604 --> 00:08:53,160
compile it by yourself and start using.

132
00:08:53,850 --> 00:08:57,790
Here you can see I'm not sure if it's the latest

133
00:08:57,890 --> 00:09:01,786
diagram, but it shows that how many components is already

134
00:09:01,888 --> 00:09:05,670
included in the gluon TS

135
00:09:05,750 --> 00:09:09,514
and the framework is still developed and used for

136
00:09:09,552 --> 00:09:13,326
time. Time, time series forecasting

137
00:09:13,348 --> 00:09:16,798
methods method, which I wanted to mention is

138
00:09:16,884 --> 00:09:20,686
settling machine. It is based on the stohastic learning

139
00:09:20,788 --> 00:09:26,610
automata invented by the russian

140
00:09:26,950 --> 00:09:31,282
scientist. Settling in the

141
00:09:31,336 --> 00:09:35,302
previous centuries are quite old, but for the long time

142
00:09:35,356 --> 00:09:38,646
this algorithm was used only for

143
00:09:38,668 --> 00:09:43,462
the scientific purposes, but now it's used for both

144
00:09:43,516 --> 00:09:47,310
machine learning and also for the reinforcement

145
00:09:47,490 --> 00:09:52,060
learning. So for supervised learning and reinforcement learning.

146
00:09:52,750 --> 00:09:55,626
And from my point of view,

147
00:09:55,808 --> 00:09:59,078
the biggest innovation of this approach

148
00:09:59,174 --> 00:10:03,360
is that it allows to create or to learn the

149
00:10:04,210 --> 00:10:07,530
stochastic distribution for each of the parameters.

150
00:10:07,610 --> 00:10:11,182
So this algorithm is learning the

151
00:10:11,236 --> 00:10:14,974
probabilistic distributions, which is learned

152
00:10:15,022 --> 00:10:18,740
in the supervised way

153
00:10:19,430 --> 00:10:22,786
and also is constantly updated after

154
00:10:22,888 --> 00:10:27,362
each predictions. So that's the reason that this

155
00:10:27,496 --> 00:10:31,302
approach is considered self learning and

156
00:10:31,436 --> 00:10:35,302
can be used for the reinforcement learning and also

157
00:10:35,356 --> 00:10:39,442
for predictions. And the advantage

158
00:10:39,506 --> 00:10:43,370
is that we do not need to retrain a model

159
00:10:43,440 --> 00:10:47,654
after the each prediction, but this model is somehow

160
00:10:47,702 --> 00:10:52,206
retrained after each prediction. So the weights of

161
00:10:52,228 --> 00:10:56,000
the probabilistic distribution are changed after

162
00:10:57,170 --> 00:11:00,302
each prediction. And yeah,

163
00:11:00,356 --> 00:11:04,270
very briefly, it works that way that we have the

164
00:11:04,340 --> 00:11:07,922
input, and for the, each parameter of the

165
00:11:07,976 --> 00:11:11,982
input, the separate stochastic

166
00:11:12,046 --> 00:11:15,830
distribution is created. And based on the rules

167
00:11:16,330 --> 00:11:19,762
in this settling machine, the probabilistic

168
00:11:19,826 --> 00:11:24,150
distribution is updated and for the prediction,

169
00:11:24,730 --> 00:11:28,298
the final prediction, for the final

170
00:11:28,384 --> 00:11:31,690
value for the output of each

171
00:11:31,760 --> 00:11:35,050
parameter is sampled for the

172
00:11:35,200 --> 00:11:38,986
currently learned distribution and finally

173
00:11:39,088 --> 00:11:42,346
it is ensampled in the

174
00:11:42,368 --> 00:11:46,414
given way. Disciplined machine is a

175
00:11:46,532 --> 00:11:50,730
very different approach. Comparing CTO, the, let's say traditional machine

176
00:11:50,810 --> 00:11:54,514
learning, which is based on

177
00:11:54,552 --> 00:11:58,414
the neural networks usually, and stochastic

178
00:11:58,462 --> 00:12:01,380
gradient destined and pet propagation process,

179
00:12:02,550 --> 00:12:06,630
because it's lent in a different way and

180
00:12:06,700 --> 00:12:10,818
it could be used as the one additional method,

181
00:12:10,914 --> 00:12:14,374
for example, to be included in

182
00:12:14,412 --> 00:12:18,050
the ensembling Anya enhanced session

183
00:12:18,130 --> 00:12:21,366
will present traditional machine learning

184
00:12:21,468 --> 00:12:25,734
method. So years, hybrid and bits

185
00:12:25,782 --> 00:12:29,526
and one more methods called temporal

186
00:12:29,558 --> 00:12:33,742
fusion transformer, which is considered as one

187
00:12:33,796 --> 00:12:37,982
of the most advanced currently available

188
00:12:38,116 --> 00:12:41,434
machine learning methods.

189
00:12:41,562 --> 00:12:44,278
And also Anya will introduce these methods.

190
00:12:44,314 --> 00:12:47,774
So I will skip this temporal fusion

191
00:12:47,822 --> 00:12:52,562
transformer for now. So that's all

192
00:12:52,616 --> 00:12:56,006
about reviewing the methods. Very short

193
00:12:56,188 --> 00:12:59,670
summary the highlights of the forecasting.

194
00:13:00,250 --> 00:13:04,694
From my point of view, the most important thing is that currently

195
00:13:04,892 --> 00:13:08,538
the time, time, time, time series forecasting methods

196
00:13:08,624 --> 00:13:11,914
are very dynamically developed and the

197
00:13:11,952 --> 00:13:15,434
new methods appears. They are

198
00:13:15,552 --> 00:13:19,162
not, say typical convolutional LSTM or

199
00:13:19,216 --> 00:13:22,862
transformer methods, but much more advanced and

200
00:13:22,916 --> 00:13:26,986
the efficiency is much higher in terms of the predictions.

201
00:13:27,098 --> 00:13:30,954
Accuracy is much higher than statistical methods.

202
00:13:31,082 --> 00:13:34,674
Of course, forecasting methods has many, many area of

203
00:13:34,712 --> 00:13:38,226
application in the AI investments. We are

204
00:13:38,248 --> 00:13:41,554
using them for the financial time series and

205
00:13:41,592 --> 00:13:44,946
we achieve over 60% accuracy on

206
00:13:44,968 --> 00:13:49,110
the long 110 years test

207
00:13:49,180 --> 00:13:53,030
periods. But of course, the application of time time time

208
00:13:53,370 --> 00:13:56,726
series forecasting methods used for the many different

209
00:13:56,828 --> 00:13:59,622
areas like business,

210
00:13:59,756 --> 00:14:04,182
sales, lead, retail, and also for social proposals

211
00:14:04,246 --> 00:14:08,678
like health, environment and mobility,

212
00:14:08,774 --> 00:14:12,126
and many more. So having the

213
00:14:12,228 --> 00:14:16,106
more accurate method gives a significant

214
00:14:16,218 --> 00:14:19,594
edge in many areas.

215
00:14:19,722 --> 00:14:24,462
So that's the reason that we are presenting it

216
00:14:24,596 --> 00:14:28,014
here. Okay, now it's time for the

217
00:14:28,052 --> 00:14:31,394
handsome session by Anya. As I said, Anya will

218
00:14:31,512 --> 00:14:35,720
show the nBeats method, TFD method and also

219
00:14:36,650 --> 00:14:41,810
some basic introduction, how to properly forecast

220
00:14:41,890 --> 00:14:45,910
time series using machine learning approaches.

221
00:14:46,410 --> 00:14:50,390
I hope you find our sessions valuable

222
00:14:50,550 --> 00:14:54,522
and can learn something interesting from

223
00:14:54,576 --> 00:14:57,690
that. So that's all from my side and

224
00:14:57,760 --> 00:15:01,238
now it's time for Anna Warno handsome session

225
00:15:01,334 --> 00:15:05,310
hello. After the theoretical introduction I would like to show something

226
00:15:05,380 --> 00:15:09,274
practical. I would like to present how we struggle with time series

227
00:15:09,322 --> 00:15:13,506
data on a daily basis. I will talk shortly about the

228
00:15:13,528 --> 00:15:17,998
data processing models, choice models, evaluation, boosting accuracy

229
00:15:18,094 --> 00:15:22,098
and explainable AI which can be used with time series data.

230
00:15:22,264 --> 00:15:26,134
And to have some examples, I chose a publicly available data

231
00:15:26,172 --> 00:15:29,890
set. The selection criteria were multidimensionality,

232
00:15:29,970 --> 00:15:33,602
difficulty and data size and I will briefly

233
00:15:33,666 --> 00:15:36,918
show what can be done with such data.

234
00:15:37,084 --> 00:15:40,694
So, as I mentioned before, these data are open sourced.

235
00:15:40,822 --> 00:15:44,310
There is around 40,000 of rows

236
00:15:44,390 --> 00:15:47,610
each for one timestamp. Frequency of the data

237
00:15:47,680 --> 00:15:51,226
is 1 hour and we have around 15 columns,

238
00:15:51,338 --> 00:15:54,202
six main air pollutants,

239
00:15:54,346 --> 00:15:57,994
six connected with weather and the rest express

240
00:15:58,042 --> 00:16:00,746
the date. Here we have some examples.

241
00:16:00,778 --> 00:16:03,954
Columns plotted. As we can see,

242
00:16:04,152 --> 00:16:07,726
data look messy. We have large amplitudes.

243
00:16:07,918 --> 00:16:11,374
After zooming data plots, it looks slightly

244
00:16:11,422 --> 00:16:15,302
better. However, there is no visible pattern at the first

245
00:16:15,356 --> 00:16:19,510
site. Only after aggregation example

246
00:16:19,660 --> 00:16:23,986
over a week you can see some regularities and normally

247
00:16:24,098 --> 00:16:28,094
now we would do some explanatory data analysis,

248
00:16:28,162 --> 00:16:32,074
et cetera. However, we don't have as much time so

249
00:16:32,112 --> 00:16:35,990
we will focus only on parts which are absolutely necessary,

250
00:16:36,070 --> 00:16:39,306
which are crucial for modeling. So one

251
00:16:39,328 --> 00:16:43,294
of the first things which needs to be done is handling the missing data.

252
00:16:43,492 --> 00:16:46,634
Firstly, we need to understand the source of missingness.

253
00:16:46,762 --> 00:16:50,106
Does it occur regularly? What are the largest gaps

254
00:16:50,138 --> 00:16:54,250
between consecutive not nuns values? Here I have plotted

255
00:16:54,330 --> 00:16:57,706
some missing data statistics starting from basic bar

256
00:16:57,748 --> 00:17:01,954
plot. As you can see, many columns do not contain any

257
00:17:01,992 --> 00:17:05,602
nuns, but there exist columns with significant

258
00:17:05,666 --> 00:17:08,950
amount of missing values such as carbon monoxide

259
00:17:09,370 --> 00:17:13,222
next heat map. Heat map helps us determine which

260
00:17:13,276 --> 00:17:16,950
occurrences of nuns in different columns are correlated.

261
00:17:17,390 --> 00:17:20,886
We can see a strong correlation of columns describing

262
00:17:20,918 --> 00:17:23,478
the weather such as pressure or temperature.

263
00:17:23,654 --> 00:17:27,686
Correlation of occurrences of missing values in different columns

264
00:17:27,718 --> 00:17:32,446
may be also expressed with dendogram here

265
00:17:32,628 --> 00:17:35,898
and apart from basic statistics and correlations,

266
00:17:35,994 --> 00:17:39,034
we can check the distribution for specific columns.

267
00:17:39,162 --> 00:17:42,630
We can select column and here we have length

268
00:17:42,730 --> 00:17:46,306
of consecutive nuns histogram. As we can see

269
00:17:46,408 --> 00:17:49,582
in this example, most of consecutive nuns sequences

270
00:17:49,646 --> 00:17:53,394
are short. However, series as long as

271
00:17:53,432 --> 00:17:57,574
60 also exist. And the red plot shows the

272
00:17:57,612 --> 00:18:00,786
length of gaps between missing values.

273
00:18:00,978 --> 00:18:05,074
So if it was a straight line, that would mean that non

274
00:18:05,202 --> 00:18:07,670
missing values occur regularly.

275
00:18:08,170 --> 00:18:11,558
They do not in this case. So now we need to handle

276
00:18:11,574 --> 00:18:15,274
the missing data. We could apply standard basic methods like

277
00:18:15,312 --> 00:18:19,162
backward filling layer or polynomial interpolation. We could also

278
00:18:19,216 --> 00:18:22,406
use more advanced methods, for example based on machine

279
00:18:22,438 --> 00:18:26,398
learning models. Here we have examples in the plot we

280
00:18:26,404 --> 00:18:30,426
can see fragment of one of time series for visualization

281
00:18:30,538 --> 00:18:34,902
purposes we can artificially increase number of missileness.

282
00:18:35,066 --> 00:18:39,534
We can select the percentage of values which will be randomly removed

283
00:18:39,662 --> 00:18:43,102
and see how different simple inputation

284
00:18:43,166 --> 00:18:47,218
methods will fill these values. So starting

285
00:18:47,304 --> 00:18:51,910
from forward filling fraud in our interpolarization

286
00:18:52,890 --> 00:18:56,582
to spline with the higher order which will give

287
00:18:56,636 --> 00:19:00,070
us smoother curve. From analysis

288
00:19:00,150 --> 00:19:03,674
of missing values, we know that in our case, sometimes the

289
00:19:03,712 --> 00:19:07,754
gap between two not missing values is very large.

290
00:19:07,952 --> 00:19:11,614
Here we have plotted example. We will not insert anything

291
00:19:11,732 --> 00:19:14,874
in that case, but just split series

292
00:19:14,922 --> 00:19:18,510
into two shorter series. Second thing

293
00:19:18,580 --> 00:19:21,386
which needs to be done are data transformations.

294
00:19:21,578 --> 00:19:25,710
This step is crucial for some statistical models which require

295
00:19:25,790 --> 00:19:29,570
often series in specific format, for example stationary.

296
00:19:29,910 --> 00:19:34,034
For more advanced models it's often not essential, but may

297
00:19:34,072 --> 00:19:37,734
also help with numerical issues. Here we have listed some

298
00:19:37,772 --> 00:19:41,814
basic transformations which can be applied for time series and

299
00:19:41,852 --> 00:19:45,650
we can see how our series would look after the given transformation.

300
00:19:45,810 --> 00:19:49,530
We can also use more advanced transformations like embeddings.

301
00:19:50,110 --> 00:19:54,454
Example of simple but effective time transformation is encoding

302
00:19:54,502 --> 00:19:57,654
cyclical features like hours, days, et cetera

303
00:19:57,702 --> 00:20:01,790
in the unit circle like in the presented GIF,

304
00:20:02,130 --> 00:20:05,886
and for that we are using

305
00:20:06,068 --> 00:20:10,078
this formula. So before the modeling for

306
00:20:10,164 --> 00:20:14,142
our task, we will still be missing values with linear interpolation

307
00:20:14,286 --> 00:20:17,922
normalized features. Sometimes we use also box

308
00:20:17,976 --> 00:20:21,886
cock transformation and encode cyclical features into a unit

309
00:20:21,918 --> 00:20:25,060
circle, in our case hours and days.

310
00:20:25,450 --> 00:20:29,074
And for modeling we will choose one column nitrogen

311
00:20:29,122 --> 00:20:32,534
dioxide and prediction horizon equal to

312
00:20:32,572 --> 00:20:36,882
six. And firstly, we will train baselines

313
00:20:36,946 --> 00:20:40,074
and simpler statistical models to have some point of

314
00:20:40,112 --> 00:20:43,930
reference. And then we will move to neural network methods.

315
00:20:44,270 --> 00:20:49,370
And before the models results,

316
00:20:50,290 --> 00:20:53,070
a few words about the training setup,

317
00:20:53,490 --> 00:20:56,670
we'll use train validation and test split.

318
00:20:57,170 --> 00:21:00,350
And for evaluation we'll use rolling window.

319
00:21:01,010 --> 00:21:05,134
Here we have plotted train validation, test splits and

320
00:21:05,172 --> 00:21:09,570
we will start with extremely simple models. Nave predictors

321
00:21:10,310 --> 00:21:13,854
it's good to always look at them in time series forecasting.

322
00:21:13,982 --> 00:21:17,750
They are very easy to use and it often happens

323
00:21:17,820 --> 00:21:21,414
that the metrics graphs results. Statistics of

324
00:21:21,452 --> 00:21:25,222
our model look okay at the first glance, but then

325
00:21:25,276 --> 00:21:29,034
it turns out that the naive prediction is better and

326
00:21:29,072 --> 00:21:32,806
our model is worthless. So it's

327
00:21:32,838 --> 00:21:37,046
a good practice to start first with naive

328
00:21:37,078 --> 00:21:40,960
prediction. And it's worth to mention that

329
00:21:41,490 --> 00:21:46,254
some alternatives for naive predictions would

330
00:21:46,292 --> 00:21:50,350
be usage of metrics like mean absolute scaled error.

331
00:21:50,930 --> 00:21:54,286
And apart from baby baselines,

332
00:21:54,318 --> 00:21:57,906
we also train some classical models. For example,

333
00:21:58,008 --> 00:22:02,062
sarima proffered Tibet's or exponential time series.

334
00:22:02,206 --> 00:22:05,894
It's moving and these models will be fine

335
00:22:05,932 --> 00:22:09,954
tuned with rolling window, with hyperparameters grid

336
00:22:10,002 --> 00:22:14,040
search or biasion based hyperparameter search.

337
00:22:14,890 --> 00:22:18,886
Okay, so as we have seasonal data, we use two naive

338
00:22:18,918 --> 00:22:22,282
predictors, last value repetition and

339
00:22:22,336 --> 00:22:25,722
repetition of values from the previous day with the same

340
00:22:25,776 --> 00:22:29,954
hour. And here are the results. We'll compare

341
00:22:30,022 --> 00:22:32,110
them later with other methods.

342
00:22:32,850 --> 00:22:36,350
And for advanced neural network models,

343
00:22:37,570 --> 00:22:41,626
we'll choose two methods, temporal fusion transformer

344
00:22:41,658 --> 00:22:45,586
and nbids. Both of them are the state of

345
00:22:45,608 --> 00:22:47,970
the art models,

346
00:22:48,870 --> 00:22:52,994
but they have different advantage and complete each

347
00:22:53,032 --> 00:22:56,866
other very well. In this picture we can see architecture

348
00:22:56,898 --> 00:23:01,270
of temporal current transformer and as you can see it's quite complicated,

349
00:23:01,930 --> 00:23:05,174
but we will not talk about the technical details. We will

350
00:23:05,212 --> 00:23:08,666
focus on advantage of this model. So first of

351
00:23:08,688 --> 00:23:12,454
all, good results. According to the paper comparing

352
00:23:12,502 --> 00:23:16,410
CTO statistical and neural network models,

353
00:23:16,910 --> 00:23:20,182
the very, very big advantage

354
00:23:20,246 --> 00:23:23,870
of temporal sugar transformers is the fact that it works

355
00:23:23,940 --> 00:23:27,758
with multivariate time series with different types of data,

356
00:23:27,844 --> 00:23:31,070
like categorical, continuous or static

357
00:23:33,090 --> 00:23:36,494
temporarily. Transformer has also implemented variable

358
00:23:36,542 --> 00:23:39,650
selection network, so it allows us save

359
00:23:39,720 --> 00:23:43,714
time during the data proprietary. Its results are

360
00:23:43,752 --> 00:23:48,070
interpretable thanks to attention mechanism.

361
00:23:49,210 --> 00:23:52,774
It also works with known feature inputs and

362
00:23:52,812 --> 00:23:56,818
that allows us creating conditional

363
00:23:56,914 --> 00:24:00,950
predictions. In general, it's applicable

364
00:24:01,030 --> 00:24:04,874
without modification CTO a wide range of problems and

365
00:24:04,912 --> 00:24:08,906
we could obtain some explainable predictions. And the

366
00:24:08,928 --> 00:24:14,394
second chosen model was NBITs. And nbeats outperformed

367
00:24:14,442 --> 00:24:17,950
the winner model from prestigious and for competition.

368
00:24:18,690 --> 00:24:22,640
It means that it achieved the highest scores on

369
00:24:23,170 --> 00:24:27,502
100,000 time series from different domains.

370
00:24:27,646 --> 00:24:31,246
So it's a bunch of quality. It's designed

371
00:24:31,278 --> 00:24:33,730
for univariate time series.

372
00:24:34,470 --> 00:24:38,182
Its results are also interpretable. But thanks

373
00:24:38,236 --> 00:24:42,054
to special models which try to explain the trend and

374
00:24:42,092 --> 00:24:46,534
seasonality, and to sum up. TFT and

375
00:24:46,652 --> 00:24:50,470
NBIT both have very good scores

376
00:24:51,210 --> 00:24:55,002
and try to deliver interpretable predictions but are

377
00:24:55,056 --> 00:24:57,580
optimized for different types of data.

378
00:24:58,110 --> 00:25:01,298
NBIT is optimized for universe

379
00:25:01,334 --> 00:25:06,062
time series and TFT is optimized for any

380
00:25:06,116 --> 00:25:09,786
type of time series with any types

381
00:25:09,818 --> 00:25:13,858
of data. Okay, so for

382
00:25:13,944 --> 00:25:17,842
neural network training we use

383
00:25:17,976 --> 00:25:22,034
early stopping learning rate scheduler and

384
00:25:22,072 --> 00:25:26,002
guardian clipping and sometimes but not in this

385
00:25:26,056 --> 00:25:30,086
case, we use also biological based framework like

386
00:25:30,188 --> 00:25:33,910
optuna for hyperparameters

387
00:25:34,490 --> 00:25:37,670
and networks architecture optimization.

388
00:25:38,350 --> 00:25:41,798
Okay, so let's move to results.

389
00:25:41,974 --> 00:25:45,290
The accurate metrics results will be presented later

390
00:25:45,360 --> 00:25:49,050
in the table, but here we have some GIF for

391
00:25:49,120 --> 00:25:53,486
NBID performance on the test set and

392
00:25:53,588 --> 00:25:58,042
this gray rectangle presented the prediction horizontal

393
00:25:58,106 --> 00:26:02,080
so its width is equal to six because

394
00:26:02,450 --> 00:26:05,602
our horizon is equal to

395
00:26:05,656 --> 00:26:09,454
6 hours and the same GIF was delivered

396
00:26:09,502 --> 00:26:12,654
for TFT. And data are noisy

397
00:26:12,702 --> 00:26:15,960
but predictions sometimes look okay.

398
00:26:16,970 --> 00:26:20,390
Model often correctly predicts future

399
00:26:20,460 --> 00:26:23,720
forecast direction which is good.

400
00:26:24,490 --> 00:26:28,394
And here are some predictions for PFT from

401
00:26:28,432 --> 00:26:32,314
test set which are actually very good and

402
00:26:32,512 --> 00:26:37,402
they were selected randomly but luckily we

403
00:26:37,456 --> 00:26:41,214
can very good samples for

404
00:26:41,252 --> 00:26:44,560
sure. There are also worse examples in this test set.

405
00:26:45,250 --> 00:26:49,422
And here we have table with different experiments with

406
00:26:49,476 --> 00:26:53,630
tnt and nbeats and different loss functions

407
00:26:53,790 --> 00:26:58,482
different loss functions for regression problem and

408
00:26:58,616 --> 00:27:02,286
here we have also metrics results like typical

409
00:27:02,478 --> 00:27:06,134
metrics for regressions like mean

410
00:27:06,172 --> 00:27:10,946
absolute error or mean average

411
00:27:11,058 --> 00:27:14,822
percentage error et cetera and

412
00:27:14,876 --> 00:27:18,140
the best scores are highlighted in green.

413
00:27:18,670 --> 00:27:22,374
And as we can see, temporary fusion transformer

414
00:27:22,502 --> 00:27:26,060
with quantile loss scored the best

415
00:27:27,550 --> 00:27:30,922
mean average error for naive prediction for for

416
00:27:30,976 --> 00:27:35,070
naive predictions were around 25.

417
00:27:35,220 --> 00:27:39,854
So our neural networks clearly learn anything

418
00:27:39,972 --> 00:27:43,860
and are significantly better

419
00:27:44,310 --> 00:27:47,010
than nave predictions.

420
00:27:47,430 --> 00:27:50,194
And the next question is can we do better?

421
00:27:50,392 --> 00:27:54,594
Of course we can try to optimize caper parameters or

422
00:27:54,632 --> 00:27:58,022
network architecture, but there is one

423
00:27:58,076 --> 00:28:01,622
thing which requires less time and

424
00:28:01,676 --> 00:28:05,426
is extremely effective. Ensembling even models

425
00:28:05,458 --> 00:28:08,890
with the same architecture trained with different

426
00:28:08,960 --> 00:28:12,586
loss function, input length training,

427
00:28:12,688 --> 00:28:16,182
hyperparameters or transformation can contribute

428
00:28:16,246 --> 00:28:21,134
to score improvement in assembling. And here

429
00:28:21,172 --> 00:28:25,066
we have a proof. These are experiments

430
00:28:25,098 --> 00:28:28,874
with TFT or nbeats and differing

431
00:28:28,922 --> 00:28:33,310
only in loss function. So for example

432
00:28:33,460 --> 00:28:37,762
we used quantity loss or mean

433
00:28:37,816 --> 00:28:42,130
absolute error loss or root mean square error loss

434
00:28:42,630 --> 00:28:46,626
or new loss function delayed

435
00:28:46,658 --> 00:28:50,374
loss which is

436
00:28:50,412 --> 00:28:54,614
different than which

437
00:28:54,652 --> 00:28:58,814
is significantly different than these other loss

438
00:28:58,962 --> 00:29:02,570
losses and we end up with over 15

439
00:29:02,640 --> 00:29:06,234
percentage of mean absolute error improvement over the

440
00:29:06,272 --> 00:29:10,266
best single model and even single models

441
00:29:10,298 --> 00:29:14,346
with low score like this one. TFT with delayed

442
00:29:14,378 --> 00:29:18,110
loss, the worst model from all experiments with TFT

443
00:29:18,450 --> 00:29:22,510
contributes positively to score improvement.

444
00:29:23,490 --> 00:29:27,470
As I mentioned, both TFT and nbeats aim to give explainable

445
00:29:27,550 --> 00:29:30,850
predictions and here we have results obtained from TFT.

446
00:29:31,350 --> 00:29:35,186
First lot shows the value's importance in time. So the higher is

447
00:29:35,208 --> 00:29:38,402
the value here, the more important was that time

448
00:29:38,456 --> 00:29:40,790
point during the predictions.

449
00:29:42,010 --> 00:29:45,906
In this case, the most influential data were measured

450
00:29:46,018 --> 00:29:49,322
168 hours, so seven days

451
00:29:49,376 --> 00:29:53,414
before the prediction time. So it means that it suggests

452
00:29:53,462 --> 00:29:56,986
that we can have weak seasonality here and here we

453
00:29:57,008 --> 00:30:01,534
have features importance from variable selection submodel as

454
00:30:01,572 --> 00:30:05,946
expected, the most important feature is nitrogen

455
00:30:06,138 --> 00:30:10,030
dioxide. So our target and

456
00:30:10,100 --> 00:30:14,382
decoded variable importance plot for feature

457
00:30:14,446 --> 00:30:17,218
known values like those connected with time,

458
00:30:17,384 --> 00:30:20,750
so it shows which features

459
00:30:20,830 --> 00:30:25,382
were the most important from

460
00:30:25,436 --> 00:30:28,230
the known feature inputs.

461
00:30:28,890 --> 00:30:32,726
With small modification of architecture, we can also see

462
00:30:32,828 --> 00:30:37,590
which features were the most important for specific timestamp.

463
00:30:38,430 --> 00:30:40,940
As I mentioned, to obtain such result,

464
00:30:41,950 --> 00:30:46,102
we need to slightly change architecture. So there is no guarantee

465
00:30:46,166 --> 00:30:50,038
that model will work as good as the original one

466
00:30:50,224 --> 00:30:53,886
on any type of data. But for some examples it also

467
00:30:53,988 --> 00:30:57,914
works. It relies

468
00:30:57,962 --> 00:31:02,094
on the same mechanism like the

469
00:31:02,132 --> 00:31:07,322
original safety, so it uses attention

470
00:31:07,386 --> 00:31:11,230
layer for explainability.

471
00:31:11,730 --> 00:31:15,042
Okay, and that's all. Thank you all for attending

472
00:31:15,066 --> 00:31:15,250
tension.

