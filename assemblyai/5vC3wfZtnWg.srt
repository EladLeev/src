1
00:01:54,090 --> 00:01:56,926
Over the next half hour, I'm going to talk through a story of when I

2
00:01:56,948 --> 00:02:00,586
worked at a cloud consultancy firm, how we bread

3
00:02:00,618 --> 00:02:04,474
teamed ourselves, how we hacked our own AWS

4
00:02:04,522 --> 00:02:08,434
environment, and to see what would happen, see what we'd

5
00:02:08,482 --> 00:02:12,006
learn along the way. To see whether

6
00:02:12,188 --> 00:02:15,478
people knew our security policies and procedures as

7
00:02:15,484 --> 00:02:18,780
well as we thought they did. To see who would step up

8
00:02:19,630 --> 00:02:24,170
given the chance. So I'm going to kick off today

9
00:02:24,240 --> 00:02:26,154
with a wonderful japanese word,

10
00:02:26,272 --> 00:02:30,102
sujugiri, which is the closest

11
00:02:30,166 --> 00:02:34,000
embodiment of the spirit with which we undertook that fateful day.

12
00:02:34,450 --> 00:02:37,658
And for those who are unaware of the meaning,

13
00:02:37,834 --> 00:02:41,946
it means trying out a new samurai sword on a random passerby,

14
00:02:42,138 --> 00:02:45,374
which is a practice that japanese samurai used to

15
00:02:45,412 --> 00:02:48,370
have. They'd used to test out new swords.

16
00:02:48,870 --> 00:02:52,322
And this random passerby bit was kind of the interesting bit here,

17
00:02:52,376 --> 00:02:55,554
because the red team that we had for the day

18
00:02:55,592 --> 00:02:59,046
knew what was going to happen. The blue team didn't know they were

19
00:02:59,068 --> 00:03:02,358
a blue team until everything started.

20
00:03:02,444 --> 00:03:06,246
And we'll have a look at exactly what happens when you do

21
00:03:06,268 --> 00:03:09,402
that as we go along. Now, a little bit about

22
00:03:09,456 --> 00:03:13,146
me. I'm Josh. I'm a distinguished technologist at Contino. I'm the

23
00:03:13,168 --> 00:03:17,782
author of the Cloud Native Security cookbook with O'Reilly Hash

24
00:03:17,846 --> 00:03:21,470
ambassador, AWS ambassador. I do lots of stuff in cloud

25
00:03:21,540 --> 00:03:24,160
and I write and run my mouth a lot.

26
00:03:26,370 --> 00:03:30,334
So kind of the overarching thing we were trying to do with this

27
00:03:30,372 --> 00:03:32,880
red beating exercise was shift left learning.

28
00:03:34,550 --> 00:03:37,922
How do we make it so people are learning things ahead

29
00:03:37,976 --> 00:03:42,260
of time or as early as possible. Make the learning cheap and

30
00:03:43,430 --> 00:03:47,058
how can we do better? You really don't want all your

31
00:03:47,144 --> 00:03:50,982
employees understanding what the instant response process while

32
00:03:51,036 --> 00:03:55,046
a real instant is going on. That's very much too late to be

33
00:03:55,068 --> 00:03:57,560
learning the ins and outs and to get that experience.

34
00:03:58,590 --> 00:04:02,790
When you look at this ubiquitous curve of time versus

35
00:04:02,870 --> 00:04:06,874
cost during an active security

36
00:04:06,992 --> 00:04:10,506
event, it's very expensive to have people learning. What you

37
00:04:10,528 --> 00:04:13,706
want is people that know what they're doing who've been around before. Yeah, of course,

38
00:04:13,728 --> 00:04:16,350
you're going to have to do some exploration. There's always going to be new things,

39
00:04:16,420 --> 00:04:20,414
but a solid foundation of what the process is, what tools we

40
00:04:20,452 --> 00:04:24,302
have. Making sure that our security approach

41
00:04:24,446 --> 00:04:27,826
is robust and rigorous and resilient and gives everyone what

42
00:04:27,848 --> 00:04:31,490
they need is really important. And for a consultancy,

43
00:04:31,830 --> 00:04:35,266
you shouldn't be telling clients to do what you're unwilling to do

44
00:04:35,288 --> 00:04:38,594
yourselves. You shouldn't be trying to tell them

45
00:04:38,632 --> 00:04:41,826
to adopt principles and approaches that you are unwilling

46
00:04:41,858 --> 00:04:45,334
to do yourself. So this was really an example for us to put our

47
00:04:45,372 --> 00:04:47,960
foot forward and try and see what happened.

48
00:04:48,970 --> 00:04:52,230
Now, to kick off, I'm just going to go through a few mental models,

49
00:04:52,310 --> 00:04:55,290
some of which you may be aware of previously.

50
00:04:56,190 --> 00:05:00,178
This is a fairly classic one about different kinds of knowledge.

51
00:05:00,374 --> 00:05:03,866
You have known knowns, known unknowns

52
00:05:03,898 --> 00:05:07,354
and unknowns. Unknowns. Now known

53
00:05:07,402 --> 00:05:10,400
knowns are the things you know, you know,

54
00:05:10,850 --> 00:05:14,526
you know, you know these things, right? And these are some principles,

55
00:05:14,558 --> 00:05:18,500
like security is everyone's responsibility, right? We all know that.

56
00:05:19,670 --> 00:05:23,106
At least we did this consultancy. We all knew that it was all of

57
00:05:23,128 --> 00:05:26,646
our responsibilities. But what

58
00:05:26,668 --> 00:05:29,400
does it actually look like when the rubber meets the road?

59
00:05:31,370 --> 00:05:35,046
Every company on earth pretty much has an response process that

60
00:05:35,068 --> 00:05:39,014
outlines what should happen in the instance of a breach

61
00:05:39,062 --> 00:05:43,020
or a potential breach. For us,

62
00:05:43,870 --> 00:05:47,654
this was pre Covid as well. For us, being a consultancy

63
00:05:47,702 --> 00:05:50,906
firm, we were generally geographically dispersed across the

64
00:05:50,928 --> 00:05:54,334
city in which we operated. So for this, we actually

65
00:05:54,372 --> 00:05:58,510
brought everyone together. We didn't want to make it too challenging

66
00:05:59,010 --> 00:06:02,426
for everyone, right? And we wanted to bring people together, so the bandwidth

67
00:06:02,458 --> 00:06:06,206
of communication was really high so people could chat face to face. We weren't

68
00:06:06,318 --> 00:06:09,806
doing this all over slack. We weren't getting distracted while on client

69
00:06:09,838 --> 00:06:13,460
side. So we've brought everyone together for this. Everyone in the same room.

70
00:06:15,190 --> 00:06:19,046
We also thought we had a good idea of the expected avenues of when

71
00:06:19,068 --> 00:06:22,726
we started the red team event, we thought there

72
00:06:22,748 --> 00:06:25,846
were things we expected the blue team would do, things they would try.

73
00:06:26,028 --> 00:06:30,054
Naturally, us being the masochists that we were, we decided

74
00:06:30,102 --> 00:06:32,970
to disable some of these by default.

75
00:06:33,630 --> 00:06:37,066
Some of these things were making it so

76
00:06:37,168 --> 00:06:40,894
the most senior people who were going to be on the blue team side,

77
00:06:41,012 --> 00:06:44,560
their access was broken. Right.

78
00:06:45,410 --> 00:06:48,894
We made sure we planned out our red

79
00:06:48,932 --> 00:06:52,154
team approach. Don't think it technically classifies

80
00:06:52,202 --> 00:06:55,486
as Osint, but you would expect that a real threat

81
00:06:55,518 --> 00:06:59,698
actor would map out the environment and understand,

82
00:06:59,784 --> 00:07:03,780
potentially, the way that the blue team is going to respond to things,

83
00:07:05,110 --> 00:07:08,306
all right, into known unknowns, things we know we didn't

84
00:07:08,338 --> 00:07:12,054
know for this. How do we measure performance in

85
00:07:12,092 --> 00:07:15,734
this? This isn't so much measure performance to

86
00:07:15,772 --> 00:07:18,886
measure against anyone else, but more just to give us a benchmark of when

87
00:07:18,908 --> 00:07:20,860
we do it again. Have we got better?

88
00:07:22,670 --> 00:07:26,314
Have we, as a team and AWS a company improved over time,

89
00:07:26,352 --> 00:07:29,878
or have we stayed the same? Have we got worse? So we picked some

90
00:07:29,984 --> 00:07:33,438
measures. Feel free to steal them if you

91
00:07:33,444 --> 00:07:36,606
try this kind of thing yourself. So first was time to

92
00:07:36,628 --> 00:07:40,574
identify. So from when the red team

93
00:07:40,612 --> 00:07:43,810
event started, how long was it until

94
00:07:43,960 --> 00:07:48,242
the breach was identified, recognized, and called

95
00:07:48,296 --> 00:07:51,730
that there was something weird going on in the environment.

96
00:07:52,310 --> 00:07:55,770
The second was the time to contain. So from once we're

97
00:07:55,790 --> 00:08:00,726
identified, how long was it until the blue team got control and managed to get

98
00:08:00,748 --> 00:08:04,086
the red team out of all systems? Very interesting.

99
00:08:04,188 --> 00:08:08,694
One percentage of intrusion detected. So of

100
00:08:08,732 --> 00:08:12,326
all the things we managed to do as a red team, how many were found?

101
00:08:12,508 --> 00:08:15,686
And this was not during the event, but also including some cleanup

102
00:08:15,718 --> 00:08:18,858
time afterwards. Like, how much stuff did people find? Of the things that we did

103
00:08:19,024 --> 00:08:23,040
as the red team were able to leave things lying around

104
00:08:23,570 --> 00:08:27,102
in the cloud that people weren't aware of, that could have been the site

105
00:08:27,156 --> 00:08:30,846
for a follow up breach. Who will

106
00:08:30,868 --> 00:08:34,462
take responsibility? So, with this,

107
00:08:34,516 --> 00:08:37,538
the red team was made of myself and one of

108
00:08:37,544 --> 00:08:41,362
the guys who are the more senior engineers in the company and the entire

109
00:08:41,416 --> 00:08:44,594
leadership team. The three directors at this company knew what we're doing.

110
00:08:44,632 --> 00:08:47,734
And what actually happened on the day was we all went

111
00:08:47,772 --> 00:08:51,414
somewhere else to do the red team event.

112
00:08:51,532 --> 00:08:54,962
So the blue team was left in the officers, but the leadership

113
00:08:55,026 --> 00:08:58,486
team and the two guys on the red team, we were

114
00:08:58,508 --> 00:09:02,106
elsewhere. And this was a really interesting thing to see. What would happen if we

115
00:09:02,128 --> 00:09:05,514
left the leadership vacuum? Who would stand up

116
00:09:05,552 --> 00:09:10,986
and take control of the situation? Who would take up the

117
00:09:11,008 --> 00:09:13,440
mantle of leadership as it was on the day?

118
00:09:14,610 --> 00:09:18,160
Another thing this is a kind of rule of thumb that

119
00:09:18,530 --> 00:09:21,870
I subscribe to, is that processors generally break

120
00:09:21,940 --> 00:09:25,074
around three x. Now, when I joined this company, it was

121
00:09:25,112 --> 00:09:28,020
about ten people, and about some of us were about 30.

122
00:09:28,870 --> 00:09:32,894
We had a feeling that maybe our processes weren't

123
00:09:32,942 --> 00:09:36,562
fit for purpose anymore. And rather than

124
00:09:36,616 --> 00:09:39,702
try and come up with new processes just based out of our

125
00:09:39,756 --> 00:09:43,302
heads, we figured, how about we actually try some stuff and build the process around

126
00:09:43,356 --> 00:09:46,690
what we find, as opposed to based them on evidence,

127
00:09:46,770 --> 00:09:48,540
rather than just what we think.

128
00:09:50,670 --> 00:09:54,170
And the last known unknown for me is a really interesting

129
00:09:54,240 --> 00:09:58,730
one. I think about a lot is in terms of what lenses do people possess.

130
00:09:59,150 --> 00:10:02,966
So when developers generally

131
00:10:02,998 --> 00:10:06,138
have a good development lens, they might not have a good operational lens

132
00:10:06,154 --> 00:10:09,934
or a good security lens. Like, they look at problems a particular way, they build

133
00:10:09,972 --> 00:10:13,646
solutions of a particular style because of the nature of

134
00:10:13,668 --> 00:10:17,058
their experience and what they do. And I always find it's interesting when

135
00:10:17,064 --> 00:10:20,706
you can find the generalists or the t shaped or m shaped people or

136
00:10:20,728 --> 00:10:23,714
whatever letter we're using nowadays to talk through.

137
00:10:23,752 --> 00:10:27,154
Well, can they look at stuff through a security lens? Can they

138
00:10:27,272 --> 00:10:30,486
understand it that way? Can they empathize with security? Can they look at

139
00:10:30,508 --> 00:10:34,006
things in that way? And this was an

140
00:10:34,028 --> 00:10:37,734
experiment on my behalf to find out, well, what security skills do we

141
00:10:37,772 --> 00:10:41,514
have in the business? Do we have people with that inclination? Do people have that

142
00:10:41,552 --> 00:10:45,254
ability to switch? Because we were made of mostly developers,

143
00:10:45,302 --> 00:10:48,570
we worked as DevOps teams like you build, you run. That was

144
00:10:48,640 --> 00:10:52,378
our table stakes, what we did every day. So this was a

145
00:10:52,384 --> 00:10:55,694
really interesting point to find out what people cloud,

146
00:10:55,732 --> 00:10:59,786
we cultivate into security champions, because they just that way inclined,

147
00:10:59,818 --> 00:11:02,800
and they have that ability to see the world the right way.

148
00:11:03,810 --> 00:11:07,598
Unless there's the unknowns. Unknowns we knew going into this. We discover

149
00:11:07,694 --> 00:11:11,906
answers to questions we didn't even know we had. And you

150
00:11:11,928 --> 00:11:15,838
just find all these things. And when you think about unknowns, I mean, chaos engineering

151
00:11:15,854 --> 00:11:19,542
is the classic example that we do nowadays. You break

152
00:11:19,596 --> 00:11:23,494
things on purpose to realize what you find out from

153
00:11:23,532 --> 00:11:25,958
there. You don't necessarily know the question up front,

154
00:11:26,044 --> 00:11:29,242
but you decide to test things and see what happens.

155
00:11:29,296 --> 00:11:32,886
This is very much not the systems chaos

156
00:11:32,918 --> 00:11:37,162
engineering of Netflix fame, but more a chaos engineering of a really,

157
00:11:37,216 --> 00:11:40,460
really interesting day. It was.

158
00:11:42,270 --> 00:11:45,706
Naturally, we were set some rules of engagement. There were

159
00:11:45,728 --> 00:11:49,674
some limits to what we were allowed to do. The CEO naturally put a financial

160
00:11:49,722 --> 00:11:52,046
limit on what we, as the red team were able to do. We couldn't just

161
00:11:52,068 --> 00:11:55,646
go spin up some crypto mining and beating in mind, this was 2019,

162
00:11:55,678 --> 00:11:59,074
so crypto mining was quite valuable back then as well.

163
00:11:59,272 --> 00:12:02,674
Went hard to do that, and a big thing was

164
00:12:02,712 --> 00:12:06,482
making sure that we didn't overly demoralize. We didn't make it too

165
00:12:06,536 --> 00:12:10,680
hard for the blue team to counteract. Right. When you're doing these things,

166
00:12:11,370 --> 00:12:14,726
you win together or you lose together. There's no blue team one

167
00:12:14,748 --> 00:12:18,198
or red team one. It's about improving everyone together, really kind

168
00:12:18,204 --> 00:12:21,420
of that holistic, like, more wholesome approach to moving forward.

169
00:12:22,590 --> 00:12:26,122
All right. And with all that, we'll actually shift on to the

170
00:12:26,256 --> 00:12:29,626
day itself. So it started at 10:00 a.m.

171
00:12:29,648 --> 00:12:32,766
Because be kind, let people have their caffeine, let it kick in. Make sure the

172
00:12:32,788 --> 00:12:36,106
coffees are nicely imbibed before you kick

173
00:12:36,138 --> 00:12:39,854
off, and you start channeling a huge amount of stress into

174
00:12:39,892 --> 00:12:43,198
people. A really important

175
00:12:43,284 --> 00:12:46,466
thing. We set out with this, and I kind of alluded to

176
00:12:46,488 --> 00:12:50,626
it before, where we disabled senior members access to

177
00:12:50,648 --> 00:12:54,306
our cloud environments. This was an opportunity for us to channel learning

178
00:12:54,488 --> 00:12:57,010
through our more junior members of staff.

179
00:12:57,670 --> 00:13:01,442
So rather than have it so all the senior engineers

180
00:13:01,506 --> 00:13:05,126
all sit in a group of five, and the 15 more

181
00:13:05,148 --> 00:13:08,726
junior people end up being kind of pushed to the side. We set it

182
00:13:08,748 --> 00:13:13,142
up so the only people with access to affect change on the environment

183
00:13:13,286 --> 00:13:16,586
were the more junior members of staff. So very much in like a

184
00:13:16,608 --> 00:13:19,962
pair programming style. The seniors had to channel their

185
00:13:20,016 --> 00:13:23,470
ideas and their expertise through the more junior people.

186
00:13:23,540 --> 00:13:27,182
So we're actually able to get this really nice passage of knowledge going

187
00:13:27,236 --> 00:13:30,030
through them. So everyone got to learn. It didn't end up in,

188
00:13:30,180 --> 00:13:34,046
well, the guys who are the most senior guys, they've got

189
00:13:34,068 --> 00:13:37,138
it. We'll just sit back. We really didn't want that happen. We really want to

190
00:13:37,144 --> 00:13:40,580
feel the more junior staff to feel involved. Right.

191
00:13:41,110 --> 00:13:45,202
So at ten three, we tripped the wire, so to speak.

192
00:13:45,336 --> 00:13:48,626
So being a serverless first consultancy, as they still are

193
00:13:48,648 --> 00:13:52,162
to this day, we said that we booted up a virtual machine

194
00:13:52,226 --> 00:13:56,098
in our AWS environment because we had alerts that if anyone booted

195
00:13:56,114 --> 00:13:59,734
up a virtual machine, it would trigger alerts in slack to say, something weird is

196
00:13:59,772 --> 00:14:03,434
going on. We shouldn't be doing this. At the same time,

197
00:14:03,552 --> 00:14:06,986
we sent out a phishing email as well that we created just

198
00:14:07,008 --> 00:14:10,766
to see what would happen, right? Give them

199
00:14:10,788 --> 00:14:15,102
a chance to figure out that something's going on. And six

200
00:14:15,156 --> 00:14:16,160
minutes later,

201
00:14:18,530 --> 00:14:22,222
six minutes later, someone did notice what was going

202
00:14:22,276 --> 00:14:25,390
on. Someone called out that, hey,

203
00:14:25,540 --> 00:14:28,486
something's looking a bit fishy. We could see it on slack that they were messaging.

204
00:14:28,538 --> 00:14:31,250
Like, is anyone trying to do something? Because this looks a bit weird.

205
00:14:32,070 --> 00:14:35,934
And they started asking for some help, and a minute

206
00:14:35,982 --> 00:14:39,814
later, someone came to help them. Now, interestingly, both those

207
00:14:39,852 --> 00:14:43,926
people access we killed as soon as they started trying

208
00:14:43,948 --> 00:14:47,526
to do anything. You might be wondering at this point how

209
00:14:47,548 --> 00:14:50,806
we know what's going on in the room and we actually had

210
00:14:50,828 --> 00:14:54,202
to fly on the wall. So we actually had one person on the blue team

211
00:14:54,256 --> 00:14:58,154
side who was aware ahead of time that something was going to happen and was

212
00:14:58,192 --> 00:15:01,946
a communication conduit between ourselves and kind of what was going

213
00:15:01,968 --> 00:15:05,866
on in the room. Right? Like kind of said before, we didn't want to overly

214
00:15:05,898 --> 00:15:08,958
stress people. We want to make sure this was something that's going

215
00:15:08,964 --> 00:15:11,630
to remembered, at least mostly fondly,

216
00:15:12,450 --> 00:15:16,586
and make sure that we weren't pushing

217
00:15:16,618 --> 00:15:19,682
too hard. And at the same time, we weren't making too easy for them either.

218
00:15:19,736 --> 00:15:22,098
We want them to be stretched. We want them to try. I want it to

219
00:15:22,104 --> 00:15:24,420
be a challenge. Right?

220
00:15:25,910 --> 00:15:29,138
So two minutes after, the second person came with a

221
00:15:29,144 --> 00:15:32,550
pair of hands, not a drill, it was called out on slack that,

222
00:15:32,700 --> 00:15:35,606
okay, something's actually happening. Everyone needs the down tools and help.

223
00:15:35,628 --> 00:15:39,094
We need to mob around this problem and get there. Just exactly what we wanted

224
00:15:39,132 --> 00:15:42,634
to see, right? You want to see that when something serious is going

225
00:15:42,672 --> 00:15:45,946
on, that people will down tools and get involved and pitch in

226
00:15:45,968 --> 00:15:49,482
to help. Five minutes after that,

227
00:15:49,536 --> 00:15:53,146
the CEO was called. And I note this purely

228
00:15:53,178 --> 00:15:57,370
because call the CEO was number one, our instant response

229
00:15:57,530 --> 00:16:01,322
process. So from the initial

230
00:16:01,386 --> 00:16:04,942
time when something was noticed, to actually get into step one was eight

231
00:16:04,996 --> 00:16:08,210
minutes. And I had the lovely opportunity

232
00:16:08,280 --> 00:16:11,282
to see the CEO pick up his phone,

233
00:16:11,416 --> 00:16:13,934
look at it, put it back down on the table, and go back to drinking

234
00:16:13,982 --> 00:16:15,780
his coffee without a worry, in the words.

235
00:16:17,510 --> 00:16:21,394
So this was something that we had thought about a little bit.

236
00:16:21,432 --> 00:16:24,006
We didn't really know how it was going to go was organically, what structure was

237
00:16:24,028 --> 00:16:26,786
going to evolve out of the blue team with everyone in the office, what structure

238
00:16:26,818 --> 00:16:30,418
were they going to try and form to kind of combat what was

239
00:16:30,444 --> 00:16:34,026
going on? And the initial version looked like this. You had

240
00:16:34,128 --> 00:16:37,466
Paul, who was the first person to notice anything was

241
00:16:37,488 --> 00:16:40,806
going on, and he went, okay, I'm going to take ownership

242
00:16:40,838 --> 00:16:44,126
of this situation. And he had a whole bunch of people beneath him kind

243
00:16:44,148 --> 00:16:47,920
of reporting into him. Naturally, what happened there was this.

244
00:16:49,330 --> 00:16:52,734
There was too much going on, too much communication. He was trying to hold

245
00:16:52,772 --> 00:16:56,046
everything in his head. And bear in mind, it wasn't eight people talking to him,

246
00:16:56,148 --> 00:16:59,374
it was about 20. So when you've got that many people all trying to report

247
00:16:59,412 --> 00:17:02,542
into one person in a high pressure environment with a lot of going on,

248
00:17:02,676 --> 00:17:05,560
we all know that's not going to work, right? That's not possible. All.

249
00:17:07,050 --> 00:17:10,498
So a little bit later on, they had to stop and regroup.

250
00:17:10,594 --> 00:17:14,102
I think Paul decided his head was on fire enough

251
00:17:14,156 --> 00:17:18,294
and went, okay, let's actually stop and think

252
00:17:18,332 --> 00:17:21,706
about this and figure out what we're going to do. They ran an

253
00:17:21,728 --> 00:17:25,340
access poll to figure out who still has access

254
00:17:25,790 --> 00:17:29,066
to AWS, who can still do things, so they could understand

255
00:17:29,248 --> 00:17:33,454
what parallelization they could actually action things and how they could channel and

256
00:17:33,492 --> 00:17:36,830
best set up to approach the problem. They also

257
00:17:36,900 --> 00:17:41,242
ended up adopting a communication and leadership

258
00:17:41,306 --> 00:17:45,386
role duality. So instead of just one leader with everyone reporting

259
00:17:45,418 --> 00:17:49,650
into him, there was Paul, the leader, who was trying to took ownership for everything.

260
00:17:49,800 --> 00:17:53,026
And then Zinab, who was the second pair of hands to help out

261
00:17:53,048 --> 00:17:56,702
in that first place as well. She ended up stepping into this kind of communication

262
00:17:56,766 --> 00:18:00,226
facilitator filter role. So she would take all the

263
00:18:00,248 --> 00:18:03,158
information and pass it to Paul and filter it down for him to be able

264
00:18:03,164 --> 00:18:06,706
to make the decisions and calls that he needed to do. So that filtering

265
00:18:06,738 --> 00:18:09,974
of information actually allowed him to actually take more ownership

266
00:18:10,022 --> 00:18:13,450
and understand what was going on. Four minutes

267
00:18:13,520 --> 00:18:17,526
after that, they realized that Pete fly

268
00:18:17,558 --> 00:18:21,046
on the wall was not being very helpful and kind of just sitting

269
00:18:21,078 --> 00:18:25,246
there. And once they put

270
00:18:25,268 --> 00:18:28,766
him in a bit of interrogation, they realized that he knew what was going on

271
00:18:28,788 --> 00:18:32,366
and really wasn't there to help, and they kicked him out just

272
00:18:32,468 --> 00:18:36,162
more than fair enough. Seven minutes after

273
00:18:36,216 --> 00:18:39,838
that, I actually managed to break out of AWS.

274
00:18:39,934 --> 00:18:43,326
So I found one of our engineers GitHub credentials sitting

275
00:18:43,358 --> 00:18:46,726
there in parameter store. So naturally I start using those

276
00:18:46,748 --> 00:18:50,406
credentials and I create a lot of private repos with funny names under his

277
00:18:50,428 --> 00:18:54,614
GitHub account as well. And this

278
00:18:54,652 --> 00:18:57,240
just came down to, for me,

279
00:18:57,690 --> 00:19:01,186
an always interesting bit that one of the core components of AWS that people don't

280
00:19:01,218 --> 00:19:04,710
talk about enough, I feel like people talk about more than they used to.

281
00:19:04,860 --> 00:19:08,600
Is KMS now what it turned out

282
00:19:09,370 --> 00:19:11,486
in the end, the day when we went back and I had a chat to

283
00:19:11,508 --> 00:19:14,366
the engineer whose GitHub credentials I got, he thought he'd done the right thing by

284
00:19:14,388 --> 00:19:18,218
putting in a parameter store and he had encrypted it with kms. He just hadn't

285
00:19:18,234 --> 00:19:22,160
set up the KMS key properly, so it was open to any principle within the,

286
00:19:23,250 --> 00:19:27,262
you know, there's always these things know learnings.

287
00:19:27,326 --> 00:19:30,994
And for me, KMS is one of the

288
00:19:31,032 --> 00:19:34,766
fundamental services within AWS, or similarly in GCP

289
00:19:34,798 --> 00:19:38,354
or Azure that you have to get really comfortable and really,

290
00:19:38,392 --> 00:19:41,894
really good with, because it is such a key part of actually able

291
00:19:41,932 --> 00:19:45,398
to protect within an account or a subscription or a project, depending on

292
00:19:45,404 --> 00:19:46,840
your cloud of choice. Right?

293
00:19:48,730 --> 00:19:52,006
So at 1050, which was 50 minutes after we started all silent

294
00:19:52,038 --> 00:19:55,274
on the western front, they realized that potentially seeing

295
00:19:55,312 --> 00:19:59,226
as this was something happening internally, maybe conversing on

296
00:19:59,248 --> 00:20:02,894
Slack, where we could also see it, as in we, as in the red team,

297
00:20:03,012 --> 00:20:06,750
probably wasn't the best of ideas on their behalf.

298
00:20:07,090 --> 00:20:10,602
So instead they moved to a Google group, if memory serves,

299
00:20:10,666 --> 00:20:13,326
and started chatting there instead so they could cut us out of loop. So we

300
00:20:13,348 --> 00:20:16,914
didn't know what was going on, so naturally we still

301
00:20:16,952 --> 00:20:20,674
wanted to have eyes and ears in the room. So we cloud

302
00:20:20,712 --> 00:20:24,286
understand what was going on. So we sent them reinforcements, which were the COO

303
00:20:24,318 --> 00:20:27,426
and the CTO of the company. They weren't going to be kicking them out of

304
00:20:27,448 --> 00:20:31,126
the room. And also they wanted to get in there and

305
00:20:31,228 --> 00:20:34,886
help Paul and Zina about who've been taking on the majority of the workload and

306
00:20:34,908 --> 00:20:38,906
the stress, and just try and take some of the stress off and

307
00:20:38,928 --> 00:20:41,820
just make sure that, again, we weren't pushing people too hard.

308
00:20:42,910 --> 00:20:45,130
1057, the false contain.

309
00:20:46,910 --> 00:20:50,346
The blue team thought they got us they asked the

310
00:20:50,368 --> 00:20:53,546
CTO and co asked, do you have it under control? Like, yeah, we think we've

311
00:20:53,578 --> 00:20:56,254
got them out of the systems. No,

312
00:20:56,372 --> 00:20:59,662
they had not. It's about 18 minutes

313
00:20:59,716 --> 00:21:02,830
after that, we got more brazen in what we were doing,

314
00:21:02,900 --> 00:21:05,534
kind of putting stuff right in their faces where we knew they were looking,

315
00:21:05,572 --> 00:21:09,566
just to realize, no, we were still in. We still had accounts that we're accessing

316
00:21:09,598 --> 00:21:13,522
and all that kind of stuff. Five minutes after

317
00:21:13,576 --> 00:21:17,294
that, we did send them a photo on slack of us, of our faces,

318
00:21:17,342 --> 00:21:20,930
which, as you would imagine, copped quite a bit of written abuse,

319
00:21:21,010 --> 00:21:24,614
all in good taste and all in good fun. And eight minutes after

320
00:21:24,652 --> 00:21:28,006
that, they did actually manage to contain us. So it was about an

321
00:21:28,028 --> 00:21:31,800
hour and a half to get the full contino and we lost access to everything.

322
00:21:32,730 --> 00:21:35,686
A little bit after that, we took our sweet time. We were just next door

323
00:21:35,718 --> 00:21:38,666
in a coffee shop. We took a sweet time going back to the office and

324
00:21:38,688 --> 00:21:42,466
we walked in and proceeded to be. There were hand gestures

325
00:21:42,598 --> 00:21:46,126
made at us, AWS. We walked back in the door, which quite

326
00:21:46,228 --> 00:21:49,694
a thrill being having that done to you by 25

327
00:21:49,732 --> 00:21:53,390
people all at once. But we came back because

328
00:21:53,460 --> 00:21:56,550
it was time to move from containment to remediation.

329
00:21:56,730 --> 00:22:00,050
So now they had locked us out and we wanted to actually

330
00:22:00,120 --> 00:22:04,610
help with the cleanup. Right. We'd done things in lots of places. We wanted

331
00:22:04,680 --> 00:22:08,338
to help the blue team find what we'd done to a large degree and make

332
00:22:08,344 --> 00:22:11,714
sure that they were cleaning things up and not necessarily

333
00:22:11,762 --> 00:22:15,206
telling them where everything was, but giving them leads and clues and all that

334
00:22:15,228 --> 00:22:16,840
kind of stuff so they could figure out.

335
00:22:18,250 --> 00:22:21,990
So, Kaizen, change for good, continuous improvement,

336
00:22:22,730 --> 00:22:25,610
all that good stuff from lean theory.

337
00:22:26,030 --> 00:22:29,738
And this was kind of what the second half of the day was based around

338
00:22:29,824 --> 00:22:33,686
is how do we understand and reflect on

339
00:22:33,728 --> 00:22:37,646
the experience of what that morning was, realize where

340
00:22:37,668 --> 00:22:40,846
our problems were, where our gaps were, where we did really well, where we

341
00:22:40,868 --> 00:22:43,860
didn't do so well, the opportunities for improvement, all that kind of stuff.

342
00:22:44,710 --> 00:22:48,770
So scores on the doors. Time to identify

343
00:22:49,110 --> 00:22:53,026
was twelve minutes from them to go from

344
00:22:53,208 --> 00:22:56,894
us starting to do things to actually calling that an incident

345
00:22:56,942 --> 00:23:00,466
was happening a few minutes earlier. They did kind of get a sniff

346
00:23:00,498 --> 00:23:04,166
of it and realized something was going on, but it was twelve minutes. Time to

347
00:23:04,188 --> 00:23:07,734
contain was an hour and 28 minutes.

348
00:23:07,932 --> 00:23:11,626
So an hour and a half to get us from into

349
00:23:11,808 --> 00:23:15,974
locked out of all systems. Percentage of intrusion

350
00:23:16,022 --> 00:23:19,526
detected. So they caught about two thirds and we ran

351
00:23:19,558 --> 00:23:22,646
up a tally. As the red team, we were making notes

352
00:23:22,678 --> 00:23:25,422
of every single thing we did in the private slack channel. Just to make sure

353
00:23:25,476 --> 00:23:28,000
that when we stepped back through, we could find everything.

354
00:23:29,250 --> 00:23:32,830
One of the interesting things, one of the interesting questions

355
00:23:32,900 --> 00:23:36,494
we got as we were going through this process was, oh,

356
00:23:36,532 --> 00:23:40,066
but that wasn't a realistic scenario, which was an interesting question to get when

357
00:23:40,088 --> 00:23:43,730
we'd spent a lot of time thinking about it, making sure that

358
00:23:43,800 --> 00:23:45,800
we tried to make it as realistic as possible.

359
00:23:46,650 --> 00:23:50,790
The initial breach was one person's set of credentials,

360
00:23:51,530 --> 00:23:53,480
and it just kind of went from there,

361
00:23:55,210 --> 00:23:59,218
I think a lot of the time. Sometimes I think maybe it's

362
00:23:59,234 --> 00:24:03,574
getting a little bit better. People realize that security problems and breaches

363
00:24:03,622 --> 00:24:07,226
are a matter of when, not if, but just making it

364
00:24:07,248 --> 00:24:10,666
so people. It felt real for people for a little bit, which I think was

365
00:24:10,688 --> 00:24:13,694
important that it did. And then we could talk about,

366
00:24:13,732 --> 00:24:16,414
no, this was a perfectly realistic. This could well happen to us.

367
00:24:16,452 --> 00:24:19,818
Right? We always come to backups, never fail.

368
00:24:19,994 --> 00:24:23,426
Restores do like having these processes and

369
00:24:23,448 --> 00:24:27,154
everything else. If you've got this security process and approach and

370
00:24:27,192 --> 00:24:30,450
everything else that you think is valuable,

371
00:24:31,030 --> 00:24:34,654
test it. If you're not testing it with realistic scenarios,

372
00:24:34,702 --> 00:24:37,766
then you don't have a process at all. Right? The same as

373
00:24:37,788 --> 00:24:40,418
if you take backups, but you never restore them. You don't really have backups,

374
00:24:40,434 --> 00:24:43,974
do you? This was

375
00:24:44,012 --> 00:24:47,766
something that I'd been reading about at the time we

376
00:24:47,788 --> 00:24:51,974
did this, and I just thought was it helped

377
00:24:52,102 --> 00:24:55,434
me reason about why, as the

378
00:24:55,472 --> 00:24:59,606
red team, we felt we were one step ahead continually to the blue

379
00:24:59,638 --> 00:25:03,354
team, like, yeah, they did catch an hour and a half, but to

380
00:25:03,392 --> 00:25:06,478
a fair degree, we let them catch us again. We didn't want

381
00:25:06,484 --> 00:25:09,306
to make it too hard. We didn't want to spend all day with them chasing

382
00:25:09,338 --> 00:25:13,006
us. Right. There are diminishing returns to these things. And the loop

383
00:25:13,038 --> 00:25:16,014
comes from seven second John Boyd,

384
00:25:16,062 --> 00:25:19,394
who was a fighter pilot and trainee and

385
00:25:19,432 --> 00:25:23,330
trainer and effectively had this

386
00:25:23,400 --> 00:25:26,914
loop that used to describe how he was able to beat people in dogfights,

387
00:25:26,962 --> 00:25:29,400
and he was nigh unbeatable. Right?

388
00:25:30,410 --> 00:25:33,922
And the idea is this loop you go through, observe, orient,

389
00:25:33,986 --> 00:25:37,730
decide, and act. The ooda loop, first you observe,

390
00:25:37,810 --> 00:25:41,226
then you orient, then you decide, then you act. And what we

391
00:25:41,248 --> 00:25:44,298
found, and the idea with this loop is,

392
00:25:44,384 --> 00:25:48,406
the faster you go through the loop, the more you can outmaneuver

393
00:25:48,438 --> 00:25:51,260
and outperform the person you're against.

394
00:25:52,510 --> 00:25:56,270
And with our observability, especially on a blue team perspective,

395
00:25:56,770 --> 00:26:00,046
what we found was the blue team just didn't have a good idea of what

396
00:26:00,068 --> 00:26:03,694
was going on. They couldn't see what was going on. They couldn't find what was

397
00:26:03,732 --> 00:26:06,740
going on everything was very manual for them to find things,

398
00:26:07,190 --> 00:26:10,626
and there was just a lack of tooling that

399
00:26:10,648 --> 00:26:13,762
we had. They didn't have the right tools to be able to fight back,

400
00:26:13,896 --> 00:26:17,046
because as the red team, we kind of knew where they were, and we were

401
00:26:17,068 --> 00:26:20,338
running out ahead, and we were dictating the pace

402
00:26:20,514 --> 00:26:23,766
of everything, and we could create things faster than

403
00:26:23,788 --> 00:26:27,094
they could find things. And at that point, it's just

404
00:26:27,212 --> 00:26:30,842
an exponential curve where you outrun. So this was

405
00:26:30,896 --> 00:26:34,166
a definite thing. And what was the painful

406
00:26:34,198 --> 00:26:37,466
bit was that this lack of tooling, we had tooling on

407
00:26:37,488 --> 00:26:41,686
random people's laptops from client engagements for visibility pieces

408
00:26:41,718 --> 00:26:44,894
and other bits and pieces that we need, but just never

409
00:26:44,932 --> 00:26:48,266
been put back into common repositories. It wasn't shared.

410
00:26:48,298 --> 00:26:50,894
It was talked about after the fact that, oh, well, I've got something that does

411
00:26:50,932 --> 00:26:54,478
that. Well, why not all those kind of things again? It's one of

412
00:26:54,484 --> 00:26:57,858
those things that sometimes you need this catalyst, this catalyst give

413
00:26:57,864 --> 00:27:01,540
you a bias to action and to find where these holes are.

414
00:27:02,150 --> 00:27:05,842
One of the things that we definitely did do was every idea,

415
00:27:05,976 --> 00:27:08,790
every gap, everything that we came up with through this process,

416
00:27:08,860 --> 00:27:12,434
we captured them, and naturally, they became Jira tickets in the backlog,

417
00:27:12,482 --> 00:27:16,470
because how else do you capture your best intentions and best wishes?

418
00:27:18,830 --> 00:27:22,300
But our bench capacity going forward,

419
00:27:22,670 --> 00:27:26,154
it was like a rite of passage to pick stuff off this backlog and work

420
00:27:26,192 --> 00:27:30,186
against it, because the story became a

421
00:27:30,208 --> 00:27:33,342
myth, a legend. I haven't worked at this company in three

422
00:27:33,396 --> 00:27:36,862
years, yet. If I walk in the door, people who I've never met before,

423
00:27:36,916 --> 00:27:40,526
new employees, know who I am and know what I did on that

424
00:27:40,548 --> 00:27:44,606
day. It's an interesting legacy

425
00:27:44,638 --> 00:27:48,046
to have. And, yeah, it became the beginning

426
00:27:48,078 --> 00:27:51,774
of a tradition from there. From there, we ran an internal

427
00:27:51,822 --> 00:27:54,210
CTF. We ran public ctfs.

428
00:27:54,970 --> 00:27:58,390
We did more of these security game days,

429
00:27:58,460 --> 00:28:02,278
workshops, red team events. It became something

430
00:28:02,364 --> 00:28:05,814
of a tradition that

431
00:28:05,852 --> 00:28:09,094
is really, still really strongly cherished at

432
00:28:09,132 --> 00:28:12,266
that company. It was something that really brought security up

433
00:28:12,288 --> 00:28:15,894
to a first class citizen, up to part a thread

434
00:28:15,942 --> 00:28:18,730
in the weave of the culture of the company.

435
00:28:18,800 --> 00:28:21,200
That's just never going to go away now.

436
00:28:22,450 --> 00:28:25,918
And I'll just leave you with a final

437
00:28:26,084 --> 00:28:29,200
thought, which is, why, red team,

438
00:28:29,810 --> 00:28:33,066
why do this? And I'm going to just dip

439
00:28:33,098 --> 00:28:36,386
into a very quick chess analogy, which is

440
00:28:36,408 --> 00:28:39,774
the difference between a novice and a master. Novice chess

441
00:28:39,822 --> 00:28:43,314
players look at the board in pieces. They see

442
00:28:43,352 --> 00:28:46,920
every piece individually, and they have to hold all that in their head.

443
00:28:47,370 --> 00:28:51,366
The master looks at the board and sees patterns, things they've seen before.

444
00:28:51,548 --> 00:28:54,758
That's how they're able to move and understand

445
00:28:54,844 --> 00:28:58,118
how to go and be as good as they are

446
00:28:58,204 --> 00:29:01,670
and be 100,000

447
00:29:01,740 --> 00:29:04,898
times better than a novice. Right? Like a master chess player

448
00:29:04,914 --> 00:29:07,990
can play a novice a thousand times and not lose.

449
00:29:08,410 --> 00:29:11,614
And really, when it comes down to, if I was in the trenches with someone

450
00:29:11,652 --> 00:29:15,534
during a real security incident, would I rather have a Novitz next

451
00:29:15,572 --> 00:29:18,890
to me or a master? The choice is yours.

