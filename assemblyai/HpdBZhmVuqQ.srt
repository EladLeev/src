1
00:00:34,690 --> 00:00:38,294
Hello, my name is Andre and today I'm going to tell you about two

2
00:00:38,332 --> 00:00:41,926
different approaches of managing Kubernetes clusters. About these

3
00:00:41,948 --> 00:00:45,542
OpenStack Magnum and about the cluster API. But before

4
00:00:45,596 --> 00:00:48,886
that, let me introduce myself and introduce my

5
00:00:48,908 --> 00:00:52,222
company. So name is Andrei Novoselov. I work

6
00:00:52,276 --> 00:00:55,594
at these Gcore company and I'm a system engineer

7
00:00:55,642 --> 00:01:00,218
team lead. Juicor has a lots of products such as can

8
00:01:00,314 --> 00:01:03,694
DNS cloud and a lot of more.

9
00:01:03,812 --> 00:01:07,220
But I work at the Gcore cloud. So I'll tell you about the Gcore cloud.

10
00:01:08,470 --> 00:01:12,034
We have more than 20 locations around the globe where we

11
00:01:12,072 --> 00:01:16,040
are provided the public cloud service. So users can

12
00:01:16,810 --> 00:01:20,278
use some basic cloud services such as

13
00:01:20,444 --> 00:01:22,870
virtual machines, load balancers,

14
00:01:23,210 --> 00:01:28,182
bare metal servers and firewalls,

15
00:01:28,246 --> 00:01:32,138
or more complicated platform services such as

16
00:01:32,224 --> 00:01:35,738
managed kubernetes as a service, function as a

17
00:01:35,744 --> 00:01:39,130
service, logging as a service, basically anything

18
00:01:39,200 --> 00:01:42,698
as a service is in my team's responsibility zone.

19
00:01:42,874 --> 00:01:46,986
But today we'll talk about the managed kubernetes as a service and about the OpenStack

20
00:01:47,018 --> 00:01:50,874
Magnum and the cluster API. And we'll start with OpenStack

21
00:01:50,922 --> 00:01:53,940
Magnum. So what does Magnum do?

22
00:01:54,310 --> 00:01:58,206
It orchestrates container clusters and it supports

23
00:01:58,318 --> 00:02:01,934
two technologies, dockers form and kubernetes.

24
00:02:01,982 --> 00:02:05,826
And we'll talk about kubernetes. But Magnum itself cannot configure

25
00:02:05,858 --> 00:02:09,990
the virtual machines and it uses another Openstack

26
00:02:11,210 --> 00:02:15,346
component which is called OpenStack heat. And heat

27
00:02:15,378 --> 00:02:19,398
creates cloud init config for the Kubernetes nodes. And it also updates

28
00:02:19,494 --> 00:02:23,530
application versions and configuration on the Kubernetes node.

29
00:02:24,110 --> 00:02:27,450
So let's take a look at the Magnum architecture.

30
00:02:27,790 --> 00:02:31,866
First of all, Magnum has an API and it's an HTTP

31
00:02:31,898 --> 00:02:35,434
API. It gets some requests,

32
00:02:35,482 --> 00:02:39,470
and basically all those requests are some tasks and tasks.

33
00:02:40,690 --> 00:02:44,462
For example, create a cluster, update these cluster, delete the cluster.

34
00:02:44,606 --> 00:02:47,758
So Magnum API puts those tasks into the Rabbitm queue,

35
00:02:47,854 --> 00:02:51,778
and Magnum conductor gets those tasks from the rabbitm queue and

36
00:02:51,864 --> 00:02:55,662
makes them done. And it's a pretty common architecture

37
00:02:55,726 --> 00:02:58,974
for the OpenStack service. So an API

38
00:02:59,022 --> 00:03:03,000
RabbitMQ and some engine and

39
00:03:03,530 --> 00:03:07,506
heat is pretty same. It also has heat API

40
00:03:07,698 --> 00:03:11,674
RabbitMQ to handle the tasks from the

41
00:03:11,872 --> 00:03:14,620
API to the engine and heat engine.

42
00:03:16,270 --> 00:03:20,394
It also has a heat agent. Heat agent runs

43
00:03:20,442 --> 00:03:23,338
on every virtual machine configured with heat,

44
00:03:23,514 --> 00:03:26,698
and it updates

45
00:03:26,794 --> 00:03:29,966
the application version and application configuration if

46
00:03:29,988 --> 00:03:33,106
needed. So what's the

47
00:03:33,128 --> 00:03:36,370
limitations of this approach? Well,

48
00:03:36,440 --> 00:03:39,746
first of all, there is no control plane isolation from the

49
00:03:39,768 --> 00:03:43,570
user. So the user gets the full

50
00:03:43,640 --> 00:03:47,302
admin access to the Kubernetes cluster and he can do whatever

51
00:03:47,356 --> 00:03:51,480
he wants with the control plane components. And that's not how you

52
00:03:51,930 --> 00:03:55,554
do the managed Kubernetes service. And there's

53
00:03:55,682 --> 00:03:59,286
one more minor thing. It's an OpenStack API and

54
00:03:59,308 --> 00:04:02,506
we are not providing users access to the OpenStack API. So we

55
00:04:02,528 --> 00:04:06,202
have to hide this API behind the Gcore cloud

56
00:04:06,256 --> 00:04:10,570
API. But it's not a big deal. Let's talk about the control plane isolation.

57
00:04:12,210 --> 00:04:15,706
Here's a scheme of Gcore Magnum based managed

58
00:04:15,738 --> 00:04:18,960
Kubernetes service architecture on the cluster level.

59
00:04:19,410 --> 00:04:22,962
So managed Kubernetes cluster is always inside

60
00:04:23,016 --> 00:04:27,154
the client private network and client can select which

61
00:04:27,192 --> 00:04:31,166
networks he prefers. And let's

62
00:04:31,198 --> 00:04:34,754
talk about the control plane. Control plane is free

63
00:04:34,792 --> 00:04:38,454
virtual machines. And on every virtual machines we have

64
00:04:38,652 --> 00:04:42,566
all the control plane components, Kubernetes, control plane components such

65
00:04:42,588 --> 00:04:46,658
as EDCD, Kubernetes API control, managed Kedola,

66
00:04:46,834 --> 00:04:50,186
kubedns and so on and so forth. All of

67
00:04:50,208 --> 00:04:54,378
them are Portman containers controlled by the system

68
00:04:54,464 --> 00:04:55,100
D,

69
00:04:57,870 --> 00:05:01,402
etCD, Kubernetes and control and

70
00:05:01,456 --> 00:05:04,954
Kube API have these ports

71
00:05:05,002 --> 00:05:08,526
exposed outside of these virtual machines. And we

72
00:05:08,548 --> 00:05:12,358
have load balancers. So there's free virtual machines, free TCD

73
00:05:12,474 --> 00:05:16,238
replica, free Kube APIs and we have cloud load

74
00:05:16,254 --> 00:05:20,034
balancers to

75
00:05:20,072 --> 00:05:24,242
hide all these replicas behind these load balancers. And we also

76
00:05:24,296 --> 00:05:28,198
have a firewall which only allows access to

77
00:05:28,364 --> 00:05:31,000
those free exposed services.

78
00:05:32,810 --> 00:05:36,166
And none of that is visible for

79
00:05:36,188 --> 00:05:39,878
the client. So client cannot see the firewall,

80
00:05:39,974 --> 00:05:43,606
the control plane nodes and the load

81
00:05:43,638 --> 00:05:47,482
balancers. Client can have as much

82
00:05:47,536 --> 00:05:51,162
worker nodes as he want. And on the worker node,

83
00:05:51,226 --> 00:05:54,682
Kubernetes is also inside the podman container

84
00:05:54,746 --> 00:05:56,960
which is controlled by the system D.

85
00:05:57,650 --> 00:06:01,470
And for the Kubernetes workload,

86
00:06:01,810 --> 00:06:04,740
the container engine is docker itself.

87
00:06:05,830 --> 00:06:09,538
And if kubernetes or any other

88
00:06:09,624 --> 00:06:12,814
pod inside the kubernetes need to access Kube API,

89
00:06:12,862 --> 00:06:15,242
etcd or kubedns,

90
00:06:15,326 --> 00:06:19,314
well, you have to go through the cloud load balancers

91
00:06:19,362 --> 00:06:23,394
and through the firewall inside the master nodes,

92
00:06:23,442 --> 00:06:25,350
inside the control plane nodes.

93
00:06:26,410 --> 00:06:28,380
And that's it.

94
00:06:30,910 --> 00:06:35,530
And what's the pros and cons of OpenStack Magnum?

95
00:06:36,110 --> 00:06:39,450
Well, the big plus is that it is

96
00:06:39,520 --> 00:06:43,566
OpenStack and it has a great community which

97
00:06:43,588 --> 00:06:46,894
is developing Magnum and supports it and adds new

98
00:06:46,932 --> 00:06:49,520
features and fixes bugs for us.

99
00:06:50,370 --> 00:06:53,442
And I guess that's it.

100
00:06:53,496 --> 00:06:57,266
That's the biggest plus. And let's take a look

101
00:06:57,368 --> 00:07:00,354
at the downsides. Well, first of all,

102
00:07:00,472 --> 00:07:03,886
it is extra RPS for the cloud API.

103
00:07:03,998 --> 00:07:07,358
Like I said, we had to hide the

104
00:07:07,384 --> 00:07:10,822
Magnum behind and these heat APIs behind the cloud

105
00:07:10,876 --> 00:07:14,402
API. So now all the requests to those APIs

106
00:07:14,466 --> 00:07:17,898
go through the cloud API and that's a lot of requests if you have a

107
00:07:17,904 --> 00:07:20,890
lot of clusters, but that's not a big deal.

108
00:07:21,040 --> 00:07:25,114
Well, the second thing is that this

109
00:07:25,152 --> 00:07:28,986
constructor is really fragile. So what?

110
00:07:29,008 --> 00:07:32,926
I mean if something went wrong while the

111
00:07:32,948 --> 00:07:35,710
cluster was creating or updating,

112
00:07:36,290 --> 00:07:39,840
the Magnum cluster goes to the error state

113
00:07:40,370 --> 00:07:44,038
and there is no way using the OpenStack

114
00:07:44,074 --> 00:07:47,490
Cli to make it alive again. So basically

115
00:07:47,560 --> 00:07:50,482
the debug looks like this. You have to find the reason.

116
00:07:50,616 --> 00:07:54,478
For example, OpenStack could not create a load balancer

117
00:07:54,494 --> 00:07:57,638
for the ETCD or for the cloud API while was creating these

118
00:07:57,644 --> 00:08:01,094
cluster. So cluster went to the

119
00:08:01,132 --> 00:08:05,426
failed state while it was creating. So let's

120
00:08:05,458 --> 00:08:08,866
say you fix it. The original

121
00:08:08,898 --> 00:08:12,650
reason, I don't know, you restarted the Octavia. Maybe the problem

122
00:08:12,720 --> 00:08:16,330
was in the rabbitm queue, but now it's fixed and

123
00:08:16,400 --> 00:08:20,060
load balancing can be created and now you have to

124
00:08:20,430 --> 00:08:24,010
log in into the production MariaDB and update

125
00:08:24,090 --> 00:08:26,750
some roles for this cluster and set.

126
00:08:26,820 --> 00:08:29,930
Like now this cluster is active,

127
00:08:30,010 --> 00:08:33,966
not in an error state, and the same for the heat.

128
00:08:34,078 --> 00:08:37,518
And if it's

129
00:08:37,534 --> 00:08:40,702
just one line for the cluster inside the mammum DB,

130
00:08:40,846 --> 00:08:45,010
the heat has a little complicated

131
00:08:45,670 --> 00:08:49,234
structure of heat templates and heat stacks and

132
00:08:49,352 --> 00:08:53,318
you have to find these gained stack and update it

133
00:08:53,404 --> 00:08:57,422
well. And it's a lot of update operations on the production database,

134
00:08:57,506 --> 00:09:01,402
which is not what you want to do

135
00:09:01,536 --> 00:09:02,970
on daily basis.

136
00:09:04,670 --> 00:09:07,734
And like I said, if Octavia

137
00:09:07,782 --> 00:09:11,390
was in some error state

138
00:09:11,460 --> 00:09:14,990
or maybe rabbit MQ, it happens

139
00:09:15,060 --> 00:09:18,478
from time to time. So you have to not only

140
00:09:18,564 --> 00:09:22,078
to fix that OpenStack component, but also to fix

141
00:09:22,164 --> 00:09:25,726
the Magnum clusters if they were creating while Octavia

142
00:09:25,758 --> 00:09:28,500
or rabbit were not available.

143
00:09:30,950 --> 00:09:33,570
The other thing is observability.

144
00:09:34,630 --> 00:09:38,434
So let's say you want to know the state of all

145
00:09:38,472 --> 00:09:42,214
the Cube API containers in all your clusters. Well,

146
00:09:42,252 --> 00:09:46,262
you have to log in via Ssh to the control

147
00:09:46,316 --> 00:09:49,610
plane nodes and to find out is everything okay

148
00:09:49,680 --> 00:09:53,626
or not. Or you may create can

149
00:09:53,808 --> 00:09:57,334
Prometheus exporter if you're using Prometheus to monitor these clusters

150
00:09:57,462 --> 00:10:01,230
or do something with it, but from the box there's no

151
00:10:01,300 --> 00:10:05,022
such thing as health

152
00:10:05,076 --> 00:10:09,066
check for all the system D units on the master cluster.

153
00:10:09,098 --> 00:10:12,606
And if you want to be sure that everything is okay, you have to log

154
00:10:12,628 --> 00:10:16,506
in via Ssh to those nodes and there is no bare

155
00:10:16,538 --> 00:10:19,410
metal nodes. And that's a huge minus.

156
00:10:20,710 --> 00:10:24,210
And one more thing, here's the

157
00:10:24,280 --> 00:10:27,526
compatibility metrics. On the left column you

158
00:10:27,548 --> 00:10:31,158
can see the OpenStack version and

159
00:10:31,244 --> 00:10:34,882
on the second column you can see the supported Kubernetes

160
00:10:34,946 --> 00:10:38,522
version. So if you are using

161
00:10:38,656 --> 00:10:41,980
yoga, which is just one year old,

162
00:10:42,990 --> 00:10:47,578
the highest Kubernetes version for you is 1.23.

163
00:10:47,744 --> 00:10:51,254
And here's the Kubernetes supported

164
00:10:51,302 --> 00:10:55,166
versions for now, and it starts with 1.24

165
00:10:55,268 --> 00:10:58,794
and 1.25 and 1.26 is the highest version.

166
00:10:58,842 --> 00:11:02,734
But in April we'll have a release I hope, and this

167
00:11:02,772 --> 00:11:06,530
will change and the supported versions will be 1.25,

168
00:11:06,600 --> 00:11:10,158
1.26 and 1.27. So the OpenStack

169
00:11:10,254 --> 00:11:13,762
is about two Kubernetes version behind the

170
00:11:13,816 --> 00:11:17,640
Kubernetes. So I guess it's a big deal.

171
00:11:18,330 --> 00:11:21,880
And we did not wish to

172
00:11:25,610 --> 00:11:29,450
add support to those Kubernetes versions to the Magnum

173
00:11:29,870 --> 00:11:33,930
and we decided to take a look at

174
00:11:34,080 --> 00:11:37,740
other tools for

175
00:11:38,510 --> 00:11:41,626
managing the lifecycle of the Kubernetes cluster.

176
00:11:41,738 --> 00:11:45,642
And there's a project which is called Cluster API

177
00:11:45,706 --> 00:11:49,354
and it's a Kubernetes project. And what's the goal

178
00:11:49,402 --> 00:11:53,146
of this project? Well first of all it manages the lifecycle

179
00:11:53,338 --> 00:11:57,438
and by that I mean create, scale, upgrade and destroy all the Kubernetes cluster

180
00:11:57,534 --> 00:12:01,742
using a declarative API. It works in different environments,

181
00:12:01,806 --> 00:12:05,522
both on premises and in the cloud, and it defines common

182
00:12:05,576 --> 00:12:09,234
operations and provides the default implementations and provided the ability

183
00:12:09,282 --> 00:12:12,758
to swap out implementations for the alternative ones.

184
00:12:12,924 --> 00:12:16,534
So if you don't like it, if you don't like something in

185
00:12:16,572 --> 00:12:20,360
cluster API you can do it your way

186
00:12:20,890 --> 00:12:24,666
and cluster API can be extended to support

187
00:12:24,768 --> 00:12:27,580
any infrastructure and.

188
00:12:28,990 --> 00:12:32,942
Sounds great. So what

189
00:12:32,996 --> 00:12:36,666
is it made of? What are the main components of the cluster

190
00:12:36,698 --> 00:12:40,030
API? Well first of all it's a controller manager,

191
00:12:41,250 --> 00:12:45,250
a bootstrap controller manager, control plane controller manager and

192
00:12:45,320 --> 00:12:46,930
infrastructure provider.

193
00:12:51,590 --> 00:12:55,474
Let's talk about it a little. So cluster

194
00:12:55,522 --> 00:12:59,282
API, basically it's four Kubernetes

195
00:12:59,346 --> 00:13:02,598
controllers. So it runs inside the Kubernetes. So you need a

196
00:13:02,604 --> 00:13:05,762
Kubernetes cluster to create another Kubernetes cluster.

197
00:13:05,906 --> 00:13:10,294
And it operates with Kubernetes

198
00:13:10,342 --> 00:13:14,154
objects with the custom resources. So these

199
00:13:14,192 --> 00:13:17,910
controllers, these control those custom resources

200
00:13:17,990 --> 00:13:21,678
and reconciliate them and what

201
00:13:21,764 --> 00:13:25,646
do we have out of the box? Bootstrap control. Out of

202
00:13:25,668 --> 00:13:29,082
the box supports KubeadM microcades,

203
00:13:29,146 --> 00:13:32,426
Taylors and EKs control plane

204
00:13:32,458 --> 00:13:35,654
control. Out of the box supports KubeadM

205
00:13:35,802 --> 00:13:39,362
microcades, talos and the project called

206
00:13:39,416 --> 00:13:40,290
Nested.

207
00:13:41,990 --> 00:13:46,242
And there's a bunch of infrastructure controllers for

208
00:13:46,376 --> 00:13:50,534
AWS Asia Wesphere metal free.

209
00:13:50,732 --> 00:13:53,666
Well lots of, lot of, lot of providers,

210
00:13:53,698 --> 00:13:56,120
but there is no jquery provider yet.

211
00:13:58,030 --> 00:14:02,278
So let's

212
00:14:02,454 --> 00:14:06,826
talk a little bit more about how

213
00:14:06,848 --> 00:14:10,638
it works. So we

214
00:14:10,804 --> 00:14:14,986
have those binaries

215
00:14:15,178 --> 00:14:18,478
like controller manager, bootstrap controller manager and

216
00:14:18,484 --> 00:14:22,202
control plane controller manager. They handle the whole lifecycle

217
00:14:22,266 --> 00:14:27,170
of the Kubernetes cluster and they know nothing about the infrastructure

218
00:14:27,510 --> 00:14:31,550
behind them. So that's why you need an infrastructure provider.

219
00:14:31,710 --> 00:14:36,802
Infrastructure provided is a thing that allows

220
00:14:36,866 --> 00:14:42,514
cluster API to create some basic infrastructure

221
00:14:42,642 --> 00:14:45,510
objects such as load balancers,

222
00:14:45,930 --> 00:14:47,190
virtual machines,

223
00:14:47,950 --> 00:14:51,366
firewalls, server groups, et cetera,

224
00:14:51,398 --> 00:14:55,354
et cetera. And the cluster API says okay,

225
00:14:55,552 --> 00:14:59,514
if your infrastructure provided allows me to

226
00:14:59,552 --> 00:15:03,814
create a virtual machines, load balancers and firewall these,

227
00:15:03,952 --> 00:15:06,240
I can do anything on that cloud.

228
00:15:08,050 --> 00:15:12,590
So let's take a look at some yaml

229
00:15:13,670 --> 00:15:17,140
and let's take a look at the cluster object.

230
00:15:17,830 --> 00:15:21,374
And there's obviously some cluster

231
00:15:21,422 --> 00:15:25,334
configuration inside these pack. But I

232
00:15:25,372 --> 00:15:28,982
wanted to point your attention at the control plane ref

233
00:15:29,036 --> 00:15:32,710
and infrastructure ref. So we have a cluster and

234
00:15:32,780 --> 00:15:37,106
it has infrastructure reference to the GcoRe

235
00:15:37,138 --> 00:15:40,642
cluster object which is the implementation

236
00:15:40,706 --> 00:15:43,830
of this specific cluster on the Gcore provider.

237
00:15:43,990 --> 00:15:47,494
And if we take a look at the control plane we'll

238
00:15:47,542 --> 00:15:50,590
see pretty same thing.

239
00:15:50,740 --> 00:15:55,306
It says okay, we need free replicas of control plane

240
00:15:55,418 --> 00:15:59,230
nodes with some Kubernetes version and

241
00:15:59,380 --> 00:16:02,838
it has a reference to the infrastructure provider

242
00:16:03,034 --> 00:16:06,802
to the kind Gcore machine template. So we have

243
00:16:06,856 --> 00:16:10,334
a template for the Gcore specific infrastructure

244
00:16:10,382 --> 00:16:14,882
provider for the machine to create control

245
00:16:14,936 --> 00:16:18,786
plane node and we need three of them. So control plane has

246
00:16:18,888 --> 00:16:22,518
a reference to the Gcore machine template on

247
00:16:22,524 --> 00:16:26,166
these infrastructure and the same for the machine deployment. Machine deployment is an

248
00:16:26,188 --> 00:16:30,440
object which describes the worker group of

249
00:16:30,810 --> 00:16:34,602
some Kubernetes cluster. So this one says okay, we need six

250
00:16:34,656 --> 00:16:38,522
workers and to create them please

251
00:16:38,576 --> 00:16:42,462
use the infrastructure reference to the Jacob machine with this

252
00:16:42,516 --> 00:16:45,934
name and that's it. So what

253
00:16:45,972 --> 00:16:49,946
I tried to say that this is how cluster

254
00:16:49,978 --> 00:16:53,890
API works. It has some basic objects

255
00:16:56,070 --> 00:17:00,638
which do not change from one cloud provider to another, and the cloud provider

256
00:17:00,734 --> 00:17:03,390
will always give an infrastructure reference.

257
00:17:03,550 --> 00:17:06,898
Infrastructure provider, my bad. So cluster

258
00:17:06,994 --> 00:17:10,306
is always referred to the Gcore cluster. Control plane

259
00:17:10,338 --> 00:17:14,150
and machine deployment. Both are created from the Gcore machine templates.

260
00:17:14,730 --> 00:17:18,266
And one more thing, meshing. Meshing is an

261
00:17:18,288 --> 00:17:22,794
object which describes control

262
00:17:22,912 --> 00:17:26,490
plane node or virtual machine or

263
00:17:26,640 --> 00:17:29,930
worker virtual machine or bare metal machine.

264
00:17:31,250 --> 00:17:35,006
So what are the limitations of the

265
00:17:35,028 --> 00:17:38,398
cluster API? Out of the box we have

266
00:17:38,484 --> 00:17:42,286
Kubeadm bootstrap provider which is more

267
00:17:42,308 --> 00:17:45,646
or less suitable for us, cube ADM control plane

268
00:17:45,678 --> 00:17:48,994
provider. So that basically means that the

269
00:17:49,032 --> 00:17:52,894
Kubeadm will be used to bootstrap the control plane

270
00:17:52,942 --> 00:17:56,210
and to join workers to that control plane,

271
00:17:56,710 --> 00:18:00,174
and has a provider for the cluster wrapping

272
00:18:00,222 --> 00:18:03,894
which uses Kubeadm and there is no infrastructure provider for

273
00:18:03,932 --> 00:18:07,622
the Gcore infrastructure. And what else

274
00:18:07,676 --> 00:18:11,222
limitations can we see? There's also no control plane isolation

275
00:18:11,286 --> 00:18:14,986
from the user. You need a Kubernetes cluster to

276
00:18:15,008 --> 00:18:18,314
create another Kubernetes cluster and there is no

277
00:18:18,352 --> 00:18:22,286
jiggle provider. And we still have free virtual machines for

278
00:18:22,308 --> 00:18:24,510
the control plane if we use Kubeadm.

279
00:18:25,250 --> 00:18:28,734
So we decided to do it our way

280
00:18:28,852 --> 00:18:33,070
and this is our implementation of these cluster API.

281
00:18:33,670 --> 00:18:37,518
We decided not to use free virtual machines

282
00:18:37,614 --> 00:18:40,210
for the control plane.

283
00:18:41,990 --> 00:18:45,522
We already have a Kubernetes cluster to handle

284
00:18:45,586 --> 00:18:48,854
cluster API and we decided to

285
00:18:48,892 --> 00:18:53,234
put the control plane containers

286
00:18:53,362 --> 00:18:56,934
into these cluster as well. So we have

287
00:18:56,972 --> 00:19:00,026
a namespace for each client cluster where

288
00:19:00,048 --> 00:19:04,490
we have some custom resources describing cluster juicy cluster,

289
00:19:05,070 --> 00:19:08,522
some control planes, some machine deployments and

290
00:19:08,576 --> 00:19:12,318
we also have the control plane pods inside

291
00:19:12,404 --> 00:19:16,186
it. So the control plane components

292
00:19:16,218 --> 00:19:19,902
are pods inside the service Kubernetes cluster and

293
00:19:19,956 --> 00:19:23,474
all the cluster API objects are at the same namespace as

294
00:19:23,512 --> 00:19:26,654
the control plane pods. And we have no virtual

295
00:19:26,702 --> 00:19:28,370
machines for the control plane.

296
00:19:29,830 --> 00:19:33,682
And so to do that we had to create

297
00:19:33,816 --> 00:19:37,378
Gcore bootstrap controller, Gcore control plane

298
00:19:37,394 --> 00:19:40,674
controller and infrastructure provider

299
00:19:40,722 --> 00:19:44,502
which is called CAPGC. And thanks for our

300
00:19:44,556 --> 00:19:48,460
colleagues from these working for help with that. Thank you guys.

301
00:19:50,350 --> 00:19:54,086
And we has to do two more things. We had to create open ven

302
00:19:54,118 --> 00:19:57,978
controller and we use Argo CD and we'll talk about it a

303
00:19:57,984 --> 00:20:01,834
little bit later. So let's

304
00:20:01,882 --> 00:20:06,538
take a look at the Gcore cluster API based managed Kate

305
00:20:06,634 --> 00:20:11,070
service architecture. So now

306
00:20:11,140 --> 00:20:14,754
you can see that there are some difference.

307
00:20:14,952 --> 00:20:18,706
In client private network there is

308
00:20:18,728 --> 00:20:23,090
no master nodes anymore, only worker nodes.

309
00:20:23,510 --> 00:20:27,346
And in Gcore private network we have a Gcore

310
00:20:27,378 --> 00:20:30,934
service Kubernetes cluster where in some namespace we

311
00:20:30,972 --> 00:20:35,030
keep all the custom

312
00:20:35,100 --> 00:20:38,906
resources of the cluster API such as like I

313
00:20:38,928 --> 00:20:42,602
said, cluster machine deployment, control plane and

314
00:20:42,656 --> 00:20:43,420
so on.

315
00:20:45,790 --> 00:20:49,686
And also we have control plane

316
00:20:49,718 --> 00:20:53,002
binaries such as ETCD, Kube API controller, manager,

317
00:20:53,066 --> 00:20:56,000
scheduler and some more.

318
00:20:56,690 --> 00:21:00,862
So since because of the

319
00:21:00,916 --> 00:21:04,910
service cluster is in some Jacob private cluster private network

320
00:21:04,990 --> 00:21:08,526
and the worker nodes are in the client's

321
00:21:08,558 --> 00:21:12,110
private network, there's no directly connectivity,

322
00:21:12,190 --> 00:21:16,242
network connectivity. So what did we do? We used a cloud

323
00:21:16,296 --> 00:21:20,310
load balancer to expose the Cube API to these public

324
00:21:20,380 --> 00:21:24,194
ip address, so all these components on the worker nodes

325
00:21:24,242 --> 00:21:28,410
can access the Kube API via the Internet.

326
00:21:29,550 --> 00:21:32,954
And that's it. That's simple. But what

327
00:21:32,992 --> 00:21:37,260
about the reverse connectivity? What if someone would like to do

328
00:21:38,110 --> 00:21:41,374
to type the command Kubectl logs? And what

329
00:21:41,412 --> 00:21:45,466
happens when you do that? Your Kubernetes API

330
00:21:45,578 --> 00:21:51,866
works as proxy to the Kubernetes

331
00:21:52,058 --> 00:21:56,130
and shows the log from the Kubelet so Kubelet can get the logs

332
00:21:56,870 --> 00:22:00,242
from the node. What if someone would like

333
00:22:00,296 --> 00:22:03,380
to use a port forward? It's pretty same.

334
00:22:03,910 --> 00:22:07,314
Kube API works as a proxy to

335
00:22:07,352 --> 00:22:10,946
some service in the Kubernetes. And Cube API

336
00:22:10,978 --> 00:22:14,822
has no way to access Kubelet because the Kubelet is inside

337
00:22:14,876 --> 00:22:19,226
the client private network and is

338
00:22:19,328 --> 00:22:24,154
not accessible. So and

339
00:22:24,192 --> 00:22:27,462
the admission web hooks. So what if Cube API

340
00:22:27,526 --> 00:22:31,354
would have to validate some custom resource before putting it to the ATCD?

341
00:22:31,402 --> 00:22:35,194
It will need to access some pod

342
00:22:35,242 --> 00:22:38,286
inside the client clusters and there's no way to do it.

343
00:22:38,468 --> 00:22:43,314
So VPN we

344
00:22:43,352 --> 00:22:46,180
decided to do it this way.

345
00:22:46,710 --> 00:22:50,290
On these client side, we put

346
00:22:50,360 --> 00:22:54,386
a pod with OpenVPN server and we expose it

347
00:22:54,488 --> 00:22:58,310
with cloud load balancer to the Internet

348
00:22:58,970 --> 00:23:03,542
in the control plane in

349
00:23:03,596 --> 00:23:07,318
Kube API port there are two containers. One of them is

350
00:23:07,404 --> 00:23:11,414
Kube API and the other one is these OpenVPN client.

351
00:23:11,542 --> 00:23:16,678
And the OpenVPN client connects to the OpenVPN

352
00:23:16,774 --> 00:23:20,474
server port through the cloud load balancer and

353
00:23:20,512 --> 00:23:26,534
it gets the routes to the node,

354
00:23:26,582 --> 00:23:30,862
network, port, network and service network. And now right

355
00:23:30,916 --> 00:23:34,830
inside the port of the Kube API, we have some routes

356
00:23:35,350 --> 00:23:39,186
to all those networks. And Kube API can access kubelets on

357
00:23:39,208 --> 00:23:43,170
the node networks, services and pods inside the Kubernetes cluster.

358
00:23:44,890 --> 00:23:48,194
But what if client deletes something? What if client deletes

359
00:23:48,242 --> 00:23:52,310
an openupian server Kaliko KQ proxy

360
00:23:52,890 --> 00:23:55,720
and we have an argo CD for that.

361
00:23:56,490 --> 00:23:59,946
So inside our service cluster we

362
00:23:59,968 --> 00:24:03,318
have an argo CD which have apps

363
00:24:03,414 --> 00:24:07,770
for all infrastructure which should be controlled

364
00:24:08,430 --> 00:24:12,730
by Jacob inside the client's

365
00:24:14,030 --> 00:24:18,062
worker nodes such as Kubeproxy, Kube DNS, Calico and

366
00:24:18,116 --> 00:24:21,070
OpenVPN service for the reverse connectivity.

367
00:24:21,410 --> 00:24:25,202
So argo CD manage

368
00:24:25,336 --> 00:24:28,686
these renders

369
00:24:28,718 --> 00:24:32,066
and manifests for all that infrastructure and puts them directly into

370
00:24:32,088 --> 00:24:35,590
the Kube API which is located inside

371
00:24:35,660 --> 00:24:39,938
the Gcore service cluster. And then the Kubelet accesses

372
00:24:40,034 --> 00:24:44,214
the Kube API and finds out what

373
00:24:44,252 --> 00:24:48,278
ports should be run on the node. So if clients deletes

374
00:24:48,374 --> 00:24:53,766
anything using the Kubectl,

375
00:24:53,958 --> 00:24:56,460
the Argo CD will recreate it.

376
00:24:58,190 --> 00:25:01,630
And one Gcore thing about the observability.

377
00:25:02,130 --> 00:25:06,446
So comparing to

378
00:25:06,468 --> 00:25:09,966
the OpenStack magnum, well, we could

379
00:25:09,988 --> 00:25:13,380
not find any suitable tool

380
00:25:13,990 --> 00:25:18,114
to find out. Is everything okay with cluster? We have it

381
00:25:18,232 --> 00:25:21,566
just out of the box. So we can use a command

382
00:25:21,598 --> 00:25:25,662
like Cubectl, get cluster minus a capital

383
00:25:25,726 --> 00:25:29,762
a and we'll get all the clusters which were provided,

384
00:25:29,826 --> 00:25:33,046
which were provisioned using the

385
00:25:33,068 --> 00:25:36,486
cluster API in our service cluster. So we can see that we

386
00:25:36,508 --> 00:25:40,022
have 1234 clusters and all of them are

387
00:25:40,076 --> 00:25:44,060
ready and these version is 1.24 point ten.

388
00:25:44,910 --> 00:25:48,170
We also can get information about all the

389
00:25:48,240 --> 00:25:51,582
control planes so the output is pretty same,

390
00:25:51,636 --> 00:25:55,358
but it tells us everything about the control planes, not about

391
00:25:55,444 --> 00:25:59,214
all costs. So we can see that all control planes are

392
00:25:59,252 --> 00:26:02,234
ready. So EDCD, Cube,

393
00:26:02,282 --> 00:26:05,794
API, Kubescadal, controller, manager, all of them are

394
00:26:05,832 --> 00:26:09,330
up and running. And here's the version of the control plane.

395
00:26:09,750 --> 00:26:13,646
We also can take a look at the worker

396
00:26:13,678 --> 00:26:16,946
pools which are called machine deployments in the cluster API

397
00:26:17,058 --> 00:26:20,770
and we can see that in this namespace,

398
00:26:20,850 --> 00:26:24,434
the first one there should be free replicas,

399
00:26:24,562 --> 00:26:28,134
so free worker nodes and all these are ready and all these are

400
00:26:28,172 --> 00:26:31,690
updated and they are running and they're two days old

401
00:26:31,760 --> 00:26:36,026
and the version is 1.24 point ten and

402
00:26:36,048 --> 00:26:40,302
we can get information about any cluster we're interested just with single

403
00:26:40,356 --> 00:26:44,702
Kubectl comment or about all of them and

404
00:26:44,836 --> 00:26:49,162
we can get a machine. So Kubectl get machine

405
00:26:49,226 --> 00:26:53,374
will minus capital a will bring us

406
00:26:53,492 --> 00:26:57,466
all the virtual machines or biometric

407
00:26:57,498 --> 00:27:00,690
machines which are controlled by cluster API inside

408
00:27:00,760 --> 00:27:04,722
this cluster and we even can see the

409
00:27:04,776 --> 00:27:08,902
Openstack id of these virtual machines and how long

410
00:27:08,956 --> 00:27:12,454
it exists and the Kubernetes version on

411
00:27:12,492 --> 00:27:15,400
it. So it's really great.

412
00:27:16,970 --> 00:27:18,600
And that's it.

413
00:27:20,570 --> 00:27:24,730
We moved from the cluster API to the Magnum and

414
00:27:24,800 --> 00:27:28,780
what we got from it we got a great speed up.

415
00:27:29,470 --> 00:27:33,520
All the control plane is just

416
00:27:34,130 --> 00:27:37,834
a bunch of ports and it's really much easier

417
00:27:37,882 --> 00:27:41,150
to spin up some ports than to create a virtual machines.

418
00:27:41,650 --> 00:27:45,486
We got the easy updates and that's much faster.

419
00:27:45,518 --> 00:27:48,562
Obviously we got these easy updates. We have

420
00:27:48,616 --> 00:27:52,562
1.24, 1.25 and 1.26

421
00:27:52,616 --> 00:27:58,422
version of the Kubernetes out of the box and

422
00:27:58,476 --> 00:28:02,002
we can update all the infrastructure inside the client's

423
00:28:02,066 --> 00:28:05,974
cluster with our Argo CD applications and

424
00:28:06,012 --> 00:28:09,458
it's really easy as well. We got the reconciliation loop.

425
00:28:09,554 --> 00:28:13,002
So if the heat tried to create for

426
00:28:13,056 --> 00:28:16,378
example a load balancer and failed, then you are the one who have

427
00:28:16,384 --> 00:28:19,674
to fix it. If cluster API tries to create

428
00:28:19,712 --> 00:28:23,390
a virtual machine, a load balancer or whatever, and fails,

429
00:28:23,970 --> 00:28:27,994
it just was a little and tries again and tries again and tries

430
00:28:28,042 --> 00:28:31,406
again until everything is done well

431
00:28:31,588 --> 00:28:35,114
and it's really great. So after you fixed

432
00:28:35,162 --> 00:28:38,658
your Octavia or Nova or RabbitmQ inside the

433
00:28:38,664 --> 00:28:41,826
OpenStack, you do not need to reconfigure all the

434
00:28:41,848 --> 00:28:45,862
cluster in the region. Cluster API will just do another try and

435
00:28:45,916 --> 00:28:48,840
succeed and we'll be happy with that.

436
00:28:49,290 --> 00:28:53,494
We got the biometal nodes with our

437
00:28:53,692 --> 00:28:57,814
powered by intel and it's a

438
00:28:57,852 --> 00:29:02,038
great feature and a lot of our clients wanted biometal worker

439
00:29:02,054 --> 00:29:06,470
nodes and now we have it because we created an infrastructure provider

440
00:29:06,550 --> 00:29:10,018
which can use biometal nodes as worker nodes

441
00:29:10,054 --> 00:29:14,346
for the Kubernetes we got that transparency.

442
00:29:14,458 --> 00:29:18,254
So if you want to look at these specific machine or

443
00:29:18,292 --> 00:29:21,950
at the machine deployment or at cluster using the

444
00:29:22,100 --> 00:29:24,590
Kube native way with the Kubectl,

445
00:29:25,090 --> 00:29:28,590
you got it. And we

446
00:29:28,660 --> 00:29:31,280
have no control plane nodes no more.

447
00:29:32,850 --> 00:29:36,786
So there's no extra cost for us for managing control plane

448
00:29:36,818 --> 00:29:40,694
nodes. And I guess

449
00:29:40,732 --> 00:29:44,150
that's it. Thank you for your attention and

450
00:29:44,300 --> 00:29:48,040
feel free to contact me if you have any questions.

