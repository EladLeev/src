1
00:00:23,050 --> 00:00:26,994
Hey folks, welcome to my talk about techniques for slos

2
00:00:27,042 --> 00:00:30,470
and air budgets at scale. I'm proud to be presenting here at

3
00:00:30,540 --> 00:00:34,006
Conf 42 observability in 2023. I've been talking a lot

4
00:00:34,028 --> 00:00:37,942
about slos and air budgets over the past six or so years, and over that

5
00:00:37,996 --> 00:00:41,462
time I've given variations of this talk in different

6
00:00:41,516 --> 00:00:45,378
online conferences and also a couple in person. So I'm

7
00:00:45,394 --> 00:00:48,518
pleased to be presenting some of my learnings over the past couple of years.

8
00:00:48,604 --> 00:00:51,994
This is basically the greatest its version of the previous talks I've

9
00:00:52,042 --> 00:00:55,566
given on the subject. Let's get started. But first, I know

10
00:00:55,588 --> 00:00:59,050
this is an online conference, but let's do a little survey.

11
00:00:59,130 --> 00:01:02,142
Raise your hand. I know you're sitting there just at home by yourself,

12
00:01:02,196 --> 00:01:05,518
but raise your hand if you know what this graph is. And this is one

13
00:01:05,524 --> 00:01:08,930
of those graphs that if you know, you know. We're all here today

14
00:01:09,000 --> 00:01:12,418
in part because of this thing. And if you don't know what this is,

15
00:01:12,504 --> 00:01:15,858
that's okay. We'll go ahead and come back to that a little bit later.

16
00:01:15,944 --> 00:01:19,170
So, hi, I'm Fred. I'm a observability

17
00:01:19,250 --> 00:01:23,542
engineer at a large public company, and this talk is

18
00:01:23,676 --> 00:01:27,458
my own opinions, not those my employers. Basic disclaimer

19
00:01:27,554 --> 00:01:31,590
and so I've been working on

20
00:01:31,740 --> 00:01:35,446
monitoring observability for about as long as the graph on the previous slide,

21
00:01:35,478 --> 00:01:38,538
but focusing on it heavily over the past ten or so years.

22
00:01:38,704 --> 00:01:42,138
I like to think about slos, Slis and air budgets,

23
00:01:42,154 --> 00:01:45,434
hence the SLOS Jason term, I think that was coined in the original

24
00:01:45,482 --> 00:01:48,560
Google SlO paper. I like to hack on

25
00:01:48,930 --> 00:01:52,394
histograms, metrics, logs and traces. Been programming

26
00:01:52,442 --> 00:01:56,498
a lot of stuff for the past 20 years, and I've got two young kids,

27
00:01:56,584 --> 00:01:59,746
so I definitely am in need of more sleep and coffee. But let's go

28
00:01:59,768 --> 00:02:03,250
ahead and kick this off. So how

29
00:02:03,320 --> 00:02:06,654
do you implement slos for 1000 plus engineers?

30
00:02:06,782 --> 00:02:10,774
And this was a challenge I encountered about four years

31
00:02:10,812 --> 00:02:14,146
ago when I started a role at a company called Zendesk.

32
00:02:14,178 --> 00:02:17,554
And I got tasked with a project to bring slos to an engineering

33
00:02:17,602 --> 00:02:21,446
organization that had over a thousand engineers, which was

34
00:02:21,548 --> 00:02:24,682
quite a few. And there was a big push to make the product

35
00:02:24,736 --> 00:02:28,614
as reliable as possible. We called reliability our number one feature.

36
00:02:28,742 --> 00:02:31,534
So I had to come up with a way to roll out slos and air

37
00:02:31,572 --> 00:02:35,662
budgets across all those engineers. And to

38
00:02:35,716 --> 00:02:39,454
do that effectively, I really had to understand what

39
00:02:39,572 --> 00:02:43,786
slis and slos, and hence air budgets were programmatically.

40
00:02:43,898 --> 00:02:47,266
So I really dove in and started to research the subject a lot to kind

41
00:02:47,288 --> 00:02:48,930
of go back to the basics.

42
00:02:50,470 --> 00:02:54,062
And speaking of basics, I started off by reading the original Google SRE

43
00:02:54,126 --> 00:02:57,666
book, followed that up with the SRE workbook,

44
00:02:57,858 --> 00:03:01,282
watched Liz Fong Jones and Seth Fargo's

45
00:03:01,346 --> 00:03:04,754
Google Cloud presentation on slos titled slis,

46
00:03:04,802 --> 00:03:08,182
slos, slos. Oh my. Which was an inspiration to me.

47
00:03:08,316 --> 00:03:11,782
And I've given a number of SLO talks previously, most notably

48
00:03:11,846 --> 00:03:15,706
one called Latency SLos done right, which was also given at

49
00:03:15,728 --> 00:03:19,094
Srecon by Theo Schlossnagel and Heinrich Hartman,

50
00:03:19,142 --> 00:03:22,922
who've written a lot on the subject. And even

51
00:03:22,976 --> 00:03:26,702
looking back at that talk I gave, I can spot the errors in it,

52
00:03:26,756 --> 00:03:30,074
which were kind of subtle. But what I found researching this topic

53
00:03:30,122 --> 00:03:33,850
here is there wasn't really a prescription for slis and slos.

54
00:03:34,010 --> 00:03:37,266
The Google books talked a lot about slis, but were vague on the

55
00:03:37,288 --> 00:03:41,058
subject as far as specific examples were concerned. And even working through some

56
00:03:41,064 --> 00:03:45,086
of the examples in the workbook, I either found subtle

57
00:03:45,118 --> 00:03:49,000
omissions or places where the examples weren't completely

58
00:03:49,370 --> 00:03:53,142
flushed out and tested. Liz and Seth's Google Cloud video

59
00:03:53,196 --> 00:03:56,626
had some concise definitions, so I took those as a base and expanded

60
00:03:56,658 --> 00:04:00,026
on them, and those are in use by some of the major SLO vendors out

61
00:04:00,048 --> 00:04:03,098
there now. And over the next few years,

62
00:04:03,184 --> 00:04:06,950
there was kind of what I call a cambrian SLO explosion.

63
00:04:07,030 --> 00:04:10,522
Get it? Slo explosion. Little dad joke there.

64
00:04:10,656 --> 00:04:14,314
But there was this explosion in SLO material with SLO

65
00:04:14,362 --> 00:04:18,554
specific conferences and also Alex Sedalgo's book on implementing slos.

66
00:04:18,682 --> 00:04:23,098
So I got to work creating formulas that can be shared across a large organization,

67
00:04:23,274 --> 00:04:26,962
which would leave little room for creativity and variance because I wanted everyone

68
00:04:27,016 --> 00:04:30,814
on the same page, I wanted to be able to give prescriptive formulas

69
00:04:30,862 --> 00:04:33,650
that could be implemented at broad scale.

70
00:04:35,430 --> 00:04:39,106
So this is what I came up with. The definition

71
00:04:39,138 --> 00:04:42,870
of an SLI is what I use to put examples together.

72
00:04:43,020 --> 00:04:47,014
Now, there's two major SLI opinionations, though the difference between them

73
00:04:47,052 --> 00:04:50,330
is a bit subtle at first glance. The first is from the Google

74
00:04:50,400 --> 00:04:53,942
SRE book, which describes an SLI,

75
00:04:54,006 --> 00:04:58,026
pardon me, as a measurement of system performance. The second,

76
00:04:58,128 --> 00:05:01,830
which I found first in Liz and Seth's video,

77
00:05:01,920 --> 00:05:05,946
describes an SLI as something that delineates good requests from bad requests.

78
00:05:06,058 --> 00:05:09,934
And that second opinion is really one that resonated well with me.

79
00:05:10,052 --> 00:05:13,554
So in know, both opinionations did

80
00:05:13,592 --> 00:05:16,910
service at Google, even though they're

81
00:05:17,070 --> 00:05:20,910
somewhat conflicting. But the second, as I mentioned, is implemented

82
00:05:20,990 --> 00:05:24,210
more broadly by practitioners and vendors that I found.

83
00:05:24,360 --> 00:05:28,458
And I decided to base my example on that second opinionation,

84
00:05:28,654 --> 00:05:32,454
not only because it had wider acceptance because intuitively it made more sense

85
00:05:32,492 --> 00:05:35,554
to me. I spent quite a bit of time dissecting

86
00:05:35,602 --> 00:05:39,190
those examples in the Google SRE book and the SRE workbook,

87
00:05:39,350 --> 00:05:42,410
and they were good.

88
00:05:42,480 --> 00:05:46,518
But I think the evolution of slis

89
00:05:46,614 --> 00:05:50,346
and slos at Google probably bifurcated because they

90
00:05:50,368 --> 00:05:53,806
have a lot of teams there. And that's not a criticism of the book or

91
00:05:53,828 --> 00:05:57,406
the organization. But the definitions that I came across seemed to

92
00:05:57,428 --> 00:06:01,006
be more abstract than what I was looking for. And so here are

93
00:06:01,028 --> 00:06:05,354
three examples of slis for the second SLI opinionation

94
00:06:05,402 --> 00:06:08,434
that I moved forward with. They each consist of three things.

95
00:06:08,552 --> 00:06:12,578
A metric identifier, a metric operator, and a metric value.

96
00:06:12,744 --> 00:06:15,954
This approach is one that is straightforward for a human being to understand,

97
00:06:16,072 --> 00:06:19,746
but also fits easily into most of the open source

98
00:06:19,778 --> 00:06:22,742
and commercial monitoring and observability software out there.

99
00:06:22,876 --> 00:06:26,406
The part of the SLI definition which requires the most consideration is what

100
00:06:26,428 --> 00:06:30,630
that metric value should be. And that's one that is often tuned

101
00:06:30,710 --> 00:06:34,182
or calibrated by an engineering team, either for latency,

102
00:06:34,326 --> 00:06:38,278
most often, sometimes with error

103
00:06:38,374 --> 00:06:41,514
response codes, like a five, xx. That's pretty clear that that's a bad

104
00:06:41,552 --> 00:06:45,274
request, but it's going to be up to engineering teams to determine,

105
00:06:45,322 --> 00:06:49,086
like, is a 404 a bad request, or is

106
00:06:49,108 --> 00:06:52,802
that just clients thinking that they're going to the right place?

107
00:06:52,856 --> 00:06:56,658
Because really, all of this stuff is about feeling customer

108
00:06:56,744 --> 00:06:59,620
pain and wanting to make sure that they have a great experience.

109
00:07:01,510 --> 00:07:05,010
And so I kind of cemented this

110
00:07:05,160 --> 00:07:08,402
example so that I could socialize widely

111
00:07:08,466 --> 00:07:11,320
within the engineering of what an SLI was,

112
00:07:11,770 --> 00:07:14,306
which teams to what's an slO?

113
00:07:14,498 --> 00:07:18,054
And that definition came

114
00:07:18,092 --> 00:07:21,494
down to the number of good requests divided by the number of bad requests

115
00:07:21,542 --> 00:07:25,066
over a time range. And this is often called a

116
00:07:25,088 --> 00:07:29,146
request based slos, where you count up the number of requests and see if

117
00:07:29,168 --> 00:07:32,090
you got 99% of them right over a certain time range.

118
00:07:32,430 --> 00:07:36,254
And I called the

119
00:07:36,292 --> 00:07:39,534
three different components here a little bit different. In the red, we have the

120
00:07:39,572 --> 00:07:42,698
success objective, which is your typical how many nines?

121
00:07:42,794 --> 00:07:46,098
And then we drop the SLI in, which works really well for a lot of

122
00:07:46,104 --> 00:07:49,746
the tooling out there. Then we have a period. And if you don't have a

123
00:07:49,768 --> 00:07:52,542
time period here, you don't really have an slO,

124
00:07:52,686 --> 00:07:55,986
because it's really important to specify this so that

125
00:07:56,008 --> 00:07:59,320
you're evaluating it over something that's meaningful to the customer.

126
00:08:00,570 --> 00:08:03,878
And one question that has come up is, how do I

127
00:08:03,884 --> 00:08:07,778
know how many nines to choose for this success objective? When I was at Zenesk,

128
00:08:07,794 --> 00:08:11,354
we had an engineering vp named Jason Smale, who was very

129
00:08:11,392 --> 00:08:14,726
technical and engineers had him highly regarded.

130
00:08:14,838 --> 00:08:17,914
And so he said, we need to hit three and a half

131
00:08:17,952 --> 00:08:22,058
nines. And so that 99.95%

132
00:08:22,224 --> 00:08:25,786
number became known as Smale's number. And if reliability dipped

133
00:08:25,818 --> 00:08:28,942
below that number, it usually meant that a customer somewhere was feeling pain.

134
00:08:29,076 --> 00:08:32,474
And this is really, if you want to get into enterprise software,

135
00:08:32,522 --> 00:08:35,970
this is kind of, you must meet this criteria to

136
00:08:36,040 --> 00:08:40,434
get on the ride. And so now

137
00:08:40,472 --> 00:08:43,426
that you realize you're dealing with enterprise customers and you need three and a half

138
00:08:43,448 --> 00:08:46,638
nines, how do you pick an appropriate metric value for your SLI,

139
00:08:46,734 --> 00:08:52,054
since that's the only dependent variable? Now that you fix the objective at 99.95,

140
00:08:52,172 --> 00:08:55,766
and this is essentially what I call calibrating your slO. Take a

141
00:08:55,788 --> 00:08:59,718
time period of known good performance, set your objective at 99.95,

142
00:08:59,804 --> 00:09:02,986
and iterate across your SLI to figure out what

143
00:09:03,008 --> 00:09:06,810
latency value gives you that 99.95%.

144
00:09:06,960 --> 00:09:10,842
In this example, it could be 100 milliseconds. And I was able to develop

145
00:09:10,896 --> 00:09:14,318
some simple tooling to do that, or use

146
00:09:14,404 --> 00:09:18,106
our commercial monitoring tooling to do that, and developed a dashboard

147
00:09:18,138 --> 00:09:21,566
where engineers could set their objective at 99 and

148
00:09:21,588 --> 00:09:25,300
a half and then iterate over their latency to see kind of

149
00:09:25,750 --> 00:09:28,978
what latency value was it that the customers were getting these three

150
00:09:28,984 --> 00:09:32,386
and a half nines performance. And just

151
00:09:32,408 --> 00:09:35,234
to reiterate, the time period here is very important,

152
00:09:35,352 --> 00:09:38,366
and this is a common oversight that I've seen in most of the literature.

153
00:09:38,478 --> 00:09:43,286
They'll say, take an slo of 100 milliseconds at 99.9%,

154
00:09:43,388 --> 00:09:46,098
but what time period is that over? Is it over a minute,

155
00:09:46,194 --> 00:09:49,846
an hour, a week? And you

156
00:09:49,948 --> 00:09:53,786
can, and you probably should have slos which use the same success objective in

157
00:09:53,808 --> 00:09:58,038
SLI but different time operations. Depending on the stakeholder, an engineers

158
00:09:58,054 --> 00:10:01,578
manager might want to know the reliability Moyer a week so they

159
00:10:01,584 --> 00:10:04,918
can schedule reliability work. A director might want to know it

160
00:10:04,944 --> 00:10:08,574
over a month, and a vp might want to know how reliable the service was

161
00:10:08,612 --> 00:10:12,122
over a quarter for reporting to c staff or putting the direction of technical

162
00:10:12,186 --> 00:10:15,634
efforts. And the purpose of

163
00:10:15,672 --> 00:10:19,090
slos is often to prioritize reliability work.

164
00:10:19,160 --> 00:10:22,574
That is, if you aren't meeting your slos, you want to deprioritize

165
00:10:22,622 --> 00:10:25,390
feature work in favor of reliability engineering.

166
00:10:25,550 --> 00:10:29,070
And we want to do this. We want to

167
00:10:29,080 --> 00:10:33,234
use these operations to do this because we want to be accurate. If we reprioritize

168
00:10:33,282 --> 00:10:37,026
engineering resources, that is expensive. So we want to make sure that we're

169
00:10:37,058 --> 00:10:40,454
doing that based off data that's correct and precise.

170
00:10:40,582 --> 00:10:42,810
Now let's take a quick look at error budgets.

171
00:10:43,630 --> 00:10:46,870
So an error budget is essentially just an inverted slO.

172
00:10:46,950 --> 00:10:50,806
You subtract your success objective from one and you get your allowed failure

173
00:10:50,838 --> 00:10:54,606
rate. For user requests like a financial budget, you have a certain amount of

174
00:10:54,628 --> 00:10:57,946
errors that you can spend over a time period, and ideally

175
00:10:57,978 --> 00:11:01,214
this is an amount that does not make your customers think that your service is

176
00:11:01,252 --> 00:11:04,926
unreliable. You can create monitors for this with most of

177
00:11:04,948 --> 00:11:08,146
the tooling out there and perhaps alert when, say, 80% of your

178
00:11:08,168 --> 00:11:11,906
error budget has been used up for a given time period, which will let

179
00:11:11,928 --> 00:11:15,330
your engineering teams know that it's time to work on reliability.

180
00:11:16,390 --> 00:11:20,354
You can also alert when the rate of an error budget burn

181
00:11:20,402 --> 00:11:23,814
predicts that you will exhaust your error budget before the time period has

182
00:11:23,852 --> 00:11:27,874
elapsed. And tooling. A lot of the tooling out there has functionality

183
00:11:27,922 --> 00:11:31,626
for that. So there are really two conditions that your error budget should

184
00:11:31,648 --> 00:11:35,398
spur action. First, if it's being used up too quickly and is in danger

185
00:11:35,414 --> 00:11:38,662
of being exhausted for that period, that should prioritize

186
00:11:38,726 --> 00:11:42,266
reliability focused work. The second is if your air budget is

187
00:11:42,288 --> 00:11:46,202
not being used up at all, that could indicate an improperly

188
00:11:46,266 --> 00:11:50,122
calibrated slo. Or it might mean that your service is normally so reliable

189
00:11:50,186 --> 00:11:53,706
that you're not prioritizing enough feature work, or that you should embark

190
00:11:53,738 --> 00:11:57,234
on controlled error budgets. Burns Google did that and mentioned

191
00:11:57,272 --> 00:12:00,846
it in the SRE book. With their chubby service, which was the distributed lock

192
00:12:00,878 --> 00:12:04,562
service, they introduced artificial air budget burn into

193
00:12:04,616 --> 00:12:08,134
their consumption into the service so that

194
00:12:08,252 --> 00:12:11,640
consumers of chubby would have to

195
00:12:12,410 --> 00:12:15,894
make their services be able to tolerate those chubby failures and

196
00:12:15,932 --> 00:12:19,250
hence become more reliable. And again, like slos,

197
00:12:19,330 --> 00:12:22,714
air budgets should reflect the mindset of the customer as much as

198
00:12:22,752 --> 00:12:26,538
possible. If the air budget is not exhausted but your customer is on

199
00:12:26,544 --> 00:12:30,234
the phone with your vp, go take a look at what you are measuring and

200
00:12:30,272 --> 00:12:33,050
if it really reflects what the customer is experiencing.

201
00:12:34,030 --> 00:12:37,566
So, to sum up what I've showed you so far, there's a few points on

202
00:12:37,588 --> 00:12:41,306
getting thousands of engineers on the same page for slos and air budgets.

203
00:12:41,418 --> 00:12:44,918
First, you need real world examples. Most of the published

204
00:12:44,954 --> 00:12:48,626
books out there are a bit abstract and hand wavy and

205
00:12:48,648 --> 00:12:52,274
don't really give you complete examples, so you need to have those to show

206
00:12:52,312 --> 00:12:56,302
folks. Second, present formulas for each of those entities

207
00:12:56,446 --> 00:13:00,258
which can be read easily both by humans and machines, and I've

208
00:13:00,354 --> 00:13:04,006
shown you what I used at scale there. Third, you have to be

209
00:13:04,028 --> 00:13:07,254
detailed and consistent. I see so many slos out there

210
00:13:07,292 --> 00:13:10,666
that leave off the time period, you might say well, the time range can

211
00:13:10,688 --> 00:13:15,514
be whatever you want, but then it's not can actual

212
00:13:15,632 --> 00:13:19,290
or actionable slos or air budget without a time range.

213
00:13:20,190 --> 00:13:23,274
So we've looked at some example slos

214
00:13:23,322 --> 00:13:27,658
that most engineers can parse and memorize, and which engineering managers and product managers

215
00:13:27,674 --> 00:13:31,290
can use to correlate user happiness with. In most cases,

216
00:13:31,370 --> 00:13:35,154
that happiness means your service is available and it's running fast.

217
00:13:35,352 --> 00:13:39,246
We can take the formulas I just showed and extend them to cover both conditions

218
00:13:39,278 --> 00:13:43,390
at once. So here we're talking about not only availability,

219
00:13:43,470 --> 00:13:46,998
but also latency and both of those. You need to

220
00:13:47,004 --> 00:13:51,394
have both of those. So here's an example SLI SLO

221
00:13:51,442 --> 00:13:55,366
and error budget, which covers both latency and availability. So if

222
00:13:55,388 --> 00:13:58,902
the page response is not a five xx or request was

223
00:13:58,956 --> 00:14:02,826
served in under 100 milliseconds, that request can be considered to

224
00:14:02,848 --> 00:14:06,026
be a good request. That's our slis to which we

225
00:14:06,048 --> 00:14:09,194
can add a success objective of three and a half nines and a time

226
00:14:09,232 --> 00:14:12,766
range of seven days to be evaluated on. To get the

227
00:14:12,788 --> 00:14:17,742
error budget, we can subtract a success objective of 99.95%

228
00:14:17,796 --> 00:14:22,238
from one, which gives us can error budget of zero 5%.

229
00:14:22,404 --> 00:14:26,174
It's easy to understand and you can also easily create multiple

230
00:14:26,222 --> 00:14:30,510
slos and error budgets from the base SLI just by extending the time range.

231
00:14:30,670 --> 00:14:33,826
Now, on the point of the success objective here I

232
00:14:33,848 --> 00:14:36,846
have 99.95% listed.

233
00:14:36,958 --> 00:14:40,674
It's three and a half nine. Realistically, this is what enterprise

234
00:14:40,722 --> 00:14:44,722
customers demand these days. That means out of a million requests,

235
00:14:44,786 --> 00:14:48,534
you only get 500 requests that are slow or

236
00:14:48,572 --> 00:14:52,842
return what we also known was a fail or the 500

237
00:14:52,896 --> 00:14:56,902
internal server error as an example. And so if you're at scale,

238
00:14:56,966 --> 00:15:00,806
this should be your success objective. And I go into this in depth

239
00:15:00,838 --> 00:15:04,442
a little bit in the presentation shown below on the link for my Srecon

240
00:15:04,506 --> 00:15:07,994
presentation. So at this point we've

241
00:15:08,042 --> 00:15:11,806
got example formulas for slis, slos and air budgets that should

242
00:15:11,828 --> 00:15:15,374
be easy for folks to understand and also straightforward to implement

243
00:15:15,422 --> 00:15:18,322
with most monitoring and observability tooling out there,

244
00:15:18,376 --> 00:15:21,982
both open source and commercial. Of the two components of latency

245
00:15:22,046 --> 00:15:25,790
and availability, availability is generally pretty easy to measure.

246
00:15:25,950 --> 00:15:29,110
The most simple example is a 500 response.

247
00:15:29,450 --> 00:15:33,346
You see the sorry a problem occurred. Web page latency,

248
00:15:33,458 --> 00:15:37,350
however, is more difficult to get right at scale. And when I say

249
00:15:37,420 --> 00:15:41,034
get it right, there are two aspects of being right. First,

250
00:15:41,152 --> 00:15:44,810
does your measurement have the right precision for your scale?

251
00:15:45,150 --> 00:15:48,726
That is, if I have 1 million user requests, can you generate

252
00:15:48,758 --> 00:15:52,586
a latency aggregate which means you aren't leaving more than a few dozen users

253
00:15:52,618 --> 00:15:56,698
off precision. Here is the number of decimal places.

254
00:15:56,874 --> 00:16:00,426
The other aspect is accuracy. Is your latency aggregate

255
00:16:00,458 --> 00:16:04,194
for an SLO or a monitor actually correct? In many cases I've seen

256
00:16:04,232 --> 00:16:08,654
that answer is no to both and precision

257
00:16:08,702 --> 00:16:12,190
versus accuracy. Precision is the number of decimal places.

258
00:16:12,270 --> 00:16:16,006
Accuracy is are the values in those decimal places correct?

259
00:16:16,108 --> 00:16:19,942
So let's dive in. So coming

260
00:16:19,996 --> 00:16:23,750
back to this chart. This chart is an

261
00:16:23,900 --> 00:16:27,426
RRD graph, and it measures network usage and calculates the 95th

262
00:16:27,458 --> 00:16:30,826
percentile over a time period. And at the

263
00:16:30,848 --> 00:16:34,054
time of the.com boom, you saw a lot of these RRD

264
00:16:34,102 --> 00:16:37,994
graphs, and these were mostly used for

265
00:16:38,192 --> 00:16:41,750
metering bandwidth. Bandwidth was built on something like five megabits at

266
00:16:41,760 --> 00:16:45,866
95th percentile, meaning that if you took all your five minute bandwidth

267
00:16:45,898 --> 00:16:50,170
usage measurement slices and ordered them, and took the 95th percentile,

268
00:16:50,330 --> 00:16:53,902
if that number was above five megabits, you incur

269
00:16:53,966 --> 00:16:57,634
overage charges. And this first popularized the

270
00:16:57,672 --> 00:17:02,082
approach of using percentiles. And that would really

271
00:17:02,136 --> 00:17:05,746
notably be seen about ten years later in 2011 with the

272
00:17:05,768 --> 00:17:09,166
advent of the statSD protocol developed by Etsy, which provided

273
00:17:09,198 --> 00:17:12,726
the p 95 was a latency aggregation metric. And I wrote more

274
00:17:12,748 --> 00:17:15,846
about this in a blog post I published last year, and I'll go into some

275
00:17:15,868 --> 00:17:20,230
of the content in the next slides, but this is the historical

276
00:17:20,310 --> 00:17:22,010
significance of this graph.

277
00:17:23,310 --> 00:17:26,870
So let's talk about percentiles. This is a slide

278
00:17:26,950 --> 00:17:30,534
from an SLO presentation I gave at Srecon 2019.

279
00:17:30,662 --> 00:17:33,886
It illustrates two latency distribution profiles, which are meant to

280
00:17:33,908 --> 00:17:36,746
represent service nodes that are behaving differently.

281
00:17:36,858 --> 00:17:40,554
The blue distribution represents a bimodal latency

282
00:17:40,602 --> 00:17:44,830
profile with lower latencies than the single mode red latency distribution.

283
00:17:44,990 --> 00:17:48,690
Basically, this could be two web servers, one performing well and

284
00:17:48,760 --> 00:17:52,354
one performing not as well. The red server is not performing as

285
00:17:52,392 --> 00:17:56,194
well, and if we take the p 95

286
00:17:56,232 --> 00:17:59,894
values for latency for each server, and we average those, we could

287
00:17:59,932 --> 00:18:03,506
get an indicator of around 430 milliseconds,

288
00:18:03,538 --> 00:18:07,080
and we might think that hes that's the performance of our service.

289
00:18:07,450 --> 00:18:11,522
But if we combine the raw latency values from each of these distribution

290
00:18:11,586 --> 00:18:14,938
sets and calculate the aggregate p 95 from those,

291
00:18:15,104 --> 00:18:18,602
we'll get 230 milliseconds, and the error there

292
00:18:18,656 --> 00:18:22,350
is almost 100%. And many,

293
00:18:22,420 --> 00:18:26,314
if not all, of the monitoring and observability tools out there will happily

294
00:18:26,362 --> 00:18:29,934
let you use an averaging function for percentiles generated from

295
00:18:29,972 --> 00:18:33,434
different hosts, nodes or clusters. If your distribution

296
00:18:33,482 --> 00:18:37,026
profiles are the same, no problem. That works great. But it's when

297
00:18:37,048 --> 00:18:40,866
your services are behaving asymmetrically that you'll encounter large errors with

298
00:18:40,888 --> 00:18:44,318
this approach, and this is a problem with percentiles.

299
00:18:44,414 --> 00:18:47,762
And I talked about that in depth in that presentation.

300
00:18:47,906 --> 00:18:51,026
So beware of using percentiles.

301
00:18:51,138 --> 00:18:53,880
I've talked about this and ranted about this,

302
00:18:54,410 --> 00:18:59,174
and this kind of illustrates the

303
00:18:59,212 --> 00:19:02,442
prime condition where that's can issue. And again, if everything's running

304
00:19:02,496 --> 00:19:05,706
smoothly or if you have a single node, percentiles work just great.

305
00:19:05,808 --> 00:19:09,610
But it's the real world scenarios where we have different

306
00:19:09,680 --> 00:19:13,466
node performance profiles and possibly hundreds

307
00:19:13,498 --> 00:19:16,926
or thousands of nodes services requests that we want to be

308
00:19:16,948 --> 00:19:20,622
able to handle and evaluate how our service

309
00:19:20,676 --> 00:19:24,366
is performing accurately that teams into histograms

310
00:19:24,398 --> 00:19:27,490
for measuring web service latency.

311
00:19:28,230 --> 00:19:32,094
And I give an internal

312
00:19:32,142 --> 00:19:35,698
talk. I called Dr. Histogram how I learned to stop worrying and

313
00:19:35,704 --> 00:19:39,000
love latency bands at Zendesk a few years ago,

314
00:19:39,370 --> 00:19:43,286
and I went into more depth on the intricacies of

315
00:19:43,308 --> 00:19:46,758
these three different types of histograms in the SLO comp link below.

316
00:19:46,924 --> 00:19:48,120
But in short,

317
00:19:51,070 --> 00:19:55,526
there's a couple of different approaches you can use for measuring latency with histogram.

318
00:19:55,718 --> 00:19:59,286
And this involves essentially collecting a latency sample and fitting

319
00:19:59,318 --> 00:20:02,274
it into what we call a bucket or a bin.

320
00:20:02,342 --> 00:20:06,074
And you'll see the gray and blue

321
00:20:06,122 --> 00:20:09,226
bars here. Those are your buckets or bins. And so let's

322
00:20:09,258 --> 00:20:12,958
take a look at how these are implemented differently. First, we could have

323
00:20:13,044 --> 00:20:16,306
a log linear histogram, which you

324
00:20:16,328 --> 00:20:19,422
can see the details of at openhistogram IO.

325
00:20:19,566 --> 00:20:23,806
And if we have a latency value here of 125 milliseconds,

326
00:20:23,918 --> 00:20:27,974
we could say like, oh, we'll just slot that sample into

327
00:20:28,172 --> 00:20:31,666
the greater than 100 millisecond, but less than 200 millisecond

328
00:20:31,698 --> 00:20:34,946
bucket. And so this is a data structure

329
00:20:34,978 --> 00:20:38,646
that is fairly easy to represent, because all you have is an

330
00:20:38,668 --> 00:20:42,054
array representing different histogram buckets,

331
00:20:42,182 --> 00:20:45,654
and then you increase the value of that array, essentially a counter

332
00:20:45,702 --> 00:20:49,194
for each of those. And this is a volume invariant way

333
00:20:49,232 --> 00:20:52,606
of storing large amounts of latency data that you

334
00:20:52,628 --> 00:20:56,570
can also use to generate highly accurate aggregates

335
00:20:56,650 --> 00:21:00,590
for an entire cluster or any set of hosts.

336
00:21:01,650 --> 00:21:05,774
And folks might also be familiar with the middle structure. That's the cumulative

337
00:21:05,822 --> 00:21:09,230
histogram which prometheus uses.

338
00:21:09,390 --> 00:21:12,526
So if I have a latency value of 125 milliseconds,

339
00:21:12,638 --> 00:21:16,130
it will assign labels starting at less than

340
00:21:16,200 --> 00:21:19,414
infinity all the way down to less than 200.

341
00:21:19,532 --> 00:21:22,706
So this takes a few more data structures

342
00:21:22,898 --> 00:21:26,214
or a few more counter values to implement, and it's not quite as

343
00:21:26,252 --> 00:21:29,926
efficient as the logged linear histogram. And at Zendesk, I flipped that

344
00:21:29,948 --> 00:21:34,262
on its head and came up what was called an inverse cumulative histogram,

345
00:21:34,406 --> 00:21:37,706
where, for an example, if we have 125 milliseconds, I could

346
00:21:37,728 --> 00:21:41,558
have a counter data structure, bump the counter and assign

347
00:21:41,654 --> 00:21:45,194
these labels to it, which are often known as metric tags.

348
00:21:45,322 --> 00:21:48,906
I could assign greater than ten, greater than 50, greater than 100 milliseconds,

349
00:21:48,938 --> 00:21:52,970
but not greater than 200 milliseconds. And this approach

350
00:21:53,050 --> 00:21:56,606
made my head hurt for a little bit. But it has some advantages

351
00:21:56,718 --> 00:22:00,578
in terms of operator efficiency and ease of use of implementing with a

352
00:22:00,584 --> 00:22:04,866
lot of the tooling out there and all

353
00:22:04,888 --> 00:22:08,730
these buckets that can also be referred to as latency bands.

354
00:22:08,830 --> 00:22:11,926
So you can kind of take a look at each of these different types of

355
00:22:11,948 --> 00:22:15,894
histograms and decide, I might want to try to use histograms for

356
00:22:15,932 --> 00:22:19,366
storing latency. So one of these should give you

357
00:22:19,388 --> 00:22:22,934
some good results. And you might ask,

358
00:22:22,972 --> 00:22:26,554
well, okay, well, now I know how to capture latency in a histogram at

359
00:22:26,592 --> 00:22:30,074
scale. How do I generate an SLO from it? Well, let's go back to our

360
00:22:30,112 --> 00:22:33,614
definition. It's the number of good requests divided by the number of bad

361
00:22:33,652 --> 00:22:37,726
requests over a time range. And so in this case,

362
00:22:37,908 --> 00:22:41,562
we can use a histogram data for the SLI.

363
00:22:41,626 --> 00:22:45,226
We can sum up the number of requests below 100 milliseconds,

364
00:22:45,418 --> 00:22:48,978
and we can divide that by the total number of requests, which would just be

365
00:22:49,064 --> 00:22:52,386
the count sum of all the bands, and we can multiply that

366
00:22:52,488 --> 00:22:55,702
by 100. In the case of the

367
00:22:55,756 --> 00:22:58,946
number of requests under 100 milliseconds,

368
00:22:59,058 --> 00:23:01,910
with the inverse cumulative histogram,

369
00:23:03,450 --> 00:23:06,150
we add up the counts of the blue bars.

370
00:23:07,290 --> 00:23:10,514
With the log linear histogram,

371
00:23:10,562 --> 00:23:14,026
we just add up all those, the counts of the three bars to

372
00:23:14,048 --> 00:23:16,986
the left of the three gray bars to the left of the blue bar.

373
00:23:17,088 --> 00:23:19,894
So, mathematically, this is very simple to implement,

374
00:23:20,022 --> 00:23:23,626
and it's fast, it works quickly with all monitoring solutions

375
00:23:23,658 --> 00:23:26,986
out there. And it's also extremely accurate because you're adding

376
00:23:27,018 --> 00:23:30,240
up counts of essentially raw data,

377
00:23:30,610 --> 00:23:34,430
and it also gives you essentially arbitrary precision.

378
00:23:34,510 --> 00:23:38,430
So this is a very robust and accurate approach,

379
00:23:38,590 --> 00:23:42,322
and I highly recommend this because this will give you some great numbers

380
00:23:42,376 --> 00:23:46,114
at scale. Now, you might

381
00:23:46,152 --> 00:23:49,014
say, like, well, this is a lot of work to do, but again, it goes

382
00:23:49,052 --> 00:23:52,646
back to prioritizing reliability work. So we want to make sure that our

383
00:23:52,668 --> 00:23:56,066
data about, if we're hitting our slos is accurate,

384
00:23:56,098 --> 00:23:59,482
because we're spending, likely spending hundreds of thousands or millions of dollars

385
00:23:59,616 --> 00:24:02,780
on shifting this engineering work. Now,

386
00:24:05,630 --> 00:24:08,700
I showed some raw histograms there,

387
00:24:09,550 --> 00:24:12,658
where we keep count of a number of samples in each bin,

388
00:24:12,774 --> 00:24:16,554
and that way we can sum them up. But there's some approximate

389
00:24:16,602 --> 00:24:19,326
structures out there which you can use,

390
00:24:19,508 --> 00:24:23,690
and some of the vendors provide to do the same things. And they're

391
00:24:23,770 --> 00:24:27,214
often called sketches, like the GK sketch or the DD

392
00:24:27,262 --> 00:24:31,246
sketch structure by one of the vendors. And there's also approximate

393
00:24:31,278 --> 00:24:35,178
histograms such as t Digest, made by Ted Dunning, which stores

394
00:24:35,374 --> 00:24:38,934
approximations of distributions. And these

395
00:24:38,972 --> 00:24:43,266
two charts here were taken from the log linear circ

396
00:24:43,298 --> 00:24:47,394
Slis paper for open histogram, and they represent error

397
00:24:47,442 --> 00:24:51,146
percentages for two different takes of workloads across different

398
00:24:51,328 --> 00:24:54,666
p nine x values on the x axis. And you

399
00:24:54,688 --> 00:24:58,262
can see the red line here, which is the open histogram implementation

400
00:24:58,326 --> 00:25:02,146
that's got very low errors. But then you look at like the T Digest

401
00:25:02,278 --> 00:25:06,014
DD sketch and HDR histogram, which do relatively well

402
00:25:06,052 --> 00:25:09,774
in terms of errors. However, there's a detail that is not in these

403
00:25:09,812 --> 00:25:13,170
charts. These errors are for single node evaluations only,

404
00:25:13,240 --> 00:25:16,718
say for one web server. Now, how do approximate histograms

405
00:25:16,734 --> 00:25:21,218
and sketches behave across asymmetric node workloads of

406
00:25:21,384 --> 00:25:25,006
hundreds of web servers or arbitrary time windows? And that's

407
00:25:25,038 --> 00:25:28,306
a very difficult question to answer. But by and large, the errors are likely

408
00:25:28,338 --> 00:25:31,606
to be unbounded and using histograms which

409
00:25:31,628 --> 00:25:35,202
store the exact sample counts, as I termed raw histograms

410
00:25:35,266 --> 00:25:39,382
on the previous slide, those avoid that problem entirely,

411
00:25:39,446 --> 00:25:42,938
ensuring that any aggregates generated for them

412
00:25:43,104 --> 00:25:46,134
for slos are highly accurate and precise.

413
00:25:46,262 --> 00:25:49,354
So the sketches are good

414
00:25:49,392 --> 00:25:52,446
to a certain extent, but they don't really hit

415
00:25:52,468 --> 00:25:55,866
the same level of precision as these raw

416
00:25:55,898 --> 00:25:59,278
histograms. Now, while we're on

417
00:25:59,284 --> 00:26:02,586
the subject of histograms, I want to highlight some recent work in this area by

418
00:26:02,628 --> 00:26:07,038
Adrian Cockroft. Adrian published a medium post titled percentiles

419
00:26:07,054 --> 00:26:10,770
don't work. I think he coined them as wrong,

420
00:26:10,840 --> 00:26:14,834
but useful analyzing the distribution of response times for web

421
00:26:14,872 --> 00:26:17,240
services. A few months ago,

422
00:26:18,810 --> 00:26:21,894
hes started doing some work here, where he

423
00:26:22,012 --> 00:26:25,734
looked at operational telemetry, which is usually latency, and using some

424
00:26:25,772 --> 00:26:29,774
r based tooling to decompose it into component normalish

425
00:26:29,842 --> 00:26:33,866
distributions. So this image here was taken from his blog post, where he was

426
00:26:33,888 --> 00:26:37,594
able to take a bimodal histogram here and decompose it into

427
00:26:37,632 --> 00:26:41,450
two normal distributions using the mixed tools r package.

428
00:26:41,790 --> 00:26:45,246
Now why is this important and what does this have to do

429
00:26:45,268 --> 00:26:48,778
with slos? We just took a look at what magnitude of errors

430
00:26:48,794 --> 00:26:52,506
can arise from using percentiles for latency measurements. So we

431
00:26:52,548 --> 00:26:57,090
follow that up with looking at histograms to measure latency distributions.

432
00:26:57,510 --> 00:26:59,220
So with something like this,

433
00:27:02,470 --> 00:27:05,438
we can pull out these normal distributions.

434
00:27:05,614 --> 00:27:09,254
And this could be relevant if we wanted to make an SLO for something like

435
00:27:09,292 --> 00:27:13,138
disk writes, where you might have writing to a block device,

436
00:27:13,234 --> 00:27:16,818
versus just writing to cache or reading

437
00:27:16,834 --> 00:27:20,714
from the block device, as opposed to reading to cache. We can use these

438
00:27:20,752 --> 00:27:23,980
to implement fine grained slos for each of the different

439
00:27:24,350 --> 00:27:28,790
moyer of kind of the physical manifestations

440
00:27:28,870 --> 00:27:32,640
of the system in the cloud. It could be like writing to s three or

441
00:27:33,090 --> 00:27:36,906
different storage levels there. And so there's some really promising

442
00:27:36,938 --> 00:27:40,894
work here. And I think that this

443
00:27:40,932 --> 00:27:44,866
is definitely something to follow going ahead, because if you

444
00:27:44,888 --> 00:27:48,306
really want to get fine grained with, say, a system that

445
00:27:48,328 --> 00:27:52,350
has a few different modes at very large scale, this approach

446
00:27:52,510 --> 00:27:54,180
would allow you to do that.

447
00:27:55,270 --> 00:27:58,678
Now, one common question I've gotten about slos and

448
00:27:58,684 --> 00:28:03,362
error budgets is how do you implement them across a distributed service architectures?

449
00:28:03,506 --> 00:28:07,062
Now, one approach is to use an SLO and error budget for each service,

450
00:28:07,116 --> 00:28:10,438
and this includes third party vendor services, as shown

451
00:28:10,454 --> 00:28:13,734
in blue here. Now, the error rates I've shown

452
00:28:13,782 --> 00:28:17,846
here and documented in red are error

453
00:28:17,878 --> 00:28:21,386
rates across these different services. So you can have a

454
00:28:21,408 --> 00:28:25,306
different error rate contribution from the third party service, the mid tier and the edge

455
00:28:25,338 --> 00:28:28,606
tier. And you can take those and you

456
00:28:28,628 --> 00:28:31,886
can add those up and essentially get a compound or

457
00:28:31,908 --> 00:28:35,294
composite error rate for what the customer is seeing. So in this

458
00:28:35,332 --> 00:28:38,866
case, you might see that, hey, our in house back

459
00:28:38,888 --> 00:28:41,460
end service has a 0.1% error rate.

460
00:28:42,150 --> 00:28:45,386
But then if you roll that up to the mid tier,

461
00:28:45,518 --> 00:28:49,346
now you've got 1% error

462
00:28:49,378 --> 00:28:52,562
rate also from the third party, which exceeds your mid tier

463
00:28:52,626 --> 00:28:56,200
error budget of 1%. And so

464
00:28:57,050 --> 00:29:00,418
you can kind of put these diagrams together, and it

465
00:29:00,444 --> 00:29:03,830
will help you understand where you need to focus reliability

466
00:29:03,910 --> 00:29:07,498
work. In this case, you need to focus reliability work on the

467
00:29:07,504 --> 00:29:10,714
third party and either pull that in house or do

468
00:29:10,752 --> 00:29:14,094
some sort of interface around

469
00:29:14,132 --> 00:29:17,162
it to make it more reliable. And the goal here is not to assign blames

470
00:29:17,226 --> 00:29:20,414
to teams or to different services. It's to

471
00:29:20,452 --> 00:29:24,426
prioritize reliability work. And that's

472
00:29:24,458 --> 00:29:28,738
really what this is all about. Because for

473
00:29:28,824 --> 00:29:31,986
most of almost, I would say almost all of you out there, you're using some

474
00:29:32,008 --> 00:29:35,494
sort of distributed system like this, and you're going to say like,

475
00:29:35,532 --> 00:29:38,822
well, how do we use slos across that?

476
00:29:38,876 --> 00:29:42,226
Remember to be customer centric, and you can roll

477
00:29:42,258 --> 00:29:45,000
those error budgets up, starting from,

478
00:29:46,090 --> 00:29:49,398
I'll call it upstream, which is further away from the client. You can roll

479
00:29:49,414 --> 00:29:52,886
those error rates up and get a composite error

480
00:29:52,918 --> 00:29:56,266
rate fairly simply and see

481
00:29:56,288 --> 00:30:00,086
what the client is seeing. And that's it.

482
00:30:00,128 --> 00:30:04,122
My tour through techniques for slos

483
00:30:04,186 --> 00:30:08,334
and air budgets at scale I hope you enjoyed this presentation. Feel free

484
00:30:08,372 --> 00:30:11,962
to reach out to me on LinkedIn or Twitter.

485
00:30:12,026 --> 00:30:15,438
And that Twitter handle also works across Mastodon and

486
00:30:15,444 --> 00:30:18,654
a couple of the other news sites popping up. I'd love to hear about your

487
00:30:18,692 --> 00:30:22,350
experiences and talk about how you're using

488
00:30:22,420 --> 00:30:25,622
slos and air budgets at scale. Thanks 42.

489
00:30:25,676 --> 00:30:26,420
We'll see you next time.

