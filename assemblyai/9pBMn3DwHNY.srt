1
00:00:35,730 --> 00:00:39,074
Hello, everybody. My name is Mikolaj

2
00:00:39,122 --> 00:00:42,822
Pawlikowski, and I would like to thank you for coming

3
00:00:42,876 --> 00:00:46,422
to this talk. I'm going to talk to you about something that

4
00:00:46,476 --> 00:00:50,138
really excites me. I'm going to talk about chaos engineering,

5
00:00:50,274 --> 00:00:54,430
and in particular how it overlaps with sres

6
00:00:54,930 --> 00:00:58,602
and why sres really should love chaos

7
00:00:58,666 --> 00:01:01,934
engineering to begin with. Here's the

8
00:01:01,972 --> 00:01:05,282
plan. For the next half an hour or so, like any good

9
00:01:05,336 --> 00:01:08,578
scientist, we're going to start with defining our terms.

10
00:01:08,664 --> 00:01:12,802
We're going to see what SRE is, we're going to see what

11
00:01:12,936 --> 00:01:16,430
chaos engineering is, and then we're going to focus

12
00:01:16,520 --> 00:01:19,766
on the actual overlap where the two come together

13
00:01:19,868 --> 00:01:22,966
to create an extra value. I'm also going to

14
00:01:22,988 --> 00:01:27,074
use this opportunity to talk about my latest book, Chaos Engineering.

15
00:01:27,122 --> 00:01:30,522
Crash test your applications, and I'm going to finish with two

16
00:01:30,576 --> 00:01:34,346
demos to kind of illustrate what I really mean

17
00:01:34,448 --> 00:01:38,214
and kind of show you in practice where chaos engineering

18
00:01:38,262 --> 00:01:42,206
can really help the sres. Sounds like

19
00:01:42,228 --> 00:01:46,430
a plan. Well, hopefully. So let's start with SRE, the site

20
00:01:46,500 --> 00:01:49,854
reliability engineering. I'm pretty sure

21
00:01:49,892 --> 00:01:53,186
that some of you at least, are familiar with that. It's a

22
00:01:53,208 --> 00:01:56,020
concept, a term that was coined at Google.

23
00:01:56,390 --> 00:01:59,746
And in one sentence, it's basically

24
00:01:59,848 --> 00:02:03,362
what you end up with if you give the questions work to

25
00:02:03,416 --> 00:02:06,040
software engineers. Right?

26
00:02:06,410 --> 00:02:10,406
So the way that I'd like to think about is that

27
00:02:10,508 --> 00:02:14,210
if you, on a Friday evening, go to an SRE

28
00:02:14,290 --> 00:02:17,350
pub, assuming that kind of place exists,

29
00:02:17,950 --> 00:02:21,914
imagine what you're going to hear about. You're probably going

30
00:02:21,952 --> 00:02:26,374
to hear things like reliability, like performance, like latency,

31
00:02:26,502 --> 00:02:29,754
like availability. You might hear

32
00:02:29,792 --> 00:02:33,470
someone in these corner ranting about on call and

33
00:02:33,540 --> 00:02:37,680
alerting and monitoring. You're probably going to hear someone

34
00:02:38,050 --> 00:02:41,130
talking about business objectives and slos

35
00:02:41,210 --> 00:02:45,218
and slas and stuff like that. You might even hear

36
00:02:45,304 --> 00:02:50,930
someone mention latency,

37
00:02:51,430 --> 00:02:54,994
and that's all good. That's basically what

38
00:02:55,032 --> 00:02:58,514
you gives you a good idea of what these

39
00:02:58,552 --> 00:03:02,290
people actually care about. If this is what you talk about on Friday evening,

40
00:03:02,370 --> 00:03:05,766
whats means that they must deeply care about and this know kind of

41
00:03:05,788 --> 00:03:10,394
like the kernel of what SRE really is. These are people

42
00:03:10,592 --> 00:03:14,218
who deeply care about making sure that things

43
00:03:14,304 --> 00:03:18,154
run smoothly, right? So it's one

44
00:03:18,192 --> 00:03:21,802
thing to kind of write software and features,

45
00:03:21,866 --> 00:03:25,598
and it's kind of like a different

46
00:03:25,684 --> 00:03:28,846
problem to actually run that and run it well

47
00:03:29,028 --> 00:03:31,994
and at scale and without hiccups,

48
00:03:32,042 --> 00:03:35,806
right? And this is where we get to the core

49
00:03:35,838 --> 00:03:40,146
of what SrE is, right? So all these

50
00:03:40,328 --> 00:03:43,906
things that I mentioned in the virtual pub, they all kind

51
00:03:43,928 --> 00:03:47,286
of have to do with reliability. Reliability is

52
00:03:47,308 --> 00:03:51,094
kind of this encompassing term that is

53
00:03:51,132 --> 00:03:55,458
a little bit vague and kind of can involve

54
00:03:55,554 --> 00:03:59,226
pretty much all of those words here, right? So it

55
00:03:59,248 --> 00:04:03,050
might be that your system is

56
00:04:03,200 --> 00:04:05,980
performance based. It might be that, for example,

57
00:04:06,670 --> 00:04:10,022
the value that your system provides depends on

58
00:04:10,096 --> 00:04:13,280
how many operations per second it can

59
00:04:13,890 --> 00:04:16,974
produce, which case these reliability will

60
00:04:17,012 --> 00:04:20,414
be to make sure that that performance can be sustained and that

61
00:04:20,452 --> 00:04:23,182
it can be sustained long term.

62
00:04:23,326 --> 00:04:27,442
Or it might be that it's much more important

63
00:04:27,576 --> 00:04:31,794
for your system to be highly available because if

64
00:04:31,832 --> 00:04:35,670
users can't use it for even a small fraction of time,

65
00:04:35,820 --> 00:04:39,080
they're going to be much more upset than if

66
00:04:39,690 --> 00:04:43,478
the performance is degraded. So that might be your

67
00:04:43,644 --> 00:04:47,262
reality of reliability. It might be latency.

68
00:04:47,426 --> 00:04:52,294
It might be that people SrE browsing funny

69
00:04:52,342 --> 00:04:55,386
videos and they really care about latency. And if

70
00:04:55,408 --> 00:04:58,810
the video is too slow and they're going to go

71
00:04:58,880 --> 00:05:02,174
and spend their money elsewhere or

72
00:05:02,212 --> 00:05:05,786
watch the funny videos elsewhere, so that latency,

73
00:05:05,978 --> 00:05:10,186
you can measure that and this is your reality of reliability

74
00:05:10,298 --> 00:05:12,854
to make Sre that that latency is maintained.

75
00:05:12,922 --> 00:05:16,402
Right. Then you have all the monitoring, alerting, on call.

76
00:05:16,456 --> 00:05:20,082
These are the things, these are the tools that show us

77
00:05:20,136 --> 00:05:23,486
the visibility, give us this visibility

78
00:05:23,598 --> 00:05:27,330
into what's actually going on under the hood in the systems

79
00:05:27,490 --> 00:05:31,126
and alert us when our attention is needed.

80
00:05:31,308 --> 00:05:35,398
The dreaded on call and the paging where

81
00:05:35,484 --> 00:05:38,806
people sometimes have to wake up to fix something,

82
00:05:38,988 --> 00:05:42,506
these are the people who do that. These are the people who set up the

83
00:05:42,528 --> 00:05:46,202
alerting and set up the monitoring so that they know that

84
00:05:46,256 --> 00:05:50,482
something's wrong. Right. The capacity planning,

85
00:05:50,566 --> 00:05:54,270
that is also part of your reliability. It's not

86
00:05:54,340 --> 00:05:58,894
the most glamorous. Hardly anyone

87
00:05:59,092 --> 00:06:02,414
likes to talk about a massive excel file when they try

88
00:06:02,452 --> 00:06:06,014
to predict how much resources they need. But it's

89
00:06:06,062 --> 00:06:09,426
essential. Someone needs to do that. Someone needs to think, okay,

90
00:06:09,528 --> 00:06:13,138
if we're going to grow by this many users in

91
00:06:13,144 --> 00:06:16,230
the next quarter, we're going to need more

92
00:06:16,380 --> 00:06:19,974
disks and more cpus and more ram and more x

93
00:06:20,012 --> 00:06:23,286
and y, right? This is part of the

94
00:06:23,308 --> 00:06:26,200
reliability or the slos, right?

95
00:06:27,130 --> 00:06:31,114
Probably heard of the slas. You might not have heard of

96
00:06:31,152 --> 00:06:35,420
slis and slos. SLI stands for service

97
00:06:35,870 --> 00:06:39,290
level indicator, and it can be

98
00:06:39,360 --> 00:06:43,246
any quantity that's measurable that you

99
00:06:43,268 --> 00:06:47,694
care about in terms of verifying that

100
00:06:47,732 --> 00:06:50,846
your system runs well. So let's say that, for example,

101
00:06:50,948 --> 00:06:54,370
you run some kind of API. It's an API where

102
00:06:54,440 --> 00:06:57,826
people can send a request and they get the

103
00:06:57,848 --> 00:07:01,714
right meme. Right. Your meme API and

104
00:07:01,912 --> 00:07:05,846
the SLI that you might care about is for example,

105
00:07:05,948 --> 00:07:09,442
the speed of the response, let's say whats the ninety ninth

106
00:07:09,506 --> 00:07:13,222
percentile of the response time is

107
00:07:13,276 --> 00:07:15,880
something that you care about, right?

108
00:07:17,050 --> 00:07:20,426
With that you can create design an

109
00:07:20,448 --> 00:07:23,050
SLO which is an objective,

110
00:07:23,550 --> 00:07:27,354
service level objective, which is basically a range of

111
00:07:27,392 --> 00:07:30,422
a particular SLI that you care about,

112
00:07:30,576 --> 00:07:34,426
like these mentioned ninety ninth percentile of response

113
00:07:34,458 --> 00:07:39,054
time. You might want it to stay within one

114
00:07:39,092 --> 00:07:41,760
hundred and two hundred milliseconds, for example.

115
00:07:42,370 --> 00:07:45,870
Right. That's an objective. And then SLA

116
00:07:45,950 --> 00:07:49,986
is an agreement. It's basically a contract between

117
00:07:50,088 --> 00:07:53,266
two parties when one of the parties provides some kind of

118
00:07:53,288 --> 00:07:57,650
service and they promise that this SLO

119
00:07:57,730 --> 00:08:00,360
is going to be satisfied or else,

120
00:08:00,730 --> 00:08:04,710
and typically the or else is some kind of financial

121
00:08:05,930 --> 00:08:10,146
compensation or something like that. It can be a cool t shirt,

122
00:08:10,258 --> 00:08:13,606
can be anything really. But the idea is that we're

123
00:08:13,638 --> 00:08:17,146
going to do whatever we can to make sure

124
00:08:17,168 --> 00:08:20,654
that that SLO is satisfied. And if it's not, we're going to

125
00:08:20,692 --> 00:08:24,926
make it up to you somehow. Right. So this really

126
00:08:25,108 --> 00:08:29,166
is kind of an example, a sample of what the

127
00:08:29,268 --> 00:08:32,506
SRE is. And some of you

128
00:08:32,548 --> 00:08:36,846
will be like, okay, well that kind of sounds like operations.

129
00:08:36,958 --> 00:08:40,418
And yeah, bingo. That's basically what it is.

130
00:08:40,504 --> 00:08:43,918
It's the operations just with more software

131
00:08:44,014 --> 00:08:47,682
engineering to it. The software engineering

132
00:08:47,746 --> 00:08:51,366
being used to remove the bad bits, to kind

133
00:08:51,388 --> 00:08:54,006
of automate the bad bits away.

134
00:08:54,188 --> 00:08:57,962
And the bad bits have a really funky name, a nice name

135
00:08:58,016 --> 00:09:01,466
of toil. Right? We typically speak about

136
00:09:01,568 --> 00:09:05,514
toil when we talk about SRE. So the deal

137
00:09:05,632 --> 00:09:09,580
for the SREs is that they're going to spend

138
00:09:10,110 --> 00:09:13,798
less of their time dealing with the actual operations,

139
00:09:13,894 --> 00:09:17,422
these on call, the crisis management, and spend

140
00:09:17,476 --> 00:09:21,390
most of their time using their software engineering skills to

141
00:09:21,460 --> 00:09:24,574
roll out automation to automate

142
00:09:24,622 --> 00:09:28,258
the toil away out of these equation. Right.

143
00:09:28,344 --> 00:09:32,180
So they're going to write software to automate things

144
00:09:33,110 --> 00:09:36,994
so that there is less ops and on call to

145
00:09:37,032 --> 00:09:39,620
do to begin with. Right.

146
00:09:39,930 --> 00:09:42,994
That said, there is a lot of hype

147
00:09:43,042 --> 00:09:46,280
about that, but at the core of it, this is what it is.

148
00:09:48,250 --> 00:09:52,394
A lot of the systems are pretty big and pretty amazing and pretty

149
00:09:52,512 --> 00:09:55,878
everything. But you don't have to be at Google to be an SRE.

150
00:09:55,974 --> 00:09:59,226
And there are plenty of systems that need the same kind

151
00:09:59,248 --> 00:10:03,054
of treatment to verify that it runs smooth as

152
00:10:03,092 --> 00:10:05,040
silk. Right.

153
00:10:05,730 --> 00:10:09,770
Okay, so kind of defined SRE.

154
00:10:09,930 --> 00:10:14,050
What about chaos engineering? I'm guessing whats

155
00:10:14,200 --> 00:10:17,742
some of you probably first came into contact with chaos

156
00:10:17,806 --> 00:10:21,262
engineering in the context of chaos monkey.

157
00:10:21,406 --> 00:10:24,974
And that's great, that's fantastic. But it also

158
00:10:25,032 --> 00:10:29,000
kind of gives chaos engineering the bad rep if you google whats

159
00:10:29,770 --> 00:10:33,574
chaos monkey was randomly taking down

160
00:10:33,692 --> 00:10:37,330
vms for Netflix. So whats these can detect

161
00:10:37,410 --> 00:10:41,690
things that they didn't detect with other testing techniques.

162
00:10:42,190 --> 00:10:45,466
But if you google that, you're probably going to end up

163
00:10:45,488 --> 00:10:49,270
with something along the lines, let's breaking things in production

164
00:10:49,350 --> 00:10:52,426
and breaking things on

165
00:10:52,448 --> 00:10:56,080
purpose. And that's great. But the kind of

166
00:10:56,770 --> 00:11:00,334
breaking things is not really what we're after here.

167
00:11:00,452 --> 00:11:04,542
What we're after is experimenting

168
00:11:04,686 --> 00:11:08,050
to verify hypothesis about our

169
00:11:08,120 --> 00:11:11,358
system's behavior in the presence of failure.

170
00:11:11,534 --> 00:11:14,942
So yes, we want to inject the

171
00:11:15,016 --> 00:11:18,838
kind of failure, introduce the kind of failure that we reasonably expect

172
00:11:18,924 --> 00:11:22,486
to happen. But we don't really

173
00:11:22,668 --> 00:11:25,958
try to break the system. We actually try to learn

174
00:11:26,044 --> 00:11:29,706
whats either the system behaves the way that

175
00:11:29,728 --> 00:11:32,410
we expect, that's the hypothesis,

176
00:11:32,750 --> 00:11:35,962
or denying that and learn that

177
00:11:36,016 --> 00:11:39,146
it needs to be fixed. Okay, so at

178
00:11:39,168 --> 00:11:43,514
the core of it, it's much more scientific than just going and randomly

179
00:11:43,642 --> 00:11:46,734
smashing things in production. We actually want

180
00:11:46,772 --> 00:11:51,086
to very finely control the

181
00:11:51,108 --> 00:11:54,866
amount of failure and these type of failure that

182
00:11:54,888 --> 00:11:58,674
we inject most of the time to

183
00:11:58,712 --> 00:12:02,482
verify that what we think is going to happen is

184
00:12:02,536 --> 00:12:05,640
actually happening, right. That we are right.

185
00:12:06,010 --> 00:12:09,894
Thinking about properties of the system. And this is really where

186
00:12:09,932 --> 00:12:13,942
the value comes from. And sure, there is the

187
00:12:13,996 --> 00:12:18,046
aspect of kind of like on the verge of gas

188
00:12:18,098 --> 00:12:22,122
engineering and fuzzing techniques where you

189
00:12:22,256 --> 00:12:26,294
want to create this kind of like half random

190
00:12:26,422 --> 00:12:30,394
pseudorandum situations where you can end

191
00:12:30,432 --> 00:12:34,686
up with combinations that

192
00:12:34,708 --> 00:12:37,966
you didn't think about yourself to test out, but this

193
00:12:37,988 --> 00:12:40,954
is just a part of it, right? So next time you see that it's,

194
00:12:41,002 --> 00:12:44,270
oh, let's randomly scratch things in production,

195
00:12:44,430 --> 00:12:47,970
that's not really what it is about, and at least

196
00:12:48,120 --> 00:12:51,234
it's not really what it's about for everybody.

197
00:12:51,352 --> 00:12:55,102
If this is a good idea for you, if your

198
00:12:55,176 --> 00:12:58,642
production system is of a nature

199
00:12:58,706 --> 00:13:01,686
that allows you to do things like that, that's great.

200
00:13:01,788 --> 00:13:05,698
That's absolutely stunning. If you are at the maturity

201
00:13:05,794 --> 00:13:09,174
level these, you can actually run this kind of thing in production.

202
00:13:09,222 --> 00:13:12,394
That's great. Because if you think

203
00:13:12,432 --> 00:13:16,090
about that, you can never really one hundred

204
00:13:16,160 --> 00:13:20,042
percent attest anything before it

205
00:13:20,096 --> 00:13:23,646
hits production, right? Because you can try very hard to

206
00:13:23,668 --> 00:13:27,022
reproduce the kind of failure that you expect. You can try

207
00:13:27,076 --> 00:13:31,566
very hard to reproduce the same environment in

208
00:13:31,668 --> 00:13:34,858
some kind of test stage, dev stage,

209
00:13:34,954 --> 00:13:38,306
pre production stage. But at the end of the day,

210
00:13:38,408 --> 00:13:42,686
there will be things that will be different. It might vary very slightly,

211
00:13:42,718 --> 00:13:46,594
it might just be like a user pattern, but it's

212
00:13:46,642 --> 00:13:51,650
technically either impossible or prohibitively

213
00:13:51,730 --> 00:13:55,494
expensive to actually do something like that right.

214
00:13:55,612 --> 00:13:58,734
So if you can, this is like the holy grail,

215
00:13:58,802 --> 00:14:02,442
when you run things, kind of things in production and

216
00:14:02,496 --> 00:14:06,154
you can verify and uncover real problem on a real

217
00:14:06,192 --> 00:14:10,060
production system, but it also gives it bad rep because

218
00:14:10,530 --> 00:14:14,382
when people read about that, they stop taking it seriously. It's like,

219
00:14:14,436 --> 00:14:17,678
well, yeah, okay, cool. We would never do it here.

220
00:14:17,844 --> 00:14:22,794
So just kind of want to remind

221
00:14:22,842 --> 00:14:25,780
you that this is the case.

222
00:14:26,470 --> 00:14:29,854
So now we have these two concepts.

223
00:14:29,902 --> 00:14:33,058
We have the SRE on the left hand side and we have

224
00:14:33,064 --> 00:14:36,774
the chaos engineering on the right hand side. And I

225
00:14:36,812 --> 00:14:40,374
would like to argue and spend the rest of this time

226
00:14:40,412 --> 00:14:44,630
that we have together now to kind of see show

227
00:14:44,700 --> 00:14:48,570
to you that where the real magic happens, where the love

228
00:14:48,640 --> 00:14:51,994
happens, is when you start using chaos engineering for your

229
00:14:52,032 --> 00:14:56,250
SRE purposes. Okay, I'm not just saying that

230
00:14:56,400 --> 00:14:59,914
I deeply believe that. In fact, I believe so

231
00:14:59,952 --> 00:15:03,274
deeply in that that I wrote a book about

232
00:15:03,312 --> 00:15:06,402
it. It's called chaos engineering crush.

233
00:15:06,486 --> 00:15:10,126
Test your applications. It's available right now in the

234
00:15:10,148 --> 00:15:13,874
early access from money. If you go to money com looks

235
00:15:13,992 --> 00:15:18,100
chaos engineering. And it's trying to show you that

236
00:15:19,670 --> 00:15:23,058
you don't need a massive distributed system. And if

237
00:15:23,064 --> 00:15:26,850
you have one, that's great. But you can apply

238
00:15:26,920 --> 00:15:30,358
things, chaos engineering techniques to pretty much any system.

239
00:15:30,444 --> 00:15:34,438
It can be as simple as a single process running in a single

240
00:15:34,524 --> 00:15:38,362
computer. I have a chapter. These whats shows you

241
00:15:38,416 --> 00:15:42,426
how you can treat that single process and that single computer

242
00:15:42,608 --> 00:15:46,730
as a system and verify things like

243
00:15:46,880 --> 00:15:50,326
for example, block some system calls and

244
00:15:50,448 --> 00:15:54,654
verify that actually this process as

245
00:15:54,692 --> 00:15:58,174
a system might not behave the way that

246
00:15:58,212 --> 00:16:01,914
you expect it to behave. It might not have the error

247
00:16:01,962 --> 00:16:05,362
handling or the retrieves that you expected it to have.

248
00:16:05,496 --> 00:16:09,326
It might actually work differently. And then it kind of builds

249
00:16:09,358 --> 00:16:11,570
up from the small examples,

250
00:16:12,310 --> 00:16:15,986
looks into how to introduce failures between

251
00:16:16,088 --> 00:16:19,266
components, how to introduce slowness in

252
00:16:19,288 --> 00:16:23,078
between components. On the networking level, it talks about

253
00:16:23,164 --> 00:16:26,722
introducing failure through modifying

254
00:16:26,786 --> 00:16:31,274
code. On the fly. If you happen to be running

255
00:16:31,392 --> 00:16:34,410
something that executes in a JVM,

256
00:16:34,750 --> 00:16:40,038
there is a chapter pardon where

257
00:16:40,064 --> 00:16:43,310
you can learn how to inject bytecode

258
00:16:43,730 --> 00:16:47,358
into your classes without actually modifying the source code.

259
00:16:47,524 --> 00:16:51,326
So you can take someone else's code and inject the

260
00:16:51,348 --> 00:16:55,086
type of failure that you expect and

261
00:16:55,108 --> 00:16:59,602
then verify that the system behaves as a whole in that

262
00:16:59,656 --> 00:17:03,458
manner that you expected. And these, it goes all the way, builds up all

263
00:17:03,464 --> 00:17:07,234
the way to things like Docker, where you test out things running Docker,

264
00:17:07,362 --> 00:17:11,026
or you test out Docker itself. And kubernetes,

265
00:17:11,138 --> 00:17:15,430
if you have larger systems that are running these distributed

266
00:17:16,650 --> 00:17:19,994
and kind of anything in between, or even if you want to

267
00:17:20,032 --> 00:17:23,866
test chaos engineering, test the

268
00:17:23,888 --> 00:17:27,366
kind of failure that you expect in your front end Javascript.

269
00:17:27,478 --> 00:17:31,446
It's really a great tool and

270
00:17:31,568 --> 00:17:35,214
I'm really wanting to show people

271
00:17:35,332 --> 00:17:38,698
that this is something that you can use in many situations, and it's

272
00:17:38,714 --> 00:17:42,094
not just for Netflix and for Google. This is

273
00:17:42,132 --> 00:17:45,300
much more broad than that. Okay,

274
00:17:45,670 --> 00:17:49,518
so these are the things that we just discussed.

275
00:17:49,614 --> 00:17:53,330
And as you can see, apart from

276
00:17:53,400 --> 00:17:58,146
snarky t shirts that don't really need an improvement, typically sres

277
00:17:58,338 --> 00:18:02,358
have them on point. All of these things can

278
00:18:02,444 --> 00:18:05,826
be helped with the use of chaos engineering.

279
00:18:05,938 --> 00:18:09,798
All of these things can be designed,

280
00:18:09,894 --> 00:18:15,130
experiments on, and can be verified

281
00:18:16,990 --> 00:18:21,398
through this experiment, verified in terms of hypothesis

282
00:18:21,494 --> 00:18:24,620
and assumptions that we have about these things.

283
00:18:25,010 --> 00:18:28,862
Okay, so just to show you

284
00:18:28,996 --> 00:18:33,370
these overlap is big, I kind of felt like the previous slide

285
00:18:33,450 --> 00:18:36,786
was showing the overlap as a little bit tiny. So I kind of

286
00:18:36,808 --> 00:18:40,434
zoomed in here and I would

287
00:18:40,472 --> 00:18:44,126
like to now show you a little bit more in practice,

288
00:18:44,238 --> 00:18:48,866
what I actually mean about when I say that the

289
00:18:48,888 --> 00:18:52,274
case engineering can be leveraged for SRE. So let's jump

290
00:18:52,322 --> 00:18:56,694
to my first demo. What you're seeing here is the

291
00:18:56,732 --> 00:18:59,562
VM that comes with my book.

292
00:18:59,696 --> 00:19:03,382
It's more or less vanilla ubuntu

293
00:19:03,446 --> 00:19:07,254
with all these things that you need for KS engineering pre installed

294
00:19:07,302 --> 00:19:10,734
here. I'm going to use one of the examples that

295
00:19:10,772 --> 00:19:14,426
I have here. This is coming from a chapter

296
00:19:14,458 --> 00:19:18,474
on Docker, and it's a descriptor for Docker Swarm

297
00:19:18,522 --> 00:19:22,426
or Docker stack that describes two services.

298
00:19:22,548 --> 00:19:26,290
One of them is called Ghost and it basically just runs the

299
00:19:26,360 --> 00:19:30,274
image for Ghost and then provides some

300
00:19:30,312 --> 00:19:34,046
configuration for the database and the database

301
00:19:34,078 --> 00:19:37,750
itself, which is mysql five point seven

302
00:19:37,900 --> 00:19:41,798
with not a particularly safe password. So the thing

303
00:19:41,884 --> 00:19:45,350
that what it does is basically

304
00:19:45,420 --> 00:19:48,950
start these two containers with the

305
00:19:49,020 --> 00:19:52,266
configuration two point each other so that we

306
00:19:52,288 --> 00:19:56,218
can run ghost. And if you take a look, I actually already

307
00:19:56,304 --> 00:19:59,754
have it running. I already run

308
00:19:59,792 --> 00:20:03,786
the docker stack and you can see my ghost container

309
00:20:03,898 --> 00:20:07,742
and you can see my MySQL container. If you're not familiar with

310
00:20:07,796 --> 00:20:10,734
ghost, it's a blogging engine.

311
00:20:10,852 --> 00:20:14,530
It's a little bit like WordPress, but just a little bit more

312
00:20:14,600 --> 00:20:18,274
modern. Also note that the

313
00:20:18,312 --> 00:20:21,982
names of the containers, one of them starts with Meower Ghost

314
00:20:22,046 --> 00:20:25,686
and the other starts with MeowDB because we're going to

315
00:20:25,708 --> 00:20:30,854
use these names later on. So first

316
00:20:30,892 --> 00:20:34,134
thing we should do is actually verify whats this thing is

317
00:20:34,172 --> 00:20:38,280
working. So we should be able to go to

318
00:20:39,210 --> 00:20:42,762
one hundred and twenty seven to

319
00:20:42,816 --> 00:20:46,060
port eighty, eighty and we should be seeing the application.

320
00:20:46,750 --> 00:20:49,980
And boom, looks like it's actually working.

321
00:20:50,290 --> 00:20:53,934
Okay, so this is great. We get something,

322
00:20:54,052 --> 00:20:57,214
but what we actually care about is some

323
00:20:57,252 --> 00:21:00,986
kind of sli, some kind of metric

324
00:21:01,098 --> 00:21:04,146
that we care about. And we want to

325
00:21:04,168 --> 00:21:07,970
make sure that we satisfy because that's how we do as sres.

326
00:21:08,710 --> 00:21:12,174
Okay, so one of the most basic

327
00:21:12,222 --> 00:21:16,450
things that we can do is use something like Apache benchmark

328
00:21:16,610 --> 00:21:21,058
to basically create a lot of requests

329
00:21:21,154 --> 00:21:24,966
and verify how quickly this request return.

330
00:21:25,148 --> 00:21:28,890
This is just running ab with ten seconds

331
00:21:29,870 --> 00:21:33,354
and concurrency of one to the one

332
00:21:33,392 --> 00:21:36,826
twenty seven, zero, zero, one eighty, eighty. So that we get an

333
00:21:36,848 --> 00:21:40,670
idea. So you can see that during this ten seconds we got

334
00:21:40,740 --> 00:21:44,862
one hundred and five requests that were complete and

335
00:21:44,996 --> 00:21:48,634
we got no failed request, which is also great, which translates

336
00:21:48,682 --> 00:21:52,058
into ninety five milliseconds per request,

337
00:21:52,234 --> 00:21:55,782
which is, let's be honest here, running a local host,

338
00:21:55,866 --> 00:21:59,106
not the record of the world, but it's not too

339
00:21:59,128 --> 00:22:02,450
bad either. Okay, so this thing,

340
00:22:02,600 --> 00:22:06,242
what we just did in the chaos engineering parlance

341
00:22:06,306 --> 00:22:11,046
would be called steady state things, is what the

342
00:22:11,068 --> 00:22:14,550
metric that we have, which is time per request.

343
00:22:14,970 --> 00:22:19,110
An Sli time per request is

344
00:22:19,180 --> 00:22:22,602
roughly ninety five. If we run it again, we're probably going to get

345
00:22:22,656 --> 00:22:26,138
slightly different number. But I would expect that this is not going to

346
00:22:26,144 --> 00:22:29,466
be very different because we didn't change anything.

347
00:22:29,648 --> 00:22:33,450
Okay, so let's just finish the ten seconds.

348
00:22:34,350 --> 00:22:37,914
Actually, now it's fifty milliseconds. I guess it was warming

349
00:22:37,962 --> 00:22:41,326
up a little bit, in which case I'm going

350
00:22:41,348 --> 00:22:44,898
to run it again so that we can actually verify that

351
00:22:45,064 --> 00:22:49,086
the steady state is okay. Ten seconds

352
00:22:49,118 --> 00:22:52,446
is not particularly long. So it looks like there was some cache

353
00:22:52,478 --> 00:22:58,134
warm up. And now we have about fifty milliseconds time

354
00:22:58,172 --> 00:23:01,334
per request. Brilliant. All right,

355
00:23:01,372 --> 00:23:06,086
so it would be a shame now if someone

356
00:23:06,188 --> 00:23:09,754
went ahead and introduced some failure, right? So we have two

357
00:23:09,792 --> 00:23:13,722
components. We have the database and the

358
00:23:13,776 --> 00:23:17,594
engine, the blogging engine. And what

359
00:23:17,632 --> 00:23:21,262
we might want to do is just see what happens if we

360
00:23:21,316 --> 00:23:25,070
introduce some slowness, some delay

361
00:23:25,890 --> 00:23:29,178
between the two at runtime.

362
00:23:29,274 --> 00:23:32,880
Right? And as it turns out, it's actually pretty simple.

363
00:23:33,670 --> 00:23:37,490
We can do it pretty easily. One of the things that make it easy

364
00:23:37,640 --> 00:23:41,662
is this tool called Pumba. It's an open source resilience

365
00:23:41,726 --> 00:23:45,254
tool that essentially, apart from doing things

366
00:23:45,292 --> 00:23:49,506
like killing containers, wraps up TC

367
00:23:49,618 --> 00:23:53,720
traffic control Linux utility in a really cool way.

368
00:23:54,170 --> 00:23:57,686
So I'm not going

369
00:23:57,708 --> 00:24:01,306
to go too much into the detail, but I just

370
00:24:01,328 --> 00:24:04,666
wanted to know that what you can do is you

371
00:24:04,688 --> 00:24:08,682
can ask Pumbaa to actually run these TC from

372
00:24:08,736 --> 00:24:12,474
inside of a container that's attached to the container

373
00:24:12,522 --> 00:24:17,082
that you care about to introduce slowness.

374
00:24:17,226 --> 00:24:21,022
So what we want to do is we want to

375
00:24:21,076 --> 00:24:25,346
introduce slowness on this container and

376
00:24:25,368 --> 00:24:29,262
we're going to go ahead and actually introduce that to all the networking.

377
00:24:29,406 --> 00:24:32,706
And with Pumba that's pretty simple.

378
00:24:32,808 --> 00:24:36,440
Actually. I already have

379
00:24:36,890 --> 00:24:40,914
a command here, whats I use, but I use this NETM

380
00:24:40,962 --> 00:24:44,454
subcommand. We're going to run it for one hundred and

381
00:24:44,492 --> 00:24:48,630
twenty seconds the entire experiment.

382
00:24:49,050 --> 00:24:53,422
We're going to specify a TC image because Pumba

383
00:24:53,506 --> 00:24:57,386
allows you to either rely on TC being

384
00:24:57,488 --> 00:25:01,714
available in that container that you're targeting,

385
00:25:01,862 --> 00:25:05,582
or if you're using someone else's container or

386
00:25:05,636 --> 00:25:08,750
you just don't want to have TC there, you can start another

387
00:25:08,820 --> 00:25:12,298
container like I just described. Like for example this one,

388
00:25:12,484 --> 00:25:16,354
that chaos TC built in to connect to that other

389
00:25:16,392 --> 00:25:19,854
container and execute

390
00:25:19,902 --> 00:25:23,998
TC in there. And then we're going to add a delay

391
00:25:24,094 --> 00:25:28,226
time delay of one hundred milliseconds. And we're going to ignore

392
00:25:28,258 --> 00:25:31,382
jitter and correlation for now both

393
00:25:31,436 --> 00:25:34,840
set to zero so that we can

394
00:25:35,770 --> 00:25:39,542
just sre the results more easily. And then the nice feature

395
00:25:39,606 --> 00:25:43,226
is that you can specify these the container name or if

396
00:25:43,248 --> 00:25:46,410
you prepend it with re two colon,

397
00:25:46,990 --> 00:25:49,978
you can use regular expression. So for example,

398
00:25:50,064 --> 00:25:54,080
my Meower underscore DB is going to match everything

399
00:25:54,850 --> 00:25:58,238
that starts with it or includes that. So in particular

400
00:25:58,404 --> 00:26:01,870
these name of Meowerdb that we were looking at before,

401
00:26:02,020 --> 00:26:05,314
the one here is going to be matched. All right.

402
00:26:05,352 --> 00:26:08,754
So I'm going to go ahead and run it. And what

403
00:26:08,792 --> 00:26:11,630
it should do is start that other container,

404
00:26:11,790 --> 00:26:16,900
execute the stuff and end.

405
00:26:18,250 --> 00:26:22,006
So I'm going to start another tab so

406
00:26:22,028 --> 00:26:25,734
that we can see what's going on. And the

407
00:26:25,772 --> 00:26:29,482
funny thing is, an interesting thing is that you see

408
00:26:29,616 --> 00:26:33,734
these actual container being created and exiting nineteen

409
00:26:33,782 --> 00:26:37,910
seconds ago. And that container

410
00:26:37,990 --> 00:26:41,054
executes DC. And when this command is done,

411
00:26:41,172 --> 00:26:44,858
it's actually going to go ahead and execute another container

412
00:26:45,034 --> 00:26:48,846
that's also going to be visible here.

413
00:26:48,948 --> 00:26:52,282
So I'm going to go ahead and I'm going to rerun

414
00:26:52,346 --> 00:26:56,046
the same AB on port eighty

415
00:26:56,078 --> 00:27:00,434
eighty to verify our state right now with

416
00:27:00,472 --> 00:27:02,850
the one hundred milliseconds added.

417
00:27:09,410 --> 00:27:13,230
And boom, look, we went from fifty

418
00:27:13,380 --> 00:27:16,990
milliseconds roughly to five

419
00:27:17,060 --> 00:27:20,786
hundred milliseconds. So from roughly two

420
00:27:20,808 --> 00:27:24,450
hundred requests in ten seconds to

421
00:27:24,520 --> 00:27:27,746
just eighteen. So what happened here, right,

422
00:27:27,848 --> 00:27:31,774
you could have expected, we can also verify that rerun

423
00:27:31,822 --> 00:27:35,218
whats just to make sure that we get consistent results.

424
00:27:35,314 --> 00:27:39,430
But what happened is whats you might have

425
00:27:39,500 --> 00:27:44,406
expected, the one hundred milliseconds in

426
00:27:44,428 --> 00:27:48,378
between the database and these

427
00:27:48,464 --> 00:27:51,946
ghost container to translate into an

428
00:27:51,968 --> 00:27:55,706
extra one hundred milliseconds of delay to

429
00:27:55,728 --> 00:27:59,294
the user. But what actually happened is that we

430
00:27:59,332 --> 00:28:02,990
got almost five hundred milliseconds of

431
00:28:03,060 --> 00:28:06,414
delay. So we got a multiple of

432
00:28:06,452 --> 00:28:10,550
that. And the reason for that is that ghost

433
00:28:10,730 --> 00:28:14,066
probably talks to the database more than once,

434
00:28:14,168 --> 00:28:18,290
even for the index page that we are querying.

435
00:28:18,790 --> 00:28:22,366
And that means that if that container's

436
00:28:22,398 --> 00:28:25,538
networking is slowed down by one hundred milliseconds, what we're

437
00:28:25,554 --> 00:28:29,302
actually going to see is closer to five hundred

438
00:28:29,356 --> 00:28:32,438
milliseconds delay. So just to confirm these,

439
00:28:32,524 --> 00:28:36,326
I'm going to run it again so that we can see whats we're

440
00:28:36,358 --> 00:28:39,866
back to fifty milliseconds because the pumba setup is

441
00:28:39,888 --> 00:28:40,460
done.

442
00:28:44,190 --> 00:28:47,882
So about sixty milliseconds, which is good enough. And then

443
00:28:47,936 --> 00:28:51,950
if you look at the Docker PSA,

444
00:28:52,450 --> 00:28:56,126
you can see that we have the other one, which the

445
00:28:56,148 --> 00:28:59,294
first one was TcQdisk add, and then the

446
00:28:59,332 --> 00:29:04,434
next one was TCQdisc delete that

447
00:29:04,552 --> 00:29:08,274
exited thirty nine seconds ago.

448
00:29:08,392 --> 00:29:12,580
Okay. This is how Pumba was able to actually

449
00:29:13,210 --> 00:29:16,854
affect the networking of the

450
00:29:16,892 --> 00:29:20,454
container that was running an image that

451
00:29:20,492 --> 00:29:24,054
we didn't instrument in any way.

452
00:29:24,252 --> 00:29:28,070
So this is like a really very short version

453
00:29:28,670 --> 00:29:32,362
of this demo. But my goal here was to just kind of show

454
00:29:32,416 --> 00:29:36,582
you how easy it can be with the right tooling and the right knowledge

455
00:29:36,726 --> 00:29:40,800
to verify things like that. And now we know that

456
00:29:41,170 --> 00:29:44,990
if we can expect reasonably,

457
00:29:46,130 --> 00:29:49,806
the database networking to have delays like

458
00:29:49,828 --> 00:29:53,810
one hundred milliseconds, that will affect our overall

459
00:29:54,630 --> 00:29:58,430
delay for the ghost

460
00:29:58,510 --> 00:30:01,614
setup by much more than one hundred milliseconds,

461
00:30:01,662 --> 00:30:05,458
and in particular by something closer to five hundred milliseconds.

462
00:30:05,554 --> 00:30:08,966
So if the delay rose to a second, we could probably

463
00:30:09,068 --> 00:30:12,534
expect to actually sre something closer to

464
00:30:12,572 --> 00:30:16,090
five seconds rather than just one. Okay, and this is my second

465
00:30:16,160 --> 00:30:20,314
demo where I would like to show you

466
00:30:20,512 --> 00:30:23,914
a little bit more on the Kubernetes side

467
00:30:23,952 --> 00:30:27,674
of things for all the people who are

468
00:30:27,712 --> 00:30:31,246
using Kubernetes to deploy their software. So let's take a

469
00:30:31,268 --> 00:30:35,354
quick look at slos and Kubernetes. The purpose

470
00:30:35,402 --> 00:30:39,246
of the second demo is to show you how

471
00:30:39,348 --> 00:30:43,086
useful chaos engineering can be for sres

472
00:30:43,278 --> 00:30:46,766
to verify their slos and to detect

473
00:30:46,878 --> 00:30:49,730
breaches in their slos.

474
00:30:50,230 --> 00:30:54,322
So I've got here mini cube set

475
00:30:54,376 --> 00:30:57,762
up, just a basic one with a single master

476
00:30:57,826 --> 00:31:01,474
here, and I've got a bunch of pods

477
00:31:01,522 --> 00:31:05,330
running. Also, nothing out of extraordinary.

478
00:31:05,410 --> 00:31:09,494
This is just the stuff that minikube starts

479
00:31:09,542 --> 00:31:10,140
with.

480
00:31:13,310 --> 00:31:17,482
And what I'm going to do is I'm going to use a tool

481
00:31:17,616 --> 00:31:21,470
called powerful seal. It's something

482
00:31:21,540 --> 00:31:24,654
that I wrote a while back,

483
00:31:24,852 --> 00:31:28,154
and it's a tool for chaos engineering

484
00:31:28,202 --> 00:31:31,842
for kubernetes specifically. If you've not

485
00:31:31,896 --> 00:31:35,266
used things before, I recommend going to GitHub and giving it a

486
00:31:35,288 --> 00:31:38,526
try. But basically what it does is that it allows

487
00:31:38,558 --> 00:31:42,542
you to write this yaml descriptions

488
00:31:42,686 --> 00:31:46,386
of scenarios. And then for each of these scenarios

489
00:31:46,418 --> 00:31:50,114
you can configure a bunch of things that you can do to verify

490
00:31:50,242 --> 00:31:53,334
that your assumptions are correct. And if they're not

491
00:31:53,372 --> 00:31:57,420
correct it's going to error and you can alert on that.

492
00:31:58,510 --> 00:32:01,786
So if you want to get started with that,

493
00:32:01,888 --> 00:32:06,106
there's a get started click

494
00:32:06,208 --> 00:32:10,182
quick little tutorial. But the kind of

495
00:32:10,336 --> 00:32:14,314
most important stuff is about writing policies in things section

496
00:32:14,362 --> 00:32:17,966
here, where you can see the different examples of the different policies that

497
00:32:17,988 --> 00:32:21,902
you can see that you can implement using

498
00:32:21,956 --> 00:32:25,650
powerful Sil. And if you are

499
00:32:25,720 --> 00:32:28,500
wondering what the syntax looks like,

500
00:32:28,870 --> 00:32:33,390
there is an up to date, automatically generated documentation

501
00:32:33,470 --> 00:32:37,542
that shows you what kind of things you can do. So if you do

502
00:32:37,596 --> 00:32:41,190
scenarios sres, then you can see the kind of things that

503
00:32:41,260 --> 00:32:44,130
are available to you. So probe, HTTP,

504
00:32:44,290 --> 00:32:47,994
kubectl, production node, action weight and stuff like

505
00:32:48,032 --> 00:32:51,850
that. But this is for another day. I just wanted to kind of give you

506
00:32:52,000 --> 00:32:55,306
a quick insight into where

507
00:32:55,328 --> 00:32:59,294
to look for those kind of things. But let's take

508
00:32:59,332 --> 00:33:02,734
a look at whats actually looks like in

509
00:33:02,772 --> 00:33:06,970
action. So back to our little mini cube setup.

510
00:33:07,130 --> 00:33:10,480
I have a seal already available here

511
00:33:10,950 --> 00:33:14,500
that I preinstalled and

512
00:33:15,030 --> 00:33:19,470
I also prepared two little examples

513
00:33:19,550 --> 00:33:22,786
of a policy. So I'm going to start with

514
00:33:22,808 --> 00:33:26,502
these. Hello world. And this is whats it looks like.

515
00:33:26,636 --> 00:33:30,274
It's a simple yaml with scenario,

516
00:33:30,402 --> 00:33:33,686
a single scenario called count pods, not in

517
00:33:33,708 --> 00:33:36,810
the running state. And what it does is that

518
00:33:36,880 --> 00:33:40,678
it has a single step with pod action.

519
00:33:40,854 --> 00:33:44,394
And inside of the pod action there's always these things

520
00:33:44,432 --> 00:33:48,454
that you can do. You match a certain initial set of

521
00:33:48,592 --> 00:33:52,494
pods, you can filter them by

522
00:33:52,692 --> 00:33:56,382
whatever property or whatever filters that you feel like.

523
00:33:56,516 --> 00:33:59,966
So in our case, I'm going to match all the pods from all the

524
00:33:59,988 --> 00:34:03,746
namespaces, and then I'm going to pick the ones

525
00:34:03,848 --> 00:34:07,774
that have the state property that is negative,

526
00:34:07,902 --> 00:34:11,138
not running. And then I'm going to count these

527
00:34:11,224 --> 00:34:14,630
and I'm going to verify that the count is always zero.

528
00:34:14,780 --> 00:34:17,974
So what it's going to do for me, we show the

529
00:34:18,012 --> 00:34:20,600
git pods, all of them were running.

530
00:34:21,050 --> 00:34:23,830
So this kind of verification,

531
00:34:24,650 --> 00:34:27,994
very simplistic here, shows how you can kind

532
00:34:28,032 --> 00:34:31,946
of continuously verify that the assumption that

533
00:34:31,968 --> 00:34:35,642
you make the assumption being all pods sre running is actually

534
00:34:35,696 --> 00:34:39,646
true, and you can do that with the twenty lines of yaml. So in

535
00:34:39,668 --> 00:34:44,030
order to run this, we're just going to do seal autonomous.

536
00:34:44,690 --> 00:34:48,206
To invoke the autonomous mode and

537
00:34:48,228 --> 00:34:51,914
these we need to specify the policy file

538
00:34:52,042 --> 00:34:55,950
and this is simply done by the policy file

539
00:34:56,290 --> 00:35:00,306
flag. If we run that powerful seal is

540
00:35:00,328 --> 00:35:04,262
going to connect our cluster. And whats you can see here is that it

541
00:35:04,316 --> 00:35:07,638
matched the namespace star, so it matched all the

542
00:35:07,644 --> 00:35:11,014
namespaces matched eleven pods in that

543
00:35:11,212 --> 00:35:13,894
corresponding to the pod that we found here.

544
00:35:14,092 --> 00:35:18,138
And it found an empty set after the

545
00:35:18,224 --> 00:35:21,722
running negative true. So the filtered set

546
00:35:21,776 --> 00:35:26,038
length is zero and the scenario finished successfully

547
00:35:26,214 --> 00:35:29,774
by default. It's also going to go ahead and sleep for

548
00:35:29,812 --> 00:35:34,282
some time and retry that later on, which is also configurable.

549
00:35:34,426 --> 00:35:37,902
So if we just wanted to verify that this

550
00:35:37,956 --> 00:35:42,190
is actually working, what we could do is remove the negative

551
00:35:42,350 --> 00:35:45,618
and verify that it's failing. If we

552
00:35:45,784 --> 00:35:49,330
try to count the running state and the checkpoint,

553
00:35:50,790 --> 00:35:54,514
the count is not zero. So if we run it

554
00:35:54,552 --> 00:35:57,170
now, this should fail.

555
00:35:57,530 --> 00:36:01,026
Complaining whats we got? Eleven pods

556
00:36:01,058 --> 00:36:04,918
instead of zero, which is exactly what we saw here.

557
00:36:05,084 --> 00:36:08,374
And you can configure powerful seal to either fail

558
00:36:08,502 --> 00:36:11,514
quickly if this happens, or if you want

559
00:36:11,552 --> 00:36:15,210
this kind of ongoing stuff, it can produce metrics that you can later

560
00:36:15,280 --> 00:36:18,826
scrape. So with just a few lines

561
00:36:18,858 --> 00:36:23,070
of yaml we're able to verify NSLO,

562
00:36:23,650 --> 00:36:26,910
which is kind of silly, all pods running,

563
00:36:27,060 --> 00:36:30,522
but gives you an example of what you can do

564
00:36:30,676 --> 00:36:34,338
with this kind of thing. So let's do another example,

565
00:36:34,504 --> 00:36:38,542
a little bit more complex than that. I prepared

566
00:36:38,606 --> 00:36:42,514
another one called policy one for you here and

567
00:36:42,552 --> 00:36:45,958
let's take a look. So this time we

568
00:36:46,044 --> 00:36:49,026
actually specify these run strategy.

569
00:36:49,218 --> 00:36:53,218
So we want to just run it once we got an exit strategy

570
00:36:53,314 --> 00:36:56,662
fail fast. And the scenario is a little bit more complex

571
00:36:56,726 --> 00:37:00,298
this time. So what I'm trying to verify here

572
00:37:00,384 --> 00:37:03,850
is that the deployment SLO is

573
00:37:03,920 --> 00:37:07,690
that after a new deployment and a service are scheduled,

574
00:37:08,270 --> 00:37:12,106
it can be called within thirty seconds. So let's

575
00:37:12,138 --> 00:37:16,346
say that I designed my Kubernetes setup and I designed

576
00:37:16,538 --> 00:37:20,378
all of the bricks in a way that I am fairly confident

577
00:37:20,554 --> 00:37:24,354
that at any given time, when I schedule a new

578
00:37:24,392 --> 00:37:28,334
deployment and a corresponding service, within thirty seconds

579
00:37:28,382 --> 00:37:31,922
everything will be up and running and I'll be able to actually call

580
00:37:31,976 --> 00:37:36,114
it. So the way that I implement that is through the Kubectl

581
00:37:36,162 --> 00:37:39,462
action. Kubectl action lets you more or less

582
00:37:39,516 --> 00:37:44,194
specify the payload and these action. So apply or delete.

583
00:37:44,322 --> 00:37:47,866
It's an equivalent to Kubectl, apply f

584
00:37:47,968 --> 00:37:50,090
of the standard input.

585
00:37:51,710 --> 00:37:54,860
It also allows you to

586
00:37:55,550 --> 00:37:59,246
automatically delete at the end so that you can clean up and

587
00:37:59,268 --> 00:38:03,498
so that you don't leave some kind of artifact

588
00:38:03,594 --> 00:38:07,006
after you're done with that. So the payload here is a little bit

589
00:38:07,028 --> 00:38:11,874
more complex. It's actually deploying another application

590
00:38:12,072 --> 00:38:17,474
that I wrote that is very

591
00:38:17,512 --> 00:38:21,166
useful for kubernetes. It's called Goldpinger.

592
00:38:21,278 --> 00:38:24,546
And Goldpinger allows you to basically

593
00:38:24,648 --> 00:38:28,354
deploy an instance of Goldpinger per

594
00:38:28,392 --> 00:38:31,510
node by using a demo set typically.

595
00:38:32,090 --> 00:38:35,910
Or you can deploy it more or less

596
00:38:36,060 --> 00:38:38,602
whenever you like, whichever way you like.

597
00:38:38,736 --> 00:38:42,282
But the default use case is that use a demo set

598
00:38:42,336 --> 00:38:45,580
so that you run an instance of Goldfinger per

599
00:38:46,110 --> 00:38:50,062
node and these, they continuously create

600
00:38:50,196 --> 00:38:53,870
this full graph of connectivity

601
00:38:54,210 --> 00:38:57,662
between these nodes. Whats you can

602
00:38:57,716 --> 00:39:02,278
use to verify whether there is any issues connecting

603
00:39:02,394 --> 00:39:06,050
on whether your networking is slower between

604
00:39:06,120 --> 00:39:09,474
certain nodes and stuff like that. So this is like a

605
00:39:09,512 --> 00:39:13,006
drop in that you can run on your cluster and you can

606
00:39:13,048 --> 00:39:16,194
verify this kind of things. It also produces metrics

607
00:39:16,242 --> 00:39:19,686
and things like heat maps and stuff like

608
00:39:19,708 --> 00:39:22,854
that. But going

609
00:39:22,892 --> 00:39:26,618
back to our example, in order for that to work, it is a

610
00:39:26,624 --> 00:39:30,614
service account so that it can list pods,

611
00:39:30,662 --> 00:39:34,874
so that every Goldpinger instance can actually see what

612
00:39:34,912 --> 00:39:39,100
other Goldpinger instances are there to send

613
00:39:39,550 --> 00:39:43,594
pings to. And then we've got the deployment and the deployment

614
00:39:43,642 --> 00:39:47,038
is fairly standard. Right now I only have a

615
00:39:47,044 --> 00:39:50,510
single node, so I'm just going to deploy a single replica.

616
00:39:50,850 --> 00:39:54,674
It has a selector, it uses service account that we just set

617
00:39:54,712 --> 00:39:58,574
up and a bunch of variables here that are not particularly relevant

618
00:39:58,622 --> 00:40:02,194
to us right now. This is just to make sure that things

619
00:40:02,312 --> 00:40:06,066
working. It also comes with a liveness probe and readiness probe,

620
00:40:06,098 --> 00:40:09,446
so that we know that if

621
00:40:09,468 --> 00:40:13,202
we can ping it, whats means that it was able to verify

622
00:40:13,266 --> 00:40:16,630
the probe initially. And finally we've got

623
00:40:16,700 --> 00:40:20,922
a gold pinger service,

624
00:40:21,056 --> 00:40:24,618
a service that we're going to use to actually

625
00:40:24,704 --> 00:40:28,366
issue a request. And then after that, this is where

626
00:40:28,388 --> 00:40:31,946
our slo kicks in. We verify

627
00:40:32,058 --> 00:40:35,098
that after thirty seconds.

628
00:40:35,194 --> 00:40:38,714
We expect that. So the magic number here, our magic

629
00:40:38,762 --> 00:40:43,122
range is between zero and thirty seconds. And finally

630
00:40:43,256 --> 00:40:47,490
this is where the verification happens. We have an HTTP probe

631
00:40:47,910 --> 00:40:51,314
that calls the helps the endpoint of

632
00:40:51,512 --> 00:40:55,094
the Goldpinger service in the

633
00:40:55,132 --> 00:40:59,382
default namespace, which is the one that we defined just

634
00:40:59,436 --> 00:41:02,614
here. So all in all what it's going to do, it's going

635
00:41:02,652 --> 00:41:05,730
to go create the thing,

636
00:41:05,900 --> 00:41:09,478
wait thirty seconds and then issue these HTTP

637
00:41:09,574 --> 00:41:12,838
request to verify that it gets a response

638
00:41:13,014 --> 00:41:16,762
on the particular port. Okay, so with

639
00:41:16,816 --> 00:41:20,094
that we can go ahead and this

640
00:41:20,132 --> 00:41:24,430
time instead of hello world we're just going to run the policy one.

641
00:41:24,500 --> 00:41:28,302
But before we do that actually just to show

642
00:41:28,356 --> 00:41:33,826
you, we're going to do get pod aw in

643
00:41:33,848 --> 00:41:37,534
the background so that we get all the new pods

644
00:41:37,582 --> 00:41:40,706
that come up and all the paths that are

645
00:41:40,728 --> 00:41:44,150
being terminated. So whats it's actually visible to you too.

646
00:41:44,300 --> 00:41:47,894
So again our seal and we have these

647
00:41:47,932 --> 00:41:51,526
policy one yaml I'm just

648
00:41:51,548 --> 00:41:55,886
going to go ahead and run it. So it starts,

649
00:41:55,938 --> 00:41:59,162
it read the scenario. You can see that it started

650
00:41:59,296 --> 00:42:02,940
created the deployment and here our

651
00:42:03,790 --> 00:42:07,434
kubectl in the background is actually displaying these new

652
00:42:07,472 --> 00:42:12,560
pod that is already running after four seconds and

653
00:42:13,250 --> 00:42:17,230
now we've got about twenty five seconds to wait.

654
00:42:17,380 --> 00:42:21,122
So if there was some kind of elevator music

655
00:42:21,176 --> 00:42:24,574
that will be good. It's running for twenty five seconds

656
00:42:24,622 --> 00:42:27,810
so we're not that far off. And these making

657
00:42:27,880 --> 00:42:31,582
a call, powerful shield.

658
00:42:31,646 --> 00:42:35,282
Try to make the call. It got a response. You can see

659
00:42:35,336 --> 00:42:39,554
the response generated by a gold finger scenario finished,

660
00:42:39,682 --> 00:42:42,966
cleanup started. As you can see that's the thing, whats I

661
00:42:42,988 --> 00:42:46,682
was describing before the auto delete, it deletes all the things

662
00:42:46,816 --> 00:42:52,502
the pod gets terminated and powerful

663
00:42:52,566 --> 00:42:54,620
seal carries on.

664
00:42:55,870 --> 00:43:00,254
So if we list our pods again we can see that

665
00:43:00,452 --> 00:43:03,962
it's actually terminated already. The Goldfinger.

666
00:43:04,106 --> 00:43:07,546
And if you run this continuously you'll

667
00:43:07,578 --> 00:43:11,458
be able to verify that your slo of thirty seconds for

668
00:43:11,544 --> 00:43:15,246
a new pod coming up is actually being satisfied

669
00:43:15,358 --> 00:43:18,642
or not depending on what's going on.

670
00:43:18,776 --> 00:43:22,574
So I don't want it to be too deep

671
00:43:22,622 --> 00:43:26,646
of a dive but if

672
00:43:26,668 --> 00:43:30,342
you want to dive deeper that's absolutely great.

673
00:43:30,476 --> 00:43:33,830
I would recommend going to the powerful seal

674
00:43:34,170 --> 00:43:38,010
documentation back in the browser here and

675
00:43:38,160 --> 00:43:42,022
just at least go through the different examples here we have like the new pod

676
00:43:42,086 --> 00:43:46,026
startup, we get the pod reschedule where we

677
00:43:46,128 --> 00:43:49,982
actually go ahead and we kill a pod and then we wait

678
00:43:50,036 --> 00:43:54,080
a certain amount of time and then we verify that the pod is running.

679
00:43:54,930 --> 00:43:58,202
Powerful silk can also integrate

680
00:43:58,266 --> 00:44:03,566
with cloud providers. So things like Aws,

681
00:44:03,758 --> 00:44:06,130
Azure, OpenStack,

682
00:44:07,110 --> 00:44:10,354
Google Cloud, there are drivers for that.

683
00:44:10,552 --> 00:44:16,054
So you can say things like node action like

684
00:44:16,092 --> 00:44:19,558
this. You can say for example

685
00:44:19,644 --> 00:44:22,806
pick all the masters, pick the masters that

686
00:44:22,828 --> 00:44:26,986
are up and take a random sample of size one to just take a

687
00:44:27,008 --> 00:44:30,682
single master that is up and stop that

688
00:44:30,736 --> 00:44:34,218
thing. And then we can verify that

689
00:44:34,384 --> 00:44:38,038
things continue working the way that we want it. And if you

690
00:44:38,064 --> 00:44:41,754
want you can put them back up explicitly

691
00:44:41,802 --> 00:44:45,806
like that. Or there's also in the stop action you

692
00:44:45,828 --> 00:44:49,418
can do auto restart et cetera.

693
00:44:49,514 --> 00:44:53,374
Et cetera. And that's all

694
00:44:53,412 --> 00:44:57,134
I had for you today. Once again,

695
00:44:57,332 --> 00:45:00,670
go grab my book. If you want to reach out,

696
00:45:00,740 --> 00:45:04,326
there's my contact details available there. If you have

697
00:45:04,348 --> 00:45:08,098
any questions, I'm happy to chat. And hopefully

698
00:45:08,194 --> 00:45:12,118
I'm going to just leave you with this new tool that you can use.

699
00:45:12,284 --> 00:45:16,086
And if you are an SRE, you should be using it.

700
00:45:16,188 --> 00:45:19,494
If you're not an SRE and you would like to become one, this is

701
00:45:19,532 --> 00:45:23,286
something that's going to help you with that. Thank you very much

702
00:45:23,388 --> 00:45:24,500
and see you next time.

