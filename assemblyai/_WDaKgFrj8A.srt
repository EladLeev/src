1
00:00:39,250 --> 00:00:43,154
Hello everyone and welcome to this deep

2
00:00:43,202 --> 00:00:47,030
dive. Talk about the Kubernetes operators.

3
00:00:48,170 --> 00:00:51,742
Well, let's begin. Well before

4
00:00:51,796 --> 00:00:56,026
we go forward, I think it's

5
00:00:56,058 --> 00:01:00,560
always a good idea to give ourselves a quick primer about

6
00:01:00,930 --> 00:01:04,866
Kubernetes architecture, right? How kubernetes is

7
00:01:04,888 --> 00:01:08,674
constructed, how it is being laid out. So we all know

8
00:01:08,792 --> 00:01:12,194
that kubernetes can be

9
00:01:12,232 --> 00:01:15,954
seen as a composition of two different categories

10
00:01:16,002 --> 00:01:19,622
of workloads, right? These is

11
00:01:19,676 --> 00:01:23,810
the control plane which you can relate

12
00:01:23,890 --> 00:01:27,374
to like a brain behind your Kubernetes

13
00:01:27,442 --> 00:01:30,620
cluster. That's where all the intelligence is built.

14
00:01:31,310 --> 00:01:35,274
And then you have the worker nodes, also called

15
00:01:35,312 --> 00:01:38,714
as data plane. This is where your

16
00:01:38,912 --> 00:01:41,998
custom applications, custom workloads are

17
00:01:42,164 --> 00:01:45,226
designated to run. Now control plane

18
00:01:45,258 --> 00:01:49,194
is actually an overarching term for a collection

19
00:01:49,242 --> 00:01:52,750
of different services which are native to kubernetes,

20
00:01:53,510 --> 00:01:57,106
including your API server. That is kind

21
00:01:57,128 --> 00:02:00,930
of the gateway through which you interact with a cluster.

22
00:02:01,270 --> 00:02:05,126
Then there is controller manager. This specific component will

23
00:02:05,148 --> 00:02:08,870
be our area of focus as we move forward because this is exactly

24
00:02:08,940 --> 00:02:12,646
where the control loops reside and control loops are

25
00:02:12,668 --> 00:02:16,390
indeed the foundation for building operators.

26
00:02:17,130 --> 00:02:20,794
Then there is the Kubernetes scheduler which is

27
00:02:20,832 --> 00:02:25,050
responsible for scheduling your pods, your workloads,

28
00:02:25,470 --> 00:02:29,210
finding the appropriate node or nodes

29
00:02:29,890 --> 00:02:33,566
for your workloads to execute. Then the

30
00:02:33,748 --> 00:02:37,246
persistent store or the key value store which is

31
00:02:37,268 --> 00:02:41,070
used by Kubernetes where the state of your resources

32
00:02:41,410 --> 00:02:45,326
is actually saved, called as EtCD or etzed

33
00:02:45,438 --> 00:02:49,010
depending upon how you pronounce it. Then there is a cloud

34
00:02:49,080 --> 00:02:53,006
controller manager which is sort of an entry controller

35
00:02:53,038 --> 00:02:56,486
manager consisting of control loops, but it is

36
00:02:56,508 --> 00:03:00,598
dealing with all things external to the cluster and can be

37
00:03:00,764 --> 00:03:04,486
very specific to certain cloud providers. So we are

38
00:03:04,508 --> 00:03:08,374
not going to go too deep into what a cloud controller manager

39
00:03:08,422 --> 00:03:12,490
is and what its roles and responsibilities are.

40
00:03:12,640 --> 00:03:15,754
However, just for awareness so that you know that there

41
00:03:15,792 --> 00:03:19,478
are in fact two controller managers in the control plane,

42
00:03:19,654 --> 00:03:23,066
the controller manager responsible for managing

43
00:03:23,258 --> 00:03:27,034
and operating upon the native Kubernetes

44
00:03:27,082 --> 00:03:30,602
resources within the cluster. And then there is the CCM

45
00:03:30,666 --> 00:03:34,660
or cloud control manager. And then of course you have

46
00:03:35,030 --> 00:03:38,946
the data plane or these worker nodes which is

47
00:03:38,968 --> 00:03:43,442
where your workloads get scheduled and execute data

48
00:03:43,496 --> 00:03:48,182
plane in Kubernetes. Every node especially will

49
00:03:48,236 --> 00:03:51,890
also be running a couple of pretty specialized

50
00:03:51,970 --> 00:03:55,506
workloads. One is your kubelet which is sort of a node

51
00:03:55,538 --> 00:03:59,734
agent in Kubernetes, and then the kuberoxy which

52
00:03:59,772 --> 00:04:03,574
basically uses IP tables for various

53
00:04:03,622 --> 00:04:07,062
different purposes like packet filtering,

54
00:04:07,126 --> 00:04:10,906
source netting, dnating and to an extent load

55
00:04:10,938 --> 00:04:14,320
balancing the traffic. When you create your

56
00:04:14,690 --> 00:04:18,126
services with a set of pods as these back end. So it

57
00:04:18,148 --> 00:04:20,190
serves a bunch of those different purposes,

58
00:04:20,610 --> 00:04:22,510
networking related mostly.

59
00:04:24,470 --> 00:04:25,780
As we move on,

60
00:04:27,110 --> 00:04:29,890
let's try to build some foundation first.

61
00:04:29,960 --> 00:04:34,354
Right? So we touched upon the

62
00:04:34,392 --> 00:04:37,240
controller or the controller manager, right?

63
00:04:37,850 --> 00:04:41,350
Controller manager is responsible for

64
00:04:41,500 --> 00:04:44,982
executing a bunch of control loops, but what these control

65
00:04:45,036 --> 00:04:48,700
loops are, what specific

66
00:04:49,230 --> 00:04:53,114
action are they going to perform or

67
00:04:53,152 --> 00:04:55,580
are performing and on what.

68
00:04:56,510 --> 00:05:00,880
So control loops are implemented by?

69
00:05:01,570 --> 00:05:04,990
Well, pretty obvious controller and

70
00:05:05,060 --> 00:05:09,070
their responsibility is really to watch over

71
00:05:09,140 --> 00:05:12,754
the state of the

72
00:05:12,792 --> 00:05:16,626
specific Kubernetes API object that they are

73
00:05:16,648 --> 00:05:20,402
assigned to. And then based

74
00:05:20,456 --> 00:05:23,860
upon the state changes that they are watching,

75
00:05:24,470 --> 00:05:28,134
perform a specific action, either make

76
00:05:28,172 --> 00:05:32,470
a change or request a change and delegate to someone

77
00:05:32,540 --> 00:05:36,166
else, most likely another controller to act upon it. But the

78
00:05:36,188 --> 00:05:39,974
idea is very simple, right? It's a continuous

79
00:05:40,102 --> 00:05:43,174
loop which is noticing

80
00:05:43,222 --> 00:05:47,450
these state changes as they are coming in and

81
00:05:47,520 --> 00:05:51,374
taking a certain action. But what's these purpose of it?

82
00:05:51,572 --> 00:05:54,958
Right, the purpose is that

83
00:05:55,124 --> 00:05:58,240
as and when the stateful change happens,

84
00:05:59,090 --> 00:06:02,880
a control loop would actually try to reconcile it

85
00:06:03,190 --> 00:06:07,310
with the state that is actually being observed

86
00:06:07,470 --> 00:06:10,610
in real time in that very cluster.

87
00:06:11,110 --> 00:06:15,442
So that is the pretty foundational

88
00:06:15,506 --> 00:06:19,346
or fundamental aspect of the mechanics

89
00:06:19,538 --> 00:06:23,302
on which a control loop operates. That is, to watch for the state

90
00:06:23,356 --> 00:06:26,710
changes, compare it with the actual

91
00:06:26,780 --> 00:06:30,730
state, and then take the appropriate action

92
00:06:31,070 --> 00:06:34,570
to reconcile or to match the

93
00:06:34,720 --> 00:06:39,126
desired state. That is something that you apply as a cluster administrator

94
00:06:39,238 --> 00:06:42,966
or as a developer deploying your workloads on that very

95
00:06:43,008 --> 00:06:46,318
cluster with the state that

96
00:06:46,404 --> 00:06:49,806
these Kubernetes control loop happens to observe in

97
00:06:49,828 --> 00:06:53,806
the cluster. It could be something as simple as scaling

98
00:06:53,838 --> 00:06:57,726
out the number of replicas for your web server,

99
00:06:57,918 --> 00:07:01,460
or it could be something pretty complicated like

100
00:07:02,310 --> 00:07:06,150
provisioning persistent storage for your

101
00:07:06,300 --> 00:07:09,426
database workloads and setting up replication

102
00:07:09,538 --> 00:07:13,430
backups, snapshots, point in time recovery, et cetera, et cetera.

103
00:07:14,970 --> 00:07:18,700
All right, moving on. Now,

104
00:07:19,230 --> 00:07:22,314
this is actually a

105
00:07:22,352 --> 00:07:26,666
pretty oversimplified view of what

106
00:07:26,768 --> 00:07:30,338
a control loop might actually look like in Kubernetes,

107
00:07:30,454 --> 00:07:33,582
right? But I wanted to keep things simple and

108
00:07:33,636 --> 00:07:37,006
not to kind of overwhelm everyone with

109
00:07:37,188 --> 00:07:40,606
all the complexities. Otherwise this

110
00:07:40,628 --> 00:07:43,220
diagram could get really messy, trust me.

111
00:07:43,750 --> 00:07:47,922
But yeah, the idea is like, hey, we are in the very

112
00:07:47,976 --> 00:07:51,566
initial phase of understanding

113
00:07:51,598 --> 00:07:55,222
what controllers and control loops are. So let's start with something

114
00:07:55,276 --> 00:07:58,886
which is clean and simple. So as we

115
00:07:58,908 --> 00:08:01,826
can see here, right, a pretty simple flow.

116
00:08:02,018 --> 00:08:06,214
You as the cluster administrator or

117
00:08:06,332 --> 00:08:10,186
cluster user apply the desired state.

118
00:08:10,368 --> 00:08:14,010
The control loop is executing continuously,

119
00:08:14,910 --> 00:08:18,170
watching for those resources as are they're being created,

120
00:08:20,370 --> 00:08:23,390
updated, deleted, modified,

121
00:08:23,890 --> 00:08:26,842
and take specific set of actions.

122
00:08:26,906 --> 00:08:30,174
So let's say you deployed a web server with three

123
00:08:30,212 --> 00:08:33,666
replicas control loop will say, hey, I see

124
00:08:33,688 --> 00:08:37,266
a new deployment coming in. Let me go and

125
00:08:37,448 --> 00:08:41,762
start spinning up three identical pods which

126
00:08:41,816 --> 00:08:45,794
basically meet the criteria that you just defined

127
00:08:45,842 --> 00:08:49,254
through your manifestation or through your yaml. And that

128
00:08:49,292 --> 00:08:53,106
is the whole value proposition of control loop

129
00:08:53,138 --> 00:08:56,550
if you ask me. Because look, as an end user,

130
00:08:57,470 --> 00:09:01,562
you are not really dealing with pretty

131
00:09:01,616 --> 00:09:05,334
low level or the intricacies of kubernetes.

132
00:09:05,462 --> 00:09:08,854
End of the day, when you supply

133
00:09:08,902 --> 00:09:12,686
a deployment or when you supply a stateful set, everything is

134
00:09:12,708 --> 00:09:16,714
going to translate into a set of pods and pods, into containers

135
00:09:16,762 --> 00:09:21,194
and whatnot. But this is the beauty of control loops,

136
00:09:21,242 --> 00:09:24,740
this is the beauty of controllers, that these are letting you work with

137
00:09:25,510 --> 00:09:28,370
kind of more higher level abstractions,

138
00:09:29,990 --> 00:09:33,374
what is also called as controller objects.

139
00:09:33,502 --> 00:09:37,094
So if you look at deployment, stateful, set job,

140
00:09:37,292 --> 00:09:41,426
cron jobs, all these are basically controller

141
00:09:41,458 --> 00:09:45,142
objects which offer you kind of a higher

142
00:09:45,196 --> 00:09:49,038
level abstraction so that you don't have to deal with low level mechanics

143
00:09:49,074 --> 00:09:52,694
of kubernetes. Like what are the resources that should be created

144
00:09:52,742 --> 00:09:56,166
when you create a deployment, right? And the additional benefit

145
00:09:56,198 --> 00:09:59,930
is that not only you are being abstracted away

146
00:10:00,080 --> 00:10:03,674
and being offered something more simple in order to provision the workloads,

147
00:10:03,802 --> 00:10:07,790
but there is this continuous reconciliation which is happening

148
00:10:07,860 --> 00:10:11,166
behind the scenes to make sure that

149
00:10:11,348 --> 00:10:15,682
the state of your application remains as

150
00:10:15,736 --> 00:10:19,090
you defined it initially, so that there are no surprises.

151
00:10:19,590 --> 00:10:22,946
Like you go to bed in the night and one of

152
00:10:22,968 --> 00:10:26,326
the pod or one of the replicas of your

153
00:10:26,348 --> 00:10:30,214
deployment crashes out. Control loop is observing that,

154
00:10:30,252 --> 00:10:34,614
hey, I see there's a delta. Because the user wanted three

155
00:10:34,652 --> 00:10:38,600
replicas of this web server. I see only two. So let me go and

156
00:10:39,770 --> 00:10:43,834
request another replica. Right? That's the whole value add that

157
00:10:43,952 --> 00:10:47,482
control loops actually bring in. I have

158
00:10:47,536 --> 00:10:49,980
linked one excellent resource here.

159
00:10:50,610 --> 00:10:54,506
There's a book programming kubernetes from O'Reilly.

160
00:10:54,698 --> 00:10:58,000
I think it's an excellent book. If you have some

161
00:10:58,610 --> 00:11:01,966
go programming background for you, go check it

162
00:11:01,988 --> 00:11:06,674
out. There's a lot of good information about the

163
00:11:06,712 --> 00:11:10,366
client APIs and the internal mechanics

164
00:11:10,398 --> 00:11:13,090
of Kubernetes, how control loops really operate.

165
00:11:13,430 --> 00:11:17,750
It's a very good resource if you are into Kubernetes native development.

166
00:11:20,090 --> 00:11:23,362
All right, so we test upon the control loop.

167
00:11:23,506 --> 00:11:27,522
But what are these building blocks of the control

168
00:11:27,596 --> 00:11:31,274
loop? Right? I mean, if you were to design, if you were to

169
00:11:31,312 --> 00:11:34,890
create a controller for your workloads,

170
00:11:35,310 --> 00:11:38,534
how would you basically go about creating

171
00:11:38,582 --> 00:11:42,654
that? Right? So there

172
00:11:42,692 --> 00:11:45,706
are several different frameworks,

173
00:11:45,898 --> 00:11:49,294
utilities, sdks which

174
00:11:49,332 --> 00:11:53,220
are available today in order for you to

175
00:11:54,150 --> 00:11:57,730
start writing your own operators or custom

176
00:11:57,800 --> 00:12:00,878
controllers. But fundamentally,

177
00:12:01,054 --> 00:12:04,274
regardless of what SDK or what

178
00:12:04,312 --> 00:12:07,366
framework you choose, there are

179
00:12:07,388 --> 00:12:11,302
going to be three fundamental building

180
00:12:11,356 --> 00:12:15,030
blocks, three fundamental components of any controller,

181
00:12:15,530 --> 00:12:19,546
which are the informer, the work queue and the

182
00:12:19,568 --> 00:12:23,066
events. Okay? And they all

183
00:12:23,248 --> 00:12:27,158
have a pretty specific assigned

184
00:12:27,254 --> 00:12:30,830
set of responsibility within your controller.

185
00:12:31,330 --> 00:12:35,150
So informer, as an example, watches these state

186
00:12:35,220 --> 00:12:38,954
changes, right as it propagates

187
00:12:39,002 --> 00:12:42,518
in the cluster. It implements resync and reconciliation

188
00:12:42,554 --> 00:12:44,930
mechanism that we just touched upon.

189
00:12:45,910 --> 00:12:49,854
These, there are work queues which are basically responsible

190
00:12:49,902 --> 00:12:53,810
for queuing up these changes. Implement retries

191
00:12:54,230 --> 00:12:57,506
if something fails upon the first attempt

192
00:12:57,538 --> 00:13:01,190
to reconcile, and then there are events

193
00:13:01,690 --> 00:13:05,378
which are basically the native representation

194
00:13:05,554 --> 00:13:09,594
for these state changes. Think about the crud operations that

195
00:13:09,632 --> 00:13:12,966
you perform on your resources, creating a new stateful

196
00:13:12,998 --> 00:13:16,810
set, creating a new deployment, updating a number of replicas,

197
00:13:18,190 --> 00:13:21,822
or a replica crashing out for whatever

198
00:13:21,876 --> 00:13:25,034
reasons, and your controller

199
00:13:25,082 --> 00:13:28,826
is now responsible for creating

200
00:13:28,858 --> 00:13:32,240
another one. So all of these state changes,

201
00:13:32,550 --> 00:13:36,174
all of these modifications

202
00:13:36,222 --> 00:13:39,582
which take place are represented as events

203
00:13:39,646 --> 00:13:40,930
to your controller.

204
00:13:45,050 --> 00:13:49,254
Well, I apologize for the not

205
00:13:49,292 --> 00:13:53,126
so clear sequence diagram because indeed there are a lot

206
00:13:53,148 --> 00:13:57,106
of pillars and boxes here. That's why I have linked

207
00:13:57,218 --> 00:14:00,646
the resource where I kind of drew

208
00:14:00,678 --> 00:14:03,830
a lot of inspiration from. There's an excellent blog

209
00:14:03,910 --> 00:14:07,610
from Andrew Chen and Dominic Torno

210
00:14:08,110 --> 00:14:12,194
called the mechanics of kubernetes, which is very thorough, very detailed,

211
00:14:12,342 --> 00:14:15,886
and it was written four or five years ago, so it's been

212
00:14:15,908 --> 00:14:19,486
some time since it's out there. Please go and check

213
00:14:19,508 --> 00:14:23,520
it out and how they have kind of laid out

214
00:14:24,130 --> 00:14:27,566
how Kubernetes control loops really work behind the scene.

215
00:14:27,598 --> 00:14:32,158
I think it's a fantastic resource if you want to dive deep into controllers

216
00:14:32,174 --> 00:14:34,900
and control loops, but on a high level.

217
00:14:35,450 --> 00:14:40,054
What is represented here through

218
00:14:40,092 --> 00:14:45,042
an example is basically the extent of coordination

219
00:14:45,186 --> 00:14:49,354
that happens within a Kubernetes cluster when

220
00:14:49,392 --> 00:14:53,050
a state change is detected. So one thing to note

221
00:14:53,470 --> 00:14:58,330
is that kubernetes by design is distributed.

222
00:14:58,750 --> 00:15:02,842
I mean, right now we talk about monolithics

223
00:15:02,906 --> 00:15:06,750
and microservices, which is better versus

224
00:15:07,570 --> 00:15:10,606
which is worse. So we talk

225
00:15:10,628 --> 00:15:14,846
about all this. Kubernetes was fundamentally

226
00:15:15,038 --> 00:15:17,170
designed and architected,

227
00:15:17,750 --> 00:15:20,980
keeping a distributed system in mind.

228
00:15:21,670 --> 00:15:24,922
So we saw the architecture,

229
00:15:25,086 --> 00:15:29,234
right? And how the control plane is comprised

230
00:15:29,282 --> 00:15:33,186
of different components. Why all the business logic

231
00:15:33,218 --> 00:15:37,454
is not built into a single binary for the sake of scale.

232
00:15:37,522 --> 00:15:42,134
Of course, distributed architectures are capable of scaling

233
00:15:42,182 --> 00:15:45,530
better. There could be challenges,

234
00:15:45,950 --> 00:15:50,038
especially if these components require a

235
00:15:50,064 --> 00:15:53,966
lot of coordination between the different components. It can

236
00:15:53,988 --> 00:15:57,706
get complicated, it can get complex. But Kubernetes

237
00:15:57,818 --> 00:16:00,270
is built with scalability,

238
00:16:01,250 --> 00:16:05,440
reliability and efficiency in mind,

239
00:16:06,050 --> 00:16:10,026
right? And with microservices distributed

240
00:16:10,058 --> 00:16:12,880
architectures in general, you get that,

241
00:16:13,250 --> 00:16:17,030
right? So again, going back to

242
00:16:17,180 --> 00:16:19,800
this little diagram here on the left,

243
00:16:20,810 --> 00:16:24,662
it is just a representation of these different

244
00:16:24,716 --> 00:16:28,486
control loops getting involved when you actually request

245
00:16:28,518 --> 00:16:32,026
a deployment to be created, right? So let's say I

246
00:16:32,048 --> 00:16:35,706
want to deploy Nginx as a deployment in a

247
00:16:35,728 --> 00:16:38,954
Kubernetes cluster and I want three

248
00:16:38,992 --> 00:16:41,340
replicas of it, right?

249
00:16:41,890 --> 00:16:45,870
As a user I can happily apply my manifestation

250
00:16:46,530 --> 00:16:49,870
and well go and fetch some coffee.

251
00:16:50,370 --> 00:16:54,226
But let's look at the work that Kubernetes is

252
00:16:54,248 --> 00:16:56,862
performing through its own control loops.

253
00:16:57,006 --> 00:17:00,578
When you perform an action such as creating a

254
00:17:00,584 --> 00:17:04,414
deployment in the cluster. So of course API

255
00:17:04,462 --> 00:17:07,010
server will intake your request,

256
00:17:07,670 --> 00:17:11,906
right? It creates the deployment resource

257
00:17:12,098 --> 00:17:16,200
persisted. What happens afterwards is really interesting

258
00:17:16,970 --> 00:17:21,430
because after the API server you see a bunch of different controllers,

259
00:17:21,510 --> 00:17:25,130
right? So there's a deployment controller which is watching

260
00:17:25,200 --> 00:17:28,602
over the Kubernetes API resources of type

261
00:17:28,656 --> 00:17:32,430
deployment. Deployment is a kind in Kubernetes.

262
00:17:33,330 --> 00:17:36,574
Now it sees that hey,

263
00:17:36,612 --> 00:17:39,710
a new deployment has just been created.

264
00:17:40,130 --> 00:17:43,986
What am I going to do now? I'm going to

265
00:17:44,168 --> 00:17:47,860
request the API server to create a replica set.

266
00:17:48,550 --> 00:17:51,714
So it sends out a request to the API server say hey,

267
00:17:51,752 --> 00:17:55,038
corresponding to this deployment, can you create a

268
00:17:55,064 --> 00:17:59,334
replica set? API server says yes, why not?

269
00:17:59,532 --> 00:18:02,934
It creates another resource. You can think of

270
00:18:02,972 --> 00:18:06,162
that as a child resource of the deployment.

271
00:18:06,306 --> 00:18:10,540
But the resource created now is a replica set.

272
00:18:11,230 --> 00:18:14,554
Then there's a replica set controller which is watching over

273
00:18:14,592 --> 00:18:18,906
the creation of new replica sets. It observes that

274
00:18:19,088 --> 00:18:23,070
these, there's a new replica set that has just been created

275
00:18:23,810 --> 00:18:27,754
and it says it needs three replicas,

276
00:18:27,802 --> 00:18:31,514
three identical replicas of a certain type of container.

277
00:18:31,562 --> 00:18:34,638
The container here being your NgInx web server.

278
00:18:34,814 --> 00:18:37,650
But how many do I have right now? None.

279
00:18:38,070 --> 00:18:41,326
So let me ask the Kubernetes API server

280
00:18:41,358 --> 00:18:45,846
to go ahead and create three

281
00:18:46,028 --> 00:18:49,670
pods which will represent three replicas

282
00:18:50,090 --> 00:18:53,560
of this deployment or this replica set.

283
00:18:54,490 --> 00:18:58,774
Again, Kubernetes API server creates

284
00:18:58,902 --> 00:19:02,090
the pods, pod is created.

285
00:19:02,510 --> 00:19:06,186
After that what happens is the scheduler which

286
00:19:06,208 --> 00:19:10,006
is another component of the control plane that we saw in the architecture

287
00:19:10,038 --> 00:19:13,758
overview looks at these pods and

288
00:19:13,924 --> 00:19:17,246
figures out that hey, these are new pods and they need to

289
00:19:17,268 --> 00:19:20,830
find a node in order to execute. So let me go and find

290
00:19:20,900 --> 00:19:24,302
the appropriate nodes for these pods.

291
00:19:24,446 --> 00:19:28,638
It happens to find it and your pods get scheduled

292
00:19:28,734 --> 00:19:32,990
to run on a certain node. Then these kubelet,

293
00:19:33,150 --> 00:19:37,190
which is also a kind of control loop

294
00:19:37,930 --> 00:19:41,846
is looking for the pods which

295
00:19:41,868 --> 00:19:45,426
are designated to run on the very node

296
00:19:45,618 --> 00:19:49,174
where this kubernetes is. You can have like hundreds and thousands

297
00:19:49,222 --> 00:19:52,502
of nodes in a Kubernetes cluster,

298
00:19:52,566 --> 00:19:56,746
right? And each node is going to run Kubelet. So think of

299
00:19:56,848 --> 00:20:00,794
these individual kubernetes on these nodes as a control loop

300
00:20:00,842 --> 00:20:04,174
of their own, which are waking up periodically to

301
00:20:04,212 --> 00:20:07,422
check if there are any pods which are

302
00:20:07,476 --> 00:20:11,658
scheduled to run on the node on which they are executing.

303
00:20:11,754 --> 00:20:15,730
And if they find, then their next job

304
00:20:15,800 --> 00:20:19,234
is to go and well launch the

305
00:20:19,272 --> 00:20:22,994
containers. And that's how your workload comes to

306
00:20:23,032 --> 00:20:26,358
life. So as you can see, right, there's a

307
00:20:26,364 --> 00:20:29,874
lot of coordination happening between different components.

308
00:20:29,922 --> 00:20:33,766
But the idea here is that how beautifully through

309
00:20:33,788 --> 00:20:37,106
the control loops Kubernetes is basically providing

310
00:20:37,138 --> 00:20:41,046
you this automation and a pretty good degree

311
00:20:41,078 --> 00:20:45,194
of abstraction that you as an end user is

312
00:20:45,232 --> 00:20:49,542
only requesting one single API resource,

313
00:20:49,686 --> 00:20:52,926
which is a deployment. And behind the scenes all

314
00:20:52,948 --> 00:20:55,600
the complexities that comes in,

315
00:20:56,050 --> 00:21:00,000
whether it is creating replica sets, then the pods, and then

316
00:21:00,370 --> 00:21:03,406
finding out the right node to

317
00:21:03,428 --> 00:21:07,486
run these pods, because it depends upon the resources

318
00:21:07,518 --> 00:21:10,914
that are available on that node versus what you are requesting in terms

319
00:21:10,952 --> 00:21:14,306
of cpu and memory. Kubernetes does

320
00:21:14,328 --> 00:21:18,374
not want you to worry about all of this. And it

321
00:21:18,412 --> 00:21:22,646
basically provides that automation, that intelligence to

322
00:21:22,668 --> 00:21:26,662
deal with the lower level mechanics, which is a huge plus.

323
00:21:26,796 --> 00:21:30,426
And that's one of the biggest value proposition of Kubernetes, right. Why it is

324
00:21:30,448 --> 00:21:32,780
so popular? Because yes,

325
00:21:33,630 --> 00:21:37,306
workload orchestration, workload management, all that is

326
00:21:37,328 --> 00:21:41,034
good, right? But look at the degree of automation it provides.

327
00:21:41,162 --> 00:21:44,110
Load balancing, scaling out, auto healing,

328
00:21:44,770 --> 00:21:47,882
auto repairing, restarting the crash containers.

329
00:21:48,026 --> 00:21:50,000
This is all fantastic right?

330
00:21:52,050 --> 00:21:55,922
Now if you move on here now again, it's a pretty

331
00:21:55,976 --> 00:21:58,962
difficult to see type of a screenshot here,

332
00:21:59,096 --> 00:22:02,882
but the idea is not to kind of go

333
00:22:02,936 --> 00:22:06,114
deep into what this code is trying

334
00:22:06,152 --> 00:22:09,314
to do and in which language or runtime

335
00:22:09,362 --> 00:22:12,742
it is being written. So it's go code, it's a snippet of go code.

336
00:22:12,876 --> 00:22:16,694
Kubernetes is written in go. And if

337
00:22:16,732 --> 00:22:20,394
you do not know go or have not programmed enough in go,

338
00:22:20,512 --> 00:22:25,226
that's perfectly fine. But I would still encourage all of you to

339
00:22:25,248 --> 00:22:29,382
at least if you really want to take your Kubernetes expertise

340
00:22:29,446 --> 00:22:33,406
to the next level and maybe in future become a

341
00:22:33,428 --> 00:22:37,070
contributor to one of these projects

342
00:22:37,570 --> 00:22:41,246
hosted under CNCF. Majority of CNCF hosted projects are

343
00:22:41,268 --> 00:22:45,860
actually written in go. So as Kubernetes and

344
00:22:47,110 --> 00:22:50,820
here what this code is trying to do. By the way,

345
00:22:51,510 --> 00:22:55,454
this is the source code for these replica set controller which we just discussed

346
00:22:55,582 --> 00:22:58,910
on the previous slide, right? This is the real code

347
00:22:59,000 --> 00:23:02,326
hosted on GitHub. You can go and check it out if

348
00:23:02,348 --> 00:23:05,590
you want to. After these talk, this is the real code.

349
00:23:05,740 --> 00:23:10,410
And what this code is trying to do is just

350
00:23:10,480 --> 00:23:14,154
do something pretty intuitive, pretty simple,

351
00:23:14,352 --> 00:23:18,074
right? It's just looking at the desired number of

352
00:23:18,112 --> 00:23:22,282
replicas. And if the replicas

353
00:23:22,346 --> 00:23:25,614
requested are less than the

354
00:23:25,652 --> 00:23:28,990
replicas which were observed in these cluster,

355
00:23:29,410 --> 00:23:33,700
it's creating new one. And if the

356
00:23:34,390 --> 00:23:38,306
replicas requested are less than what

357
00:23:38,328 --> 00:23:42,530
is observed in the cluster, it is going ahead and terminating some pods,

358
00:23:43,110 --> 00:23:46,950
right? But end of the day, the idea

359
00:23:47,020 --> 00:23:48,040
is very simple.

360
00:23:49,930 --> 00:23:53,506
Whatever is the persisted

361
00:23:53,698 --> 00:23:57,654
state in the persistent store

362
00:23:57,852 --> 00:24:02,226
of your kubernetes cluster, which is etcd

363
00:24:02,258 --> 00:24:06,346
or et cetera, that is being taken as a

364
00:24:06,368 --> 00:24:10,114
source of truth because that is what the declared

365
00:24:10,182 --> 00:24:14,222
state is. And all it is trying to do is

366
00:24:14,276 --> 00:24:18,206
to basically match that declared state,

367
00:24:18,308 --> 00:24:21,822
which is a desired state, to what is

368
00:24:21,876 --> 00:24:25,330
being observed real time in the cluster,

369
00:24:26,230 --> 00:24:30,386
right? So that's pretty much how all

370
00:24:30,408 --> 00:24:33,742
the control loops are going to work depending upon

371
00:24:33,886 --> 00:24:37,238
what type of resources they are dealing with. That's why I thought it would

372
00:24:37,244 --> 00:24:41,506
be useful just to kind of put this little code snippet without worrying

373
00:24:41,538 --> 00:24:45,382
about whether, you know, go or not. Or your

374
00:24:45,436 --> 00:24:49,322
preferred programming language of choice is not go. That's perfectly fine,

375
00:24:49,456 --> 00:24:53,574
because the idea is, again, not to overwhelm anyone with the mechanics

376
00:24:53,622 --> 00:24:56,954
of go and how go language work, but just

377
00:24:56,992 --> 00:25:01,374
to kind of show you that how

378
00:25:01,412 --> 00:25:04,830
a controller is going to execute

379
00:25:05,330 --> 00:25:09,054
its assigned set of responsibilities. When it comes to

380
00:25:09,092 --> 00:25:11,070
resource state reconciliation,

381
00:25:14,070 --> 00:25:19,186
there is another very important

382
00:25:19,288 --> 00:25:22,674
concept in Kubernetes to understand. So we

383
00:25:22,712 --> 00:25:26,658
discussed about Kubernetes being distributed.

384
00:25:26,834 --> 00:25:30,438
It's a collection of different components which you can

385
00:25:30,604 --> 00:25:33,778
broadly think as different microservices

386
00:25:33,874 --> 00:25:36,760
kind of work in a cohesive fashion together.

387
00:25:38,750 --> 00:25:42,134
The concept here is the optimistic concurrency.

388
00:25:42,182 --> 00:25:44,540
Now, why optimistic concurrency is important?

389
00:25:46,190 --> 00:25:50,720
I mean, it's very obvious that when you work with

390
00:25:51,250 --> 00:25:54,480
distributed systems, right, and then you have

391
00:25:55,330 --> 00:25:58,430
state and the state is shared,

392
00:25:59,090 --> 00:26:02,160
integrity of the state becomes very important.

393
00:26:03,110 --> 00:26:07,010
Now, Kubernetes is meant for

394
00:26:07,080 --> 00:26:10,050
running workloads at scale.

395
00:26:10,630 --> 00:26:14,514
That is why it consciously does not employ anything

396
00:26:14,632 --> 00:26:18,386
like resource locking or synchronization,

397
00:26:18,578 --> 00:26:22,962
because that could be a hindrance to performance

398
00:26:23,026 --> 00:26:26,822
and scalability both, and might actually

399
00:26:26,876 --> 00:26:30,166
result in higher resource usage

400
00:26:30,278 --> 00:26:34,060
than it should otherwise. But at the same time,

401
00:26:34,910 --> 00:26:38,314
maintaining the integrity of the

402
00:26:38,352 --> 00:26:42,160
state is also equally important.

403
00:26:43,010 --> 00:26:46,526
That is why Kubernetes employs something called as

404
00:26:46,628 --> 00:26:48,350
optimistic concurrency.

405
00:26:49,810 --> 00:26:53,518
Optimistic concurrency, to put it simply,

406
00:26:53,694 --> 00:26:57,058
is Kubernetes way of dealing with

407
00:26:57,224 --> 00:27:01,730
concurrent rights that might be affecting the same resource.

408
00:27:02,150 --> 00:27:06,050
So what kubernetes does, basically behind the scenes,

409
00:27:06,210 --> 00:27:10,694
it maintains a resource version with

410
00:27:10,732 --> 00:27:15,110
every resource, right? And as and when this resource undergoes

411
00:27:15,450 --> 00:27:18,570
changes, the resource version keeps changing.

412
00:27:19,070 --> 00:27:20,650
So as a client,

413
00:27:21,710 --> 00:27:25,210
when you happen to fetch the resource,

414
00:27:25,790 --> 00:27:29,754
make an update and go

415
00:27:29,792 --> 00:27:33,294
for persisting that resource through

416
00:27:33,332 --> 00:27:36,986
the API server. The Kubernetes API

417
00:27:37,018 --> 00:27:41,322
server is going to check if the resource version has changed

418
00:27:41,386 --> 00:27:44,626
since then, right? And if it has,

419
00:27:44,808 --> 00:27:48,510
then that request is going to be rejected

420
00:27:48,590 --> 00:27:52,398
because you might be operating upon stale

421
00:27:52,574 --> 00:27:56,374
data, right? And at this point in

422
00:27:56,412 --> 00:28:00,166
time, what Kubernetes, typically this is

423
00:28:00,268 --> 00:28:03,526
what even the control loops will end up

424
00:28:03,548 --> 00:28:06,760
doing, and we will see that. But in this case,

425
00:28:07,950 --> 00:28:11,580
your client, let's say it's a control loop in this case,

426
00:28:12,270 --> 00:28:16,230
becomes responsible for handling

427
00:28:16,310 --> 00:28:20,010
these errors

428
00:28:20,590 --> 00:28:24,382
related to concurrent writes and

429
00:28:24,436 --> 00:28:28,202
simply re queue these resource to be retried

430
00:28:28,266 --> 00:28:31,934
later so that you can be

431
00:28:31,972 --> 00:28:35,466
sure that when you retry you

432
00:28:35,508 --> 00:28:39,026
fetch it again. And hopefully this time you get the latest and

433
00:28:39,048 --> 00:28:43,170
there are no concurrent rights going around. And then you make your mods

434
00:28:43,510 --> 00:28:46,694
and these you apply it. So that is what

435
00:28:46,732 --> 00:28:50,200
the principle of optimistic concurrency is,

436
00:28:50,810 --> 00:28:53,378
that don't do any explicit locking,

437
00:28:53,554 --> 00:28:56,994
don't do any synchronization, rather rely

438
00:28:57,042 --> 00:29:00,698
on resource version. And if a specific

439
00:29:00,784 --> 00:29:04,774
client happened to run into a problem where its request

440
00:29:04,822 --> 00:29:08,842
to write or change the state of the resource was

441
00:29:08,896 --> 00:29:11,210
rejected, re queue,

442
00:29:11,810 --> 00:29:15,680
refresh, update, and then try.

443
00:29:16,210 --> 00:29:19,280
All right, so let's move on a little bit.

444
00:29:19,970 --> 00:29:24,114
Let's talk about operators now. So we discussed about

445
00:29:24,312 --> 00:29:28,370
control loops, we discussed about reconciliation.

446
00:29:28,950 --> 00:29:32,398
Operators are basically control loops,

447
00:29:32,494 --> 00:29:36,518
or you can think of them as custom control loops and more.

448
00:29:36,604 --> 00:29:40,422
But what is that more, that more is the

449
00:29:40,556 --> 00:29:43,734
operational intelligence that an

450
00:29:43,772 --> 00:29:47,510
operator has about your workloads.

451
00:29:47,950 --> 00:29:51,770
Operators were first introduced in 2016. So they are not a

452
00:29:51,920 --> 00:29:55,114
very novel, very new concept. They have been

453
00:29:55,152 --> 00:29:59,858
in use. And one of the founding principles

454
00:30:00,054 --> 00:30:03,902
for creating operators, or the whole framework to

455
00:30:03,956 --> 00:30:07,390
help you create your own operators

456
00:30:07,970 --> 00:30:12,590
was that, hey, can we codify,

457
00:30:13,990 --> 00:30:18,130
can we translate all the operational knowledge

458
00:30:18,870 --> 00:30:23,730
that the support engineers

459
00:30:24,710 --> 00:30:28,550
DevOps, engineers, cytoliaty engineers

460
00:30:29,210 --> 00:30:33,494
have developed over time by operating a

461
00:30:33,532 --> 00:30:36,870
specific type of workload in production.

462
00:30:37,630 --> 00:30:40,860
So think about a database, right?

463
00:30:41,390 --> 00:30:45,894
When you create a database, it's not just about start consuming

464
00:30:45,942 --> 00:30:48,700
them as and when you create it.

465
00:30:49,630 --> 00:30:54,074
There's a lot of operational exercises

466
00:30:54,202 --> 00:30:59,194
that you would have to do to manage a production

467
00:30:59,242 --> 00:31:02,778
scale, a production level database. Think about

468
00:31:02,964 --> 00:31:05,810
backups, think about snapshots,

469
00:31:06,230 --> 00:31:09,886
think about point in time recovery,

470
00:31:10,078 --> 00:31:14,194
think. But the transaction logs, how you archive them, how you

471
00:31:14,232 --> 00:31:17,510
back them up, think about high availability,

472
00:31:18,250 --> 00:31:21,926
think about replicas, right? So there

473
00:31:21,948 --> 00:31:25,240
is a lot of operators complexity. Now,

474
00:31:26,330 --> 00:31:30,380
back in the days, maybe you had individuals who did that.

475
00:31:30,990 --> 00:31:34,730
But today in cloud native ecosystem,

476
00:31:35,230 --> 00:31:37,290
running your workloads at scale,

477
00:31:38,110 --> 00:31:41,366
you want to automate as much as you can and reduce

478
00:31:41,398 --> 00:31:44,846
this toil. It was okay probably if you had like two

479
00:31:44,868 --> 00:31:48,906
or three, but you can't do this for thousands

480
00:31:48,938 --> 00:31:52,946
and thousands of databases that you are running in production, right? So how

481
00:31:52,968 --> 00:31:56,674
do you automate, especially if you were to run these

482
00:31:56,712 --> 00:31:59,886
databases in Kubernetes as stateful workloads,

483
00:32:00,078 --> 00:32:04,610
how do you basically bring about this operational excellence

484
00:32:05,450 --> 00:32:09,142
through automation, so that you just

485
00:32:09,196 --> 00:32:13,462
worry about creating your databases and

486
00:32:13,516 --> 00:32:16,562
leave the rest to Kubernetes?

487
00:32:16,626 --> 00:32:20,134
In this case, that is where operators add

488
00:32:20,172 --> 00:32:23,746
a lot of value. And this is exactly the purpose

489
00:32:23,858 --> 00:32:26,390
for which the operators were actually created,

490
00:32:27,050 --> 00:32:30,010
right? It is an end user.

491
00:32:30,050 --> 00:32:33,470
You work with these abstractions,

492
00:32:33,970 --> 00:32:38,654
maybe like a simple manifestation which defines what

493
00:32:38,692 --> 00:32:42,720
your database is, provides some fundamental information.

494
00:32:44,150 --> 00:32:47,874
And when it comes to the more

495
00:32:47,912 --> 00:32:51,250
like day two, day, these type of operational

496
00:32:52,710 --> 00:32:55,490
exercises or operational activities,

497
00:32:56,410 --> 00:33:00,850
you leave it to Kubernetes. There is an excellent blog,

498
00:33:01,010 --> 00:33:05,654
it's a pretty old blog written in 2016 by

499
00:33:05,692 --> 00:33:09,654
a few folks from Coreos, the company which actually created

500
00:33:09,702 --> 00:33:13,820
the operator framework, which is pretty widely used

501
00:33:14,590 --> 00:33:18,198
today to build custom operators

502
00:33:18,214 --> 00:33:21,614
and controllers. You can go and

503
00:33:21,812 --> 00:33:25,054
check out this blog. But this blog kind of

504
00:33:25,092 --> 00:33:29,278
lays out the ideology behind

505
00:33:29,444 --> 00:33:33,026
creating operators, why we need them, and what

506
00:33:33,048 --> 00:33:36,260
is the whole value proposition of having them.

507
00:33:38,870 --> 00:33:43,102
So any operator in Kubernetes

508
00:33:43,246 --> 00:33:47,026
has two fundamental

509
00:33:47,138 --> 00:33:50,694
building blocks and we will discuss both of them.

510
00:33:50,732 --> 00:33:54,182
However, we already touched upon what controllers are.

511
00:33:54,236 --> 00:33:57,106
In this case, it's just going to be custom controllers,

512
00:33:57,138 --> 00:34:01,030
which you will write using the available utilities,

513
00:34:01,110 --> 00:34:04,634
sdks and libraries. But there is one more

514
00:34:04,752 --> 00:34:08,694
concept here, which is the Kubernetes API

515
00:34:08,742 --> 00:34:11,790
extensions or the custom resources.

516
00:34:12,130 --> 00:34:15,934
So what are custom resources? Right, let's dive into

517
00:34:15,972 --> 00:34:19,166
that. Custom resources. Well,

518
00:34:19,268 --> 00:34:22,866
it's not a new concept. It's been out

519
00:34:22,888 --> 00:34:26,862
there since pretty primitive version of Kubernetes,

520
00:34:26,926 --> 00:34:29,780
which is one seven, right?

521
00:34:30,310 --> 00:34:34,180
But custom resources offer you

522
00:34:34,630 --> 00:34:37,730
a mechanism to extend

523
00:34:37,890 --> 00:34:40,470
the Kubernetes API,

524
00:34:40,810 --> 00:34:44,822
which is to help you define your

525
00:34:44,876 --> 00:34:48,150
own custom kinds, right? I mean, if you look at deployment,

526
00:34:48,230 --> 00:34:51,994
stateful, set, pod, these all are

527
00:34:52,192 --> 00:34:54,826
a kind of object,

528
00:34:55,008 --> 00:34:58,506
right? These all are predefined kinds in

529
00:34:58,528 --> 00:35:02,670
Kubernetes. Kubernetes understands these. When you deploy

530
00:35:03,490 --> 00:35:07,374
a deployment through a YAML file and

531
00:35:07,412 --> 00:35:11,006
you mention the kind is deployment and the API version is

532
00:35:11,028 --> 00:35:14,690
this, which is the API group, and the version of the API,

533
00:35:15,110 --> 00:35:18,786
Kubernetes has an understanding of it, it knows what it

534
00:35:18,808 --> 00:35:19,700
is going to do.

535
00:35:21,430 --> 00:35:25,750
Similarly, if you have very

536
00:35:25,820 --> 00:35:29,334
specific type of workloads, and again, I will

537
00:35:29,372 --> 00:35:31,560
take database as an example,

538
00:35:32,570 --> 00:35:35,798
right? Yes. You can deploy, let's say a

539
00:35:35,804 --> 00:35:39,306
postgres database in production as a stateful set and be done with

540
00:35:39,328 --> 00:35:42,554
it, right? But is that

541
00:35:42,752 --> 00:35:45,866
the only thing that your database as a

542
00:35:45,888 --> 00:35:49,020
system consists of? Probably not,

543
00:35:49,550 --> 00:35:53,486
right? Because there is of

544
00:35:53,508 --> 00:35:57,454
course the storage part, which is the persistent volume and

545
00:35:57,492 --> 00:35:59,310
persistent volume claims.

546
00:36:00,290 --> 00:36:04,206
Additionally, you might want to create a couple of

547
00:36:04,308 --> 00:36:08,222
service fronting these pods so that your client can

548
00:36:08,276 --> 00:36:12,282
connect to these databases. You may want to define

549
00:36:12,346 --> 00:36:16,642
some access patterns, some database users

550
00:36:16,706 --> 00:36:20,882
their roles. You might want to define

551
00:36:20,946 --> 00:36:24,310
some secrets and passwords

552
00:36:24,650 --> 00:36:28,358
to be stored either natively or outside of the cluster,

553
00:36:28,534 --> 00:36:31,594
depending upon your architecture. So you can see

554
00:36:31,632 --> 00:36:36,086
or you can probably imagine by now that hey, it's kind of getting complicated,

555
00:36:36,198 --> 00:36:39,986
right? Because a database in a Kubernetes

556
00:36:40,038 --> 00:36:43,614
cluster may not just be a single resource, it's basically a

557
00:36:43,652 --> 00:36:45,950
collection of different resources,

558
00:36:46,530 --> 00:36:50,474
right? So how do I basically make it more abstract,

559
00:36:50,602 --> 00:36:53,810
something more generic which represents

560
00:36:54,310 --> 00:36:57,486
database to the end user who's

561
00:36:57,518 --> 00:37:01,554
deploying it, but at the same time does

562
00:37:01,592 --> 00:37:05,282
not overwhelm them, right? And also

563
00:37:05,336 --> 00:37:09,080
let Kubernetes figure out that what are different

564
00:37:09,770 --> 00:37:13,798
lower level mechanics it has to apply in

565
00:37:13,884 --> 00:37:17,400
order to honor this specific

566
00:37:17,790 --> 00:37:21,866
abstraction or the resource abstraction and get

567
00:37:21,888 --> 00:37:25,322
it functional and running, right? That's where custom

568
00:37:25,376 --> 00:37:29,286
resources come in, right? And they're

569
00:37:29,318 --> 00:37:33,046
pretty common. I mean almost every Kubernetes

570
00:37:33,078 --> 00:37:36,894
cluster that you are dealing with today in production would probably have some

571
00:37:36,932 --> 00:37:40,106
or the other custom resource either deployed explicitly

572
00:37:40,138 --> 00:37:44,770
by you or by the provider that you use, whether it's AWS

573
00:37:45,270 --> 00:37:49,858
or Azure or GCP, because that's how

574
00:37:50,024 --> 00:37:53,522
these providers are deploying a lot of managed components for

575
00:37:53,576 --> 00:37:56,770
added value is through custom resources and operators.

576
00:37:57,190 --> 00:38:00,754
But if you look at all these different CNCF hosted

577
00:38:00,802 --> 00:38:04,406
projects, istio Flux, argo link id,

578
00:38:04,508 --> 00:38:08,054
they heavily use custom resources and the idea

579
00:38:08,092 --> 00:38:11,574
is the same. Take istio as an example. So service mesh,

580
00:38:11,622 --> 00:38:14,650
right? And you just want to define,

581
00:38:17,150 --> 00:38:20,362
hey, these are the different policies that should

582
00:38:20,416 --> 00:38:23,818
exist. These are the different authorization

583
00:38:23,914 --> 00:38:25,790
rules that should exist,

584
00:38:26,450 --> 00:38:30,906
right? End of the day this has to translate into lower level constructs,

585
00:38:30,938 --> 00:38:34,980
right? And that lower level construct could be something

586
00:38:35,670 --> 00:38:39,090
as simple as an iptable rule,

587
00:38:39,910 --> 00:38:43,918
dropping the packets when one ip attempts

588
00:38:43,934 --> 00:38:47,506
to talk to another ip. But as an

589
00:38:47,528 --> 00:38:50,882
end user you don't want to deal with iptables directly,

590
00:38:50,946 --> 00:38:54,774
right? And probably there is no way for you to deal with them directly either.

591
00:38:54,972 --> 00:38:58,354
So how do you go about configuring them? You use custom resources

592
00:38:58,402 --> 00:39:02,826
which will tell Kubernetes in a way that hey, this means

593
00:39:03,008 --> 00:39:06,570
some change in the networking stack so let me go and perform

594
00:39:06,640 --> 00:39:10,010
it as a user, you don't worry about it, right?

595
00:39:10,160 --> 00:39:14,174
So that's the idea of custom resources is to help you define those higher

596
00:39:14,212 --> 00:39:17,566
level abstractions that end of the day you want to offer to

597
00:39:17,588 --> 00:39:22,118
your own end users, but at the same time are being understood

598
00:39:22,314 --> 00:39:25,634
by Kubernetes, which is responsible for taking the

599
00:39:25,672 --> 00:39:27,540
actions at a much lower level.

600
00:39:29,990 --> 00:39:33,762
But how do you go about building custom

601
00:39:33,816 --> 00:39:37,702
resources? You just can't create custom resources out of thin air,

602
00:39:37,756 --> 00:39:40,920
right? Every kind,

603
00:39:41,530 --> 00:39:45,526
every type that you deal with in Kubernetes has

604
00:39:45,548 --> 00:39:48,730
a schema. It has to follow a set of rules,

605
00:39:49,230 --> 00:39:51,898
right? I mean, a certain property can be of type,

606
00:39:51,984 --> 00:39:55,930
integer versus string versus boolean versus

607
00:39:56,350 --> 00:39:59,050
a map or a list, a dictionary,

608
00:39:59,710 --> 00:40:00,460
right?

609
00:40:02,210 --> 00:40:06,570
So when you create custom resource in Kubernetes,

610
00:40:06,650 --> 00:40:09,962
you actually start with something called as custom resource

611
00:40:10,026 --> 00:40:13,838
definition, right? Custom resource definition

612
00:40:13,934 --> 00:40:17,346
is essentially what provides you a

613
00:40:17,368 --> 00:40:21,250
well defined schema for creating your custom

614
00:40:21,320 --> 00:40:22,290
resources.

615
00:40:24,550 --> 00:40:27,794
Sometimes we call these CRD, sometimes customer source definition.

616
00:40:27,842 --> 00:40:31,320
I think you will hear the word CRD crds a lot

617
00:40:31,930 --> 00:40:35,830
throughout your Kubernetes journey. But crds essentially

618
00:40:36,490 --> 00:40:39,910
are what which provide the schema definition

619
00:40:39,990 --> 00:40:43,766
for your custom resources. So the sequence

620
00:40:43,798 --> 00:40:48,026
would be that you will write a CRD first and

621
00:40:48,048 --> 00:40:52,102
then you will apply it to your cluster. Then you will write a custom resource

622
00:40:52,166 --> 00:40:55,774
based on the CRD and you will apply the resource to the cluster and

623
00:40:55,812 --> 00:40:59,214
when you will do so, just like how API server is

624
00:40:59,252 --> 00:41:02,650
capable of checking the native objects for correctness

625
00:41:02,730 --> 00:41:06,206
and for adherence to the schema. Similarly,

626
00:41:06,238 --> 00:41:09,630
your custom resource would also be checked against its definition,

627
00:41:09,710 --> 00:41:12,820
whether it is meeting the criteria or not. And if not,

628
00:41:13,510 --> 00:41:17,030
your request would be rejected and the resource will not be created.

629
00:41:19,130 --> 00:41:24,018
Again, there's a quick sneak peek. This is from the official documentation,

630
00:41:24,194 --> 00:41:27,446
Kubernetes documentation. You can go and take a

631
00:41:27,468 --> 00:41:31,034
look yourself, but these

632
00:41:31,072 --> 00:41:34,220
idea here is that you have a CRD on the left.

633
00:41:35,310 --> 00:41:39,210
You define your API schema and then you eventually

634
00:41:39,710 --> 00:41:44,366
start translating or creating resources out

635
00:41:44,388 --> 00:41:48,026
of this CRD. As long as they adhere

636
00:41:48,058 --> 00:41:51,486
and they comply with what you have defined, you should

637
00:41:51,508 --> 00:41:54,370
be able to create these custom resources in your cluster.

638
00:41:55,670 --> 00:41:58,500
All right, but here's the problem, right?

639
00:42:00,630 --> 00:42:04,354
In one of the earlier slides, we saw the sequence of

640
00:42:04,392 --> 00:42:07,620
events which happen when you

641
00:42:07,990 --> 00:42:11,602
create a deployment, right? I mean, a bunch of controllers

642
00:42:11,666 --> 00:42:15,810
getting invoked and acting upon it within their own capacity

643
00:42:15,890 --> 00:42:18,700
and doing something, right,

644
00:42:19,150 --> 00:42:23,078
because they're aware of a certain type of resource,

645
00:42:23,254 --> 00:42:26,890
whether it's deployment or replica set or pod, whatever,

646
00:42:26,960 --> 00:42:30,114
right? But what about this custom resource?

647
00:42:30,262 --> 00:42:33,386
Who's aware of it? Yeah, it got created,

648
00:42:33,418 --> 00:42:37,360
it got persisted, you can query it. But what's really happening?

649
00:42:38,050 --> 00:42:42,914
Technically, nothing, right? Because the

650
00:42:42,952 --> 00:42:47,140
control loops that Kubernetes provide, they are specific to

651
00:42:47,590 --> 00:42:51,458
a certain kind. So in that case, that kind was either a

652
00:42:51,464 --> 00:42:55,166
deployment, a replica set, or a pod. Now,

653
00:42:55,208 --> 00:42:58,502
I have defined my own custom type here. There's nothing

654
00:42:58,556 --> 00:43:02,966
in that cluster. There's absolutely nothing in that cluster which

655
00:43:02,988 --> 00:43:06,614
is actually aware of any action that

656
00:43:06,652 --> 00:43:10,490
can be taken when this type of custom

657
00:43:10,560 --> 00:43:14,410
resource is created. That's where we write custom

658
00:43:14,480 --> 00:43:18,122
controllers. That's the missing piece of puzzle. And when

659
00:43:18,176 --> 00:43:21,886
we glue them together, these is exactly what

660
00:43:21,908 --> 00:43:24,640
we get through operators, right?

661
00:43:25,250 --> 00:43:28,154
The custom resource that we define,

662
00:43:28,282 --> 00:43:32,366
we create, and the custom controllers which

663
00:43:32,388 --> 00:43:36,062
are basically the control loops which are now going to be aware

664
00:43:36,126 --> 00:43:39,746
of this custom resource and will implement the

665
00:43:39,768 --> 00:43:44,226
business logic, which will take a

666
00:43:44,248 --> 00:43:48,390
list of action and will deal with the lower level mechanics

667
00:43:49,450 --> 00:43:53,826
of kubernetes. Whether your custom resource demanded creation

668
00:43:53,858 --> 00:43:57,586
of a stateful set and persistent storage,

669
00:43:57,698 --> 00:44:01,334
a bunch of secrets and config maps, or it requested

670
00:44:01,382 --> 00:44:04,746
something else, doesn't matter. But now you have

671
00:44:04,768 --> 00:44:08,666
a controller who's looking for it and

672
00:44:08,688 --> 00:44:12,234
the very last bullet point here, right, these operator SDK.

673
00:44:12,362 --> 00:44:16,000
So that is a utility that we will actually use or we'll actually

674
00:44:16,370 --> 00:44:20,702
take a look at to

675
00:44:20,756 --> 00:44:24,002
build out a custom resource and a custom

676
00:44:24,056 --> 00:44:27,426
controller. There are a couple of

677
00:44:27,448 --> 00:44:30,834
other frameworks also, but we will walk

678
00:44:30,872 --> 00:44:33,700
through operator SDK in this talk.

679
00:44:35,130 --> 00:44:38,694
Of course, it's one of the more widely used and pretty

680
00:44:38,732 --> 00:44:41,110
easy to use framework.

681
00:44:43,450 --> 00:44:47,834
Yeah, we will quickly take a look at it. Like all

682
00:44:47,872 --> 00:44:51,734
the boilerplate stuff that operator SDK

683
00:44:51,782 --> 00:44:56,134
provides and how it basically simplifies the operator

684
00:44:56,182 --> 00:44:59,594
development in kubernetes. So there

685
00:44:59,632 --> 00:45:01,630
is Kube builder framework.

686
00:45:03,490 --> 00:45:06,558
We briefly talked about the operator SDK, which is part of the

687
00:45:06,564 --> 00:45:10,654
operator framework. It's not super important to

688
00:45:10,692 --> 00:45:14,066
know Kube builder framework in

689
00:45:14,088 --> 00:45:17,634
and out, but what is definitely good

690
00:45:17,672 --> 00:45:21,186
to know, since you will probably be dealing more with,

691
00:45:21,288 --> 00:45:24,850
if you happen to work with operator framework and operator SDK,

692
00:45:25,690 --> 00:45:29,074
you would be dealing with these specific toolkit that operator SDK

693
00:45:29,122 --> 00:45:32,530
provide. But Cube builder awareness

694
00:45:32,610 --> 00:45:36,178
is important because one thing to note

695
00:45:36,194 --> 00:45:39,894
here is that operator framework is

696
00:45:39,932 --> 00:45:43,078
actually built upon cube builder framework. So Cube

697
00:45:43,094 --> 00:45:46,954
builder framework existed before and then

698
00:45:47,152 --> 00:45:50,398
the operator framework kind of came in, made it

699
00:45:50,404 --> 00:45:53,854
a little bit more simpler, intuitive to use,

700
00:45:54,052 --> 00:45:57,520
but the fundamental building blocks, right? Things like

701
00:45:58,130 --> 00:46:01,018
informers, workers, clients,

702
00:46:01,114 --> 00:46:04,778
reconcilers, they were all defined by these cube builder

703
00:46:04,794 --> 00:46:08,918
framework. So just for your awareness that there is Kube

704
00:46:08,954 --> 00:46:12,674
builder framework, of course you can build

705
00:46:12,712 --> 00:46:16,438
an operator using Kube builder framework directly. Many people do that.

706
00:46:16,604 --> 00:46:20,226
And if you want to explore more about the Cube builder

707
00:46:20,338 --> 00:46:23,480
framework, there's a link here at the bottom.

708
00:46:23,930 --> 00:46:28,230
There's an excellent online book about Kubebuilder,

709
00:46:28,890 --> 00:46:32,346
which also has a lot of examples and

710
00:46:32,528 --> 00:46:35,882
tutorials and DevOps for you to take a look at. So please do refer

711
00:46:35,936 --> 00:46:39,494
to it. And then there's operator SDK,

712
00:46:39,542 --> 00:46:42,650
of course, right. So operator SDK is part of the operator framework.

713
00:46:42,810 --> 00:46:44,240
It is based on,

714
00:46:46,370 --> 00:46:49,680
you know, developed by Coreos and Red Hat together.

715
00:46:50,370 --> 00:46:53,902
And like I was mentioning the boilerplate

716
00:46:53,966 --> 00:46:57,860
stuff on the previous slide. So operator SDK actually

717
00:46:58,630 --> 00:47:02,430
uses a bunch of libraries

718
00:47:02,510 --> 00:47:06,550
like controller runtime, API machinery

719
00:47:08,090 --> 00:47:11,782
and many others actually to make the

720
00:47:11,916 --> 00:47:14,550
operator development easier,

721
00:47:14,890 --> 00:47:18,026
right? And take care of some

722
00:47:18,128 --> 00:47:21,754
rudimentary stuff which

723
00:47:21,792 --> 00:47:25,210
you otherwise as a developer would probably prefer not to do,

724
00:47:25,360 --> 00:47:27,130
right? So scaffolding,

725
00:47:28,430 --> 00:47:32,190
creating automated unit test cases,

726
00:47:32,610 --> 00:47:36,206
a lot of code generation, bootstrapping, a way to

727
00:47:36,228 --> 00:47:39,466
run your operator locally while connecting

728
00:47:39,498 --> 00:47:42,510
to a remote Kubernetes cluster for testing purposes.

729
00:47:43,190 --> 00:47:47,422
And what is more interesting about the operator framework

730
00:47:47,486 --> 00:47:51,646
that we saw examples in the previous slides

731
00:47:51,838 --> 00:47:54,980
from the replica set controller and how it was written in Go.

732
00:47:55,350 --> 00:47:58,790
But that's what I was mentioning, that you don't have to kind of bog yourself

733
00:47:58,860 --> 00:48:02,082
down if you do not know Go, because operators

734
00:48:02,146 --> 00:48:05,494
SDK not only supports writing operator with

735
00:48:05,532 --> 00:48:10,486
Go, but you could actually write operators

736
00:48:10,518 --> 00:48:13,020
with ansible and help.

737
00:48:13,790 --> 00:48:17,370
And the link is there at the bottom. You can refer

738
00:48:17,440 --> 00:48:21,242
to the operator SDK documentation or operator framework documentation.

739
00:48:21,306 --> 00:48:25,246
There are a lot of details out there, but the idea is

740
00:48:25,428 --> 00:48:28,942
that if you have

741
00:48:28,996 --> 00:48:32,366
a choice of runtime and it's not

742
00:48:32,468 --> 00:48:36,526
go, but rather you are comfortable writing your operator in something else like ansible

743
00:48:36,558 --> 00:48:39,634
or helm, you could actually do that. And by the way,

744
00:48:39,832 --> 00:48:42,274
if you have, let's say,

745
00:48:42,312 --> 00:48:45,230
python programming background,

746
00:48:45,390 --> 00:48:48,866
there is an operator framework which

747
00:48:48,888 --> 00:48:52,482
is called as Kopf or cough, which is Kubernetes operator

748
00:48:52,546 --> 00:48:55,990
pythonic framework. You can check that, but as well.

749
00:48:56,060 --> 00:49:00,362
So like I was saying, knowledge of Go is

750
00:49:00,416 --> 00:49:04,374
nice to have, especially if you want to kind of dive deep

751
00:49:04,422 --> 00:49:08,470
into some of the engineering decisions that Kubernetes

752
00:49:08,550 --> 00:49:12,718
team has made or will make in future as the Kubernetes platform

753
00:49:12,804 --> 00:49:17,150
continue to evolve. But if you are trying to extend Kubernetes,

754
00:49:17,890 --> 00:49:21,454
if you're trying to create your own custom resources and operators and write your own

755
00:49:21,492 --> 00:49:25,134
control loops and you do not know go, that's perfectly

756
00:49:25,182 --> 00:49:29,250
fine because there are other options out there and people are using frameworks like

757
00:49:29,320 --> 00:49:32,594
Kopf. This talk is revolving around

758
00:49:32,712 --> 00:49:36,018
Kube builder and operator SDK or the operator framework essentially.

759
00:49:36,114 --> 00:49:38,790
But like I said, it's not a hard limitation.

760
00:49:41,930 --> 00:49:45,186
Now, I have given some pretty basic commands

761
00:49:45,218 --> 00:49:49,786
here for you to refer. And of

762
00:49:49,808 --> 00:49:53,578
course, given the time foundation and the

763
00:49:53,584 --> 00:49:56,954
length of this talk, while I won't be able to do

764
00:49:57,152 --> 00:50:00,314
a live development of an

765
00:50:00,352 --> 00:50:03,770
operator, these are still very handy commands,

766
00:50:03,850 --> 00:50:07,374
and there's a lot of documentation and information about what these

767
00:50:07,412 --> 00:50:11,306
commands are really trying to do. If you refer to the official documentation

768
00:50:11,338 --> 00:50:15,374
of the operator framework. But executing

769
00:50:15,422 --> 00:50:19,266
these commands and what you should expect as you

770
00:50:19,288 --> 00:50:23,234
execute the command, that is something we will definitely go

771
00:50:23,272 --> 00:50:26,614
through in a moment. I will share my

772
00:50:26,732 --> 00:50:30,406
screen with my ide so we

773
00:50:30,428 --> 00:50:34,722
can actually take a look at all the scaffolding,

774
00:50:34,786 --> 00:50:38,662
all the boilerplate code generation and what's really happening

775
00:50:38,716 --> 00:50:42,246
behind the scene. But these are some of the key commands

776
00:50:42,358 --> 00:50:45,926
that you would actually need to know or need to be aware

777
00:50:45,958 --> 00:50:49,498
of, starting from the initialization where I give

778
00:50:49,584 --> 00:50:53,166
a appdomain option. So that domain option would basically become a

779
00:50:53,188 --> 00:50:56,526
part of my API group, right? If you

780
00:50:56,548 --> 00:50:59,598
know that API groups are basically qualified subdomain names in

781
00:50:59,604 --> 00:51:03,698
Kubernetes, the repo here is nothing

782
00:51:03,784 --> 00:51:07,390
but kind of a name for my Go module,

783
00:51:07,470 --> 00:51:11,646
because end of the day my operator would be packaged

784
00:51:11,678 --> 00:51:15,106
and served as a Go module. So this

785
00:51:15,128 --> 00:51:18,162
GitHub.com slash acme redis operator,

786
00:51:18,226 --> 00:51:21,986
no, it does not have to exist somewhere on GitHub.

787
00:51:22,098 --> 00:51:24,550
It is just the go module naming convention.

788
00:51:25,850 --> 00:51:28,950
The second command creates the APIs and the types,

789
00:51:29,450 --> 00:51:32,858
the Go types. So very important to understand as well,

790
00:51:32,944 --> 00:51:36,890
the group here is cache. So when you will actually create your manifestation,

791
00:51:37,230 --> 00:51:41,078
like how you see apps, one when you create a deployment

792
00:51:41,174 --> 00:51:44,394
or networking k IO if you create a network

793
00:51:44,442 --> 00:51:48,206
policy, when you will create a

794
00:51:48,228 --> 00:51:51,374
resource of type redis which is

795
00:51:51,412 --> 00:51:54,450
basically provided in the kind option. In the second command,

796
00:51:55,110 --> 00:51:58,606
the API group would be cache appdomain,

797
00:51:58,638 --> 00:52:03,054
which is Acme IO, and then the API

798
00:52:03,102 --> 00:52:06,754
version. So if you're comfortable, if you know how

799
00:52:06,792 --> 00:52:10,646
the version semantics really work within Kubernetes. So we

800
00:52:10,668 --> 00:52:14,546
are starting with v one, alpha one, which is of course not a production

801
00:52:14,578 --> 00:52:18,342
ready API. And as we mature it, we go to beta one,

802
00:52:18,396 --> 00:52:21,562
we go to beta, and then we go to g. Eight becomes v one,

803
00:52:21,696 --> 00:52:24,522
right? So that's how the Kubernetes version semantics really work.

804
00:52:24,576 --> 00:52:27,690
Again, not something we need to go into details.

805
00:52:28,270 --> 00:52:32,762
And then of course there are some make commands here. Operator SDK

806
00:52:32,826 --> 00:52:36,122
uses a make file with some specific target,

807
00:52:36,186 --> 00:52:41,950
and each command has its own significance, starting from generating

808
00:52:43,110 --> 00:52:47,326
the types, the kinds to generating manifestation,

809
00:52:47,438 --> 00:52:51,522
which involves creating crds and some

810
00:52:51,576 --> 00:52:55,774
bases and some samples and some cluster

811
00:52:55,822 --> 00:52:58,994
roles and role bindings for your operator

812
00:52:59,042 --> 00:53:02,886
so that it has the appropriate permissions to operate on a specific type of

813
00:53:02,908 --> 00:53:06,054
resource. And then the make install run command is

814
00:53:06,092 --> 00:53:10,074
basically a utility command which

815
00:53:10,112 --> 00:53:13,974
is included in the framework to help you run the operator locally.

816
00:53:14,022 --> 00:53:17,926
However, you can assume that this is only for local testing

817
00:53:17,958 --> 00:53:21,454
and development, and as you would

818
00:53:21,652 --> 00:53:25,470
develop an operator for your production systems,

819
00:53:25,890 --> 00:53:30,606
end of the day, your operator would be deployed to your cluster as

820
00:53:30,628 --> 00:53:34,050
a Kubernetes deployment eventually. Right?

821
00:53:34,200 --> 00:53:37,986
So let's do

822
00:53:38,168 --> 00:53:42,020
a quick walk through. So I'm just going to

823
00:53:42,550 --> 00:53:46,200
unshare this for a moment and

824
00:53:48,170 --> 00:53:52,086
share my screen. Just give

825
00:53:52,108 --> 00:53:53,560
me a moment here.

826
00:53:56,090 --> 00:54:00,234
All right, so let's see a working example

827
00:54:00,432 --> 00:54:04,362
of an operator. On the previous slide we

828
00:54:04,416 --> 00:54:08,474
saw bunch of different commands with

829
00:54:08,512 --> 00:54:12,334
respect to the operator SDK and what each and every command would

830
00:54:12,372 --> 00:54:15,934
actually do. However, the idea is kind of not to

831
00:54:15,972 --> 00:54:20,110
go into the details of what I am really getting by

832
00:54:20,180 --> 00:54:23,726
each and every individual command that I execute, because you

833
00:54:23,748 --> 00:54:26,894
can very well find that level of details in the official

834
00:54:26,942 --> 00:54:30,546
documentation of the operator framework. The idea here is

835
00:54:30,568 --> 00:54:33,762
to kind of really make you understand that how

836
00:54:33,816 --> 00:54:37,410
these operators are going to behave at runtime.

837
00:54:37,490 --> 00:54:40,200
Right now, before we begin,

838
00:54:40,890 --> 00:54:45,202
I just want to kind of make you aware of the directory

839
00:54:45,266 --> 00:54:49,750
structure that I'm using here. So conf 42 Redis operator

840
00:54:49,910 --> 00:54:54,630
is my project directory. Now here you see, these are a lot of subdirectories,

841
00:54:54,710 --> 00:54:57,500
a bunch of files in here. There's a lot of go code.

842
00:54:59,310 --> 00:55:02,446
Let me make you understand a

843
00:55:02,468 --> 00:55:06,730
couple of things, right, the operator

844
00:55:06,810 --> 00:55:10,106
framework, or even for that sake, if you're

845
00:55:10,138 --> 00:55:13,762
using cube builder, their job is

846
00:55:13,816 --> 00:55:17,790
to simplify the operator development

847
00:55:17,870 --> 00:55:22,100
task, and they do so by

848
00:55:22,630 --> 00:55:26,226
taking care of lot of boilerplate stuff that you

849
00:55:26,248 --> 00:55:29,078
will otherwise have to write by yourself, right?

850
00:55:29,244 --> 00:55:33,126
So there's a lot of scaffolding that happens behind the scene as

851
00:55:33,148 --> 00:55:36,406
you run those commands. That includes generation of lot

852
00:55:36,428 --> 00:55:40,994
of these files, go code markers,

853
00:55:41,122 --> 00:55:44,774
annotations. Even your custom resource definition

854
00:55:44,822 --> 00:55:48,410
is created based upon what

855
00:55:48,480 --> 00:55:52,534
values do you provide against some of those options when you do an operator

856
00:55:52,662 --> 00:55:57,040
in it and create APIs? There is also,

857
00:55:57,410 --> 00:56:01,210
like I said, a lot of go code that gets generated.

858
00:56:01,290 --> 00:56:04,900
For example, redistypes. Go basically

859
00:56:05,670 --> 00:56:09,438
defines the go structure representation

860
00:56:09,534 --> 00:56:12,834
for my custom kind, which in this case is Redis. Again,

861
00:56:12,872 --> 00:56:16,962
if you do not know go programming or have not worked

862
00:56:17,016 --> 00:56:20,390
exclusively in Go, that is perfectly fine.

863
00:56:20,540 --> 00:56:23,958
The idea here is kind of not to make you

864
00:56:24,044 --> 00:56:28,098
a Go expert or assume that you are a Go expert, but just

865
00:56:28,124 --> 00:56:31,482
to kind of show you the whole value proposition of

866
00:56:31,536 --> 00:56:35,514
using a framework like operator framework, right? All these go

867
00:56:35,552 --> 00:56:38,954
files, trust me, I didn't write anything from

868
00:56:38,992 --> 00:56:42,382
the scratch. A lot of skeleton code was created for me

869
00:56:42,516 --> 00:56:45,934
and then I happened to just kind of decorate this

870
00:56:45,972 --> 00:56:49,726
code as per my needs and my requirements and run some

871
00:56:49,748 --> 00:56:53,506
of those, make commands to create my

872
00:56:53,528 --> 00:56:57,362
CID, generate a skeleton code for my

873
00:56:57,496 --> 00:57:00,500
controller, things like that.

874
00:57:01,190 --> 00:57:04,626
So if you look

875
00:57:04,648 --> 00:57:08,366
at right here under config CID,

876
00:57:08,558 --> 00:57:11,482
since we just mentioned CID,

877
00:57:11,566 --> 00:57:15,634
look at this, I have my custom resource definition created,

878
00:57:15,762 --> 00:57:19,094
right? This is where I have the whole

879
00:57:19,132 --> 00:57:22,694
API schema for my custom resource, which is redis

880
00:57:22,742 --> 00:57:26,870
in this case. And what's going to look like it has taken care of specifying

881
00:57:27,030 --> 00:57:29,740
all different things here, right? Whether the group,

882
00:57:30,050 --> 00:57:31,040
the kind,

883
00:57:33,170 --> 00:57:37,262
the API version, whether my type is

884
00:57:37,396 --> 00:57:40,974
namespace is scoped or cluster is scoped, all these details are actually

885
00:57:41,012 --> 00:57:44,282
captured under CMD.

886
00:57:44,346 --> 00:57:47,680
I have my main go. So if you're familiar with,

887
00:57:48,390 --> 00:57:51,890
you know, package main function main is always going to be your

888
00:57:52,040 --> 00:57:55,906
entry point for the program to execute.

889
00:57:56,098 --> 00:58:00,370
And here I see that with the help of controller runtime package,

890
00:58:00,530 --> 00:58:04,946
which I'm importing here, I'm able to instantiate a manager.

891
00:58:05,138 --> 00:58:08,802
And if you recollect on the architecture slide

892
00:58:08,866 --> 00:58:12,026
that we reviewed like what Kubernetes architecture looks like,

893
00:58:12,128 --> 00:58:15,434
there was this controller manager which was responsible for

894
00:58:15,472 --> 00:58:19,226
running and managing a bunch of different control loops. This is exactly what

895
00:58:19,248 --> 00:58:22,878
it is. This is my controller manager, and this

896
00:58:22,884 --> 00:58:26,510
is where I'll be bootstrapping my custom control

897
00:58:26,580 --> 00:58:30,080
loops, right? And then if I go here,

898
00:58:31,010 --> 00:58:34,766
internal controller, I see the redis underscore

899
00:58:34,798 --> 00:58:38,770
controller go. This is where all the business

900
00:58:38,840 --> 00:58:43,294
logic when it comes to handling the resource or resources

901
00:58:43,342 --> 00:58:46,906
of type redis, all the business logic

902
00:58:46,958 --> 00:58:50,534
goes in here, right? So what I'm going to do when a resource of type

903
00:58:50,572 --> 00:58:54,310
redis is created, of course I'm going to go and

904
00:58:54,460 --> 00:58:58,006
try to find one, right? Because the event says the

905
00:58:58,028 --> 00:59:02,426
resource has been created. If I'm not able to find, maybe this

906
00:59:02,448 --> 00:59:05,750
is resource deletion. And if it is resource deletion,

907
00:59:05,830 --> 00:59:09,222
then let's see if it has got finalizers. If you're familiar

908
00:59:09,286 --> 00:59:13,038
with what finalizers are meant for in Kubernetes, it's basically

909
00:59:13,124 --> 00:59:16,794
to do some cleanup work before the resource

910
00:59:16,842 --> 00:59:20,430
is actually deleted from the cluster. In this case, I don't have any such

911
00:59:20,500 --> 00:59:24,782
complicated scenarios, but just for the sake of it, the methods exist.

912
00:59:24,926 --> 00:59:28,366
So if you have finalizer go and honor the finalizer

913
00:59:28,558 --> 00:59:31,090
before proceeding with the resource deletion.

914
00:59:31,670 --> 00:59:35,780
Now this is where it actually gets interesting. Line 159

915
00:59:36,250 --> 00:59:39,922
when I'm actually requesting a deployment for my redis resource.

916
00:59:39,986 --> 00:59:43,702
Because like I was saying, redis as such,

917
00:59:43,836 --> 00:59:46,770
as a kind, as a type, means nothing to kubernetes,

918
00:59:46,850 --> 00:59:50,522
right? It's not a native Kubernetes object, right? I am these one

919
00:59:50,576 --> 00:59:53,978
who is providing it some meaning by the means of

920
00:59:53,984 --> 00:59:57,180
this controller, right? But end of the day it has to

921
00:59:58,190 --> 01:00:01,546
roll out into lower level Kubernetes constructs. That is,

922
01:00:01,568 --> 01:00:05,066
there has to be a deployment or a stateful set, there has to be pods,

923
01:00:05,098 --> 01:00:08,286
it has to be containers, right? And this is exactly what I'm trying to do

924
01:00:08,308 --> 01:00:12,698
here, is to basically try and find a

925
01:00:12,724 --> 01:00:16,370
deployment if it exists. If not, go ahead and

926
01:00:16,440 --> 01:00:20,018
create one, right? There is a bit of a

927
01:00:20,024 --> 01:00:23,502
reconciliation logic going on here as well, where I'm comparing

928
01:00:23,566 --> 01:00:26,626
these size as in like the size that

929
01:00:26,648 --> 01:00:30,514
I specify when I create my redis resource versus

930
01:00:30,562 --> 01:00:34,086
what is the observed state, right? How many number of replicas that I

931
01:00:34,108 --> 01:00:37,486
have and if there is a difference, if there's a drift,

932
01:00:37,618 --> 01:00:40,938
go ahead and correct that drift. So in a

933
01:00:40,944 --> 01:00:44,986
nutshell, my control loop as we can

934
01:00:45,008 --> 01:00:48,650
see here is kind of managing the whole

935
01:00:48,800 --> 01:00:53,058
lifecycle aspect of a resource type redis

936
01:00:53,254 --> 01:00:56,686
without even really revealing all these details to

937
01:00:56,708 --> 01:00:59,722
the end user. And that is what I was trying to stress upon,

938
01:00:59,786 --> 01:01:03,870
that when you deal with custom resources, when you deal with operators,

939
01:01:03,950 --> 01:01:06,610
when you deal with custom control loops,

940
01:01:07,190 --> 01:01:11,826
that's the whole value proposition of it, right? That you

941
01:01:11,848 --> 01:01:15,418
can really simplify how your workloads

942
01:01:15,454 --> 01:01:19,602
are actually represented to your consumers. Because kubernetes

943
01:01:19,666 --> 01:01:23,030
is hard, it's complicated and it's probably

944
01:01:23,180 --> 01:01:27,046
not a fair assumption to make that everybody

945
01:01:27,148 --> 01:01:30,554
but there is super familiar with all

946
01:01:30,592 --> 01:01:34,026
the lower level details and know the functionings of

947
01:01:34,048 --> 01:01:37,274
kubernetes. So how can you basically abstract it out, right?

948
01:01:37,312 --> 01:01:40,670
How can you basically make life simple for them without

949
01:01:40,740 --> 01:01:44,782
compromising on the Kubernetes best

950
01:01:44,836 --> 01:01:48,510
practices automation standards

951
01:01:49,330 --> 01:01:52,702
operators gives you that control, it gives you that way.

952
01:01:52,756 --> 01:01:56,818
And you can also get very opinionated, right? Because you

953
01:01:56,824 --> 01:02:00,126
can only expect abstractions to be created by your user

954
01:02:00,158 --> 01:02:03,586
and then you can basically build your controls in

955
01:02:03,608 --> 01:02:07,590
terms of what really happens when user actually end up requesting

956
01:02:07,930 --> 01:02:11,634
your custom resources, right? However you want to control it, you want to specify

957
01:02:11,682 --> 01:02:14,280
some specific security measures. For example,

958
01:02:14,810 --> 01:02:19,106
the container should not run as root, the container

959
01:02:19,138 --> 01:02:23,066
file system should be read only or

960
01:02:23,168 --> 01:02:26,586
this specific image tag may not be used or

961
01:02:26,608 --> 01:02:30,140
is not approved. So think about it, right? I mean,

962
01:02:30,590 --> 01:02:34,398
how far can you go with these operators and controllers? There is absolutely

963
01:02:34,484 --> 01:02:38,846
no limit. And the example here is a pretty simple one just

964
01:02:38,948 --> 01:02:42,522
stateless redis cache. But when you think about

965
01:02:42,596 --> 01:02:46,542
more complicated workloads like say postgres

966
01:02:46,606 --> 01:02:49,950
database in production, and think about all the operational

967
01:02:50,030 --> 01:02:52,370
aspect of a database,

968
01:02:52,950 --> 01:02:57,830
like starting from backup recovery snapshots,

969
01:02:58,410 --> 01:03:02,514
point in time recovery log archivals,

970
01:03:02,642 --> 01:03:06,774
everything becomes important, right? But you necessarily don't want to

971
01:03:06,812 --> 01:03:10,566
assume that your end user is a database administrator,

972
01:03:10,598 --> 01:03:14,282
right? So you can give them a bit of abstraction through these custom

973
01:03:14,336 --> 01:03:17,830
resources and then you basically transform

974
01:03:17,910 --> 01:03:21,782
them, translate them through your own operational

975
01:03:21,846 --> 01:03:25,482
expertise in that specific field, which is postgres or kubernetes

976
01:03:25,546 --> 01:03:29,338
in general. How do you want these resource to be handled?

977
01:03:29,434 --> 01:03:32,866
What should happen when a backup is requested? What should happen when a point in

978
01:03:32,888 --> 01:03:36,260
time recovery is requested? Things like that.

979
01:03:36,630 --> 01:03:40,354
So let's see it in action. Like I said,

980
01:03:40,472 --> 01:03:44,386
I have done a lot of stuff already

981
01:03:44,488 --> 01:03:48,382
before the talk. Well, for the Kubernetes

982
01:03:48,446 --> 01:03:52,310
cluster I'm using local kind cluster,

983
01:03:52,970 --> 01:03:56,920
though it's not a hard requirement, you are free to experiment with

984
01:03:57,450 --> 01:04:00,762
any other distribution of kubernetes, even if you have

985
01:04:00,896 --> 01:04:04,038
a cluster at your disposal from EKs,

986
01:04:04,134 --> 01:04:07,786
AKs or GKE. Please feel free to use that as

987
01:04:07,808 --> 01:04:11,786
long as you're authenticated to the cluster and you have cluster admin rights.

988
01:04:11,978 --> 01:04:15,530
For all the local development and experimentation purposes,

989
01:04:15,610 --> 01:04:17,950
I prefer using kind or minikube.

990
01:04:19,330 --> 01:04:22,970
They're pretty good. And for this demo I'm using a

991
01:04:22,980 --> 01:04:26,530
local kind cluster. So just want to make sure that my

992
01:04:26,600 --> 01:04:30,290
cluster is up and running and listening to the API request,

993
01:04:31,110 --> 01:04:33,780
which it does. So great.

994
01:04:34,390 --> 01:04:38,920
Now let's start from beginning and

995
01:04:39,850 --> 01:04:43,654
let us start the control loop here.

996
01:04:43,772 --> 01:04:47,462
So I'll be using some specific make commands to

997
01:04:47,516 --> 01:04:50,966
bootstrap my controller locally. So it's not going to run within the cluster,

998
01:04:50,998 --> 01:04:54,714
but it's going to run outside as a standalone process and would

999
01:04:54,752 --> 01:04:58,134
actually use the Kubeconfig

1000
01:04:58,182 --> 01:05:01,962
file and its current context, which is an authenticated cluster admin

1001
01:05:02,026 --> 01:05:05,198
user towards my kind cluster to make the

1002
01:05:05,204 --> 01:05:08,506
API request, right? So I'm going to run this command,

1003
01:05:08,618 --> 01:05:12,238
make, install, run. This is going to

1004
01:05:12,324 --> 01:05:15,726
bootstrap my controller manager

1005
01:05:15,758 --> 01:05:18,450
and that's my controller locally.

1006
01:05:21,350 --> 01:05:24,434
All right, so it looks like my controller is up.

1007
01:05:24,552 --> 01:05:28,498
Now I go to this terminal here in this directory.

1008
01:05:28,594 --> 01:05:33,042
I have already created some sample

1009
01:05:33,106 --> 01:05:36,870
manifest files. This is the file of our interest.

1010
01:05:37,020 --> 01:05:40,614
So if you copy it and just look at it like what I'm really trying

1011
01:05:40,652 --> 01:05:41,400
to do,

1012
01:05:43,850 --> 01:05:47,834
just a very simple definition for

1013
01:05:47,872 --> 01:05:51,614
my redis type of resource. I'm just

1014
01:05:51,652 --> 01:05:56,158
requesting the cluster that hey, create a

1015
01:05:56,164 --> 01:05:59,806
redis cache with three replicas, right? So you could

1016
01:05:59,828 --> 01:06:02,750
see that I'm requesting redis,

1017
01:06:03,170 --> 01:06:06,754
a resource of type redis, right? But when

1018
01:06:06,792 --> 01:06:10,626
my controller will see it, it's actually going to translate it to

1019
01:06:10,648 --> 01:06:14,290
a deployment with three replicas. And that is exactly what we want to verify.

1020
01:06:15,190 --> 01:06:19,460
So let's do kubectl apply f

1021
01:06:20,390 --> 01:06:23,990
sample cache yaml.

1022
01:06:24,730 --> 01:06:29,062
Okay, so it says it created it. Now this resource

1023
01:06:29,126 --> 01:06:33,414
was a namespace scoped resource. So let's

1024
01:06:33,462 --> 01:06:37,850
verify if there is a resource of type redis

1025
01:06:38,510 --> 01:06:42,220
namespacecon 42 in it.

1026
01:06:42,610 --> 01:06:44,960
And there indeed is.

1027
01:06:46,050 --> 01:06:49,770
Now let's see if it also created

1028
01:06:49,850 --> 01:06:53,746
a deployment in these

1029
01:06:53,848 --> 01:06:56,862
namespace which corresponds to the redis resource.

1030
01:06:57,006 --> 01:07:00,462
And there is, you could see that there are three replicas,

1031
01:07:00,526 --> 01:07:03,300
all three are healthy up and running.

1032
01:07:03,750 --> 01:07:08,200
Right now, if I go ahead and

1033
01:07:09,050 --> 01:07:12,230
delete this redis

1034
01:07:12,570 --> 01:07:13,590
cache,

1035
01:07:15,610 --> 01:07:18,940
this redis cache resource, what do you think should happen?

1036
01:07:19,310 --> 01:07:22,358
It should not only delete the custom resource,

1037
01:07:22,534 --> 01:07:25,914
but if you remember, we saw it

1038
01:07:25,952 --> 01:07:29,370
here in the controller code towards

1039
01:07:29,440 --> 01:07:33,498
the bottom. When I am setting up the control

1040
01:07:33,584 --> 01:07:37,610
group with my controller manager that I'm specifying,

1041
01:07:37,770 --> 01:07:41,950
it also owns the deployment

1042
01:07:42,790 --> 01:07:46,180
that it creates. So that is very important.

1043
01:07:46,550 --> 01:07:50,434
And it actually is reflected through this

1044
01:07:50,472 --> 01:07:54,190
line of code right here when I'm

1045
01:07:54,270 --> 01:07:58,466
creating the deployment before associating it with my redis resource

1046
01:07:58,658 --> 01:08:02,358
set controller reference. If I don't set it,

1047
01:08:02,524 --> 01:08:05,926
then this deployment will actually fall under the

1048
01:08:05,948 --> 01:08:09,494
control of the control loop which is

1049
01:08:09,692 --> 01:08:13,142
built into the controller manager inside of these Kubernetes control

1050
01:08:13,196 --> 01:08:17,254
plane. And we don't want it to happen. We want the lifecycle

1051
01:08:17,302 --> 01:08:20,670
of this deployment to be managed by this custom

1052
01:08:20,740 --> 01:08:24,560
controller. Okay, so if we delete this

1053
01:08:25,250 --> 01:08:28,330
and then see if I'm still able to find a deployment.

1054
01:08:28,490 --> 01:08:32,702
No, these was found. So this way we were able to

1055
01:08:32,836 --> 01:08:36,590
kind of tie up the lifecycle of the custom

1056
01:08:36,660 --> 01:08:40,126
resource and everything that custom resource kind of rolled

1057
01:08:40,158 --> 01:08:43,140
out into, in this case a Kubernetes deployment together.

1058
01:08:44,390 --> 01:08:48,022
So that's on a high level how operators really work.

1059
01:08:48,076 --> 01:08:52,278
Of course you can play around more than that, like try

1060
01:08:52,364 --> 01:08:55,606
modifying the size, try deleting one of

1061
01:08:55,628 --> 01:08:59,610
the replicas from the deployment and see what happens behind the scenes.

1062
01:09:00,430 --> 01:09:02,940
But the idea is that, well,

1063
01:09:05,870 --> 01:09:09,674
operators are there to provide you

1064
01:09:09,712 --> 01:09:13,258
a mechanism to apply the operators

1065
01:09:13,354 --> 01:09:18,366
knowledge that you have about a specific type of workload in

1066
01:09:18,388 --> 01:09:22,046
a Kubernetes native way. Right? So you could

1067
01:09:22,068 --> 01:09:25,474
do it in maybe several other different ways. We took

1068
01:09:25,512 --> 01:09:29,074
the database example and of course those

1069
01:09:29,112 --> 01:09:32,658
who have been into DBA roles, they understand pretty well

1070
01:09:32,744 --> 01:09:36,242
what it means to take periodic backups,

1071
01:09:36,386 --> 01:09:39,926
full backups, partial backups, snapshots and whatnot. Right?

1072
01:09:40,108 --> 01:09:44,178
But end of the day, when you look at these systems

1073
01:09:44,274 --> 01:09:48,790
running at large scale and especially in kubernetes,

1074
01:09:49,450 --> 01:09:53,490
if kubernetes is offering you a way to eliminate this operational

1075
01:09:53,570 --> 01:09:57,506
toil and codify this operational excellence,

1076
01:09:57,538 --> 01:10:01,562
this operational knowledge that you have garnered over the years by operating

1077
01:10:01,626 --> 01:10:05,310
on this specific type of workloads, you're going to be benefited.

1078
01:10:06,130 --> 01:10:09,918
So that's all I had for these talk.

1079
01:10:10,084 --> 01:10:13,406
Thank you so much for joining and I hope you

1080
01:10:13,428 --> 01:10:17,406
enjoyed this talk. Again, thank you

1081
01:10:17,428 --> 01:10:19,340
so much, have a good day.

