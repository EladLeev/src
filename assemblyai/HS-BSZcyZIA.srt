1
00:00:17,450 --> 00:00:21,070
Welcome to this session about blazing fat serverless with

2
00:00:21,140 --> 00:00:24,950
rust. It's a pleasure to be here to tell you how you can love,

3
00:00:24,980 --> 00:00:28,758
leverage your rust knowledge to build super fast

4
00:00:28,844 --> 00:00:33,282
serverless functions and level up your computing

5
00:00:33,346 --> 00:00:36,582
to the next wave of technology. Hi,

6
00:00:36,636 --> 00:00:39,766
first of all, I am Luca Bianchi. I am chief

7
00:00:39,798 --> 00:00:43,718
technology officer at Neosperience and AWS Hero.

8
00:00:43,814 --> 00:00:47,274
I am passionate about serverless and machine learning.

9
00:00:47,392 --> 00:00:51,934
Here are my references. You can find me. Please catch up and

10
00:00:52,132 --> 00:00:55,120
tell me what you think about this.

11
00:00:56,130 --> 00:00:59,738
Talk about using rust with serverless,

12
00:00:59,914 --> 00:01:03,826
but let's begin introducing to

13
00:01:03,848 --> 00:01:07,746
you what is serverless. This is a quite common definition that you

14
00:01:07,768 --> 00:01:11,474
can find on books about serverless and

15
00:01:11,512 --> 00:01:15,874
it talks about ephemeral computing resources

16
00:01:16,002 --> 00:01:19,862
that are created, that are built when you need them

17
00:01:19,916 --> 00:01:23,746
and they are disposed right after your usage.

18
00:01:23,938 --> 00:01:28,138
It is quite true, it is quite

19
00:01:28,304 --> 00:01:31,786
precise definition, but to my understanding is not a

20
00:01:31,808 --> 00:01:35,100
practical definition. I would aim for something more

21
00:01:35,470 --> 00:01:39,082
related to the meaning of serverless. And to me,

22
00:01:39,136 --> 00:01:42,814
serverless means that you don't have servers to

23
00:01:42,852 --> 00:01:46,574
manage, of course, which is translating into the fact

24
00:01:46,612 --> 00:01:49,774
that if this one is the stack that

25
00:01:49,812 --> 00:01:54,174
you have to take account when you are building new applications,

26
00:01:54,302 --> 00:01:57,986
you can remove the bottom part of the stack. So you don't have to

27
00:01:58,008 --> 00:02:02,050
provision our world, you don't have to buy physical servers,

28
00:02:03,430 --> 00:02:07,410
you don't need an IT service team that installs

29
00:02:07,490 --> 00:02:11,462
and upgrades hardware. And of course it is still someone else

30
00:02:11,516 --> 00:02:14,694
service and you are paying someone else to manage the

31
00:02:14,732 --> 00:02:18,486
serverless on your behalf. But service means also

32
00:02:18,588 --> 00:02:22,130
that you can remove the concept of built on machines. You don't have built on

33
00:02:22,140 --> 00:02:25,418
machines to manage. You cannot under

34
00:02:25,504 --> 00:02:29,034
over provision the resources that you need, which means that

35
00:02:29,072 --> 00:02:32,334
you are using at every time the exact amount of

36
00:02:32,372 --> 00:02:36,094
resources that are needed for your workload. And it

37
00:02:36,132 --> 00:02:39,902
also means that you are not paying for idle and

38
00:02:40,036 --> 00:02:43,460
you don't have also to take account for any

39
00:02:43,830 --> 00:02:47,794
VM disaster recoveries. But it is more than that.

40
00:02:47,912 --> 00:02:51,790
Serverless means also that you don't have to patch any operative

41
00:02:51,870 --> 00:02:55,182
system, which is a great

42
00:02:55,256 --> 00:02:59,222
things because you don't have to manage security patches, you don't have

43
00:02:59,276 --> 00:03:03,442
to configure system for built in best practices,

44
00:03:03,586 --> 00:03:07,046
or provision automatically new operating systems when you

45
00:03:07,068 --> 00:03:10,986
need them. They are provided to you for free and it

46
00:03:11,008 --> 00:03:14,262
is a great things which brings

47
00:03:14,326 --> 00:03:18,646
your app into the so called cloud native

48
00:03:18,838 --> 00:03:22,382
way of computing. But serverless is more than

49
00:03:22,436 --> 00:03:27,098
that. Serverless means also that you don't have to manage schedulers,

50
00:03:27,194 --> 00:03:31,370
you don't have to manage containers, you don't have CTO define

51
00:03:31,450 --> 00:03:34,606
whether or not a container is fired and the

52
00:03:34,628 --> 00:03:38,354
logic upon a container unspawned, which means that

53
00:03:38,392 --> 00:03:42,146
your code is invoked by the platform and also the

54
00:03:42,168 --> 00:03:46,038
language support and all the upgrades, the language itself

55
00:03:46,204 --> 00:03:49,782
are packed within the runtime. And it means also

56
00:03:49,836 --> 00:03:53,766
that you can benefit from analytics out of the box.

57
00:03:53,948 --> 00:03:57,830
It is a fringe benefits that you can collect

58
00:03:58,490 --> 00:04:01,914
using serverless. You don't have to worry about that. So you

59
00:04:01,952 --> 00:04:05,654
remove all the parts of your stacks that are not related

60
00:04:05,702 --> 00:04:09,802
to your business logic, that are not related to the things that your

61
00:04:09,856 --> 00:04:14,218
customers is paying you for. And it also means

62
00:04:14,384 --> 00:04:18,106
that serverless is serviceful

63
00:04:18,218 --> 00:04:21,630
because you can focus directly on what is

64
00:04:21,700 --> 00:04:25,698
providing added value for your customers and

65
00:04:25,784 --> 00:04:29,874
someone else. In my case, Amazon Web Services or

66
00:04:29,912 --> 00:04:34,500
AWS, is managing all the stacks below

67
00:04:34,890 --> 00:04:38,360
the serverless line, and I'm just writing the code.

68
00:04:39,050 --> 00:04:42,726
But by serviceful we

69
00:04:42,748 --> 00:04:46,054
can also understand a bit more what

70
00:04:46,092 --> 00:04:49,290
is needed. What is meant with serviceful?

71
00:04:49,710 --> 00:04:52,774
It means that we can use managed

72
00:04:52,822 --> 00:04:56,250
services that offer storage, databases,

73
00:04:56,750 --> 00:05:00,446
queues and a lot of infrastructure services that

74
00:05:00,468 --> 00:05:03,598
are provided through an API. Also,

75
00:05:03,684 --> 00:05:07,642
AI services are provided managed

76
00:05:07,706 --> 00:05:11,582
as rest services that you can invoke through

77
00:05:11,636 --> 00:05:15,682
an API, which means that you move from

78
00:05:15,816 --> 00:05:18,450
an idea where you build your app,

79
00:05:18,600 --> 00:05:21,842
packaging all the libraries that you need within your

80
00:05:21,896 --> 00:05:25,650
executable to an app in which you have to coordinate

81
00:05:25,730 --> 00:05:29,682
a variety of services. But more than that, API endpoints

82
00:05:29,746 --> 00:05:33,158
are secured and managed using services.

83
00:05:33,324 --> 00:05:36,370
So you can write your code for your APIs.

84
00:05:36,530 --> 00:05:40,138
But my suggestion is CTO use some great services

85
00:05:40,224 --> 00:05:43,366
that you have at your disposal, such as Amazon API

86
00:05:43,398 --> 00:05:47,254
gateway for rest and web books, and AWS appsync

87
00:05:47,302 --> 00:05:51,150
for GraphQl that can secure API. They can

88
00:05:51,220 --> 00:05:54,430
guarantee that scaling up

89
00:05:54,500 --> 00:05:57,550
policies will be applied. They can also provide

90
00:05:57,620 --> 00:06:01,294
you web application firewalls when needed, and a lot

91
00:06:01,332 --> 00:06:04,900
of security monitors that can

92
00:06:05,350 --> 00:06:08,978
make the difference between the failure and the success of your service.

93
00:06:09,144 --> 00:06:13,026
So your code doesn't belong to

94
00:06:13,048 --> 00:06:17,250
the part in which you have to manage APIs.

95
00:06:17,330 --> 00:06:21,094
You don't have to manage APIs anymore, you have to configure a

96
00:06:21,132 --> 00:06:24,886
service that is providing that APIs for you and

97
00:06:24,908 --> 00:06:28,546
your business. Logic is handled through functions

98
00:06:28,578 --> 00:06:32,342
of service. Services such as AWS

99
00:06:32,406 --> 00:06:36,262
lambda and AWS lambda is able to receive

100
00:06:36,406 --> 00:06:40,226
your code and deploy your code for runtime.

101
00:06:40,358 --> 00:06:44,190
So your code is called lambda function

102
00:06:44,340 --> 00:06:48,810
because you are packaging your code as a function which is invoked

103
00:06:48,970 --> 00:06:52,994
calling a function handler. And from that point

104
00:06:53,112 --> 00:06:56,386
the lambda runtime handles all the

105
00:06:56,408 --> 00:06:59,860
execution of the code. And more than that,

106
00:07:00,470 --> 00:07:03,774
AWS lambda supports a variety of languages,

107
00:07:03,902 --> 00:07:06,774
from Java to node, typescript to c,

108
00:07:06,812 --> 00:07:11,378
sharp to Powershell, Python Golang and rust

109
00:07:11,554 --> 00:07:15,074
and rust. We will see that provides an unfair

110
00:07:15,122 --> 00:07:17,640
advantage amongst all of them.

111
00:07:18,270 --> 00:07:22,214
But let me introduce you all the lifecycle

112
00:07:22,262 --> 00:07:25,754
of a lambda function, because I've told you that when

113
00:07:25,792 --> 00:07:29,306
you are managing lambda function, you are packaging your code and

114
00:07:29,328 --> 00:07:33,390
you are providing your code to AWS, to the AWS lambda service.

115
00:07:33,540 --> 00:07:36,862
And after that AWS brings that

116
00:07:36,916 --> 00:07:40,606
code and upon request. So when someone

117
00:07:40,708 --> 00:07:44,086
is invoking your function, maybe it could be invoked

118
00:07:44,218 --> 00:07:47,198
through a queue, or through a stream,

119
00:07:47,294 --> 00:07:50,882
or in response to a file uploaded to an s three

120
00:07:50,936 --> 00:07:54,366
bucket, or the most common way of invoking your lambda

121
00:07:54,398 --> 00:07:57,826
function is through an API call, through Amazon API

122
00:07:57,858 --> 00:08:02,450
gateway. And whenever your lambda function is invoked, the lambda

123
00:08:02,610 --> 00:08:06,194
service brings up a micro

124
00:08:06,242 --> 00:08:10,070
virtual machine, which is something similar to a container

125
00:08:10,150 --> 00:08:13,866
that is managed for you and package your code.

126
00:08:13,968 --> 00:08:17,814
Initialize the container, initialize the runtime, and initialize

127
00:08:17,862 --> 00:08:21,486
your function with your code and passes the context. And the

128
00:08:21,508 --> 00:08:25,486
event which collects all

129
00:08:25,508 --> 00:08:29,198
the data, contains all the data of the invocation to your code.

130
00:08:29,284 --> 00:08:33,034
You can process your code, you can manage

131
00:08:33,092 --> 00:08:36,402
your code, you can do whatever you want, and then you have

132
00:08:36,456 --> 00:08:40,254
to send back a response to that event, or otherwise

133
00:08:40,302 --> 00:08:44,954
you can just complete with no additional response.

134
00:08:45,022 --> 00:08:48,674
But one important part is that whenever the container,

135
00:08:48,722 --> 00:08:51,560
whenever the microbial machine is built,

136
00:08:52,250 --> 00:08:55,846
you can experience the so called cold start because you

137
00:08:55,868 --> 00:08:59,414
have some operations, you have operations

138
00:08:59,462 --> 00:09:03,402
that are done, but they

139
00:09:03,456 --> 00:09:07,370
require time, and initializing the extensions within

140
00:09:07,440 --> 00:09:10,890
your lambda runtime, or initializing the runtime,

141
00:09:10,970 --> 00:09:15,358
or initializing also the interpreter or the

142
00:09:15,364 --> 00:09:19,374
executable of your code is something that could require fun.

143
00:09:19,412 --> 00:09:22,586
It could take time. And if we

144
00:09:22,628 --> 00:09:26,178
can consider the execution time of

145
00:09:26,184 --> 00:09:29,470
a lambda function, the first part of the execution

146
00:09:29,550 --> 00:09:33,138
time is spent on the call start and

147
00:09:33,144 --> 00:09:36,438
the second part is spent on your code.

148
00:09:36,604 --> 00:09:41,042
But the problem is that whenever you are firing

149
00:09:41,106 --> 00:09:45,014
a new lambda function, which occurs basically in two

150
00:09:45,212 --> 00:09:48,506
ways. The first way is that you are calling a function that

151
00:09:48,528 --> 00:09:51,898
has not been called for a while, say for

152
00:09:51,984 --> 00:09:55,850
almost 30 minutes. So it is something that needs

153
00:09:55,920 --> 00:09:59,398
to be rebuilt from scratch, so you cannot leverage

154
00:09:59,494 --> 00:10:03,754
on the service, keeping the container warm.

155
00:10:03,882 --> 00:10:08,110
And the second case, which is much more frequent

156
00:10:08,450 --> 00:10:11,934
and worrisome, is whenever you are scaling up your

157
00:10:11,972 --> 00:10:15,634
service. So if you are scaling up your service, a lot

158
00:10:15,672 --> 00:10:20,286
of containers will be forced to instantiate

159
00:10:20,398 --> 00:10:24,110
many different civil,

160
00:10:24,190 --> 00:10:27,078
many different identical unit of code.

161
00:10:27,164 --> 00:10:31,094
So say you are on the Black Friday, a lot of people are

162
00:10:31,212 --> 00:10:34,040
buying on your ecommerce website,

163
00:10:34,410 --> 00:10:38,422
and there are a lot of requests arriving to your endpoint for

164
00:10:38,476 --> 00:10:43,142
each one of them. The Lambda service forks a new computational

165
00:10:43,206 --> 00:10:46,762
unit. It forks a new runtime, which means that for

166
00:10:46,816 --> 00:10:49,782
every one of them you are experiencing the cold start.

167
00:10:49,936 --> 00:10:54,222
And it could be an issue because the cold start could

168
00:10:54,356 --> 00:10:56,800
slow down your function,

169
00:10:59,010 --> 00:11:02,250
in some cases even for seconds,

170
00:11:02,410 --> 00:11:06,194
and it is something that is not acceptable. But the good

171
00:11:06,232 --> 00:11:09,506
news is that it is strictly dependent on the kind

172
00:11:09,528 --> 00:11:12,866
of language that you choose. And this is a

173
00:11:12,888 --> 00:11:16,838
chart in which I want you to focus on different

174
00:11:16,924 --> 00:11:20,514
lambda configurations. You cannot choose the number of cpus

175
00:11:20,562 --> 00:11:24,374
for your lambdas, but you can choose the amount of

176
00:11:24,412 --> 00:11:28,586
memory which is presented to the runtime execution and

177
00:11:28,688 --> 00:11:34,422
that amount of memory also implies consistent

178
00:11:34,486 --> 00:11:38,170
number of cpu build cpus and for different

179
00:11:38,240 --> 00:11:43,680
configuration ranging from 128 CtO

180
00:11:44,130 --> 00:11:47,502
one gigabytes, you can see

181
00:11:47,556 --> 00:11:52,406
that there is huge difference between different runtime.

182
00:11:52,538 --> 00:11:56,660
So if you choose to write your code using

183
00:11:57,190 --> 00:12:00,926
Java, you will be much slower

184
00:12:01,038 --> 00:12:04,414
than if you use Ruby or node

185
00:12:04,462 --> 00:12:07,974
JS or python. But the great news is

186
00:12:08,012 --> 00:12:11,826
that if you choose to use rust

187
00:12:12,018 --> 00:12:14,790
at every step, rust,

188
00:12:15,130 --> 00:12:19,094
it is much faster than ADR language,

189
00:12:19,142 --> 00:12:23,286
which means reduced call start, which means faster execution

190
00:12:23,398 --> 00:12:27,814
has a lot of benefits. This could be greater

191
00:12:27,862 --> 00:12:31,302
response to the question why rust for lambda?

192
00:12:31,446 --> 00:12:35,178
Rust is designed to be a safe and like performing language

193
00:12:35,274 --> 00:12:39,354
and lambda can leverage all these capabilities.

194
00:12:39,482 --> 00:12:43,950
But more than that, rust is also a statically typed language,

195
00:12:44,030 --> 00:12:47,294
which is super important because you are developing your machine,

196
00:12:47,342 --> 00:12:50,802
then you are deploying the cloud and you are doing

197
00:12:50,856 --> 00:12:54,014
back and forth from your machine

198
00:12:54,062 --> 00:12:58,040
to the cloud whenever you are developing your application

199
00:12:58,410 --> 00:13:02,360
and being able to use a language that can

200
00:13:02,810 --> 00:13:06,950
use that can leverage good

201
00:13:07,020 --> 00:13:10,546
compiler that prevents error and brings

202
00:13:10,578 --> 00:13:13,930
error at compiles time and

203
00:13:14,000 --> 00:13:17,930
also enforces statically typed language, it is

204
00:13:18,000 --> 00:13:21,470
something that could dramatically speed up

205
00:13:21,540 --> 00:13:25,966
your development cycle. And not

206
00:13:26,068 --> 00:13:29,950
to forget that Rust also has a great

207
00:13:30,020 --> 00:13:34,706
tooling system and great community that

208
00:13:34,728 --> 00:13:38,610
is providing a lot of libraries that can be used within

209
00:13:38,680 --> 00:13:42,674
your lambda. But the first question that

210
00:13:42,712 --> 00:13:46,134
you could ask yourself could be how to start with Rust with

211
00:13:46,172 --> 00:13:50,690
lambda. And it is super easy because AWS released

212
00:13:50,850 --> 00:13:53,960
a template for the serverless application model,

213
00:13:55,370 --> 00:13:58,540
SAm for short, and SaM is

214
00:13:59,310 --> 00:14:03,274
CLI tool that can

215
00:14:03,472 --> 00:14:06,970
help you to get started configuring your first

216
00:14:07,040 --> 00:14:10,894
project and just bright typing SaM in

217
00:14:10,932 --> 00:14:15,102
it. You can choose within a

218
00:14:15,156 --> 00:14:18,638
number of templates, available templates, and you can

219
00:14:18,724 --> 00:14:21,950
also select the runtime,

220
00:14:22,390 --> 00:14:26,162
the kind of cpu that you want to run your code on

221
00:14:26,216 --> 00:14:30,174
because lambda supports either intel

222
00:14:30,222 --> 00:14:34,222
cpus, XH 86 and ARm

223
00:14:34,286 --> 00:14:37,990
64. And the great things is that

224
00:14:38,140 --> 00:14:41,346
SAM is going to configure all the projects

225
00:14:41,378 --> 00:14:45,202
for you. It is going to write the SaM template

226
00:14:45,266 --> 00:14:48,986
file, which is the configuration file that

227
00:14:49,008 --> 00:14:52,826
is managed by SAM command line to

228
00:14:53,008 --> 00:14:56,870
translate the code to fire cloud formation to configure

229
00:14:56,950 --> 00:14:59,258
your infrastructure AWS code.

230
00:14:59,424 --> 00:15:03,038
And it also scaffold the project for you.

231
00:15:03,124 --> 00:15:07,150
And it is great things because in this project scaffolding

232
00:15:07,810 --> 00:15:11,626
you can find the skeleton of basic

233
00:15:11,818 --> 00:15:15,380
rust service that can be used

234
00:15:15,750 --> 00:15:18,594
to implement a use case. In this example,

235
00:15:18,712 --> 00:15:22,366
we are writing data into DynamoDB,

236
00:15:22,398 --> 00:15:25,558
which DynamoDb is a key value store provided as a

237
00:15:25,564 --> 00:15:29,430
service by AWS. It is a super fast storage

238
00:15:29,930 --> 00:15:33,574
and we are collecting data coming from an

239
00:15:33,612 --> 00:15:36,966
endpoint, a rest endpoint and then we are pushing that

240
00:15:36,988 --> 00:15:40,490
data into dynamodb. And the first

241
00:15:40,560 --> 00:15:44,842
part we are importing the libraries and

242
00:15:44,896 --> 00:15:49,610
we are using lambda HTTP crates,

243
00:15:49,970 --> 00:15:53,806
which is great because it provided support for

244
00:15:53,988 --> 00:15:58,414
request handler and also for lambda runtime object.

245
00:15:58,612 --> 00:16:02,662
And then we are using also AWS

246
00:16:02,746 --> 00:16:06,354
SDK which offers support for

247
00:16:06,392 --> 00:16:09,774
a number of structures

248
00:16:09,822 --> 00:16:13,762
that can be used to abstract away the complexity of

249
00:16:13,816 --> 00:16:18,246
writing data into dynamodb. Then we

250
00:16:18,268 --> 00:16:22,530
can use Tokyo to make the main function asynchronous.

251
00:16:22,610 --> 00:16:26,006
The main function will be invoked by the

252
00:16:26,028 --> 00:16:29,274
lambda runtime whenever your function is

253
00:16:29,312 --> 00:16:33,850
called, and it also supports

254
00:16:34,270 --> 00:16:37,846
lambda through an idiomatic way of handling errors

255
00:16:37,878 --> 00:16:41,834
in rust which is returning a result

256
00:16:41,952 --> 00:16:44,522
object optionals.

257
00:16:44,666 --> 00:16:48,190
Then we can configure

258
00:16:48,770 --> 00:16:53,142
and extract we can configure clients and extract

259
00:16:53,306 --> 00:16:56,546
environment variables that are provided as a

260
00:16:56,568 --> 00:17:00,450
context for our lambda. This part is super important because

261
00:17:00,520 --> 00:17:03,410
all the variables that are defined in this section,

262
00:17:03,910 --> 00:17:07,618
they are persistent from invocation to invocation

263
00:17:07,714 --> 00:17:12,550
and they are just dismissed. When lander runtime destroys

264
00:17:12,890 --> 00:17:16,386
your context, destroys your exe

265
00:17:16,418 --> 00:17:20,234
computational unit. Then we

266
00:17:20,272 --> 00:17:24,282
can initialize the lambda runtime using that

267
00:17:24,336 --> 00:17:28,102
closure and passing a reference

268
00:17:28,166 --> 00:17:31,518
to the handler that should be invoked and

269
00:17:31,684 --> 00:17:35,310
also all the context variable that we want

270
00:17:35,380 --> 00:17:39,582
to set for

271
00:17:39,636 --> 00:17:43,098
our system that we want to use. In our case

272
00:17:43,124 --> 00:17:46,942
we are processing a reference to the dynamoDB client and the table

273
00:17:47,006 --> 00:17:50,366
name, and also the request containing

274
00:17:50,398 --> 00:17:53,874
all the parameters of the record that we need to

275
00:17:54,072 --> 00:17:57,698
insert into the database and the context params.

276
00:17:57,794 --> 00:18:01,186
But in this part, in these lines

277
00:18:01,298 --> 00:18:04,902
you can also decide whenever or not you want

278
00:18:04,956 --> 00:18:08,934
to parallelize the computation. And here you

279
00:18:08,972 --> 00:18:12,930
can run multiple threads in parallel

280
00:18:13,090 --> 00:18:17,450
using graph features. Then the invocation is blocked

281
00:18:17,790 --> 00:18:21,946
until the results came back and error

282
00:18:21,978 --> 00:18:26,526
propagation is handled. And finally we

283
00:18:26,548 --> 00:18:30,094
are going to tell the caller that the

284
00:18:30,132 --> 00:18:34,334
things were successful. Then the

285
00:18:34,372 --> 00:18:37,922
handler itself, which is the main function, which is the function

286
00:18:37,976 --> 00:18:40,770
that is called from our runtime,

287
00:18:41,510 --> 00:18:44,814
extracts data from requests from the request,

288
00:18:44,862 --> 00:18:48,194
then parses the data from the HTTP request

289
00:18:48,322 --> 00:18:52,390
and it builds using the builder

290
00:18:52,730 --> 00:18:55,926
pattern. It builds dynamodb client and

291
00:18:55,948 --> 00:18:59,900
then invoke the putitem function

292
00:19:00,670 --> 00:19:04,250
to create a new item into

293
00:19:04,320 --> 00:19:08,426
dynamodb. Then finally we

294
00:19:08,448 --> 00:19:12,986
are matching the result the response

295
00:19:13,178 --> 00:19:20,158
in order to handle errors or correct

296
00:19:20,244 --> 00:19:23,662
response and then we are sending back using the builder

297
00:19:23,726 --> 00:19:28,030
we are sending back proper configured

298
00:19:28,110 --> 00:19:32,354
header that can be used by the API gateway to

299
00:19:32,392 --> 00:19:36,470
synthesize the HTTP 200 okay

300
00:19:36,540 --> 00:19:39,110
response or 500 error.

301
00:19:39,770 --> 00:19:44,294
Then going to the comparison the

302
00:19:44,332 --> 00:19:48,474
best part is that if we consider in

303
00:19:48,512 --> 00:19:52,314
this small sample we compare a

304
00:19:52,352 --> 00:19:56,202
node which is the most common choice when you are

305
00:19:56,256 --> 00:20:00,334
using lambda and rust execution times. We can

306
00:20:00,372 --> 00:20:04,142
see that in the worst case scenario rust is

307
00:20:04,276 --> 00:20:07,902
two or twice faster than node. But in the best

308
00:20:07,956 --> 00:20:13,682
case scenario rust is more than 1000%

309
00:20:13,816 --> 00:20:17,474
faster than node so it is four

310
00:20:17,512 --> 00:20:20,722
milliseconds compared to 60 milliseconds and

311
00:20:20,776 --> 00:20:24,366
in some cases the call start within node

312
00:20:24,558 --> 00:20:27,938
can range up to almost

313
00:20:28,024 --> 00:20:31,786
1 second and within withdraw

314
00:20:31,918 --> 00:20:35,480
you will be much way faster than that.

315
00:20:35,850 --> 00:20:39,670
And this is just a glimpse of

316
00:20:39,820 --> 00:20:43,314
the performance, the right level of performance that you can achieve

317
00:20:43,442 --> 00:20:47,720
using rust with lambda and how to get started using

318
00:20:48,090 --> 00:20:51,694
serverless application framework or same. So thank you very

319
00:20:51,732 --> 00:20:53,850
much for listening and have a great conference.

