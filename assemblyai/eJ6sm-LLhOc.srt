1
00:00:23,930 --> 00:00:27,922
Hello everyone. Mario Garcia I'm so happy to be here sharing

2
00:00:27,986 --> 00:00:31,590
with the global Python community at Conf 42

3
00:00:31,660 --> 00:00:34,902
Python 2023. Today's talk will be about

4
00:00:34,956 --> 00:00:38,518
how to generate this data for your database project.

5
00:00:38,684 --> 00:00:42,262
I'm a technical evangelist at Percona. You can find me

6
00:00:42,316 --> 00:00:45,960
on Twitter LinkedIn or you can contact me by email.

7
00:00:46,330 --> 00:00:49,382
And this is the agenda that I will follow today.

8
00:00:49,516 --> 00:00:53,194
If you need to generate data for testing

9
00:00:53,242 --> 00:00:56,570
your application or validating the schema

10
00:00:56,650 --> 00:01:00,190
of your database, you have two options.

11
00:01:00,340 --> 00:01:04,130
You can get a data set from sites like Kaggle

12
00:01:04,470 --> 00:01:08,990
and then use pandas for processing that data and insert

13
00:01:09,150 --> 00:01:13,138
that data into the database. Or you can use a

14
00:01:13,304 --> 00:01:16,920
data generator. We have one that is available

15
00:01:17,770 --> 00:01:21,270
in one of our repositories, but it only works for

16
00:01:21,340 --> 00:01:25,320
MySQL. If you need a data generator for

17
00:01:25,690 --> 00:01:29,434
postgres or MongoDB, I will show you how you can use

18
00:01:29,472 --> 00:01:32,810
Python for creating your own data generator.

19
00:01:33,230 --> 00:01:36,522
First we need to install the

20
00:01:36,576 --> 00:01:40,022
dependencies for our project. Create the

21
00:01:40,176 --> 00:01:43,434
database where we will insert

22
00:01:43,562 --> 00:01:47,402
the data that we will generate. I will explain the directory

23
00:01:47,466 --> 00:01:51,722
structure. I will show you how you can use Faker

24
00:01:51,786 --> 00:01:55,906
that is a library for generating data and also how

25
00:01:55,928 --> 00:01:59,326
to create a Pandas data frame. We need a data frame

26
00:01:59,358 --> 00:02:02,642
for storing the data that we will generate and then

27
00:02:02,696 --> 00:02:06,098
how to establish connection to the database.

28
00:02:06,194 --> 00:02:09,542
How to define the schema of the database we have two

29
00:02:09,596 --> 00:02:13,286
options here. We would use SQL Alchemy for establishing the

30
00:02:13,308 --> 00:02:17,590
connection to our MySQL or postgres database.

31
00:02:18,190 --> 00:02:22,186
And SQL Alchemy has a

32
00:02:22,208 --> 00:02:25,786
schema definition language that we can use for

33
00:02:25,968 --> 00:02:30,090
defining the schema. But as we are using pandas,

34
00:02:30,770 --> 00:02:34,960
Pandas has a built in method for inserting data into

35
00:02:36,210 --> 00:02:39,434
SQL databases like MySQL

36
00:02:39,482 --> 00:02:43,562
or postgres, and we will use that building

37
00:02:43,636 --> 00:02:45,300
method for that.

38
00:02:46,550 --> 00:02:49,790
And also we will see how to use multiprocessing

39
00:02:49,870 --> 00:02:53,426
for improving the runtime of our application,

40
00:02:53,608 --> 00:02:57,910
and then we will see how we generate data

41
00:02:58,060 --> 00:03:02,040
by running the scripts that we will create.

42
00:03:04,090 --> 00:03:07,694
First we need to create requirements txt

43
00:03:07,842 --> 00:03:10,986
that will contain a list of the

44
00:03:11,008 --> 00:03:14,854
dependencies of our project. Or if you're using Anaconda,

45
00:03:14,902 --> 00:03:18,394
we need to create an environment YamL file that will

46
00:03:18,432 --> 00:03:22,346
contain the name of our environment. The version

47
00:03:22,378 --> 00:03:26,074
of Python that we are using. The libraries

48
00:03:26,122 --> 00:03:29,594
that we are using for this project, including pandas,

49
00:03:29,642 --> 00:03:33,774
Sqlalchemy, TKDM that is a library for adding a progress

50
00:03:33,822 --> 00:03:37,118
bar, faker that is the library

51
00:03:37,294 --> 00:03:40,260
that we use for generating data.

52
00:03:42,950 --> 00:03:46,774
And also we need the drivers for

53
00:03:46,892 --> 00:03:50,790
the database that we are generating data for,

54
00:03:50,940 --> 00:03:55,030
including MySQL, postgres and MongoDB.

55
00:03:55,370 --> 00:03:58,602
After we add that information into our

56
00:03:58,736 --> 00:04:02,838
requirements txt file or youre environment

57
00:04:02,934 --> 00:04:06,662
YamL file, we need to install our dependencies

58
00:04:06,806 --> 00:04:10,750
by running PIP, install or

59
00:04:10,820 --> 00:04:15,066
conda and create depending on the way that we are configuring

60
00:04:15,178 --> 00:04:17,950
our development environment.

61
00:04:21,330 --> 00:04:24,782
Then after we install our dependencies,

62
00:04:24,846 --> 00:04:27,250
we need to create the database.

63
00:04:27,750 --> 00:04:32,434
This is an example. You can adjust this

64
00:04:32,472 --> 00:04:35,922
information according to the requirements

65
00:04:35,986 --> 00:04:40,150
of your project. But first we create a database

66
00:04:40,890 --> 00:04:44,882
for MySQL or postgres for MongoDB.

67
00:04:44,946 --> 00:04:48,582
We don't need to do that. The database

68
00:04:48,646 --> 00:04:52,506
and the collection where the information

69
00:04:52,608 --> 00:04:56,458
will be stored is created while

70
00:04:56,544 --> 00:04:57,850
running the script.

71
00:05:00,370 --> 00:05:04,062
Well, this is the directory structure of our project.

72
00:05:04,196 --> 00:05:08,910
We have three modules, one for establishing

73
00:05:09,490 --> 00:05:13,038
information about how to establish the connection to the database,

74
00:05:13,134 --> 00:05:16,802
including information for MySQL, Postgres and

75
00:05:16,856 --> 00:05:21,220
MongoDB. And here

76
00:05:22,310 --> 00:05:26,162
this model will contain instructions for generating

77
00:05:26,226 --> 00:05:29,714
the data and storing that data into a pandas

78
00:05:29,762 --> 00:05:33,782
data frame that will later be processed for

79
00:05:33,836 --> 00:05:36,950
inserting that information into our database.

80
00:05:37,950 --> 00:05:42,390
And here in the schema py

81
00:05:42,550 --> 00:05:47,050
file we specify

82
00:05:49,390 --> 00:05:52,110
the schema of our database.

83
00:05:52,450 --> 00:05:56,510
We will see that later, but we are not using SQL Alchemy

84
00:05:57,090 --> 00:06:01,934
schema definition language. Here we

85
00:06:01,972 --> 00:06:05,726
create an environment pile

86
00:06:05,838 --> 00:06:10,370
file, a requirements TxT file for our dependencies

87
00:06:12,390 --> 00:06:16,226
as the way for establishing the connection

88
00:06:16,258 --> 00:06:20,242
to the database and inserting the data into our database

89
00:06:20,306 --> 00:06:23,506
is slightly similar for MySQL and Mongo.

90
00:06:23,538 --> 00:06:27,250
We create a script for both database technologies

91
00:06:27,330 --> 00:06:30,970
and we create a separate script for MongoDB.

92
00:06:33,470 --> 00:06:37,002
Well, how we use Faker Faker has some built

93
00:06:37,056 --> 00:06:40,206
in providers and properties that

94
00:06:40,228 --> 00:06:43,454
we use for generating data. Name is

95
00:06:43,492 --> 00:06:47,054
one of those properties. So first we need

96
00:06:47,092 --> 00:06:50,622
to initialize we need to import

97
00:06:50,756 --> 00:06:55,170
faker and initialize

98
00:06:56,390 --> 00:06:59,618
faker. And then by calling

99
00:06:59,704 --> 00:07:04,980
one of the properties we generate ten

100
00:07:05,530 --> 00:07:08,710
randomly generated names.

101
00:07:10,970 --> 00:07:14,582
This list will change every time you run this

102
00:07:14,636 --> 00:07:19,158
code as this data is generated

103
00:07:19,334 --> 00:07:21,370
by using a random method.

104
00:07:24,910 --> 00:07:29,882
What providers and properties are

105
00:07:29,936 --> 00:07:34,486
available with Faker?

106
00:07:34,598 --> 00:07:38,894
We have some built in providers but

107
00:07:38,932 --> 00:07:42,446
also we have some providers created by

108
00:07:42,468 --> 00:07:46,206
the community. The ones listed here are the

109
00:07:46,228 --> 00:07:49,700
built in ones. The ones created by the

110
00:07:50,710 --> 00:07:54,370
developers behind Faker

111
00:07:55,130 --> 00:07:58,694
well, these are some of the providers that

112
00:07:58,732 --> 00:08:02,262
are available, including person that we can use for

113
00:08:02,316 --> 00:08:06,342
generating name. This will generate a pair of

114
00:08:06,476 --> 00:08:10,374
first name and last name or we can generate

115
00:08:10,422 --> 00:08:14,042
those data separated by calling the first name and

116
00:08:14,096 --> 00:08:17,610
last name methods or properties.

117
00:08:18,830 --> 00:08:22,186
And we have an address providers

118
00:08:22,378 --> 00:08:25,840
that we can use for generating address city or country.

119
00:08:27,010 --> 00:08:30,970
We have a job provider that can be uses for generating

120
00:08:31,050 --> 00:08:34,786
job company an Internet provider that we

121
00:08:34,808 --> 00:08:39,780
uses for generating email or company email well,

122
00:08:40,470 --> 00:08:44,322
now that we know how to use faker for generating data,

123
00:08:44,456 --> 00:08:48,278
it's time to create our script. First we need to

124
00:08:48,444 --> 00:08:52,086
import some libraries and method that

125
00:08:52,108 --> 00:08:55,560
we would use for the whole process.

126
00:08:56,010 --> 00:08:59,580
First we import this method from the multi processing model.

127
00:09:00,510 --> 00:09:03,900
Cpu can will return

128
00:09:04,270 --> 00:09:07,180
the number of cpu cores available.

129
00:09:07,550 --> 00:09:11,950
We need this information as we are implementing multi processing

130
00:09:12,290 --> 00:09:15,358
for using all the cpu cores available.

131
00:09:15,524 --> 00:09:19,310
We will see that later and then we import

132
00:09:19,380 --> 00:09:22,234
pandas as well as TQDM and Faker.

133
00:09:22,282 --> 00:09:26,210
TQDM is a library for adding a

134
00:09:26,280 --> 00:09:27,300
progress bar.

135
00:09:29,830 --> 00:09:33,810
We create and initialize a faker generator.

136
00:09:34,230 --> 00:09:37,762
We get the number of cpu cores available minus

137
00:09:37,826 --> 00:09:41,062
one. This way we avoid that

138
00:09:41,116 --> 00:09:45,298
the computer freezes when using all the cpu cores

139
00:09:45,394 --> 00:09:49,100
available. So instead, for example, instead of using

140
00:09:50,670 --> 00:09:54,234
16 cores on

141
00:09:54,272 --> 00:09:57,562
a cpu that has this

142
00:09:57,616 --> 00:10:03,102
number of cores available, we use 15 of them and

143
00:10:03,156 --> 00:10:06,910
we leave one free to avoid that the computer freezes.

144
00:10:09,010 --> 00:10:12,522
Okay, and these are the columns that we will add

145
00:10:12,676 --> 00:10:16,626
to this table. And we will generate that data by calling

146
00:10:16,808 --> 00:10:20,210
some of the properties available in the faker library.

147
00:10:20,710 --> 00:10:23,714
So we will generate first name, last name,

148
00:10:23,752 --> 00:10:27,526
job, company and the

149
00:10:27,548 --> 00:10:31,400
other columns listed there.

150
00:10:32,730 --> 00:10:36,550
So how we generate that data and then

151
00:10:36,700 --> 00:10:41,030
insert or store that data into pandas data frame.

152
00:10:41,190 --> 00:10:45,238
So first as we are implementing multiprocessing,

153
00:10:45,414 --> 00:10:50,206
the idea with multi processing is that we

154
00:10:50,228 --> 00:10:53,902
divide the whole process into small

155
00:10:53,956 --> 00:10:57,978
pieces. So instead of for example, if we are generating

156
00:10:58,074 --> 00:11:01,694
60,000 records, we divide that number of

157
00:11:01,732 --> 00:11:04,914
records into every cpu youre available

158
00:11:05,112 --> 00:11:08,802
and every cpu cord will

159
00:11:08,856 --> 00:11:12,066
generate part of that information. And then we

160
00:11:12,088 --> 00:11:15,560
will concatenate that data

161
00:11:15,930 --> 00:11:19,926
for creating a

162
00:11:20,028 --> 00:11:22,870
single one pandas data frame.

163
00:11:24,090 --> 00:11:27,758
So that's why we are dividing

164
00:11:27,874 --> 00:11:31,834
the number of records that we will generate into the number

165
00:11:32,032 --> 00:11:35,994
of cpu cores minus one. Then we

166
00:11:36,032 --> 00:11:40,380
create an empty data frame and

167
00:11:41,250 --> 00:11:44,446
we will create all the columns in

168
00:11:44,468 --> 00:11:48,590
the data frame and assign as values

169
00:11:49,650 --> 00:11:53,154
the values return by calling some

170
00:11:53,192 --> 00:11:56,130
of the properties provided by Faker.

171
00:12:00,270 --> 00:12:04,810
Okay, now that we have

172
00:12:04,880 --> 00:12:08,506
generated data, it's time to establish a connection

173
00:12:08,538 --> 00:12:12,286
to the database. This is part of

174
00:12:12,468 --> 00:12:16,382
the base PY model that we created before that

175
00:12:16,436 --> 00:12:19,986
I explained as part of the

176
00:12:20,008 --> 00:12:23,554
directory structure. So what we do here

177
00:12:23,592 --> 00:12:27,934
is first we import the create engine and session maker

178
00:12:27,982 --> 00:12:32,098
methods, the first one for establishing

179
00:12:32,194 --> 00:12:35,874
the connection to the database. And then we create a session

180
00:12:35,922 --> 00:12:40,066
that is bind to the connection

181
00:12:40,098 --> 00:12:44,114
that we're establishing to our

182
00:12:44,172 --> 00:12:48,662
database. So for MysQL

183
00:12:48,806 --> 00:12:52,730
we use this driver. So we need to specify

184
00:12:53,150 --> 00:12:57,040
the database technologies that we are using

185
00:12:57,970 --> 00:13:01,262
plus the driver that we will use for

186
00:13:01,316 --> 00:13:04,734
establishing the connection to the database as well as

187
00:13:04,772 --> 00:13:09,362
running all the queries that we need okay,

188
00:13:09,416 --> 00:13:11,970
then we specify user password,

189
00:13:13,990 --> 00:13:17,746
the URL for the host that can be localhost or the IP address

190
00:13:17,848 --> 00:13:21,330
or a URL if you are hosting

191
00:13:22,490 --> 00:13:24,520
our database in the cloud.

192
00:13:25,690 --> 00:13:29,426
And then we create a session and that is bind

193
00:13:29,458 --> 00:13:32,230
to our connection to the database.

194
00:13:34,430 --> 00:13:35,850
For postgres,

195
00:13:38,510 --> 00:13:42,780
the way to establish a connection is

196
00:13:43,150 --> 00:13:43,900
similar.

197
00:13:47,410 --> 00:13:51,454
What changes here is that we

198
00:13:51,492 --> 00:13:54,778
are using postgres and we are using a different driver.

199
00:13:54,874 --> 00:13:57,310
We need to specify user password,

200
00:13:58,530 --> 00:14:02,226
the port that

201
00:14:02,248 --> 00:14:06,366
is assigned to this server and the name of the database.

202
00:14:06,478 --> 00:14:09,874
Then we create a session and for

203
00:14:09,912 --> 00:14:12,370
MongoDB we use Pymongo.

204
00:14:14,810 --> 00:14:18,386
We establish a connection to the database by calling the Mongo

205
00:14:18,498 --> 00:14:22,262
client method. We need to

206
00:14:22,316 --> 00:14:25,818
specify the user password, the port

207
00:14:25,904 --> 00:14:29,466
and the URL of our

208
00:14:29,648 --> 00:14:31,290
MongoDB server.

209
00:14:34,430 --> 00:14:38,110
Okay, so there are two ways for

210
00:14:38,180 --> 00:14:41,786
defining the schema of youre database. We can use the schema

211
00:14:41,818 --> 00:14:46,030
definition language provided by SQL alchemy.

212
00:14:46,450 --> 00:14:50,830
But remember that we can use SQL alchemy

213
00:14:50,990 --> 00:14:54,430
for MySQL, postgres and SQLite,

214
00:14:54,590 --> 00:14:58,238
but we can use SQL alchemy for MongoDB.

215
00:14:58,334 --> 00:15:01,522
So this time

216
00:15:01,576 --> 00:15:04,834
we are using a built in method provided

217
00:15:04,882 --> 00:15:08,486
by pandas for inserting that

218
00:15:08,588 --> 00:15:12,630
data frame that we previously created into the database.

219
00:15:13,290 --> 00:15:17,878
And the way that we define

220
00:15:17,974 --> 00:15:21,386
our schema is different from one,

221
00:15:21,568 --> 00:15:25,606
from the way that we define the schema

222
00:15:25,798 --> 00:15:29,726
when we use the schema definition language. So we

223
00:15:29,748 --> 00:15:33,134
only need to specify the name

224
00:15:33,172 --> 00:15:36,880
of the columns, the value

225
00:15:37,250 --> 00:15:40,450
of the value types,

226
00:15:41,510 --> 00:15:45,780
and then well after

227
00:15:46,790 --> 00:15:50,420
we define the schema of a database we need to

228
00:15:50,790 --> 00:15:55,110
understand why we are using multiprocessing.

229
00:15:55,770 --> 00:15:59,554
With multi processing, we take advantage of all the cpu core

230
00:15:59,602 --> 00:16:03,366
available. So when we

231
00:16:03,388 --> 00:16:07,834
are running this script, this is how

232
00:16:07,952 --> 00:16:12,966
the cpu uses appears.

233
00:16:12,998 --> 00:16:16,810
For example in our monitoring tool,

234
00:16:16,880 --> 00:16:21,534
percona monitoring and management. This is how we see all

235
00:16:21,652 --> 00:16:26,174
the cpu running part

236
00:16:26,212 --> 00:16:30,270
of the process at the same time. So that's

237
00:16:30,770 --> 00:16:33,510
why we are implementing multi processing.

238
00:16:33,690 --> 00:16:37,826
Instead of having just one cpu core running all

239
00:16:37,848 --> 00:16:41,778
the script and generating all the records

240
00:16:41,794 --> 00:16:45,510
that we need. For example, we are generating 60,000

241
00:16:45,580 --> 00:16:49,446
records. So by default Python will only uses

242
00:16:49,548 --> 00:16:53,222
one cpu core. So with

243
00:16:53,276 --> 00:16:56,714
multi processing we divide all

244
00:16:56,752 --> 00:17:00,154
the process into small pieces and every

245
00:17:00,192 --> 00:17:03,542
cpu core run part of the script.

246
00:17:03,686 --> 00:17:07,434
And for example here we are using 15 cpu

247
00:17:07,482 --> 00:17:12,558
cores and every cpu core is generating 4000

248
00:17:12,644 --> 00:17:16,474
records. After that we concatenate

249
00:17:16,522 --> 00:17:20,482
all that data into a single one data

250
00:17:20,536 --> 00:17:21,250
frame.

251
00:17:23,270 --> 00:17:27,346
Okay, after learning

252
00:17:27,528 --> 00:17:31,454
a little about multiprocessing, we need to import all the

253
00:17:31,592 --> 00:17:34,920
methods and libraries for

254
00:17:35,370 --> 00:17:38,950
the rest of our script.

255
00:17:40,330 --> 00:17:43,880
We are going to create a multiprocessing pool for

256
00:17:44,510 --> 00:17:48,474
running for

257
00:17:48,592 --> 00:17:52,074
every cpu cord to run part

258
00:17:52,112 --> 00:17:55,306
of the process of generating all

259
00:17:55,328 --> 00:17:58,640
the data, we need to get

260
00:17:59,730 --> 00:18:03,994
the number of CPU cores available by calling the cpu count method.

261
00:18:04,122 --> 00:18:08,480
We need to import pandas as well as some of the methods available

262
00:18:08,790 --> 00:18:12,050
as part of the models that we created before,

263
00:18:12,120 --> 00:18:15,966
including the base PY schema Py

264
00:18:15,998 --> 00:18:19,490
and data frame PY by calling some

265
00:18:19,560 --> 00:18:21,300
methods available there.

266
00:18:24,090 --> 00:18:30,142
And then we

267
00:18:30,196 --> 00:18:31,920
call well,

268
00:18:33,970 --> 00:18:37,378
every cpu core called the

269
00:18:37,464 --> 00:18:41,842
create data frame method and every

270
00:18:41,896 --> 00:18:45,474
cpu core will generate 4000 records and

271
00:18:45,512 --> 00:18:52,054
then we concatenate that data into

272
00:18:52,092 --> 00:18:55,960
a data frame. And for

273
00:18:56,330 --> 00:18:59,446
MySQL and postgres we

274
00:18:59,468 --> 00:19:04,010
are calling to SQL method provided by pandas,

275
00:19:05,630 --> 00:19:09,562
we need to specify the name of the table where that

276
00:19:09,616 --> 00:19:11,290
information will be stored,

277
00:19:15,010 --> 00:19:17,120
the database connection information,

278
00:19:17,810 --> 00:19:20,858
and if that data exists

279
00:19:20,954 --> 00:19:24,846
we will append that data. We can change that

280
00:19:25,028 --> 00:19:28,642
later. And also we need to

281
00:19:28,696 --> 00:19:32,318
specify the schema, the one that we defined

282
00:19:32,414 --> 00:19:36,002
before, so that pandas know where

283
00:19:36,056 --> 00:19:39,686
to insert all

284
00:19:39,708 --> 00:19:40,440
the information.

285
00:19:42,810 --> 00:19:46,838
Okay, for MongoDB, this is the way that we

286
00:19:46,924 --> 00:19:51,014
insert data into MongoDB database.

287
00:19:51,142 --> 00:19:55,562
First we need to convert the

288
00:19:55,616 --> 00:20:01,606
data that we generated into a python

289
00:20:01,638 --> 00:20:05,054
dictionary. Then we need

290
00:20:05,092 --> 00:20:08,522
to specify the name of the database

291
00:20:08,586 --> 00:20:12,170
and the collection where the information will be stored.

292
00:20:12,330 --> 00:20:17,106
And by calling the insert menu, we insert all

293
00:20:17,128 --> 00:20:21,330
the data to the MongoDB database.

294
00:20:22,870 --> 00:20:25,490
After we inserted,

295
00:20:27,850 --> 00:20:31,862
after we insert the data into every

296
00:20:31,916 --> 00:20:35,606
database, MySQL, postgres and Mongo, we need

297
00:20:35,628 --> 00:20:39,590
to modify the tables

298
00:20:40,190 --> 00:20:44,138
especially for MySQL and postgres because

299
00:20:44,304 --> 00:20:48,890
we are not defining

300
00:20:51,950 --> 00:20:56,190
a column in that table,

301
00:20:56,770 --> 00:21:01,040
that will be our primary key. So we need to add

302
00:21:01,890 --> 00:21:08,562
another column and this

303
00:21:08,616 --> 00:21:11,630
will be auto increment values.

304
00:21:11,790 --> 00:21:15,506
And the way that we do that for MySQL and

305
00:21:15,528 --> 00:21:17,854
postgres is slightly different.

306
00:21:17,992 --> 00:21:22,322
So for MySQL we will execute

307
00:21:22,386 --> 00:21:26,210
this SQL statement alter table

308
00:21:26,290 --> 00:21:29,480
employees at id. That will be our

309
00:21:29,930 --> 00:21:33,894
primary key. This column

310
00:21:34,022 --> 00:21:37,818
will be not null and this will be an

311
00:21:37,904 --> 00:21:42,010
auto increment. The values will be auto increment.

312
00:21:43,250 --> 00:21:47,662
And with the first option here

313
00:21:47,796 --> 00:21:51,774
we are saying that that column will

314
00:21:51,812 --> 00:21:55,470
be added at the beginning of the table.

315
00:21:56,390 --> 00:22:00,420
For postgres, we don't have the auto increment option.

316
00:22:01,270 --> 00:22:04,562
What we have in postgres is the

317
00:22:04,616 --> 00:22:07,886
serial option that is similar to the

318
00:22:07,928 --> 00:22:11,926
auto increment in MySQL. So this is why we

319
00:22:11,948 --> 00:22:15,602
have two different instructions

320
00:22:15,666 --> 00:22:18,070
for both database technologies.

321
00:22:20,350 --> 00:22:24,170
Okay, so it's time for

322
00:22:24,320 --> 00:22:27,050
running a script.

323
00:22:29,310 --> 00:22:32,666
Well, I already created a repository where you

324
00:22:32,688 --> 00:22:36,160
can find all the scripts that I explained here.

325
00:22:37,090 --> 00:22:41,354
I'm using an anaconda, but you can install all the dependencies

326
00:22:41,402 --> 00:22:45,086
by running Pip install. But if

327
00:22:45,108 --> 00:22:49,758
you want to use Anaconda for better managing your environment

328
00:22:49,854 --> 00:22:53,874
and all the dependencies of your project, you can download it

329
00:22:53,992 --> 00:22:59,202
here. You can find installation

330
00:22:59,266 --> 00:23:03,490
files for Windows,

331
00:23:03,570 --> 00:23:06,918
Mac and Linux. Then we install

332
00:23:07,004 --> 00:23:11,214
Anaconda. This is the instruction for Linux distributions

333
00:23:11,362 --> 00:23:15,034
and then we clone the repository. You can find

334
00:23:15,152 --> 00:23:18,794
the repository that contains all the example that

335
00:23:18,832 --> 00:23:22,122
I explained here. At this

336
00:23:22,176 --> 00:23:25,934
repository we

337
00:23:25,972 --> 00:23:29,070
change to the data generator directory,

338
00:23:29,650 --> 00:23:33,246
configure the python environment by installing all the

339
00:23:33,268 --> 00:23:35,750
dependencies and configuring the environment.

340
00:23:35,930 --> 00:23:39,966
This will create the environment defined

341
00:23:39,998 --> 00:23:44,386
in the environment yaml file. We need to edit the

342
00:23:44,568 --> 00:23:48,374
base Pymodel that contain information about

343
00:23:48,412 --> 00:23:52,258
how to establish the connection to the database.

344
00:23:52,434 --> 00:23:55,880
And then we

345
00:23:56,410 --> 00:24:00,200
run the script. And finally we can check

346
00:24:00,670 --> 00:24:04,714
the database for the

347
00:24:04,752 --> 00:24:08,010
data that we generated by running this script.

348
00:24:09,630 --> 00:24:13,534
Here are some resources that can be useful. You can

349
00:24:13,572 --> 00:24:17,086
check the Faker website. You can

350
00:24:17,108 --> 00:24:20,686
find more information on bundle providers that are the

351
00:24:20,708 --> 00:24:24,702
ones that are

352
00:24:24,756 --> 00:24:28,434
available with Faker by default. You can use

353
00:24:28,552 --> 00:24:32,626
community provider. You can find more information on

354
00:24:32,728 --> 00:24:37,774
the website. You can check the

355
00:24:37,832 --> 00:24:41,400
SQL Alchemy website as well as check the

356
00:24:42,090 --> 00:24:45,874
schema definition language to know how you can define

357
00:24:45,922 --> 00:24:49,378
the schema of your database by using SQL alchemy.

358
00:24:49,554 --> 00:24:53,098
We didn't do that because

359
00:24:53,184 --> 00:24:57,206
we are using the built in method provided

360
00:24:57,238 --> 00:25:01,130
by Panda for inserting data into the database.

361
00:25:03,170 --> 00:25:07,610
Here you can find more information about using multiprocessing

362
00:25:07,690 --> 00:25:11,710
to make Python code faster. That is the idea behind

363
00:25:11,780 --> 00:25:15,510
multi processing. Instead of having one cpu

364
00:25:15,690 --> 00:25:18,926
core running all the tasks in our script,

365
00:25:19,038 --> 00:25:22,354
we have all the cpu cores available

366
00:25:22,472 --> 00:25:27,474
running part of the process and we

367
00:25:27,512 --> 00:25:31,270
can improve the runtime of our

368
00:25:31,340 --> 00:25:31,960
application.

369
00:25:34,810 --> 00:25:35,560
Okay,

370
00:25:40,190 --> 00:25:44,794
so here

371
00:25:44,832 --> 00:25:48,300
youre can find more information about the careers available.

372
00:25:51,470 --> 00:25:56,910
I just want to say thank you for being

373
00:25:56,980 --> 00:26:01,600
here and I

374
00:26:03,090 --> 00:26:06,398
really enjoy sharing with the community today.

375
00:26:06,484 --> 00:26:10,734
If you have any questions about what

376
00:26:10,772 --> 00:26:14,158
I shared today, you can contact

377
00:26:14,244 --> 00:26:18,440
me on social media or by email or are.

378
00:26:20,010 --> 00:26:23,286
You can also check the blog post that I

379
00:26:23,308 --> 00:26:26,918
already published about this topic and I'm planning to

380
00:26:27,004 --> 00:26:29,300
update the code of this project.

