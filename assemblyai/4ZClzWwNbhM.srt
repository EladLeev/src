1
00:02:05,500 --> 00:02:09,144
Hi everybody, my name is Chris Ashton and I'm here with Ravi and Rohith from

2
00:02:09,182 --> 00:02:12,836
Microsoft, and we're going to talk to you today about using chaos engineering

3
00:02:12,868 --> 00:02:16,000
practices to ship left and prevent outages.

4
00:02:16,580 --> 00:02:20,496
I'm going to talk to you a little bit about chaos engineering in

5
00:02:20,518 --> 00:02:23,728
general and give an introduction to Azure Chaos studio. And then

6
00:02:23,734 --> 00:02:27,184
Ravi and Rohith will take over and talk more in depth about practices they've

7
00:02:27,232 --> 00:02:29,300
developed and are using in their teams.

8
00:02:32,840 --> 00:02:36,500
Software as a service has revolutionized cloud application development.

9
00:02:37,240 --> 00:02:40,308
It's now easier to get applications up

10
00:02:40,314 --> 00:02:43,608
and running in a shorter period of time, and you have a variety of

11
00:02:43,694 --> 00:02:47,128
microservices and other services available in a

12
00:02:47,134 --> 00:02:51,476
toolbox to pull together and compose and build these applications.

13
00:02:51,668 --> 00:02:54,764
While this has great benefits, with it comes

14
00:02:54,882 --> 00:02:56,300
a new set of challenges.

15
00:02:57,680 --> 00:03:01,388
Service outages and disruptions can happen at any time and this

16
00:03:01,474 --> 00:03:05,336
impacts your overall application unavailability and can therefore upset

17
00:03:05,368 --> 00:03:08,784
your customers. And these

18
00:03:08,822 --> 00:03:11,984
disruptions can happen at any time. You have a lot less

19
00:03:12,022 --> 00:03:15,456
control over the environment in which your things are running, and you

20
00:03:15,478 --> 00:03:19,220
could be subject to power outage at a data center,

21
00:03:19,370 --> 00:03:23,012
a flood in a control room, network going

22
00:03:23,066 --> 00:03:26,864
down, bugs being introduced by developers into different components,

23
00:03:26,912 --> 00:03:31,004
configuration changes coming at any time, lots of variables

24
00:03:31,152 --> 00:03:33,400
make this a very unfriendly environment.

25
00:03:34,220 --> 00:03:37,450
So now more than ever, applications must be

26
00:03:37,900 --> 00:03:41,080
resilient. They must be designed to handle these failures.

27
00:03:41,980 --> 00:03:45,916
Microsoft provides the well architected framework in Azure Architecture center

28
00:03:46,018 --> 00:03:49,292
to give application developers guidance on how to build

29
00:03:49,346 --> 00:03:53,196
resilient applications. But basically,

30
00:03:53,378 --> 00:03:56,588
resilience is a shared responsibility. Microsoft needs to

31
00:03:56,594 --> 00:04:00,208
ensure that the platform is resilient and build a lot of capabilities into the

32
00:04:00,214 --> 00:04:03,276
services that make up all of the components

33
00:04:03,308 --> 00:04:07,264
that you can use when you're composing your applications and the infrastructure that they run

34
00:04:07,302 --> 00:04:11,024
on. But then it's also responsibility

35
00:04:11,072 --> 00:04:14,640
of the application developer to validate their resilience

36
00:04:14,720 --> 00:04:17,936
and make sure that you can handle the conditions that you've

37
00:04:17,968 --> 00:04:21,576
designed for and unexpected things that

38
00:04:21,598 --> 00:04:25,000
come up. Basically trust but verify.

39
00:04:25,820 --> 00:04:28,984
Build your application using these guidelines and

40
00:04:29,022 --> 00:04:33,130
then make sure that it operates in the way that you expect and understand.

41
00:04:33,840 --> 00:04:36,956
These quality practices can be done on an

42
00:04:36,978 --> 00:04:40,332
ad hoc basis. Some folks perform game days

43
00:04:40,386 --> 00:04:44,472
and drill events and that's great. But more mature

44
00:04:44,536 --> 00:04:48,476
organizations are building these quality practices right into their service development

45
00:04:48,508 --> 00:04:52,064
lifecycle. Add chaos engineering to your

46
00:04:52,102 --> 00:04:55,840
CI CD pipeline, test in a pre production environment.

47
00:04:58,420 --> 00:05:02,340
Shield your customers from these outages and issues and

48
00:05:02,490 --> 00:05:06,260
shift left and do these things earlier so that you can catch the defects early

49
00:05:06,330 --> 00:05:08,580
and prevent them from going to production.

50
00:05:10,600 --> 00:05:14,164
This is where chaos engineering comes in. Chaos Engineering is a practice

51
00:05:14,212 --> 00:05:17,640
that has evolved over the last decade of subjecting your

52
00:05:17,710 --> 00:05:20,852
application and services to the real world outages

53
00:05:20,916 --> 00:05:23,768
and disruptions that they will face in production.

54
00:05:23,944 --> 00:05:27,340
You've designed for failure, you've architected for resilience.

55
00:05:28,160 --> 00:05:31,932
Make sure that you can stand

56
00:05:31,986 --> 00:05:34,300
up to the disruptions that will be encountered.

57
00:05:35,360 --> 00:05:38,656
With chaos Engineering. You're not just validating that

58
00:05:38,678 --> 00:05:42,396
you've chosen the right architecture and that your code doesn't have any bugs.

59
00:05:42,508 --> 00:05:45,728
You're also validating that you've configured everything

60
00:05:45,814 --> 00:05:49,184
correctly, that you've got the correct observability and monitoring

61
00:05:49,232 --> 00:05:53,792
in place, and then you're also able to validate your process aspects.

62
00:05:53,936 --> 00:05:57,364
Does your DRI, your designated responsible individual,

63
00:05:57,482 --> 00:06:01,412
or your on call engineer have the reporting and

64
00:06:01,466 --> 00:06:05,028
monitoring and metrics in place that they can see what's

65
00:06:05,044 --> 00:06:08,664
going on with your service? Do they have the correct troubleshooting guides in place when

66
00:06:08,702 --> 00:06:11,988
something goes wrong and they need to investigate and see what's

67
00:06:12,004 --> 00:06:15,596
going on further? So all of these things

68
00:06:15,778 --> 00:06:19,384
are basically enabled by and validated

69
00:06:19,432 --> 00:06:22,604
through chaos engineering. But a big

70
00:06:22,642 --> 00:06:26,748
component of Chris is to do it systematically and in a controlled way.

71
00:06:26,914 --> 00:06:29,596
There's a lot of value in doing this in production.

72
00:06:29,708 --> 00:06:33,436
There's a lot of value in sitting down and just injecting

73
00:06:33,468 --> 00:06:37,292
faults, and that could be quite fun, but really, it's very dangerous,

74
00:06:37,356 --> 00:06:40,868
especially doing it in production. You could impact your customers. You could accidentally cause an

75
00:06:40,874 --> 00:06:44,256
outage. It's much better to do this in an earlier stage,

76
00:06:44,288 --> 00:06:47,590
in a pre production environment that's very production like,

77
00:06:47,960 --> 00:06:51,304
that represents the infrastructure it will run on

78
00:06:51,342 --> 00:06:54,984
all of the services that are running and how they're configured. With a

79
00:06:55,022 --> 00:06:58,760
production like workload, usually synthetic,

80
00:06:59,420 --> 00:07:02,810
but still representing what your real workload would be.

81
00:07:03,600 --> 00:07:07,244
And then you're able to, in a much

82
00:07:07,282 --> 00:07:10,510
more systematic and scientific way,

83
00:07:11,120 --> 00:07:14,924
assess the resilience of your solutions. So with

84
00:07:14,962 --> 00:07:18,208
chaos engineering, there's a little bit of a

85
00:07:18,214 --> 00:07:22,144
metaphor around following the scientific method, we have

86
00:07:22,262 --> 00:07:26,604
chaos experiments. But just like with the scientific method,

87
00:07:26,732 --> 00:07:30,324
you don't just start down and doing an experiment. It's good to sit down and

88
00:07:30,362 --> 00:07:34,800
think about what you want to do, form a hypothesis. Which resiliency

89
00:07:34,880 --> 00:07:38,704
measures have you taken advantage of, and what are your expectations

90
00:07:38,752 --> 00:07:42,276
of your application when it is faced

91
00:07:42,308 --> 00:07:46,180
with disruptions? Will you fail over to another region?

92
00:07:46,340 --> 00:07:49,992
What happens if an availability zone goes down? What happens

93
00:07:50,046 --> 00:07:52,920
if there's an AAD outage or a DNS outage?

94
00:07:54,080 --> 00:07:57,724
Formulate an experiment, go out and perform it. Monitor how

95
00:07:57,762 --> 00:08:01,496
your system works while it's being disrupted

96
00:08:01,528 --> 00:08:05,964
and affected. And then do some post experiment

97
00:08:06,012 --> 00:08:09,536
analysis, make some improvements, and then rinse and repeat. This can

98
00:08:09,558 --> 00:08:12,992
become part of a very virtuous cycle and for

99
00:08:13,046 --> 00:08:16,960
mature teams this fits very nicely into a typical

100
00:08:17,860 --> 00:08:22,012
product lifecycle and validation cycle. Adding chaos

101
00:08:22,076 --> 00:08:25,712
to a CI CD pipeline is a great way to make this very methodical,

102
00:08:25,776 --> 00:08:27,300
systematic and organized.

103
00:08:31,710 --> 00:08:35,546
There are lots and lots of use cases for

104
00:08:35,568 --> 00:08:39,774
chaos engineering and then of course Azure Chaos studio. You can basically,

105
00:08:39,892 --> 00:08:43,534
a developer can sit down in their office and just do ad hoc experimentation and

106
00:08:43,572 --> 00:08:47,040
disrupt dependencies systematically and one by one,

107
00:08:47,650 --> 00:08:51,618
hopefully in a dev or test environment to validate their new code or

108
00:08:51,704 --> 00:08:55,042
validate some upcoming configuration changes and just

109
00:08:55,096 --> 00:08:58,290
see how the system will handle it. That's a great way to get started.

110
00:08:58,440 --> 00:09:01,734
Another thing lots of folks like to do is host drill events or game

111
00:09:01,772 --> 00:09:05,318
days. These are great ways to gather a

112
00:09:05,324 --> 00:09:09,106
group of people together in a designated period of time and validate

113
00:09:09,138 --> 00:09:12,330
one or more hypotheses and see how the system

114
00:09:12,480 --> 00:09:16,102
stands up to these. Again, best to do in a safe

115
00:09:16,166 --> 00:09:19,930
environment, shifting left, but also often done in production.

116
00:09:20,910 --> 00:09:24,494
These are great ways to catch monitoring gaps, validate your

117
00:09:24,692 --> 00:09:27,786
livesight process, but of course catch code defects

118
00:09:27,818 --> 00:09:31,440
and architectural issues. But as I've mentioned,

119
00:09:33,010 --> 00:09:36,270
we really believe that it is important to do this early

120
00:09:36,340 --> 00:09:39,422
and if at all possible, do it in automation.

121
00:09:39,486 --> 00:09:43,202
Get it into your CI CD pipeline or some other automated process

122
00:09:43,256 --> 00:09:46,434
that you have. You dont have to shift all the way

123
00:09:46,472 --> 00:09:50,038
left and do this in a dev stage with your

124
00:09:50,124 --> 00:09:53,574
acceptance tests. Although some teams have been quite successful doing

125
00:09:53,612 --> 00:09:57,442
that, most teams that are doing this are quite successful

126
00:09:57,506 --> 00:10:01,254
integrating in their integration

127
00:10:01,302 --> 00:10:05,034
stage and often augment existing stress or

128
00:10:05,072 --> 00:10:08,902
performance benchmarks or some other synthetic workload. With chaos,

129
00:10:09,046 --> 00:10:12,858
they're able to run the existing tests and

130
00:10:13,024 --> 00:10:17,086
gauge their baseline or how do they perform against known metrics and

131
00:10:17,108 --> 00:10:20,446
trends, as well as gauge the impact to those trends and

132
00:10:20,468 --> 00:10:24,890
baselines when chaos is applied and apply that to overall availability

133
00:10:24,970 --> 00:10:28,258
metrics. Digging a

134
00:10:28,264 --> 00:10:30,610
little bit further, we've talked about the chaos experiment.

135
00:10:32,310 --> 00:10:35,746
Formulate a hypothesis, think about the things that you

136
00:10:35,768 --> 00:10:39,254
want to validate and in automation, what would this

137
00:10:39,292 --> 00:10:43,330
look like? Sometimes it's very systematic.

138
00:10:43,410 --> 00:10:46,722
Disrupt each of your dependencies one by one. Other cases

139
00:10:46,786 --> 00:10:50,582
it's craft things around a scenario so

140
00:10:50,636 --> 00:10:53,958
you could systematically go through and say, I'm dependent on Cosmos

141
00:10:53,974 --> 00:10:57,334
DB, I'm dependent on SQL server, and introduce

142
00:10:57,382 --> 00:11:00,906
different disruptions for each of these one by one. Or you could

143
00:11:00,928 --> 00:11:04,670
think about this from a scenario perspective and look at

144
00:11:04,740 --> 00:11:08,766
things like what happens if an availability zone goes down or a

145
00:11:08,788 --> 00:11:11,200
DNS outage and so on.

146
00:11:15,570 --> 00:11:18,400
So then here's where Azure Chaos studio comes in.

147
00:11:18,930 --> 00:11:22,466
Azure chaos Studio is a tool that

148
00:11:22,488 --> 00:11:26,446
we developed for use inside Microsoft to validate

149
00:11:26,558 --> 00:11:29,666
the resilience of the components that

150
00:11:29,688 --> 00:11:33,480
make up Azure. And these are things like

151
00:11:34,090 --> 00:11:38,680
our identity with Azure Active directory or

152
00:11:39,130 --> 00:11:43,042
the storage platform networking. These teams are using chaos

153
00:11:43,106 --> 00:11:46,602
to validate the resilience of the platform itself, but then also

154
00:11:46,656 --> 00:11:50,262
other solutions inside Microsoft that are building on top of that platform use chaos

155
00:11:50,326 --> 00:11:54,122
to validate their resilience. Microsoft Teams dynamics power

156
00:11:54,176 --> 00:11:57,226
platform but then we didn't want this just

157
00:11:57,248 --> 00:12:00,782
to be an internal tool and to keep it to Microsoft. We wanted

158
00:12:00,836 --> 00:12:04,666
our customers to be able to do these same validations. So we're actually releasing Azure

159
00:12:04,698 --> 00:12:08,266
Chaos Studio as a product. We want our customers to be able to validate

160
00:12:08,298 --> 00:12:11,650
their resilience to things that can go wrong on the Azure platform,

161
00:12:11,800 --> 00:12:15,086
but also to validate the resilience of their own applications and solutions

162
00:12:15,118 --> 00:12:18,030
and the communications amongst their own microservices,

163
00:12:18,110 --> 00:12:21,910
et cetera. So Azure Castio is currently in

164
00:12:22,060 --> 00:12:25,702
public preview and we're currently planning on going

165
00:12:25,756 --> 00:12:29,126
ga general availability later this year. But in

166
00:12:29,148 --> 00:12:32,562
general, Azure Chaos Studio is a fully managed service. We basically

167
00:12:32,636 --> 00:12:36,374
offer chaos as a service. We have deep Azure integration.

168
00:12:36,502 --> 00:12:39,974
We have an Azure portal user interface, we have Azure resource

169
00:12:40,022 --> 00:12:43,942
manager arm compliant APIs

170
00:12:44,086 --> 00:12:48,046
so that you can use Chaos studio manually from

171
00:12:48,068 --> 00:12:51,178
the user interface, or you can use it programmatically through the APIs

172
00:12:51,354 --> 00:12:55,374
to validate the resilience of your solutions by

173
00:12:55,412 --> 00:12:56,830
introducing faults.

174
00:12:59,350 --> 00:13:02,974
A key component of Azure Chaos Studio is an expandable and growing

175
00:13:03,022 --> 00:13:07,074
fault library. This fault library has capabilities that

176
00:13:07,112 --> 00:13:10,386
allow you to introduce faults in a variety of ways. I'll talk

177
00:13:10,408 --> 00:13:13,622
about that a little bit more in a second that allow

178
00:13:13,676 --> 00:13:17,506
you to perturb your application in realistic

179
00:13:17,538 --> 00:13:20,918
ways, and we have workload execution that allows you

180
00:13:20,924 --> 00:13:24,810
to do orchestration that allows you to execute cause experiments

181
00:13:25,150 --> 00:13:28,694
that have fault actions and disruptions that can be applied

182
00:13:28,742 --> 00:13:32,490
sequentially and in parallel to make up these scenarios.

183
00:13:33,230 --> 00:13:36,300
Again, I'll talk about that a little bit more in a second,

184
00:13:37,310 --> 00:13:40,814
but we don't want you to accidentally cause

185
00:13:40,852 --> 00:13:44,346
an outage while you're doing this, and we don't want your testing activities to bleed

186
00:13:44,378 --> 00:13:48,160
over and affect production. So we're building a lot of safeguards into the product.

187
00:13:49,330 --> 00:13:52,514
We've got role based access control for the Azure resources that

188
00:13:52,552 --> 00:13:55,730
can be used in experiments, and we have role based access

189
00:13:55,800 --> 00:14:00,046
control for who can run the experiments and

190
00:14:00,088 --> 00:14:04,150
even what faults can be used on individual resources.

191
00:14:05,930 --> 00:14:10,002
One other final thing super important, we're building integration

192
00:14:10,066 --> 00:14:14,146
in so that we've got observability

193
00:14:14,258 --> 00:14:18,134
aspects to this. We want you to be able to correlate

194
00:14:18,182 --> 00:14:21,434
your chaos activities with the things that are going on in your service

195
00:14:21,552 --> 00:14:25,058
so that you can see right in your own telemetry and monitoring what's

196
00:14:25,094 --> 00:14:28,366
happening and enable evaluation of

197
00:14:28,388 --> 00:14:32,154
what's going on, as well as to enable you to validate

198
00:14:32,202 --> 00:14:35,840
that you have the correct monitoring and observability components in place.

199
00:14:38,130 --> 00:14:43,986
So even more detail on Azure Chaos studio on

200
00:14:44,008 --> 00:14:47,426
the right here you'll see the faults and actions that kind

201
00:14:47,448 --> 00:14:50,606
of make up our library. We have agent based faults.

202
00:14:50,718 --> 00:14:54,286
We have a chaos agent that can be installed or deployed to

203
00:14:54,328 --> 00:14:57,814
your windows or your Linux virtual machines. And then

204
00:14:57,852 --> 00:15:00,822
through that we have a variety of faults that can affect things that are running

205
00:15:00,876 --> 00:15:04,534
right there on that virtual machine. Lots of resource pressure faults,

206
00:15:04,582 --> 00:15:08,266
cpu, memory disk I o. We can delay the

207
00:15:08,288 --> 00:15:11,914
network calls, we can actually block access to

208
00:15:12,032 --> 00:15:15,718
IP addresses. We can change firewall rules

209
00:15:15,734 --> 00:15:19,374
on Windows systems, a few other things. And we have more

210
00:15:19,412 --> 00:15:22,846
faults coming on the other side of the house. We have what we call

211
00:15:22,868 --> 00:15:26,446
service direct faults, no agent required. We can

212
00:15:26,468 --> 00:15:29,954
disrupt Azure services directly. This allows us to do things

213
00:15:29,992 --> 00:15:33,998
like shut down or kill gracefully or abruptly.

214
00:15:34,094 --> 00:15:36,370
We can shut down virtual machines.

215
00:15:37,350 --> 00:15:41,026
We have a cosmos DB Fillover fault. We can reboot

216
00:15:41,058 --> 00:15:45,160
redis, so several very specific

217
00:15:45,610 --> 00:15:49,334
faults that can be applied to services. Another group

218
00:15:49,372 --> 00:15:51,910
of faults that we have like this is for aks.

219
00:15:53,070 --> 00:15:56,902
We leverage chaos mesh to be able to apply fault actions

220
00:15:56,966 --> 00:16:00,826
to aks. And then we

221
00:16:00,848 --> 00:16:04,698
have some more generic fault activities that can be done. So we have

222
00:16:04,784 --> 00:16:08,238
network security group rules faults that allow you to block access

223
00:16:08,324 --> 00:16:12,554
to IP addresses and ranges so that you could disrupt your own microservice

224
00:16:12,602 --> 00:16:16,618
communications. And then through Azure service tags

225
00:16:16,794 --> 00:16:20,414
we can also block access to other Azure

226
00:16:20,462 --> 00:16:24,718
resources. So we can take the Azure Cosmos

227
00:16:24,734 --> 00:16:28,094
DB service tag, put that in an NSG rules

228
00:16:28,142 --> 00:16:32,002
fault, and block access to Cosmos DB. And then a really cool

229
00:16:32,136 --> 00:16:35,302
sub aspect of that is Azure service tags support

230
00:16:35,356 --> 00:16:39,110
a regional component. So you could do things like block access to

231
00:16:39,180 --> 00:16:42,834
Azure Cosmos DB east us and affect

232
00:16:42,882 --> 00:16:46,266
only the Cosmos DB databases that are running in east us. And that would

233
00:16:46,288 --> 00:16:50,758
allow you to do things like a region failover

234
00:16:50,854 --> 00:16:54,346
or region isolation drill. And so that's what's on the left of

235
00:16:54,368 --> 00:16:58,230
the slide is a lot of the scenarios that we recommend that you validate.

236
00:16:58,390 --> 00:17:01,902
It's good to think about the faults and the individual disruptions that you can apply

237
00:17:01,956 --> 00:17:05,534
to your dependencies, but it's much better to think about these things

238
00:17:05,572 --> 00:17:08,938
a lot more systematically in terms of the incidents

239
00:17:08,954 --> 00:17:12,980
and outages that will affect you and your application or service.

240
00:17:13,670 --> 00:17:17,534
So again, I've mentioned these several times, but what happens when an availability zone

241
00:17:17,582 --> 00:17:21,650
goes down? What happens when there's a DNS outage? Gas studio

242
00:17:21,990 --> 00:17:26,006
today has faults that help you validate these scenarios in sort

243
00:17:26,028 --> 00:17:30,102
of fairly comprehensive but somewhat limited ways over time

244
00:17:30,156 --> 00:17:33,474
and by the time we go general availability, we want to have much more robust

245
00:17:33,522 --> 00:17:37,158
capabilities to do a full unavailability zone down exercise.

246
00:17:37,254 --> 00:17:40,614
So today we can affect vms, vms service fabric,

247
00:17:40,662 --> 00:17:44,454
virtual machines and aks and have it appear that zone

248
00:17:44,502 --> 00:17:48,060
two has gone down in the east us region or something.

249
00:17:48,590 --> 00:17:52,206
But over time we'll be able to impact Cosmos, DB, SQL server and

250
00:17:52,228 --> 00:17:56,346
other things in that same way. Same with all of our faults and the library.

251
00:17:56,458 --> 00:18:00,542
We're just going to keep working with individual Azure service teams to enable

252
00:18:00,606 --> 00:18:03,774
more fault capabilities and expose those in our library

253
00:18:03,902 --> 00:18:07,970
so that you can then build up to more scenarios that you validate.

254
00:18:08,710 --> 00:18:12,230
Now I'm going to hand off to Ravi who will talk to you more about

255
00:18:12,380 --> 00:18:16,098
how they're using Chaos engineering

256
00:18:16,114 --> 00:18:20,086
and Azure Chaos Studio in the cloud security organization to

257
00:18:20,108 --> 00:18:24,054
shift left and add a lot of chaos validation

258
00:18:24,182 --> 00:18:28,106
right in their development pipelines and in their development process to

259
00:18:28,208 --> 00:18:31,462
catch defects early and to prevent outages

260
00:18:31,526 --> 00:18:35,338
and disruptions. Thanks everybody for your time. Hello everyone,

261
00:18:35,424 --> 00:18:39,046
I am Ravi Belam, part of Microsoft Security organization that provides

262
00:18:39,078 --> 00:18:42,362
solutions to protect our customer and Microsoft workloads.

263
00:18:42,506 --> 00:18:46,430
I'm leading the charge in delivering proactive security and reliability through

264
00:18:46,500 --> 00:18:49,934
platform data, site reliability engineering and machine learning solutions

265
00:18:49,982 --> 00:18:52,690
in the Microsoft Security Management Plane organization.

266
00:18:53,430 --> 00:18:57,294
Thanks Chris for a great overview on Chaos engineering practices and Azure

267
00:18:57,342 --> 00:19:00,986
Chaos Studio which is a cutting edge platform that helps organizations embrace

268
00:19:01,038 --> 00:19:04,370
uncertainty and drive innovation. In this segment,

269
00:19:04,450 --> 00:19:08,086
I will dwell into how a big organization like ours scaled at

270
00:19:08,108 --> 00:19:12,226
chaos engineering adoption with a combination of process and technology innovation,

271
00:19:12,418 --> 00:19:16,102
followed by role's presentation on their experience with adopting

272
00:19:16,166 --> 00:19:19,914
Azure Chaos studio for their services. As Chris mentioned,

273
00:19:20,032 --> 00:19:23,862
Chaos Engineering is a discipline that focuses on creating

274
00:19:23,926 --> 00:19:27,038
and managing systems with a high reliability by

275
00:19:27,124 --> 00:19:31,230
intentionally introducing controlled chaos and testless resilience.

276
00:19:32,050 --> 00:19:36,126
Cloud ecosystem Security organization is a home for critical services

277
00:19:36,308 --> 00:19:40,306
and it's crucial for us to implement chaos engineering practices to

278
00:19:40,328 --> 00:19:44,034
proactively identify and mitigate potential problems before

279
00:19:44,072 --> 00:19:47,694
they affect our customers. However, the successful

280
00:19:47,742 --> 00:19:50,914
adoption of Chaos engineering can be challenging. In this

281
00:19:50,952 --> 00:19:54,434
talk, I would like to share the key steps that we have learned to scale

282
00:19:54,482 --> 00:19:57,030
Chaos engineering adoption proactively.

283
00:19:57,370 --> 00:20:00,886
The first step in scaling chaos engineering adoption is to set

284
00:20:00,908 --> 00:20:05,014
the vision by defining the problem and setting a clear goal chaos

285
00:20:05,062 --> 00:20:08,906
engineering practices are designed to improve the reliability and security of

286
00:20:08,928 --> 00:20:12,298
systems, and it's crucial to identify the areas where

287
00:20:12,384 --> 00:20:15,818
these improvements are needed the most. By defining the problem,

288
00:20:15,904 --> 00:20:19,742
we wanted to focus our effort on improving the areas that needed the most

289
00:20:19,796 --> 00:20:23,418
attention and set a clear goal for what we want to achieve.

290
00:20:23,594 --> 00:20:27,214
In a bit, I will go over into the details of how we set the

291
00:20:27,252 --> 00:20:30,562
vision. The second step is to anticipate the common

292
00:20:30,616 --> 00:20:34,114
pitfalls and forecast potential problems that may arise during

293
00:20:34,152 --> 00:20:37,326
this process. For example, organizations may struggle

294
00:20:37,358 --> 00:20:41,186
with obtaining buy in from stakeholders, ensuring that the chaos experiments

295
00:20:41,218 --> 00:20:44,466
do not impact end users. Measuring the success of chaos

296
00:20:44,498 --> 00:20:48,514
engineering experimentation, the cost of onboarding and prioritization

297
00:20:48,642 --> 00:20:51,366
these are just a few examples out of many.

298
00:20:51,548 --> 00:20:55,242
By anticipating these potential problems, we have prepared and put

299
00:20:55,296 --> 00:20:58,346
measures in place to mitigate them. Once the

300
00:20:58,368 --> 00:21:01,526
vision is set and the potential problems are anticipated,

301
00:21:01,638 --> 00:21:05,818
we have then set a strategic approach to scaling chaos engineering adoption.

302
00:21:05,914 --> 00:21:09,306
Chris approach include following

303
00:21:09,338 --> 00:21:12,702
some key elements like defining the scope of Chaos engineering program,

304
00:21:12,836 --> 00:21:17,210
establishing a governance of the program, developing a

305
00:21:17,380 --> 00:21:21,010
plan for conducting and automating the chaos experiments.

306
00:21:22,230 --> 00:21:26,206
Once the vision is set and potential problems are anticipated,

307
00:21:26,318 --> 00:21:29,886
we have then set a strategic approach to scaling chaos engineering

308
00:21:29,918 --> 00:21:33,522
adoption. This approach included the following key elements

309
00:21:33,666 --> 00:21:36,438
one, defining the scope of the program two,

310
00:21:36,524 --> 00:21:40,518
establishing the governance of the program three,

311
00:21:40,604 --> 00:21:44,126
developing a plan for conducting and automating the chaos

312
00:21:44,178 --> 00:21:47,594
experiments. The final step in scaling the adoption or

313
00:21:47,632 --> 00:21:51,146
onboarding of the Chaos engineering practices is to measure the

314
00:21:51,168 --> 00:21:54,714
success and continuously improve the approach. We have achieved this

315
00:21:54,752 --> 00:21:58,174
by gathering the feedback from the stakeholders, analyzing the data from

316
00:21:58,212 --> 00:22:01,850
Chaos experiment, analyzing the data from the services telemetry,

317
00:22:01,930 --> 00:22:05,386
and making changes to the approach. Based upon the insights gained,

318
00:22:05,498 --> 00:22:09,282
we have regularly reviewed and updated our program to ensure that

319
00:22:09,416 --> 00:22:13,330
it continues to meet the evolving needs of our customers and teams.

320
00:22:14,870 --> 00:22:18,242
Let's look into how we can achieve the key steps that we have

321
00:22:18,376 --> 00:22:21,910
discussed so far. In this section, I will dive deep into

322
00:22:21,980 --> 00:22:25,810
our approach to setting a vision and aligning with certain keystones.

323
00:22:25,970 --> 00:22:29,362
As a quick background, our distributed systems are exponentially

324
00:22:29,426 --> 00:22:32,786
evolving, growing complex by the day. Our services are required

325
00:22:32,818 --> 00:22:36,442
to be updated frequently while expected to deliver the highest level

326
00:22:36,496 --> 00:22:40,006
of quality, performance, availability and reliability.

327
00:22:40,198 --> 00:22:43,690
Service availability and performance could be impacted either from a direct

328
00:22:43,760 --> 00:22:47,582
or dependency related change. The primary causes of major

329
00:22:47,636 --> 00:22:51,546
incidents and outages are service degradation caused by unavailability,

330
00:22:51,658 --> 00:22:54,826
congestion, heavy traffic, load cascading failures

331
00:22:54,858 --> 00:22:58,670
to the combination of degradation and load. There is also another interesting

332
00:22:58,740 --> 00:23:02,626
instance where heavy load crumbles, a system that is recovering from an outage or

333
00:23:02,648 --> 00:23:05,726
incident which we have seen in past major outages.

334
00:23:05,918 --> 00:23:09,422
Instead of following the status quo of reacting to the outages,

335
00:23:09,566 --> 00:23:12,754
preventing one as early as possible from reaching the production

336
00:23:12,802 --> 00:23:16,134
has positive effect on customer experience. Overall well being

337
00:23:16,172 --> 00:23:19,602
of the on calls, developer productivity and revenue

338
00:23:19,746 --> 00:23:23,410
the objective of our chaos engineering effort is an ambiguous

339
00:23:23,490 --> 00:23:26,474
which is to improve the customer and our team's experience.

340
00:23:26,672 --> 00:23:30,554
We have centered the vision for implementing the chaos engineering practices around

341
00:23:30,592 --> 00:23:34,454
a few core principles. First, we put higher emphasis

342
00:23:34,502 --> 00:23:38,278
on the importance of culture of continuous improvement where our teams

343
00:23:38,294 --> 00:23:42,074
are empowered to implement and iterate on their process to drive positive

344
00:23:42,122 --> 00:23:46,046
outages. Second, we wanted to proactively identify and

345
00:23:46,068 --> 00:23:49,726
mitigate potential failures before they occur, reducing the likelihood

346
00:23:49,758 --> 00:23:53,346
of outages downtime and improving the overall customer experience.

347
00:23:53,528 --> 00:23:57,502
Our data suggests the cost of problem skyrockets

348
00:23:57,566 --> 00:24:01,326
as the software engineering progresses. This is where chaos engineering

349
00:24:01,358 --> 00:24:05,366
practices shine, allowing girls to simulate the real world failures or

350
00:24:05,388 --> 00:24:09,458
nonhappy catch scenarios at the early stages of the software development

351
00:24:09,634 --> 00:24:13,126
saving value experience. Our data suggests the cost

352
00:24:13,228 --> 00:24:16,806
of problems skyrockets as the software engineering progresses.

353
00:24:16,918 --> 00:24:20,346
This is where chaos engineering practices shine, allowing us to

354
00:24:20,368 --> 00:24:23,946
simulate real world failures or nonhappy pass scenarios as at the

355
00:24:23,968 --> 00:24:27,930
early stages of the software development, saving valuable time and resources.

356
00:24:28,010 --> 00:24:31,562
Chris also improves the overall resilience, reliability and stability

357
00:24:31,626 --> 00:24:35,066
of our systems and processes. The next core

358
00:24:35,098 --> 00:24:39,466
principle is duplication of effort which is costly and ineffective approach

359
00:24:39,578 --> 00:24:43,026
as it results in the redundant work and resources being spent on

360
00:24:43,048 --> 00:24:46,786
the same task. By centralizing our chaos engineering efforts through the

361
00:24:46,808 --> 00:24:50,366
use of Chaos studio, we have been able to avoid duplication

362
00:24:50,398 --> 00:24:54,134
of work and instead focused on partnering with the Azure Chaos studio team.

363
00:24:54,252 --> 00:24:58,022
This collaboration has allowed us to directly implement the features our

364
00:24:58,076 --> 00:25:01,714
organization requires and has resulted in more robust and useful

365
00:25:01,762 --> 00:25:05,798
product for both Microsoft and our customers. Our contributions

366
00:25:05,814 --> 00:25:09,660
such as adding features for Cosmos DB Failover Network Security

367
00:25:10,110 --> 00:25:13,654
group, Azure Keyword access denial and Azure

368
00:25:13,702 --> 00:25:17,506
keyword certificate rotation, attribute changes and validity

369
00:25:17,558 --> 00:25:21,386
related faults have made chaos studio an essential tool in our journey

370
00:25:21,418 --> 00:25:25,018
towards enhancing our customer and team experiences. To summarize,

371
00:25:25,114 --> 00:25:28,654
our ultimate goal is to make sure our services deliver faster

372
00:25:28,702 --> 00:25:30,580
with high quality for our customer.

373
00:25:31,990 --> 00:25:35,586
Having established the vision and core principles, we then took a

374
00:25:35,608 --> 00:25:39,982
proactive approach and considered the potential hurdles we might encounter during

375
00:25:40,056 --> 00:25:43,974
the onboarding process of hundreds of our services. Our analysis was

376
00:25:44,012 --> 00:25:47,974
based upon the previous similar initiatives. Despite many benefits,

377
00:25:48,092 --> 00:25:51,586
many organizations face challenges in adopting chaos engineering

378
00:25:51,618 --> 00:25:54,958
practices into the software development. One of the big challenge

379
00:25:54,994 --> 00:25:58,826
in adopting chaos engineering is the lack of understanding of the concept and

380
00:25:58,848 --> 00:26:03,254
its benefits. Many people are not familiar with chaos engineering

381
00:26:03,302 --> 00:26:06,934
and may got see the value in introducing chaos into their systems.

382
00:26:07,062 --> 00:26:10,606
This lack of understanding can lead to resistance and reluctance to

383
00:26:10,628 --> 00:26:13,934
dont the practices. Even with organizations that have good

384
00:26:13,972 --> 00:26:17,434
understanding, the challenge in adopting chaos engineering is the technical aspect.

385
00:26:17,562 --> 00:26:21,406
Creating chaos scenarios and conducting experiments can be complex and

386
00:26:21,428 --> 00:26:25,474
require specialized skill and knowledge. This can especially be

387
00:26:25,512 --> 00:26:29,518
difficult for organizations that have limited technical resources and other priorities.

388
00:26:29,694 --> 00:26:32,806
Additionally, the infrastructure required for chaos engineering can be also

389
00:26:32,828 --> 00:26:36,594
be costly and time consuming to implement. Integrating chaos

390
00:26:36,642 --> 00:26:40,066
engineering practices into existing workflows can also be challenging.

391
00:26:40,178 --> 00:26:43,826
Organizations need to figure out how to incorporate chaos engineering into the development

392
00:26:43,858 --> 00:26:47,114
and operational process without disrupting the work they already

393
00:26:47,232 --> 00:26:50,406
doing. This requires careful planning and a deep understanding

394
00:26:50,438 --> 00:26:54,246
about the current workflows. Finally, measuring the success of chaos

395
00:26:54,278 --> 00:26:57,658
engineering can be difficult. There are many factors that contribute to the success of

396
00:26:57,664 --> 00:27:01,418
chaos engineering and it can be challenging to quantify these factors and determine

397
00:27:01,434 --> 00:27:05,690
the overall impact of the system. This can make it difficult for organizations

398
00:27:05,770 --> 00:27:09,746
to see the value of investment they are making in chaos engineering and

399
00:27:09,768 --> 00:27:13,570
to make the decisions about continuing or expanding the adoption of the practices.

400
00:27:15,190 --> 00:27:18,814
So far, we have covered the vision, key principles and challenges

401
00:27:18,862 --> 00:27:22,882
we may encounter during the adoption process. With our goals and potential

402
00:27:22,946 --> 00:27:26,822
obstacles in mind, we devised the strategy for success and

403
00:27:26,876 --> 00:27:30,786
scalability that we will now dwell into. As outlined

404
00:27:30,818 --> 00:27:34,398
previously, our objective is to design dependable and efficient

405
00:27:34,434 --> 00:27:37,626
service that delivers a smooth experience for the customers and

406
00:27:37,648 --> 00:27:41,366
our teams. To accomplish this, we need a comprehensive chaos

407
00:27:41,398 --> 00:27:44,758
engineering platform. In our case, it is Azure Chaos studio

408
00:27:44,854 --> 00:27:48,318
and streamlined processes that are easy to use, even those with a

409
00:27:48,324 --> 00:27:52,042
limited knowledge. Apart from the previously discovered challenges,

410
00:27:52,186 --> 00:27:56,282
one other challenge of adopting chaos engineering is the complexity

411
00:27:56,346 --> 00:27:59,518
of individual services. With hundreds of services,

412
00:27:59,604 --> 00:28:03,246
it could be difficult to identify all the potential failure scenarios

413
00:28:03,278 --> 00:28:07,246
and to create a meaningful chaos experiment. Another challenge is the risk

414
00:28:07,278 --> 00:28:11,006
of introducing failures into a live system. The consequence of failed

415
00:28:11,038 --> 00:28:14,950
experiments can be severe, causing downtime and loss of revenue.

416
00:28:15,370 --> 00:28:19,014
To overcome these challenges, it's important to develop a strategy that

417
00:28:19,052 --> 00:28:22,754
considered the system's complexity and the potential risks involved.

418
00:28:22,882 --> 00:28:26,166
Our approach is to break down the scenarios into nominal

419
00:28:26,198 --> 00:28:29,354
and disaster scenarios. Nominal scenarios are those

420
00:28:29,392 --> 00:28:32,906
that can be quickly executed in the CI CD pipelines as part

421
00:28:32,928 --> 00:28:36,526
of the build qualification process. They are typically less risky and

422
00:28:36,548 --> 00:28:41,098
focus on simple failures such as network connectivity or resource exhaustion.

423
00:28:41,274 --> 00:28:45,210
For example, cpu pressure, memory pressure, disk ivo pressure

424
00:28:45,290 --> 00:28:48,610
and dependency disruption are a few examples.

425
00:28:49,510 --> 00:28:53,022
Disaster scenarios, on the other hand, focus on more complex

426
00:28:53,086 --> 00:28:56,878
failures such as for our cases, it's like ad outage,

427
00:28:56,974 --> 00:29:00,850
DNS outages, the load balancer based outages,

428
00:29:01,010 --> 00:29:05,234
availability zone outages, data center outages, and or load related

429
00:29:05,282 --> 00:29:08,454
large scale cascading failures. We put together

430
00:29:08,572 --> 00:29:12,134
recommendations within our documented standards that

431
00:29:12,172 --> 00:29:16,630
these scenarios should be carefully planned and executed in a controlled environment,

432
00:29:16,710 --> 00:29:20,186
such as staging environment or sandbox, and should not

433
00:29:20,208 --> 00:29:24,006
move to production unless the teams gain confidence with repeated

434
00:29:24,038 --> 00:29:27,582
experimentation and recovery. Validation as

435
00:29:27,636 --> 00:29:31,402
discussed before, one of the core aspects of chaos

436
00:29:31,466 --> 00:29:34,894
adoption is the continuous measurement of the effectiveness of

437
00:29:34,932 --> 00:29:38,402
the scenario validation. In order

438
00:29:38,456 --> 00:29:42,494
to validate the effectiveness of the chaos

439
00:29:42,542 --> 00:29:46,274
scenario testing, we teamed up with Azure Chaos Studio to track

440
00:29:46,312 --> 00:29:50,130
the resiliency of our services before, during, and after

441
00:29:50,200 --> 00:29:53,766
each experiment using the telemetry data. This not only gave us

442
00:29:53,788 --> 00:29:56,866
a clear picture of the resiliency of our services, but also enabled

443
00:29:56,898 --> 00:30:00,546
us to set standards and guard against the future deviations

444
00:30:00,578 --> 00:30:04,586
in our resiliency. With a solid understanding of the technology and

445
00:30:04,608 --> 00:30:07,786
process in place, we chose to pilot the implementation of

446
00:30:07,808 --> 00:30:11,450
chaos engineering within the critical services in our organization.

447
00:30:11,870 --> 00:30:15,706
Our selection process was strategic, choosing services that provided

448
00:30:15,738 --> 00:30:19,386
a comprehensive representation of compute, networking, storage,

449
00:30:19,418 --> 00:30:23,374
and authentication stacks. This allowed us to standardize nominal and

450
00:30:23,412 --> 00:30:26,926
disaster scenarios and gather valuable feedback from the

451
00:30:26,948 --> 00:30:30,674
pilot services onboarding, which allowed us to eventually improve our

452
00:30:30,712 --> 00:30:34,782
process and raise the needed features within the Chaos studio,

453
00:30:34,846 --> 00:30:37,966
which allowed other organizations and other teams within Microsoft

454
00:30:37,998 --> 00:30:39,620
to be successful as well.

455
00:30:42,090 --> 00:30:46,082
Here is an updated overview of our organization's onboarding progress

456
00:30:46,226 --> 00:30:49,506
made possible by the power of Azure Chaos Studio and our effective

457
00:30:49,538 --> 00:30:52,946
onboarding strategy. To date, we have effectively onboarded

458
00:30:52,978 --> 00:30:56,378
55 services with an additional 65 services in the

459
00:30:56,384 --> 00:30:59,754
process of onboarding. Of the 55 onboarded services,

460
00:30:59,872 --> 00:31:03,242
six are utilizing nominal scenarios for build qualification and

461
00:31:03,296 --> 00:31:06,170
two are conducting monthly disaster scenario validation.

462
00:31:06,330 --> 00:31:09,546
Our efforts have thus far uncovered 60 previously

463
00:31:09,578 --> 00:31:13,626
undetected bugs and prevented 22 critical issues that could have negatively

464
00:31:13,658 --> 00:31:17,626
impacted the customer experience. This is an ongoing effort

465
00:31:17,738 --> 00:31:21,202
and we are continuously working with our Azure Chaos Studio team to

466
00:31:21,256 --> 00:31:24,754
enhance the features within the Chaos studio, which would eventually help

467
00:31:24,792 --> 00:31:28,930
us to get better at adapting much more further scenarios

468
00:31:30,550 --> 00:31:33,762
to summarize our learnings. Scaling Chaos Engineering practices

469
00:31:33,826 --> 00:31:36,642
for your organization requires a clear vision,

470
00:31:36,706 --> 00:31:40,246
a strategic approach, and continuous improvement. By following the

471
00:31:40,268 --> 00:31:43,942
steps we have discussed thus far, organizations can effectively implement chaos

472
00:31:44,006 --> 00:31:47,514
engineering practices and achieve proactively reliability and security.

473
00:31:47,712 --> 00:31:51,386
By constantly measuring the success and making improvements, your organizations can

474
00:31:51,408 --> 00:31:54,902
continuously improve your chaos engineering program or adoption

475
00:31:54,966 --> 00:31:58,320
and ensure that it stays aligned with your evolving needs.

476
00:31:59,570 --> 00:32:02,814
With that, I'm going to hand over to Rohit, who will go over

477
00:32:02,852 --> 00:32:06,778
his team's successful journey of using Azure Cloud Studio and the strategy

478
00:32:06,794 --> 00:32:10,362
we discussed so far. Thanks everyone. Hello everyone.

479
00:32:10,436 --> 00:32:13,986
I'm Rohit Gundadi, part of Microsoft Secrets Automation. I'm the

480
00:32:14,008 --> 00:32:17,234
engineering manager for the key management platform that protects the data

481
00:32:17,272 --> 00:32:20,626
of Microsoft and our customers. Thanks Chris and Ravi for the

482
00:32:20,648 --> 00:32:24,406
overview of Chaos Engineering, Azure Chaos Studio, and the detailed explanation on how

483
00:32:24,428 --> 00:32:28,406
can automation scale the Chaos engineering adoption. During this

484
00:32:28,428 --> 00:32:32,850
presentation, I will concentrate on how we boosted our confidence in our service resiliency,

485
00:32:33,010 --> 00:32:36,518
understood the bounds and limitations by incorporating Azure Chaos Studio.

486
00:32:36,614 --> 00:32:40,534
I'll also talk about the specifics of our method for creating scenarios

487
00:32:40,582 --> 00:32:44,086
integrating with our CSCD pipelines and the situations

488
00:32:44,118 --> 00:32:47,914
we successfully prevented from affecting production. The expected

489
00:32:47,962 --> 00:32:51,434
outages of investing in chaos engineering are twofold.

490
00:32:51,562 --> 00:32:54,986
First, to enhance the customer experience by accelerating

491
00:32:55,018 --> 00:32:58,574
the delivery of top quality features. Second, to avoid production

492
00:32:58,622 --> 00:33:02,302
problems, thereby improving the customer satisfaction and mitigating

493
00:33:02,366 --> 00:33:06,398
engineering fighting, thus enabling more time for future development,

494
00:33:06,494 --> 00:33:10,414
which supports the accomplishments of the first outcome. Aligned with the strategy

495
00:33:10,462 --> 00:33:14,294
that Ravi mentioned, our strategy to achieve the outcome revolves around

496
00:33:14,332 --> 00:33:17,826
these five pillars. First, service level agreements establish

497
00:33:17,858 --> 00:33:21,174
the performance and reliability standards for a service. Rather than

498
00:33:21,212 --> 00:33:25,114
focusing solely on individual faults, our approach considers scenarios that

499
00:33:25,152 --> 00:33:28,922
impact our customer commitments and internal goals, such as service

500
00:33:28,976 --> 00:33:33,062
level objectives and service level indicators. This approach provides a comprehensive

501
00:33:33,126 --> 00:33:36,778
solution offering both recommending standard scenarios

502
00:33:36,874 --> 00:33:40,046
commonly shared across teams and unique scenarios specific to

503
00:33:40,068 --> 00:33:43,658
our service. Second, it is crucial to constant,

504
00:33:43,754 --> 00:33:47,518
continuously test chaos scenarios across the entire application

505
00:33:47,604 --> 00:33:51,362
stack, including compute, networking, storage, and auth,

506
00:33:51,496 --> 00:33:54,798
as real world failures can arise from within a single stack

507
00:33:54,894 --> 00:33:58,126
or as a cascading effect across multiple stacks.

508
00:33:58,238 --> 00:34:02,006
Testing only one stack at a time may not fully reflect the complexity and

509
00:34:02,028 --> 00:34:05,286
interdependency of the entire system, leaving it vulnerable to

510
00:34:05,308 --> 00:34:08,642
unforeseen disruptions. By testing the full application stack,

511
00:34:08,706 --> 00:34:12,898
we want to better prepare for the unpredictability. Having determined

512
00:34:12,914 --> 00:34:16,490
to adopt a top down approach and test both within

513
00:34:16,560 --> 00:34:20,202
and across the application stack, the next step is to map out specific

514
00:34:20,256 --> 00:34:24,182
nominal and disaster scenarios for our service. Before inducing failures

515
00:34:24,246 --> 00:34:27,690
into our system, we documented various service specific scenarios

516
00:34:27,770 --> 00:34:30,954
and their predicted outcomes, referred to as hypothesis,

517
00:34:31,082 --> 00:34:34,746
and then tested them using the Chaos studio simulation. This allowed

518
00:34:34,778 --> 00:34:38,266
us to concentrate our chaos engineering efforts and create targeted

519
00:34:38,298 --> 00:34:41,902
experiments. During this stage, we also had the opportunity to request

520
00:34:41,966 --> 00:34:45,540
desired features from the Azure Chaos studio and Azure low test.

521
00:34:46,950 --> 00:34:50,962
Next, we began conducting chaos scenarios and monitored the service behavior

522
00:34:51,106 --> 00:34:54,406
before, during, and after the experiments. This allowed us

523
00:34:54,428 --> 00:34:57,186
to experience the robust features of Azure Chaos Studio,

524
00:34:57,298 --> 00:35:01,234
including its ability to introduce faults, versatile experiment

525
00:35:01,282 --> 00:35:05,338
creation automation friendly design, a portal where we can review

526
00:35:05,424 --> 00:35:09,066
past runs, and most importantly, its ability to undo the

527
00:35:09,088 --> 00:35:12,726
simulation. Post experiment, we also provided feedback to Chaos

528
00:35:12,758 --> 00:35:16,154
Studio on how to enhance the customer onboarding experience and

529
00:35:16,192 --> 00:35:19,594
contributed for features that can help us monitor service health and

530
00:35:19,632 --> 00:35:23,326
automate the end to end process. We conducted a dry run of these

531
00:35:23,348 --> 00:35:26,634
scenarios to validate our hypothesis. After several

532
00:35:26,682 --> 00:35:30,862
dry runs, we compiled a list of validated scenarios or experiments.

533
00:35:31,006 --> 00:35:34,594
We incorporated these chaos engineering scenarios into

534
00:35:34,632 --> 00:35:38,654
our CI CD pipelines to automate the chaos engineering process and ensure

535
00:35:38,702 --> 00:35:42,066
that our systems are constantly tested and validated

536
00:35:42,178 --> 00:35:45,400
as the changes to our code cause are made.

537
00:35:46,570 --> 00:35:50,306
By following these steps, we were able to implement chaos engineering

538
00:35:50,338 --> 00:35:54,150
in a methodical and controlled way, enhancing the reliability and durability

539
00:35:54,230 --> 00:35:57,930
of our service and reducing the likelihood of failures in production.

540
00:36:03,900 --> 00:36:07,764
Here is a high level overview of our service architecture. Our service architecture

541
00:36:07,812 --> 00:36:11,988
consists of three major components, the data plane, control plane, and management plane.

542
00:36:12,084 --> 00:36:15,752
To meet our service needs, we utilize infrastructure as service virtual machines

543
00:36:15,816 --> 00:36:19,048
for the data plane and service fabric backed virtual machine skill

544
00:36:19,064 --> 00:36:23,152
sets. For the rest, we store our data using Azure Cosmos, DB and

545
00:36:23,206 --> 00:36:26,508
Azure storage accounts. Our customers access our endpoints

546
00:36:26,524 --> 00:36:30,236
through a load balancer which is managed by DNS. Our applications

547
00:36:30,268 --> 00:36:34,104
are secured within a virtual network within a network

548
00:36:34,172 --> 00:36:37,504
security group. Our authentication stack is based on Azure

549
00:36:37,552 --> 00:36:41,392
Active directory and we use an internal certificate authority PK

550
00:36:41,456 --> 00:36:45,780
service and a keyword equivalent for certificate and secret management.

551
00:36:47,400 --> 00:36:51,012
Next, let's go over the details of our service scenarios and automation

552
00:36:51,076 --> 00:36:56,706
we were able to achieve to

553
00:36:56,728 --> 00:37:00,494
meet our adoption needs, we have developed some nominal and disaster scenario

554
00:37:00,542 --> 00:37:04,706
standards. Although these standards are stack specific, we have tested combination

555
00:37:04,738 --> 00:37:08,278
of these stacks. For instance, we load each stack with both nominal and

556
00:37:08,284 --> 00:37:11,110
disaster scenarios. As without specific leads,

557
00:37:11,610 --> 00:37:15,094
the nontypical scenarios will not be triggered. The same

558
00:37:15,132 --> 00:37:19,034
applies to combining network and compute scenarios, et cetera. What we

559
00:37:19,072 --> 00:37:22,826
see here is a small slice of standards we were able to

560
00:37:22,848 --> 00:37:26,742
come up with. It is essential to test for resource exhaustion

561
00:37:26,806 --> 00:37:30,714
in our compute stack, whether caused by bugs of bugs

562
00:37:30,762 --> 00:37:34,522
or resource leaks. Our initial runs aim to establish

563
00:37:34,586 --> 00:37:38,778
the boundaries of our service, evaluate its ability to self recover using auto

564
00:37:38,794 --> 00:37:42,642
scaling and set benchmarks. This helps us ensure that any

565
00:37:42,696 --> 00:37:45,902
future changes to the service do not compromise our ability

566
00:37:45,966 --> 00:37:49,746
to handle resource exhaustion. We also wanted to determine if we

567
00:37:49,768 --> 00:37:53,186
are over provisioning and if reducing the capacity while increasing

568
00:37:53,218 --> 00:37:56,902
the load through a combination of load and fault injections could lead

569
00:37:56,956 --> 00:37:59,110
to sock lead to cost savings.

570
00:38:00,730 --> 00:38:04,194
In terms of compute disaster, we wanted to assess the preparedness

571
00:38:04,242 --> 00:38:07,798
of our service monitoring systems and recovery procedures

572
00:38:07,894 --> 00:38:11,334
in an event of a partial or full outage of availability zone,

573
00:38:11,382 --> 00:38:15,222
availability, set rack or data center. In regards to storage,

574
00:38:15,286 --> 00:38:18,638
we tested the overall service resiliency during a failover from the

575
00:38:18,644 --> 00:38:22,640
primary to secondary. We also evaluated network degradation between

576
00:38:23,090 --> 00:38:27,594
compute and storage, resulting in valuable insights further

577
00:38:27,642 --> 00:38:31,042
network we continuously validate important scenarios such as

578
00:38:31,096 --> 00:38:34,578
interdependency communication outages implemented using

579
00:38:34,664 --> 00:38:38,366
network security group faults. This helped us test cross

580
00:38:38,398 --> 00:38:41,918
stack resilience and identify any interesting design flaws and

581
00:38:41,944 --> 00:38:45,442
bugs. The real Chaos studio experimentation produced excellent

582
00:38:45,506 --> 00:38:49,090
results. In some cases, our hypothesis was validated,

583
00:38:49,170 --> 00:38:53,266
such as detecting a bug in preproduction by replicating 99% cpu

584
00:38:53,298 --> 00:38:56,954
pressure on our fleet. However, we also encountered an

585
00:38:56,992 --> 00:39:00,934
unexpected operation failures. Despite the expected increase in latency

586
00:39:00,982 --> 00:39:05,002
due to hardware limitations preventing auto scaling, we have since

587
00:39:05,056 --> 00:39:06,060
fixed the issue.

588
00:39:08,910 --> 00:39:12,526
Here are a few outcomes of the real Chaos studio experimentation where we

589
00:39:12,548 --> 00:39:15,978
were able to successfully validate the hypothesis in certain cases,

590
00:39:16,154 --> 00:39:19,546
whereas failed in others. For example, we were able to catch bug

591
00:39:19,578 --> 00:39:23,118
in preproduction by replicating 99% cpu pressure on our fleet.

592
00:39:23,214 --> 00:39:26,642
While the latency increase is expected as our service account

593
00:39:26,696 --> 00:39:30,734
auto scale due to certain hardware requirements, we have seen operation failures

594
00:39:30,862 --> 00:39:34,242
which is not expected here. As you can see, in the other scenario,

595
00:39:34,306 --> 00:39:37,714
the services crashed and failed to auto recover when we replicated

596
00:39:37,762 --> 00:39:41,878
a scenario of slow network packet transmission from computation to

597
00:39:41,964 --> 00:39:45,526
storage under heavy traffic, these type of bugs are difficult to

598
00:39:45,548 --> 00:39:49,414
detect unless a real world event occurs. With the help of Chaos studio,

599
00:39:49,462 --> 00:39:52,986
we were not only able to replicate the issue, but continuously validate it

600
00:39:53,008 --> 00:39:56,534
as part of our CI CD process. Our load testing,

601
00:39:56,662 --> 00:40:00,326
which we currently conduct using Azure load testing, helped prevent

602
00:40:00,358 --> 00:40:03,946
performance related bugs caused by heavy load from reaching

603
00:40:03,978 --> 00:40:07,066
production. We expect to achieve the same results with Azure

604
00:40:07,098 --> 00:40:10,306
Chaos Studio in the near future. As previously mentioned,

605
00:40:10,408 --> 00:40:14,414
it is essential to shift left in the development cycle and continuously measure

606
00:40:14,462 --> 00:40:18,420
the resiliency of the service by emulating manual touches to

607
00:40:19,190 --> 00:40:22,994
chaos experimentation. To accomplish this, we have developed

608
00:40:23,042 --> 00:40:26,374
and internally released an Azure DevOps extension for

609
00:40:26,412 --> 00:40:29,974
Azure Chaos Studio, which can be used in release pipelines that

610
00:40:30,012 --> 00:40:34,230
helps teams automate nominal and disaster scenarios in their pipelines.

611
00:40:35,150 --> 00:40:39,146
For our service, we benchmark official builds in pre production by running

612
00:40:39,248 --> 00:40:42,806
load and performance and chaos engineering nominal

613
00:40:42,838 --> 00:40:46,502
scenarios before allowing the build to reach production. We also

614
00:40:46,576 --> 00:40:50,094
run automatic bi weekly disaster scenarios that gives

615
00:40:50,132 --> 00:40:52,350
us confidence on our service resiliency.

616
00:40:53,090 --> 00:40:57,040
In conclusion, by utilizing Azure Chaos Studio and incorporating technology

617
00:40:57,490 --> 00:41:01,342
and processes with standards and nominal disaster scenarios,

618
00:41:01,406 --> 00:41:04,658
we have initiated a journey of continuous verification of our services

619
00:41:04,744 --> 00:41:08,878
secrets and reliability resiliency. This approach

620
00:41:08,974 --> 00:41:12,614
has already began to yield benefits, enhancing the customer

621
00:41:12,732 --> 00:41:16,786
and engineering experience in today's dynamic

622
00:41:16,818 --> 00:41:20,102
and complex technology landscape. It's more important than ever

623
00:41:20,156 --> 00:41:23,874
to ensure that our services are secure, reliable and resilient.

624
00:41:24,002 --> 00:41:27,274
This is where chaos engineering and Azure Chaos Studio comes in,

625
00:41:27,392 --> 00:41:30,986
providing a proactive approach to identify and mitigate potential issues

626
00:41:31,088 --> 00:41:33,430
before they impact real world scenarios.

627
00:41:33,590 --> 00:41:37,246
Automation using CI CD helps to streamline and speed up the

628
00:41:37,268 --> 00:41:41,150
process, allowing for quicker and more frequent testing and deployment.

629
00:41:41,570 --> 00:41:45,066
By standardizing nominal and disaster scenarios and scaling

630
00:41:45,098 --> 00:41:48,206
adoption within large organizations, we're able to

631
00:41:48,228 --> 00:41:51,826
create a consistent and systemic approach to

632
00:41:51,848 --> 00:41:55,954
continuous verification. This not only benefits our customers by

633
00:41:55,992 --> 00:41:59,810
providing a more stable and dependable service, resulting in better customer

634
00:41:59,880 --> 00:42:03,622
experience, but also our engineers by reducing fatigue and

635
00:42:03,676 --> 00:42:06,470
allowing them to focus on innovation and growth.

636
00:42:06,970 --> 00:42:09,874
Overall. Our presentation on Chaos engineering,

637
00:42:09,922 --> 00:42:14,114
Azure Chaos Studio, and the scaling adoption with large organizations

638
00:42:14,242 --> 00:42:17,526
standardizing nominal and disaster scenarios is a

639
00:42:17,548 --> 00:42:21,190
game changer in the world of technology, and we believe it will help

640
00:42:21,260 --> 00:42:24,326
take your organization to the new heights of success. Thank you

641
00:42:24,348 --> 00:42:28,234
for watching. We hope you have a success. Full journey in implementing kiosk engineering

642
00:42:28,282 --> 00:42:29,610
in a controlled and secure manner.

