1
00:00:36,610 --> 00:00:40,226
Machine learning in production this session

2
00:00:40,258 --> 00:00:44,294
is an introduction to running machine learning in production, which is

3
00:00:44,332 --> 00:00:48,086
being called MlOps. I'm Ryan Dawson and I'm an

4
00:00:48,108 --> 00:00:51,150
engineer working on Mlops solutions at Seldon.

5
00:00:51,970 --> 00:00:54,958
The MLops scene is complex and new.

6
00:00:55,124 --> 00:00:58,394
It's distinct from mainstream DevOps. So we'll start by comparing

7
00:00:58,442 --> 00:01:02,030
mlops to DevOps. To understand why it's so different,

8
00:01:02,100 --> 00:01:05,330
we need to understand how data science is different from programming.

9
00:01:05,910 --> 00:01:09,380
We'll find out that the difference centers on how data is used.

10
00:01:09,830 --> 00:01:13,650
When we're clear about that difference, then we'll look at how the build

11
00:01:13,720 --> 00:01:17,830
deploy monitor workflows for DevOps differ from Mlops.

12
00:01:18,970 --> 00:01:23,042
From there, we'll be able to go deeper on particular steps in the mlops

13
00:01:23,106 --> 00:01:26,454
build deploy monitor workflow. I'll try to explain

14
00:01:26,492 --> 00:01:30,438
that MlOps challenges vary by use case, and that some use cases

15
00:01:30,534 --> 00:01:34,278
have especially advanced challenges. Lastly,

16
00:01:34,374 --> 00:01:37,578
I'll go into some of the advanced challenges and how they relate to the

17
00:01:37,584 --> 00:01:39,820
topic of governance for running machine learning.

18
00:01:42,290 --> 00:01:46,830
So before we try to understand mlops, let's make sure we're clear about DevOps.

19
00:01:47,330 --> 00:01:51,306
As I see it, DevOps is all about making the build deploy monitor

20
00:01:51,338 --> 00:01:55,106
workflow for applications as smooth as possible. It tends to

21
00:01:55,128 --> 00:01:59,918
focus on CI, CD and infrastructure SRE

22
00:02:00,014 --> 00:02:04,350
or site reliability engineering. As I see, it is an overlapping role,

23
00:02:04,510 --> 00:02:08,550
but with a bit more focus on the monitoring stage of the workflows.

24
00:02:10,250 --> 00:02:13,670
This whole workflows is a key enabler for software projects.

25
00:02:14,250 --> 00:02:17,394
Fortunately, there's some great tools in the space that have become pretty well

26
00:02:17,452 --> 00:02:20,630
established across the industry, tools like git, Jenkins,

27
00:02:20,710 --> 00:02:23,770
Docker, Ansible, Prometheus, et cetera.

28
00:02:26,190 --> 00:02:29,020
MLOPS is in a very different space right now.

29
00:02:29,550 --> 00:02:32,794
There's surveys suggesting that 80% to 90% of machine learning

30
00:02:32,832 --> 00:02:36,126
models never make it to live, and at least part of that is

31
00:02:36,148 --> 00:02:38,830
due to the complexity of running machine learning in production.

32
00:02:39,570 --> 00:02:43,022
There's a famous paper called hidden technical debt in machine learning

33
00:02:43,076 --> 00:02:47,042
systems, and it explains about all the effort that goes into

34
00:02:47,096 --> 00:02:50,546
running production grade machine training systems. It has

35
00:02:50,568 --> 00:02:54,062
a diagram with boxes showing the relative size of different tasks,

36
00:02:54,206 --> 00:02:57,414
and there's this tiny little box for ML code and

37
00:02:57,452 --> 00:03:00,994
really big boxes for data collection, data processing,

38
00:03:01,122 --> 00:03:03,990
runtime, infrastructure monitoring.

39
00:03:06,650 --> 00:03:10,586
The Linux foundation for AI have tried to help by producing a diagram of

40
00:03:10,608 --> 00:03:14,106
the whole mlops tool landscape. It's great, but it

41
00:03:14,208 --> 00:03:17,594
has loads of tools in loads of sections, and even

42
00:03:17,632 --> 00:03:21,070
the section titles won't make much sense for newcomers to mlops.

43
00:03:21,730 --> 00:03:25,662
But let's try to understand more about the fundamentals of Mlops and where it's coming

44
00:03:25,716 --> 00:03:26,320
from.

45
00:03:28,770 --> 00:03:32,366
Fundamentally, MLops is different from DevOps because machine learning is

46
00:03:32,388 --> 00:03:36,206
different from programming. Traditional programming codifies rules

47
00:03:36,238 --> 00:03:39,810
explicitly, rules that say how to respond to inputs.

48
00:03:40,470 --> 00:03:43,842
Machine learning does not codify explicitly. Instead,

49
00:03:43,976 --> 00:03:47,942
rules are set indirectly by capturing patterns from data and

50
00:03:47,996 --> 00:03:50,920
reapplying the extracted patterns to new input data.

51
00:03:52,410 --> 00:03:56,054
This makes machine learning more applicable to problems that center on data,

52
00:03:56,172 --> 00:03:58,380
especially focused numerical problems.

53
00:04:00,750 --> 00:04:03,894
So with traditional programming, we've got applications that respond

54
00:04:03,942 --> 00:04:07,574
directly to user inputs, such as terminal systems or GUI

55
00:04:07,622 --> 00:04:11,102
based systems. You code these by starting with hello

56
00:04:11,156 --> 00:04:13,310
world and adding more control structures.

57
00:04:14,290 --> 00:04:17,486
Data science problems fall into classification problems.

58
00:04:17,668 --> 00:04:21,258
Regression problems classification

59
00:04:21,354 --> 00:04:25,234
problems put data into categories. An example would be,

60
00:04:25,352 --> 00:04:27,860
is this image a cat or not a cat?

61
00:04:29,430 --> 00:04:32,674
Regression problems look for numerical output, for example,

62
00:04:32,792 --> 00:04:35,918
predicting sales revenue from how advertising spend is

63
00:04:35,944 --> 00:04:39,186
directed. The hello world of data science

64
00:04:39,218 --> 00:04:42,454
is the mnist dataset, which is a data set

65
00:04:42,492 --> 00:04:45,670
of handwritten digits. And the problem is to categorize each

66
00:04:45,740 --> 00:04:49,050
handwritten sample correctly as the number that it represents.

67
00:04:51,790 --> 00:04:55,722
When I think of machine learning as capturing patterns from data,

68
00:04:55,856 --> 00:04:59,290
I think about fitting for regression problems basically

69
00:04:59,360 --> 00:05:03,098
have data points on a graph, and you draw a line through the

70
00:05:03,104 --> 00:05:06,478
data points and try to get the line as close to as many of the

71
00:05:06,484 --> 00:05:10,062
data points as possible. The distance from each data

72
00:05:10,116 --> 00:05:13,806
point to the line is called the error, and you keep adjusting the

73
00:05:13,828 --> 00:05:16,530
equation of the line to minimize the total error.

74
00:05:17,670 --> 00:05:21,618
The coefficients of the equation of the line correspond to the weights of a

75
00:05:21,624 --> 00:05:25,570
machine learning model, and you then use that to make new predictions.

76
00:05:28,510 --> 00:05:32,506
Of course, the machine learning training process is more complex than the way I'm explaining

77
00:05:32,538 --> 00:05:36,138
it. For example, there's more to the process of adjusting

78
00:05:36,154 --> 00:05:40,260
the weights than to try to get the line to fit the data.

79
00:05:40,710 --> 00:05:43,858
It's done promogrammatically by using an algorithm called

80
00:05:43,944 --> 00:05:47,634
gradient descent. Essentially randomly pick

81
00:05:47,672 --> 00:05:51,186
a way to shift the line, but it's only pseudoram, as it will

82
00:05:51,208 --> 00:05:54,470
take a step in a given direction and then check whether that

83
00:05:54,540 --> 00:05:57,766
reduced the error before deciding whether to keep going that way or go

84
00:05:57,788 --> 00:06:01,494
a different direction. That step size

85
00:06:01,692 --> 00:06:04,974
can be tweaked and you can get different results,

86
00:06:05,042 --> 00:06:07,290
so the overall process is tunable.

87
00:06:10,510 --> 00:06:14,522
So basically, data scientists are looking for patterns in data and trying to find

88
00:06:14,576 --> 00:06:17,930
which methods are best for capturing those patterns in models.

89
00:06:18,530 --> 00:06:21,994
This is an exploratory process, and the tools data scientists

90
00:06:22,042 --> 00:06:26,206
use reflect this. Jupyter notebooks, for example, are great for

91
00:06:26,228 --> 00:06:29,390
playing around with slices of data and visualizing patterns.

92
00:06:32,180 --> 00:06:35,636
These differences between programming and machine learning have implications for

93
00:06:35,658 --> 00:06:39,460
how we can best build, deploying, and run machine learning

94
00:06:39,530 --> 00:06:43,092
systems. So let's get into more detail about how different

95
00:06:43,146 --> 00:06:45,530
these build deploy monitoring journeys are.

96
00:06:48,330 --> 00:06:51,734
Let's go on an imaginary development journey. We can

97
00:06:51,772 --> 00:06:55,110
start with a user story. Let's say we're building

98
00:06:55,180 --> 00:06:59,146
a calculator and our user story says that our lazy users want to

99
00:06:59,168 --> 00:07:02,586
put numerical operations into a screen so they don't have

100
00:07:02,608 --> 00:07:04,090
to work out the answers.

101
00:07:06,860 --> 00:07:10,090
We could write a Java program to satisfy the story,

102
00:07:11,040 --> 00:07:14,556
compile it, and distribute it as a

103
00:07:14,578 --> 00:07:18,284
binary. But this

104
00:07:18,322 --> 00:07:21,964
is 2020, so we'll more likely package the code to

105
00:07:22,002 --> 00:07:25,516
run as a web server so that users will interact with it via

106
00:07:25,548 --> 00:07:29,084
a browser. Most likely we'll also dockerize

107
00:07:29,132 --> 00:07:31,920
the web app and run it on some cloud infrastructure.

108
00:07:35,850 --> 00:07:38,790
Now let's think of a machine learning build journey.

109
00:07:39,610 --> 00:07:43,260
This is more likely to start with some data and maybe a question.

110
00:07:43,870 --> 00:07:47,274
Let's say we've got data on employees and their experience and

111
00:07:47,312 --> 00:07:51,066
skills and salaries, and we want to see whether we

112
00:07:51,088 --> 00:07:55,034
could clean whether we could use it to benchmark salaries

113
00:07:55,082 --> 00:07:57,760
for other employees during a pay review.

114
00:07:58,850 --> 00:08:02,686
Let's assume the data is already available and clean, though this

115
00:08:02,708 --> 00:08:05,902
is a pretty big assumption. But let's assume

116
00:08:05,966 --> 00:08:09,902
we've got good data and we can create a regression models that maps employee

117
00:08:09,966 --> 00:08:11,300
experience to pay,

118
00:08:13,350 --> 00:08:15,330
maybe using scikitlearn.

119
00:08:16,650 --> 00:08:19,846
So we train the model and then

120
00:08:19,868 --> 00:08:23,526
it can be used to make a production for any given

121
00:08:23,628 --> 00:08:27,446
employee a

122
00:08:27,468 --> 00:08:30,120
prediction about what the salary benchmark would be.

123
00:08:30,830 --> 00:08:34,966
So let's say we give our predictions for a particular set of employees

124
00:08:34,998 --> 00:08:37,340
to the business and they're happy with that.

125
00:08:38,510 --> 00:08:42,046
So happy that they want to use it again next year or more

126
00:08:42,068 --> 00:08:45,326
regularly. Then our situation changes.

127
00:08:45,508 --> 00:08:49,486
Because then what we want isn't just a prediction but

128
00:08:49,508 --> 00:08:54,504
a predict function as

129
00:08:54,542 --> 00:08:57,836
we might not want to have to rerun the training process every time the

130
00:08:57,858 --> 00:09:01,230
business has some new employees to check.

131
00:09:02,640 --> 00:09:06,396
This problem would be magnified if another department says that they want to

132
00:09:06,418 --> 00:09:10,732
make predictions too. Actually, that would add extra complication

133
00:09:10,876 --> 00:09:14,096
as even if we know that the patterns from our

134
00:09:14,118 --> 00:09:17,852
training data are applicable to our department, we don't necessarily

135
00:09:17,916 --> 00:09:21,664
know about the new department. But let's assume

136
00:09:21,712 --> 00:09:25,284
that it is applicable. Then our main problem is a problem

137
00:09:25,322 --> 00:09:28,784
of scaling. How do we make all these predictions

138
00:09:28,832 --> 00:09:31,350
without burning ourselves out?

139
00:09:32,040 --> 00:09:35,464
Probably we're going to be interested in using the machine learning model in

140
00:09:35,502 --> 00:09:36,490
a web app.

141
00:09:38,540 --> 00:09:42,296
So maybe we add a rest API around our python code and

142
00:09:42,318 --> 00:09:45,796
look to run it as a web application. We might naturally package

143
00:09:45,828 --> 00:09:49,870
it in a docker container like we would for a traditional web app.

144
00:09:50,640 --> 00:09:53,784
This is a valid and common approach, but it's just one approach

145
00:09:53,832 --> 00:09:57,704
with machine learning deploying it does present a challenge

146
00:09:57,752 --> 00:10:01,996
about how to dockerize the predict function without including the training data in the docker

147
00:10:02,028 --> 00:10:05,456
image. So it's also common to

148
00:10:05,478 --> 00:10:08,752
separate the model from the data by taking

149
00:10:08,806 --> 00:10:12,292
the Python variable for the model production and

150
00:10:12,346 --> 00:10:15,540
serializing that to a file using Python pickling.

151
00:10:16,600 --> 00:10:20,740
Then the file can be loaded into another training Python application server.

152
00:10:21,720 --> 00:10:24,956
So if we load the model into a suitable Python

153
00:10:25,088 --> 00:10:28,890
web server app, then we can serve predictions that way.

154
00:10:29,900 --> 00:10:33,556
This varies a little from framework to framework, and can vary

155
00:10:33,588 --> 00:10:35,820
quite a lot if the language is not Python.

156
00:10:37,280 --> 00:10:41,160
But basically this is a good picture for the machine learning lifecycle.

157
00:10:41,320 --> 00:10:45,404
Get data, clean it, experiment with it, train a model,

158
00:10:45,522 --> 00:10:48,480
package the model into something that can serve predictions.

159
00:10:50,340 --> 00:10:53,760
And there are tools pitched at each stage of this lifecycle

160
00:10:54,420 --> 00:10:58,432
for data storage and prep. There's tools like s three and

161
00:10:58,486 --> 00:11:02,348
Hadoop training can use

162
00:11:02,374 --> 00:11:05,604
a lot of compute resource and take a long time.

163
00:11:05,642 --> 00:11:08,992
So there's tools that help with running long running training jobs,

164
00:11:09,136 --> 00:11:12,810
and also tools for training the operations performed during training.

165
00:11:15,100 --> 00:11:18,804
There are tools specifically aimed at helping make batch

166
00:11:18,852 --> 00:11:21,000
predictions on a regular cycle,

167
00:11:23,500 --> 00:11:26,904
say for just getting predictions every month or whatever the cycle

168
00:11:26,952 --> 00:11:28,590
is that the business works to.

169
00:11:29,760 --> 00:11:33,148
Or predictions could be needed at any time.

170
00:11:33,314 --> 00:11:36,988
And then there's tools for real time serving of

171
00:11:37,074 --> 00:11:40,560
predictions using a rest API.

172
00:11:40,980 --> 00:11:44,220
Some real time serving tools are specific to the framework

173
00:11:44,300 --> 00:11:47,996
and some SRe more general. I personally work on seldon

174
00:11:48,028 --> 00:11:51,536
core, which is a framework agnostic open source serving

175
00:11:51,568 --> 00:11:55,332
tool. The seldon team also collaborates on another tool

176
00:11:55,386 --> 00:11:58,724
called KF serving. Both of these are part

177
00:11:58,762 --> 00:12:02,244
of the Kubeflow ecosystem, which is an end to end

178
00:12:02,282 --> 00:12:05,680
platform. That's another space of tools,

179
00:12:05,760 --> 00:12:09,080
end to end platforms that try to join up the whole journey.

180
00:12:09,660 --> 00:12:13,512
Platforms can save you the effort of stitching together several different

181
00:12:13,566 --> 00:12:17,352
tools, but platforms are also opinionated

182
00:12:17,496 --> 00:12:20,764
can be, so they don't necessarily fit every use

183
00:12:20,802 --> 00:12:24,396
case. I'm listing these types of tools because I think

184
00:12:24,418 --> 00:12:27,884
it helps to divide the machine training lifecycle up like this into

185
00:12:27,922 --> 00:12:30,240
data prep, training and serving.

186
00:12:30,980 --> 00:12:34,316
This helps us make sense of the concept landscape of mlops

187
00:12:34,348 --> 00:12:38,208
tools out there, as we can then put them into categories mapped to the

188
00:12:38,214 --> 00:12:41,588
lifecycle. There's also the monitoring part of

189
00:12:41,594 --> 00:12:44,310
the lifecycle, but we'll get to that later.

190
00:12:46,920 --> 00:12:50,528
For now, the key point to see is that mlops is different from DevOps,

191
00:12:50,624 --> 00:12:54,280
mostly because of the role of data. In particular,

192
00:12:54,430 --> 00:12:58,024
models are built by extracting patterns from data using

193
00:12:58,222 --> 00:13:01,850
code, so that the training data is

194
00:13:02,460 --> 00:13:06,060
a key part of the model. The training

195
00:13:06,130 --> 00:13:09,340
data volumes can be large,

196
00:13:09,490 --> 00:13:12,910
and that leads to complexity in storing and processing the data,

197
00:13:13,360 --> 00:13:16,030
which there's specialized tools to help with.

198
00:13:18,880 --> 00:13:22,492
You also get different toolkits for building machine learning models,

199
00:13:22,636 --> 00:13:26,076
which results in models for different formats and adds

200
00:13:26,108 --> 00:13:29,928
some complexity to the space of tools for getting predictions out of models,

201
00:13:30,044 --> 00:13:33,508
space called serving. So the complexity of the

202
00:13:33,514 --> 00:13:36,576
way the ML build deploying monitor lifecycle uses

203
00:13:36,608 --> 00:13:40,100
data has knock on effects to the tool landscape.

204
00:13:40,920 --> 00:13:43,776
We've not talked about the post deployment stage yet,

205
00:13:43,898 --> 00:13:47,160
but there's also complexity there. For example,

206
00:13:47,310 --> 00:13:50,504
you can sometimes need to retrain your model,

207
00:13:50,622 --> 00:13:54,392
your running model, not because of any bugs in it, but because the data

208
00:13:54,446 --> 00:13:56,670
coming in from the outside world changes.

209
00:13:58,080 --> 00:14:01,096
Think, for example, of how fashion is seasonal.

210
00:14:01,288 --> 00:14:05,096
Let's say you've got a model trained to recommend clothes for an online fashion

211
00:14:05,128 --> 00:14:08,860
store, and you trained it based on purchases made in winter.

212
00:14:10,080 --> 00:14:13,328
Then it might perform great in winter and make lots of money.

213
00:14:13,494 --> 00:14:17,344
But when it comes to summer, it's still going to be recommending coats when

214
00:14:17,382 --> 00:14:19,120
people are looking for summer clothes.

215
00:14:20,500 --> 00:14:24,292
So you would need to be regularly updating the model with new data and

216
00:14:24,346 --> 00:14:28,388
ideally checking that it's leading to sales. That's a

217
00:14:28,394 --> 00:14:31,328
complex you don't normally get with traditional software.

218
00:14:31,504 --> 00:14:35,012
These complexities about handling data, they ripple

219
00:14:35,076 --> 00:14:37,800
all the way through the whole mlops lifecycle.

220
00:14:38,940 --> 00:14:42,456
We've talked about this at a high level so far, but let's now think

221
00:14:42,478 --> 00:14:46,190
about the individual steps of the workflow and the tools used in them.

222
00:14:50,120 --> 00:14:53,744
So let's just remind ourselves of the workflows steps with traditional

223
00:14:53,792 --> 00:14:57,188
DevOps. We'll start with the user story

224
00:14:57,354 --> 00:15:00,872
specifying a business need. From that a developer will write

225
00:15:00,926 --> 00:15:04,824
code and submit a pull request. Hopefully test will

226
00:15:04,862 --> 00:15:07,992
run automatically on the pull request. Somebody will

227
00:15:08,046 --> 00:15:11,912
review it and merge. It gets to merged to master there,

228
00:15:11,966 --> 00:15:15,564
our pipeline will build a new version of the app and deploy that to

229
00:15:15,602 --> 00:15:19,736
the test environment. Perhaps further tests

230
00:15:19,768 --> 00:15:23,756
will be run and it'll get promoted to the next environment where there

231
00:15:23,778 --> 00:15:27,024
might be more deeper tests, and then it'll go to

232
00:15:27,062 --> 00:15:30,704
production. And in production we'll monitor for anything going

233
00:15:30,742 --> 00:15:34,240
wrong, probably in the form of stack traces or error codes.

234
00:15:35,940 --> 00:15:39,236
The pipeline producing these builds and running the tests will most likely be

235
00:15:39,258 --> 00:15:42,864
a CI system like Jenkins. The driver for the pipeline

236
00:15:42,912 --> 00:15:46,704
will most likely be a code change in git. The artifact

237
00:15:46,752 --> 00:15:50,488
we'll be promoting will probably be an executable inside a

238
00:15:50,494 --> 00:15:51,560
docker image.

239
00:15:54,780 --> 00:15:58,280
ML workflows are different. The driver for

240
00:15:58,350 --> 00:16:01,864
automation might be a code change, or it might be new data,

241
00:16:02,062 --> 00:16:05,756
and the data probably won't be in git as git isn't a great store for

242
00:16:05,778 --> 00:16:08,140
data getting into the gigabytes,

243
00:16:09,600 --> 00:16:12,540
the workflows are more experimental and data driven.

244
00:16:12,960 --> 00:16:16,236
You start with a data set and need to experiment to

245
00:16:16,258 --> 00:16:19,376
find usable patterns in the data set that can

246
00:16:19,398 --> 00:16:22,928
be captured in a model. When you've got a model, then it

247
00:16:22,934 --> 00:16:26,144
might not be enough to just check it for past fail conditions and monitor for

248
00:16:26,182 --> 00:16:29,536
errors like you would. Traditional software likely have

249
00:16:29,558 --> 00:16:32,710
to check how well it performs against the data. In numerical terms,

250
00:16:33,240 --> 00:16:36,180
there can be quite a bit of variation with ML workflows.

251
00:16:36,600 --> 00:16:40,344
One major point of variation is whether the model is trained offline or

252
00:16:40,382 --> 00:16:43,210
online. With online learning,

253
00:16:43,740 --> 00:16:47,464
a model is constantly being updated by adjusting itself through each

254
00:16:47,502 --> 00:16:51,508
new data point that it sees. So every prediction it makes also adjusts

255
00:16:51,524 --> 00:16:54,956
the model. Whereas with offline learning, the training is

256
00:16:54,978 --> 00:16:58,376
done separately from prediction. You train the model and deploying

257
00:16:58,408 --> 00:17:01,084
it, and when you want to update the model, you need to train a new

258
00:17:01,122 --> 00:17:04,816
one. We have to pick somewhere to

259
00:17:04,838 --> 00:17:08,396
focus, and offline learning is probably the more common case. So let's

260
00:17:08,428 --> 00:17:10,960
focus on offline training workflows.

261
00:17:13,540 --> 00:17:17,264
As we've talked about already, an ML workflow starts with data.

262
00:17:17,462 --> 00:17:21,300
It can be very large and typically needs to be cleaned and processed.

263
00:17:21,640 --> 00:17:25,188
A slice of that data can be taken so that the data scientists can

264
00:17:25,194 --> 00:17:29,140
work with it locally to explore the data on their own machine.

265
00:17:30,520 --> 00:17:34,584
When the data scientist has started to make some progress, then they might

266
00:17:34,622 --> 00:17:38,964
move to a hosted training environment to run some longer running experiments

267
00:17:39,012 --> 00:17:42,556
on a larger sample of the data. There will

268
00:17:42,578 --> 00:17:45,964
likely be collaboration with other data scientists, most likely using

269
00:17:46,002 --> 00:17:49,724
Jupyter notebooks. The artifact produced will

270
00:17:49,762 --> 00:17:53,544
be a model, commonly a model that's pickled

271
00:17:53,592 --> 00:17:57,008
or serialized to a file. That model can be

272
00:17:57,014 --> 00:18:01,840
integrated into a running app to serve real time production through HTTP.

273
00:18:03,380 --> 00:18:07,056
There will probably be a consumer of those predictions, which may be

274
00:18:07,078 --> 00:18:11,104
another app, perhaps a traditional web app. So you may need to integration

275
00:18:11,152 --> 00:18:13,620
test the SRE model against the consumer.

276
00:18:14,760 --> 00:18:18,196
And when you roll out the model to a production, you want to

277
00:18:18,218 --> 00:18:22,452
monitor the model by picking some metrics that represent how well performing

278
00:18:22,516 --> 00:18:23,930
against the live data,

279
00:18:27,260 --> 00:18:31,076
the rollout and monitoring phases of the workflow can be linked.

280
00:18:31,268 --> 00:18:35,108
An example might help to understand this. Say we've

281
00:18:35,124 --> 00:18:39,116
got an online store with ecommerce. A common way to

282
00:18:39,138 --> 00:18:42,670
roll out new versions of a model is an A B test.

283
00:18:43,360 --> 00:18:46,700
With an A B test, you'd have a live version that's already

284
00:18:46,770 --> 00:18:50,464
running and that's called the control. And then you run

285
00:18:50,502 --> 00:18:54,450
other versions alongside it. Let's call them version a and version b.

286
00:18:55,060 --> 00:18:58,848
So we're running three versions of the model in parallel, each training a

287
00:18:58,854 --> 00:19:01,620
bit differently to see which gives the best results.

288
00:19:02,680 --> 00:19:06,192
You can do that by splitting the traffic between the versions

289
00:19:06,256 --> 00:19:10,100
to minimize the risk. We'd send most of the traffic to the control version.

290
00:19:11,560 --> 00:19:15,796
A subset of the traffic will go to a and to b, and we'll

291
00:19:15,828 --> 00:19:20,084
run that splitting process for a while until we've got a statistically significant

292
00:19:20,132 --> 00:19:23,844
sample. Let's say that variation a has the highest

293
00:19:23,892 --> 00:19:27,352
conversion rate, so a higher proportion of the recommendations

294
00:19:27,416 --> 00:19:31,036
lead to sales. That's a useful metric, and it

295
00:19:31,058 --> 00:19:34,684
might be enough for us to choose variation a, but it

296
00:19:34,722 --> 00:19:38,144
might not be the only metric. These situations can get

297
00:19:38,182 --> 00:19:41,824
complex. For example, it might be that model a

298
00:19:41,862 --> 00:19:45,824
is recommending controversial products. So some customers might really

299
00:19:45,862 --> 00:19:49,456
like the recommendations and buy the products, but other customers are

300
00:19:49,478 --> 00:19:51,990
really put off and they just go to a different website.

301
00:19:54,040 --> 00:19:57,824
So there are trade offs to consider, and monitoring

302
00:19:57,872 --> 00:20:00,790
can need more than one metric depending on the use case.

303
00:20:03,760 --> 00:20:07,836
So we're seeing that MLOPs is complex, and in many organizations

304
00:20:07,868 --> 00:20:11,152
right now, the complexity is enhanced by challenges from

305
00:20:11,206 --> 00:20:15,004
organizational silos. You can find data scientists

306
00:20:15,052 --> 00:20:19,024
that work just in a world of Jupyter notebooks and model accuracy

307
00:20:19,072 --> 00:20:22,516
on training data that then gets handed over to

308
00:20:22,538 --> 00:20:25,776
a traditional DevOps team with the expectation they'll

309
00:20:25,808 --> 00:20:28,810
be able to take this work and build it into a production system.

310
00:20:29,660 --> 00:20:32,788
Without proper context. The traditional DevOps team is likely

311
00:20:32,804 --> 00:20:36,890
to look at those notebooks and just react like, what is this stuff?

312
00:20:40,590 --> 00:20:44,134
In a more mature setup, you might have better understood handoffs.

313
00:20:44,262 --> 00:20:47,486
For example, you might have data engineers who deal with

314
00:20:47,508 --> 00:20:51,390
obtaining the data and getting it into the right state for the data scientists.

315
00:20:51,890 --> 00:20:55,806
Once the data is ready for the data scientists, then they can take over and

316
00:20:55,828 --> 00:20:59,106
build the models. And from there, data science will have an

317
00:20:59,128 --> 00:21:03,374
understood handoff to ML engineers. And the ML engineers

318
00:21:03,422 --> 00:21:06,814
might still be a DevOps team, but a DevOps team that knows about the context

319
00:21:06,862 --> 00:21:10,198
of this particular machine learning application and knows how to run it in

320
00:21:10,204 --> 00:21:11,110
the production.

321
00:21:14,290 --> 00:21:17,994
This is new territory. There are special challenges for mlops

322
00:21:18,042 --> 00:21:21,040
that are not a normal part of DevOps, at least not right now.

323
00:21:22,290 --> 00:21:25,474
Now that we've got a high level understanding of where MLOPs is coming

324
00:21:25,512 --> 00:21:29,490
from, we can next go into more detail on particular MLOps topics.

325
00:21:29,910 --> 00:21:32,638
So let's take these in order and go first into training,

326
00:21:32,744 --> 00:21:35,750
then serving, finally rollout and monitoring.

327
00:21:38,090 --> 00:21:41,254
So there's tools that are pitched, particularly at the training space,

328
00:21:41,452 --> 00:21:44,386
to name a few examples. There's Kubeflow pipelines,

329
00:21:44,498 --> 00:21:46,570
MLflow Polyaxon.

330
00:21:47,550 --> 00:21:50,726
These are all about making it easy to run long running training jobs

331
00:21:50,758 --> 00:21:55,030
on a hosted environment. Typically, that means providing

332
00:21:55,190 --> 00:21:58,366
some manifest that specifies which steps sre to be done and

333
00:21:58,388 --> 00:22:01,982
in which order. That's a manifest for a training

334
00:22:02,036 --> 00:22:06,046
pipeline. As an example,

335
00:22:06,228 --> 00:22:09,854
a pipeline might have as its step an action to

336
00:22:09,892 --> 00:22:14,202
download data from wherever it's stored. That could be the first step.

337
00:22:14,356 --> 00:22:17,380
Then it gets split into training and validation data.

338
00:22:17,910 --> 00:22:21,374
The training data will then be used to train the model, and the validation

339
00:22:21,422 --> 00:22:24,630
data will be used as a check on the quality of the model's predictions.

340
00:22:26,330 --> 00:22:30,146
When we check the quality of the predictions, we'll want to record those checks

341
00:22:30,178 --> 00:22:33,350
somewhere and ideally also have an automated way

342
00:22:33,420 --> 00:22:37,080
to decide whether we should consider this as a good model or not.

343
00:22:37,710 --> 00:22:41,498
If we do consider it a good model, then we'll probably want to serialize it

344
00:22:41,584 --> 00:22:44,746
so that the serialized model would be available for promotion to

345
00:22:44,768 --> 00:22:47,946
a running environment. This is

346
00:22:47,968 --> 00:22:51,454
probably sounding rather like continuous integration pipelines. It is

347
00:22:51,492 --> 00:22:55,274
similar, but also different. The difference can be seen in the specialized

348
00:22:55,322 --> 00:22:59,054
tools dedicated to training. One tool

349
00:22:59,092 --> 00:23:01,390
for handling training is Kubeflow pipelines.

350
00:23:02,070 --> 00:23:06,190
In Kubeflow pipelines, you can define your pipeline with all its steps,

351
00:23:06,270 --> 00:23:10,290
and also visualize it and watch it progress and see any steps that fail.

352
00:23:10,630 --> 00:23:13,922
But the pipelines aren't only called pipelines,

353
00:23:14,066 --> 00:23:17,190
they're also called experiments, and they're parameterized,

354
00:23:17,930 --> 00:23:21,270
so there's options in its UI where you can enter parameters.

355
00:23:21,690 --> 00:23:25,014
Remember I mentioned before that the process can be

356
00:23:25,052 --> 00:23:28,986
tunable. There are tunable parameters on training, such as

357
00:23:29,008 --> 00:23:32,486
the step size, so you can kick off runs

358
00:23:32,518 --> 00:23:36,138
in parallel of the same pipeline using different parameters to

359
00:23:36,144 --> 00:23:38,540
see which parameters might result in the best model.

360
00:23:40,750 --> 00:23:44,186
Cube flow pipelines is not alone in having this idea of being able to kick

361
00:23:44,218 --> 00:23:46,590
off runs of an experiment with different parameters.

362
00:23:47,010 --> 00:23:50,506
MLflow, for example, uses the same terminology and has

363
00:23:50,548 --> 00:23:54,526
a similar interface. So there's

364
00:23:54,558 --> 00:23:57,886
similarity here with traditional CI systems, as the training platforms

365
00:23:57,918 --> 00:24:01,140
execute a series of steps and an artifact gets built.

366
00:24:01,510 --> 00:24:05,366
But it's different, as you've also got this idea of running experiments with

367
00:24:05,388 --> 00:24:07,640
different parameters to see which is best.

368
00:24:08,570 --> 00:24:12,086
That means you have to have a definition of which is good,

369
00:24:12,188 --> 00:24:13,560
which is the best model,

370
00:24:14,890 --> 00:24:18,966
whereas traditionally with continuous integration you would just be building from master,

371
00:24:19,078 --> 00:24:22,090
and if it passes the tests, then you're good to promote.

372
00:24:22,910 --> 00:24:26,474
But so long as you can automate, which counts as the best model,

373
00:24:26,592 --> 00:24:30,254
then your training can build an artifact from a promotion, much like

374
00:24:30,292 --> 00:24:34,218
with CI. And sometimes these CI systems do have integrations

375
00:24:34,314 --> 00:24:38,730
to rather, sometimes these training systems do have integrations

376
00:24:38,810 --> 00:24:43,898
available to CI systems let's

377
00:24:43,914 --> 00:24:46,466
say we've got a way of building our model and we want to be able

378
00:24:46,488 --> 00:24:50,242
to serve it. So we want to make predictions available in real time

379
00:24:50,296 --> 00:24:54,454
via HTTP, perhaps using a rest API. We might

380
00:24:54,492 --> 00:24:57,734
use a serving solution, as there's a range of them

381
00:24:57,772 --> 00:25:01,554
out there, some that are particular to a machine learning toolkit

382
00:25:01,602 --> 00:25:05,458
such as Tensorflow serving, Tensorflow or Torch SRE

383
00:25:05,474 --> 00:25:09,494
or Pytorch. There's also serving solutions provided by cloud providers

384
00:25:09,542 --> 00:25:13,450
as well, some that are more toolkit agnostic.

385
00:25:15,150 --> 00:25:18,846
For example, there's the toolkit agnostic open source offering that

386
00:25:18,868 --> 00:25:21,898
I work on. Seldom. Typically,

387
00:25:21,994 --> 00:25:25,610
serving solutions use the idea of a model being packaged and hosted,

388
00:25:25,690 --> 00:25:28,350
perhaps in a storage bucket or a disk location,

389
00:25:29,570 --> 00:25:33,026
so the serving solution can then obtain the model from that location and

390
00:25:33,048 --> 00:25:36,654
run it. Serving solutions often come with support for rollout

391
00:25:36,702 --> 00:25:38,740
and some support from monitoring as well.

392
00:25:41,030 --> 00:25:44,306
As an example of a serving solution, I'll explain the concept

393
00:25:44,338 --> 00:25:47,826
behind Seldon and how it's used. Seldon is aimed

394
00:25:47,858 --> 00:25:51,106
in particular at serving on kubernetes, and the models

395
00:25:51,138 --> 00:25:55,778
are served by creating a Kubernetes custom resource. The manifest

396
00:25:55,874 --> 00:25:59,146
of the custom resource is designed to make it simple to plug in a

397
00:25:59,168 --> 00:26:02,620
URI to a storage bucket containing a serialized model.

398
00:26:02,990 --> 00:26:06,470
So at a minimum, you can just put in the URI to the storage bucket

399
00:26:06,550 --> 00:26:09,694
and specify which toolkit was used to build

400
00:26:09,732 --> 00:26:13,626
the model. Then you submit that manifest to Kubernetes

401
00:26:13,738 --> 00:26:17,482
and it will create the lower level Kubernetes resources necessary

402
00:26:17,546 --> 00:26:21,310
to expose an API and serve the model's HTTP traffic.

403
00:26:22,130 --> 00:26:25,950
There's also a docker option to serve a model from a custom image.

404
00:26:26,030 --> 00:26:29,394
I'm emphasizing the serialized or pickled models in this talk,

405
00:26:29,512 --> 00:26:32,942
mostly because it's common to see those with serving solutions,

406
00:26:33,086 --> 00:26:35,960
and it's not very common outside of the mlops space.

407
00:26:39,890 --> 00:26:43,466
The serving stage links into rollout and monitoring I've

408
00:26:43,498 --> 00:26:47,290
talked a little bit already about a b testing as a rollout strategy.

409
00:26:47,450 --> 00:26:50,722
With that strategy, the traffic during the rollout is split between

410
00:26:50,776 --> 00:26:54,242
different versions, and you monitor that over a period of time until

411
00:26:54,296 --> 00:26:57,540
you've got enough data to be able to decide which is best.

412
00:26:58,710 --> 00:27:02,306
There's a more simple rollout strategy, which also involves splitting the traffic

413
00:27:02,338 --> 00:27:05,890
between different versions. With the Canary strategy,

414
00:27:05,970 --> 00:27:09,650
you split traffic between the live version of a model and a new version

415
00:27:09,730 --> 00:27:13,458
that you're evaluating. But typically with a canary,

416
00:27:13,554 --> 00:27:16,838
you just have one new model, and you evaluate it over a shorter

417
00:27:16,854 --> 00:27:20,426
period of time than with the A B test. It's more of

418
00:27:20,448 --> 00:27:23,126
a sanity check than an in depth evaluation,

419
00:27:23,318 --> 00:27:26,800
and you just promote if everything looks okay.

420
00:27:28,210 --> 00:27:30,350
Another strategy is shadowing.

421
00:27:31,010 --> 00:27:34,686
With shadowing, all of the traffic goes to both the new and the

422
00:27:34,708 --> 00:27:38,106
old model, but it's only the responses from the older

423
00:27:38,138 --> 00:27:41,986
model, the live model's responses that are used and which go back to

424
00:27:42,008 --> 00:27:45,646
the consumer. The new model is called the shadow version,

425
00:27:45,758 --> 00:27:50,210
and its responses are just stored. They don't go back to any live consumers.

426
00:27:50,550 --> 00:27:54,086
The reason for doing this is to monitor the shadow and compare it against the

427
00:27:54,108 --> 00:27:58,226
live version so it makes sense to be storing the shadow's output

428
00:27:58,258 --> 00:27:59,510
for later evaluation.

429
00:28:02,010 --> 00:28:05,946
Serving solutions have some support for rollout strategies. In the

430
00:28:05,968 --> 00:28:09,270
case of Seldon, for example, you can create a Kubernetes manifest

431
00:28:09,350 --> 00:28:13,222
with two sections, one for the main model and one for the Canary.

432
00:28:13,366 --> 00:28:17,786
The traffic will automatically be split between these two models by

433
00:28:17,808 --> 00:28:21,470
default. Seldom will split traffic evenly between the models. In a manifest,

434
00:28:21,890 --> 00:28:24,874
you can set a field against each model called traffic.

435
00:28:25,002 --> 00:28:28,174
That field takes a numeric percentage that tells seldom how much

436
00:28:28,212 --> 00:28:31,040
of the traffic each model should get.

437
00:28:34,050 --> 00:28:38,078
Each rollout strategies involve gathering metrics on running models.

438
00:28:38,254 --> 00:28:41,758
With seldom, there's out of the box integration available for Prometheus,

439
00:28:41,854 --> 00:28:44,390
and some Grafana dashboards are provided.

440
00:28:44,970 --> 00:28:48,710
These cover general metrics like frequency of requests and latency.

441
00:28:49,130 --> 00:28:52,998
You may also want to monitor for metrics that are specific to your use case,

442
00:28:53,164 --> 00:28:56,746
and there are defined interfaces so that extra metrics can be exposed in

443
00:28:56,768 --> 00:28:58,300
the usual Prometheus way.

444
00:29:01,390 --> 00:29:04,682
I mentioned earlier that in the shadow use case, you might want to be

445
00:29:04,736 --> 00:29:08,874
recording the predictions that the shadow is making so that you can compare its performance

446
00:29:08,922 --> 00:29:12,334
against the live model. This can be handled through

447
00:29:12,372 --> 00:29:15,790
logging all of the requests and responses to a database.

448
00:29:16,850 --> 00:29:20,466
There are other use cases as well where recording all predictions can be

449
00:29:20,488 --> 00:29:24,414
useful. For example, if you're working in a compliance heavy industry

450
00:29:24,462 --> 00:29:27,940
and an auditor requires to know of every prediction that's been made

451
00:29:28,950 --> 00:29:32,626
in the shadow use case, you'd use that database then

452
00:29:32,648 --> 00:29:36,134
to run queries against the data and compare the shadow's performance against

453
00:29:36,172 --> 00:29:37,480
that of the live model.

454
00:29:38,970 --> 00:29:43,030
In the case of Seldon, there's an out of the box integration

455
00:29:43,450 --> 00:29:47,290
which provides a way to asynchronously log everything to elasticsearch

456
00:29:48,110 --> 00:29:51,802
so that everything can then be made available for running queries on later, but without

457
00:29:51,856 --> 00:29:54,490
slowing down the request path of the live models.

458
00:29:57,830 --> 00:30:01,266
This idea of taking the live request and asynchronously sending

459
00:30:01,298 --> 00:30:05,234
it elsewhere can also be useful for some monitoring use cases,

460
00:30:05,282 --> 00:30:08,806
and not just for audit. In particular, there's some

461
00:30:08,828 --> 00:30:12,146
advanced monitoring use cases that relate to the data that's coming

462
00:30:12,188 --> 00:30:15,900
into the live model, how well it matches to the training data.

463
00:30:17,230 --> 00:30:20,906
If the live data doesn't fall within the distribution of the training data,

464
00:30:21,008 --> 00:30:24,320
then you can't be sure that your model will perform well on that data.

465
00:30:24,930 --> 00:30:27,760
Your model is based on patterns from the training data.

466
00:30:28,370 --> 00:30:33,150
So data that doesn't fit that training distribution might have different patterns.

467
00:30:33,970 --> 00:30:37,242
One thing we can do about this is to send all the request data

468
00:30:37,316 --> 00:30:40,946
through to detector components that will look for anything that

469
00:30:40,968 --> 00:30:44,900
might be going wrong so that we can flag those predictions if we need to.

470
00:30:45,670 --> 00:30:48,806
So let's drill a bit, a little bit further into what we might need to

471
00:30:48,828 --> 00:30:49,510
detect.

472
00:30:52,090 --> 00:30:55,814
One thing we might need to detect is an outlier. This is when

473
00:30:55,852 --> 00:30:59,714
there's the occasional data point which is significantly outside of the training

474
00:30:59,772 --> 00:31:03,514
data distribution, even though most of the data does fall

475
00:31:03,552 --> 00:31:07,174
within the distribution. Sometimes models express

476
00:31:07,222 --> 00:31:09,818
their predictions using a score. So, for example,

477
00:31:09,984 --> 00:31:13,886
classifiers often give a probability of how likely a data point is to

478
00:31:13,908 --> 00:31:16,080
be of a certain class for the model.

479
00:31:17,810 --> 00:31:22,814
You might expect to get a lower probability on everything when

480
00:31:22,852 --> 00:31:27,330
the data points are outliers.

481
00:31:27,670 --> 00:31:31,186
Unfortunately, it doesn't work that way. And for outliers, sometimes models can

482
00:31:31,208 --> 00:31:34,450
give very high probabilities for data points that they're getting completely

483
00:31:34,520 --> 00:31:38,466
wrong. This is called overconfidence. So if

484
00:31:38,488 --> 00:31:41,954
your live data has outliers and your use case has risk associated

485
00:31:42,002 --> 00:31:45,910
with those, then you might want to detect and track outliers.

486
00:31:46,490 --> 00:31:49,510
Depending on your use case, you might choose to make it part of your business

487
00:31:49,580 --> 00:31:52,854
logic, for example, to handle outlier cases differently,

488
00:31:52,982 --> 00:31:55,500
perhaps scheduling a manual review on them.

489
00:31:58,490 --> 00:32:01,734
Worse than the outlier case is when the whole data distribution is different

490
00:32:01,772 --> 00:32:08,442
from the training data. It can even start

491
00:32:08,496 --> 00:32:11,820
out similar to the training data and then shift over time.

492
00:32:12,190 --> 00:32:15,270
Think, for example, of the fashion recommendation.

493
00:32:15,350 --> 00:32:19,182
The example that we mentioned earlier was trained on data from

494
00:32:19,236 --> 00:32:23,354
winter, and then you continue using it into the summer. Then it's

495
00:32:23,402 --> 00:32:26,190
recommending coats when it should be recommending t shirts.

496
00:32:27,090 --> 00:32:30,542
If you have a component that knows the distribution of the training data,

497
00:32:30,676 --> 00:32:34,050
then you can asynchronously feed it all of the live requests,

498
00:32:34,390 --> 00:32:38,526
feed them into that component, and keep a watch. That component

499
00:32:38,558 --> 00:32:42,514
will keep a watch so you can use it to set up notifications in case

500
00:32:42,552 --> 00:32:45,606
the distribution shifts. You could then use

501
00:32:45,628 --> 00:32:49,078
that notification to decide if you need to train a new version of the model

502
00:32:49,164 --> 00:32:52,418
using updated data. Or perhaps you've

503
00:32:52,434 --> 00:32:55,698
got other metrics that let you track model performance, and if

504
00:32:55,724 --> 00:32:59,066
they're still showing as good, you might just choose to check those metrics more

505
00:32:59,088 --> 00:33:03,050
frequently while you look more closely into what's happening with live data distribution.

506
00:33:05,860 --> 00:33:09,344
These monitoring and prediction quality concerns also feed into

507
00:33:09,382 --> 00:33:12,756
governance for machine learning. It's a big topic and we

508
00:33:12,778 --> 00:33:16,304
can't go into everything in detail, but I want to give an impression

509
00:33:16,352 --> 00:33:20,532
of the area, so I'll at least mention of a few things I've

510
00:33:20,596 --> 00:33:24,548
talked about. Detection for data drift and outliers

511
00:33:24,724 --> 00:33:28,520
another thing detectors might be applicable for is adversarial attacks.

512
00:33:28,860 --> 00:33:32,040
These are when manipulated data is

513
00:33:32,190 --> 00:33:35,900
fed to a model in order to trick the model. Think, for example,

514
00:33:35,970 --> 00:33:40,536
of how face recognition systems can sometimes be tricked by somebody wearing a mask.

515
00:33:40,728 --> 00:33:43,544
That's a big problem in high security situations,

516
00:33:43,672 --> 00:33:47,280
and there are analogous attacks that have appeared for other use cases.

517
00:33:50,160 --> 00:33:53,436
I also mentioned that in high compliance situations you might

518
00:33:53,458 --> 00:33:56,670
want to record all of the predictions in case you need to review them later.

519
00:33:57,120 --> 00:34:00,400
This can also be relevant for dealing with customer domains.

520
00:34:00,820 --> 00:34:04,320
This relates to the topic of explainability. For example,

521
00:34:04,390 --> 00:34:07,680
if you've got a system that makes decisions on whether to approve loans,

522
00:34:08,260 --> 00:34:11,856
you're deploying somebody a loan. Then you might want to be able to explain why

523
00:34:11,878 --> 00:34:15,236
you denied them the loan. You'll want to be able to

524
00:34:15,258 --> 00:34:19,364
revisit exactly what was fed into the model. The explainability part

525
00:34:19,402 --> 00:34:23,248
is a data challenges data science challenge in itself, but it links

526
00:34:23,264 --> 00:34:26,792
into mlops because you'll need to know what data to

527
00:34:26,846 --> 00:34:30,376
get explanations for and what model was being used to

528
00:34:30,398 --> 00:34:32,120
make the original loan decision.

529
00:34:33,500 --> 00:34:37,980
The topic of explainability also relates to concerns about bias and ethics.

530
00:34:38,320 --> 00:34:41,736
Let's imagine that your model is biased and is unfairly denying

531
00:34:41,768 --> 00:34:43,340
loans to certain groups.

532
00:34:44,960 --> 00:34:48,296
You'll have a better chance of discovering that bias if you can

533
00:34:48,338 --> 00:34:52,480
explain which data points are contributing most towards its decisions.

534
00:34:54,100 --> 00:34:57,616
There's also a big governance question around being able to say exactly what

535
00:34:57,638 --> 00:35:01,276
was training and when. In traditional DevOps, it's a familiar

536
00:35:01,308 --> 00:35:04,864
idea that we'd want to be able to say which version of the software was

537
00:35:04,902 --> 00:35:08,484
running at a given point in time and what code it was built from,

538
00:35:08,602 --> 00:35:11,524
so that we can delve into that code and build it again if we need

539
00:35:11,562 --> 00:35:14,996
to. This can

540
00:35:15,018 --> 00:35:18,836
be much more difficult to achieve with mlOps, as it would also require

541
00:35:18,868 --> 00:35:21,912
being able to get access to all the data was used to train the model,

542
00:35:22,046 --> 00:35:25,544
and likely also being able to reproduce all of the transformations that were

543
00:35:25,582 --> 00:35:29,550
performed on the data and the parameters that were used in the training run.

544
00:35:30,160 --> 00:35:33,852
Even then, there can be elements of randomness in the training process

545
00:35:33,986 --> 00:35:37,710
that can scupper reproducibility. You don't plan for them.

546
00:35:40,430 --> 00:35:43,450
So let's finish up by summarizing what we've learned.

547
00:35:44,110 --> 00:35:48,134
MLOPs is a new terrain. ML workflows

548
00:35:48,182 --> 00:35:52,330
are more exploratory and data driven than traditional dev workflows.

549
00:35:53,730 --> 00:35:57,354
MLOPs enables ML workflows. It provides

550
00:35:57,402 --> 00:36:01,754
tools and practices for enabling training runs and experiments

551
00:36:01,882 --> 00:36:05,650
that are very data intensive and which use a lot of compute resources.

552
00:36:07,350 --> 00:36:10,594
Provides facilities for tracking artifacts produced and

553
00:36:10,632 --> 00:36:13,250
operations on data during those training runs.

554
00:36:14,630 --> 00:36:18,210
There are MLOps tools specifically for serving machine learning models

555
00:36:18,290 --> 00:36:21,858
and specialized strategies for safely rolling out new models for serving

556
00:36:21,874 --> 00:36:25,986
in a production environment. There's also tools

557
00:36:26,018 --> 00:36:30,214
and approaches for monitoring models running in a production environment and

558
00:36:30,252 --> 00:36:33,366
checking that model performance stays acceptable, or at least

559
00:36:33,388 --> 00:36:35,720
that you find out if something does go wrong.

560
00:36:37,370 --> 00:36:41,046
So that's my perspective on the field of mlops right at

561
00:36:41,068 --> 00:36:43,850
least as it is right now. Thanks very much for listening.

