1
00:00:00,410 --> 00:00:03,742
Sometimes being the last talk. Actually being the last talk is always super

2
00:00:03,796 --> 00:00:06,560
dope. But there's different reasons, right?

3
00:00:07,010 --> 00:00:10,302
These best thing about being the last talk is you can kind of tie things

4
00:00:10,356 --> 00:00:13,774
together. You know, I can listen to all the other talks that happen

5
00:00:13,812 --> 00:00:17,006
throughout the day and then tell the story.

6
00:00:17,188 --> 00:00:20,366
The other thing is sometimes all the things you wanted to say have already been

7
00:00:20,388 --> 00:00:23,294
said. So talks over. We're done. Be. Let's go. No,

8
00:00:23,332 --> 00:00:27,270
no, seriously. I'm using to try to be a little flexible

9
00:00:27,850 --> 00:00:31,910
as how we're going based

10
00:00:31,980 --> 00:00:35,222
on these ideas of things that we've already talked about.

11
00:00:35,276 --> 00:00:38,390
So going to take this opportunity. I've got some.

12
00:00:38,540 --> 00:00:41,400
That's weird. Well, sorry. On that screen,

13
00:00:41,770 --> 00:00:45,194
just noticed that you can't really see the top. Well, all the really good

14
00:00:45,232 --> 00:00:48,806
information is at the very top of the slide, so you're missing

15
00:00:48,838 --> 00:00:50,860
all the real thought leadership over there.

16
00:00:53,570 --> 00:00:56,666
This talk is called the psychology of chaos engineering.

17
00:00:56,778 --> 00:00:59,966
So it's thinking along the lines of a lot

18
00:00:59,988 --> 00:01:03,230
of the human factors that come into play. My name is

19
00:01:03,300 --> 00:01:07,122
Maddie Stratton. I'm a DevOps advocate for

20
00:01:07,256 --> 00:01:11,054
Pagerduty. So I'm not a huge fan of resume slides,

21
00:01:11,102 --> 00:01:13,074
but I really like this one. So you're just using to have to sit through

22
00:01:13,112 --> 00:01:16,306
it. It's cool. So anyway, I work for Pagerduty. How many

23
00:01:16,328 --> 00:01:19,646
people here have heard of Pagerduty? You already heard about it in some earlier slides

24
00:01:19,678 --> 00:01:23,382
today, too. So if you didn't raise your hand, you're lying. Cool.

25
00:01:23,516 --> 00:01:26,838
That's as much as we're going to talk about Pagerduty right now. Business. It's the

26
00:01:26,844 --> 00:01:29,858
bottom of every slide. I do a podcast called Arrested DevOps.

27
00:01:29,874 --> 00:01:33,734
We're one of the longest running, still running DevOps podcasts.

28
00:01:33,862 --> 00:01:36,986
If you're a listener and you want a sticker, come see me later. I got

29
00:01:37,008 --> 00:01:40,886
lots of stickers. I also have cool pager duty stickers, too, because Deverel

30
00:01:40,918 --> 00:01:44,170
life is giving away stickers. That's my real job, sticker engineer.

31
00:01:44,330 --> 00:01:47,758
I founded DevOps days Chicago, which is going

32
00:01:47,764 --> 00:01:50,926
to be in September. So if you're into DevOps and you're going to be in

33
00:01:50,948 --> 00:01:53,934
Chicago, you should come to it and we'll talk about it later. And I help

34
00:01:53,972 --> 00:01:57,474
run DevOps days all over the world. And this

35
00:01:57,512 --> 00:02:00,866
used to be my license plate. So one might say I'm

36
00:02:00,888 --> 00:02:04,574
invested in DevOps. But then DevOps is like ten years old. It literally

37
00:02:04,622 --> 00:02:07,778
is, right? Like, the first DevOps days was ten years ago. I want

38
00:02:07,784 --> 00:02:10,226
to be cutting edge and everything, so I actually had to get a new license

39
00:02:10,258 --> 00:02:13,622
plate. So this is my current license plate. That's true,

40
00:02:13,676 --> 00:02:15,960
by the way, that is my car.

41
00:02:16,650 --> 00:02:20,510
So I might have $200 more than cents

42
00:02:20,690 --> 00:02:24,166
might be because that's what a vanity license plate costs in Illinois.

43
00:02:24,278 --> 00:02:27,738
But in reality, what I like to do at this part of

44
00:02:27,744 --> 00:02:30,966
the talk is kind of get a level set and get

45
00:02:31,008 --> 00:02:34,606
some agreement so that we're kind of speaking a

46
00:02:34,628 --> 00:02:38,254
similar language. And in the case of this being towards the end of the day,

47
00:02:38,292 --> 00:02:41,870
we've seen all these talks. Fortunately,

48
00:02:42,370 --> 00:02:45,934
the level setting that I wanted to do has already been done.

49
00:02:46,052 --> 00:02:49,678
Right. Nobody really said anything today that concretely disagrees

50
00:02:49,694 --> 00:02:52,306
with anything I was going to say, so thank God, because that would have been

51
00:02:52,328 --> 00:02:55,746
awkward. But a couple of things that I do like to, like to

52
00:02:55,768 --> 00:02:59,366
kind of stress and you may find. So the nice thing, but doing

53
00:02:59,388 --> 00:03:02,886
this talk at the end of the day, especially at these conference, is that the

54
00:03:02,908 --> 00:03:06,726
things that you already know, those are review. These is all.

55
00:03:06,908 --> 00:03:09,830
It's like it's on purpose, right? We're tying it all back together.

56
00:03:09,980 --> 00:03:13,754
So, first of all, also, I don't always give this talk

57
00:03:13,792 --> 00:03:16,986
at chaos engineering conferences. So sometimes I have to tell people what I

58
00:03:17,008 --> 00:03:20,746
think chaos engineering is. And by I, I mean somebody else's definition that I like.

59
00:03:20,848 --> 00:03:24,634
So this is from principles of chaos. But really, the couple of the things

60
00:03:24,672 --> 00:03:27,946
about that that I always like to kind of bring. But we talked about experimenting.

61
00:03:27,978 --> 00:03:31,374
I think we're all pretty common and thinking about that, and we're looking about

62
00:03:31,412 --> 00:03:35,534
building confidence to be able to withstand these turbulent conditions.

63
00:03:35,582 --> 00:03:39,198
Right. You'll notice there was nothing in here about prediction.

64
00:03:39,374 --> 00:03:43,102
So that's just kind of my little thought. This isn't

65
00:03:43,166 --> 00:03:45,890
an old definition, but we can go back here. I was just talking about.

66
00:03:45,960 --> 00:03:50,018
It's almost ten years ago. So this was from these Netflix tech blog

67
00:03:50,114 --> 00:03:53,686
talking about chaos monkey. But there's three things I like

68
00:03:53,708 --> 00:03:57,366
to pick out of this definition that are

69
00:03:57,388 --> 00:04:00,854
generally interesting. And again, think about at some conferences I'm giving

70
00:04:00,892 --> 00:04:04,458
this talk, this is a brand new idea here. We're reviewing, we're getting on the

71
00:04:04,464 --> 00:04:07,802
same page. So it's. Right. So they're running

72
00:04:07,856 --> 00:04:11,066
elements in the middle of the business day. And this is

73
00:04:11,088 --> 00:04:15,434
similar to at page of duty. We run our failure Fridays during

74
00:04:15,472 --> 00:04:18,906
the day. They're run it at lunchtime, Pacific time, because frankly,

75
00:04:18,938 --> 00:04:22,014
there's no good time for pager duty to be down. If there is any

76
00:04:22,052 --> 00:04:25,294
good time for us to take any kind of an incident, it's during the day

77
00:04:25,332 --> 00:04:29,026
in San Francisco when most of the people are in that particular

78
00:04:29,128 --> 00:04:32,514
office. People in our Toronto office will

79
00:04:32,552 --> 00:04:35,758
tell me that, no, the best time is during the day in Toronto because Canadians

80
00:04:35,774 --> 00:04:38,360
are just as important as valley people. And that's totally true.

81
00:04:39,690 --> 00:04:42,998
Carefully monitored environment is a big key part of this. Right?

82
00:04:43,084 --> 00:04:46,374
And then again, having your engineers standing by. So these is all

83
00:04:46,412 --> 00:04:50,498
stuff that hopefully in this room is like things we consider as table stakes.

84
00:04:50,594 --> 00:04:53,546
If we don't consider this table stakes. First of all, if any of this comes

85
00:04:53,568 --> 00:04:57,178
as a big surprise to you right now, you've probably been sleeping all day,

86
00:04:57,264 --> 00:05:00,922
right? Which I totally get. So cool. But perceptions.

87
00:05:00,986 --> 00:05:04,862
So here's the thing. We just talked about what we understand this to be,

88
00:05:04,996 --> 00:05:08,622
but there's different perceptions around what

89
00:05:08,676 --> 00:05:12,474
chaos engineering is and what it provides and also what's

90
00:05:12,522 --> 00:05:16,702
involved. And sometimes people say perception is reality.

91
00:05:16,846 --> 00:05:21,214
What you're trying to do is affect change in an organization, likely. So perception

92
00:05:21,342 --> 00:05:24,926
super matters. It's not about being right. That's what Twitter

93
00:05:24,958 --> 00:05:29,106
is for. When you're doing change inside your organization, it's about understanding

94
00:05:29,138 --> 00:05:32,406
the perceptions. So this

95
00:05:32,428 --> 00:05:36,038
has been alluded to before, but this drives me up a freaking wall.

96
00:05:36,204 --> 00:05:39,474
Anytime I try to go somewhere and collect information about chaos

97
00:05:39,522 --> 00:05:42,902
engineering, several people think they are the most clever person

98
00:05:42,956 --> 00:05:46,262
in the world. That will be the first ones who ever make these joke,

99
00:05:46,406 --> 00:05:49,478
right? They're not also to see the people who come up to the pager duty

100
00:05:49,494 --> 00:05:51,466
booth and go, ha, I hate you. You wake me up in the middle of

101
00:05:51,488 --> 00:05:54,814
the night, we're like, clever. Never heard that one before. So this is generally my

102
00:05:54,852 --> 00:05:58,590
response to that. It's like, first of all, you're wrong,

103
00:05:58,740 --> 00:06:02,618
right? That's why it's on social media. But also, it's not even clever.

104
00:06:02,714 --> 00:06:06,654
So that's the other thing. It offends me. As Jerry Seinfeld

105
00:06:06,782 --> 00:06:10,670
said before, to sort of paraphrase. It offends me. Has a chaos engineering

106
00:06:10,750 --> 00:06:14,366
advocate, and it offends me as a comedian, because it's

107
00:06:14,398 --> 00:06:17,906
not even funny. If you're using to troll, at least be funny.

108
00:06:18,018 --> 00:06:20,840
Okay? And again,

109
00:06:21,450 --> 00:06:24,854
it's not about breaking things, right? Our intent is

110
00:06:24,892 --> 00:06:28,246
not to actually break something and try to

111
00:06:28,268 --> 00:06:31,546
push it to its limit and go kind of, haha. I figured out how

112
00:06:31,568 --> 00:06:35,178
to. But your system, right? That's a different thing. And if you don't believe me,

113
00:06:35,264 --> 00:06:39,514
believe Sylvia. Because if you know anything about Sylvia Botros from

114
00:06:39,552 --> 00:06:42,698
Sengrid, she is the expert at breaking things. And if Sylvia tells you

115
00:06:42,704 --> 00:06:46,046
it's not about breaking things, these it super isn't. So that's got to be true.

116
00:06:46,228 --> 00:06:49,898
Look, I know you know this, right? So why am I bringing

117
00:06:49,914 --> 00:06:53,570
it up? One is, it's kind of a nice way to round out these day.

118
00:06:53,720 --> 00:06:57,106
The other is we have to kind

119
00:06:57,128 --> 00:07:00,466
of continually remind ourselves of these points and

120
00:07:00,488 --> 00:07:04,306
these principles, because almost by virtue of

121
00:07:04,328 --> 00:07:08,082
you sitting in this room, you're at a certain level of understanding

122
00:07:08,226 --> 00:07:11,666
of this practices and of this field and the first principles surrounding

123
00:07:11,698 --> 00:07:15,506
it. The people you're working with in your organization

124
00:07:15,618 --> 00:07:19,210
may not be. And it's very easy, as we become

125
00:07:19,280 --> 00:07:22,746
further advanced in our understanding of something, to kind

126
00:07:22,768 --> 00:07:25,786
of forget where we came from,

127
00:07:25,808 --> 00:07:30,118
not even necessarily where we came from, but where people at a different mode

128
00:07:30,134 --> 00:07:33,306
of understanding might be. Right. So, again, I know you

129
00:07:33,328 --> 00:07:37,310
know this I'm using to say it. Anyway, the good thing is

130
00:07:37,380 --> 00:07:39,966
this just sort of blends right into the end of the day. So this is

131
00:07:39,988 --> 00:07:43,658
almost just like bullshitting at the bar. We're kind of at a bar, so it's

132
00:07:43,674 --> 00:07:47,806
cool, right? All right. So like I said, you know this, but they're experiments.

133
00:07:47,838 --> 00:07:51,266
I think that's a really key thing, and it's a really key way

134
00:07:51,288 --> 00:07:55,778
to help with that understanding. We've talked about hypotheses,

135
00:07:55,874 --> 00:07:59,622
right? And our hypothesis should

136
00:07:59,676 --> 00:08:03,334
be that if we do this thing,

137
00:08:03,452 --> 00:08:07,310
if this condition exists, my hypothesis

138
00:08:07,410 --> 00:08:10,540
is it will still work.

139
00:08:10,910 --> 00:08:14,582
If. My hypothesis is, if I shut down this node,

140
00:08:14,646 --> 00:08:17,994
everything's going to go to shit. I probably shouldn't run

141
00:08:18,032 --> 00:08:20,140
that experiment, right?

142
00:08:21,070 --> 00:08:24,394
Again, we're testing out assumptions and

143
00:08:24,432 --> 00:08:27,646
hypothesis now. If your hypothesis is everything will go terrible, then, yeah,

144
00:08:27,748 --> 00:08:29,918
maybe you still want to run it, but you definitely should run that in a

145
00:08:29,924 --> 00:08:32,986
lower environment. That's a whole different talk to talk about the myth of staging.

146
00:08:33,018 --> 00:08:35,506
So we're just not going to talk about that right now. That's cool, right?

147
00:08:35,608 --> 00:08:38,734
So, again, taking a scientific approach,

148
00:08:38,782 --> 00:08:42,494
I absolutely loved that convergence divergence

149
00:08:42,542 --> 00:08:46,334
thing that Adrian talked about, and I'm going to steal it for the next iteration

150
00:08:46,382 --> 00:08:49,894
of this talk that I give when Adrian hasn't given that talk right before it.

151
00:08:50,012 --> 00:08:53,094
So, these. I will look like the clever one. I may

152
00:08:53,132 --> 00:08:57,458
actually attribute it to Adrian. We'll see how that goes. So you're

153
00:08:57,474 --> 00:09:00,746
like, we know this, Maddie. We got it right. And why does it

154
00:09:00,768 --> 00:09:03,900
matter? Because how we talk about things matter.

155
00:09:04,350 --> 00:09:08,570
And this is a little tricky. Sometimes words

156
00:09:08,640 --> 00:09:12,794
matter, and sometimes these don't, right? So getting nerd

157
00:09:12,842 --> 00:09:16,286
snipe points on Twitter for someone using a word wrong just

158
00:09:16,308 --> 00:09:20,490
to be right is not helpful. So sometimes a word matters,

159
00:09:20,570 --> 00:09:24,398
and a word matters when it affects how we think about something.

160
00:09:24,564 --> 00:09:28,654
So I'm going to take a couple of examples, not directly

161
00:09:28,702 --> 00:09:32,158
related, but to illustrate that point so we can see how it applies.

162
00:09:32,334 --> 00:09:35,814
So before working at pagerduty, I worked

163
00:09:35,852 --> 00:09:39,094
at chef, come from an infrastructure, has code background, so automate all

164
00:09:39,132 --> 00:09:43,654
things. That's amazing. But the

165
00:09:43,692 --> 00:09:47,078
components that make up chef code, one of

166
00:09:47,084 --> 00:09:50,610
the elements of those are called recipes. Because of course they are. Because chef,

167
00:09:50,690 --> 00:09:53,466
by the way, if you hate food puns, you definitely should go use puppet or

168
00:09:53,488 --> 00:09:57,146
ansible, don't come to chef because chef is a t shirt company that

169
00:09:57,168 --> 00:10:02,278
also sells software. And then we also make a lot of would

170
00:10:02,384 --> 00:10:05,418
oftentimes, instead of calling it a recipe,

171
00:10:05,594 --> 00:10:08,906
people, customers I was working with, users that are trying to adopt

172
00:10:08,938 --> 00:10:12,906
this would talk about chef scripts. And that's

173
00:10:12,938 --> 00:10:16,962
one that I would correct gently and with an explanation. Not because

174
00:10:17,016 --> 00:10:20,238
I'm like, oh no, it's a recipe because chef and food and blah, blah,

175
00:10:20,254 --> 00:10:24,420
blah. But it's a different way of thinking about what that actual application

176
00:10:25,030 --> 00:10:29,410
of a concept is doing, right? A script is iterative

177
00:10:29,490 --> 00:10:33,442
going through step. It's a stepwise thing. A recipe,

178
00:10:33,506 --> 00:10:37,142
maybe it's not a perfect analogy, but the point is that recipe is

179
00:10:37,196 --> 00:10:40,374
good and script is bad, but script is not the was we want to think,

180
00:10:40,412 --> 00:10:41,980
so we're going to use a different word.

181
00:10:43,310 --> 00:10:46,426
There's other things that I choose not to get a pedantic about.

182
00:10:46,528 --> 00:10:49,814
I work at pager duty. We talk about post mortems a lot because incidents,

183
00:10:49,862 --> 00:10:53,326
right? So how many people are aware that there's some people that

184
00:10:53,348 --> 00:10:56,510
don't like calling them post mortems and for good reason.

185
00:10:56,660 --> 00:11:00,666
Right? But it's not fundamentally,

186
00:11:00,778 --> 00:11:04,222
by calling it a post incident review or a retrospective or an after

187
00:11:04,276 --> 00:11:08,194
action report or whatever is not inherently changing how you think about

188
00:11:08,232 --> 00:11:10,978
it. You can have a very good reason for not wanting to call it a

189
00:11:10,984 --> 00:11:14,594
post mortem, but it's not related to a

190
00:11:14,632 --> 00:11:17,780
change in behavior of how you do it.

191
00:11:19,190 --> 00:11:21,990
How many of you follow me on Twitter? It's okay if the answer is no.

192
00:11:22,060 --> 00:11:25,414
But if you do, you can probably take a guess as to what the next

193
00:11:25,452 --> 00:11:29,106
word is that I'm going to say I'm picky about. And that's the word root

194
00:11:29,138 --> 00:11:32,854
cause, right? These reason, first of all, I'm not getting

195
00:11:32,892 --> 00:11:34,986
nerd points here. If you want to call it root cause, that's great. You're a

196
00:11:35,008 --> 00:11:37,962
fine human being and I love you. But here's why I think,

197
00:11:38,016 --> 00:11:41,900
contributing factor, why this one matters, because it changes how we think

198
00:11:42,590 --> 00:11:46,814
when we use that word. It makes us think about a singular cause,

199
00:11:47,012 --> 00:11:50,670
which in complex systems is not there. So my whole point of this,

200
00:11:50,820 --> 00:11:53,646
this is not about stop using the word root cause, by the way, I stopped

201
00:11:53,678 --> 00:11:57,378
using these word root cause. It's about the words we're going

202
00:11:57,384 --> 00:12:01,650
to use when we're trying to affect change using chaos engineering within

203
00:12:01,720 --> 00:12:06,338
organizations. Right. The thing is, people get

204
00:12:06,424 --> 00:12:09,718
nervous. It's kind of how we live. It's kind

205
00:12:09,724 --> 00:12:13,894
of how we've survived as a species is because we're worried about

206
00:12:13,932 --> 00:12:17,714
risk. If we weren't worried about risk, we have all been eaten by antelopes.

207
00:12:17,762 --> 00:12:21,046
Well, maybe not antelopes. I don't know. I'm not good at animals, but something bigger

208
00:12:21,078 --> 00:12:24,826
with teeth. So risk aversion is kind

209
00:12:24,848 --> 00:12:28,886
of baked into us. So we're going to get nervous about things that seem

210
00:12:28,918 --> 00:12:31,850
to imply additional risk.

211
00:12:32,010 --> 00:12:35,230
Right. You're going to do what in production?

212
00:12:35,570 --> 00:12:40,366
How many people think about folks inside

213
00:12:40,468 --> 00:12:45,282
your organization whose entire focus

214
00:12:45,416 --> 00:12:48,578
and lens by which they view

215
00:12:48,744 --> 00:12:52,340
their relationship to your company's business

216
00:12:52,710 --> 00:12:56,274
is mitigating risk? There's a lot of people that. That's their

217
00:12:56,312 --> 00:13:00,374
job, and that's great if

218
00:13:00,412 --> 00:13:02,982
you are going to come to them and say, I would like to do this

219
00:13:03,036 --> 00:13:07,030
thing, and they hear it as I would like to create more risk.

220
00:13:07,370 --> 00:13:11,370
The conversation is now over. The irony

221
00:13:12,110 --> 00:13:15,206
is that folks who want to, we know we're all sitting here going, but that's

222
00:13:15,238 --> 00:13:18,534
ridiculous. Chaos engineering helps lower risk.

223
00:13:18,582 --> 00:13:22,058
It's good. It's blah, blah, great. And they will love that once they understand

224
00:13:22,144 --> 00:13:25,642
it. But you want to be able to have that conversation.

225
00:13:25,706 --> 00:13:28,414
So Adrian talked about that a little bit before. Right? And again, this is beautiful.

226
00:13:28,452 --> 00:13:31,882
Everyone's already given my talk, but this is why it matters.

227
00:13:32,026 --> 00:13:35,354
And when we're thinking about mitigating

228
00:13:35,402 --> 00:13:39,090
risk, I also like to mean, again, use your monitoring like it's for real.

229
00:13:39,160 --> 00:13:42,302
We've had already conversations around this, your chaos

230
00:13:42,366 --> 00:13:45,758
experiment, to be successful. And by successful, not to prove

231
00:13:45,774 --> 00:13:49,686
your point, but it's not actually get you fired. You need

232
00:13:49,708 --> 00:13:53,554
to be looking at the impact to production. You need to be monitoring like it's

233
00:13:53,602 --> 00:13:57,526
for real. And it's because it is another way to

234
00:13:57,548 --> 00:14:00,838
think about this. And we've seen some examples, but at pager duty,

235
00:14:00,854 --> 00:14:04,474
we run our failure Fridays like a regular incident. We already

236
00:14:04,512 --> 00:14:08,890
start incident response as part of the failure experiment,

237
00:14:09,470 --> 00:14:13,166
and there's two reasons for that. One is we're already tuned up for it.

238
00:14:13,268 --> 00:14:17,082
For us, it's also, we believe very strongly in always practicing incident

239
00:14:17,146 --> 00:14:20,782
response. And a failure experiment is a really great

240
00:14:20,836 --> 00:14:24,682
way to normalize the practice of incident response.

241
00:14:24,746 --> 00:14:27,570
This is a little different. When Ross was talking about, like, a fire drill,

242
00:14:27,910 --> 00:14:31,234
it makes you complacent. The difference of this is just being

243
00:14:31,272 --> 00:14:34,706
users to the motions. Right. It doesn't mean because we do want

244
00:14:34,728 --> 00:14:38,002
to try to reduce some of the stress. So, for example, the way we train

245
00:14:38,056 --> 00:14:41,586
incident commanders, if you want to be an incident commander at pager duty,

246
00:14:41,698 --> 00:14:44,998
before you go on call as an incident commander, you have to have run a

247
00:14:45,004 --> 00:14:49,234
failure Friday because that's giving an opportunity to

248
00:14:49,372 --> 00:14:52,634
practice that under a relatively low stress situation.

249
00:14:52,832 --> 00:14:56,266
The reason I'm bringing this up is so we might take

250
00:14:56,288 --> 00:14:59,798
this and say, like, oh, well, then a failure day. A failure

251
00:14:59,814 --> 00:15:03,130
game day is a great way to practice a stressful situation.

252
00:15:03,280 --> 00:15:06,202
It's a great way to practice stress. It's a great way to practice trouble.

253
00:15:06,266 --> 00:15:09,934
No, it's not. No, don't do that. Because the first thing is your people

254
00:15:09,972 --> 00:15:13,690
don't need training and practice of being under stress.

255
00:15:13,770 --> 00:15:17,358
They get plenty of that already. Right. What we want to do is the opposite.

256
00:15:17,454 --> 00:15:20,926
So, because we have some insight into what's likely

257
00:15:20,958 --> 00:15:24,222
to happen, or at least the systems that are affected

258
00:15:24,286 --> 00:15:27,410
and we know what we actually acted upon,

259
00:15:27,750 --> 00:15:31,478
it's a really great way to practice and go through the motions, which create kind

260
00:15:31,484 --> 00:15:35,266
of a physiological. So, you know, as Paige

261
00:15:35,298 --> 00:15:38,600
says, something's broken, it's your fault. In this case, it actually is.

262
00:15:39,130 --> 00:15:42,300
You can be a little blameful in chaos days,

263
00:15:42,830 --> 00:15:45,978
but it's good blame, right? That's cool. Okay, so what

264
00:15:45,984 --> 00:15:48,826
about the people? So we've talked about a lot about tech. We've talked a lot

265
00:15:48,848 --> 00:15:52,422
about the systems, the technology, the providers,

266
00:15:52,566 --> 00:15:55,662
the terraforming, the Kubernetes. That's all the fun stuff,

267
00:15:55,716 --> 00:15:59,758
right? It's also the easy stuff. The humans pieces come in.

268
00:15:59,844 --> 00:16:03,070
So when we think about the people that are involved, and there's lots of

269
00:16:03,140 --> 00:16:06,846
people that come in, there's your employees,

270
00:16:06,878 --> 00:16:10,366
right? There's your delivery teams that are for the systems that we're

271
00:16:10,398 --> 00:16:14,606
running elements on. There's the people that are engaging in the experiment.

272
00:16:14,798 --> 00:16:18,374
And, I don't know, you might have some customers or

273
00:16:18,412 --> 00:16:22,402
users that might be wanting to use these systems. Those are people. They're involved.

274
00:16:22,546 --> 00:16:25,080
Right. So I always kind of, like, ask this question.

275
00:16:27,770 --> 00:16:30,914
If I said, how does it make you feel to know that someone like Netflix

276
00:16:30,962 --> 00:16:34,346
is practices these principles? We're actually pretty down with it,

277
00:16:34,368 --> 00:16:38,266
right? We're like, that's cool, man. They're Netflix. They're DevOps and all the. You know,

278
00:16:38,288 --> 00:16:41,594
and that's cool. So I know that I can get

279
00:16:41,632 --> 00:16:44,734
my new movies, and I can binge all the things

280
00:16:44,772 --> 00:16:48,702
and whatever. Yeah. What about your bank again?

281
00:16:48,756 --> 00:16:52,062
And everybody in this room buys into this, and this one

282
00:16:52,116 --> 00:16:55,458
still made you. Hmm. Right.

283
00:16:55,544 --> 00:16:58,754
But we totally know why it matters. And the thing is,

284
00:16:58,792 --> 00:17:01,090
the blast radius is what matters.

285
00:17:02,630 --> 00:17:06,034
I was pleased to see that Adrian also goes to Twitter like I do.

286
00:17:06,232 --> 00:17:09,646
So I had a very scientific survey and said, if you discover

287
00:17:09,678 --> 00:17:12,994
a service you consume uses chaos engineering production, do you feel reassured

288
00:17:13,042 --> 00:17:16,598
or uneasy? And most people said they were reassured. This is not scientific at

289
00:17:16,604 --> 00:17:20,134
all, by the way. And also there's a little selection bias,

290
00:17:20,182 --> 00:17:23,418
but graphs mean results. So I had to

291
00:17:23,424 --> 00:17:27,226
add some data and a little more data, such as it

292
00:17:27,248 --> 00:17:30,874
is. I did a few surveys around words that people might

293
00:17:30,912 --> 00:17:34,606
use to describe. So I thought it was interesting to say what words describe your

294
00:17:34,628 --> 00:17:38,174
personal feeling towards use of chaos engineering on your team.

295
00:17:38,212 --> 00:17:41,934
And a lot of people were optimistic, but there was quite a fair amount of

296
00:17:41,972 --> 00:17:45,326
uneasy and cynical and everything. And then this is

297
00:17:45,348 --> 00:17:48,658
what tells you everything. But engineers, when we flipped it

298
00:17:48,744 --> 00:17:52,594
and we actually said, what about if products you use?

299
00:17:52,632 --> 00:17:56,478
They were like, oh, my God, that's fantastic. But not us, because we're terrible.

300
00:17:56,574 --> 00:18:00,038
Right. So I thought that was really interesting. But one of

301
00:18:00,044 --> 00:18:03,094
the things that happens is when there's an understanding of

302
00:18:03,132 --> 00:18:07,026
the effects, this can actually have a really positive effect on your delivery

303
00:18:07,058 --> 00:18:10,562
teams, because we feel more comfortable making changes,

304
00:18:10,716 --> 00:18:14,346
we feel we have greater trust in the system, but we

305
00:18:14,368 --> 00:18:17,994
have to understand it because it has the opposite effect if we don't understand it,

306
00:18:18,032 --> 00:18:22,806
right? So this principle that when someone thinks

307
00:18:22,838 --> 00:18:26,506
it's about breaking things on purpose and all those things, it's actually going to reduce

308
00:18:26,698 --> 00:18:30,266
their. It's going to make them feel uneasy, but when there's a greater understanding.

309
00:18:30,298 --> 00:18:34,946
So this really boils down to education being helpful. And we

310
00:18:34,968 --> 00:18:38,878
talked about people getting nervous. Management can get very nervous.

311
00:18:39,054 --> 00:18:42,546
And we think about considering our words,

312
00:18:42,728 --> 00:18:46,146
and this is usually the part when I say,

313
00:18:46,328 --> 00:18:49,734
I don't have a great suggestion of what you should

314
00:18:49,772 --> 00:18:53,042
call it business failure injection or chaos

315
00:18:53,106 --> 00:18:56,598
engineering, because it might be different. Fortunately, a bunch of

316
00:18:56,604 --> 00:18:59,462
people today have already given you a bunch of really good of.

317
00:18:59,596 --> 00:19:03,018
And the thing that's important is accuracy is not the most important

318
00:19:03,104 --> 00:19:05,770
part. So, like, Ross talked about system verification,

319
00:19:06,110 --> 00:19:09,750
and the nerd snipes and us went, well, technically, it's not system verification

320
00:19:09,830 --> 00:19:13,166
because that's a formal process. It doesn't matter. You're talking to your

321
00:19:13,188 --> 00:19:17,086
CFO, right? Because all you want to actually do is get in the

322
00:19:17,108 --> 00:19:20,062
door to talk about it, right?

323
00:19:20,196 --> 00:19:23,946
Have that conversation. I like what Adrian says, just call it engineering.

324
00:19:23,978 --> 00:19:27,518
But that doesn't work as well for when you're trying to explain a new practice.

325
00:19:27,614 --> 00:19:30,354
So I invite everybody to kind of think on this, and I'd love to talk

326
00:19:30,392 --> 00:19:33,598
about it if people have examples of ideas,

327
00:19:33,694 --> 00:19:36,938
but it will vary depending upon your organizations.

328
00:19:37,054 --> 00:19:40,726
Right. And really it comes down to the understanding of

329
00:19:40,748 --> 00:19:44,150
the philosophy. And when you're trying to bring people along for a ride,

330
00:19:44,650 --> 00:19:48,114
you want to be somewhat like minded. Right.

331
00:19:48,172 --> 00:19:51,734
So I really like these. So Cody doesn't really use chaos

332
00:19:51,782 --> 00:19:55,514
engineers fully, but had some interesting failures from

333
00:19:55,552 --> 00:19:58,410
interns, which I guess is some form of chaos engineers.

334
00:19:59,790 --> 00:20:03,514
And he says now, honestly, feeling excitement when

335
00:20:03,552 --> 00:20:06,878
confronted with a new error that hasn't occurred before. This is really a

336
00:20:06,884 --> 00:20:10,526
lot more about learning from incidents. But if you have those things that have to

337
00:20:10,548 --> 00:20:13,790
be coupled and learning from incidents,

338
00:20:15,650 --> 00:20:18,706
it's a thing that we're all not very good at. By all, I don't mean

339
00:20:18,728 --> 00:20:20,866
all of us are bad at it, but I mean few of us are good

340
00:20:20,888 --> 00:20:24,226
at it because we usually look at

341
00:20:24,248 --> 00:20:27,586
incidents as something to be avoided. Again, we don't encourage

342
00:20:27,618 --> 00:20:31,126
them. We don't want to say like, boy, I sure wish that I had a

343
00:20:31,148 --> 00:20:34,806
ton and ton of incidents. Although one

344
00:20:34,988 --> 00:20:38,342
potentially controversial way to say it is it's been said,

345
00:20:38,396 --> 00:20:42,150
hey, you want to get better at incident response. If you tried having more incidents,

346
00:20:42,310 --> 00:20:46,582
and that immediately sounds funny, but actually the way it's phrased is scope

347
00:20:46,646 --> 00:20:50,042
more things to be considered as an incident, you will practice it more.

348
00:20:50,176 --> 00:20:53,946
And so incidents are a gift. If that's a little hard to wrap your to

349
00:20:53,968 --> 00:20:57,226
kind of swallow, maybe you say incidents are unplanned investment,

350
00:20:57,338 --> 00:21:01,034
but if we don't think about, if we're not focusing on being a learning culture,

351
00:21:01,082 --> 00:21:04,666
we're not going to get a lot of value out of all the practices we've

352
00:21:04,698 --> 00:21:07,666
talked about today, it's about the learning. And that's why I thought it was so

353
00:21:07,688 --> 00:21:10,866
great that we had to talk about post mortems as they apply to

354
00:21:10,888 --> 00:21:14,994
that. We need to run all the things we would do for an incident on

355
00:21:15,032 --> 00:21:16,740
our elements as well,

356
00:21:17,270 --> 00:21:20,646
especially. But the only reason that makes sense is

357
00:21:20,668 --> 00:21:24,098
if your after incident review, your pir, your post mortem is focused

358
00:21:24,114 --> 00:21:27,286
on learning. If the whole reason you do a post mortem is

359
00:21:27,308 --> 00:21:30,614
to write down these root case, it doesn't make any

360
00:21:30,652 --> 00:21:33,898
sense to you to do it after a chaos experiment because you already know what

361
00:21:33,904 --> 00:21:37,674
the root cause was, it was turned off, this thing. So those two

362
00:21:37,712 --> 00:21:40,954
things, I think these practices are so well coupled which is why

363
00:21:40,992 --> 00:21:44,346
in a lot of people's mind, they all kind of run under resiliency engineering

364
00:21:44,378 --> 00:21:48,010
practices, right? Learning from incidents. These things are all loosely coupled,

365
00:21:48,090 --> 00:21:51,674
right? They don't give you those things, but they all end up being connected

366
00:21:51,722 --> 00:21:55,186
because they all come back to us wanting to learn and have

367
00:21:55,208 --> 00:21:59,474
better understanding and be able to reason about our systems more

368
00:21:59,512 --> 00:22:00,290
broadly.

369
00:22:03,510 --> 00:22:06,146
I'm going a little quick. Part of it is because we're getting to the end

370
00:22:06,168 --> 00:22:09,438
of the day, and I know we started running behind, and I'd love to have

371
00:22:09,464 --> 00:22:12,374
a little more hallway track and everything like that. But just a couple of things

372
00:22:12,412 --> 00:22:15,560
I'd like to consider. Again, safety first, right?

373
00:22:16,010 --> 00:22:20,246
And this is in the safety two idea of safety, but safety first.

374
00:22:20,348 --> 00:22:23,926
We want to think about all sorts of things we've talked about today, about minimizing

375
00:22:23,958 --> 00:22:27,434
blast radius, about making sure your responders, by the way,

376
00:22:27,472 --> 00:22:30,918
just speaking of one thing I forgot, was I the only one who was waiting

377
00:22:30,934 --> 00:22:33,926
for the last slide for Muri's? Talk about where we could, like where the git

378
00:22:33,958 --> 00:22:37,646
repo was for Gabetta, and they're like, oh, no, we don't have that yet because

379
00:22:37,668 --> 00:22:41,454
I want it. Right. So that was exciting to me because of,

380
00:22:41,492 --> 00:22:44,682
again, the thoughts around the whole experiment,

381
00:22:44,746 --> 00:22:48,386
right? So a couple things I think that are very key

382
00:22:48,488 --> 00:22:53,346
to keep in mind, and these may seem like table stakes, but to

383
00:22:53,368 --> 00:22:56,566
use a terrible analogy, I used to do swing dancing, and we

384
00:22:56,588 --> 00:23:00,466
used to used to say about dancers, we'd say beginning dancers

385
00:23:00,578 --> 00:23:04,354
take intermediate classes, intermediate dancers take advanced classes,

386
00:23:04,402 --> 00:23:07,714
advanced dancers take beginner classes. So it's always helpful.

387
00:23:07,762 --> 00:23:11,094
As much as we're big experts in this room, a couple of these things.

388
00:23:11,132 --> 00:23:14,838
If it is common sense, great, it's a good review. If it's table stakes,

389
00:23:14,854 --> 00:23:18,266
I assume you're already doing all of these. But knowing your conditions of when do

390
00:23:18,288 --> 00:23:21,722
you shut down the experiment? And that's knowing what your key business

391
00:23:21,776 --> 00:23:25,534
indicators are, right? You're not shutting down the experiment because

392
00:23:25,652 --> 00:23:28,906
SQL server is using too much memory. All of a sudden, you're shutting

393
00:23:28,938 --> 00:23:32,618
down the experiment because your average card size has dropped below where it's

394
00:23:32,634 --> 00:23:36,594
supposed to be. Know what your key business metrics are that are these,

395
00:23:36,632 --> 00:23:39,074
you're just going to call it. So you need to know what those are.

396
00:23:39,192 --> 00:23:42,642
Right? And again, we want to build

397
00:23:42,696 --> 00:23:46,942
resiliency, and resiliency comes from people having adaptive capacity,

398
00:23:47,086 --> 00:23:50,470
right? And what we're not trying to do here is stress test

399
00:23:50,620 --> 00:23:54,134
people, right? Even if Adrian's going to break necks of your

400
00:23:54,172 --> 00:23:57,074
key sres, you're not trying to add stress,

401
00:23:57,122 --> 00:24:01,046
you're trying to find challenges. So it

402
00:24:01,068 --> 00:24:04,234
can be very tempting to look at a case experiment as a way to

403
00:24:04,272 --> 00:24:07,500
test people's ability to troubleshoot or

404
00:24:08,190 --> 00:24:12,026
to simulate the stress of on call. We want

405
00:24:12,048 --> 00:24:15,198
to have transparency on everything that's happened. Never surprise anybody with your

406
00:24:15,204 --> 00:24:19,230
chaos experiment. Right. And at these end of all these wonderful numbers

407
00:24:19,300 --> 00:24:22,974
about reducing MTTR and availability, numbers that are happening,

408
00:24:23,012 --> 00:24:26,866
everything, there are always people. And where I bring that up to mind is that

409
00:24:26,888 --> 00:24:30,850
some of the scale at which we work, a small

410
00:24:30,920 --> 00:24:34,642
number is not a small number. We're like, oh, well, we only

411
00:24:34,696 --> 00:24:38,406
impacted one 10th of a percent of our users. Okay, well, that might have been

412
00:24:38,428 --> 00:24:42,150
like 10,000 people that just had a shitty hour because

413
00:24:42,220 --> 00:24:46,806
we weren't really watching those key metrics. So at

414
00:24:46,828 --> 00:24:50,082
the other side of all of your grafana and all of this, there are humans.

415
00:24:50,146 --> 00:24:53,738
And we always want to keep the humans involved. And thinking about these,

416
00:24:53,824 --> 00:24:57,258
it doesn't mean we don't take risks. But remember that at the end

417
00:24:57,264 --> 00:24:59,994
of the day, it's really easy to look at a graph, but all those little

418
00:25:00,032 --> 00:25:03,194
things at the end are probably a person most likely.

419
00:25:03,242 --> 00:25:05,946
So trying to remember the humans is what matters.

420
00:25:06,058 --> 00:25:09,354
So, not that these slides are terribly exciting,

421
00:25:09,402 --> 00:25:12,702
but if you want to check them out, that's my speaking

422
00:25:12,756 --> 00:25:16,478
website. The slides are there, some supporting links, some other articles that

423
00:25:16,484 --> 00:25:19,918
I found interesting. There's a couple of articles that I linked to in there that

424
00:25:19,924 --> 00:25:23,418
I didn't talk about, that are about how we do failure Fridays

425
00:25:23,434 --> 00:25:26,902
and stuff at page of duty, which is just not saying you should do exactly

426
00:25:26,956 --> 00:25:30,646
what we do, but you're all interested in this domain, so it's more things to

427
00:25:30,668 --> 00:25:33,814
know, right? And yeah,

428
00:25:33,852 --> 00:25:37,186
if you like Twitter, that's where you can find me. I'm Matt Stratton, and Peggy

429
00:25:37,218 --> 00:25:38,340
says, follow me.

