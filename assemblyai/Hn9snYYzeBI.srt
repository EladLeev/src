1
00:01:28,210 --> 00:01:31,606
Hi, my name is Jason Dudash. I'm a chief architect at Red Hat, and I

2
00:01:31,628 --> 00:01:34,802
focus on modern application development and cloud technologies.

3
00:01:34,946 --> 00:01:38,162
Today I'm going to talk about the introduction to service mesh,

4
00:01:38,306 --> 00:01:42,286
and I'm going to focus on the, the how, the what,

5
00:01:42,468 --> 00:01:45,642
the why of service mesh. And I apologize in advance.

6
00:01:45,706 --> 00:01:47,998
This is going to be a little bit of a fire hose. I've got a

7
00:01:48,004 --> 00:01:50,718
lot of information I want to get through in kind of a short amount of

8
00:01:50,724 --> 00:01:53,966
time to do the talk. So I'll also, at the

9
00:01:53,988 --> 00:01:57,442
end, be demonstrating a lot of the key concepts that I talk about

10
00:01:57,496 --> 00:02:00,706
in the beginning slides. So really to set the

11
00:02:00,728 --> 00:02:04,414
stage, what we're talking about here is in the context of distributed computing,

12
00:02:04,462 --> 00:02:08,246
distributed systems at the most basic level, and distributed systems are

13
00:02:08,268 --> 00:02:12,754
great. There's a lot of capability that you can get from distributing

14
00:02:12,802 --> 00:02:15,826
your software across multiple geographies,

15
00:02:15,938 --> 00:02:19,270
across multiple systems,

16
00:02:19,350 --> 00:02:22,294
and you can meet all these non functional requirements,

17
00:02:22,342 --> 00:02:26,410
right? Those things that systems engineers like to call the illities, the scalability,

18
00:02:27,070 --> 00:02:30,810
the supportability, the reliability.

19
00:02:31,490 --> 00:02:34,302
And so this isn't a new concept, right?

20
00:02:34,356 --> 00:02:37,578
So distributed systems have been around since ethernet was invented,

21
00:02:37,594 --> 00:02:41,706
like in the late 70s, but it's

22
00:02:41,738 --> 00:02:45,282
just now become more ubiquitous and we see things

23
00:02:45,336 --> 00:02:48,260
happening at much larger scales than ever before.

24
00:02:49,670 --> 00:02:53,806
Distributing your software across multiple systems

25
00:02:53,918 --> 00:02:57,634
is really advantageous, but it also brings a lot of challenges,

26
00:02:57,762 --> 00:03:01,826
and those challenges are inherent to distributing

27
00:03:02,018 --> 00:03:05,686
software capability across networks. And so there were

28
00:03:05,708 --> 00:03:08,922
these things that were identified 25 plus

29
00:03:08,976 --> 00:03:12,918
years ago, known as the fallacies of distributed computing,

30
00:03:13,014 --> 00:03:15,722
and it impacts the way that we develop,

31
00:03:15,856 --> 00:03:19,062
deploy and manage our software systems.

32
00:03:19,206 --> 00:03:22,800
So things that novice developers don't think about,

33
00:03:23,330 --> 00:03:26,682
maybe experienced developers are writing some additional

34
00:03:26,826 --> 00:03:30,474
code to deal with the challenges you're facing.

35
00:03:30,522 --> 00:03:33,902
But for the most part, people aren't thinking about the reliability

36
00:03:33,966 --> 00:03:37,250
of the network, the latency that you're going to expect

37
00:03:37,320 --> 00:03:41,662
to experience in production, the bandwidth that you'll

38
00:03:41,726 --> 00:03:45,194
have, and even like, security concerns

39
00:03:45,262 --> 00:03:48,914
are often overlooked. And so those sorts of challenges

40
00:03:49,042 --> 00:03:52,562
have existed for a long time, but they're even worse,

41
00:03:52,706 --> 00:03:56,886
and they're even more impactful today

42
00:03:56,988 --> 00:04:00,474
because we're trending towards moving out of our data

43
00:04:00,512 --> 00:04:03,926
centers into cloud environments and we're transitioning monolithic

44
00:04:03,958 --> 00:04:07,274
systems into microservice architectures. And those

45
00:04:07,312 --> 00:04:10,990
microservice architectures bring us all this extra agility,

46
00:04:11,410 --> 00:04:14,602
but it also means that we're distributing things at a much larger

47
00:04:14,666 --> 00:04:18,362
scale than ever before. And so all these independently

48
00:04:18,426 --> 00:04:22,666
scalable, single purpose services that compose your

49
00:04:22,788 --> 00:04:26,306
overall application means

50
00:04:26,488 --> 00:04:30,034
lots and lots of little network connections back and forth and

51
00:04:30,072 --> 00:04:32,660
chains of network connections between all these services.

52
00:04:33,030 --> 00:04:36,662
And so you've probably seen this before. If you've been building

53
00:04:36,716 --> 00:04:40,134
microservice architectures, I've definitely seen it in

54
00:04:40,172 --> 00:04:43,094
my systems and customer systems that I work with.

55
00:04:43,212 --> 00:04:47,094
But once you start building these things, everything looks

56
00:04:47,132 --> 00:04:51,034
good. In development, everything actually looks good. A lot of cases in test,

57
00:04:51,232 --> 00:04:54,518
you fix a lot of bugs in QA and you're

58
00:04:54,534 --> 00:04:57,450
like, cool, let's ship this thing. Everything's good to go.

59
00:04:57,520 --> 00:05:00,874
But once you get into production, things become less predictable.

60
00:05:00,922 --> 00:05:04,302
And especially over time and

61
00:05:04,356 --> 00:05:08,638
under production level loads, things really don't perform

62
00:05:08,724 --> 00:05:12,954
the way you expected them to perform. And scaling up isn't

63
00:05:13,002 --> 00:05:16,674
like the solution that fixes a lot of the problems you have.

64
00:05:16,872 --> 00:05:21,342
In fact, when we fix problems, we do that with some workarounds.

65
00:05:21,486 --> 00:05:25,054
You might be thinking, hey, I know there's a lot of companies that are successful

66
00:05:25,102 --> 00:05:28,430
if you haven't done microservices already, there's a lot of companies that are doing it

67
00:05:28,440 --> 00:05:31,926
and they're very successful. And you're right, because they've found ways to work around and

68
00:05:31,948 --> 00:05:35,506
deal with those types of challenge. So historically, what we've

69
00:05:35,538 --> 00:05:39,314
seen is those challenges are addressed by boilerplate

70
00:05:39,362 --> 00:05:43,046
code and third party libraries. Netflix is probably best well known

71
00:05:43,078 --> 00:05:45,770
for creating some of these things, the eureka and zool.

72
00:05:46,430 --> 00:05:50,358
These frameworks get bundled into every microservice and provide

73
00:05:50,464 --> 00:05:53,726
solutions to deal with the things we're talking. But, but that's not

74
00:05:53,748 --> 00:05:56,618
really ideal. It can kind of reduce agility.

75
00:05:56,794 --> 00:06:00,590
And we're talking about adding extra work

76
00:06:00,660 --> 00:06:03,930
to developers plates to incorporate,

77
00:06:04,010 --> 00:06:07,150
load these libraries in and actually manage these dependency

78
00:06:07,230 --> 00:06:10,526
chains. Right? So imagine how much more challenges

79
00:06:10,558 --> 00:06:14,206
that gets if you're not only developing Java based microservices, but you're

80
00:06:14,238 --> 00:06:17,422
also using go and you're also using node js.

81
00:06:17,486 --> 00:06:21,334
And now you've got this problem. It's kind of replicated across the

82
00:06:21,372 --> 00:06:25,160
different tools and the different programming languages that you need to support.

83
00:06:25,770 --> 00:06:29,100
And so what we're talking about here today is

84
00:06:30,270 --> 00:06:33,562
a common approach to deal

85
00:06:33,616 --> 00:06:37,990
with those challenges by moving the responsibility

86
00:06:38,150 --> 00:06:41,306
to the platform and so you can address it at the infrastructure

87
00:06:41,338 --> 00:06:45,006
layer so developers don't have to reinvent the wheel for each new service that

88
00:06:45,028 --> 00:06:48,094
they're developing. And so that lets us

89
00:06:48,212 --> 00:06:51,486
apply policy consistently, also across an

90
00:06:51,508 --> 00:06:55,570
entire application, across an entire series, instead of microservices.

91
00:06:56,070 --> 00:06:59,570
So I think a really good analogy that helps explain

92
00:06:59,640 --> 00:07:03,454
what a service mesh does is with roads and traffic

93
00:07:03,502 --> 00:07:07,318
control. If your company or your organization is a city,

94
00:07:07,484 --> 00:07:11,350
then the red hat connect people's homes and

95
00:07:11,420 --> 00:07:14,918
businesses and places of work, those are the

96
00:07:14,924 --> 00:07:18,534
networks, right? And so if you live in a really

97
00:07:18,572 --> 00:07:21,878
small town, you have just a few roads, you might not

98
00:07:21,884 --> 00:07:24,906
need a whole lot of traffic control, but once you get to a city of

99
00:07:24,928 --> 00:07:28,010
a certain size, you're probably now in a position

100
00:07:28,080 --> 00:07:31,418
where you can't trust everyone to obey the speed limit and do the

101
00:07:31,424 --> 00:07:36,846
right thing, or even that they'll do something the same way as each other if

102
00:07:36,868 --> 00:07:40,042
there's no traffic signs and there's no guidance

103
00:07:40,106 --> 00:07:42,686
for them to do those things. So what do we do in a city?

104
00:07:42,788 --> 00:07:46,574
We put in place traffic control. We have police officers,

105
00:07:46,622 --> 00:07:49,982
we have speed limit signs, we have bike lanes,

106
00:07:50,046 --> 00:07:53,566
we have stoplights and walk signs and don't

107
00:07:53,598 --> 00:07:57,574
walk signs. And we control what's going on

108
00:07:57,612 --> 00:08:02,466
in that city. And the same thing should be true of organizations

109
00:08:02,498 --> 00:08:06,402
that are deploying microservice based applications across Kubernetes

110
00:08:06,466 --> 00:08:09,978
environments. So you need to

111
00:08:09,984 --> 00:08:13,498
be able to assert control over how traffic moves between

112
00:08:13,584 --> 00:08:17,414
those services. And the service mesh is the control plane

113
00:08:17,462 --> 00:08:20,998
for asserting that control. Right? So you could probably take the

114
00:08:21,024 --> 00:08:24,446
analogy even further if you wanted to, and talk about how

115
00:08:24,628 --> 00:08:28,590
observability is important because cities also

116
00:08:28,660 --> 00:08:32,634
have traffic cameras. And if you can see what's going on in traffic,

117
00:08:32,682 --> 00:08:36,098
you can identify bottlenecks in the system and you can audit and figure out

118
00:08:36,104 --> 00:08:39,730
how to improve those things. Again, service mesh

119
00:08:40,310 --> 00:08:43,810
has observability capabilities as well, and we'll get into that

120
00:08:43,880 --> 00:08:47,986
when I get into the demo. But under the hood it's

121
00:08:48,018 --> 00:08:51,234
pretty straightforward. I'm going to give a high chief architecting

122
00:08:51,282 --> 00:08:54,918
overview of the service mesh. It starts with a

123
00:08:54,924 --> 00:08:58,282
Kubernetes cluster like Openshift. And so your service mesh is

124
00:08:58,336 --> 00:09:02,074
part of that platform. And there are

125
00:09:02,112 --> 00:09:05,338
two big concepts of a data plane and

126
00:09:05,344 --> 00:09:08,554
a control plane, and I'll explain both of those.

127
00:09:08,592 --> 00:09:12,570
So the data plane is essentially this mediation layer

128
00:09:12,650 --> 00:09:16,682
that controls all the network communication between the microservices.

129
00:09:16,826 --> 00:09:21,354
That's its role in life, and it does that transparently.

130
00:09:21,482 --> 00:09:24,514
And one of the really cool things about this is how it works is that

131
00:09:24,712 --> 00:09:28,414
the mesh deploys a sidecar container, which is a Kubernetes architecture

132
00:09:28,462 --> 00:09:31,826
pattern that is colocated with your application.

133
00:09:32,008 --> 00:09:35,814
And so your applications are in this data plane. They're all talking to each

134
00:09:35,852 --> 00:09:39,746
other, but they're doing that through this sidecar proxy

135
00:09:39,778 --> 00:09:41,960
called envoy. That's the open source project.

136
00:09:42,890 --> 00:09:46,818
It's a really fast and dynamically configurable

137
00:09:46,914 --> 00:09:51,494
proxy. There's an API it provides so that you don't have to reload

138
00:09:51,542 --> 00:09:54,810
things. It just concepts, new configurations.

139
00:09:55,230 --> 00:09:59,100
And so we are able to program

140
00:09:59,470 --> 00:10:03,534
these envoy sidecars to

141
00:10:03,572 --> 00:10:07,230
do all the policy enforcement that we've identified,

142
00:10:07,570 --> 00:10:11,306
and we define that policy in a control plane

143
00:10:11,338 --> 00:10:14,746
layer. So your policy is part of this control plane.

144
00:10:14,778 --> 00:10:18,466
And the reason this is really really important is because imagine you

145
00:10:18,488 --> 00:10:22,530
have hundreds of microservices and hundreds of proxies. You wouldn't want to programmatically

146
00:10:22,870 --> 00:10:25,774
have to configure each of one of those individually.

147
00:10:25,902 --> 00:10:29,846
The control plane lets you define your policy and it applies it across all

148
00:10:29,868 --> 00:10:32,934
of your proxies for you. And so the separation of the

149
00:10:32,972 --> 00:10:36,886
control and the data planes lets you make changes to

150
00:10:36,908 --> 00:10:40,586
your mesh without having to change any of your application source code.

151
00:10:40,768 --> 00:10:43,834
And honestly that's like really probably the most cool part

152
00:10:43,872 --> 00:10:47,206
about all this, is that it's truly dynamic and you're

153
00:10:47,238 --> 00:10:50,006
solving your challenges without having to write new code,

154
00:10:50,128 --> 00:10:54,014
without having to rebuild your services. And once your services are

155
00:10:54,052 --> 00:10:57,994
part of the mesh, you don't even have to redeploy containerized

156
00:10:58,042 --> 00:11:01,760
into kubernetes to apply policy changes.

157
00:11:03,430 --> 00:11:07,300
So with that introduction, that firehose of information,

158
00:11:07,670 --> 00:11:10,802
let's take a look at it in action and see how some of this stuff

159
00:11:10,856 --> 00:11:14,130
works. Okay,

160
00:11:14,200 --> 00:11:17,154
let's dig into some observability capabilities of the mesh.

161
00:11:17,282 --> 00:11:20,966
I've got a simple microservices application here with a

162
00:11:20,988 --> 00:11:24,646
single sign on and a user interface and a

163
00:11:24,668 --> 00:11:28,110
profile service and a couple of databases.

164
00:11:28,210 --> 00:11:32,380
And altogether those microservices make up this web application.

165
00:11:32,910 --> 00:11:36,522
So it lets you create boards and

166
00:11:36,576 --> 00:11:40,640
add little items to boards. So I can come over here and I can say

167
00:11:41,730 --> 00:11:45,600
storing something to share

168
00:11:46,130 --> 00:11:48,080
red hat to the list.

169
00:11:49,010 --> 00:11:53,258
And in this particular

170
00:11:53,364 --> 00:11:57,086
example I've introduced some problems so that we can explore

171
00:11:57,118 --> 00:12:01,058
the observability features in action. So there's three

172
00:12:01,144 --> 00:12:05,074
main observability tools that I want to showcase. The first one is called

173
00:12:05,112 --> 00:12:08,686
Kiali, and it's like the main dashboard

174
00:12:08,718 --> 00:12:11,910
for the service mesh. So I can come into this graph view

175
00:12:11,980 --> 00:12:15,954
here in Kiali and I can see everything that's happening. I can see the ingress

176
00:12:16,082 --> 00:12:19,830
into my mesh, I can see the services that are running

177
00:12:19,900 --> 00:12:23,286
and what workloads are behind them. And so you see the

178
00:12:23,308 --> 00:12:27,570
user interface called App UI running the latest version of its container

179
00:12:27,730 --> 00:12:32,026
aboard service, which provides an API to edit

180
00:12:32,058 --> 00:12:35,498
and store data into a database that's MongoDB.

181
00:12:35,674 --> 00:12:39,294
And then our user profile service that actually has two different

182
00:12:39,332 --> 00:12:43,006
versions backing it, version one and version two. And so I can

183
00:12:43,028 --> 00:12:46,594
also see that same information in this applications view and

184
00:12:46,632 --> 00:12:49,906
I can click on this app UI, it gives

185
00:12:49,928 --> 00:12:53,198
me the little graph overview, but I can click on traffic,

186
00:12:53,374 --> 00:12:56,614
I can see all the inbound sources of data and the

187
00:12:56,652 --> 00:13:00,306
outbound destinations. I can see the protocol types

188
00:13:00,338 --> 00:13:03,494
and some metrics and their success rates on all these

189
00:13:03,532 --> 00:13:07,318
things. So right now I'm running a

190
00:13:07,324 --> 00:13:11,370
couple of for loops to just ping,

191
00:13:11,710 --> 00:13:15,558
simulate some user load here. And there's

192
00:13:15,574 --> 00:13:19,258
some problems with this load and so we're going to dig into that.

193
00:13:19,424 --> 00:13:22,906
So over here in the Grafana dashboard, I've opened

194
00:13:22,938 --> 00:13:26,574
up the service viewpoint and it

195
00:13:26,612 --> 00:13:30,606
shows me data and metrics about

196
00:13:30,628 --> 00:13:35,482
what's happening with my services. Right now I've got this user profile

197
00:13:35,546 --> 00:13:38,786
service selected. I could select one of the other services if I wanted

198
00:13:38,808 --> 00:13:42,594
to see data on that. But if I scroll down and

199
00:13:42,632 --> 00:13:45,670
see what's going on, I can right away see that these

200
00:13:45,740 --> 00:13:49,714
graphs show me incoming requests are getting satisfied very slowly

201
00:13:49,762 --> 00:13:53,462
in some cases. And we're seeing 20

202
00:13:53,516 --> 00:13:57,106
seconds for this user profile,

203
00:13:57,138 --> 00:14:00,458
two service to respond, and only three to

204
00:14:00,464 --> 00:14:04,538
five milliseconds for version one. So that's a problem.

205
00:14:04,704 --> 00:14:08,310
And I can see that same information via trace bands

206
00:14:08,470 --> 00:14:12,522
in this distributed tracing dashboard.

207
00:14:12,666 --> 00:14:16,590
So if I select the services for the user interface

208
00:14:16,930 --> 00:14:20,894
and the operation call to the profile and

209
00:14:20,932 --> 00:14:24,910
I click find traces, we'll see these drastically

210
00:14:24,990 --> 00:14:29,054
different bubbles taking a lot longer in these calls

211
00:14:29,182 --> 00:14:32,020
than these calls down at the bottom.

212
00:14:32,630 --> 00:14:36,690
And this tracing tool comes in really handy when you've got microservice

213
00:14:36,770 --> 00:14:40,754
chains that are like long calls

214
00:14:40,882 --> 00:14:44,546
that provide a return path

215
00:14:44,738 --> 00:14:47,926
to display some information on a GUI or something like that.

216
00:14:48,028 --> 00:14:51,974
And when things go wrong, it lets you dig down into the details

217
00:14:52,022 --> 00:14:55,722
and see exactly where the problem is. In this case, the problem

218
00:14:55,776 --> 00:14:59,546
is at the end of the chain. So it just looks like the

219
00:14:59,568 --> 00:15:02,614
bars are full. But if something happened in the middle,

220
00:15:02,752 --> 00:15:06,160
it would be really obvious and visualized very nicely to see that.

221
00:15:06,930 --> 00:15:10,894
But yeah, you can get a lot of information from this trace span, all the

222
00:15:10,932 --> 00:15:14,850
HTTP header information. And you can see again like

223
00:15:15,000 --> 00:15:18,178
we saw in Grafana, that this user profile two

224
00:15:18,344 --> 00:15:21,870
is causing us these long delays.

225
00:15:22,030 --> 00:15:25,458
And that looks like this on the app. If I click profile it's

226
00:15:25,474 --> 00:15:29,110
like oh man, it's chugging along, but nothing's happening. It's just

227
00:15:29,180 --> 00:15:32,406
ticking, ticking and ticking, and then finally it

228
00:15:32,428 --> 00:15:37,222
comes up. So those

229
00:15:37,276 --> 00:15:40,360
observability pieces have told us something's going wrong.

230
00:15:40,890 --> 00:15:43,962
So what we're going to do to fix that is we're going to go run

231
00:15:44,016 --> 00:15:46,060
some commands to apply some policy.

232
00:15:47,870 --> 00:15:51,834
So the first thing I'm going to do is create some destination

233
00:15:51,882 --> 00:15:55,518
rules and virtual services and

234
00:15:55,684 --> 00:15:59,754
I can see by using Openshift

235
00:15:59,802 --> 00:16:03,040
or kubectl type commands to see what those things are.

236
00:16:03,570 --> 00:16:06,786
And I can see I've got destination rules, and I've got

237
00:16:06,808 --> 00:16:11,474
virtual services now. And if

238
00:16:11,512 --> 00:16:14,610
I want to go and look at what that looks like in Kali now,

239
00:16:14,680 --> 00:16:18,438
things are a little bit different now that

240
00:16:18,444 --> 00:16:21,894
we're in Kali, I can go to this istio config and I can

241
00:16:21,932 --> 00:16:25,266
see all those different configuration items that we created.

242
00:16:25,378 --> 00:16:28,726
We'll notice that there's actually pretty nice capability where if

243
00:16:28,748 --> 00:16:32,346
you've got problems in this case, I've got an intentional problem in

244
00:16:32,368 --> 00:16:35,642
a destination rule. It's going to give you this error that tells you something's not

245
00:16:35,696 --> 00:16:39,274
right. But let's go back over and show you

246
00:16:39,312 --> 00:16:42,942
how we can apply some policy. We're going to

247
00:16:42,996 --> 00:16:46,478
change a destination to say, hey,

248
00:16:46,564 --> 00:16:50,734
we saw that we have this virtual service

249
00:16:50,932 --> 00:16:53,780
in Kiali that was, if you remember,

250
00:16:54,470 --> 00:16:57,380
splitting traffic between version one and version two.

251
00:16:58,070 --> 00:17:01,490
And we can see that by doing that.

252
00:17:01,640 --> 00:17:05,210
And if I want to flip,

253
00:17:05,390 --> 00:17:07,720
which I just applied. Oops, sorry.

254
00:17:09,450 --> 00:17:13,640
I can apply this configuration to flip all that

255
00:17:16,330 --> 00:17:19,980
traffic back from version two, which was giving us problems,

256
00:17:20,430 --> 00:17:25,606
to version one of our services. And I'm

257
00:17:25,638 --> 00:17:28,982
already throwing all this load in, so it should happen pretty immediately.

258
00:17:29,046 --> 00:17:32,378
And we should see that when I

259
00:17:32,384 --> 00:17:35,662
go to the profile now, it should just work. And I'm not getting those long

260
00:17:35,716 --> 00:17:38,979
delay problems right. So that works here. And I'm going to

261
00:17:39,479 --> 00:17:44,126
go to Kiali. We'll see that traffic has shifted over the

262
00:17:44,148 --> 00:17:47,300
last minute or so, and it'll eventually get up to 100%.

263
00:17:48,390 --> 00:17:52,082
So right away you can see how quickly

264
00:17:52,136 --> 00:17:56,194
you can fix a problem with routing by just changing the dynamic rules

265
00:17:56,242 --> 00:17:57,590
behind the scenes.

266
00:18:01,610 --> 00:18:05,638
So another thing I could do is deploy a third version of this service,

267
00:18:05,804 --> 00:18:08,866
and let's do that right now. I'm going

268
00:18:08,908 --> 00:18:12,058
to add a v three.

269
00:18:12,144 --> 00:18:15,898
So let's say we fixed that problem in v two. We want to

270
00:18:15,904 --> 00:18:19,574
go now ahead and patch that, what we had. So we'll

271
00:18:19,622 --> 00:18:23,066
run a command to create the user

272
00:18:23,098 --> 00:18:26,526
profile, v three. We'll take a check here and make sure

273
00:18:26,548 --> 00:18:30,590
it comes up and runs. It's almost

274
00:18:30,660 --> 00:18:34,480
there. It's trying to find itself a stable state.

275
00:18:34,850 --> 00:18:37,602
Cool. It looks like it's running. That's good.

276
00:18:37,736 --> 00:18:40,882
Now what we want to do is

277
00:18:41,016 --> 00:18:44,434
route traffic to it. But instead of making the same mistake we made

278
00:18:44,472 --> 00:18:48,286
last time, let's do a canary deployment, which is an advanced

279
00:18:48,318 --> 00:18:52,200
deployments technique to put just some of the traffic, but there.

280
00:18:52,730 --> 00:18:56,198
So we'll say, like, let's put 90% of the

281
00:18:56,204 --> 00:18:59,654
traffic still going to go to the one we know that works, but 10% will

282
00:18:59,692 --> 00:19:03,366
start shifting over to this newer service.

283
00:19:03,548 --> 00:19:06,886
And let's curl some. Actually, I'm already curling

284
00:19:06,918 --> 00:19:10,394
it, so it's getting traffic already. So now if I go back over

285
00:19:10,432 --> 00:19:14,186
to Kiali, I can see this version three is showed

286
00:19:14,218 --> 00:19:14,800
up,

287
00:19:17,490 --> 00:19:21,374
and it should start getting traffic pretty quickly

288
00:19:21,492 --> 00:19:24,858
now that we've applied

289
00:19:24,874 --> 00:19:28,094
that rule. So let's see.

290
00:19:28,132 --> 00:19:31,454
Yes, there we go. It's starting to shift up to 10% of that traffic

291
00:19:31,502 --> 00:19:35,282
over. And so it's just averaging the last 1 minute of data.

292
00:19:35,336 --> 00:19:38,398
So that's why it took a little while to catch up. But the traffic was

293
00:19:38,424 --> 00:19:41,526
already starting to show up there as soon as I hit enter on that

294
00:19:41,548 --> 00:19:45,826
command in the command line. And so let's

295
00:19:45,858 --> 00:19:49,142
go back over our toward. Let's start popping in.

296
00:19:49,196 --> 00:19:51,740
That's v three. That's v one.

297
00:19:52,270 --> 00:19:55,450
V one. V one. V one. V one. V one. V three.

298
00:19:55,520 --> 00:19:59,046
V one. Cool. So loading fast. We fixed

299
00:19:59,078 --> 00:20:02,798
that 20 to 32nd delay bug. Everything looks good.

300
00:20:02,884 --> 00:20:06,078
So now we could go ahead and shift it to like

301
00:20:06,084 --> 00:20:09,454
a 50 50 if we wanted to and

302
00:20:09,492 --> 00:20:12,800
see that. And then eventually we would just say, hey,

303
00:20:14,070 --> 00:20:17,666
let's put everything over to version three, which would look

304
00:20:17,688 --> 00:20:21,086
like that. Another traffic

305
00:20:21,118 --> 00:20:24,334
management capability I want to showcase is called circuit breaking.

306
00:20:24,462 --> 00:20:28,246
And if we go back to the profile page, we can see that

307
00:20:28,268 --> 00:20:31,382
right now I'm balancing traffic between version three

308
00:20:31,436 --> 00:20:35,154
of the profile service and version one of the profile

309
00:20:35,202 --> 00:20:37,830
service. And you can see there are different colors.

310
00:20:38,730 --> 00:20:42,122
And the circuit breaking concept is essentially that

311
00:20:42,256 --> 00:20:45,994
similar to a circuit breaker in your house, where if

312
00:20:46,032 --> 00:20:49,462
there's going to be a circuit overload, current is going to overload

313
00:20:49,526 --> 00:20:51,726
what's going on and create cause problems.

314
00:20:51,908 --> 00:20:54,682
Then it interrupts that flow.

315
00:20:54,826 --> 00:20:58,830
And in this case, if the network has detected

316
00:20:59,730 --> 00:21:02,954
failures that are happening at a certain threshold,

317
00:21:03,082 --> 00:21:06,866
we'll trip that circuit, and we'll prevent further calls from

318
00:21:06,968 --> 00:21:10,434
being made. We'll eject, essentially, that workload from the

319
00:21:10,472 --> 00:21:14,340
serviceable pool. So this looks kind of like this.

320
00:21:14,950 --> 00:21:18,422
We'll create a destination rule, and down

321
00:21:18,476 --> 00:21:21,526
here, we'll define an outlier section, and we'll say how

322
00:21:21,548 --> 00:21:25,462
many consecutive errors, and we'll define some other properties and

323
00:21:25,516 --> 00:21:29,582
policy details. And if I start sending

324
00:21:29,746 --> 00:21:33,706
load again and I

325
00:21:33,728 --> 00:21:36,620
go over to the kiali dashboard, I can see.

326
00:21:37,550 --> 00:21:40,586
Give it a second. There we go. So we're getting load, and you can see

327
00:21:40,608 --> 00:21:44,362
this 50 50 balance is happening. You can see also that I've created

328
00:21:44,426 --> 00:21:47,626
a. It's visualizing that there's a circuit

329
00:21:47,658 --> 00:21:51,406
breaker here, has circuit breaker. And if

330
00:21:51,428 --> 00:21:55,410
I wreak some havoc over in my cluster and just

331
00:21:55,480 --> 00:21:58,770
kick out delete that pod,

332
00:22:00,870 --> 00:22:04,946
the version three pod will just destroy it.

333
00:22:05,128 --> 00:22:09,234
So where's that going to be? This. So we killed version

334
00:22:09,282 --> 00:22:13,142
three. If I flip back over to here, we'll notice that version three

335
00:22:13,196 --> 00:22:15,400
will stop working in a second.

336
00:22:17,530 --> 00:22:21,802
And you can see here version three started

337
00:22:21,856 --> 00:22:25,980
getting errors and now the traffic is being

338
00:22:26,830 --> 00:22:30,614
sent back over to version one because that circuit

339
00:22:30,662 --> 00:22:32,910
breaker has tripped.

340
00:22:36,400 --> 00:22:39,916
Okay, so I flipped over here to the container platform dashboard to

341
00:22:39,938 --> 00:22:43,244
showcasing. A lot of things we've been doing are with the command line,

342
00:22:43,282 --> 00:22:46,944
but there's also graphical ways to do that. Right now we're looking

343
00:22:46,982 --> 00:22:50,284
at the control plane overview for the installed service mesh.

344
00:22:50,412 --> 00:22:53,628
And we can see that while we turned

345
00:22:53,644 --> 00:22:57,644
on all the observability capability, we didn't turn on security capabilities

346
00:22:57,692 --> 00:23:00,692
for the control plane or the data plane. And we want to do that now

347
00:23:00,746 --> 00:23:04,068
because we decided that maybe our policy

348
00:23:04,154 --> 00:23:08,256
should take into account the need to encrypt all the service to service communication.

349
00:23:08,448 --> 00:23:11,912
So if I go over here now, I can kind of visualize and

350
00:23:11,966 --> 00:23:16,520
showcase like with a simple little curl command that hey, anybody can

351
00:23:16,590 --> 00:23:20,520
jump on, run a container and

352
00:23:20,670 --> 00:23:23,948
get the data out of these microservices. So we're not

353
00:23:23,954 --> 00:23:28,632
really enforcing a strong identity in a nice secure

354
00:23:28,696 --> 00:23:32,476
way. So let's fix that by adding a

355
00:23:32,498 --> 00:23:35,984
peer authentication policy. And that's pretty straightforward. It just looks

356
00:23:36,022 --> 00:23:39,890
like this, we'll do a create command, and now

357
00:23:41,460 --> 00:23:44,688
I need to set some destination rules to tell the rest of the

358
00:23:44,694 --> 00:23:48,560
services to communicate using mutual tls.

359
00:23:49,220 --> 00:23:52,640
So I'll do that right now. That's going to create the destination rules.

360
00:23:52,800 --> 00:23:56,304
And the quick test, if I run that curl

361
00:23:56,352 --> 00:24:00,320
command again, we'll see that it failed because it wasn't

362
00:24:00,480 --> 00:24:04,104
a known identity, it wasn't a member of the mesh that was able to make

363
00:24:04,142 --> 00:24:05,560
these sorts of calls.

364
00:24:10,750 --> 00:24:15,114
And if I flip back over to the web console, I could have easily done

365
00:24:15,152 --> 00:24:19,214
that in a similar way just by turning the checkboxes on

366
00:24:19,332 --> 00:24:20,910
for the whole mesh.

367
00:24:22,370 --> 00:24:25,280
So that would be another way to do it.

368
00:24:25,730 --> 00:24:29,682
The last thing I want to showcase security wise is just

369
00:24:29,736 --> 00:24:33,246
a quick look at some of the additional resources

370
00:24:33,278 --> 00:24:37,198
that you can configure beyond just the mutual TLS.

371
00:24:37,294 --> 00:24:41,714
You can also have the services verify

372
00:24:41,842 --> 00:24:45,254
JSON web tokens for trust, and that

373
00:24:45,292 --> 00:24:48,200
would be through a request authentication policy.

374
00:24:48,730 --> 00:24:51,186
And in addition to authentication,

375
00:24:51,298 --> 00:24:55,462
authorization can be done via authorization policies.

376
00:24:55,606 --> 00:24:58,060
And those both look a little bit like that.

377
00:24:58,430 --> 00:25:01,738
So with that demo, that's all the things we

378
00:25:01,744 --> 00:25:03,020
have taught to demo today.

379
00:25:06,110 --> 00:25:10,094
We just scratched the service today so there's a whole lot more information.

380
00:25:10,212 --> 00:25:14,270
If you want to get deeper, I recommend you go to developers Red Hat

381
00:25:14,340 --> 00:25:18,122
check out the service mesh topic. We've got tutorials and articles

382
00:25:18,186 --> 00:25:21,774
and videos you can check out so it's a great resource for you.

383
00:25:21,892 --> 00:25:25,166
And if you want to ask me questions, reach out to me directly. Feel free

384
00:25:25,188 --> 00:25:29,370
to scan this QR code, it'll take you to my social accounts

385
00:25:29,530 --> 00:25:32,966
and also red hat links are below so you can find out more about

386
00:25:32,988 --> 00:25:34,660
red Hat. Thank you for watching.

