1
00:00:22,090 --> 00:00:25,698
Hi, I'm Nicola Pietroluongo, AWS senior solution architect

2
00:00:25,794 --> 00:00:29,394
and one of my responsibilities to help customers innovate and unlock

3
00:00:29,442 --> 00:00:32,694
new possibilities with machine learning. Today I'm going to talk

4
00:00:32,732 --> 00:00:36,498
about how to deploy your machine learning model as a serverless API.

5
00:00:36,594 --> 00:00:40,070
Let's start to say that only a small fraction of real

6
00:00:40,140 --> 00:00:44,086
world machine learning systems is composed of the machine learning code.

7
00:00:44,188 --> 00:00:47,250
There is a vast and sometimes complex infrastructure

8
00:00:47,330 --> 00:00:50,954
surrounding a machine learning workload with its own challenges.

9
00:00:51,082 --> 00:00:54,906
What if we can minimize or remove some concerns? And I'm

10
00:00:54,938 --> 00:00:58,846
talking about issues around model deployment and serving the part

11
00:00:58,868 --> 00:01:02,686
when you have done all your work of creating a machine learning model and

12
00:01:02,708 --> 00:01:06,626
you are going to face questions as where to host your model to

13
00:01:06,648 --> 00:01:10,146
serve predictions at scale cost effectively, and how to do

14
00:01:10,168 --> 00:01:14,334
that in the easiest way. The answer to where to host is AWS

15
00:01:14,382 --> 00:01:18,358
Lambda, a serverless compute service that lets you run code without

16
00:01:18,444 --> 00:01:22,614
provisioning or managing servers. AWS Lambda has many benefits

17
00:01:22,732 --> 00:01:26,066
as subsequent automatic scaling, high availability,

18
00:01:26,178 --> 00:01:29,510
and a bay as you go model. But more importantly for this session,

19
00:01:29,590 --> 00:01:32,998
it supports function deployed as container images.

20
00:01:33,094 --> 00:01:36,554
In other words, you can bundle a machine learning model and

21
00:01:36,592 --> 00:01:40,758
its code in a container image and deploy as a serverless API.

22
00:01:40,854 --> 00:01:44,186
Now that we know the where, let's see how to deploy a container

23
00:01:44,218 --> 00:01:47,642
on lambda. The answer to how is to use the AWS serverless

24
00:01:47,706 --> 00:01:51,086
application model or AWS SAM. It is

25
00:01:51,108 --> 00:01:54,610
an open source framework that you can use to build serverless application

26
00:01:54,680 --> 00:01:58,206
on AWS. It provides a single deployment configuration,

27
00:01:58,318 --> 00:02:01,358
built in, best practices, local debugging,

28
00:02:01,454 --> 00:02:04,722
testing, and more. AWS SAM allows you

29
00:02:04,776 --> 00:02:08,030
to define your serverless application using a template.

30
00:02:08,190 --> 00:02:12,658
Sum template are an extension of AWS cloud formation templates.

31
00:02:12,754 --> 00:02:16,002
AWS Cloudformation is a service that allow you to provision

32
00:02:16,066 --> 00:02:19,734
infrastructure as a code. Here you can see a sample template composed

33
00:02:19,782 --> 00:02:23,674
of three blocks. The first block instructs cloudformation to

34
00:02:23,712 --> 00:02:27,178
perform serverless transformation. The central block creates a

35
00:02:27,184 --> 00:02:31,310
lambda function connected with Amazon API gateway with code

36
00:02:31,380 --> 00:02:35,518
and all necessary permission. API Gateway is a fully managed service

37
00:02:35,604 --> 00:02:39,802
that handles all the tasks involved in accepting and processing

38
00:02:39,866 --> 00:02:43,614
API calls. Last block helps AWS SAM

39
00:02:43,662 --> 00:02:47,278
to manage the container images. To summarize,

40
00:02:47,374 --> 00:02:51,394
with few lines of code, this template creates all we need to

41
00:02:51,432 --> 00:02:55,454
run a serverless machine learning container. It exposes a public endpoint

42
00:02:55,502 --> 00:02:59,378
we can call using a post request to run inferences. We don't

43
00:02:59,394 --> 00:03:03,142
need to worry about all the configurations, role and permissions needed

44
00:03:03,196 --> 00:03:06,758
by those resources to run. SAM will help us on that.

45
00:03:06,844 --> 00:03:10,186
This is effectively what we're using to create in this talk,

46
00:03:10,288 --> 00:03:14,374
but without writing one line of code. Because AWS

47
00:03:14,422 --> 00:03:17,754
SAM provides comma line tool the SAM CLI, they make

48
00:03:17,792 --> 00:03:21,530
it easy to create, manage and test serverless applications,

49
00:03:21,610 --> 00:03:24,506
and it's available for Linux, Windows and macOS.

50
00:03:24,618 --> 00:03:27,914
You can use the CLI to build, validate and test serverless

51
00:03:27,962 --> 00:03:31,694
application and integrate with added resources as well as

52
00:03:31,732 --> 00:03:35,278
database. Let's see what we can run with the SAM CLI.

53
00:03:35,374 --> 00:03:39,534
We can use SAM init to generate pre configured template SAM

54
00:03:39,582 --> 00:03:43,154
package to create a deployment package build and

55
00:03:43,192 --> 00:03:46,738
deploy as you can imagine to build and deploy an application.

56
00:03:46,904 --> 00:03:50,770
Finally, some local to test the application locally.

57
00:03:50,850 --> 00:03:54,550
Now it's time to see this tool in action and deploy a machine learning model

58
00:03:54,620 --> 00:03:57,926
as serverless API I'm going to show how to use the

59
00:03:57,948 --> 00:04:01,734
SAm CLI to create a machine learning container running on AWS

60
00:04:01,782 --> 00:04:05,690
lambda exposed via Amazon API gateway. In this way,

61
00:04:05,760 --> 00:04:09,654
a client application can make requests to API gateway which will invoke

62
00:04:09,702 --> 00:04:13,418
the lambda function and return the inference output.

63
00:04:13,594 --> 00:04:16,874
We are going to generate the template with some init,

64
00:04:17,002 --> 00:04:20,894
build the solution with some build, test locally with

65
00:04:20,932 --> 00:04:24,546
some local create a container registry. This is optional if you

66
00:04:24,568 --> 00:04:27,634
already have a container registry. Deploy with some,

67
00:04:27,672 --> 00:04:31,586
deploy and finally test the deployment. The first step

68
00:04:31,688 --> 00:04:35,060
is to run some init to initialize the project.

69
00:04:35,510 --> 00:04:39,590
First we need to choose between an AWS quick start template or a custom

70
00:04:39,660 --> 00:04:42,738
one, and I'm going to choose quick start template.

71
00:04:42,914 --> 00:04:46,006
The next question is about the package we would like to use.

72
00:04:46,108 --> 00:04:49,586
The choice is between a zip file or a container image.

73
00:04:49,698 --> 00:04:53,402
The choice will be an image. AWS provides already base

74
00:04:53,456 --> 00:04:56,794
images for common tasks, but you have the possibility to

75
00:04:56,832 --> 00:05:00,650
create your own. For this demo, I'm going to use the Python three

76
00:05:00,720 --> 00:05:03,886
eight base image. Now it's time to select a

77
00:05:03,908 --> 00:05:07,354
name for the project. I'm going to accept the default sum

78
00:05:07,402 --> 00:05:10,666
app. At this point, sum is fetching the required file

79
00:05:10,698 --> 00:05:14,642
to create the app. The final step is to select which type

80
00:05:14,696 --> 00:05:17,794
of application we would like to run. As you can see, there are some

81
00:05:17,832 --> 00:05:22,142
pre configured scenarios to get started quickly. Hello word Pythorch

82
00:05:22,206 --> 00:05:26,650
scikit tensorflow xgboost I'm going to choose Pythorch

83
00:05:26,750 --> 00:05:30,434
machine learning inference API. At this stage, Sam generates

84
00:05:30,482 --> 00:05:34,166
a directory with all the required file to run the application.

85
00:05:34,348 --> 00:05:37,942
Let's move inside the application directory. I'm going to use the three

86
00:05:37,996 --> 00:05:41,286
command to show the directory structure. Here you

87
00:05:41,308 --> 00:05:45,318
can see all the file generated by Sam. The quick start I've chose

88
00:05:45,414 --> 00:05:49,638
contains a sample model to identify handwritten digits.

89
00:05:49,814 --> 00:05:53,246
There is some template yaml to generate the infrastructure we

90
00:05:53,268 --> 00:05:56,686
saw before. An example of it, a sample training file to train the

91
00:05:56,708 --> 00:05:59,562
model, an event file to test the API,

92
00:05:59,626 --> 00:06:03,246
and more. Two files that are particularly important

93
00:06:03,428 --> 00:06:06,302
are the docker file to build the container,

94
00:06:06,446 --> 00:06:09,618
the app file which contains the code. Let's inspect the

95
00:06:09,624 --> 00:06:13,534
docker file. You can see here the base image previously

96
00:06:13,582 --> 00:06:17,474
selected in the from statement and other statements to bundle

97
00:06:17,522 --> 00:06:21,442
and run the application. At the very bottom you can see the docker CMD

98
00:06:21,506 --> 00:06:25,014
command that specifies what needs to be executed when

99
00:06:25,052 --> 00:06:28,674
docker container starts. In this case, it will execute

100
00:06:28,722 --> 00:06:32,550
the function called lambda handler inside the app file.

101
00:06:32,630 --> 00:06:35,898
So let's have a look at the app file. As you can see is a

102
00:06:35,904 --> 00:06:39,574
python file with all required statements to run inference,

103
00:06:39,702 --> 00:06:42,814
preprocessing steps, model load and here

104
00:06:42,852 --> 00:06:46,414
you can see the lambda handler function which runs the inference and

105
00:06:46,452 --> 00:06:50,846
returns a JSON output with a prediction back

106
00:06:50,868 --> 00:06:54,226
to the main folder. Now it's time to

107
00:06:54,248 --> 00:06:57,826
build the application with sum build. In short,

108
00:06:57,928 --> 00:07:01,710
this operation build and tag a docker container locally

109
00:07:01,870 --> 00:07:05,506
before we saw a file called event JSON which

110
00:07:05,528 --> 00:07:09,298
can be used to test the application. Let's zoom

111
00:07:09,314 --> 00:07:12,806
out a bit. The file contains a JSON request with

112
00:07:12,828 --> 00:07:16,482
a payload body which is the base 64 representation

113
00:07:16,546 --> 00:07:19,786
of an image. I can decode the body and show

114
00:07:19,808 --> 00:07:23,686
you the image with this statement. As you can see it's the representation

115
00:07:23,718 --> 00:07:28,150
of a ramp tree. Let's try to test the application locally

116
00:07:28,230 --> 00:07:31,774
with some invoke using the event JSON file which

117
00:07:31,812 --> 00:07:35,360
contains the representation of the number three as we saw before.

118
00:07:37,650 --> 00:07:41,354
As you can see, the response is exactly what we expected predicted.

119
00:07:41,402 --> 00:07:44,574
Label the number three to recap we

120
00:07:44,612 --> 00:07:47,786
used the quick start to generate the code and related assets.

121
00:07:47,898 --> 00:07:51,694
We builtin the docker image and test the container locally.

122
00:07:51,822 --> 00:07:56,238
Now we are entering in the deployment phase to

123
00:07:56,264 --> 00:07:59,522
deploy the solution. We need to make our local container

124
00:07:59,586 --> 00:08:03,346
available for cloud resources. One way is to create an Amazon

125
00:08:03,378 --> 00:08:06,866
elastic container registry or ECR and push

126
00:08:06,898 --> 00:08:10,166
the docker image there. ECR is a fully managed

127
00:08:10,198 --> 00:08:13,846
service to store, manage and deploy container images.

128
00:08:13,958 --> 00:08:17,078
We need to authenticate the request before creating

129
00:08:17,094 --> 00:08:20,886
the registry in ECR and this statement allows

130
00:08:20,918 --> 00:08:24,526
us to retrieve temporary credential. This series of commands are

131
00:08:24,548 --> 00:08:27,450
run with the AWS CLI, not the AWS sum.

132
00:08:27,530 --> 00:08:30,942
You might notice that we need to substitute region and account id

133
00:08:30,996 --> 00:08:34,046
in this statement. I've reducted some parts,

134
00:08:34,078 --> 00:08:37,902
but this is how the request might look like. The authentication

135
00:08:37,966 --> 00:08:41,982
is successful. Now we can create a repository called ML

136
00:08:42,046 --> 00:08:45,262
demo with this command. AWS ECR

137
00:08:45,326 --> 00:08:48,966
create repository this is the output and

138
00:08:48,988 --> 00:08:52,662
we need to copy the repository URI which will be used during

139
00:08:52,716 --> 00:08:56,502
the deployment phase. The final step is to run some

140
00:08:56,556 --> 00:08:59,590
deploy guided and follow the instructions.

141
00:09:00,250 --> 00:09:03,130
The first step is to bow to give a name to the stack.

142
00:09:03,470 --> 00:09:06,778
The second is about choosing the region in which the

143
00:09:06,784 --> 00:09:10,286
stack will be deployed. In the next step, we need to

144
00:09:10,308 --> 00:09:14,126
specify the docker image and we're going to use the uri we

145
00:09:14,148 --> 00:09:17,566
saw before a confirmation step to

146
00:09:17,588 --> 00:09:21,530
apply the changes. Some will need permissions

147
00:09:21,610 --> 00:09:24,846
to set up the resources. This step tells

148
00:09:24,878 --> 00:09:28,862
us that our API doesn't have any authorization method.

149
00:09:29,006 --> 00:09:32,386
This is okay for this demo, but it's good practice to secure the

150
00:09:32,408 --> 00:09:35,666
access. All those choices are going

151
00:09:35,688 --> 00:09:40,230
to be saved in a configuration file which will make the next deployment faster.

152
00:09:40,650 --> 00:09:44,454
Let's accept the default name for the config file and

153
00:09:44,492 --> 00:09:47,422
keep a default environment for this configuration.

154
00:09:47,586 --> 00:09:51,082
At this stage, SAM is pushing the docker image we built

155
00:09:51,136 --> 00:09:53,770
locally into ECR repository.

156
00:09:54,830 --> 00:09:57,690
Finally, everything is ready to be deployed.

157
00:09:59,070 --> 00:10:02,842
As you can see, the stack has been created successfully. That's wonderful.

158
00:10:02,906 --> 00:10:06,270
Job done. Sam created all resources for us,

159
00:10:06,340 --> 00:10:09,722
the API gateway and the lambda function. We saw earlier

160
00:10:09,786 --> 00:10:12,926
that the API gateway is publicly exposed and can

161
00:10:12,948 --> 00:10:16,578
be used by an application to run inferences. So if we want

162
00:10:16,584 --> 00:10:19,902
to test our cloud stack, we need to grab the API gateway

163
00:10:19,966 --> 00:10:24,126
endpoint which is actually part of this output. So scrolling

164
00:10:24,158 --> 00:10:26,790
up a bit we can see the API endpoint.

165
00:10:28,090 --> 00:10:31,494
Let's clean up a bit terminal and as

166
00:10:31,532 --> 00:10:35,318
a very final step we can test our serverless machine learning model,

167
00:10:35,404 --> 00:10:39,158
making a request against the API endpoint. You can see we

168
00:10:39,164 --> 00:10:42,954
can use curl to create a post request and send the base 64

169
00:10:42,992 --> 00:10:46,390
representation of the number three. Let's send a request

170
00:10:46,550 --> 00:10:50,390
and celebrate. The output of the inference is what we expected

171
00:10:50,550 --> 00:10:54,394
to recap. We use the sum CLI to generate and deploy a serverless

172
00:10:54,442 --> 00:10:57,854
machine learning model. The model and the application code has

173
00:10:57,892 --> 00:11:01,518
been bundled in a container and deployed in a lambda function

174
00:11:01,604 --> 00:11:04,846
which resides in a private network, while an API

175
00:11:04,878 --> 00:11:08,446
gateway has been deployed to handle public API requests.

176
00:11:08,558 --> 00:11:12,082
As a final conclusion, before getting into serverless machine learning

177
00:11:12,136 --> 00:11:15,890
solutions, you need to carefully validate your use case and define clear

178
00:11:15,960 --> 00:11:19,602
KPIs. With the serverless approach, you run code with zero

179
00:11:19,656 --> 00:11:23,238
administration and with the pay as you go model, you don't have to

180
00:11:23,244 --> 00:11:27,202
pay for unused server time. Moreover, you benefit from continuous

181
00:11:27,266 --> 00:11:30,854
scaling. To date, serverless machine learning solutions

182
00:11:30,902 --> 00:11:34,586
are more suited when performances are not a big concern and

183
00:11:34,608 --> 00:11:38,374
when you work with batch processing since everything runs

184
00:11:38,422 --> 00:11:42,058
independently in parallel. So it's important to be aware

185
00:11:42,074 --> 00:11:45,822
of the service quotas to see if they affect your use case.

186
00:11:45,956 --> 00:11:49,690
For instance, AWS Lambda supports container images

187
00:11:49,770 --> 00:11:53,626
of up to ten gigabyte in size. And this leads

188
00:11:53,658 --> 00:11:57,754
us to the final conclusion, which is to continuously test and validate

189
00:11:57,802 --> 00:12:01,214
your assumptions and AWS. Sam surely gives

190
00:12:01,252 --> 00:12:05,058
an advantage in terms of fast prototyping and experimentation.

191
00:12:05,194 --> 00:12:08,500
That's great if you want to innovate faster. Thank you.

