1
00:00:22,010 --> 00:00:26,486
Hi everyone, welcome to monitoring pipelines, AI pipelines as

2
00:00:26,588 --> 00:00:30,274
product my name is Hila Fox and I'm a squad

3
00:00:30,322 --> 00:00:33,666
leader and a backend developer at Auguri, currently leading

4
00:00:33,778 --> 00:00:37,346
a squad that is responsible for taking the AI insights

5
00:00:37,378 --> 00:00:41,414
from the engine and distributing them to different end products.

6
00:00:41,612 --> 00:00:44,806
So let's talk about the agenda. We're going to talk

7
00:00:44,828 --> 00:00:48,214
about Auguri as a company and the product. We're going to talk about

8
00:00:48,252 --> 00:00:51,866
machine health AI and how we do it, the detection

9
00:00:51,898 --> 00:00:55,646
management layer and why it's important monitoring what we

10
00:00:55,668 --> 00:00:59,454
want to achieve using monitoring, the hybrid approach we took and being

11
00:00:59,492 --> 00:01:02,766
proactive. And in the end, the conclusions. First of

12
00:01:02,788 --> 00:01:05,818
all, let's talk about augury. Augury is a ten year

13
00:01:05,844 --> 00:01:08,946
old startup in the machine health business and this is

14
00:01:08,968 --> 00:01:13,006
in the manufacturing industry. Like it says on the slide, the world runs

15
00:01:13,038 --> 00:01:16,886
on machines and we're on the mission to make them reliable. We do

16
00:01:16,908 --> 00:01:20,294
this by giving our customers a full SaaS solution that

17
00:01:20,332 --> 00:01:23,746
includes avoiding unplanned downtime, aggregated insights

18
00:01:23,778 --> 00:01:27,426
and line of expert vibration analysts. This helps our

19
00:01:27,468 --> 00:01:30,646
customers reach resilient production lines.

20
00:01:30,758 --> 00:01:34,422
Something we saw is very important during COVID

21
00:01:34,566 --> 00:01:38,442
With the increase in demand and also having no

22
00:01:38,496 --> 00:01:41,630
or less people on site, this became very,

23
00:01:41,700 --> 00:01:45,326
very important. We have a lot of customers which

24
00:01:45,348 --> 00:01:48,522
are enterprise companies like P. G. Fridolay,

25
00:01:48,586 --> 00:01:52,094
Pfizer and Roseburg, and even that if it's not

26
00:01:52,132 --> 00:01:55,902
written here in the slide. But we also have Heineken

27
00:01:55,966 --> 00:01:59,618
and Sat, which keeps our beer and our toilet paper

28
00:01:59,704 --> 00:02:03,362
coming even in these hard days. We are operating in

29
00:02:03,416 --> 00:02:07,154
the US, in Israel and Europe, and we are expanding.

30
00:02:07,282 --> 00:02:11,190
So how does it work? Augury's main flow starts from

31
00:02:11,340 --> 00:02:14,930
our IoT devices. We make our IoT

32
00:02:15,010 --> 00:02:18,306
devices and these are sensors. We have three

33
00:02:18,348 --> 00:02:21,334
types of sensors in these devices, vibration,

34
00:02:21,462 --> 00:02:25,366
temperature and magnetic fields. And we monitor our machines

35
00:02:25,478 --> 00:02:29,146
24/7 once the data is recorded, we pass it

36
00:02:29,168 --> 00:02:32,814
on to the cloud, into our AI engine to

37
00:02:32,852 --> 00:02:36,270
get diagnosed. With this AI engine,

38
00:02:36,420 --> 00:02:39,594
we also have a line of expert

39
00:02:39,642 --> 00:02:43,194
vibration analysts to give even more precise

40
00:02:43,322 --> 00:02:46,994
diagnosis. Once we have the diagnosis, we need to visualize it

41
00:02:47,032 --> 00:02:50,494
and communicate it to the customers. And we do this via

42
00:02:50,542 --> 00:02:53,746
web, mobile, emails, SMS and more.

43
00:02:53,848 --> 00:02:57,458
So let's just see, how does it look? We can see here

44
00:02:57,544 --> 00:03:01,714
in some manufacturing plant. We have here three and a half pumps.

45
00:03:01,842 --> 00:03:05,862
The one at the back is a bit cut off, but we have here four

46
00:03:05,916 --> 00:03:09,402
pumps. Each of the pumps have our sensors installed on it

47
00:03:09,456 --> 00:03:12,730
and the sensors are communicating with what you can see

48
00:03:12,800 --> 00:03:16,074
at the upper left corner. This is also

49
00:03:16,112 --> 00:03:19,654
our device. We develop it's called a node.

50
00:03:19,702 --> 00:03:23,374
And what it does, it communicates to the sensors with

51
00:03:23,412 --> 00:03:27,102
Bluetooth, aggregates the information and sends it

52
00:03:27,156 --> 00:03:30,446
to the cloud. This is a snapshot of one

53
00:03:30,468 --> 00:03:34,178
of the machines, and we can see that on each machine we have

54
00:03:34,264 --> 00:03:37,282
four sensors, and we have two sensors for

55
00:03:37,336 --> 00:03:40,580
each component. In this picture, we can see

56
00:03:40,950 --> 00:03:44,542
one component, which is a motor, and another component,

57
00:03:44,606 --> 00:03:48,194
which is a driven pump. So, machine health AI,

58
00:03:48,322 --> 00:03:51,926
we talked about how we install on the machines and

59
00:03:51,948 --> 00:03:55,714
how we collect the data. Let's talk about the AI and the complexity

60
00:03:55,842 --> 00:03:59,740
in it. First of all, some numbers, because numbers

61
00:04:00,110 --> 00:04:04,406
helps us understand the amount of data that we have and the complexity

62
00:04:04,518 --> 00:04:08,138
we are tackling. So we have over 50 million

63
00:04:08,224 --> 00:04:11,486
hour of machines monitoring 80,000

64
00:04:11,588 --> 00:04:14,714
machines, diagnosed, multiple customers with global

65
00:04:14,762 --> 00:04:17,882
expansion, dozens of thousands of machines.

66
00:04:18,026 --> 00:04:21,854
We also have dozens of machine learning algorithms, which are

67
00:04:21,892 --> 00:04:24,878
based on time series deep neural networks,

68
00:04:24,974 --> 00:04:28,354
MLP decision trees, and more. On top of that,

69
00:04:28,392 --> 00:04:31,938
we have three product squads, which are developing the

70
00:04:32,024 --> 00:04:36,706
customer facing products which are using these

71
00:04:36,808 --> 00:04:40,786
insights, and also three algo squads and a data engineering

72
00:04:40,818 --> 00:04:44,326
team that work on the AA engine itself.

73
00:04:44,508 --> 00:04:48,578
So, let's take a deeper dive into the whole AI flow.

74
00:04:48,674 --> 00:04:52,554
I'm not going to bother you with the specific calculations, but for

75
00:04:52,592 --> 00:04:56,282
each machine, we collect 1.3 points per

76
00:04:56,336 --> 00:04:59,894
hour per machine, and we send it to the cloud.

77
00:05:00,032 --> 00:05:03,790
In the cloud, we first of all reach the transformation process.

78
00:05:03,940 --> 00:05:08,080
The transformation process is built

79
00:05:08,450 --> 00:05:12,110
from a validity algorithm, and afterwards

80
00:05:12,470 --> 00:05:15,794
it calibrates the data from electrical units to

81
00:05:15,832 --> 00:05:19,602
real physical units like acceleration velocity and more.

82
00:05:19,736 --> 00:05:22,770
After we have calibrated our data,

83
00:05:22,920 --> 00:05:26,406
we pass it on to our feature extraction pipeline, which is also a

84
00:05:26,428 --> 00:05:30,806
model. This is actually a dimensional reduction technique to

85
00:05:30,828 --> 00:05:34,662
capture the essential parameters for

86
00:05:34,716 --> 00:05:38,214
machine health. We collect roughly around 1000 feature

87
00:05:38,262 --> 00:05:42,010
per hour per machine and save it and pass it on

88
00:05:42,080 --> 00:05:45,926
to, of course, another machine learning algorithm,

89
00:05:46,038 --> 00:05:49,574
which is time series. In these time series

90
00:05:49,622 --> 00:05:54,174
algorithms, we collect and calculate features which are relative to themselves.

91
00:05:54,372 --> 00:05:58,314
Once we have this information, this is passed on to our ML

92
00:05:58,362 --> 00:06:02,238
platform. The ML platform is designed with two

93
00:06:02,324 --> 00:06:05,620
layers, two major layers. The first one is

94
00:06:06,150 --> 00:06:09,678
not diagnosed. The first one is for high recall,

95
00:06:09,854 --> 00:06:12,974
meaning we want to never miss a machine

96
00:06:13,022 --> 00:06:16,486
health issue. We want to always alert our customers when there

97
00:06:16,508 --> 00:06:18,630
is something going on with their machine.

98
00:06:19,850 --> 00:06:23,026
This is called anomaly detections algorithms.

99
00:06:23,138 --> 00:06:26,360
The other types of detectors that we have is

100
00:06:26,810 --> 00:06:30,438
fault detectors, which means specific faults

101
00:06:30,534 --> 00:06:34,534
that can happen on a machine. So the anomaly detector

102
00:06:34,582 --> 00:06:38,806
is a semisupervised machine learning algorithm,

103
00:06:38,918 --> 00:06:42,174
which actually calculates a relative baseline for

104
00:06:42,212 --> 00:06:46,698
each machine. And this is very important because it actually compares

105
00:06:46,874 --> 00:06:50,414
the data according to the same machine states, meaning we

106
00:06:50,452 --> 00:06:54,114
are finding anomalies per machine and not with our

107
00:06:54,152 --> 00:06:57,854
whole machine pool. The other detections, like we said, are fault

108
00:06:57,902 --> 00:07:02,226
detections and these are actually specific

109
00:07:02,328 --> 00:07:05,918
faults which are identified by a specific signature

110
00:07:06,014 --> 00:07:09,526
which is correlated with the fault. Each hour all

111
00:07:09,548 --> 00:07:13,382
of the detectors generate detections and output them

112
00:07:13,436 --> 00:07:17,394
to the detection management layer. Each detection has a confidence

113
00:07:17,442 --> 00:07:21,226
and a confidence is pretty similar to the probability from

114
00:07:21,248 --> 00:07:25,094
the machine learning algorithm. And once it reaches the detection

115
00:07:25,142 --> 00:07:28,202
management layer, it gets handled in there.

116
00:07:28,336 --> 00:07:32,214
What's amazing is that actually 99.87%

117
00:07:32,272 --> 00:07:35,786
of the detections are not being passed on from the detection

118
00:07:35,818 --> 00:07:39,214
management layer to the customers. And this is amazing because this

119
00:07:39,252 --> 00:07:43,322
helps us give our customers only the relevant information

120
00:07:43,476 --> 00:07:46,290
they need to actually handle their machines.

121
00:07:46,790 --> 00:07:50,418
Some of the detection are being passed on directly to the customers

122
00:07:50,504 --> 00:07:53,714
and some of them are passed on to our

123
00:07:53,752 --> 00:07:57,554
analysts. When our analysts are adding labeling

124
00:07:57,682 --> 00:08:01,346
on the detections, we can actually afterwards use this to retrain

125
00:08:01,378 --> 00:08:04,774
our algorithms. And this is a big

126
00:08:04,812 --> 00:08:08,950
picture of the whole flow together. So we talked about

127
00:08:09,100 --> 00:08:12,650
the AA engine from the inside of it, but now

128
00:08:12,720 --> 00:08:16,822
let's see how we use it in the product because this is very correlated

129
00:08:16,886 --> 00:08:20,406
to what I'm going to talk about as we move on with

130
00:08:20,448 --> 00:08:23,822
the presentation. So we have here two main

131
00:08:23,876 --> 00:08:27,082
usages for detections, and this is a real snapshot

132
00:08:27,226 --> 00:08:30,410
from the website from a specific machine.

133
00:08:30,570 --> 00:08:33,890
And we have two graphs over here. The graph at the top,

134
00:08:33,960 --> 00:08:37,826
like its name, says, it's machine health events. It's events happening

135
00:08:37,928 --> 00:08:41,522
on the machine's lifeline as we go along.

136
00:08:41,656 --> 00:08:45,070
And it's pretty self explanatory. Green is good,

137
00:08:45,160 --> 00:08:49,842
red is danger. And we can see the gray circles

138
00:08:49,906 --> 00:08:53,158
on the graph and each gray circle actually means something

139
00:08:53,244 --> 00:08:57,474
happened. And in our case, those gray circles

140
00:08:57,522 --> 00:09:01,466
are detections. Each time a detection is being propagated out

141
00:09:01,488 --> 00:09:05,926
of the detection management layer, it reaches our vibration analysts,

142
00:09:06,038 --> 00:09:10,026
which decides if it actually caused a change in the

143
00:09:10,048 --> 00:09:14,122
machine's health. Once it does, the analyst

144
00:09:14,266 --> 00:09:17,918
says it does, and then it notifies our customers and the

145
00:09:17,924 --> 00:09:21,806
customers can choose to take actions accordingly. We can see here that

146
00:09:21,828 --> 00:09:25,762
a customer decided to perform a repair on the machine and after the repair actually

147
00:09:25,816 --> 00:09:29,394
the machine health went back to green. So this

148
00:09:29,432 --> 00:09:32,866
is very good. The second graph that we are seeing over here

149
00:09:32,968 --> 00:09:37,142
is actually detector confidences over

150
00:09:37,196 --> 00:09:40,706
time. And this is very interesting because I chose

151
00:09:40,738 --> 00:09:44,582
to show here the bearing wear confidence output over time.

152
00:09:44,636 --> 00:09:48,066
And we can see it's very correlated with what's

153
00:09:48,098 --> 00:09:51,434
going on in the graph above on

154
00:09:51,472 --> 00:09:54,954
each detection that is actually propagated to our customers,

155
00:09:55,072 --> 00:09:58,774
we can see an increase in the confidence of this specific fault

156
00:09:58,822 --> 00:10:02,602
of bearing wear, which means that in high probability,

157
00:10:02,746 --> 00:10:06,798
this is probably what's going on in their machine. So we can also

158
00:10:06,964 --> 00:10:10,734
notify them that there is something going on, but also give

159
00:10:10,772 --> 00:10:14,234
them a specific confidence to what the specific fault

160
00:10:14,282 --> 00:10:17,330
is. So the detection management layer,

161
00:10:17,910 --> 00:10:21,342
we talked about the AI engine and the overall flows,

162
00:10:21,486 --> 00:10:24,818
and we talked about how we use it in the product. So let's do a

163
00:10:24,824 --> 00:10:28,546
deeper dive into the detection management layer. So it's

164
00:10:28,578 --> 00:10:32,534
important because of two main reasons. And the first one is that

165
00:10:32,572 --> 00:10:35,734
it connects the AI engine to

166
00:10:35,772 --> 00:10:39,222
the customer facing product. Like this diagram shows

167
00:10:39,276 --> 00:10:42,854
us, we have the AI engine on the right, and it generates

168
00:10:42,902 --> 00:10:46,326
all of the detections going downstream to the detection

169
00:10:46,358 --> 00:10:50,106
management layer and then distributed to different end products.

170
00:10:50,288 --> 00:10:54,058
So we can even call this maybe a single point of failure.

171
00:10:54,234 --> 00:10:57,626
So, being confident that the detection management layer

172
00:10:57,658 --> 00:11:01,120
is working as we would expect it to be, is very important.

173
00:11:02,450 --> 00:11:06,186
It's also a very delicate area because it has multiple consumers

174
00:11:06,218 --> 00:11:10,114
and multiple, and it

175
00:11:10,152 --> 00:11:14,062
consumes from multiple producers and also product to multiple.

176
00:11:14,126 --> 00:11:17,654
Yes, you got me. Okay, that's cool. It's complicated. That's the

177
00:11:17,692 --> 00:11:20,982
point. Another important issue here

178
00:11:21,036 --> 00:11:24,754
that algo, it contains logic and it makes decisions

179
00:11:24,802 --> 00:11:28,214
onto where to propagate to. So it

180
00:11:28,252 --> 00:11:31,434
makes this component very important in

181
00:11:31,472 --> 00:11:35,098
our flow. So we need to be confident in the changes we make.

182
00:11:35,184 --> 00:11:38,726
And as I sit, we have two type of changes. We have expected

183
00:11:38,758 --> 00:11:43,114
changes and unexpected changes. Expected changes are new features

184
00:11:43,162 --> 00:11:47,134
and we can actually mitigate the risks over there

185
00:11:47,252 --> 00:11:50,926
by testing in staging environment or even running in

186
00:11:50,948 --> 00:11:54,146
dry run in production, using feature switches and writing to

187
00:11:54,168 --> 00:11:57,950
logs without making real changes, will affect

188
00:11:58,030 --> 00:12:01,506
our customers. But the other types of changes that

189
00:12:01,528 --> 00:12:05,074
we have, which is in my opinion a little bit

190
00:12:05,112 --> 00:12:08,366
more interesting, is the expected changes, all sorts

191
00:12:08,398 --> 00:12:11,798
of bugs. So in the AI engine itself, we can

192
00:12:11,884 --> 00:12:15,302
enhance our visibility to see if there are things

193
00:12:15,356 --> 00:12:19,446
going on in the engine itself. But we also have the detection management

194
00:12:19,478 --> 00:12:23,206
layer, which creates logic, but also consumes

195
00:12:23,318 --> 00:12:26,778
the information from the AA engine. So we can add

196
00:12:26,864 --> 00:12:30,726
matrix over here was well, but what are we trying to achieve?

197
00:12:30,918 --> 00:12:35,406
So our motivation is to avoid product issues.

198
00:12:35,588 --> 00:12:39,022
It's from simple bugs to bad deployments. And when I'm saying

199
00:12:39,076 --> 00:12:42,774
bad deployments, I mean someone made a change and the change is valid,

200
00:12:42,842 --> 00:12:46,574
but just the technicality of performing the deployment

201
00:12:46,622 --> 00:12:50,386
to production. Something failed and for some reason

202
00:12:50,488 --> 00:12:53,714
a detector stopped generating detections. This happens every

203
00:12:53,752 --> 00:12:56,886
now and then and just needs to be handled. So this is

204
00:12:56,908 --> 00:13:00,566
something we would want to know. Changes in

205
00:13:00,588 --> 00:13:04,214
interfaces between squad. And this is also a very important

206
00:13:04,332 --> 00:13:08,386
point because was I said, we have three product squads,

207
00:13:08,418 --> 00:13:11,946
we have three algo squads and a data engineering team. This is

208
00:13:11,968 --> 00:13:15,642
a lot of people and a lot of communication that needs to happen.

209
00:13:15,776 --> 00:13:19,066
And it's natural that sometimes things

210
00:13:19,168 --> 00:13:22,622
wouldn't be perfect. So we want to be on top of that and figure out

211
00:13:22,676 --> 00:13:26,030
changes before they make a big impact.

212
00:13:26,930 --> 00:13:30,174
Another point that we would like to avoid is

213
00:13:30,212 --> 00:13:34,066
negative effects from configuration changes. And I'm going

214
00:13:34,088 --> 00:13:37,602
to explain about this one by using an example that actually happened

215
00:13:37,656 --> 00:13:41,330
to us after we started the monitoring,

216
00:13:41,990 --> 00:13:45,720
the monitoring initiative. And it actually caught this.

217
00:13:46,170 --> 00:13:49,654
It caught these issues. So what happened is that

218
00:13:49,692 --> 00:13:53,270
our DevOps team made security changes, something that

219
00:13:53,420 --> 00:13:57,218
needed to be done, and two of our detectors

220
00:13:57,314 --> 00:14:00,998
stopped generating detections. Now, it's all good

221
00:14:01,084 --> 00:14:04,586
stuff happened, right? But we need to figure it out very quickly.

222
00:14:04,688 --> 00:14:08,710
So the detections stopped being generated.

223
00:14:08,870 --> 00:14:12,314
We got an alert and then we just told

224
00:14:12,352 --> 00:14:16,314
them, hey, can you revert this change and please just investigate

225
00:14:16,442 --> 00:14:19,950
and see how we can make this change again in the proper

226
00:14:20,020 --> 00:14:23,458
manner. And that's what happened, and we figured it out very quickly.

227
00:14:23,624 --> 00:14:27,202
Another type of common production issues is

228
00:14:27,256 --> 00:14:31,262
making changes which you think are correct, but have effects

229
00:14:31,326 --> 00:14:34,702
that you can't even imagine, especially in complicated

230
00:14:34,766 --> 00:14:38,326
systems like this. It's very hard to understand how

231
00:14:38,348 --> 00:14:41,782
it's going to affect. So all of this can happen.

232
00:14:41,916 --> 00:14:45,394
And due to the nature of downstream flows, an error

233
00:14:45,442 --> 00:14:48,834
in the top of the funnel can cause major issues to several

234
00:14:48,882 --> 00:14:52,698
consumers. So this can affect a lot of products and a lot of customers.

235
00:14:52,784 --> 00:14:56,198
So this is very important to us, monitoring.

236
00:14:56,294 --> 00:14:58,780
It's the moment you've all been waiting for.

237
00:14:59,230 --> 00:15:02,494
So what do we want to achieve? First of all,

238
00:15:02,532 --> 00:15:06,206
we want to achieve good service and good support. It's the core of

239
00:15:06,228 --> 00:15:09,662
our product and we want to catch issues before

240
00:15:09,716 --> 00:15:13,310
our customers. It can go either way,

241
00:15:13,380 --> 00:15:17,106
even if they didn't know an issue happened, so we caught it

242
00:15:17,128 --> 00:15:20,978
beforehand. But even if a customer did notice the issue,

243
00:15:21,064 --> 00:15:25,122
we can already tell them we are handling this. So this makes us look very

244
00:15:25,176 --> 00:15:28,614
professional. Also, we want to find issues as fast as

245
00:15:28,652 --> 00:15:31,938
possible. Sometimes nobody notices

246
00:15:32,034 --> 00:15:35,574
an issue until it's too late, right? So we want to be on top

247
00:15:35,612 --> 00:15:38,954
of things, because how important it is to our

248
00:15:38,992 --> 00:15:42,650
product. We want to have consistent AI insights.

249
00:15:42,990 --> 00:15:46,362
The quality of our insights is very important. It's about

250
00:15:46,416 --> 00:15:50,122
giving our customers the consistency they expect.

251
00:15:50,256 --> 00:15:54,366
We want to find machine health issues, but also minimize the

252
00:15:54,388 --> 00:15:57,786
amount of false alerts we give to them. We want to improve

253
00:15:57,818 --> 00:16:00,814
the collaboration between our teams. I've already mentioned this,

254
00:16:00,852 --> 00:16:04,830
but we've grown from eight people working on the diagnosis flow to seven

255
00:16:04,900 --> 00:16:08,002
squad. This is a lot of people, a lot of team,

256
00:16:08,056 --> 00:16:12,046
and we need a way to improve our communication and enable first response.

257
00:16:12,158 --> 00:16:15,922
So our top goal is actually to retain the trust from our

258
00:16:15,976 --> 00:16:19,126
customers. We want to be able to give them a product.

259
00:16:19,228 --> 00:16:22,674
They know when we give an alert it's viable

260
00:16:22,802 --> 00:16:26,630
and when we don't it's all good. According to the Google

261
00:16:26,700 --> 00:16:30,154
Sre book, there are two types of monitoring. We have

262
00:16:30,192 --> 00:16:34,250
white box monitoring which is based on matrix of internal,

263
00:16:36,590 --> 00:16:40,390
internal stuff, cpu memory usage and more.

264
00:16:40,560 --> 00:16:44,282
And we also have black box monitoring which is testing

265
00:16:44,346 --> 00:16:47,582
externally visible behavior as a user would see it.

266
00:16:47,716 --> 00:16:52,010
So let's look at our use case and the title

267
00:16:52,090 --> 00:16:55,586
already gives up and also the drawing gives up

268
00:16:55,608 --> 00:16:58,510
where I'm going to. But we are talking about a hybrid approach.

269
00:16:58,590 --> 00:17:02,114
And why is that? Because from

270
00:17:02,312 --> 00:17:06,550
one side of this, I don't want to necessarily know

271
00:17:06,700 --> 00:17:11,298
about each component in my system and if its cpu

272
00:17:11,394 --> 00:17:14,726
is running low or we are out of

273
00:17:14,748 --> 00:17:19,002
memory. But from the other side, monitoring each

274
00:17:19,136 --> 00:17:22,966
end product by itself I mean it's just a piece of the puzzle,

275
00:17:23,078 --> 00:17:27,194
it's not the whole picture. So what can I do?

276
00:17:27,392 --> 00:17:30,766
So the detection management layer is actually a

277
00:17:30,788 --> 00:17:33,870
consumer or a customer to the AI engine.

278
00:17:34,020 --> 00:17:37,114
So this is pretty similar to black box monitoring,

279
00:17:37,162 --> 00:17:40,590
right. But also it

280
00:17:40,660 --> 00:17:44,450
expected product logic and decides on detection

281
00:17:44,870 --> 00:17:48,642
state. So this is very interesting too because

282
00:17:48,776 --> 00:17:52,850
this actually affects what the external users are seeing.

283
00:17:53,000 --> 00:17:56,434
So this is very similar to whitebox monitoring.

284
00:17:56,562 --> 00:17:59,926
So what we decided is to actually merge the

285
00:17:59,948 --> 00:18:04,374
two ideas together and monitoring an internal product

286
00:18:04,572 --> 00:18:08,890
process that makes also decisions about how

287
00:18:09,040 --> 00:18:11,900
external customers get this information.

288
00:18:12,350 --> 00:18:15,926
So this led us to believe that there are patterns we can commit

289
00:18:15,958 --> 00:18:18,940
to and it's very related to the product.

290
00:18:19,790 --> 00:18:23,294
We saw this example early on with the two graphs and

291
00:18:23,332 --> 00:18:26,702
the machine health events and also the detections over

292
00:18:26,756 --> 00:18:31,002
time with the confidence and actually we can commit

293
00:18:31,066 --> 00:18:34,654
to the amount of detections that are going to be propagated to users,

294
00:18:34,702 --> 00:18:38,734
not on a specific machine, but statistically propagated to users

295
00:18:38,862 --> 00:18:43,214
from our pool of machines. And also taking into consideration

296
00:18:43,342 --> 00:18:47,234
the was that it can be filtered in the detection

297
00:18:47,282 --> 00:18:50,742
management layer. Another thing that we can commit to is the amount

298
00:18:50,796 --> 00:18:54,070
of detections being generated overall and being sent

299
00:18:54,140 --> 00:18:57,706
to the detection management layer per detector. And in

300
00:18:57,728 --> 00:19:01,802
general. So this led us to understand that actually

301
00:19:01,856 --> 00:19:05,514
we have a detection lifecycle. A detection lifecycle is

302
00:19:05,552 --> 00:19:09,638
what it goes through in the detection management layer.

303
00:19:09,734 --> 00:19:13,294
It first of all reaches the detection management layer and

304
00:19:13,492 --> 00:19:16,970
afterwards it's either being filtered by the detection confidence,

305
00:19:17,050 --> 00:19:20,734
meaning we are not confident enough in this specific

306
00:19:20,852 --> 00:19:23,546
detection. We don't need to propagate it to our user,

307
00:19:23,658 --> 00:19:27,154
to our users, or even if the detection confidence is high

308
00:19:27,192 --> 00:19:30,878
enough, we might want to filter this detection due

309
00:19:30,894 --> 00:19:34,334
to the machine states maybe we already alerted the user

310
00:19:34,462 --> 00:19:38,386
on this machine and we don't need to put on another alert

311
00:19:38,418 --> 00:19:42,342
on this machine and in the end propagated to the customers.

312
00:19:42,476 --> 00:19:46,962
So these are the states that we have for a detection.

313
00:19:47,106 --> 00:19:50,758
And this actually led

314
00:19:50,774 --> 00:19:54,666
us to add the matrix on

315
00:19:54,688 --> 00:19:58,486
the detection lifecycle. And using graphite and Grafana we're

316
00:19:58,518 --> 00:20:02,094
actually able to visualize a lot of aggregated views on

317
00:20:02,132 --> 00:20:05,582
the state of our AI engine as a whole.

318
00:20:05,716 --> 00:20:09,050
In this graph we can see the amount of detections

319
00:20:09,130 --> 00:20:12,954
coming into the AI, into the detection

320
00:20:13,002 --> 00:20:16,306
management layer, daily pale detector. And this

321
00:20:16,328 --> 00:20:20,190
really gives us like a full flow of understanding of the differences

322
00:20:20,270 --> 00:20:24,194
between them. This is another very interesting graph because we can see

323
00:20:24,232 --> 00:20:27,970
here the differences between each step

324
00:20:28,040 --> 00:20:31,206
in the detection's lifecycle. And again not specifically for a

325
00:20:31,228 --> 00:20:35,014
machine and not specifically for a detection, but in general and how our

326
00:20:35,052 --> 00:20:39,350
system behaves. So let's get proactive.

327
00:20:39,430 --> 00:20:42,662
So once we had all of these aggregated

328
00:20:42,726 --> 00:20:46,266
views and now we know how our data looks like,

329
00:20:46,368 --> 00:20:50,082
we can actually use Grafana's alerts to set up alerts

330
00:20:50,166 --> 00:20:53,738
and knowing when something is not working as expected.

331
00:20:53,914 --> 00:21:00,410
So what we did is actually decide

332
00:21:00,490 --> 00:21:04,486
on the first four alerts, which is the bearing wear arriving

333
00:21:04,538 --> 00:21:08,254
to the detection. We chose a detector. I chose a detector,

334
00:21:08,302 --> 00:21:11,682
the bearing wear alerts. The bearing wear detector, sorry.

335
00:21:11,816 --> 00:21:15,186
And I decided on four alerts that we would like to

336
00:21:15,208 --> 00:21:18,486
monitoring, meaning four patterns that we

337
00:21:18,508 --> 00:21:22,322
would like to commit to. The first one is the amount of detections

338
00:21:22,386 --> 00:21:25,686
arriving to the detection management layer, meaning not too

339
00:21:25,708 --> 00:21:29,706
much and not too many. The other alert was

340
00:21:29,888 --> 00:21:33,850
about the amount of detections being filtered.

341
00:21:35,150 --> 00:21:38,710
And also I added not too many detections being propagated

342
00:21:38,790 --> 00:21:42,574
and not too less. Right. So once we

343
00:21:42,612 --> 00:21:46,286
had all of this running, I set up

344
00:21:46,308 --> 00:21:49,258
a slack channel, which is called detections monitoring,

345
00:21:49,354 --> 00:21:52,814
and started getting these alerts. Now it took some

346
00:21:52,852 --> 00:21:57,150
time because it took some time, we needed to tweak the values

347
00:21:57,310 --> 00:22:00,414
because we chose really simple absolute values

348
00:22:00,462 --> 00:22:03,982
to put our alerts by. It was really noisy

349
00:22:04,046 --> 00:22:07,490
understanding the different behaviors

350
00:22:07,650 --> 00:22:10,998
of the detector and it took some time but it did

351
00:22:11,084 --> 00:22:14,402
mellow down and just like a sort of FYi,

352
00:22:14,466 --> 00:22:18,502
we are talking right about now of changing our strategy

353
00:22:18,566 --> 00:22:22,102
with this by maybe moving to calculating

354
00:22:22,166 --> 00:22:26,314
the percentage in change on

355
00:22:26,352 --> 00:22:29,658
these numbers instead of just monitoring absolute

356
00:22:29,744 --> 00:22:33,442
values. So this is also very interesting, but out of scope for this stock.

357
00:22:33,526 --> 00:22:36,906
So after the Beringware alerts were stabilized,

358
00:22:37,098 --> 00:22:40,222
I created a workshop and together with all

359
00:22:40,276 --> 00:22:43,890
the algo team, we added dashboards and

360
00:22:44,040 --> 00:22:47,538
alerts for all of our detectors. And now we have a very full

361
00:22:47,624 --> 00:22:50,626
view of our entire AI engine,

362
00:22:50,728 --> 00:22:54,786
including the detection management layer and all of

363
00:22:54,808 --> 00:22:58,086
the detectors and all of the pipelines and everything that you

364
00:22:58,108 --> 00:23:01,560
can imagine. Everything is actually in there

365
00:23:02,090 --> 00:23:05,778
in one place, because we have alerts that indicate

366
00:23:05,874 --> 00:23:09,498
a sort of working, not working indication in a

367
00:23:09,504 --> 00:23:12,794
very high level and in how our customers would

368
00:23:12,832 --> 00:23:16,506
expect to get this as a product. This is an example

369
00:23:16,608 --> 00:23:19,990
of one of the graphs. It's a consistent detections

370
00:23:20,070 --> 00:23:23,258
generation graph and it's pretty straightforward.

371
00:23:23,354 --> 00:23:27,162
We have the red line which indicates the alerts threshold.

372
00:23:27,226 --> 00:23:30,382
There's also one on zero, so we can't see it, but it's there,

373
00:23:30,436 --> 00:23:34,450
I promise you. And another very interesting point here

374
00:23:34,520 --> 00:23:38,082
is the purple, barely dot visible line

375
00:23:38,136 --> 00:23:41,794
that we have over here that has written with it

376
00:23:41,912 --> 00:23:45,966
deployment tag. So actually tags is a feature that Grafana

377
00:23:45,998 --> 00:23:49,382
enables. You can use their open API and

378
00:23:49,516 --> 00:23:53,734
in each time send out a tag that has extra information

379
00:23:53,852 --> 00:23:57,094
on it and it gives you a point in time and you can enrich it

380
00:23:57,132 --> 00:24:00,442
over your graphs. So what you actually see here

381
00:24:00,496 --> 00:24:03,882
in this purple line is a deployment tag on

382
00:24:03,936 --> 00:24:07,354
each service and each component that we have in our system.

383
00:24:07,552 --> 00:24:10,886
We added a deployment tag

384
00:24:10,918 --> 00:24:14,826
that being created when we deploy to production. On this deployment tag

385
00:24:14,858 --> 00:24:18,622
we have a Githash, we have the name of the person that did this

386
00:24:18,676 --> 00:24:22,062
deployment and also the name of the service he deployed to.

387
00:24:22,196 --> 00:24:26,414
So when you have a very complicated system that keeps on deploying

388
00:24:26,542 --> 00:24:29,986
different component to it, but all can affect the

389
00:24:30,008 --> 00:24:33,906
downstream flow, we can use this and really have a

390
00:24:33,928 --> 00:24:37,254
quick way to identify what change was made. And they just

391
00:24:37,292 --> 00:24:40,342
like ping the person. Hey. Hi. I saw you did this change.

392
00:24:40,396 --> 00:24:44,166
I see this detection stopped generating. Can you please take

393
00:24:44,188 --> 00:24:47,426
a look? So this is very powerful.

394
00:24:47,618 --> 00:24:50,998
So in conclusion, keep the customers in the center,

395
00:24:51,084 --> 00:24:54,706
whether they're internal or external. Internal teams

396
00:24:54,738 --> 00:24:58,406
can consume products from each other. It's not

397
00:24:58,428 --> 00:25:02,430
about having zero bugs product, it's about fast response.

398
00:25:02,770 --> 00:25:06,110
To move fast, we need high confidence in our process.

399
00:25:06,260 --> 00:25:09,902
And having an easy way to communicate across

400
00:25:09,956 --> 00:25:13,134
teams is crucial. Thank you, I hope you

401
00:25:13,172 --> 00:25:16,926
enjoyed it. And if you have anything to add or say or ask, feel free

402
00:25:16,948 --> 00:25:17,580
to contact me.

