1
00:02:01,300 --> 00:02:04,864
Everyone in this session, I'll introduce you

2
00:02:04,902 --> 00:02:09,164
to what, why, and how of Kaios engineering.

3
00:02:09,292 --> 00:02:13,668
I'll dive deep into the principles principles behind kiosk engineering,

4
00:02:13,844 --> 00:02:16,936
how to keep your services up and running,

5
00:02:17,118 --> 00:02:21,352
how to apply these principles in the context of application.

6
00:02:21,486 --> 00:02:25,132
Built on top of AWS before we

7
00:02:25,186 --> 00:02:29,208
move on, I want to start with Werner Vogel's

8
00:02:29,304 --> 00:02:32,750
quote. Everything fails all the time,

9
00:02:33,120 --> 00:02:37,132
and hence we need to build systems that embrace

10
00:02:37,196 --> 00:02:39,760
failure as a natural occurrence.

11
00:02:40,980 --> 00:02:44,672
Creating technology solutions is a lot like

12
00:02:44,806 --> 00:02:48,096
constructing a physical building. If the

13
00:02:48,118 --> 00:02:51,616
foundations aren't solid, it may cause structural

14
00:02:51,648 --> 00:02:55,236
problems that undermine the integrity and the function

15
00:02:55,338 --> 00:02:58,944
of the building. The AWS well architectures

16
00:02:58,992 --> 00:03:02,792
Framework is a set of design principles and

17
00:03:02,846 --> 00:03:06,024
architectural best practices for designing and

18
00:03:06,062 --> 00:03:10,468
running services in the cloud. The framework

19
00:03:10,644 --> 00:03:15,304
is built based on years of experience architecting

20
00:03:15,352 --> 00:03:18,492
solutions across a wide variety of

21
00:03:18,546 --> 00:03:21,660
business verticals and use cases,

22
00:03:22,320 --> 00:03:26,628
and from designing and reviewing a number of architectures

23
00:03:26,744 --> 00:03:29,680
with thousands of customers on AWS,

24
00:03:30,660 --> 00:03:34,240
the framework has a set of questions

25
00:03:34,390 --> 00:03:38,224
to drive better outcomes for anyone who

26
00:03:38,262 --> 00:03:42,550
wants to build and operate your applications on cloud.

27
00:03:43,800 --> 00:03:47,584
There are six principles in the AWS well architected

28
00:03:47,632 --> 00:03:50,828
framework operational excellence,

29
00:03:51,024 --> 00:03:53,480
security reliability,

30
00:03:54,060 --> 00:03:57,160
performance efficiency, cost optimization,

31
00:03:57,740 --> 00:04:01,592
and sustainability. Following these

32
00:04:01,646 --> 00:04:06,060
guidelines will enable you to build a system that delivers

33
00:04:06,400 --> 00:04:09,980
functional requirements that meet your expectations.

34
00:04:11,920 --> 00:04:15,404
In this session, I'll touch on only the

35
00:04:15,442 --> 00:04:19,410
three pillars that are relevant to this topic today.

36
00:04:20,340 --> 00:04:23,324
Those are operational excellence, reliability,

37
00:04:23,452 --> 00:04:27,360
and performance efficiency. First, let's look at

38
00:04:27,430 --> 00:04:31,060
what does it mean by operational excellence?

39
00:04:31,640 --> 00:04:34,868
Operational excellence is the ability to support,

40
00:04:34,954 --> 00:04:39,184
development, run and monitor systems effectively

41
00:04:39,232 --> 00:04:42,308
to gain insight into your operations,

42
00:04:42,484 --> 00:04:46,372
and then deliver business value through continually supporting

43
00:04:46,516 --> 00:04:48,200
the support processes.

44
00:04:49,580 --> 00:04:53,172
Reliability encompasses the ability

45
00:04:53,236 --> 00:04:56,830
of an application or a service to perform

46
00:04:57,520 --> 00:05:01,340
its intended function correctly and consistently.

47
00:05:02,400 --> 00:05:06,408
Performance efficiency is the ability of the system to

48
00:05:06,594 --> 00:05:10,316
use the computing resources in most efficient manner

49
00:05:10,428 --> 00:05:14,348
to meet your system requirements and to maintain the efficiency

50
00:05:14,524 --> 00:05:18,640
as the demand increases or the technology evolves.

51
00:05:19,860 --> 00:05:23,712
In the reliability pillar of well architected framework,

52
00:05:23,856 --> 00:05:27,600
there is a segment that talks about testing

53
00:05:27,680 --> 00:05:30,980
your system through failure injection,

54
00:05:32,300 --> 00:05:35,716
and this comes as a recommendation from Amazon's

55
00:05:35,748 --> 00:05:39,636
many years of experience building and operating large distributed

56
00:05:39,668 --> 00:05:42,760
systems. And this practice of using

57
00:05:42,830 --> 00:05:46,376
fault isolation or fault injection

58
00:05:46,488 --> 00:05:50,012
to test your environments is also better known as

59
00:05:50,146 --> 00:05:53,404
coyotes engineering. Let's go into

60
00:05:53,442 --> 00:05:56,620
the details of what, why, and how

61
00:05:56,690 --> 00:06:00,432
of coyotes engineering. Let's understand the what.

62
00:06:00,566 --> 00:06:04,236
First, it's about designing

63
00:06:04,268 --> 00:06:08,504
your system to work despite failures, building sustainability

64
00:06:08,652 --> 00:06:12,192
and building stability in your system behavior,

65
00:06:12,336 --> 00:06:16,304
and proactively looking for problems instead of waiting

66
00:06:16,352 --> 00:06:19,972
for them to happen and be surprised by them.

67
00:06:20,106 --> 00:06:23,924
Above all, chaos engineering needs a cultural

68
00:06:23,972 --> 00:06:28,200
shift for organizations to adopt the approach,

69
00:06:29,260 --> 00:06:32,440
chaos engineering is a process of

70
00:06:32,510 --> 00:06:36,104
stressing an application by creating disruptive

71
00:06:36,152 --> 00:06:39,436
events and observing how the system responds to

72
00:06:39,458 --> 00:06:43,740
those events and finally implementing those improvements.

73
00:06:44,720 --> 00:06:48,592
So it's an approach to learning about how your

74
00:06:48,646 --> 00:06:52,284
system behaves when subject to scientific experimentation

75
00:06:52,412 --> 00:06:55,964
and finding evidence. So let's

76
00:06:56,012 --> 00:06:59,408
talk about why kiosk engineering rise

77
00:06:59,424 --> 00:07:03,360
of microservices distributed cloud architectures

78
00:07:03,520 --> 00:07:06,944
the pace of innovation, pace of development

79
00:07:06,992 --> 00:07:10,896
and deployment of software means that the systems are

80
00:07:11,018 --> 00:07:14,664
growing increasingly complex. While individual

81
00:07:14,782 --> 00:07:18,136
components in a development cycle work,

82
00:07:18,238 --> 00:07:21,944
they when integrated, some of the faults may

83
00:07:21,982 --> 00:07:26,424
be unexpected, and these failures could cause costly

84
00:07:26,552 --> 00:07:28,780
to businesses.

85
00:07:30,080 --> 00:07:33,836
Even brief outages could impact companies

86
00:07:33,938 --> 00:07:37,260
bottom line. So the cost of the downtime

87
00:07:37,420 --> 00:07:40,848
is becoming a key performance indicator in

88
00:07:41,014 --> 00:07:43,120
key engineering teams.

89
00:07:43,540 --> 00:07:47,104
Amazon.com retail online business could

90
00:07:47,142 --> 00:07:50,516
have a large impact on revenue, even for a

91
00:07:50,538 --> 00:07:52,420
few minutes of outage.

92
00:07:53,480 --> 00:07:56,980
So companies need a solution to this challenge.

93
00:07:57,400 --> 00:08:01,690
Waiting for the next costly outage is not an option.

94
00:08:02,060 --> 00:08:05,672
To meet these challenge head on, more and more companies

95
00:08:05,806 --> 00:08:09,028
are turning to chaos engineering. So let's

96
00:08:09,124 --> 00:08:12,644
learn what's involved in adopting

97
00:08:12,692 --> 00:08:17,176
this approach, we make an assumption

98
00:08:17,368 --> 00:08:20,856
about our system, and we conduct experiments

99
00:08:20,968 --> 00:08:25,496
in a controlled environment to prove or disprove our theories,

100
00:08:25,608 --> 00:08:28,988
our assumptions about our system's capability

101
00:08:29,164 --> 00:08:32,400
to handle such disruptive events.

102
00:08:32,820 --> 00:08:37,040
But rather than let those disruptive events happen

103
00:08:37,110 --> 00:08:41,652
at 03:00 a.m. During these weekend or

104
00:08:41,706 --> 00:08:44,964
in a production environment, we create them in a

105
00:08:45,002 --> 00:08:49,120
controlled work hours development

106
00:08:49,200 --> 00:08:52,824
environment and experiment and see

107
00:08:52,862 --> 00:08:56,072
how the system behaves. We repeat these

108
00:08:56,126 --> 00:08:59,976
experiments at regular intervals, and thus learn

109
00:09:00,078 --> 00:09:03,480
more and more about the ability of the system to withstand

110
00:09:03,920 --> 00:09:07,688
interruptions and improve our systems

111
00:09:07,784 --> 00:09:10,990
to bounce back and provide the best possible service.

112
00:09:11,600 --> 00:09:15,292
So we are building reliability in our systems by

113
00:09:15,346 --> 00:09:19,504
using an approach called chaos engineering. So let's talk about

114
00:09:19,622 --> 00:09:23,456
the engineering approach facilitates building

115
00:09:23,638 --> 00:09:27,404
this resilience. So there are five phases

116
00:09:27,452 --> 00:09:30,852
to this coyote engineering. First,

117
00:09:30,906 --> 00:09:34,470
about understanding the state of your system you're dealing with.

118
00:09:34,840 --> 00:09:37,616
Secondly, hypothesize,

119
00:09:37,808 --> 00:09:40,950
articulate a hypothesis about your system,

120
00:09:41,340 --> 00:09:43,160
run an experiment,

121
00:09:44,620 --> 00:09:47,412
often using fault injection,

122
00:09:47,556 --> 00:09:50,856
verify the results of the system, and then finally,

123
00:09:50,958 --> 00:09:54,236
learn from your experiments. In order to improve the

124
00:09:54,258 --> 00:09:58,344
system further, in order to build resilience

125
00:09:58,392 --> 00:10:02,760
into your systems, we should be able to identify

126
00:10:02,840 --> 00:10:07,116
under what circumstances, under what scenarios our systems

127
00:10:07,228 --> 00:10:11,004
are likely to fail. And then we can translate

128
00:10:11,052 --> 00:10:14,620
these scenarios into a set of experiments

129
00:10:14,780 --> 00:10:17,570
and learn to build stability around the system.

130
00:10:18,600 --> 00:10:22,740
For example, the kind of chaos experiments you could conduct

131
00:10:23,640 --> 00:10:26,948
could be for hardware failures where a server is going down,

132
00:10:27,034 --> 00:10:30,864
or it could be non functional requirements, such as

133
00:10:31,002 --> 00:10:34,136
a spike in the traffic. Or it could also

134
00:10:34,238 --> 00:10:37,770
be testing your software services,

135
00:10:38,140 --> 00:10:42,104
where you're sending malformed responses and see and

136
00:10:42,142 --> 00:10:45,820
learn about your system, how that would respond.

137
00:10:46,880 --> 00:10:50,284
Now, let's take a quick look into the kind

138
00:10:50,322 --> 00:10:53,864
of tooling available for us to conduct the kiosk engineering

139
00:10:53,912 --> 00:10:57,664
experiments before we go into the tooling itself, a little

140
00:10:57,702 --> 00:11:01,632
bit of a history as to how it

141
00:11:01,686 --> 00:11:04,144
originated. Back in 2010,

142
00:11:04,262 --> 00:11:08,224
Netflix's engineering teams have created a

143
00:11:08,262 --> 00:11:12,276
tool called Chaos Monkey, which is a

144
00:11:12,298 --> 00:11:15,344
tool that they have used in their systems

145
00:11:15,392 --> 00:11:19,856
to build resilience to enforce

146
00:11:20,048 --> 00:11:23,496
failures into their systems and to learn how to

147
00:11:23,518 --> 00:11:24,970
stabilize their system,

148
00:11:26,540 --> 00:11:30,584
such as by fault injection, such as by terminating services

149
00:11:30,702 --> 00:11:33,868
or terminating a server that's running a

150
00:11:33,874 --> 00:11:36,510
particular service, and so on.

151
00:11:37,040 --> 00:11:40,664
Next, in 2011, civilian army added

152
00:11:40,712 --> 00:11:44,156
additional failure modes to just kind of provide a

153
00:11:44,178 --> 00:11:48,720
full set of failure testing capability.

154
00:11:50,020 --> 00:11:53,452
In 2017, kiosk engineering toolkit for developers

155
00:11:53,516 --> 00:11:54,400
emerged,

156
00:11:59,080 --> 00:12:02,756
mostly to have an open API for the developers to be

157
00:12:02,778 --> 00:12:07,008
able to integrate kiosk

158
00:12:07,104 --> 00:12:10,336
experiments into their systems,

159
00:12:10,448 --> 00:12:14,244
and also to automate into CI CD pipelines

160
00:12:14,292 --> 00:12:17,320
and so on. Now, in 2019,

161
00:12:17,740 --> 00:12:21,432
another powerful coyote engineering platform emerged for

162
00:12:21,486 --> 00:12:24,984
Kubernetes for testing container based

163
00:12:25,102 --> 00:12:28,744
services. Chaos, the ability to perform coyotes experiments

164
00:12:28,792 --> 00:12:32,684
without modifying the deployment logic of the application

165
00:12:32,802 --> 00:12:36,850
itself. In 2009,

166
00:12:38,900 --> 00:12:42,764
actually, Colton Andrews had built fault

167
00:12:42,812 --> 00:12:46,192
isolation fault injection at Amazon. He later

168
00:12:46,246 --> 00:12:49,376
on went to found he co founded

169
00:12:49,408 --> 00:12:54,230
Gremlin, which is a failure as a service platform,

170
00:12:54,600 --> 00:12:57,540
which is launched in 2019.

171
00:12:58,200 --> 00:13:01,350
So Gremlin helps build

172
00:13:02,220 --> 00:13:06,152
resiliency into your systems by turning failure into

173
00:13:06,206 --> 00:13:10,116
resilience, and by offering engineers a fully hosted solution

174
00:13:10,308 --> 00:13:13,744
to safely conduct experiments on simple or complex

175
00:13:13,812 --> 00:13:17,900
systems, and in order to identify those weaknesses even before

176
00:13:17,970 --> 00:13:22,284
they can impact the customers experience and

177
00:13:22,482 --> 00:13:25,660
allow you to reduce any revenue loss.

178
00:13:26,100 --> 00:13:29,520
So this allows developers to run experiments against

179
00:13:29,590 --> 00:13:31,120
hosts, containers,

180
00:13:32,420 --> 00:13:36,412
or even functions, or also kubernetes

181
00:13:36,476 --> 00:13:40,180
primitives. It's available. Gremlin is available

182
00:13:40,250 --> 00:13:42,756
on AWS Marketplace as well.

183
00:13:42,938 --> 00:13:47,492
Another tool that I want to talk about today is the

184
00:13:47,626 --> 00:13:50,736
AWS fault injection simulator, or FIS.

185
00:13:50,848 --> 00:13:54,616
In short, it's a fully managed service for

186
00:13:54,638 --> 00:13:58,360
conducting coyote engineering experiments provided by AWS.

187
00:13:59,740 --> 00:14:03,412
It's designed to make it easy for the developers

188
00:14:03,556 --> 00:14:06,796
to use the service. It allows you

189
00:14:06,818 --> 00:14:10,248
to test your systems for real world failures.

190
00:14:10,344 --> 00:14:13,596
Either it could be a simple test, or it could be

191
00:14:13,618 --> 00:14:17,580
a complex test as well. FIS basically embraces

192
00:14:17,660 --> 00:14:21,564
these idea of blast

193
00:14:21,612 --> 00:14:25,712
radius and monitoring your blast radius. It does

194
00:14:25,766 --> 00:14:28,868
so by giving you the ability

195
00:14:28,954 --> 00:14:32,580
to set up conditions around your experiments.

196
00:14:33,320 --> 00:14:36,704
So basically, it's the idea of safeguarding

197
00:14:36,752 --> 00:14:39,060
your servers,

198
00:14:41,980 --> 00:14:44,520
even if by mistake,

199
00:14:45,260 --> 00:14:48,356
so that you can reduce that blast radius of the experiment.

200
00:14:48,468 --> 00:14:52,024
And some alarms can be set off if

201
00:14:52,062 --> 00:14:55,564
those conditions are met. So now let's take

202
00:14:55,602 --> 00:14:58,876
a quick look into what are

203
00:14:58,898 --> 00:15:02,780
the components that comprise of fis.

204
00:15:03,360 --> 00:15:06,840
First is actions. Actions are

205
00:15:07,010 --> 00:15:10,640
fault injection activities that you want to conduct

206
00:15:11,300 --> 00:15:14,912
experiments with as well as these actions act

207
00:15:14,966 --> 00:15:18,616
on. Targets and targets are nothing but EC

208
00:15:18,668 --> 00:15:22,704
two resources. It's not necessarily easy to but AWS

209
00:15:22,752 --> 00:15:26,512
resources that you want these actions to be performed

210
00:15:26,576 --> 00:15:30,000
on. And these resources can be identified

211
00:15:30,080 --> 00:15:33,210
via targets, sorry, via tags as well.

212
00:15:33,980 --> 00:15:37,624
You have experimentation templates or

213
00:15:37,662 --> 00:15:41,332
experiment template which forms the basis

214
00:15:41,396 --> 00:15:44,716
for conducting a simple experiment first,

215
00:15:44,818 --> 00:15:48,940
and these templates can further be used to develop multiple experiments.

216
00:15:49,680 --> 00:15:53,116
Now let's talk about how to build highly available

217
00:15:53,218 --> 00:15:55,740
fault tolerant systems on AWS.

218
00:15:56,180 --> 00:15:59,504
Before we do that, let me go through the details of

219
00:15:59,542 --> 00:16:03,452
AWS global infrastructure. AWS region

220
00:16:03,516 --> 00:16:07,280
is a physical geographical location consisting of two or

221
00:16:07,350 --> 00:16:11,424
more availability zones, and an availability zone consists

222
00:16:11,472 --> 00:16:14,944
of two or more data centers that are redundantly

223
00:16:14,992 --> 00:16:18,980
connected with power networking and connectivity.

224
00:16:19,560 --> 00:16:23,720
These availability zones are interconnected with low latency

225
00:16:24,620 --> 00:16:28,664
network cables. Now let's suppose we have this

226
00:16:28,702 --> 00:16:31,640
three tier architecture hosted on AWS.

227
00:16:31,980 --> 00:16:35,640
The web tier is hosted on elastic container service

228
00:16:35,710 --> 00:16:39,672
or ECS for short and the web tier is receiving

229
00:16:39,736 --> 00:16:44,412
traffic from Internet gateway from

230
00:16:44,466 --> 00:16:48,124
Internet through Internet gateway and it is distributing traffic

231
00:16:48,172 --> 00:16:52,576
through elastic load balancer to the ECS cluster which

232
00:16:52,598 --> 00:16:55,996
is the web tier. The web tier is further

233
00:16:56,188 --> 00:16:59,936
distributing the traffic through another elastic load balancer

234
00:17:00,048 --> 00:17:03,520
to another ECS cluster

235
00:17:03,600 --> 00:17:07,030
which is these application layer and we're using

236
00:17:07,560 --> 00:17:10,420
Amazon Aurora as the database tier.

237
00:17:10,760 --> 00:17:14,548
Now let's take a look at how to set up kiosk

238
00:17:14,564 --> 00:17:18,420
experiments using AWS fault injection simulator.

239
00:17:18,580 --> 00:17:22,490
Let's say for our first scenario, what happens when

240
00:17:22,860 --> 00:17:26,524
servers are experiencing cpu load and

241
00:17:26,562 --> 00:17:30,632
our hypothesis is that if the cpu utilization

242
00:17:30,776 --> 00:17:34,424
for a compute resource were to be under stress,

243
00:17:34,552 --> 00:17:38,492
the availability of our website would not be impacted

244
00:17:38,636 --> 00:17:41,570
due to the built in capability in these system.

245
00:17:42,100 --> 00:17:45,660
Now let's go through the steps to run this experiment.

246
00:17:45,820 --> 00:17:49,052
I'm assuming that you already have an AWS account.

247
00:17:49,206 --> 00:17:53,072
Now go to your AWS console, search for AWS

248
00:17:53,136 --> 00:17:57,104
FIS. On the left hand side you should find experiment

249
00:17:57,152 --> 00:18:00,896
templates on the FIS console similar to what you're

250
00:18:00,928 --> 00:18:04,692
seeing on these slide. Using this experiment templates

251
00:18:04,756 --> 00:18:09,000
option you can create various experiments.

252
00:18:09,500 --> 00:18:13,256
Now let's take for example creating or causing a

253
00:18:13,278 --> 00:18:16,620
cpu stress as one of the scenarios you want to perform

254
00:18:16,690 --> 00:18:20,060
the experiments on. When you navigate to create

255
00:18:20,130 --> 00:18:23,756
experiment template, you can choose from

256
00:18:23,778 --> 00:18:27,744
the pre built set of actions in FIS such as

257
00:18:27,862 --> 00:18:32,144
an AWS systems managers run command under

258
00:18:32,182 --> 00:18:36,396
the hood to create the cpu stress

259
00:18:36,588 --> 00:18:40,192
in the top right corner of this slide you can see the actions

260
00:18:40,256 --> 00:18:44,180
drop down menu which allows you to run

261
00:18:44,250 --> 00:18:47,396
the experiments. Then if you click on

262
00:18:47,418 --> 00:18:49,830
the run experiment option,

263
00:18:50,940 --> 00:18:54,296
you will be prompted with an

264
00:18:54,318 --> 00:18:58,356
input box to type the word start to confirm

265
00:18:58,388 --> 00:19:01,832
running the experiment ensure to check the state

266
00:19:01,886 --> 00:19:05,596
of the experiment is in running state now you

267
00:19:05,618 --> 00:19:09,804
will be taken to the experiment details page and note that

268
00:19:09,842 --> 00:19:12,990
the state of the experiment changes over the time.

269
00:19:13,520 --> 00:19:17,280
So in order to observe the impact of the resources,

270
00:19:17,620 --> 00:19:21,644
navigate to the monitoring dashboard of the ECS cluster

271
00:19:21,772 --> 00:19:26,384
or you can prebuild a

272
00:19:26,422 --> 00:19:29,924
custom dashboard to view the impact of the

273
00:19:29,962 --> 00:19:33,910
cpu utilization load on your application.

274
00:19:34,680 --> 00:19:37,796
So you should observe a spike in

275
00:19:37,818 --> 00:19:41,860
the monitoring graph. Similarly, you can conduct

276
00:19:42,020 --> 00:19:45,140
other experiments using the prebuilt actions

277
00:19:45,300 --> 00:19:49,284
say for example you can use SSM commands for network

278
00:19:49,332 --> 00:19:53,416
packet loss. This action can be used as

279
00:19:53,438 --> 00:19:57,420
a basis for conducting network stress experiment

280
00:19:58,080 --> 00:20:01,436
say if you want to run an experiment and try to

281
00:20:01,458 --> 00:20:04,636
mimic an application's response to an

282
00:20:04,658 --> 00:20:08,224
easy failure by

283
00:20:08,262 --> 00:20:12,476
removing the availability zone from the underlying auto scaling

284
00:20:12,508 --> 00:20:16,320
group configuration and triggering a database failover.

285
00:20:17,140 --> 00:20:19,700
Here is the hypothesis for this scenario.

286
00:20:20,360 --> 00:20:23,920
50% of our EC two instances

287
00:20:24,000 --> 00:20:27,220
will not affect the availability of our website

288
00:20:27,370 --> 00:20:31,516
and the application will remediate itself to get back to the desired

289
00:20:31,648 --> 00:20:35,528
healthy capacity because of the use of autoscaling group

290
00:20:35,614 --> 00:20:38,820
underneath the hood. Now running this experiment

291
00:20:38,900 --> 00:20:42,676
is going to terminate 50% of our total EC

292
00:20:42,708 --> 00:20:46,844
two instances, both app and the web layer the

293
00:20:46,882 --> 00:20:51,070
steps to run this experiment are navigate to the

294
00:20:52,800 --> 00:20:56,168
AWS FIS console as we have

295
00:20:56,194 --> 00:21:00,240
shown earlier. Navigate to the experiment templates

296
00:21:00,660 --> 00:21:04,370
section on the AWS FIS page,

297
00:21:04,900 --> 00:21:08,124
select the experiment template in the top right corner,

298
00:21:08,252 --> 00:21:13,350
select the actions drop down and hit run.

299
00:21:13,800 --> 00:21:17,780
Ensure to check the status of the experiment

300
00:21:18,200 --> 00:21:21,796
is in running state and then observe the impact

301
00:21:21,828 --> 00:21:25,160
of the resources from the cloud watch dashboard.

302
00:21:25,500 --> 00:21:29,192
So the action or the experiment template we are using

303
00:21:29,246 --> 00:21:32,536
here is terminate EC two which is based on the

304
00:21:32,558 --> 00:21:36,924
actions to terminate instances from

305
00:21:36,962 --> 00:21:40,396
the EC two service. To observe the

306
00:21:40,418 --> 00:21:44,200
impact of the resources you can either go to the Cloudwatch

307
00:21:44,280 --> 00:21:47,500
dashboard or you can go to the ECS

308
00:21:47,840 --> 00:21:51,708
cluster to note the healthy hosts section.

309
00:21:51,884 --> 00:21:55,570
You should see a different number from the original steady state.

310
00:21:56,500 --> 00:21:59,956
To observe the impact you can also go to

311
00:21:59,978 --> 00:22:03,540
the EC two service dashboard instances section

312
00:22:03,880 --> 00:22:07,460
where you can notice the EC two instances getting terminated

313
00:22:07,960 --> 00:22:11,124
and eventually you will see some new EC two

314
00:22:11,162 --> 00:22:14,836
instances coming back up as well. And from the auto

315
00:22:14,868 --> 00:22:18,404
scaling menu you can navigate to the auto scaling

316
00:22:18,452 --> 00:22:22,584
group section and you will see the instances count, decrease and

317
00:22:22,622 --> 00:22:26,396
automatically restore as well. And finally to observe the

318
00:22:26,418 --> 00:22:29,948
impact of user experience you can navigate to

319
00:22:29,954 --> 00:22:34,044
these Cloudwatch service and set

320
00:22:34,082 --> 00:22:37,376
up a Cloudwatch synthetics canary if you

321
00:22:37,398 --> 00:22:39,090
have this pre set up,

322
00:22:40,900 --> 00:22:46,064
you can observe these change in the user experience because

323
00:22:46,182 --> 00:22:50,076
synthetics canary actually does the monitoring of the user endpoints

324
00:22:50,108 --> 00:22:53,684
and the APIs. Basically, a synthetic scannery is

325
00:22:53,722 --> 00:22:56,964
a configurable script that can run on schedule and it

326
00:22:57,002 --> 00:23:00,436
monitors those endpoints. You can also navigate to the

327
00:23:00,458 --> 00:23:04,724
ECS service endpoint manually via the browser

328
00:23:04,772 --> 00:23:08,472
and check the user experience impact during

329
00:23:08,526 --> 00:23:12,212
the auto scaling process as well. Once this experiment

330
00:23:12,276 --> 00:23:15,950
completes, the applications should return back to its steady state.

331
00:23:16,320 --> 00:23:19,640
Now let's take a look at how to make this a reality.

332
00:23:19,800 --> 00:23:23,692
It's not sufficient to run these experiments once

333
00:23:23,746 --> 00:23:27,036
and leave it at that. Your kiosk scenarios

334
00:23:27,068 --> 00:23:30,976
and hence the experiments and hence your recovery design

335
00:23:31,158 --> 00:23:34,960
are based on certain assumptions. For example,

336
00:23:35,030 --> 00:23:38,856
if you think of a data replication scenario and assume

337
00:23:38,908 --> 00:23:42,752
that the system will complete set of replication

338
00:23:42,816 --> 00:23:46,816
tasks within the set steady state time. As the data grows

339
00:23:46,848 --> 00:23:50,144
organically, the replication times may not hold good anymore.

340
00:23:50,272 --> 00:23:53,780
It's hence important to conduct these experiments regularly,

341
00:23:53,860 --> 00:23:57,512
validate them and improve your results and

342
00:23:57,566 --> 00:24:01,416
enhance your customer experience. Reality may differ

343
00:24:01,598 --> 00:24:05,624
as I said earlier, one way to test your systems and

344
00:24:05,662 --> 00:24:09,164
bring them to as close to reality as possible is through running game

345
00:24:09,202 --> 00:24:12,892
days. It's a concept where you bring in

346
00:24:12,946 --> 00:24:16,828
a set of people from different disciplines or who have not

347
00:24:16,994 --> 00:24:20,050
used your system as much, so they do not have

348
00:24:20,580 --> 00:24:24,556
an idea of how this is going to work and give them a brief

349
00:24:24,588 --> 00:24:28,460
overview and the scenario of events to run and

350
00:24:28,630 --> 00:24:32,404
gather the feedback, analyze the results and

351
00:24:32,522 --> 00:24:36,164
follow up with items to improve your system

352
00:24:36,362 --> 00:24:39,120
and run these tests regularly.

353
00:24:39,200 --> 00:24:43,192
Conduct these game tests regularly. Integrate these

354
00:24:43,326 --> 00:24:48,584
tests AWS part of your CI CD pipeline as

355
00:24:48,622 --> 00:24:52,612
I mentioned, if you have runbooks that need to be checked

356
00:24:52,676 --> 00:24:56,028
manually, they could very easily get out of date

357
00:24:56,114 --> 00:24:59,736
and run into issues once it's in production.

358
00:24:59,928 --> 00:25:03,576
Finally, I want to leave you with this quote regarding coyotes

359
00:25:03,608 --> 00:25:06,924
engineering. It isn't about creating

360
00:25:06,972 --> 00:25:10,496
coyotes, it is about making the coyotes inherent in the system

361
00:25:10,598 --> 00:25:13,792
visible. I invite you to start

362
00:25:13,846 --> 00:25:18,012
testing reliability of your systems using coyotes engineering

363
00:25:18,156 --> 00:25:22,016
techniques you've learned today. I'll share some resources in

364
00:25:22,038 --> 00:25:26,048
the next couple of slides that will help you on this journey. If you

365
00:25:26,054 --> 00:25:29,330
have not created an AWS account,

366
00:25:29,860 --> 00:25:33,096
this link will tell you how to and here are some of the

367
00:25:33,118 --> 00:25:36,724
links to go over and understand the AWS well architected

368
00:25:36,772 --> 00:25:39,060
framework and some hands on labs.

