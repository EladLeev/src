1
00:00:22,250 --> 00:00:25,366
Hi. My name is Tempest Vansky, and I

2
00:00:25,388 --> 00:00:29,474
am a machine learning engineer at Microsoft in a team called commercial software

3
00:00:29,522 --> 00:00:33,574
engineering. And today I'm going to speak about responsible AI in

4
00:00:33,612 --> 00:00:37,266
health from principles to practice. So, an overview

5
00:00:37,298 --> 00:00:40,486
of what I'll speak about is the historical journey of

6
00:00:40,508 --> 00:00:44,326
responsible AI, lessons from biomedical research

7
00:00:44,428 --> 00:00:48,246
in responsible AI, some ethical questions to

8
00:00:48,268 --> 00:00:51,726
ask about projects, and some tools to

9
00:00:51,908 --> 00:00:55,226
help when working on AI projects.

10
00:00:55,338 --> 00:00:59,418
So I'll start with the historical journey of responsible AI

11
00:00:59,514 --> 00:01:03,154
as I see it. So, when new

12
00:01:03,192 --> 00:01:06,706
technology is developed and unleashed, safety and

13
00:01:06,728 --> 00:01:09,634
responsibility considerations usually follow.

14
00:01:09,832 --> 00:01:13,586
So, for example, the development of cars. We take

15
00:01:13,688 --> 00:01:17,222
car safety for granted these

16
00:01:17,276 --> 00:01:20,518
days, but once there was the first car on the

17
00:01:20,524 --> 00:01:23,842
street, and then there was the first car fatality,

18
00:01:23,986 --> 00:01:27,174
and then can manufacturers started adding things like

19
00:01:27,212 --> 00:01:31,162
windshields and headlights and traffic lights on the street and

20
00:01:31,216 --> 00:01:34,538
seatbelts, and eventually a driver's test,

21
00:01:34,704 --> 00:01:38,714
which only came into being in 1955.

22
00:01:38,832 --> 00:01:42,558
So we take that all for granted, but it wasn't always there.

23
00:01:42,724 --> 00:01:46,158
And I feel like responsible AI is

24
00:01:46,244 --> 00:01:49,178
in a similar position to the very early days of cars,

25
00:01:49,274 --> 00:01:52,754
where the technology has been released. And now

26
00:01:52,792 --> 00:01:56,798
there's a lot of focus on safety and responsibility. And what's

27
00:01:56,814 --> 00:02:00,450
been interesting to observe over the last couple of years

28
00:02:00,600 --> 00:02:05,186
is that machine learning researchers and practitioners have

29
00:02:05,288 --> 00:02:08,326
moved from asking, can we do it?

30
00:02:08,428 --> 00:02:12,006
To should we do it? For an example, look at

31
00:02:12,028 --> 00:02:15,906
facial recognition. A couple of years ago, this was really exciting.

32
00:02:16,018 --> 00:02:19,142
It's a genuinely exciting engineering breakthrough where

33
00:02:19,196 --> 00:02:22,662
we can get computers to recognize human faces.

34
00:02:22,806 --> 00:02:25,866
It's something that people have been working on for decades. So there was

35
00:02:25,888 --> 00:02:30,246
a lot of excitement about whether we could have this breakthrough. But now

36
00:02:30,368 --> 00:02:34,394
that we've seen the consequences of releasing facial recognition

37
00:02:34,442 --> 00:02:37,406
technology, people, even companies,

38
00:02:37,508 --> 00:02:40,590
are asking, should we be doing this?

39
00:02:40,660 --> 00:02:44,434
Should we be using this technology at all? So I will

40
00:02:44,632 --> 00:02:48,622
now speak about my personal journey with responsible AI.

41
00:02:48,766 --> 00:02:52,434
Now, my background is in biomedical research because

42
00:02:52,472 --> 00:02:54,610
I'm a biomedical engineer.

43
00:02:56,090 --> 00:03:00,440
It's been interesting to see that some of the concerns with AI today

44
00:03:01,210 --> 00:03:04,662
have actually are quite familiar from biomedical research.

45
00:03:04,796 --> 00:03:08,890
So one of the most important documents that was written

46
00:03:09,310 --> 00:03:12,694
in the medical research ethics world was the Belmont

47
00:03:12,742 --> 00:03:16,490
report in 1979. And this established

48
00:03:17,470 --> 00:03:21,146
new norms for ethical research. This was in response

49
00:03:21,178 --> 00:03:24,798
to some really bad research that

50
00:03:24,804 --> 00:03:27,742
has happened, mistakes that had happened,

51
00:03:27,876 --> 00:03:32,122
and this was a response. So this was decades

52
00:03:32,186 --> 00:03:36,290
ago. So medical research, it has a couple of decades

53
00:03:36,710 --> 00:03:40,562
of a head start on doing things in a more ethical way.

54
00:03:40,696 --> 00:03:46,086
So that's quite interesting. So some of the lessons that

55
00:03:46,108 --> 00:03:50,338
we can use from biomedical research, so some of the standards

56
00:03:50,514 --> 00:03:53,970
that we see there that are now considered part of responsible AI

57
00:03:54,130 --> 00:03:57,410
are the concerns of data transparency.

58
00:03:57,570 --> 00:04:01,670
So when you publish a medical research paper with human subjects,

59
00:04:01,750 --> 00:04:05,162
you have to be very clear about who the human

60
00:04:05,216 --> 00:04:08,700
subjects were, how many people were there,

61
00:04:09,150 --> 00:04:13,386
what was their race and sex,

62
00:04:13,498 --> 00:04:17,194
and what level of education did they attain,

63
00:04:17,322 --> 00:04:20,638
and what part of the country are they from,

64
00:04:20,724 --> 00:04:24,218
that kind of thing. You have to state that really explicitly when you publish a

65
00:04:24,244 --> 00:04:27,822
paper. And that's now being considered part of responsible

66
00:04:27,886 --> 00:04:31,122
AI is being transparent about who was in your data set

67
00:04:31,176 --> 00:04:33,060
and who was not in your data set.

68
00:04:33,990 --> 00:04:38,002
Another standard is informed consent. So initially

69
00:04:38,066 --> 00:04:41,718
it was informed concerns to have your data used in a study.

70
00:04:41,884 --> 00:04:45,782
And in terms of responsible RAI, we're obviously now

71
00:04:45,836 --> 00:04:49,046
asking, have people consented

72
00:04:49,078 --> 00:04:52,810
to having their data used by this machine learning algorithm.

73
00:04:53,470 --> 00:04:56,518
There's also the concept of group harm and individual harm.

74
00:04:56,614 --> 00:05:00,362
So if the study could

75
00:05:00,416 --> 00:05:03,870
harm an individual or even harm the whole group, that that individual

76
00:05:03,940 --> 00:05:07,566
is from. Likewise, in responsible AI, we're starting to

77
00:05:07,588 --> 00:05:10,734
consider both of these types of harm and how to avoid them,

78
00:05:10,852 --> 00:05:14,770
and then privacy. So with medical research, privacy is obviously

79
00:05:14,840 --> 00:05:18,462
about most important, keeping that research data private.

80
00:05:18,606 --> 00:05:22,866
And likewise in responsible AI, we're very interested now know

81
00:05:22,888 --> 00:05:26,630
it's our responsibility to keep people's personal data private.

82
00:05:28,250 --> 00:05:31,954
Because I had this background in biomedical research, I think it had primed

83
00:05:32,002 --> 00:05:35,526
me, and I had this kind of lens for

84
00:05:35,628 --> 00:05:39,226
working in AI, and I

85
00:05:39,248 --> 00:05:43,142
came across a project to work on. But I had ethical

86
00:05:43,206 --> 00:05:46,886
concerns with this particular project. And thankfully,

87
00:05:46,998 --> 00:05:50,958
I was supported by my leadership to ask, should we do it?

88
00:05:51,044 --> 00:05:54,080
Kind of questions, should we do this project?

89
00:05:54,450 --> 00:05:58,334
And the support that I got from my team and

90
00:05:58,372 --> 00:06:02,734
the tools that I discovered for addressing responsible AI issues

91
00:06:02,932 --> 00:06:06,366
have prepared me for recognizing and addressing responsible

92
00:06:06,398 --> 00:06:10,194
AI issues on future projects. So I wanted to share some of

93
00:06:10,392 --> 00:06:14,318
the learnings that I've gathered along the way, in case they're

94
00:06:14,334 --> 00:06:17,954
helpful for you. So now I'm going to talk a little bit about responsible AI

95
00:06:18,002 --> 00:06:22,002
reviews. So we have a responsible

96
00:06:22,066 --> 00:06:25,974
AI review board on my team because we work across really

97
00:06:26,012 --> 00:06:29,702
diverse customer AI projects. They're very complex,

98
00:06:29,766 --> 00:06:33,386
they're in different industries, each one is different, and we see a huge

99
00:06:33,488 --> 00:06:37,174
variety. So we have this responsible

100
00:06:37,222 --> 00:06:41,022
AI review board, and it's a sounding board for people to

101
00:06:41,076 --> 00:06:44,426
express different views and explore different ideas

102
00:06:44,538 --> 00:06:48,206
and ultimately provide responsible AI recommendations for

103
00:06:48,228 --> 00:06:52,314
our projects. And I find that the following

104
00:06:52,362 --> 00:06:55,710
questions are very helpful to ask when thinking about the ethical

105
00:06:55,790 --> 00:06:59,906
implications of an AI project. So, first of all,

106
00:07:00,088 --> 00:07:03,794
let's remember that AI is not magic. So is

107
00:07:03,832 --> 00:07:07,366
this problem actually solvable with RAI or can

108
00:07:07,388 --> 00:07:10,200
it be solved in a simpler way? So, for example,

109
00:07:10,810 --> 00:07:14,406
sometimes a SQL query will do the job.

110
00:07:14,508 --> 00:07:17,846
So can we just write a SQL query? Or do we need

111
00:07:17,868 --> 00:07:21,190
an advanced machine learning algorithm that needs a lot of maintenance

112
00:07:21,270 --> 00:07:25,114
and has a lot of responsibility of its own?

113
00:07:25,312 --> 00:07:29,402
And similar to this is, does this problem have

114
00:07:29,456 --> 00:07:33,598
a technical solution, or is this a problem that could be solved with

115
00:07:33,764 --> 00:07:37,806
some kind of social intervention? So get

116
00:07:37,828 --> 00:07:41,002
that out the way. Do we need technology at all? Do we need AI

117
00:07:41,066 --> 00:07:44,894
at all? If yes, then it's helpful

118
00:07:44,942 --> 00:07:48,066
to think of who are the stakeholders in this project.

119
00:07:48,248 --> 00:07:51,998
So think about each different group that this RAI impacts.

120
00:07:52,094 --> 00:07:54,858
And think especially if there are any vulnerable groups.

121
00:07:54,974 --> 00:07:59,122
So vulnerable groups might be children, the elderly,

122
00:07:59,266 --> 00:08:03,142
immigrant groups, or any groups that have been

123
00:08:03,196 --> 00:08:06,818
historically oppressed. And other stakeholders

124
00:08:06,914 --> 00:08:09,770
might be regulators,

125
00:08:10,270 --> 00:08:13,994
lawmakers, even company and

126
00:08:14,032 --> 00:08:18,246
their companies and their brands. Think about all the different stakeholders

127
00:08:18,358 --> 00:08:21,754
that could be affected by this technology. And once we've

128
00:08:21,802 --> 00:08:25,114
identified a map of different stakeholders,

129
00:08:25,242 --> 00:08:29,114
it can be helpful to think, what are the possible benefits and harms

130
00:08:29,162 --> 00:08:33,250
to each stakeholder so exhaustively list benefits

131
00:08:33,320 --> 00:08:36,660
and has to each it's useful to ask,

132
00:08:37,350 --> 00:08:41,822
does the data used by this code contain personally identifiable

133
00:08:41,886 --> 00:08:45,006
information? Most of the time when we're

134
00:08:45,038 --> 00:08:49,106
training a model, we don't need to know people's names and address and telephone

135
00:08:49,138 --> 00:08:52,550
numbers. So really we don't need to work with that data.

136
00:08:52,700 --> 00:08:56,274
If for some reason we really need that data needs to be handled

137
00:08:56,322 --> 00:08:59,994
in the appropriate ways, it's useful to ask, does this

138
00:09:00,032 --> 00:09:03,526
model impact consequential decisions like blocking

139
00:09:03,558 --> 00:09:06,810
people from getting jobs or looks or health care?

140
00:09:06,880 --> 00:09:10,734
In these situations, we have to be extremely careful, and often in these

141
00:09:10,772 --> 00:09:14,254
situations, the model needs to be explainable to

142
00:09:14,292 --> 00:09:16,880
explain why that decision was made.

143
00:09:17,490 --> 00:09:20,894
A couple more questions to ask are how could

144
00:09:20,932 --> 00:09:24,706
this technology be misused and what could go wrong? And I

145
00:09:24,728 --> 00:09:28,674
like to call this black mirror brainstorming. And the

146
00:09:28,712 --> 00:09:31,934
idea is named after the UK tv series called Black Mirror,

147
00:09:31,982 --> 00:09:35,682
where they explore how technology goes very, very wrong.

148
00:09:35,816 --> 00:09:39,590
Does the model treat different users fairly? Is the model

149
00:09:39,740 --> 00:09:42,806
accurate overall? But is there a particular group that

150
00:09:42,828 --> 00:09:46,386
it's performing very badly for? How does the training data compare

151
00:09:46,418 --> 00:09:49,894
to production data? So if we rai a language

152
00:09:49,942 --> 00:09:53,094
model using tweets,

153
00:09:53,222 --> 00:09:56,586
that is not appropriate for doing a medical literature search,

154
00:09:56,688 --> 00:10:00,210
because it's a very different language. So it's

155
00:10:00,230 --> 00:10:04,830
our responsibility to make sure that those align appropriately.

156
00:10:05,410 --> 00:10:08,826
Another question is, what is the environmental impact of the solution?

157
00:10:08,938 --> 00:10:12,590
And there's more and more interest in this topic recently.

158
00:10:13,110 --> 00:10:16,626
So, for example, if we have a

159
00:10:16,648 --> 00:10:19,940
huge language model that takes days or weeks to train.

160
00:10:20,390 --> 00:10:24,194
That's using a lot of computational power, it's using

161
00:10:24,232 --> 00:10:28,066
a lot of electricity. So what's the environmental impact

162
00:10:28,098 --> 00:10:31,846
of that? It's worth thinking about. And then how

163
00:10:31,868 --> 00:10:35,334
can we address concerns that arise? So, through answering all these

164
00:10:35,372 --> 00:10:39,126
questions, what concerns have arisen? Do we need to reformulate

165
00:10:39,158 --> 00:10:42,140
the problem, rethink it?

166
00:10:42,910 --> 00:10:46,458
And are there some risks that we can mitigate? And I'm going

167
00:10:46,464 --> 00:10:49,930
to discuss some tools that we might use shortly.

168
00:10:50,610 --> 00:10:54,282
I did want to highlight some special considerations for healthcare

169
00:10:54,426 --> 00:10:57,774
and responsible AI. And the first

170
00:10:57,812 --> 00:11:01,466
one that might come to mind for people is privacy.

171
00:11:01,658 --> 00:11:05,234
So health data is

172
00:11:05,272 --> 00:11:09,010
extremely sensitive and private, especially genetic data,

173
00:11:09,080 --> 00:11:12,594
which tells us so much about a person and even

174
00:11:12,632 --> 00:11:16,920
their family members. So it's extremely important to

175
00:11:17,610 --> 00:11:21,986
maintain that privacy and not let information leak

176
00:11:22,178 --> 00:11:25,622
through the model somehow. And on a similar note is security.

177
00:11:25,756 --> 00:11:29,034
So we need to follow the right

178
00:11:29,232 --> 00:11:33,590
regulatory requirements to handle data securely.

179
00:11:33,750 --> 00:11:37,606
And sometimes that means doing a lot of training, like HIPAA

180
00:11:37,638 --> 00:11:41,198
training, to be compliant with handling that kind of

181
00:11:41,204 --> 00:11:45,274
sensitive data. I think an important aspect of responsible AI

182
00:11:45,322 --> 00:11:48,058
in health is collaborating with domain experts.

183
00:11:48,154 --> 00:11:51,866
This is crucial, I believe, for all machine learning practitioners,

184
00:11:52,058 --> 00:11:55,554
especially so in healthcare. Are there doctors or

185
00:11:55,592 --> 00:11:59,506
nurses or even patients who can do

186
00:11:59,528 --> 00:12:00,660
a sense check?

187
00:12:03,350 --> 00:12:07,506
Do you have access to that domain expert? That's really important if

188
00:12:07,528 --> 00:12:10,886
you're working on a healthcare project. And then there

189
00:12:10,908 --> 00:12:14,486
is this idea of open, oversees, closed science, so we want

190
00:12:14,508 --> 00:12:18,550
to get the balance right. So on one hand, we have open science where,

191
00:12:18,700 --> 00:12:23,222
say, we have sequencer genome for cardiovascular

192
00:12:23,286 --> 00:12:26,794
research. But hey, this data

193
00:12:26,832 --> 00:12:30,102
set could actually be really useful for respiratory research

194
00:12:30,176 --> 00:12:33,290
as well. So could we share it with those researchers?

195
00:12:33,450 --> 00:12:36,960
Because that could benefit everyone.

196
00:12:37,570 --> 00:12:41,182
So that's open science, and we've got to balance that

197
00:12:41,236 --> 00:12:44,746
with keeping people's data private

198
00:12:44,858 --> 00:12:48,580
and secure, so we have to get that balance right.

199
00:12:48,950 --> 00:12:52,850
There's also the issue of unequal access to healthcare. So that's really something

200
00:12:52,920 --> 00:12:56,120
that we have to keep in the forefront of our mind.

201
00:12:56,570 --> 00:13:00,422
Healthcare people in wealthier parts of the world have better access

202
00:13:00,476 --> 00:13:04,726
to healthcare. And something that

203
00:13:04,908 --> 00:13:08,266
I have found in the USA, because I've recently moved to the

204
00:13:08,288 --> 00:13:11,754
USA, is how important it is to consider the

205
00:13:11,792 --> 00:13:15,526
bias that's introduced to the cost of healthcare, because healthcare

206
00:13:15,558 --> 00:13:17,530
is so expensive in the USA.

207
00:13:20,130 --> 00:13:23,626
Any data about billing costs,

208
00:13:23,738 --> 00:13:27,262
prices, we have to be really careful of,

209
00:13:27,316 --> 00:13:30,878
because it can contain a lot of bias because

210
00:13:30,964 --> 00:13:34,066
of unequal access to health care. And I'm going to show an

211
00:13:34,088 --> 00:13:37,906
example of that shortly. And then lastly, race and

212
00:13:37,928 --> 00:13:41,294
sex can be extremely important disease

213
00:13:41,342 --> 00:13:45,650
predictors as we've seen with COVID it affects

214
00:13:46,010 --> 00:13:49,286
different groups differently. However,

215
00:13:49,388 --> 00:13:52,678
race and sex can also introduce a lot of bias into the model,

216
00:13:52,764 --> 00:13:57,506
because historically these groups have been unfairly

217
00:13:57,538 --> 00:14:01,414
treated in healthcare. What I found works quite well,

218
00:14:01,452 --> 00:14:05,206
actually, is not just turning away race and sex and just ignoring

219
00:14:05,238 --> 00:14:08,934
them completely, because a model can still be biased without these features,

220
00:14:08,982 --> 00:14:12,878
as I'll show in the next slide. But what works really well

221
00:14:13,044 --> 00:14:17,038
is to capture that data and keep that data so that you can actually

222
00:14:17,124 --> 00:14:20,414
audit how fair your model is

223
00:14:20,452 --> 00:14:24,158
for those different groups. But you can only do that if you have the data.

224
00:14:24,324 --> 00:14:27,954
That's my recommendation, is that they're actually really helpful to have.

225
00:14:28,152 --> 00:14:31,710
So this is a really interesting paper by Obermayer Etel.

226
00:14:31,790 --> 00:14:35,070
It's called dissecting racial bias in an algorithm used to manage the

227
00:14:35,080 --> 00:14:39,446
health of populations. And they show how an

228
00:14:39,468 --> 00:14:42,806
algorithm that was actually used in production in the

229
00:14:42,828 --> 00:14:46,646
USA did not use race as

230
00:14:46,668 --> 00:14:49,674
a feature at all, but it was still

231
00:14:49,712 --> 00:14:53,290
very racially biased. So I think this is definitely

232
00:14:53,360 --> 00:14:56,842
worth checking out this paper. So now I'm going

233
00:14:56,896 --> 00:14:59,974
to talk about some responsible AI

234
00:15:00,022 --> 00:15:03,322
tools that you could find helpful.

235
00:15:03,466 --> 00:15:06,798
So the first tool helps us answer the question,

236
00:15:06,884 --> 00:15:10,334
is there a good representation of all users? And this tool

237
00:15:10,372 --> 00:15:13,906
is called data sheets for data sets. And I really like the

238
00:15:13,928 --> 00:15:17,806
idea because it comes from electronic

239
00:15:17,838 --> 00:15:21,458
engineering, where if you buy

240
00:15:21,624 --> 00:15:24,734
an electrical component, like a little microcontroller,

241
00:15:24,862 --> 00:15:28,050
you always get a data sheet with it,

242
00:15:28,120 --> 00:15:31,366
and the data sheet tells you all about that component, how to connect it,

243
00:15:31,388 --> 00:15:34,742
what the operating temperatures are, and so on. And the idea

244
00:15:34,796 --> 00:15:38,802
is that when you build a data set, you should compile

245
00:15:38,866 --> 00:15:42,666
a data sheet too, explaining how the

246
00:15:42,688 --> 00:15:46,218
data set has collected, who was in the

247
00:15:46,224 --> 00:15:49,162
data set, who was not in the data set, what limitations there are,

248
00:15:49,216 --> 00:15:52,490
so that every data set is accompanied by a data sheet.

249
00:15:53,310 --> 00:15:56,950
Another tool I would recommend using is it

250
00:15:56,960 --> 00:16:00,826
helps us answer the question of whether a model treats different users

251
00:16:00,858 --> 00:16:04,990
fairly. And one particular tool is called Fairlearn.

252
00:16:05,330 --> 00:16:09,534
Fairlearn is produced by Microsoft. It's an open source Python

253
00:16:09,582 --> 00:16:12,994
package. And here I've used

254
00:16:13,032 --> 00:16:17,362
it to look at overall accuracy. The first line,

255
00:16:17,496 --> 00:16:20,994
so this was a model which had an area under the RoC curve

256
00:16:21,042 --> 00:16:24,694
of 92%, which is great, but then it helps you

257
00:16:24,732 --> 00:16:27,946
break down accuracy by different groups. So we can see how

258
00:16:27,968 --> 00:16:32,422
well the model performed for female and non female

259
00:16:32,566 --> 00:16:35,020
people in this example,

260
00:16:35,390 --> 00:16:39,322
and we could also break down the accuracy for different races and see

261
00:16:39,376 --> 00:16:42,218
how accurate it is for each of these different races.

262
00:16:42,394 --> 00:16:45,614
And you can use any sensitive feature here to check

263
00:16:45,652 --> 00:16:49,422
how your model is performing. And then the

264
00:16:49,476 --> 00:16:52,754
last tool I wanted to mention helps us deal

265
00:16:52,792 --> 00:16:56,194
with models that need to be explainable. This tool is called

266
00:16:56,232 --> 00:17:00,078
interpretml. It's also an open source Python package developed

267
00:17:00,094 --> 00:17:04,954
by Microsoft, and it's a whole suite of functionality

268
00:17:05,102 --> 00:17:08,994
and visualizations, and also a model called the explainable

269
00:17:09,042 --> 00:17:13,158
boosting machine, which prioritizes explainability without

270
00:17:13,244 --> 00:17:16,866
sacrificing accuracy. Actually. And here's

271
00:17:16,898 --> 00:17:20,118
an example of the explainable boosting machine applied

272
00:17:20,134 --> 00:17:25,034
to the adult income data set where we're trying to predict who

273
00:17:25,072 --> 00:17:28,474
earns more than $50,000 a year. And you can see

274
00:17:28,512 --> 00:17:32,186
that it gives a weighting to each of the different features

275
00:17:32,218 --> 00:17:36,570
to say how important that feature was for this prediction.

276
00:17:36,730 --> 00:17:40,922
So for this person, we can see why the model decided

277
00:17:40,986 --> 00:17:44,322
whether or not they earn more than $50,000. So in

278
00:17:44,376 --> 00:17:47,358
orange we can see what was for that decision.

279
00:17:47,534 --> 00:17:51,554
So like number of years of education and

280
00:17:51,592 --> 00:17:54,750
in blue we can see what has against that decision.

281
00:17:54,830 --> 00:17:58,840
So for example, marital status and age,

282
00:17:59,210 --> 00:18:02,386
and we can see what positively affects and negatively

283
00:18:02,418 --> 00:18:06,200
affects the decision about whether someone earns more than $50,000.

284
00:18:07,550 --> 00:18:11,450
So it's very transparent and explainable, which is

285
00:18:11,520 --> 00:18:15,766
great. So I wanted to share some resources.

286
00:18:15,958 --> 00:18:19,766
I am a machine learning practitioner and I look to machine

287
00:18:19,798 --> 00:18:23,374
learning researchers that are at the forefront of thinking about this

288
00:18:23,412 --> 00:18:27,294
topic and I like to read and

289
00:18:27,332 --> 00:18:31,210
follow. Kathy O'Neill, Hannah Wallach, Temnet Jabru,

290
00:18:31,290 --> 00:18:34,894
Rachel Thomas, Deborah Rai, Kate Crawford, Arvind Narayanan

291
00:18:34,942 --> 00:18:38,738
just to name a few people. I would definitely recommend

292
00:18:38,904 --> 00:18:42,370
watching the coded bias documentary on Netflix.

293
00:18:42,790 --> 00:18:46,354
This is a great primer if you're new to this idea of responsible

294
00:18:46,402 --> 00:18:49,954
AI. Kate Crawford's book Atlas of AI

295
00:18:50,002 --> 00:18:53,942
just came out. I'm also looking forward to reading the

296
00:18:53,996 --> 00:18:57,694
redesigning AI book by Darren Ashimoglu, which is a collection

297
00:18:57,762 --> 00:19:01,386
of essays from some of these people. And here is

298
00:19:01,408 --> 00:19:04,326
a link to the Obama racial bias article,

299
00:19:04,518 --> 00:19:08,330
also to the different Microsoft responsible AI tools.

300
00:19:08,750 --> 00:19:12,586
And another resource is the GitHub repo

301
00:19:12,618 --> 00:19:15,742
that my team has where we share our best

302
00:19:15,796 --> 00:19:19,070
practices for engineering and machine learning fundamentals,

303
00:19:19,650 --> 00:19:22,894
including responsible AI. So we've shared that

304
00:19:22,932 --> 00:19:26,110
at this link. And then lastly, my team,

305
00:19:26,180 --> 00:19:30,014
commercial software engineering, the applied machine learning team is

306
00:19:30,052 --> 00:19:33,646
hiring in a number of different places. You can find our

307
00:19:33,668 --> 00:19:36,822
open job roles at this link and

308
00:19:36,876 --> 00:19:40,086
thank you very much. It's very easy to find

309
00:19:40,108 --> 00:19:43,590
me on Twitter and LinkedIn. And thanks very much to

310
00:19:43,740 --> 00:19:45,620
conf 42 for having me.

