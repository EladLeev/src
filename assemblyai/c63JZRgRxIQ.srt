1
00:00:00,410 --> 00:00:06,126
Jamaica make up real

2
00:00:06,148 --> 00:00:10,618
time feedback into the behavior of your distributed systems and observing

3
00:00:10,714 --> 00:00:14,046
changes exceptions. Errors in real time

4
00:00:14,148 --> 00:00:17,914
allows you to not only experiment with confidence, but respond

5
00:00:18,042 --> 00:00:20,480
instantly to get things working again.

6
00:00:24,610 --> 00:01:05,134
Cloud hi,

7
00:01:05,252 --> 00:01:08,578
my name is John Engelkemnetz and I'm principal program manager

8
00:01:08,664 --> 00:01:12,718
manager on the Azure Chaos studio team here at Microsoft.

9
00:01:12,814 --> 00:01:17,138
To talk to you a little bit about how we do chaos engineering at Microsoft

10
00:01:17,314 --> 00:01:20,466
using our new service, Azure Chaos Studio,

11
00:01:20,578 --> 00:01:24,322
and to tell you a little bit about some of our learnings in doing chaos

12
00:01:24,386 --> 00:01:27,846
engineering. So to start us off, I do

13
00:01:27,868 --> 00:01:31,094
like to talk a little bit about this concept of resilience,

14
00:01:31,222 --> 00:01:35,066
as well as what it means to establish and maintain quality

15
00:01:35,248 --> 00:01:38,586
in the cloud. So we know that resilience is

16
00:01:38,608 --> 00:01:42,650
the capability of a system to handle and recover from disruptions.

17
00:01:42,810 --> 00:01:47,210
And a disruption can be anything from a major outage

18
00:01:47,290 --> 00:01:50,494
that drops availability to 0% for

19
00:01:50,532 --> 00:01:54,114
a long time window to something much more

20
00:01:54,152 --> 00:01:57,682
minor, say a deviation in

21
00:01:57,736 --> 00:02:01,794
the availability that is only slight, a sudden high

22
00:02:01,832 --> 00:02:05,606
amount of stress, higher latency, et cetera. All of these are

23
00:02:05,628 --> 00:02:08,934
examples of more brownout type cases where

24
00:02:08,972 --> 00:02:12,854
there is still some disruption to the service, even though

25
00:02:12,892 --> 00:02:16,502
it is not a complete and utter in availability of that

26
00:02:16,556 --> 00:02:19,674
service. Now, regardless of whether it is a

27
00:02:19,712 --> 00:02:23,340
brownout or a true blackout major outage event,

28
00:02:23,950 --> 00:02:27,242
any sort of impact that is disruptive to

29
00:02:27,296 --> 00:02:30,800
the availability and performance of your service

30
00:02:31,490 --> 00:02:34,606
is going to impact customer experience. And we know

31
00:02:34,628 --> 00:02:37,818
that when there are outages that impact availability,

32
00:02:37,994 --> 00:02:41,774
there is business impact. You can have upset customers,

33
00:02:41,892 --> 00:02:45,570
you can lose revenue. And a key thing

34
00:02:45,640 --> 00:02:49,762
with chaos Engineering is being able to measure the impact to

35
00:02:49,816 --> 00:02:52,946
your business when there is an outage, in terms of that

36
00:02:53,048 --> 00:02:56,642
cost to the business, whether it be lost

37
00:02:56,706 --> 00:03:00,722
revenue or lost

38
00:03:00,786 --> 00:03:04,338
sales or anything that might fit into that category.

39
00:03:04,514 --> 00:03:07,698
But beyond the simple business impact of an

40
00:03:07,724 --> 00:03:10,970
outage, what we found running a major

41
00:03:11,040 --> 00:03:14,662
cloud provider is that our customers are running mission critical

42
00:03:14,726 --> 00:03:17,862
apps on Azure, and that means that beyond

43
00:03:18,006 --> 00:03:21,290
lost revenue, there can be major legal

44
00:03:21,370 --> 00:03:25,210
consequences of an outage, and even in some cases,

45
00:03:25,370 --> 00:03:29,082
life or death consequences. So in a legal

46
00:03:29,146 --> 00:03:33,138
example, many financial institutions need to provide

47
00:03:33,224 --> 00:03:37,250
audit evidence that they can successfully recover from

48
00:03:37,320 --> 00:03:40,642
a disaster. If that does not

49
00:03:40,696 --> 00:03:44,270
remain true, there can be legal consequences

50
00:03:44,350 --> 00:03:48,322
from a government. Another example in the life and safety

51
00:03:48,386 --> 00:03:51,954
area is emergency services. Increasingly,

52
00:03:52,002 --> 00:03:55,554
emergency services operate on top of cloud providers,

53
00:03:55,602 --> 00:03:59,258
and an outage in an emergency service might be the difference

54
00:03:59,344 --> 00:04:02,938
between an ambulance getting to where it needs to go on time

55
00:04:03,104 --> 00:04:07,494
or that ambulance not being able to respond

56
00:04:07,542 --> 00:04:10,450
in an emergency situation as appropriately.

57
00:04:10,550 --> 00:04:16,122
So we take this really seriously, knowing that businesses

58
00:04:16,266 --> 00:04:20,302
the stock market, finances as

59
00:04:20,356 --> 00:04:24,494
well as life and death scenarios and legal

60
00:04:24,542 --> 00:04:28,318
consequences can happen when there's an availability due

61
00:04:28,334 --> 00:04:31,922
to a service outage. Now at Microsoft, we think

62
00:04:31,976 --> 00:04:35,302
that building quality into the entire service

63
00:04:35,356 --> 00:04:38,566
development and operation lifecycle is the right way

64
00:04:38,588 --> 00:04:41,750
to tackle this challenge. And when we talk about

65
00:04:41,820 --> 00:04:45,190
building that quality into the entire service

66
00:04:45,260 --> 00:04:48,826
development and operation lifecycle, we really mean two things.

67
00:04:48,928 --> 00:04:52,582
The first is thinking about quality from the beginning

68
00:04:52,646 --> 00:04:56,374
of the ideation of a new service through the development

69
00:04:56,422 --> 00:05:00,638
of that service, and through the deployment and

70
00:05:00,804 --> 00:05:04,154
operation of the service. Now that continues

71
00:05:04,202 --> 00:05:07,934
through to the continuous deployment and development of that service,

72
00:05:08,052 --> 00:05:11,866
and even maintaining quality through deprecation

73
00:05:11,978 --> 00:05:15,474
of a legacy service. The other thing that this means

74
00:05:15,512 --> 00:05:18,830
to us at Microsoft is that beyond

75
00:05:18,910 --> 00:05:22,402
simply making quality something that our

76
00:05:22,456 --> 00:05:26,214
site reliability engineers and DevOps engineers think about,

77
00:05:26,332 --> 00:05:29,874
quality has to be something that is a part of the culture

78
00:05:30,002 --> 00:05:33,858
of the entire company. And that means including

79
00:05:33,954 --> 00:05:38,038
leaders, managers, as well as other folks

80
00:05:38,134 --> 00:05:42,134
involved in the building and development of applications.

81
00:05:42,262 --> 00:05:44,730
So product managers,

82
00:05:45,390 --> 00:05:48,534
testers, folks who are doing marketing,

83
00:05:48,582 --> 00:05:52,494
even getting them involved in thinking about quality of

84
00:05:52,532 --> 00:05:56,138
the services from a business perspective,

85
00:05:56,314 --> 00:05:59,438
help to reinforce the importance of quality.

86
00:05:59,604 --> 00:06:03,202
As a product manager, my accountability is not just

87
00:06:03,256 --> 00:06:07,422
to additional users or additional revenue,

88
00:06:07,566 --> 00:06:11,042
it is also to having a service that is

89
00:06:11,096 --> 00:06:14,542
quality. Quality is a customer requirement,

90
00:06:14,686 --> 00:06:18,082
and that means that both me as an individual contributor

91
00:06:18,146 --> 00:06:22,022
as well as my management chain all have to be thinking about

92
00:06:22,076 --> 00:06:26,130
quality and prioritizing it as a fundamental,

93
00:06:26,290 --> 00:06:30,038
similar to security that everyone takes seriously

94
00:06:30,134 --> 00:06:33,594
and contributes to as a new service is being

95
00:06:33,632 --> 00:06:36,858
built or while operating an existing service.

96
00:06:37,024 --> 00:06:40,686
At Microsoft, one initiative we're doing to tackle this

97
00:06:40,788 --> 00:06:44,382
is making sure that as part of the core priorities for

98
00:06:44,436 --> 00:06:48,010
every employee who works in our Azure division,

99
00:06:48,170 --> 00:06:52,442
we're including that we think about quality as

100
00:06:52,516 --> 00:06:56,222
one of those core priorities, and then we're measuring

101
00:06:56,286 --> 00:07:00,254
ourselves so that everyone is accountable for contributing

102
00:07:00,302 --> 00:07:04,130
to improvements in quality in some way,

103
00:07:04,200 --> 00:07:07,366
shape or form. Now, all of

104
00:07:07,388 --> 00:07:10,630
this becomes particularly important when

105
00:07:10,700 --> 00:07:13,746
we're talking about cloud applications.

106
00:07:13,938 --> 00:07:17,334
There are two interesting aspects of cloud applications that

107
00:07:17,372 --> 00:07:20,666
make building resilience a little bit more challenging than it may

108
00:07:20,688 --> 00:07:24,154
have been on premises. The first aspect of

109
00:07:24,192 --> 00:07:27,930
this is that the architecture types for

110
00:07:28,000 --> 00:07:31,434
cloud applications tend to be highly distributed,

111
00:07:31,562 --> 00:07:35,502
highly complex, and oftentimes less

112
00:07:35,556 --> 00:07:39,054
familiar to folks who are using those.

113
00:07:39,172 --> 00:07:42,910
So while there are enormous benefits in leveraging

114
00:07:43,830 --> 00:07:47,682
services like Azure Kubernetes service or Azure app

115
00:07:47,736 --> 00:07:51,678
services, there is a slight drawback

116
00:07:51,774 --> 00:07:55,306
in that the patterns for building resilience

117
00:07:55,358 --> 00:07:59,042
of those services may be a little bit less mature.

118
00:07:59,186 --> 00:08:02,754
And certainly knowledge of how to leverage those patterns

119
00:08:02,802 --> 00:08:06,086
within any given organization might be lower.

120
00:08:06,268 --> 00:08:10,342
So using cloud native applications

121
00:08:10,406 --> 00:08:14,122
can increase resilience by virtue of the fact that these are

122
00:08:14,256 --> 00:08:18,234
built to be resilient to failure. But there

123
00:08:18,272 --> 00:08:22,046
is this consequence of potentially lower knowledge and

124
00:08:22,148 --> 00:08:26,062
lower ability to execute on having those best

125
00:08:26,116 --> 00:08:29,760
practices built in when developing a cloud native application.

126
00:08:30,130 --> 00:08:33,346
The other part about migration to the cloud that can

127
00:08:33,368 --> 00:08:36,690
be challenging is the sudden increased

128
00:08:37,270 --> 00:08:41,362
difference or distance between the

129
00:08:41,496 --> 00:08:44,986
cloud consumer and the application that you've written

130
00:08:45,118 --> 00:08:48,600
and the underlying compute that's going to run

131
00:08:49,450 --> 00:08:52,902
that code. So depending on the service type

132
00:08:52,956 --> 00:08:56,374
you choose, let's say you're developing a serverless application,

133
00:08:56,572 --> 00:09:00,182
there may be three, four, five layers of compute

134
00:09:00,246 --> 00:09:04,122
between the code you've written and the actual code

135
00:09:04,176 --> 00:09:06,762
that is running in our physical data center.

136
00:09:06,896 --> 00:09:10,754
Now, that benefits you as a cloud consumer

137
00:09:10,822 --> 00:09:14,110
because you benefit from the scale and cost

138
00:09:14,180 --> 00:09:17,534
efficiency of the cloud. You also benefit from the

139
00:09:17,572 --> 00:09:21,262
resilience that can get built at scale by a large

140
00:09:21,316 --> 00:09:24,738
scale cloud provider like Azure. However,

141
00:09:24,904 --> 00:09:28,718
it does mean that there's these abstractions

142
00:09:28,894 --> 00:09:32,702
that can mean sometimes you're at the mercy of the cloud provider

143
00:09:32,766 --> 00:09:36,406
when it comes to resilience, and there's plenty that can

144
00:09:36,428 --> 00:09:40,470
be done to defend against a failure in your underlying cloud

145
00:09:40,540 --> 00:09:44,134
provider. But sometimes you're really just

146
00:09:44,172 --> 00:09:47,414
sort of hoping that the platform is

147
00:09:47,452 --> 00:09:51,046
stable, because if there is an issue in the underlying

148
00:09:51,078 --> 00:09:54,650
platform, there's not much you can do to avoid

149
00:09:55,550 --> 00:09:58,986
that becoming an issue for your upstream service.

150
00:09:59,168 --> 00:10:02,654
And this is why we believe that much like the

151
00:10:02,772 --> 00:10:06,474
security pillar of cloud development

152
00:10:06,602 --> 00:10:10,190
with resilience, there is a shared responsibility between

153
00:10:10,260 --> 00:10:13,858
the cloud provider and the cloud consumer. And when we

154
00:10:13,864 --> 00:10:18,098
use this term shared responsibility, what we mean is that we both

155
00:10:18,184 --> 00:10:21,954
have a shared accountability for ensuring that our

156
00:10:21,992 --> 00:10:24,718
applications are robust, redundant,

157
00:10:24,814 --> 00:10:27,942
reliable, so that your

158
00:10:27,996 --> 00:10:32,082
downstream customer, the consumer of your applications,

159
00:10:32,226 --> 00:10:35,606
don't see downtime. Now, if the cloud provider were

160
00:10:35,628 --> 00:10:39,334
to have 100% availability, but the cloud consumer

161
00:10:39,382 --> 00:10:43,354
were to have not implemented best practices in terms

162
00:10:43,392 --> 00:10:47,434
of resilience, or in the alternate case where a

163
00:10:47,472 --> 00:10:51,882
cloud consumer has implemented every best practice

164
00:10:51,946 --> 00:10:56,314
available to defend against any sort of failure, but the cloud provider

165
00:10:56,362 --> 00:11:00,522
is just simply having horrible SLA

166
00:11:00,586 --> 00:11:04,974
attainment and constant outages. In either of those scenarios,

167
00:11:05,102 --> 00:11:08,994
there will still be downstream impact to a customer. And that's why

168
00:11:09,032 --> 00:11:13,198
we believe that as the cloud provider, we need a solution

169
00:11:13,294 --> 00:11:17,240
that helps our customers to become resilient and to

170
00:11:17,610 --> 00:11:20,710
defend against an issue that can happen either

171
00:11:20,780 --> 00:11:24,246
within their own service, or an issue that could happen in

172
00:11:24,268 --> 00:11:28,294
the underlying platform that impacts a service depending

173
00:11:28,342 --> 00:11:32,026
on that platform. So we believe that we need to provide that sort

174
00:11:32,048 --> 00:11:36,730
of solution, as well as continue to meet our responsibility

175
00:11:37,150 --> 00:11:40,574
in our shared responsibility for continuing to

176
00:11:40,612 --> 00:11:44,414
up our availability and our resilience of the

177
00:11:44,452 --> 00:11:47,806
services that you depend on. So all of

178
00:11:47,828 --> 00:11:51,754
this is highly relevant to Azure Chaos Studio, where Azure Chaos

179
00:11:51,802 --> 00:11:56,018
Studio, the exact same product that we make available to

180
00:11:56,104 --> 00:12:00,254
customers that run on Azure, is what we're using within Azure

181
00:12:00,382 --> 00:12:03,778
across Microsoft cloud service teams to

182
00:12:03,864 --> 00:12:07,414
enable us to improve our availability by

183
00:12:07,452 --> 00:12:10,566
doing chaos engineering. So let's talk a little

184
00:12:10,588 --> 00:12:14,098
bit about this concept of chaos engineering.

185
00:12:14,274 --> 00:12:18,022
Microsoft's approach to chaos engineering fits very well

186
00:12:18,076 --> 00:12:22,154
with the models and approach that are out in the industry and

187
00:12:22,192 --> 00:12:25,638
that many of the experts in site reliability

188
00:12:25,734 --> 00:12:29,514
engineering have developed over time. One thing we do like to

189
00:12:29,552 --> 00:12:33,530
emphasize is the importance of leveraging the scientific

190
00:12:33,610 --> 00:12:37,162
method when going to do chaos engineering

191
00:12:37,226 --> 00:12:40,714
so that your chaos isn't simply chaos for Chaos's

192
00:12:40,762 --> 00:12:43,774
sake, but rather is controlled chaos,

193
00:12:43,902 --> 00:12:47,790
structured chaos, chaos that has a definitive

194
00:12:47,870 --> 00:12:52,190
outcome and results in some sort of tangible improvement.

195
00:12:52,350 --> 00:12:55,634
So if you're familiar with chaos engineering, you're likely

196
00:12:55,682 --> 00:12:58,934
familiar with the idea of starting with an understanding of your

197
00:12:58,972 --> 00:13:02,614
steady state, having appropriate observability and

198
00:13:02,652 --> 00:13:07,106
health modeling such that you can identify an SLI,

199
00:13:07,218 --> 00:13:11,354
a service level indicator, and a service level objective that

200
00:13:11,392 --> 00:13:15,350
are going to kind of be your bar for availability,

201
00:13:15,510 --> 00:13:18,586
and then leverage that steady state to

202
00:13:18,608 --> 00:13:22,798
formulate a hypothesis about your system where we say we believe

203
00:13:22,884 --> 00:13:26,638
that we won't deviate from the steady state more than

204
00:13:26,724 --> 00:13:30,366
a certain percentage given some particular

205
00:13:30,468 --> 00:13:34,020
failure scenario happening within our application.

206
00:13:34,390 --> 00:13:37,682
Now with that hypothesis, you can then go and create

207
00:13:37,736 --> 00:13:40,978
a chaos experiment and run that experiment to

208
00:13:41,064 --> 00:13:44,834
assess whether your hypothesis was valid or invalid, and that

209
00:13:44,872 --> 00:13:48,386
allows you to do some analysis to understand were we resilient

210
00:13:48,418 --> 00:13:51,846
to that failure. If not, we have some work to do to

211
00:13:51,868 --> 00:13:55,814
dig deeper into the logs, the traces, to understand exactly

212
00:13:55,932 --> 00:13:59,302
why our hypothesis was invalid, and that

213
00:13:59,356 --> 00:14:03,178
inevitably teams to some sort of improvement in the service.

214
00:14:03,344 --> 00:14:07,302
And this is cyclical, both because you're going to continuously

215
00:14:07,366 --> 00:14:10,682
want to up the bar in terms of the quality of your service,

216
00:14:10,816 --> 00:14:14,606
but also because services are going to continue to change,

217
00:14:14,708 --> 00:14:18,522
whether it is a service that is growing and there's continuous

218
00:14:18,586 --> 00:14:22,046
development happening on that service, or if it's simply the

219
00:14:22,068 --> 00:14:25,474
fact that over time the platform that

220
00:14:25,512 --> 00:14:29,966
your service depends on has mandatory upgrades, say upgrades

221
00:14:29,998 --> 00:14:33,458
to your version of kubernetes, upgrades to your operating system,

222
00:14:33,544 --> 00:14:36,854
version upgrades to your version of net or

223
00:14:36,892 --> 00:14:40,518
python or whatever that libraries are that you depend

224
00:14:40,604 --> 00:14:43,974
on, some of those will be forced on your service. And so

225
00:14:44,012 --> 00:14:47,590
that means that maintaining resilience against certain

226
00:14:47,660 --> 00:14:51,366
scenarios requires that you're thinking about this cyclically,

227
00:14:51,478 --> 00:14:54,410
and not just as a one time activity.

228
00:14:54,830 --> 00:14:58,378
Now at Microsoft, we also believe that chaos engineering can

229
00:14:58,384 --> 00:15:01,562
be used in a wide variety of scenarios from those

230
00:15:01,616 --> 00:15:05,390
that we hear of as shift right scenarios where

231
00:15:05,460 --> 00:15:08,906
we're being game days, business continuity disaster

232
00:15:08,938 --> 00:15:13,634
recovery drills, ensuring that

233
00:15:13,672 --> 00:15:17,426
our live site tooling and observability data covers all

234
00:15:17,448 --> 00:15:21,214
of our key scenarios. But also we believe strongly

235
00:15:21,262 --> 00:15:24,414
in pulling those quality learnings earlier

236
00:15:24,462 --> 00:15:27,686
into the cycle. So we prevent any regressions in what

237
00:15:27,708 --> 00:15:31,618
we've done in shift write quality validation.

238
00:15:31,794 --> 00:15:35,730
Now when to use shift write quality validation

239
00:15:35,810 --> 00:15:39,638
versus pulling something into your CI CD pipeline

240
00:15:39,734 --> 00:15:43,830
and leveraging that as a gate to a deployment

241
00:15:43,910 --> 00:15:47,226
being able to be flighted outwards? Well, I think a

242
00:15:47,248 --> 00:15:50,574
major factor here involves whether or not you

243
00:15:50,612 --> 00:15:54,970
need real customer traffic or really well simulated

244
00:15:55,050 --> 00:15:59,130
customer traffic. If there is a certain scenario

245
00:15:59,210 --> 00:16:02,814
where you can generate load on demand and you

246
00:16:02,852 --> 00:16:06,382
only need load for a specific amount of time, and that load

247
00:16:06,446 --> 00:16:10,146
doesn't have to be as random or fit the exact

248
00:16:10,248 --> 00:16:13,826
patterns or scale of true production customer

249
00:16:13,928 --> 00:16:17,158
traffic, well, that's something that you can generate via a

250
00:16:17,164 --> 00:16:20,246
load test and then perform chaos engineering in your

251
00:16:20,268 --> 00:16:24,200
CI CD pipeline. But there are plenty of cases where

252
00:16:24,650 --> 00:16:28,474
shifting right means having some percentage of

253
00:16:28,512 --> 00:16:32,470
real customer traffic or really well simulated

254
00:16:32,550 --> 00:16:36,314
synthetic traffic that would make your

255
00:16:36,352 --> 00:16:41,162
service appear to be undergoing real stress from users.

256
00:16:41,306 --> 00:16:45,102
Now shift right an interesting thing we found since

257
00:16:45,156 --> 00:16:48,846
we've introduced Azure Chaos studio is that the sort of

258
00:16:48,868 --> 00:16:51,914
colloquial wisdom that chaos engineering

259
00:16:51,962 --> 00:16:55,470
needs to happen in production, that wisdom

260
00:16:55,630 --> 00:16:59,122
may not apply very well to the mission critical services

261
00:16:59,256 --> 00:17:03,362
that a number of our customers are

262
00:17:03,416 --> 00:17:06,726
building and are running on the azure cloud. So when

263
00:17:06,748 --> 00:17:09,974
it comes to a shift right scenario being done in

264
00:17:10,012 --> 00:17:13,926
production versus pre production, we believe that there

265
00:17:13,948 --> 00:17:18,182
is a very useful and valid case for when you

266
00:17:18,236 --> 00:17:21,926
should be doing chaos in production. But there's also plenty

267
00:17:21,958 --> 00:17:25,770
of cases where chaos should really be done or start

268
00:17:25,840 --> 00:17:29,258
in pre production. The first case we hear from customers

269
00:17:29,344 --> 00:17:33,034
is simply, hey, we have not built up that confidence

270
00:17:33,082 --> 00:17:36,702
yet in a particular failure scenario to go cause

271
00:17:36,756 --> 00:17:40,746
that failure in production. So it's the beginning of our journey

272
00:17:40,778 --> 00:17:44,654
with availability zones, or we're just beginning to

273
00:17:44,692 --> 00:17:48,178
stress test a new application, chances are you're going to

274
00:17:48,184 --> 00:17:51,906
want to start in pre production where there's less risk before

275
00:17:52,008 --> 00:17:55,522
moving that sort of test out into the production environment.

276
00:17:55,586 --> 00:17:59,810
And the production test becomes more of a final checkbox

277
00:17:59,890 --> 00:18:03,270
validation that everything's working as expected.

278
00:18:03,930 --> 00:18:07,866
The other thing that comes up with shift right being in

279
00:18:07,888 --> 00:18:11,702
production versus pre production is risk tolerance

280
00:18:11,766 --> 00:18:15,626
when it comes to a particular failure. If you are a

281
00:18:15,648 --> 00:18:19,414
mission critical application, if you are that healthcare provider

282
00:18:19,542 --> 00:18:23,006
that is determining whether prescriptions are

283
00:18:23,028 --> 00:18:26,558
issued for emergency medical needs,

284
00:18:26,724 --> 00:18:30,686
chances are you may say that the risk of

285
00:18:30,868 --> 00:18:34,766
an injected fault in production

286
00:18:34,878 --> 00:18:38,802
causing an outage are too great and that

287
00:18:38,936 --> 00:18:42,754
production simply is not a suitable environment to really be

288
00:18:42,792 --> 00:18:46,882
doing chaos engineering. So keeping those factors in mind

289
00:18:46,936 --> 00:18:51,090
can help you determine when and where you might do chaos engineering.

290
00:18:51,250 --> 00:18:54,914
Now, a brief word about Azure Chaos Studio Chaos Studio

291
00:18:54,962 --> 00:18:58,954
is our new product, available as part of the Azure platform that

292
00:18:58,992 --> 00:19:02,054
enables you to do chaos engineering natively

293
00:19:02,102 --> 00:19:05,786
within Azure. It's a fully managed service, which means that

294
00:19:05,808 --> 00:19:09,318
there is no need to install any utility, make updates,

295
00:19:09,414 --> 00:19:13,166
maintain a platform. Those can be expensive and they can be

296
00:19:13,188 --> 00:19:16,974
a challenge for any service team to have to go and

297
00:19:17,012 --> 00:19:20,974
operate, maintain and secure those tools. So having this be

298
00:19:21,012 --> 00:19:24,094
fully managed means you can focus on the outcomes

299
00:19:24,222 --> 00:19:27,630
rather than the implementation. We're well integrated

300
00:19:27,710 --> 00:19:30,862
with Azure's management tools, including Azure resource

301
00:19:30,926 --> 00:19:34,690
manager, Azure policy, project Bicep,

302
00:19:35,030 --> 00:19:38,950
and several of the other aspects of Azure so that things

303
00:19:39,020 --> 00:19:42,706
fit very naturally in your ecosystem. The way you deploy

304
00:19:42,738 --> 00:19:46,386
your infrastructure is how you can deploy your chaos experiments,

305
00:19:46,498 --> 00:19:50,538
and you can manage and secure access

306
00:19:50,624 --> 00:19:53,962
to your chaos experiments exactly as you're doing

307
00:19:54,016 --> 00:19:57,594
with any other part of your infrastructure estate that

308
00:19:57,632 --> 00:20:01,178
exists in Azure. We have integration with

309
00:20:01,264 --> 00:20:05,274
observability tooling to ensure you can do that analysis

310
00:20:05,322 --> 00:20:08,714
when a chaos experiment happens. And we have an expanding

311
00:20:08,762 --> 00:20:12,174
library of faults that covers a lot of the common Azure service

312
00:20:12,292 --> 00:20:16,146
issues. One of our aspirational goals is to provide

313
00:20:16,248 --> 00:20:20,174
experiment templates for the most severe Azure

314
00:20:20,222 --> 00:20:23,666
outages that happen on the platform. And that's something we pay

315
00:20:23,688 --> 00:20:27,362
a lot of attention to, is when there is a high severity

316
00:20:27,426 --> 00:20:30,566
Azure outage that impacts a customer. How can

317
00:20:30,588 --> 00:20:34,050
we transform that into a chaos experiment template

318
00:20:34,130 --> 00:20:38,214
that would allow a customer, a cloud consumer, to go and replicate

319
00:20:38,262 --> 00:20:42,534
that failure to ensure that they are well defended

320
00:20:42,582 --> 00:20:46,442
against having an impact to their availability should

321
00:20:46,496 --> 00:20:49,754
any similar sort of outage occur. And the final

322
00:20:49,792 --> 00:20:53,886
thing I'll mention about Chaos Studio is that safety is very important to us.

323
00:20:53,988 --> 00:20:57,274
We're not a simulator, we're not simulating faults,

324
00:20:57,322 --> 00:21:00,782
the faults are really getting injected. And that means

325
00:21:00,836 --> 00:21:04,446
that when we shut down a virtual machine, the virtual machine

326
00:21:04,478 --> 00:21:08,382
is getting shut down. When we apply cpu pressure,

327
00:21:08,446 --> 00:21:12,100
that cpu pressure on an AKS cluster is really happening.

328
00:21:12,710 --> 00:21:16,106
What this means is that whether it be unintentional,

329
00:21:16,158 --> 00:21:19,734
accidental fault injection, or something a little bit

330
00:21:19,772 --> 00:21:23,382
more malicious, we want to help make sure that you can defend against

331
00:21:23,436 --> 00:21:27,080
those by having appropriate permissions built into the system

332
00:21:27,390 --> 00:21:31,990
restrictions and administrative operations on what resources

333
00:21:32,070 --> 00:21:36,730
can be targeted for fault injection and what

334
00:21:36,880 --> 00:21:40,614
faults can be used on a particular resource,

335
00:21:40,742 --> 00:21:44,222
as well as permissions for the experiment to access those

336
00:21:44,276 --> 00:21:47,742
resources. So there's plenty of safety built into

337
00:21:47,796 --> 00:21:51,114
the mechanisms in Chaos studio. Now let's

338
00:21:51,162 --> 00:21:55,422
talk a little bit about chaos engineering at Microsoft at Microsoft,

339
00:21:55,486 --> 00:21:59,186
we've been using chaos engineering for several years to improve the

340
00:21:59,208 --> 00:22:03,906
resilience of our own cloud services, and the

341
00:22:03,928 --> 00:22:07,318
majority of those learnings have contributed to us building

342
00:22:07,404 --> 00:22:10,742
Chaos studio, both as a central service

343
00:22:10,876 --> 00:22:14,754
that all of our cloud service teams can use within Microsoft,

344
00:22:14,882 --> 00:22:18,186
as well as an offering that we can make available to our customers.

345
00:22:18,288 --> 00:22:22,214
And it's currently in public preview there are over 50 teams

346
00:22:22,262 --> 00:22:25,626
at Microsoft, 50 cloud services that

347
00:22:25,648 --> 00:22:29,030
are using Chaos studio today across a range

348
00:22:29,110 --> 00:22:33,470
of Microsoft products, from the power platform to

349
00:22:33,540 --> 00:22:37,920
the office suite to the Azure cloud services,

350
00:22:38,850 --> 00:22:42,778
we believe. And there are two areas of particular

351
00:22:42,964 --> 00:22:46,462
focus in Microsoft right now when it comes to chaos

352
00:22:46,526 --> 00:22:50,094
engineering. The first is investing heavily in failure

353
00:22:50,142 --> 00:22:53,726
scenarios over adding specific faults.

354
00:22:53,838 --> 00:22:57,414
So we've learned in analyzing our incidents and in

355
00:22:57,452 --> 00:23:01,362
looking at past resilience challenges that oftentimes

356
00:23:01,426 --> 00:23:04,882
it's a more broad scenario, say a region

357
00:23:04,946 --> 00:23:09,094
failure or an inability to scale with load

358
00:23:09,222 --> 00:23:12,554
or a network configuration change is the

359
00:23:12,592 --> 00:23:16,054
real scenario that you want to be able to replicate

360
00:23:16,102 --> 00:23:19,846
when doing chaos engineering, and oftentimes you're

361
00:23:19,878 --> 00:23:23,790
leveraging a set conf 42 recreate that failure scenario.

362
00:23:24,210 --> 00:23:27,770
But at the end of the day, it's the failure scenario that matters,

363
00:23:27,850 --> 00:23:31,150
not the individual faults that contribute up to that.

364
00:23:31,220 --> 00:23:35,806
So rather than focusing on delivering faults

365
00:23:35,838 --> 00:23:39,938
for every single option, we like to deliver faults and

366
00:23:40,024 --> 00:23:44,654
encourage our teams to build experiments around those scenarios

367
00:23:44,782 --> 00:23:48,370
and light up the correct faults for those major scenarios.

368
00:23:48,450 --> 00:23:51,618
Take availability zone down an Azure active

369
00:23:51,634 --> 00:23:55,174
directory, outage a DNS outage and focus on

370
00:23:55,212 --> 00:23:59,306
those. The other thing that we've been really investing heavily in

371
00:23:59,408 --> 00:24:01,420
is shifting this left,

372
00:24:02,670 --> 00:24:06,410
particularly when it comes to high blast radius outages.

373
00:24:06,830 --> 00:24:10,670
In the past we've known that there are a couple of places where

374
00:24:10,820 --> 00:24:14,586
things like DNS or Azure active directory,

375
00:24:14,778 --> 00:24:18,490
any sort of outage in those services can have impact

376
00:24:18,570 --> 00:24:21,998
on the majority of Microsoft cloud services.

377
00:24:22,164 --> 00:24:25,730
And so while we've done a lot to defend against those

378
00:24:25,800 --> 00:24:29,490
dependencies having impact on every other cloud service,

379
00:24:29,560 --> 00:24:33,234
when they do see impact, we now want to pull that from.

380
00:24:33,352 --> 00:24:37,670
We validated it for services that are going out into production

381
00:24:38,010 --> 00:24:42,770
and shift that left into preventing any regression

382
00:24:42,930 --> 00:24:46,946
in any ability to be resilient against those particular types

383
00:24:46,978 --> 00:24:50,666
of failures. And in fact, one thing we're looking forward to doing at

384
00:24:50,688 --> 00:24:54,566
Microsoft is ensuring that at least for the Azure division,

385
00:24:54,758 --> 00:24:58,940
every single new deployment of every service has

386
00:24:59,390 --> 00:25:02,814
specific failure scenarios validated as part of

387
00:25:02,852 --> 00:25:06,462
pre production, as a pre production gate before that

388
00:25:06,516 --> 00:25:09,934
build is suitable to go out to production to ensure that

389
00:25:09,972 --> 00:25:13,442
we're never regressing a scenario and

390
00:25:13,496 --> 00:25:17,118
our resilience to high blast radius outages.

391
00:25:17,294 --> 00:25:21,646
Now, two great examples of using chaos engineering within Microsoft.

392
00:25:21,758 --> 00:25:25,286
The first is the Microsoft Power platform team. They've been

393
00:25:25,308 --> 00:25:28,754
doing region failure experiments with chaos

394
00:25:28,802 --> 00:25:32,418
Studio and have identified several opportunities

395
00:25:32,514 --> 00:25:36,502
where when a data center went down, they were unable to

396
00:25:36,636 --> 00:25:40,326
not only recover from the failure, they were able

397
00:25:40,348 --> 00:25:43,786
to recover from the failure. But they said hey, we also want to be able

398
00:25:43,808 --> 00:25:48,022
to fail over to a secondary region. So when there's a failure in region,

399
00:25:48,086 --> 00:25:51,654
also have that failover. They discovered that by leveraging Chaos

400
00:25:51,702 --> 00:25:55,278
studio to shut down all of their compute and all of their services in a

401
00:25:55,284 --> 00:25:58,590
region to validate that the backup would come up

402
00:25:58,660 --> 00:26:02,506
and when it didn't, they were able to go and identify an issue causing

403
00:26:02,538 --> 00:26:06,050
that backup not to occur. Another example from that team

404
00:26:06,120 --> 00:26:09,426
was simply during an outage event, acknowledging that

405
00:26:09,448 --> 00:26:13,214
they didn't have the appropriate observability to detect

406
00:26:13,262 --> 00:26:16,674
the outage early and respond quickly. Now with Chaos

407
00:26:16,722 --> 00:26:20,454
studio they were able to recreate the conditions and find

408
00:26:20,572 --> 00:26:23,926
new spaces where they needed to instrument further in

409
00:26:23,948 --> 00:26:27,914
our monitoring so that they could mitigate and identify those

410
00:26:27,952 --> 00:26:31,578
failures and automate responses to them

411
00:26:31,744 --> 00:26:35,802
quickly. Another great example is our Azure key vault team.

412
00:26:35,936 --> 00:26:39,642
That team has been doing several availability zone down

413
00:26:39,696 --> 00:26:43,738
outages as well as scaling up the service outages.

414
00:26:43,834 --> 00:26:47,838
And a great learning from that team was while

415
00:26:47,924 --> 00:26:51,642
collaboration is important and validating configuration

416
00:26:51,706 --> 00:26:55,826
is great. Small teams and changes over time

417
00:26:55,928 --> 00:26:59,746
might mean that an original configuration in

418
00:26:59,768 --> 00:27:03,054
an autoscale rule might not have the same effects

419
00:27:03,102 --> 00:27:06,358
over time that it originally did. So in this case,

420
00:27:06,524 --> 00:27:09,734
in a pre production environment, they were being some

421
00:27:09,772 --> 00:27:13,474
chaos engineering and discovered that for a pre production

422
00:27:13,522 --> 00:27:16,934
service, the autoscale rule was misconfigured such

423
00:27:16,972 --> 00:27:21,026
that when stress was applied, the virtual machine scale

424
00:27:21,058 --> 00:27:24,634
set they ran on was not scaling up further. And so they were actually able

425
00:27:24,672 --> 00:27:28,314
to identify that in pre production, mitigate it before it

426
00:27:28,352 --> 00:27:30,570
ever became an issue in production.

427
00:27:31,890 --> 00:27:35,546
Now, what we've learned from doing chaos engineering

428
00:27:35,578 --> 00:27:38,766
within Microsoft, as well as partnering with some of

429
00:27:38,788 --> 00:27:42,334
our big customers who are leveraging chaos engineering and

430
00:27:42,372 --> 00:27:46,270
Chaos studio to do chaos engineering in their environments,

431
00:27:46,430 --> 00:27:49,378
there are a couple of insights that I'd like to share.

432
00:27:49,544 --> 00:27:52,802
The first is that chaos engineering really

433
00:27:52,856 --> 00:27:56,594
needs to start with maturing your tooling and processes

434
00:27:56,722 --> 00:28:01,362
before you go to introduce any amount of chaos. So ensuring

435
00:28:01,426 --> 00:28:04,690
that you have great robust observability,

436
00:28:04,850 --> 00:28:08,506
that you've already built backup mechanisms and

437
00:28:08,528 --> 00:28:12,422
you've made your service respond correctly

438
00:28:12,486 --> 00:28:16,182
to outages, making sure that you have a great livesight

439
00:28:16,246 --> 00:28:19,802
process in place, and that you have troubleshooting guides and

440
00:28:19,856 --> 00:28:23,838
automatic mitigations in place. Those have to be there

441
00:28:23,924 --> 00:28:27,882
before you start to do chaos engineering, because chaos

442
00:28:27,946 --> 00:28:31,694
engineering is not suitable for a case where you're learning something you

443
00:28:31,732 --> 00:28:35,586
already knew. Chaos engineering should reveal something new

444
00:28:35,688 --> 00:28:38,750
about your service, something unexpected.

445
00:28:38,910 --> 00:28:42,658
That's when chaos engineering is best. The second

446
00:28:42,824 --> 00:28:46,166
insight we've had is that a great way

447
00:28:46,188 --> 00:28:49,718
to understand where to apply chaos engineering is to

448
00:28:49,804 --> 00:28:53,170
quantify and analyze past outages.

449
00:28:53,330 --> 00:28:56,854
Now, the quantification is something I talked about a little bit

450
00:28:56,892 --> 00:29:00,726
earlier, where being able to put a monetary amount on an outage

451
00:29:00,838 --> 00:29:04,154
can help create that visibility across a

452
00:29:04,192 --> 00:29:07,578
larger company and across different

453
00:29:07,664 --> 00:29:11,658
sets of stakeholders to make them more invested

454
00:29:11,754 --> 00:29:15,646
in the importance of reliability. Putting a dollar amount

455
00:29:15,748 --> 00:29:19,402
or a rupee amount, any sort of currency

456
00:29:19,466 --> 00:29:23,394
amount on your service when there is an outage, and what that

457
00:29:23,432 --> 00:29:27,022
dollar amount was for. The outage is a wonderful

458
00:29:27,086 --> 00:29:30,766
way of keeping everyone's head centered

459
00:29:30,798 --> 00:29:34,334
around the importance of resilience. And then once you've

460
00:29:34,382 --> 00:29:37,778
built that, going back and analyzing past outages

461
00:29:37,874 --> 00:29:41,202
to identify when you've had high blast radius

462
00:29:41,266 --> 00:29:45,938
outages and or high frequency

463
00:29:46,034 --> 00:29:49,258
outages. Those are two great places to start

464
00:29:49,344 --> 00:29:53,994
by looking back at your previous incident and

465
00:29:54,032 --> 00:29:58,122
your responses to incidents, and then deciding from there

466
00:29:58,176 --> 00:30:02,494
which chaos experiments you might want to start with. A third

467
00:30:02,612 --> 00:30:06,222
insight for us has been it is important to build

468
00:30:06,276 --> 00:30:10,266
confidence in pre production before heading out into production.

469
00:30:10,458 --> 00:30:13,618
Now this requires that you've built a great

470
00:30:13,704 --> 00:30:17,074
pre production environment within Azure, we have

471
00:30:17,112 --> 00:30:20,926
the concept of our canary regions. These are two dedicated

472
00:30:21,038 --> 00:30:25,034
regions within resource manager that are unavailable

473
00:30:25,102 --> 00:30:28,742
to our customers. But almost every

474
00:30:28,796 --> 00:30:32,694
azure service has a stamp in those clusters or in those

475
00:30:32,732 --> 00:30:36,214
regions, and services have to go

476
00:30:36,252 --> 00:30:40,282
in and bake for a certain amount of time in those regions before

477
00:30:40,336 --> 00:30:43,866
they can move into production. Now, the fact that the services

478
00:30:43,968 --> 00:30:48,250
being deployed in our canary regions are dependent on other

479
00:30:48,320 --> 00:30:51,914
services in canary regions helps us to proactively

480
00:30:51,962 --> 00:30:55,182
identify any dependency issues,

481
00:30:55,316 --> 00:30:59,226
any failures, and mitigate those before anything hits

482
00:30:59,258 --> 00:31:03,806
production. In fact, in certain services like Microsoft Teams,

483
00:31:03,918 --> 00:31:07,330
we're the dog fooders. Where Microsoft Teams,

484
00:31:08,070 --> 00:31:11,634
our own Microsoft service traffic for our

485
00:31:11,672 --> 00:31:15,722
company goes through the Teams dog food environment

486
00:31:15,806 --> 00:31:19,334
in a canary environment. And that helps us

487
00:31:19,372 --> 00:31:23,330
to make sure that we are building quality in those environments

488
00:31:23,490 --> 00:31:26,774
before something goes out to the general public who

489
00:31:26,812 --> 00:31:30,586
rely on Microsoft Teams. And a final insight for

490
00:31:30,608 --> 00:31:34,202
us is just that quality can't happen if it's only

491
00:31:34,256 --> 00:31:37,894
on one person's back or only on one team's

492
00:31:37,942 --> 00:31:41,866
back. For a large scale organization, you really have

493
00:31:41,888 --> 00:31:45,134
to create a culture of quality where everyone believes that this

494
00:31:45,172 --> 00:31:48,938
is important. And we talked about this a little bit earlier in the presentation,

495
00:31:49,114 --> 00:31:52,960
but it is critical to remind us that

496
00:31:53,810 --> 00:31:57,578
that culture of quality has to come before any investment

497
00:31:57,674 --> 00:32:00,766
in chaos engineering. So with

498
00:32:00,788 --> 00:32:04,046
that, I'd like to say thank you very much for your time, and I hope

499
00:32:04,068 --> 00:32:07,586
you enjoy the rest of the conference. If you'd like to learn more about Azure

500
00:32:07,618 --> 00:32:11,430
Chaos Studio, you can go to aka Ms.

501
00:32:12,010 --> 00:32:15,494
Azure Chaos Studio. You'll be able to learn more about

502
00:32:15,532 --> 00:32:19,074
our service, get started, read our documentation,

503
00:32:19,202 --> 00:32:23,414
as well as see some of our user studies or

504
00:32:23,452 --> 00:32:26,518
our customer studies. So enjoy the

505
00:32:26,524 --> 00:32:28,500
rest of the conference, and thanks very much.

