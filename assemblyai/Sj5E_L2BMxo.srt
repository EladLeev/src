1
00:00:23,530 --> 00:00:27,110
Hi, everyone. You should know by now that my name is Danica Fine,

2
00:00:27,180 --> 00:00:30,418
and I am a developer advocate for confluent.

3
00:00:30,594 --> 00:00:33,654
Beyond that very minimal piece of information, the only

4
00:00:33,692 --> 00:00:37,062
other thing that you really need to know about me is that I like house

5
00:00:37,116 --> 00:00:40,790
plants. And if you needed a little bit of proof,

6
00:00:41,130 --> 00:00:44,280
here it is. These are all houseplant that I own,

7
00:00:44,730 --> 00:00:48,742
but it's actually only a subset. There's actually about four or

8
00:00:48,796 --> 00:00:52,206
three or four dozen of them in my home. And if any of you

9
00:00:52,228 --> 00:00:55,726
own house plants, you probably know how much work can be

10
00:00:55,748 --> 00:01:00,158
involved, right? Especially if you have a lot of them like I do.

11
00:01:00,324 --> 00:01:03,838
For me, at the peak of the pandemic, when I had so much free time,

12
00:01:03,924 --> 00:01:07,598
hands so many more plants, there was really a lot of

13
00:01:07,604 --> 00:01:10,846
work involved, right? I needed to go around every morning and do the rounds,

14
00:01:10,878 --> 00:01:13,954
right? Check to see which plants needed to be watered, if any of them needed

15
00:01:13,992 --> 00:01:18,210
to be rotated to have better sunlight, or maybe be fertilized.

16
00:01:18,870 --> 00:01:22,406
There is a lot to do, right? And this

17
00:01:22,428 --> 00:01:25,366
is really, really great over the pandemic, because I got so much out of it,

18
00:01:25,388 --> 00:01:29,562
right? I loved having these plants. They made me so happy. But as

19
00:01:29,616 --> 00:01:33,434
I started getting back into the world, right, going back into

20
00:01:33,472 --> 00:01:37,180
the office, going to conferences, just living my life,

21
00:01:37,550 --> 00:01:41,034
I realtime that maybe this wasn't the best thing

22
00:01:41,072 --> 00:01:44,702
for me, right? I started to realtime that I was just being

23
00:01:44,756 --> 00:01:49,066
a bad plant mom, right? So this is a very, very welted plant.

24
00:01:49,178 --> 00:01:53,182
Poor guy. Totally my fault. I forgot to water him one morning before

25
00:01:53,236 --> 00:01:55,614
I went to the office. And then when I got back later in the day,

26
00:01:55,652 --> 00:01:59,634
this is what it looked like. It's completely fine now. Don't worry about

27
00:01:59,672 --> 00:02:02,626
it. But I still felt bad, right? Because I wanted to be able to care

28
00:02:02,648 --> 00:02:05,442
for my plants. I wanted to be a better plant parent.

29
00:02:05,576 --> 00:02:08,494
So something had to be done, right? This wasn't ideal.

30
00:02:08,622 --> 00:02:12,086
So I asked myself, was there a better way for me to be monitoring my

31
00:02:12,108 --> 00:02:15,446
house plants? Of course there's a better way, right?

32
00:02:15,628 --> 00:02:19,106
You can go to the store right now. I could go and buy a premade

33
00:02:19,138 --> 00:02:22,874
solution to help me monitor my house plants. But that's not very

34
00:02:22,912 --> 00:02:26,618
fun, right? That doesn't make a good story. So maybe that's not the

35
00:02:26,624 --> 00:02:29,754
right question to be asking, right? We are engineers. Or at the very least,

36
00:02:29,792 --> 00:02:33,366
you're a curious or creative person. You're attending this session. You want to see

37
00:02:33,488 --> 00:02:37,200
if there is a more interesting way, right? That's a better question.

38
00:02:38,130 --> 00:02:41,466
Just to give you a little bit of background on myself, I spent

39
00:02:41,498 --> 00:02:45,182
a handful of years as a software engineer building out streaming data pipelines

40
00:02:45,246 --> 00:02:49,330
with Apache Kafka. When I became a developer advocate, I knew

41
00:02:49,400 --> 00:02:53,202
that I wanted to build out projects with Kafka that

42
00:02:53,256 --> 00:02:57,122
were actually useful to me, that I could have in my home

43
00:02:57,176 --> 00:03:00,406
that served a real purpose for me. And on

44
00:03:00,428 --> 00:03:04,374
top of that, I always really wanted to build out a hardware project using

45
00:03:04,412 --> 00:03:08,054
a raspberry PI. So this was my opportunity. This was my chance, right?

46
00:03:08,092 --> 00:03:11,874
I could use Apache Kafka in my own home. I could build a practical,

47
00:03:11,922 --> 00:03:15,498
event driven pipeline for myself. Hands. I could also use a

48
00:03:15,504 --> 00:03:18,986
raspberry PI. So this is going to be great. I could incorporate all

49
00:03:19,008 --> 00:03:22,106
the things that I've been wanting to incorporate. So let's get into it.

50
00:03:22,128 --> 00:03:25,262
Let's talk design. So the bottom

51
00:03:25,316 --> 00:03:29,066
line is that I needed something to help me monitor my house plants,

52
00:03:29,098 --> 00:03:32,254
and at the very least, just let me know when to monitor them. That was

53
00:03:32,292 --> 00:03:35,646
good enough for me, right? So the system that I envisioned

54
00:03:35,678 --> 00:03:39,666
was pretty simple. So I'd have some soil moisture sensors in my

55
00:03:39,688 --> 00:03:42,786
plants. I would capture moisture readings regularly and

56
00:03:42,808 --> 00:03:47,060
toss them into Kafka, and I would do this as those events happen.

57
00:03:47,670 --> 00:03:51,446
That information probably wouldn't be enough on its own, though. So I

58
00:03:51,468 --> 00:03:55,026
would also need to enrich those moisture readings with some extra details,

59
00:03:55,058 --> 00:03:58,246
some metadata, and that would help me to decide whether or not

60
00:03:58,268 --> 00:04:01,914
the plants actually needed to be watered in that moment. And then I would

61
00:04:01,952 --> 00:04:04,998
combine that data and use it to compute outlier readings.

62
00:04:05,094 --> 00:04:08,694
So I would do some stream processing, and at the end, I wanted to receive

63
00:04:08,742 --> 00:04:11,946
some sort of alert, ideally on my phone. I mean, we're attached to our

64
00:04:11,968 --> 00:04:15,706
phones, right? So that made sense to me. And I'm pretty fond

65
00:04:15,738 --> 00:04:18,878
of the telegram messaging app. We'll get into that a

66
00:04:18,884 --> 00:04:21,806
little bit more later. But I thought it would be a really convenient way to

67
00:04:21,828 --> 00:04:25,418
receive that information on my phone. So that sounds good. That's a high

68
00:04:25,444 --> 00:04:29,394
level look at what I wanted to achieve. So quick

69
00:04:29,432 --> 00:04:33,074
aside, though, at this point, I've mentioned Apache Kafka a number

70
00:04:33,112 --> 00:04:36,338
of times. I don't know if you were counting, but since it's such a

71
00:04:36,344 --> 00:04:39,394
big part of my project, I really want to take a quick step back hands,

72
00:04:39,432 --> 00:04:41,746
make sure that everybody is on the same page, that we all know what we're

73
00:04:41,778 --> 00:04:46,194
talking about here. So what is Kafka? The most concise

74
00:04:46,242 --> 00:04:49,410
definition that I could come up with is this. It's a distributed

75
00:04:49,490 --> 00:04:53,146
event streaming platform, and there are really only three words in

76
00:04:53,168 --> 00:04:56,586
there that matter. Very, very concise, boiled down,

77
00:04:56,688 --> 00:05:00,346
but there's a lot to unpack in those three words. So I just want to

78
00:05:00,448 --> 00:05:03,806
do that now. The first is that Kafka is

79
00:05:03,828 --> 00:05:07,294
a streaming platform. And I don't mean streaming like Netflix, although they

80
00:05:07,332 --> 00:05:10,686
do use Kafka. So if you're curious, you can check out a number

81
00:05:10,708 --> 00:05:14,794
of talks from them. But really, I want to focus on how we're

82
00:05:14,842 --> 00:05:18,126
processing data. You may have noticed that we've been undergoing a paradigm

83
00:05:18,158 --> 00:05:20,900
shift in how we do this, how we work with our information.

84
00:05:21,430 --> 00:05:24,862
We're moving from slower, batched, focused processing

85
00:05:24,926 --> 00:05:28,114
to stream processing. Instead of waiting

86
00:05:28,162 --> 00:05:31,826
minutes, hours, or days to group information in batches

87
00:05:31,858 --> 00:05:35,670
and then process over that batch, we're readings to smaller amounts of data

88
00:05:35,740 --> 00:05:39,720
and producing actionable results more quickly, usually in real time.

89
00:05:40,810 --> 00:05:44,218
So moving to stream processing, there's a ton of different ways you can

90
00:05:44,224 --> 00:05:46,620
do it. A lot of different technologies that you can use,

91
00:05:47,310 --> 00:05:50,394
but generally it's going to have a lot of benefits, right? It's going to help

92
00:05:50,432 --> 00:05:54,062
you increase the accuracy of your data and your results. You're going to be building

93
00:05:54,116 --> 00:05:57,374
out more reactive systems, and depending on which

94
00:05:57,412 --> 00:06:00,846
technology you use for stream processing, you can

95
00:06:00,868 --> 00:06:04,678
increase the resiliency of your overall system as well. So Kafka

96
00:06:04,714 --> 00:06:08,610
is a streaming platform. It also serves as a messaging system,

97
00:06:08,760 --> 00:06:11,934
a persistent storage layer, and a processing layer,

98
00:06:11,982 --> 00:06:15,650
as we'll see in a little bit. But it's not just a streaming platform.

99
00:06:15,720 --> 00:06:19,734
It is an event platform. And this is one of those big

100
00:06:19,772 --> 00:06:24,166
takeaways. I want to make sure that everybody keeps this piece of information with them.

101
00:06:24,348 --> 00:06:27,990
In order to use Kafka successfully, you really need to start thinking

102
00:06:28,060 --> 00:06:31,706
in events. And I want to preface that by saying that that's not

103
00:06:31,728 --> 00:06:35,238
a huge ask. Okay, I promise I'm not asking you to rewire your brain,

104
00:06:35,334 --> 00:06:38,710
because as a human, as programmers, as users,

105
00:06:38,790 --> 00:06:42,398
we already think in and process in events. It's a very natural way for

106
00:06:42,404 --> 00:06:46,190
us to do things. We submit web forms, updating information

107
00:06:46,260 --> 00:06:50,426
online. We look at application logs when we're debugging projects.

108
00:06:50,538 --> 00:06:53,150
We react to notifications on our phones.

109
00:06:53,570 --> 00:06:56,818
All of those things are events. All I'm asking you to do is be a

110
00:06:56,824 --> 00:07:00,482
little more conscious of that fact. So those were

111
00:07:00,536 --> 00:07:03,778
examples of events. But what's an event, really? It's a piece of information,

112
00:07:03,864 --> 00:07:07,486
ideally the smallest amount without too much excess, that fully

113
00:07:07,518 --> 00:07:11,030
describes something that hands happened. So what do you need to know?

114
00:07:11,100 --> 00:07:14,406
To know something's happened, you need to know when it happened. Right. You need a

115
00:07:14,428 --> 00:07:18,246
timestamp. And you should also know who or what was

116
00:07:18,268 --> 00:07:21,926
involved. Right. The subject of that. And then any sort of supporting

117
00:07:21,958 --> 00:07:25,738
details you might need. So making this a little more tangible, going back to the

118
00:07:25,744 --> 00:07:29,110
system I wanted to build, this monitoring system, I would be capturing

119
00:07:29,190 --> 00:07:32,586
moisture readings from my plants. So say that yesterday at

120
00:07:32,608 --> 00:07:35,758
02:30 p.m. This plant here, my umbrella plant, had a

121
00:07:35,764 --> 00:07:39,614
moisture level reading of 19%. So we have the when.

122
00:07:39,732 --> 00:07:43,242
Yesterday at 02:30 p.m. That timestamp. We have the subject,

123
00:07:43,306 --> 00:07:46,846
the who or what, which is this umbrella plant here. And then that supporting

124
00:07:46,878 --> 00:07:50,740
detail, which is that it had a moisture reading level of 19%.

125
00:07:51,270 --> 00:07:55,410
Great. We checked all the boxes. That is an event. But another

126
00:07:55,480 --> 00:07:59,086
key component of events that you really, really need to make sure you

127
00:07:59,128 --> 00:08:02,214
keep near and dear to your heart you remember, is that they're meant to be

128
00:08:02,252 --> 00:08:05,606
immutable. And that's just an unfortunate fact of

129
00:08:05,628 --> 00:08:08,934
life. It's due to the fact that they've described things that have

130
00:08:08,972 --> 00:08:12,506
already happened. Okay, I don't have a time machine. Neither do

131
00:08:12,528 --> 00:08:15,898
you. I don't think so, at least. But going back to

132
00:08:15,904 --> 00:08:19,594
this example here, I was a little sad yesterday at 02:30

133
00:08:19,632 --> 00:08:23,706
p.m. Because this plant was clearly dry, right. It had wilted.

134
00:08:23,898 --> 00:08:27,326
So I can't undo that. Right. Yesterday at 02:30 p.m.

135
00:08:27,348 --> 00:08:30,734
That plant was dry, was wilted. What I could do

136
00:08:30,772 --> 00:08:34,286
now is I could water the plant hands, raise its moisture level to,

137
00:08:34,308 --> 00:08:37,918
say, 70%. But doing that and watering

138
00:08:37,934 --> 00:08:41,470
that plant doesn't erase the fact that yesterday it had a low moisture reading.

139
00:08:41,550 --> 00:08:44,994
Right. All I've done is I've generated a new event,

140
00:08:45,112 --> 00:08:48,518
and I've added it to the stream of immutable events that describe the

141
00:08:48,524 --> 00:08:51,494
moisture level of that plant. It's sort of a timeline, right.

142
00:08:51,532 --> 00:08:55,654
That describes that plant over time. So based

143
00:08:55,692 --> 00:08:58,766
on that, based on what you just learned, we saw how Kafka is a streaming

144
00:08:58,818 --> 00:09:02,106
platform. It facilitates the movement of data in real time across your

145
00:09:02,128 --> 00:09:04,874
system. But it's also an eventing system.

146
00:09:05,072 --> 00:09:08,134
Kafka allows you to communicate immutable facts.

147
00:09:08,182 --> 00:09:11,706
Immutable events have occurred throughout your system and then gives you

148
00:09:11,728 --> 00:09:14,974
the power to process and move and react to those events in real time,

149
00:09:15,012 --> 00:09:18,446
which is pretty wild. There's a lot you can do with that.

150
00:09:18,628 --> 00:09:22,586
But how does Kafka do this? Let's look a little bit closer at the architecture

151
00:09:22,618 --> 00:09:26,738
of Kafka. And it is a distributed platform.

152
00:09:26,824 --> 00:09:30,686
That's a really important part. So when you work with Kafka,

153
00:09:30,798 --> 00:09:33,986
the first thing you need to know is that the primary unit of storage is

154
00:09:34,008 --> 00:09:37,526
a Kafka topic. These topics typically represent a

155
00:09:37,548 --> 00:09:40,470
single data set consisting of events.

156
00:09:40,970 --> 00:09:44,214
You get data into Kafka, you write them as

157
00:09:44,252 --> 00:09:47,826
key value pairs using separate clients called producers.

158
00:09:48,018 --> 00:09:51,266
We can have as many Kafka producers as we want writing to as many kafka

159
00:09:51,298 --> 00:09:54,966
topics as we'd want from there, once the data is in Kafka,

160
00:09:54,998 --> 00:09:58,106
we can read it back out of a Kafka topic using another separate client called

161
00:09:58,128 --> 00:10:01,658
a kafka consumer. And again, we can have as many of these consumers as we

162
00:10:01,664 --> 00:10:04,320
want reading from as many kafka topics as we'd want.

163
00:10:05,330 --> 00:10:09,214
A couple cool things about consumers that you should remember is that as

164
00:10:09,252 --> 00:10:12,622
they consume and process events, consumers have a way to keep track

165
00:10:12,676 --> 00:10:16,350
of the last one, the last event that they saw that they processed.

166
00:10:16,430 --> 00:10:19,410
And they do this using a bookmark called an offset.

167
00:10:20,070 --> 00:10:23,730
Another cool thing about consumers is that they have the ability to work together

168
00:10:23,880 --> 00:10:27,538
hands, share the work of reading data from a single kafka

169
00:10:27,554 --> 00:10:31,074
topic or a set of topics and potentially parallelizing

170
00:10:31,122 --> 00:10:34,886
the processing of that data. Or they're free to consume the

171
00:10:34,908 --> 00:10:38,774
data independently from the beginning of the topic. And that's a

172
00:10:38,812 --> 00:10:41,580
really, really important thing that you should remember.

173
00:10:42,350 --> 00:10:45,546
It's easy when you're working with kafka for the first time, to compare it

174
00:10:45,568 --> 00:10:48,954
to other, maybe similar technologies. And the first one that usually comes

175
00:10:48,992 --> 00:10:52,746
to mind is a queue, a messaging queue. But a kafka topic

176
00:10:52,778 --> 00:10:56,426
is not a messaging queue. When data is consumed

177
00:10:56,458 --> 00:10:58,670
from a Kafka topic, it doesn't disappear.

178
00:10:59,410 --> 00:11:02,590
Instead, you should really be thinking of a kafka topic as a log.

179
00:11:02,740 --> 00:11:06,382
And similar to application logs that you use to debug,

180
00:11:06,446 --> 00:11:09,970
right? Think about the last application you try to debug. You open up

181
00:11:10,040 --> 00:11:13,266
the application log file and you read line by line through that

182
00:11:13,288 --> 00:11:16,678
log file. And I just want to go on a

183
00:11:16,684 --> 00:11:20,070
quick aside here. As you read through those lines,

184
00:11:20,410 --> 00:11:23,590
every line in a log file is itself an event,

185
00:11:23,660 --> 00:11:27,174
right? It has a timestamp. And every

186
00:11:27,212 --> 00:11:31,494
line in that log file describes something specific that has happened to

187
00:11:31,532 --> 00:11:35,126
a component across your system. It gives you a timestamp, it gives you supporting

188
00:11:35,158 --> 00:11:38,586
details. It tells you where it happened, right? But as you

189
00:11:38,608 --> 00:11:42,378
read line by line through that log file, nothing happens to those events,

190
00:11:42,474 --> 00:11:46,046
right? They're still there. They're durably stored in that

191
00:11:46,068 --> 00:11:49,614
log file until maybe the log file rolls at some

192
00:11:49,652 --> 00:11:52,750
point, but it's there until that point, right?

193
00:11:52,900 --> 00:11:56,450
And during that time, you can invite as many colleagues as you want

194
00:11:56,520 --> 00:12:00,238
to read that same information in that log file

195
00:12:00,334 --> 00:12:03,954
at the same time. All right? So what that means is that each of you

196
00:12:03,992 --> 00:12:07,042
are free to read line by line through that log file, build up the same

197
00:12:07,096 --> 00:12:10,354
state of that application in your mind, and potentially help debug,

198
00:12:10,482 --> 00:12:13,750
right? It's the same thing with kafka consumers, all right?

199
00:12:13,820 --> 00:12:16,886
As they read the data from a kafka topic, nothing happens to

200
00:12:16,908 --> 00:12:20,818
those events. They don't disappear. Another thing

201
00:12:20,844 --> 00:12:24,246
to keep in mind about kafka topics hands consumers and producers

202
00:12:24,278 --> 00:12:28,054
is that they are fully decoupled from one another. Producers don't

203
00:12:28,102 --> 00:12:31,834
care who's going to read that data eventually. Consumers don't care

204
00:12:31,872 --> 00:12:35,866
who wrote the data, right? The topics sort of sit in between kafka

205
00:12:35,898 --> 00:12:39,680
is that layer that sits in between the consumers and producers at any given time.

206
00:12:41,490 --> 00:12:45,238
Kafka topics are actually further broken down into smaller components

207
00:12:45,274 --> 00:12:48,722
called partitions. And these partitions themselves are actually

208
00:12:48,776 --> 00:12:52,446
the immutable append only logs where the individual events are stored.

209
00:12:52,558 --> 00:12:56,258
All right, so you see here an example of a kafka topic with three partitions.

210
00:12:56,434 --> 00:12:59,446
We have 15 events stored throughout it.

211
00:12:59,468 --> 00:13:02,834
You can see them, they're numbered, and you see that those individual events

212
00:13:02,882 --> 00:13:06,040
are distributed across those partitions. All right,

213
00:13:06,490 --> 00:13:10,582
so kafka is a distributed system. So these partitions are actually stored

214
00:13:10,646 --> 00:13:13,814
across different nodes in the cluster, ideally.

215
00:13:13,942 --> 00:13:17,494
And this is to provide better resilience and also facilitate the replication

216
00:13:17,542 --> 00:13:21,034
of your data. So just to

217
00:13:21,072 --> 00:13:24,358
give you an example of this, if you're a visual learner,

218
00:13:24,534 --> 00:13:27,706
the nodes of a kafka cluster are called brokers. And these brokers

219
00:13:27,738 --> 00:13:31,006
can be running anywhere. They can be bare metal vms, containers in the

220
00:13:31,028 --> 00:13:34,414
cloud, pretty much wherever you want them to run. So this is

221
00:13:34,452 --> 00:13:38,142
a simple cluster setup. The three nodes, we have three topics

222
00:13:38,206 --> 00:13:41,746
with some number of partitions in them, and your

223
00:13:41,768 --> 00:13:45,294
results may vary, but this might be how these partitions are distributed

224
00:13:45,342 --> 00:13:48,782
across the cluster. But why bother?

225
00:13:48,846 --> 00:13:52,594
Why bother putting the partitions on different nodes? The biggest

226
00:13:52,642 --> 00:13:55,858
reason is, when it comes to the processing of the data later on, the consuming

227
00:13:55,874 --> 00:13:59,394
of that data. As I mentioned before, consumers have the ability to work together

228
00:13:59,452 --> 00:14:03,514
to process data, and the parallelization of that work happens

229
00:14:03,552 --> 00:14:07,338
at the partition level. So what this means is that if we had,

230
00:14:07,424 --> 00:14:11,194
say, topic b, we have three partitions there. If we wanted

231
00:14:11,232 --> 00:14:15,054
to optimize or maximize the processing of this data, we'd have three

232
00:14:15,092 --> 00:14:18,718
consumers, each consuming from different partitions. And the

233
00:14:18,724 --> 00:14:22,606
fact that those partitions are on three different brokers means that those brokers aren't trying

234
00:14:22,628 --> 00:14:26,058
to serve up data for all three partitions

235
00:14:26,074 --> 00:14:29,698
at once. Each broker can handle serving up that data to the individual

236
00:14:29,784 --> 00:14:32,946
consumer at the same time. So it just helps to make that a little

237
00:14:32,968 --> 00:14:36,194
more efficient. So what this means is that you'll want your data

238
00:14:36,232 --> 00:14:39,974
to be as spread, as evenly across your partitions and your cluster as

239
00:14:40,012 --> 00:14:43,654
possible, right? And that's great. That should make sense.

240
00:14:43,692 --> 00:14:47,106
But in any distributed system, you want to consider what happens when a node

241
00:14:47,138 --> 00:14:50,834
goes down, right? That this situation clearly is not ideal.

242
00:14:50,882 --> 00:14:54,138
Broker zero went down. We've lost some data. We didn't lose it

243
00:14:54,144 --> 00:14:57,466
all, but we lost some data. But thankfully, Kafka takes

244
00:14:57,488 --> 00:15:00,646
this a step further with something called replication. And replication

245
00:15:00,678 --> 00:15:04,238
is a configurable parameter that determines how many copies of a

246
00:15:04,244 --> 00:15:07,658
given partition will exist across your kafka cluster.

247
00:15:07,754 --> 00:15:11,066
Okay, so let's look at a three node cluster

248
00:15:11,098 --> 00:15:15,018
here from the perspective of a single topic with three partitions,

249
00:15:15,114 --> 00:15:18,626
right? In this time, I'm going to use a replication factor of

250
00:15:18,648 --> 00:15:22,430
three. So with replication enabled, when a data is produced to a partition,

251
00:15:22,510 --> 00:15:26,098
say we're writing data to partition zero, it's going to

252
00:15:26,104 --> 00:15:29,654
be written to what's known as the lead partition. This is the partition that

253
00:15:29,692 --> 00:15:33,014
consumers and producers will usually interact with. All right,

254
00:15:33,132 --> 00:15:36,920
so we're going to write that data to broker zero first, partition zero.

255
00:15:37,370 --> 00:15:40,994
And then at the same time, data is synchronously going to be copied over to

256
00:15:41,052 --> 00:15:44,506
the configured number of follower partitions. So not only is

257
00:15:44,528 --> 00:15:47,754
the data first written to broker zero, it's also going to be written to

258
00:15:47,792 --> 00:15:51,330
brokers one and two to those follower partitions

259
00:15:51,430 --> 00:15:54,782
on those brokers. All right, so now

260
00:15:54,916 --> 00:15:58,942
if a broker goes down, right, we've lost our lead partition zero.

261
00:15:59,076 --> 00:16:01,760
We've lost some additional copies, but that's not too important.

262
00:16:02,850 --> 00:16:05,874
But what we can do is we can have one of our follower replicas be

263
00:16:05,912 --> 00:16:09,054
promoted to leader, and we can quickly resume

264
00:16:09,102 --> 00:16:13,540
our normal activities. Right? Our consumers and producers can do what they were doing before.

265
00:16:14,470 --> 00:16:17,218
I know that was a bit of an aside, but all that to say is

266
00:16:17,224 --> 00:16:21,202
that Kafka is pretty good at storing your data, your immutable events, your immutable facts,

267
00:16:21,266 --> 00:16:24,582
and it stores them in a reliable, persistent and distributed way

268
00:16:24,636 --> 00:16:28,098
for fast consumption, for moving those events quickly and efficiently,

269
00:16:28,194 --> 00:16:31,720
and also offering some cool data processing capabilities on top.

270
00:16:32,570 --> 00:16:35,926
All right, we all know what Kafka is. Now let's

271
00:16:35,958 --> 00:16:38,586
get back to the actual project and what I needed to do to make it

272
00:16:38,608 --> 00:16:42,266
come together. This is the high level view that I gave to you that I

273
00:16:42,288 --> 00:16:45,726
said we wanted to produce this information into Kafka, do some stream processing, and then

274
00:16:45,748 --> 00:16:48,480
get it out, right? So what do we have to do first?

275
00:16:48,850 --> 00:16:51,434
Well, we probably need a Kafka cluster,

276
00:16:51,562 --> 00:16:55,242
right? So a main goal of mine

277
00:16:55,306 --> 00:16:58,818
for this project, besides wanting to use Kafka and doing a

278
00:16:58,824 --> 00:17:02,194
hardware project, right. The other goal was to make it as simple as possible

279
00:17:02,312 --> 00:17:05,620
to manage as little infrastructure as I possibly could.

280
00:17:06,150 --> 00:17:09,254
This was my first hardware project, so I really wanted to focus on actually

281
00:17:09,292 --> 00:17:12,582
building out the physical system hands, not maintain any other

282
00:17:12,636 --> 00:17:16,066
software infrastructure if I could. So for this component,

283
00:17:16,098 --> 00:17:19,874
for my Kafka cluster, I decided to use confluent. It offers

284
00:17:19,922 --> 00:17:23,398
Kafka fully managed in the cloud and it's perfect for a

285
00:17:23,404 --> 00:17:26,106
lot of projects. But I think that it's really, really good for a project like

286
00:17:26,128 --> 00:17:29,466
this where I wanted to use Kafka, but I didn't want to deal with

287
00:17:29,488 --> 00:17:32,862
the infrastructure at all. So I was able to spin up a cloud

288
00:17:32,916 --> 00:17:36,522
based Kafka cluster and then I also got a couple additional

289
00:17:36,586 --> 00:17:40,526
auxiliary tools for free, right? And so there were some

290
00:17:40,548 --> 00:17:44,318
that would help me integrate external sources in syncs using Kafka Connect.

291
00:17:44,484 --> 00:17:48,610
I also had stream processing available in my gui

292
00:17:49,030 --> 00:17:51,858
and we'll see a little bit more of this later on, but I'm going to

293
00:17:51,864 --> 00:17:55,346
take advantage of all of these in console cloud. All right, so I set up

294
00:17:55,368 --> 00:17:58,646
a cluster. If you want to follow along and

295
00:17:58,668 --> 00:18:01,794
build something similar, just know that you need Kafka running somewhere.

296
00:18:01,842 --> 00:18:05,346
All right. I don't care where you're running it, just get a Kafka cluster.

297
00:18:05,458 --> 00:18:09,394
All right, next up we have a cluster.

298
00:18:09,442 --> 00:18:13,546
So let's build the physical system. Let's focus on the raspberry PI and the

299
00:18:13,568 --> 00:18:17,078
sensors. I know that this isn't really meant

300
00:18:17,094 --> 00:18:20,538
to be a hardware talk, so I'm not going to focus too much on it,

301
00:18:20,544 --> 00:18:24,334
but I really want to give everybody the tools that they need to build some

302
00:18:24,372 --> 00:18:27,934
system like this if you want, right? So here

303
00:18:27,972 --> 00:18:31,294
are the main things that I used to build this out.

304
00:18:31,492 --> 00:18:34,722
I'm going to gloss over it a bit, but I do want to touch on

305
00:18:34,776 --> 00:18:38,766
the sensors that I chose. So I ended up choosing these. I squared C's,

306
00:18:38,798 --> 00:18:42,770
capacitive moisture sensors. They seemed like pretty good bang for your buck.

307
00:18:43,510 --> 00:18:47,074
They were relatively high quality, they seemed pretty consistent.

308
00:18:47,122 --> 00:18:50,694
They weren't going to rust over

309
00:18:50,732 --> 00:18:54,134
time. They were going to work really well for this, but they're I

310
00:18:54,172 --> 00:18:58,406
squared C and I squared C. Components communicate over a

311
00:18:58,428 --> 00:19:01,690
unique address which is set at the component level,

312
00:19:01,760 --> 00:19:05,494
right, per sensor. So all of these sensors,

313
00:19:05,542 --> 00:19:08,954
what I do is I wire them up to the breadboard shown here,

314
00:19:09,072 --> 00:19:13,126
and then I have a single set of wires from the breadboard into

315
00:19:13,168 --> 00:19:16,574
the raspberry PI, so that all of these sensors are actually communicating over

316
00:19:16,612 --> 00:19:18,990
the same input channel into the raspberry PI.

317
00:19:19,730 --> 00:19:23,914
And then when I want to fetch a moisture reading from an individual sensor,

318
00:19:23,962 --> 00:19:27,234
I need to call that sensor by name, by its address,

319
00:19:27,352 --> 00:19:30,782
right? So, unfortunately, the sensors that I chose

320
00:19:30,846 --> 00:19:33,966
had a physical limitation in that this I squared

321
00:19:33,998 --> 00:19:36,750
C address could only be set to one of four values.

322
00:19:36,830 --> 00:19:40,246
I did not read the fine print ahead of time. So what this meant is

323
00:19:40,268 --> 00:19:43,702
that for this particular system, this first iteration of it,

324
00:19:43,756 --> 00:19:47,174
I could only monitor four plants at a given time.

325
00:19:47,372 --> 00:19:51,094
There are some ways around this if you're more adept at

326
00:19:51,132 --> 00:19:55,020
hardware projects than I am, but they're definitely outside of the scope of this talk.

327
00:19:55,630 --> 00:19:58,598
But we'll find a way around it in a little bit. So, I know that's

328
00:19:58,614 --> 00:20:02,118
a little bit of handwriting, but I built the system, I hooked up the sensors

329
00:20:02,134 --> 00:20:05,418
to the raspberry PI, and it was ready to collect data from my plants.

330
00:20:05,514 --> 00:20:08,910
All I had to do now was get this information into Kafka.

331
00:20:09,730 --> 00:20:13,018
But again, before we get a little too ahead of ourselves,

332
00:20:13,114 --> 00:20:16,654
I want to take, again, a quick aside and think about the data that we're

333
00:20:16,702 --> 00:20:20,366
writing, right. And what I really want to do is be mindful

334
00:20:20,398 --> 00:20:24,020
of it and craft a schema. Okay, I hope

335
00:20:24,790 --> 00:20:29,206
none of you are groaning, because schemas are really good.

336
00:20:29,308 --> 00:20:32,886
Best practice to adhere to. Seriously, I really recommend it

337
00:20:32,908 --> 00:20:36,006
for any project that you do. Schemas are a really great

338
00:20:36,028 --> 00:20:39,382
way to make sure that downstream systems are receiving the data that they expect

339
00:20:39,436 --> 00:20:42,746
to receive. Schemas also help you reason about your

340
00:20:42,768 --> 00:20:46,106
overall data processing as well, before you get into the weeds with

341
00:20:46,128 --> 00:20:49,498
it. Okay, so I think it's really nice to take a step back and understand

342
00:20:49,584 --> 00:20:52,746
all the fields that you might need for a given project. And so

343
00:20:52,768 --> 00:20:55,886
I did that. I defined an average schema for the

344
00:20:55,908 --> 00:20:59,310
readings that I would be capturing from the sensors. This one was pretty short,

345
00:20:59,380 --> 00:21:03,402
pretty simple. I had my percentage moisture that I'd be fetching.

346
00:21:03,546 --> 00:21:07,058
I also got temperature for free on the sensors as well. So I figured I

347
00:21:07,064 --> 00:21:10,066
would throw that in there just in case. And beyond that,

348
00:21:10,088 --> 00:21:13,266
I added another field for the plant id that would help me keep

349
00:21:13,288 --> 00:21:16,766
track of individual plants know, do the data processing

350
00:21:16,798 --> 00:21:20,230
later, and. Yeah, perfect. So I knew how this data

351
00:21:20,300 --> 00:21:23,560
should look, but how do I get it into Kafka now?

352
00:21:23,930 --> 00:21:27,686
And if you're new to Kafka, there are two main ways to go

353
00:21:27,708 --> 00:21:32,154
about getting your data into Kafka. You can use the producer API or

354
00:21:32,272 --> 00:21:35,498
Kafka connect. So the

355
00:21:35,504 --> 00:21:38,954
producer API is your low level, sort of vanilla option for writing data

356
00:21:38,992 --> 00:21:42,166
into Kafka. And it's really great because you have the ability

357
00:21:42,198 --> 00:21:45,280
to write a producer client in pretty much any language you want.

358
00:21:45,970 --> 00:21:49,390
You'll really want to use the producer API when you either

359
00:21:49,460 --> 00:21:53,002
own or have access to the application that's generating

360
00:21:53,066 --> 00:21:55,520
the data that you're dealing with. So for example,

361
00:21:56,450 --> 00:21:59,678
if I wanted to capture log events from an application and push

362
00:21:59,694 --> 00:22:02,946
them into Kafka, well, I would probably just add a couple of lines of

363
00:22:02,968 --> 00:22:06,674
code. Add a producer that writes that data to Kafka as

364
00:22:06,712 --> 00:22:09,720
that message happens, as that sort of, we hit that log.

365
00:22:11,130 --> 00:22:14,998
The other option is Kafka Connect. And as the name implies, Kafka Connect is

366
00:22:15,004 --> 00:22:18,534
a pretty good way to connect external data sources and also

367
00:22:18,572 --> 00:22:22,354
syncs to Kafka. It's really great

368
00:22:22,412 --> 00:22:25,706
because you don't actually need to write any code to make it happen. To get

369
00:22:25,728 --> 00:22:29,178
up and running, you write a quick configuration file, you point to

370
00:22:29,184 --> 00:22:32,926
your data store and bam, all right, you are capturing data in

371
00:22:32,948 --> 00:22:37,178
real time. You're converting it into events, you're pushing it into Kafka.

372
00:22:37,354 --> 00:22:41,374
It's a really, really good option to consider if you're looking to integrate data

373
00:22:41,492 --> 00:22:44,926
from outside of the Kafka ecosystem, right? Data that's at

374
00:22:44,948 --> 00:22:48,482
rest, that's within a database, that's at the other end of an API call,

375
00:22:48,536 --> 00:22:51,938
maybe. And you're just going to make that data a

376
00:22:51,944 --> 00:22:54,290
little more real time, a little more event driven.

377
00:22:54,790 --> 00:22:58,238
So what did I end up using in my case? Well, I own the raspberry

378
00:22:58,254 --> 00:23:01,558
PI, right? I have access to the script that's collecting the sensor data. So the

379
00:23:01,564 --> 00:23:05,480
producer API made sense, right? So what does that look like?

380
00:23:06,570 --> 00:23:09,974
First of all, you should know that I'm using the confluent Kafka Python library

381
00:23:10,022 --> 00:23:13,302
for my producers. You'll recall that I need to reference

382
00:23:13,366 --> 00:23:17,242
each moisture sensor by its unique address. So I

383
00:23:17,296 --> 00:23:20,726
started with a hard coded mapping of moisture sensor to plant

384
00:23:20,758 --> 00:23:24,314
id. And then within the script, every 30 seconds

385
00:23:24,362 --> 00:23:28,266
I'm looping over this mapping and then I'm capturing the moisture

386
00:23:28,298 --> 00:23:31,534
and temperature data from the appropriate sensor. From there I build

387
00:23:31,572 --> 00:23:34,794
up a readings object according to the schema that I defined.

388
00:23:34,922 --> 00:23:38,446
And then I have a serializing producer that's going to serialize

389
00:23:38,478 --> 00:23:41,922
the data and produce it into Kafka. Perfect. If you want to look

390
00:23:41,976 --> 00:23:45,774
a little bit at the code here. So I'm looping over those plant addresses.

391
00:23:45,902 --> 00:23:49,366
I am accessing the unique address, that sensor using

392
00:23:49,388 --> 00:23:53,446
its unique address, capturing the moisture. I'm doing a little bit of

393
00:23:53,468 --> 00:23:56,722
massaging of this data to convert it into a percentage,

394
00:23:56,866 --> 00:24:00,374
grabbing the temperature and packaging it and sending it off to Kafka.

395
00:24:00,502 --> 00:24:04,086
I will link to the source code later on so you'll

396
00:24:04,118 --> 00:24:06,970
see a little bit more detailed, more than a code snippet.

397
00:24:07,550 --> 00:24:10,926
All right, so using this I can capture these readings from

398
00:24:10,948 --> 00:24:14,640
my raspberry PI and start writing them into Kafka. That's great.

399
00:24:15,810 --> 00:24:19,166
But like I said, those readings, data information

400
00:24:19,268 --> 00:24:22,846
isn't actually enough for me to understand and act on the

401
00:24:22,868 --> 00:24:26,370
watering needs of my plants. Right. I need a little bit more information,

402
00:24:26,440 --> 00:24:29,410
some metadata to actually do something with the information.

403
00:24:29,560 --> 00:24:33,186
Okay. So again, after thinking about

404
00:24:33,208 --> 00:24:37,154
it for a bit, I created an Avro schema for my plant metadata,

405
00:24:37,202 --> 00:24:40,834
right. The biggest thing that this data set contains

406
00:24:40,882 --> 00:24:44,034
is the individual plants and their desired moisture

407
00:24:44,082 --> 00:24:47,782
levels. All right? I also included their names, like the scientific name, their given

408
00:24:47,836 --> 00:24:51,606
name, their common name, just for fun and make the information a little bit easier

409
00:24:51,638 --> 00:24:55,418
for me to read later on. And I also included that plant id

410
00:24:55,504 --> 00:24:59,020
in there as well for joining and processing later.

411
00:24:59,630 --> 00:25:03,626
Again, how do we get this data into Kafka? We have the producer

412
00:25:03,658 --> 00:25:06,640
API or Kafka connect. What should we use?

413
00:25:07,090 --> 00:25:10,478
So this is relatively slow changing data,

414
00:25:10,564 --> 00:25:14,994
right? And Kafka Connect is a fantastic candidate for data

415
00:25:15,032 --> 00:25:18,414
that's at rest or data in a database, something that's not changing.

416
00:25:18,462 --> 00:25:22,020
Often some reference data sets and

417
00:25:22,390 --> 00:25:26,050
metadata or reference data sets like this should probably

418
00:25:26,120 --> 00:25:29,874
live in a database. But I'm going to do some hand waving

419
00:25:29,922 --> 00:25:33,400
here. I still ended up using the producer API in this case.

420
00:25:33,930 --> 00:25:37,206
So even though my plant metadata is slow changing its data at

421
00:25:37,228 --> 00:25:40,934
rest, remember, I only had four plants that I could monitor

422
00:25:40,982 --> 00:25:43,510
at a given time due to that hardware limitation.

423
00:25:43,590 --> 00:25:46,938
Right? So it was a bit of overkill to set up a

424
00:25:46,944 --> 00:25:50,074
database to maintain such a small data set, right?

425
00:25:50,192 --> 00:25:53,226
So for now, I put together a separate python script to serialize

426
00:25:53,258 --> 00:25:56,654
my plant metadata messages according to the schema that I defined using

427
00:25:56,692 --> 00:26:00,730
Avro. And I assigned each plant an id and produced it in a Kafka.

428
00:26:00,890 --> 00:26:04,734
Great. This stage of the project was actually really interesting for me

429
00:26:04,772 --> 00:26:08,446
just as a plant parent. In order to produce the plant

430
00:26:08,478 --> 00:26:11,906
metadata, I needed to do a lot of research to understand my individual

431
00:26:12,008 --> 00:26:15,374
plant's needs. So there was a lot of guessing and checking involved,

432
00:26:15,422 --> 00:26:18,820
like letting the plant get dry hands, checking what that moisture level was.

433
00:26:19,190 --> 00:26:22,726
And it's still not perfect. I still sometimes update the metadata based on

434
00:26:22,748 --> 00:26:25,862
what I learned about my plants over time. So it's actually a really cool learning

435
00:26:25,916 --> 00:26:29,318
curve for me. So the script that I wrote to produce data

436
00:26:29,404 --> 00:26:32,758
during this stage was pretty similar to what I showed earlier.

437
00:26:32,934 --> 00:26:36,186
Again, I'll have that full code linked at the end of this talk if you

438
00:26:36,208 --> 00:26:39,706
want to check it out. So I was able to get my

439
00:26:39,728 --> 00:26:43,486
plant metadata into Kafka. I have my readings being

440
00:26:43,508 --> 00:26:45,360
generated into Kafka as well,

441
00:26:46,770 --> 00:26:49,934
but. All right, is this setup ideal? Right. I have

442
00:26:49,972 --> 00:26:53,306
my collection script on my raspberry PI and there's a hard coded

443
00:26:53,338 --> 00:26:57,026
mapping of sensors to plants. And I

444
00:26:57,048 --> 00:27:00,020
also have to add my plant metadata manually. Right.

445
00:27:00,790 --> 00:27:04,386
That's not very event driven. Right. What if I wanted to switch things up

446
00:27:04,408 --> 00:27:07,940
a bit and change which plants I'm monitoring at a given time? Right.

447
00:27:08,470 --> 00:27:11,654
The first thing I'd have to do is write new plant metadata using

448
00:27:11,692 --> 00:27:14,646
that script that I put together. And that's a manual process. Right. I have to

449
00:27:14,668 --> 00:27:16,950
input that, execute the script.

450
00:27:18,090 --> 00:27:21,530
It's silly, too much. Then I would have to go

451
00:27:21,600 --> 00:27:25,514
into the collection script and alter that hard coded mapping from

452
00:27:25,552 --> 00:27:29,126
that sensor id to the new plant id that I'm inputting.

453
00:27:29,238 --> 00:27:32,298
And then finally to have to move that physical sensor from the old plant to

454
00:27:32,304 --> 00:27:35,886
the new plant. And that's a lot of steps for something that should

455
00:27:35,908 --> 00:27:39,482
be pretty simple. Right? What can I do to make this a little more seamless

456
00:27:39,546 --> 00:27:41,310
for me, the user?

457
00:27:42,770 --> 00:27:46,098
So there's a couple of things that we can improve in here and I want

458
00:27:46,104 --> 00:27:49,906
to focus first on that first part, making it easier to write the

459
00:27:49,928 --> 00:27:53,186
plant metadata into Kafka. All right.

460
00:27:53,368 --> 00:27:56,726
Since I was going to be sending alerts to my phone through the

461
00:27:56,748 --> 00:28:00,562
Telegram app eventually, I thought it would also be cool and confluent

462
00:28:00,626 --> 00:28:04,198
to write data to Kafka, also using my phone.

463
00:28:04,364 --> 00:28:08,770
So I could actually use Telegram to define a conversation workflow

464
00:28:08,850 --> 00:28:12,294
that would allow me to input the plant metadata through a conversation

465
00:28:12,342 --> 00:28:15,914
on my phone. And Telegram could write that data directly to Kafka for

466
00:28:15,952 --> 00:28:19,786
me, which I thought was pretty cool. So for

467
00:28:19,808 --> 00:28:23,078
those of you who aren't familiar, Telegram is a messaging, you know,

468
00:28:23,104 --> 00:28:26,842
WhatsApp or WeChat. But Telegram offers a very convenient

469
00:28:26,906 --> 00:28:30,174
messaging API as well as the ability for users to

470
00:28:30,212 --> 00:28:33,342
create and define their own bots. So it's pretty easy.

471
00:28:33,396 --> 00:28:36,706
You register for a bot, you receive an API key that you can use to

472
00:28:36,728 --> 00:28:40,030
connect to that specific bot, and then from there you can define

473
00:28:40,110 --> 00:28:42,740
how users will have conversations with it.

474
00:28:43,110 --> 00:28:47,214
So specifically there's a pretty nice telegram python

475
00:28:47,262 --> 00:28:50,834
library, and with it you can write a script that processes and handles

476
00:28:50,882 --> 00:28:54,198
any incoming messages with the bot. And then at

477
00:28:54,204 --> 00:28:57,878
the end of this we're going to produce that plant metadata into Kafka. So what

478
00:28:57,884 --> 00:29:01,654
does that look like for any telegram bot

479
00:29:01,782 --> 00:29:04,842
that you design? You're going to follow a pretty similar process, right?

480
00:29:04,976 --> 00:29:08,074
You define a high level conversation handler that lays out

481
00:29:08,112 --> 00:29:11,782
how to start the conversation, the different states that that conversation

482
00:29:11,846 --> 00:29:15,578
should go through, and then the functions or message handlers

483
00:29:15,594 --> 00:29:19,166
that are associated with each conversation state. So if you're into this sort of

484
00:29:19,188 --> 00:29:22,906
thing, it's basically a finite state machine. You just kind of map out the flow

485
00:29:23,018 --> 00:29:25,906
and it's pretty easy to set up once you get the hang of it.

486
00:29:26,088 --> 00:29:29,694
So here you see I'm just laying out the conversation flow

487
00:29:29,742 --> 00:29:33,250
for updating the plant metadata. So I'm capturing the individual

488
00:29:33,320 --> 00:29:36,966
plant, the plant id that I want to update and then all the details that

489
00:29:36,988 --> 00:29:40,838
I need to adhere to that schema that I created. Right?

490
00:29:41,004 --> 00:29:45,030
Hands diving into one of these message handlers a little more closely.

491
00:29:45,690 --> 00:29:49,314
So here I've already prompted myself, the user

492
00:29:49,362 --> 00:29:53,446
for the low moisture threshold. And so I'm capturing that low moisture threshold,

493
00:29:53,558 --> 00:29:56,490
storing it temporarily within that conversation state,

494
00:29:56,640 --> 00:30:00,122
and passing it to the next stage of the conversation. So after

495
00:30:00,176 --> 00:30:03,098
I've captured this low moisture threshold, I'm going to prompt myself to fill in the

496
00:30:03,104 --> 00:30:06,974
high moisture threshold and then return the new state of the conversation that we

497
00:30:07,012 --> 00:30:10,320
should go into, which in turn is that high moisture threshold state.

498
00:30:11,090 --> 00:30:14,478
Once I've gone through all the steps of collecting that information, then it's

499
00:30:14,494 --> 00:30:18,386
time to produce the data, right? And I've captured all

500
00:30:18,408 --> 00:30:21,410
the information according to that schema that I defined.

501
00:30:21,910 --> 00:30:25,378
This is a basic Kafka producer. There's nothing really fancy up

502
00:30:25,384 --> 00:30:28,918
my sleeves here. I did make my life a little bit simpler within the

503
00:30:28,924 --> 00:30:32,358
context of this project, though. I defined some helper classes for

504
00:30:32,524 --> 00:30:35,974
both the clients and the individual data classes. So you

505
00:30:36,012 --> 00:30:40,886
see, I have a means for creating a producer using the houseplant metadata specific serializer

506
00:30:40,998 --> 00:30:44,650
as well as a function for converting a dictionary, which is

507
00:30:44,720 --> 00:30:48,780
the temporary stored state, into a houseplant data type.

508
00:30:49,150 --> 00:30:52,394
So from there, the production process is pretty standard.

509
00:30:52,592 --> 00:30:56,154
And once I start running this bot, I can have a conversation

510
00:30:56,202 --> 00:30:59,726
with it. I could choose the update plant command and

511
00:30:59,748 --> 00:31:02,446
it's going to prompt me for the information that it needs. I'm going to go

512
00:31:02,468 --> 00:31:05,058
through the full flow of it and at the end it's going to ask me

513
00:31:05,064 --> 00:31:09,090
to confirm the information before I produce it into Kafka.

514
00:31:09,430 --> 00:31:13,282
That's awesome. Now I don't have to actually go onto my computer

515
00:31:13,416 --> 00:31:16,686
to produce new plant metadata, right? I can just do it from

516
00:31:16,728 --> 00:31:20,358
my phone which is cool, but that's not really enough, right?

517
00:31:20,444 --> 00:31:23,478
We've handled one aspect, one problem of this situation,

518
00:31:23,564 --> 00:31:26,230
which is getting more metadata into Kafka.

519
00:31:26,730 --> 00:31:30,146
But I still had a hard coded mapping of sensor ids to plants

520
00:31:30,178 --> 00:31:33,338
within my raspberry PI collection script, right? Even if I updated the

521
00:31:33,344 --> 00:31:36,966
metadata and added new plants, I still needed to alter that script manually,

522
00:31:37,078 --> 00:31:40,694
which is not good. So rather than hard code the sensor

523
00:31:40,742 --> 00:31:44,286
to plant mapping in my collection script, I figured, okay, maybe I

524
00:31:44,308 --> 00:31:48,750
can make the script a little more event driven. Hands, get those mappings from Kafka.

525
00:31:49,250 --> 00:31:53,022
All right, so let's collect a little more data

526
00:31:53,076 --> 00:31:56,126
then. Before we do

527
00:31:56,148 --> 00:31:59,518
that though, we need a new schema. All right, I set up a new topic

528
00:31:59,534 --> 00:32:03,166
that would only contain the mappings from a sensor id to a plant

529
00:32:03,198 --> 00:32:06,594
id hands. This way I could easily change which plant was

530
00:32:06,632 --> 00:32:10,086
using which sensor, right? So I did exactly the same thing

531
00:32:10,108 --> 00:32:13,906
I was doing for the metadata collection within the telegram bot.

532
00:32:14,018 --> 00:32:17,170
And so I set up a very similar conversation handler.

533
00:32:17,330 --> 00:32:21,126
This time it prompts me to select a sensor id and input the new plant

534
00:32:21,158 --> 00:32:23,020
id that it should be mapped to.

535
00:32:25,550 --> 00:32:28,634
So now I just had to make my collection script use those

536
00:32:28,672 --> 00:32:32,394
new mappings from Kafka. And similar

537
00:32:32,432 --> 00:32:35,566
to producing the data, there are a couple of ways to get data out

538
00:32:35,588 --> 00:32:39,360
of Kafka. You can either use the consumer API or Kafka connect as well.

539
00:32:39,890 --> 00:32:43,630
Much of the same considerations come into play for data consumption as it does

540
00:32:43,700 --> 00:32:47,762
production. But generally, if you want to react to every kafka event,

541
00:32:47,896 --> 00:32:51,314
you're likely going to use the consumer API. This is pretty

542
00:32:51,352 --> 00:32:54,978
flexible. It's available in most languages you want, and you're free to do whatever you

543
00:32:54,984 --> 00:32:57,586
want with the data at that point. On the other hand, if you're looking to

544
00:32:57,608 --> 00:33:00,946
move the data somewhere like a database or to some longer term

545
00:33:00,978 --> 00:33:04,326
data store, then Kafka connect is going to help you consume the data and move

546
00:33:04,348 --> 00:33:08,166
it to that sync. In my case,

547
00:33:08,268 --> 00:33:11,594
the consumer API made sense. So the first thing I did was add

548
00:33:11,632 --> 00:33:15,146
a Kafka consumer to my collection script to read from

549
00:33:15,168 --> 00:33:18,886
the topic and build up a dictionary of those mappings from sensor

550
00:33:18,918 --> 00:33:22,206
id to plant id. There are a few extra details to be aware of,

551
00:33:22,228 --> 00:33:26,350
though. The first is that the mapping topic was configured to be compacted.

552
00:33:26,770 --> 00:33:30,634
This basically makes the topic like a table

553
00:33:30,682 --> 00:33:34,654
where it maintains the latest value per key. And that should

554
00:33:34,692 --> 00:33:37,954
make sense because I don't really care about which plant the sensor was mapped to

555
00:33:37,992 --> 00:33:41,154
before. What matters is what that sensor is mapped to.

556
00:33:41,192 --> 00:33:44,738
Now also recall that Kafka consumers have

557
00:33:44,744 --> 00:33:48,530
a way to keep track of the last message that they consumed and processed,

558
00:33:48,610 --> 00:33:52,198
right? It's called an offset. So offsets are

559
00:33:52,204 --> 00:33:55,846
really great when you have a way to maintain state in the consuming application.

560
00:33:56,028 --> 00:33:59,842
But I didn't want my application, this script to be stateful,

561
00:33:59,906 --> 00:34:03,894
right. I didn't want to deal with persisted state in my collection script.

562
00:34:04,022 --> 00:34:07,386
So to get around this, I needed my consumer to start from the

563
00:34:07,408 --> 00:34:10,666
earliest message in the topic every time it started up.

564
00:34:10,848 --> 00:34:14,442
So do this. I used a few configuration parameters

565
00:34:14,506 --> 00:34:17,562
for my consumer and made my consumer non

566
00:34:17,626 --> 00:34:21,146
committing. Basically, it's never going to keep track of a bookmark,

567
00:34:21,178 --> 00:34:24,638
it's never going to commit an offset. And every time I start up that

568
00:34:24,644 --> 00:34:28,514
consumer, it's going to start from the beginning of that topic, every time.

569
00:34:28,712 --> 00:34:32,126
So on startup, when I execute this script, my consumer

570
00:34:32,158 --> 00:34:35,766
is first going to block the rest of the script. It's going to read from

571
00:34:35,788 --> 00:34:39,462
the Kafka topic and keep collecting readings until it has a full

572
00:34:39,516 --> 00:34:41,750
mapping for all four sensors.

573
00:34:43,130 --> 00:34:46,962
Once that's done, in between the looping of every 30 seconds,

574
00:34:47,106 --> 00:34:50,406
my consumer is going to try to fetch new mappings just in

575
00:34:50,428 --> 00:34:53,642
case we've added a new one. So if I ever move a sensor around

576
00:34:53,696 --> 00:34:57,386
or update anything, that new mapping is going to be propagated within

577
00:34:57,488 --> 00:35:01,226
30 seconds, which is good enough for my use case. So at

578
00:35:01,248 --> 00:35:03,966
this point I was free to move my plants around, right? I was free to

579
00:35:03,988 --> 00:35:07,882
add new plants from my phone, define new mappings,

580
00:35:08,026 --> 00:35:11,198
and at that point I could rest assured that my raspberry PI was going

581
00:35:11,204 --> 00:35:15,226
to be fetching the data that it should be for the plants that it should,

582
00:35:15,348 --> 00:35:18,626
which is great. So the first half of

583
00:35:18,648 --> 00:35:22,094
my pipeline is complete, right. It's fully event driven. I'm capturing

584
00:35:22,142 --> 00:35:25,880
this data. I don't have to worry about manually inputting anything pretty much,

585
00:35:27,450 --> 00:35:30,982
but now all the information is in Kafka. Now I just need to start

586
00:35:31,036 --> 00:35:36,054
making sense of it. Using stream processing for

587
00:35:36,092 --> 00:35:39,366
stream processing with kafka you have options. You have a lot of

588
00:35:39,388 --> 00:35:43,446
options. The OG way is to use the producer and consumer API

589
00:35:43,478 --> 00:35:46,646
directly. And these APIs are great because they're available in all your favorite

590
00:35:46,678 --> 00:35:50,578
languages and then some. But to use them for streamed processing,

591
00:35:50,614 --> 00:35:54,426
you'll have to go through the full process of consuming the data from Kafka,

592
00:35:54,538 --> 00:35:57,150
then doing any of your transformations in memory.

593
00:35:57,970 --> 00:36:01,082
If you're doing stateless transformations,

594
00:36:01,226 --> 00:36:04,814
great, move on with your life. Transform them, produce the data

595
00:36:04,852 --> 00:36:08,322
back to Kafka and you're done. If you're doing stateful transformations though,

596
00:36:08,376 --> 00:36:11,474
there's a lot more to consider, right? You have to handle your state,

597
00:36:11,512 --> 00:36:15,058
you have to decide how to make it fault tolerant. What happens if your application

598
00:36:15,144 --> 00:36:19,266
goes down? And that's a lot to think of. For stream

599
00:36:19,298 --> 00:36:22,520
processing, this involves a lot more work,

600
00:36:22,970 --> 00:36:26,662
especially from the producer consumer API. They're the lowest level option

601
00:36:26,716 --> 00:36:29,958
available to you, but there are some opportunities to be a

602
00:36:29,964 --> 00:36:32,650
little more flexible in what you can do with the code at that level.

603
00:36:32,720 --> 00:36:36,618
So do what you will. If you're a

604
00:36:36,624 --> 00:36:39,898
sane person, though, you'll likely use one of the next two options. So moving

605
00:36:39,984 --> 00:36:43,398
up in ease of use is Kafka streams. And Kafka

606
00:36:43,414 --> 00:36:46,990
Streams is a Java and scala library that takes the hassle of managing state

607
00:36:47,060 --> 00:36:51,562
off of your hands. So out of the box already, it makes stateful processing

608
00:36:51,626 --> 00:36:55,454
so much easier for Kafka. It gives

609
00:36:55,492 --> 00:36:58,420
you a lot of stateful transformations out of the box available.

610
00:36:58,950 --> 00:37:02,034
It's built on top of the consumer producer API, so you get

611
00:37:02,072 --> 00:37:05,378
a lot of cool things for free, like the scalability from the

612
00:37:05,384 --> 00:37:09,314
consumer group protocol that allows the consumers to parallelize the processing

613
00:37:09,362 --> 00:37:12,886
from a given set of input topics. And what that means for you

614
00:37:12,908 --> 00:37:17,350
is that you can spin up multiple instances of a Kafka streams application and

615
00:37:17,420 --> 00:37:21,546
they're going to coordinate and share the input data across the

616
00:37:21,568 --> 00:37:24,986
running instances. And if one of those instances goes down

617
00:37:25,008 --> 00:37:28,234
for any reason, the remaining instances will

618
00:37:28,272 --> 00:37:31,398
rebalance. All right, they're going to take whichever partitions that

619
00:37:31,424 --> 00:37:34,846
one instance was processing, redistribute them

620
00:37:34,868 --> 00:37:38,734
across the other remaining instances and also bring the state with

621
00:37:38,772 --> 00:37:42,046
it, right, so you never lose that state. It's always persisted to

622
00:37:42,068 --> 00:37:46,138
Kafka. And the running instances are just going to be able to find that and

623
00:37:46,164 --> 00:37:49,906
keep going way better than the consumer producer API. And you

624
00:37:49,928 --> 00:37:52,258
get that for free, right? There's very little that you have to set up to

625
00:37:52,264 --> 00:37:55,538
make that happen. And finally,

626
00:37:55,704 --> 00:37:59,474
if Java isn't your thing, the easiest and most convenient way to transform

627
00:37:59,522 --> 00:38:02,710
your data from Kafka is using KSQlDB.

628
00:38:03,530 --> 00:38:06,962
The really cool thing about this is that it's just SQL syntax,

629
00:38:07,106 --> 00:38:10,922
and within that you have access to pretty powerful stream processing that's built

630
00:38:10,976 --> 00:38:14,666
on top of Kafka streamed. So you can filter, you can

631
00:38:14,688 --> 00:38:18,666
join, you can aggregate a lot more, and you can do this

632
00:38:18,848 --> 00:38:21,558
entirely within the confluent cloud console,

633
00:38:21,734 --> 00:38:24,478
which was another reason that I wanted to do it because again, I didn't want

634
00:38:24,484 --> 00:38:27,838
to deal with any infrastructure and this allowed me to do

635
00:38:27,844 --> 00:38:31,646
it within the web console. So that's what I chose to use. Let's get

636
00:38:31,668 --> 00:38:36,254
into it let's process this information. When you're working with KSqlDB,

637
00:38:36,302 --> 00:38:39,522
the first thing you need to do is get your data into it, right?

638
00:38:39,656 --> 00:38:43,298
And to do so, you need to decide how your data has to

639
00:38:43,304 --> 00:38:46,766
be represented. There's only two choices,

640
00:38:46,798 --> 00:38:50,710
right? There's two main constructs in KSQlDB, you have a table, you have a streamed

641
00:38:51,290 --> 00:38:54,786
streams represent unbounded ongoing series of events.

642
00:38:54,898 --> 00:38:59,062
Great tables on the other hand, show the current state for

643
00:38:59,116 --> 00:39:03,366
a particular key. So let's look at my houseplant metadata

644
00:39:03,398 --> 00:39:07,194
topic first. The metadata should

645
00:39:07,232 --> 00:39:10,986
probably be represented as a table. Tables keep track of the

646
00:39:11,008 --> 00:39:14,126
most recent value for key per key, as I said. So if I ever wanted

647
00:39:14,148 --> 00:39:17,534
to update a value for a given plant id, say tweak the low

648
00:39:17,572 --> 00:39:21,482
moisture threshold, I would probably want my processing application to leverage

649
00:39:21,546 --> 00:39:24,660
that most recent value first. I don't care about the old ones.

650
00:39:26,230 --> 00:39:29,298
So the first thing I'm going to do is point to the Kafka topic where

651
00:39:29,304 --> 00:39:32,382
that houseplant metadata lives and specify

652
00:39:32,446 --> 00:39:36,370
the value format, right? How am I viewing this data?

653
00:39:36,520 --> 00:39:39,240
And it's Avro. We have an Avro schema for it.

654
00:39:40,410 --> 00:39:43,602
You'll also note that this is a pretty SQL esque statement,

655
00:39:43,666 --> 00:39:47,366
right? A create table statement. The biggest part

656
00:39:47,388 --> 00:39:50,602
of the statement is, like I said, pointing to that Kafka topic, defining that value

657
00:39:50,656 --> 00:39:54,202
format. And the cool thing is

658
00:39:54,256 --> 00:39:57,546
about defining an Avro schema. Taking the time to do that in the

659
00:39:57,568 --> 00:40:01,030
beginning is that I don't have to specify all the fields and

660
00:40:01,040 --> 00:40:04,654
their types because KSQlDB as a consumer can

661
00:40:04,692 --> 00:40:08,506
access the schema registry where that topic schema

662
00:40:08,538 --> 00:40:11,774
is stored, access that schema and use

663
00:40:11,812 --> 00:40:15,566
it to parse that field and type information for me. So you'll recall

664
00:40:15,598 --> 00:40:18,834
that there are about ten or so fields in that

665
00:40:18,872 --> 00:40:22,226
schema and I only had to type one, right? So no

666
00:40:22,248 --> 00:40:26,030
typos here, which is great. The only fields that I hands to specify

667
00:40:26,110 --> 00:40:29,862
here explicitly is the key because a table needs

668
00:40:29,916 --> 00:40:33,750
to know which key to determine the most recent value for,

669
00:40:33,820 --> 00:40:38,006
right? So in this case, I just want to use the plant id so

670
00:40:38,028 --> 00:40:41,398
I can take that SQl that create table statement, I can run it in the

671
00:40:41,404 --> 00:40:44,614
confluent cloud KSqldb editor and I can start up my streaming

672
00:40:44,662 --> 00:40:48,122
application behind the scenes and it's going to bring this data in hands,

673
00:40:48,176 --> 00:40:50,060
start building up a table for it,

674
00:40:51,950 --> 00:40:55,914
for my houseplant readings data. Every message is relevant

675
00:40:55,962 --> 00:40:59,418
in that ongoing series of events, right? So it should be a stream,

676
00:40:59,514 --> 00:41:02,906
right? And in this case we're going to use a very very similar statement

677
00:41:02,938 --> 00:41:06,610
to the create table one, but in this case we're using create streamed.

678
00:41:07,110 --> 00:41:10,434
Again, very similar. We're pointing to the Kafka topic where

679
00:41:10,472 --> 00:41:13,358
those readings reside. We're pointing to that value format.

680
00:41:13,454 --> 00:41:16,966
And since we used Avro, there are some perks. I don't have

681
00:41:16,988 --> 00:41:20,274
to explicitly write out any of the fields and their types

682
00:41:20,402 --> 00:41:23,798
for consistency. I'm just specifying we're bringing in that

683
00:41:23,884 --> 00:41:27,094
plant id as the key. So cool.

684
00:41:27,132 --> 00:41:31,082
I have my metadata as input in my Kisco DB application.

685
00:41:31,216 --> 00:41:34,214
I have my houseplant readings being captured,

686
00:41:34,342 --> 00:41:38,298
hands reflected as a streamed within this application as well.

687
00:41:38,464 --> 00:41:41,958
Now we need to enrich the data sets

688
00:41:41,974 --> 00:41:45,134
with one another. Remember, the readings aren't good enough

689
00:41:45,172 --> 00:41:48,880
on their own. We need the metadata in order to make sense of them.

690
00:41:49,250 --> 00:41:52,558
So I needed to join these two data sets together. This is going

691
00:41:52,564 --> 00:41:55,746
to help me with that processing that I need to do later on. This is

692
00:41:55,768 --> 00:42:00,050
a pretty hefty statement. So let's focus on a couple of components individually.

693
00:42:00,710 --> 00:42:04,562
The first is that select statement. This is

694
00:42:04,616 --> 00:42:08,686
effectively SQL, right? I'm joining two data sets, so I'm going to do a

695
00:42:08,728 --> 00:42:12,102
join. I'm going to first select the fields from each data

696
00:42:12,156 --> 00:42:16,310
set that I want to be contained in the output. And I'm using

697
00:42:16,380 --> 00:42:19,686
something called a stream to table join that should feel very similar to

698
00:42:19,708 --> 00:42:23,434
a regular SQL join. I'm just doing an inner join to make sure that

699
00:42:23,472 --> 00:42:26,602
every output row has all of the information that I need from each data

700
00:42:26,656 --> 00:42:30,006
set and so that I have everything that I need for that processing

701
00:42:30,038 --> 00:42:34,270
in the next step. So if I just run this SQL query,

702
00:42:34,690 --> 00:42:38,126
it's going to just give me what's currently in the application.

703
00:42:38,228 --> 00:42:41,870
Right now, the current state of the application, all the data that's currently there,

704
00:42:41,940 --> 00:42:44,946
it will execute this join hands, spit it out.

705
00:42:45,128 --> 00:42:48,526
But what I really want is for this statement, the result of this statement,

706
00:42:48,558 --> 00:42:51,998
to be persisted somewhere. And I'm

707
00:42:52,014 --> 00:42:55,970
going to persist it to a stream, right? Because everything

708
00:42:56,040 --> 00:42:59,446
that's output from this is going to be relevant. I want to process it,

709
00:42:59,468 --> 00:43:02,934
I want to look at it. Okay, so I'm going to use a

710
00:43:02,972 --> 00:43:06,614
create stream statement. I'm going to specify the topic where I want to

711
00:43:06,652 --> 00:43:10,498
persist this data to specify the value format.

712
00:43:10,594 --> 00:43:13,866
And so now when I execute that SQL, it will give me

713
00:43:13,888 --> 00:43:16,700
everything that's currently available in the state of this application,

714
00:43:17,070 --> 00:43:20,770
do the join, output it and persist to that stream.

715
00:43:20,950 --> 00:43:24,222
Great. There is one more detail to consider though. Okay.

716
00:43:24,276 --> 00:43:28,206
And there's one little line at the bottom of this query and that

717
00:43:28,228 --> 00:43:32,282
is emit changes. So emit changes indicates

718
00:43:32,346 --> 00:43:36,254
a push query. So what this means is that the result set is

719
00:43:36,292 --> 00:43:39,054
going to be an open ended stream that's pushed me as output.

720
00:43:39,182 --> 00:43:42,494
So not only when I run this SQL statement,

721
00:43:42,542 --> 00:43:45,586
not only am I going to get the current state of the full application,

722
00:43:45,688 --> 00:43:49,470
everything that's currently in those Kafka topics. Now,

723
00:43:49,560 --> 00:43:53,090
every time a new reading flows in through the houseplant readings topic,

724
00:43:53,170 --> 00:43:56,358
we're going to execute this query. That result row is going to

725
00:43:56,364 --> 00:43:59,846
be output and it's going to be appended to this stream. So every time new

726
00:43:59,868 --> 00:44:03,114
information comes in, we're going to enrich it hands. I'm going to get that

727
00:44:03,152 --> 00:44:06,746
information out. This is contrasted with

728
00:44:06,768 --> 00:44:09,914
something called a pull query, where I'm pulling a finite result set

729
00:44:09,952 --> 00:44:12,986
that reflects the current state of the underlying streaming application.

730
00:44:13,088 --> 00:44:16,842
Right. That's the example that I saw that I gave you earlier.

731
00:44:16,906 --> 00:44:20,794
It's just going to tell me the current state of the application, execute the query.

732
00:44:20,922 --> 00:44:24,080
Done. It's not going to keep running. All right,

733
00:44:26,070 --> 00:44:30,130
so now I've enriched these data sets, I have all the information I need.

734
00:44:30,200 --> 00:44:34,162
Every time new data comes in, we're executing that join and I have

735
00:44:34,216 --> 00:44:37,714
that data available to use in the next step. So let's

736
00:44:37,762 --> 00:44:39,800
revisit the overall goal here.

737
00:44:40,410 --> 00:44:43,714
Whenever a house plant has enough low moisture readings

738
00:44:43,842 --> 00:44:47,474
telling me that it needs to be watered, right, I want to receive an alert

739
00:44:47,522 --> 00:44:51,082
on my phone. Now, I've changed this value

740
00:44:51,136 --> 00:44:54,186
a couple of times since I've started this project about a year ago,

741
00:44:54,288 --> 00:44:58,122
but right now I am collecting moisture data every 30

742
00:44:58,176 --> 00:45:01,660
seconds or so. So with that in mind, I decided that

743
00:45:02,190 --> 00:45:05,646
if a plant needs to be watered, I don't want to receive an alert every

744
00:45:05,668 --> 00:45:09,166
30 minutes. Every 30 seconds. Right. That's entirely too many alerts on my

745
00:45:09,188 --> 00:45:12,554
phone. So in the event that I'm out of the house or I'm busy,

746
00:45:12,682 --> 00:45:16,654
I figured that receiving an alert every 6 hours would

747
00:45:16,692 --> 00:45:20,354
give me enough time to act on and water my plant, or at the very

748
00:45:20,392 --> 00:45:23,938
least come home, tell my partner to go water the plant, whatever I

749
00:45:23,944 --> 00:45:27,538
need to do. That gives me enough time. I also noticed that

750
00:45:27,544 --> 00:45:30,566
these sensors weren't perfect. Right. They were a pretty good value. They were a couple

751
00:45:30,588 --> 00:45:34,086
of dollars apiece. But what that meant is

752
00:45:34,108 --> 00:45:37,442
that sometimes I can get false low or false high readings.

753
00:45:37,586 --> 00:45:40,918
But the general trend over time is downward, right. The plant

754
00:45:40,934 --> 00:45:44,570
is drying out, the water percentage is going to decrease.

755
00:45:44,910 --> 00:45:48,106
So within a given six hour period, it's not going to be

756
00:45:48,128 --> 00:45:51,414
a perfect decrease in moisture over that entire period.

757
00:45:51,462 --> 00:45:55,214
It's not going to be monotonously decreasing. So it would be good

758
00:45:55,252 --> 00:45:58,782
enough to send an alert whenever I received at least

759
00:45:58,836 --> 00:46:02,426
one hour's worth of low readings within that six hour period.

760
00:46:02,538 --> 00:46:05,886
Right? So with the readings being taken every 30 seconds,

761
00:46:05,918 --> 00:46:09,858
that means I should at least receive 120 low readings before

762
00:46:09,944 --> 00:46:11,650
I trigger an alert.

763
00:46:13,030 --> 00:46:16,946
That might sound like a lot, but this is the query that

764
00:46:16,968 --> 00:46:19,140
I wrote to achieve what I just said.

765
00:46:19,770 --> 00:46:23,190
Let's break this down, focus on a few key details.

766
00:46:23,610 --> 00:46:27,062
The first thing is that I wanted to receive an alert at most

767
00:46:27,116 --> 00:46:30,410
every 6 hours, right? So this is the most important part.

768
00:46:30,480 --> 00:46:34,070
I'm setting up the query to aggregate over non overlapping

769
00:46:34,150 --> 00:46:37,914
six hour windows. So already when an event flows in,

770
00:46:38,032 --> 00:46:41,274
the query is first going to bucket it into the appropriate

771
00:46:41,322 --> 00:46:43,520
six hour window. Great.

772
00:46:44,690 --> 00:46:48,682
Within that window, I want to count the events per plant

773
00:46:48,826 --> 00:46:52,090
where the moisture reading was lower than the low moisture threshold

774
00:46:52,170 --> 00:46:54,666
as determined by my plant metadata.

775
00:46:54,858 --> 00:46:58,674
So when I get at least 120 of those readings within that six hour

776
00:46:58,712 --> 00:47:02,498
window, we're going to output a result. And note that

777
00:47:02,504 --> 00:47:05,670
I'm grouping by plant id. Also, within that select

778
00:47:05,740 --> 00:47:09,254
statement, I am grabbing all the details that I might

779
00:47:09,292 --> 00:47:13,074
need to alert on and I'm building up an alert

780
00:47:13,122 --> 00:47:16,150
message here saying this particular plant is dry,

781
00:47:17,290 --> 00:47:21,298
so that's going to be involved as part of the output. I don't

782
00:47:21,314 --> 00:47:24,698
want to oversimplify this because it's really easy to look at this and forget that

783
00:47:24,704 --> 00:47:28,426
you're building a streaming application. And for

784
00:47:28,448 --> 00:47:32,054
better or for worse, with all the things that are involved with a stateful streaming

785
00:47:32,102 --> 00:47:35,326
application, there is state. You have to keep

786
00:47:35,348 --> 00:47:38,718
that in mind, you have to think about it. So let's review this again with

787
00:47:38,724 --> 00:47:41,998
that in mind. So first of all, another thing to

788
00:47:42,004 --> 00:47:45,434
keep in mind is that when you're windowing and conducting an aggregate in KSQldb,

789
00:47:45,482 --> 00:47:49,006
the output is going to be a table where each row is computed

790
00:47:49,038 --> 00:47:52,274
per key per window. So per plant per six hour

791
00:47:52,312 --> 00:47:55,458
period. This should make sense because a table is going to

792
00:47:55,464 --> 00:47:58,806
provide the latest value per key in the end. So that

793
00:47:58,828 --> 00:48:02,514
should feel right. So as each input

794
00:48:02,562 --> 00:48:05,654
record is processed, what happens first?

795
00:48:05,692 --> 00:48:09,546
Well, we're first bucketing it into that appropriate six hour window and we're looking

796
00:48:09,568 --> 00:48:12,380
at the key, right? We're looking at the individual plant id.

797
00:48:13,550 --> 00:48:17,606
If the record makes it through the filter, we're updating

798
00:48:17,718 --> 00:48:21,114
the underlying state for that window per

799
00:48:21,152 --> 00:48:24,326
that plant id. So every reading

800
00:48:24,358 --> 00:48:28,314
that breaches that moisture threshold is going to be counted toward that 120 readings

801
00:48:28,362 --> 00:48:32,046
that we need in order to output a result. So every time

802
00:48:32,068 --> 00:48:36,310
something flows in and breaches a threshold. We're updating the state, we're incrementing that counter,

803
00:48:36,490 --> 00:48:40,178
and then once we reach 120, we output that result.

804
00:48:40,264 --> 00:48:43,714
Great. You might be asking yourself what happens after we

805
00:48:43,752 --> 00:48:47,294
reach that 120th load moisture reading, right, because we're just saying having

806
00:48:47,352 --> 00:48:51,042
count over 120. Well, will the events

807
00:48:51,106 --> 00:48:54,646
continuously be output? Am I going to receive an event, an alert every

808
00:48:54,668 --> 00:48:58,194
30 seconds after that point? No, because I've included

809
00:48:58,242 --> 00:49:00,860
an additional line at the end. Emit final.

810
00:49:02,030 --> 00:49:05,606
I love this feature. It saves me a lot of time as opposed

811
00:49:05,638 --> 00:49:08,826
to emit changes. Emit final says wait until the

812
00:49:08,848 --> 00:49:11,882
window hands closed before we output any result.

813
00:49:12,016 --> 00:49:15,966
Okay, so we're going to wait for that six hour window to fully close

814
00:49:16,068 --> 00:49:18,766
and then we're going to assess per plant id,

815
00:49:18,948 --> 00:49:22,606
does that plant have at least 100 hands, 20 low readings? If it does

816
00:49:22,708 --> 00:49:26,980
output a row, that table is going to contain a message. All right,

817
00:49:27,590 --> 00:49:31,822
and that's great. Now, I had a Kafka topic containing messages

818
00:49:31,886 --> 00:49:35,330
for every time a plant needed to be watered. At most once

819
00:49:35,400 --> 00:49:39,206
per six hour period. All I hands to do now was get that information out

820
00:49:39,228 --> 00:49:42,598
of Kafka. So let's see how to do this

821
00:49:42,604 --> 00:49:45,906
with Telegram. We already saw how Telegram could be used to define

822
00:49:45,938 --> 00:49:49,366
a conversation flow and write data to Kafka. But this time I just

823
00:49:49,388 --> 00:49:52,774
wanted to push the data to the conversation I already had going with my telegram

824
00:49:52,822 --> 00:49:56,602
bot. So the telegram API allows me to look

825
00:49:56,656 --> 00:50:00,574
into the conversations that were currently going on with my bot and

826
00:50:00,692 --> 00:50:03,802
see which ones were there using a unique

827
00:50:03,866 --> 00:50:07,486
chat id. So I hands a chat id defining that

828
00:50:07,508 --> 00:50:09,790
conversation between me and my bot.

829
00:50:10,770 --> 00:50:14,606
Using the bot API key that I had for my bot

830
00:50:14,798 --> 00:50:19,102
as well as the conversation id, I could define a unique endpoint

831
00:50:19,166 --> 00:50:22,690
that I could use to send data directly to that conversation.

832
00:50:23,670 --> 00:50:27,294
So conveniently, there's a Kafka connect HTTP

833
00:50:27,342 --> 00:50:31,302
sync connector that only requires an HTTP endpoint to send data

834
00:50:31,356 --> 00:50:35,186
to. As a bonus, this connector is offered as a fully managed component

835
00:50:35,218 --> 00:50:38,726
in confluent cloud, meaning that I didn't need to run it on my own machines.

836
00:50:38,918 --> 00:50:42,074
All I had to do was configure it through confluent cloud,

837
00:50:42,272 --> 00:50:44,410
input that HTTP endpoint,

838
00:50:45,230 --> 00:50:48,614
and tell it how to extract

839
00:50:48,662 --> 00:50:52,870
the information from the Kafka message that's driving and triggering those alerts.

840
00:50:53,030 --> 00:50:56,666
And then once I start that connect, it's going to consume from that alert topic.

841
00:50:56,698 --> 00:50:59,886
It's going to be triggered by the messages on that topic. It's going to

842
00:50:59,908 --> 00:51:03,214
use Regex to extract the alerting message. That field

843
00:51:03,252 --> 00:51:07,054
that I wrote up that I defined, and it's going to send that alert directly

844
00:51:07,102 --> 00:51:09,890
to the conversation I had started with my bot.

845
00:51:10,790 --> 00:51:13,922
And then this is what it looks like. This is exactly

846
00:51:13,976 --> 00:51:17,306
the message that I receive on my phone every time a plant

847
00:51:17,358 --> 00:51:18,790
needs to be watered.

848
00:51:20,890 --> 00:51:24,546
So I'm pretty proud of the fact that I have a fully functioning, event driven

849
00:51:24,578 --> 00:51:27,670
pipelines for a very real use case that I care about.

850
00:51:27,740 --> 00:51:31,250
All right, this solved me. This saved me so much time and really solved a

851
00:51:31,260 --> 00:51:34,698
real problem that I had. I hope you enjoyed it.

852
00:51:34,784 --> 00:51:38,010
But what do you get out of this, right? You're probably asking yourself,

853
00:51:38,160 --> 00:51:40,826
you followed along, you saw a cool thing that I built,

854
00:51:40,928 --> 00:51:44,582
but I want you to look a little bit deeper, okay? You actually learned

855
00:51:44,646 --> 00:51:48,240
quite a bit about streaming data pipelines and what you can do with them.

856
00:51:48,610 --> 00:51:51,886
So you saw the different ways that you can get data into Kafka, right?

857
00:51:51,908 --> 00:51:55,666
Either using the producer API or maybe Kafka connect if you have

858
00:51:55,848 --> 00:51:59,694
the right data source. I introduced you to stream processing

859
00:51:59,742 --> 00:52:03,134
how to do it in different ways, either using the consumer and producer API,

860
00:52:03,182 --> 00:52:06,466
maybe Kafka streams, and specifically a KSQlDB in

861
00:52:06,488 --> 00:52:10,166
my situation. And finally you saw how to get data out

862
00:52:10,188 --> 00:52:13,666
of Kafka with Kafka Connect, and how also to use the consumer

863
00:52:13,698 --> 00:52:17,442
API to make my collection script a little more event driven.

864
00:52:17,506 --> 00:52:21,918
So every streaming data pipeline needs some combination

865
00:52:21,954 --> 00:52:25,226
of these three things. All right, so now you have the

866
00:52:25,248 --> 00:52:28,490
tools to get started on your own, and I hope you will.

867
00:52:28,560 --> 00:52:31,862
I hope that I have planted the seeds in your mind that you're now curious

868
00:52:31,926 --> 00:52:35,454
and you would want to try out Kafka maybe for something

869
00:52:35,492 --> 00:52:39,582
in your own home. So I have a link here to my link

870
00:52:39,636 --> 00:52:43,354
tree that in turn has many more links to my

871
00:52:43,412 --> 00:52:46,910
source code, the repository outlining the collection scripts,

872
00:52:46,990 --> 00:52:50,706
the metadata creation script, and also the

873
00:52:50,728 --> 00:52:54,114
telegram bot as well as the SQL that I use to

874
00:52:54,152 --> 00:52:57,394
actually do the data processing. And I've also included some additional

875
00:52:57,442 --> 00:53:01,746
resources there that link to confluent developer, our specific developer

876
00:53:01,778 --> 00:53:05,362
portal that has many tutorials, how to guides,

877
00:53:05,426 --> 00:53:08,866
language guides if you want to learn a bit more about Kafka.

878
00:53:08,898 --> 00:53:12,326
So I hope you'll check it out. And if

879
00:53:12,348 --> 00:53:15,414
you have any questions at all, feel free to reach out to me. I am

880
00:53:15,452 --> 00:53:18,966
very much open to chat about the system and kafka at

881
00:53:18,988 --> 00:53:22,526
any time. Until then, thank you for attending my talk. I really

882
00:53:22,548 --> 00:53:23,100
appreciate it.

