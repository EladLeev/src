1
00:00:00,410 --> 00:00:06,126
Jamaica make up real

2
00:00:06,148 --> 00:00:09,902
time feedback into the behavior of your distributed systems and

3
00:00:09,956 --> 00:00:13,114
observing changes exceptions errors

4
00:00:13,162 --> 00:00:16,666
in real time allows you to not only experiment with confidence,

5
00:00:16,778 --> 00:00:20,480
but respond instantly to get things working again.

6
00:00:24,610 --> 00:01:07,606
Code Samatrix

7
00:01:07,658 --> 00:01:11,982
or taming agent chaos for Conf 42 Chaos Engineering

8
00:01:12,046 --> 00:01:15,426
2022 I'm Zach Wasserman. I'm the

9
00:01:15,448 --> 00:01:19,426
CTO of Fleet and a member of the OS Query technical steering committee,

10
00:01:19,618 --> 00:01:23,686
and I'm excited to be presenting this to you today. So for

11
00:01:23,708 --> 00:01:27,320
a bit of background, I'll start by explaining what Osquery is.

12
00:01:27,710 --> 00:01:31,194
Osquery is an agent that endeavors to bring visibility to

13
00:01:31,232 --> 00:01:34,550
all of our endpoints. We're talking macOS,

14
00:01:34,630 --> 00:01:38,154
Linux and Windows. And the way that it does this

15
00:01:38,192 --> 00:01:42,174
is it exposes the operating system internals and

16
00:01:42,212 --> 00:01:46,430
the state of the system as though they were a relational database.

17
00:01:46,770 --> 00:01:50,218
So I helped co create OS query at Facebook.

18
00:01:50,394 --> 00:01:53,802
It's since been transitioned to a project with the Linux foundation.

19
00:01:53,946 --> 00:01:57,410
It's all open source, so go check it out on GitHub.

20
00:01:57,750 --> 00:02:01,234
Like I said, Oscar is cross platform. It's also read

21
00:02:01,272 --> 00:02:04,526
only by design, which is something that's a bit helpful in constraining

22
00:02:04,558 --> 00:02:08,342
the possible problem space when we're thinking about the

23
00:02:08,476 --> 00:02:11,000
possible consequences of running this agent.

24
00:02:11,690 --> 00:02:15,842
And we deploy it across quite a heterogeneous

25
00:02:15,986 --> 00:02:19,430
infrastructure, lots of different devices.

26
00:02:19,590 --> 00:02:23,350
We're talking production servers, corporate servers,

27
00:02:23,430 --> 00:02:26,470
workstations across all of these major platforms.

28
00:02:26,630 --> 00:02:30,538
So we're seeing lots of different deployments for lots

29
00:02:30,554 --> 00:02:34,750
of use cases across security, it and operations.

30
00:02:35,250 --> 00:02:39,070
And then we have fleet, which is the open code

31
00:02:39,410 --> 00:02:42,646
osquery management server. And organizations

32
00:02:42,698 --> 00:02:46,606
are using fleet to manage Osquery across hundreds

33
00:02:46,638 --> 00:02:48,530
of thousands of devices.

34
00:02:49,430 --> 00:02:52,914
Fleet is open core, so part of its

35
00:02:52,952 --> 00:02:56,258
MIT license, but the source code is all available

36
00:02:56,344 --> 00:02:59,826
there on GitHub, so feel free to also check that out if you're

37
00:02:59,858 --> 00:03:03,186
interested in things I'm talking about today. And Fleet

38
00:03:03,218 --> 00:03:06,886
is primarily a Linux server, and then it has a

39
00:03:06,908 --> 00:03:10,300
CLI tool that's cross platform and used

40
00:03:10,830 --> 00:03:14,650
typically on macOS or Linux, but also sometimes on Windows.

41
00:03:15,390 --> 00:03:19,066
And I think it's useful to understand that when we're thinking about fleet as a

42
00:03:19,088 --> 00:03:22,570
server. Fleets has two different major clients.

43
00:03:23,070 --> 00:03:26,574
One is the OS create agent client. These are

44
00:03:26,612 --> 00:03:30,206
agents checking in, looking for the configurations that they should

45
00:03:30,228 --> 00:03:32,746
run, sending data that they've collected,

46
00:03:32,938 --> 00:03:36,542
and updating their status with the fleets server.

47
00:03:36,686 --> 00:03:40,740
This is where we're talking about hundreds of thousands of devices checking in.

48
00:03:41,110 --> 00:03:44,914
And then we have the API clients, which tend to be a human clicking around

49
00:03:44,952 --> 00:03:48,314
in the UI, or maybe a script that's

50
00:03:48,462 --> 00:03:52,374
modifying the configurations that are sent to the agents or looking

51
00:03:52,412 --> 00:03:56,118
at the data that's been collect. And I want to highlight that there's kind

52
00:03:56,124 --> 00:03:59,090
of an order of magnitude difference in the traffic,

53
00:03:59,250 --> 00:04:02,854
or perhaps multiple orders of magnitude difference in the traffic

54
00:04:02,902 --> 00:04:06,794
generated by each of these clients. So we're going to treat them very

55
00:04:06,832 --> 00:04:11,022
differently as we look at the reliability characteristics of our

56
00:04:11,076 --> 00:04:14,910
system. How do we engineer this whole

57
00:04:15,060 --> 00:04:18,778
agent server system for resilience?

58
00:04:18,954 --> 00:04:22,720
Well, I think it's really important to focus

59
00:04:23,110 --> 00:04:26,962
for doing this. We need to identify our key

60
00:04:27,016 --> 00:04:30,338
areas of risk, decide which of those we want

61
00:04:30,344 --> 00:04:33,954
to prioritize, and then apply mitigations that are

62
00:04:33,992 --> 00:04:37,622
focused on those identified areas. And so when

63
00:04:37,676 --> 00:04:41,078
looking at our system that I just described, this fleet and

64
00:04:41,084 --> 00:04:44,134
os query server agent system,

65
00:04:44,332 --> 00:04:47,934
I see the following priorities.

66
00:04:48,082 --> 00:04:52,038
I think availability of the production workloads

67
00:04:52,214 --> 00:04:56,874
almost always has to be the top priority of our

68
00:04:56,912 --> 00:05:00,382
system. So we would need to be really careful that

69
00:05:00,436 --> 00:05:04,302
the production workloads in our environments, the workstations that

70
00:05:04,356 --> 00:05:09,146
folks are using to get their jobs done, are not impacted negatively

71
00:05:09,258 --> 00:05:13,026
by the agent. Our next priority is the integrity of the

72
00:05:13,048 --> 00:05:16,574
data that we collect, because our security programs

73
00:05:16,622 --> 00:05:20,094
and our operations teams rely on the integrity

74
00:05:20,142 --> 00:05:24,290
of this data in order to do their jobs effectively.

75
00:05:25,050 --> 00:05:28,502
And then as a third

76
00:05:28,556 --> 00:05:32,694
priority, I'll put cost. And of course cost is very important and

77
00:05:32,732 --> 00:05:36,786
it's going to depend on the organization and the particular scenarios.

78
00:05:36,898 --> 00:05:40,726
But typically it's the availability and the integrity

79
00:05:40,838 --> 00:05:44,106
that are going to be higher priorities, where cost is only

80
00:05:44,128 --> 00:05:48,010
going to vary by some more limited amount. And when thinking

81
00:05:48,080 --> 00:05:51,614
about all of these things and

82
00:05:51,732 --> 00:05:55,086
the focus that we want to take on risk, I think

83
00:05:55,108 --> 00:05:58,622
it's useful to look at the paredo principle or

84
00:05:58,676 --> 00:06:02,014
Omdahl's law. This idea of a small

85
00:06:02,132 --> 00:06:05,218
production of the clients are going

86
00:06:05,224 --> 00:06:08,286
to generate an outsized amount of the traffic,

87
00:06:08,398 --> 00:06:12,398
or a small part of the system is going to generate

88
00:06:12,494 --> 00:06:16,102
an outsized amount of the load. And if we want

89
00:06:16,156 --> 00:06:19,174
to optimize for our system,

90
00:06:19,292 --> 00:06:22,486
for whether it's reliability or performance or

91
00:06:22,508 --> 00:06:25,734
some other metric like that, we need to be

92
00:06:25,772 --> 00:06:30,986
looking at the places that we can have the highest impact and

93
00:06:31,008 --> 00:06:34,394
that are the greatest contributors in order to be

94
00:06:34,432 --> 00:06:37,546
the most efficient with our efforts. So looking at

95
00:06:37,568 --> 00:06:41,370
this a little bit more concretely, with our agent OS query

96
00:06:41,450 --> 00:06:45,070
and our server fleet, starting with OS query, I think

97
00:06:45,140 --> 00:06:48,366
one of our top risks is that

98
00:06:48,388 --> 00:06:52,026
OS query runs often on production servers.

99
00:06:52,218 --> 00:06:55,794
These are servers that are doing things that actually make the company

100
00:06:55,992 --> 00:06:59,570
money or serve the main purpose of the organization.

101
00:07:00,230 --> 00:07:03,922
So these production workloads and their availability is

102
00:07:03,976 --> 00:07:07,282
extremely important. A next top priority

103
00:07:07,426 --> 00:07:10,902
is the usability of workstations. We have

104
00:07:10,956 --> 00:07:14,502
people who we pay and who are dedicated to getting

105
00:07:14,556 --> 00:07:17,634
their work done, and they need to have usable workstations.

106
00:07:17,682 --> 00:07:21,306
So we can't have this agents taking down their workstations, even if this

107
00:07:21,328 --> 00:07:24,634
is perhaps just a slight bit less critical than those

108
00:07:24,672 --> 00:07:28,470
production workloads. And then of course, we've got the monitoring integrity.

109
00:07:28,630 --> 00:07:32,234
We don't want to miss out on critical security events,

110
00:07:32,362 --> 00:07:36,570
we don't want to miss out on operational

111
00:07:36,650 --> 00:07:39,854
events. And of course, as mentioned before,

112
00:07:40,052 --> 00:07:43,374
the cost of compute can actually really add up

113
00:07:43,412 --> 00:07:46,546
here, because when we're talking about hundreds of thousands of hosts that we're running an

114
00:07:46,568 --> 00:07:51,054
agent on, a small percentage increase in the resources

115
00:07:51,102 --> 00:07:55,106
consumed by OS query can lead CTO a pretty

116
00:07:55,288 --> 00:07:58,806
huge amount of extra compute cost. So when I look

117
00:07:58,828 --> 00:08:02,470
at all of these factors together and I think about

118
00:08:02,540 --> 00:08:06,854
our agent server system, I think the agent is

119
00:08:06,892 --> 00:08:10,570
pretty high risk. This is going to be a big area of focus

120
00:08:10,720 --> 00:08:14,106
for mitigating risk. Then if we look at

121
00:08:14,128 --> 00:08:17,674
fleets, the server, well, the monitoring availability of

122
00:08:17,712 --> 00:08:21,630
fleet is quite important. Again, this is because

123
00:08:21,700 --> 00:08:25,214
of those security teams and those

124
00:08:25,252 --> 00:08:29,114
questions, teams that are relying on being able to get that osquery

125
00:08:29,162 --> 00:08:31,626
data in a timely manner.

126
00:08:31,818 --> 00:08:34,898
Speaking of timely manager, the latency is important,

127
00:08:34,984 --> 00:08:39,426
but the latency is perhaps a bit less

128
00:08:39,608 --> 00:08:43,074
important than just getting the data at some

129
00:08:43,112 --> 00:08:47,270
point. And so that's a monitoring integrity. We know that

130
00:08:47,420 --> 00:08:50,898
we need to get those critical security events,

131
00:08:50,994 --> 00:08:55,026
otherwise we can be missing out on threats within our infrastructure.

132
00:08:55,218 --> 00:08:58,474
And then again, the compute cost can be a factor here.

133
00:08:58,592 --> 00:09:02,278
But I'd say this is typically a much lower consideration

134
00:09:02,454 --> 00:09:04,970
for fleet deployed,

135
00:09:05,390 --> 00:09:10,026
as we're talking about just a handful of servers to

136
00:09:10,208 --> 00:09:14,122
perhaps a dozen or a couple of dozen servers,

137
00:09:14,186 --> 00:09:17,482
plus some MySQL and redis infrastructure dependencies.

138
00:09:17,626 --> 00:09:21,038
So even if we have to do a pretty massive scale up here,

139
00:09:21,124 --> 00:09:25,186
this compute cost is not going to be

140
00:09:25,288 --> 00:09:29,138
very significant compared to even a few percentage endpoints

141
00:09:29,224 --> 00:09:32,222
in an increase in the production compute costs.

142
00:09:32,366 --> 00:09:36,306
So when I look at all of this together, I think this feels

143
00:09:36,338 --> 00:09:40,418
like medium risk. So I'll talk now about some of the mechanisms

144
00:09:40,434 --> 00:09:43,910
that we have in OS query for mitigating these risks and

145
00:09:43,980 --> 00:09:47,854
for preventing the proverbial

146
00:09:47,922 --> 00:09:51,622
chaos. So osquery has got something called the watchdog,

147
00:09:51,686 --> 00:09:56,138
which is a multiple process worker watcher model.

148
00:09:56,304 --> 00:09:59,542
So one process watches

149
00:09:59,606 --> 00:10:03,166
the other OS query process, and checking for how

150
00:10:03,188 --> 00:10:06,462
much cpu is being used, how much memory is being used.

151
00:10:06,596 --> 00:10:09,770
And if those limits are exceeded,

152
00:10:09,930 --> 00:10:13,258
then the work that the agent was doing

153
00:10:13,284 --> 00:10:16,050
at that time is blocked for 24 hours.

154
00:10:16,200 --> 00:10:19,394
So if we find ourselves in a situation

155
00:10:19,592 --> 00:10:23,186
where unexpected high levels of load are

156
00:10:23,208 --> 00:10:26,706
occurring, we can actually sort of self recover by saying that's

157
00:10:26,738 --> 00:10:30,486
not work that we're going CTo do in the next 24 hours.

158
00:10:30,668 --> 00:10:34,582
And this strategy helps to mitigate those

159
00:10:34,636 --> 00:10:38,458
production availability and workstation availability or

160
00:10:38,544 --> 00:10:42,278
usability issues, meaning we prioritize the primary

161
00:10:42,374 --> 00:10:45,690
computing tasks of these systems.

162
00:10:46,030 --> 00:10:49,386
Of course it's also worth looking at the trade offs and the downside of

163
00:10:49,408 --> 00:10:53,194
this one is that blocking those queries

164
00:10:53,322 --> 00:10:56,858
does potentially reduce the integrity of the monitoring

165
00:10:56,954 --> 00:11:00,334
that we'd like to achieve. Another mechanism that's really

166
00:11:00,372 --> 00:11:03,710
common to use with osquery, and this is actually one that's available

167
00:11:03,780 --> 00:11:07,262
for lots of services because it's not specific to OS query,

168
00:11:07,406 --> 00:11:11,058
is c groups. So of course we can ask

169
00:11:11,144 --> 00:11:14,578
the Linux kernel to maintain strict limits on the

170
00:11:14,584 --> 00:11:17,362
amount of cpu and memory utilized.

171
00:11:17,506 --> 00:11:21,170
And this is something that is commonly used in combination

172
00:11:21,330 --> 00:11:24,806
with the OS query watchdog. And we see this as

173
00:11:24,828 --> 00:11:28,426
a way to both have something kind of application specific

174
00:11:28,528 --> 00:11:32,326
that understands the concept of queries and knows

175
00:11:32,358 --> 00:11:35,946
how to block them. That's the watchdog. Whereas C groups are kind

176
00:11:35,968 --> 00:11:39,942
of a final backstop helping us be careful about

177
00:11:40,016 --> 00:11:42,560
how much of the resources on a system we use.

178
00:11:43,010 --> 00:11:45,950
And this really just mitigates the production availability.

179
00:11:46,770 --> 00:11:50,574
Although acknowledging that some users do have

180
00:11:50,692 --> 00:11:54,100
Linux workstations, mostly we're talking about productions here.

181
00:11:54,550 --> 00:11:58,610
And so that downside is that this is only compatible with Linux.

182
00:11:58,950 --> 00:12:02,354
We also think it's important to

183
00:12:02,552 --> 00:12:07,090
do profiling and estimate the performance characteristics

184
00:12:07,170 --> 00:12:10,438
of the queries that are deployed before

185
00:12:10,524 --> 00:12:15,000
doing so. And OS query has some application specific

186
00:12:16,490 --> 00:12:20,122
profiling tooling that's available. And we can see over here

187
00:12:20,176 --> 00:12:24,058
on the right, an example of given

188
00:12:24,144 --> 00:12:28,380
some input queries, OS query generates some

189
00:12:29,150 --> 00:12:32,238
sort of benchmarks for how much user time,

190
00:12:32,324 --> 00:12:35,898
system time, memory file accesses,

191
00:12:36,074 --> 00:12:40,174
and those sort of metrics are used when executing these

192
00:12:40,212 --> 00:12:44,014
queries. And that can give us a preview of how expensive this query

193
00:12:44,062 --> 00:12:48,130
is and allow us to gain some predictability

194
00:12:48,630 --> 00:12:51,694
before we go out utilizing those resources.

195
00:12:51,822 --> 00:12:55,794
And so then this is a mitigation for the monitoring integrity because

196
00:12:55,832 --> 00:12:59,426
we can be sure of which queries will and won't set off that watchdog.

197
00:12:59,538 --> 00:13:03,046
And also we can understand in advance what

198
00:13:03,068 --> 00:13:06,466
is the actual cost in terms of compute of deploying

199
00:13:06,498 --> 00:13:10,006
this kind of thing. Then there's monitoring

200
00:13:10,118 --> 00:13:13,290
that we can get in osquery as well.

201
00:13:13,360 --> 00:13:16,874
And so this is something that fleet does in combination with

202
00:13:16,912 --> 00:13:20,762
OS query is recording statistics for

203
00:13:20,816 --> 00:13:24,282
the actual execution of the queries across all of the hosts

204
00:13:24,426 --> 00:13:28,190
and then I've just provided an example

205
00:13:28,260 --> 00:13:31,786
of here of how we render this kind of information within the fleet

206
00:13:31,818 --> 00:13:35,518
UI. Just trying to give sort of a subjective assessment.

207
00:13:35,614 --> 00:13:39,794
And this allows an operator to get

208
00:13:39,832 --> 00:13:43,474
an idea of does the performance of this

209
00:13:43,512 --> 00:13:46,774
query match what I expected in the real world?

210
00:13:46,812 --> 00:13:50,390
And is this something that's worth me continuing to pursue?

211
00:13:50,730 --> 00:13:54,150
This again mitigates the monitoring integrity and

212
00:13:54,220 --> 00:13:57,926
the compute cost issues. Then over on the fleets

213
00:13:57,958 --> 00:14:01,900
server side, of course, it's worth mentioning that we use

214
00:14:02,270 --> 00:14:06,742
common server scalability practices. So multiple fleet server processes

215
00:14:06,886 --> 00:14:10,286
run behind a load balancer. The MySQL and

216
00:14:10,308 --> 00:14:14,302
redis dependencies are clustered with ready

217
00:14:14,356 --> 00:14:17,902
to failover. And this is often, we'll often

218
00:14:17,956 --> 00:14:20,880
deploy this across multiple regions or that sort of thing.

219
00:14:21,410 --> 00:14:24,814
We'll utilize auto scaling for efficient infrastructure

220
00:14:24,862 --> 00:14:28,146
sizing when this is deployed within the cloud. So the

221
00:14:28,168 --> 00:14:32,370
AWS or GCP kind of auto scaling and

222
00:14:32,440 --> 00:14:36,178
these common practices help to mitigate

223
00:14:36,274 --> 00:14:40,454
the monitoring availability, the latency integrity and

224
00:14:40,492 --> 00:14:43,634
the compute costs. But I think the downside

225
00:14:43,682 --> 00:14:47,426
to be aware of in particular with some dependencies that are

226
00:14:47,468 --> 00:14:50,906
less elastically scalable, like MySQL or

227
00:14:50,928 --> 00:14:54,314
redis, these kind of fixed infrastructure dependencies is that if

228
00:14:54,352 --> 00:14:58,202
these aren't properly sized, then you can really overestimate the

229
00:14:58,336 --> 00:15:02,174
compute costs. Fleets also relies on

230
00:15:02,372 --> 00:15:05,486
back pressure strategy that's supported by

231
00:15:05,508 --> 00:15:08,942
the OS query agent. By the agents will

232
00:15:08,996 --> 00:15:12,798
actually buffer data on each individual

233
00:15:12,964 --> 00:15:16,782
endpoint, attempt CTO, write that data to the server

234
00:15:16,926 --> 00:15:20,562
and it will not clear from the buffer on the local endpoint until

235
00:15:20,616 --> 00:15:24,626
the server has acknowledged successful receipt of the

236
00:15:24,648 --> 00:15:27,974
messages. I think this is a really great approach to

237
00:15:28,012 --> 00:15:31,654
use because we have this scenario where the

238
00:15:31,692 --> 00:15:34,978
servers are of course vastly outnumbered by the clients.

239
00:15:35,074 --> 00:15:38,474
If we can push a very small amount of storage work

240
00:15:38,592 --> 00:15:42,220
off to the clients, we can actually really benefit from

241
00:15:42,590 --> 00:15:46,538
the availability characteristics that this

242
00:15:46,624 --> 00:15:50,178
provides. And so this really mitigates

243
00:15:50,374 --> 00:15:54,574
the monitoring integrity problem because

244
00:15:54,692 --> 00:15:57,754
we know that except in extreme cases,

245
00:15:57,882 --> 00:16:01,390
no matter what happens, that data will eventually make it

246
00:16:01,460 --> 00:16:05,540
to the server and be processed properly. Of course,

247
00:16:05,910 --> 00:16:09,266
under bad scenarios, this can increase latency and like

248
00:16:09,288 --> 00:16:13,214
I said, the integrity can be compromised in extreme

249
00:16:13,262 --> 00:16:16,758
cases when these internal buffers overflow. And of

250
00:16:16,764 --> 00:16:20,806
course that's yet another level of consideration that

251
00:16:20,828 --> 00:16:24,978
we have to take into account, is that we don't want to end up overwhelming

252
00:16:25,154 --> 00:16:28,502
the agents by buffering too much data there.

253
00:16:28,556 --> 00:16:31,882
So that old data is dropped. So back to this idea of

254
00:16:31,936 --> 00:16:35,594
engineering resilience. What are the other challenges that we have

255
00:16:35,632 --> 00:16:39,654
faced and how have we approached them? So a really big challenge

256
00:16:39,702 --> 00:16:43,358
for us at fleet is this self

257
00:16:43,444 --> 00:16:46,794
managed infrastructure and self managed deployment

258
00:16:46,842 --> 00:16:51,386
of the fleets server. So all of the fleet software,

259
00:16:51,578 --> 00:16:55,614
both the agents that are running on the individual endpoints

260
00:16:55,742 --> 00:16:59,966
and the servers, the fleet servers plus their infrastructure

261
00:16:59,998 --> 00:17:03,614
dependencies, they're all in the customer environment.

262
00:17:03,662 --> 00:17:06,550
So we don't have control over those environments.

263
00:17:07,050 --> 00:17:09,990
Those environments can end up being very inconsistent.

264
00:17:10,330 --> 00:17:13,606
Deploys can be slow and are completely out of

265
00:17:13,628 --> 00:17:16,994
our control. So we might have outdated versions of the software

266
00:17:17,042 --> 00:17:20,458
running. And when we get into debugging scenarios, there's a

267
00:17:20,464 --> 00:17:24,026
very slow feedback loop. So these

268
00:17:24,048 --> 00:17:27,434
are some challenges that are introduced by that, and I'll talk about some

269
00:17:27,472 --> 00:17:31,226
strategies that we use to manage that. First, I'll note

270
00:17:31,258 --> 00:17:33,950
that it's really important, as much as possible,

271
00:17:34,020 --> 00:17:36,810
to strive for consistency.

272
00:17:36,970 --> 00:17:40,350
The more heterogeneous, the more different the deployments are,

273
00:17:40,420 --> 00:17:44,286
the more edge cases that users and operators

274
00:17:44,318 --> 00:17:49,358
will run into. And for us, this looks like things like MySQL

275
00:17:49,534 --> 00:17:52,738
could be MySQL could be MySQL five, seven,

276
00:17:52,824 --> 00:17:56,882
MySQL eight could also be MariaDB

277
00:17:57,026 --> 00:18:00,946
or Aurora on AWS.

278
00:18:01,138 --> 00:18:04,706
Redis can be running in cluster mode and redis sentinel

279
00:18:04,738 --> 00:18:08,454
mode, or just in single host mode. And so there's

280
00:18:08,502 --> 00:18:12,646
all of these different permutations of infrastructures

281
00:18:12,758 --> 00:18:16,186
that could be encountered. And these are all things that we sort

282
00:18:16,208 --> 00:18:19,890
of account for as chaotic factors

283
00:18:20,070 --> 00:18:24,880
in our engineering work. So we

284
00:18:25,330 --> 00:18:28,462
still want to strive for consistency as much as possible.

285
00:18:28,516 --> 00:18:31,102
And we've achieved that a couple of ways.

286
00:18:31,236 --> 00:18:34,722
One, by using infrastructure as code and

287
00:18:34,776 --> 00:18:38,722
trying to push folks to deploy basically

288
00:18:38,856 --> 00:18:41,918
a reference architecture of fleet.

289
00:18:42,094 --> 00:18:45,554
And just as an example, this is kind of how we specify the reference

290
00:18:45,602 --> 00:18:49,394
architectures down to a very specific version

291
00:18:49,442 --> 00:18:52,982
number of mysql, and the exact number of

292
00:18:53,036 --> 00:18:56,374
tasks, and the memory that each task should

293
00:18:56,412 --> 00:19:00,026
have for the compute. And we found that the more that we can get

294
00:19:00,128 --> 00:19:03,430
folks to adhere to these recommendations,

295
00:19:03,510 --> 00:19:07,706
the easier it is for us to work with them, anticipate the

296
00:19:07,728 --> 00:19:11,034
problems that they'll have, and reproduce problems that are

297
00:19:11,072 --> 00:19:14,462
encountered in the wild. Testing is, of course, also really

298
00:19:14,516 --> 00:19:18,234
important. And I think the biggest takeaway for us on the chaos

299
00:19:18,282 --> 00:19:21,726
front is that automation is really important.

300
00:19:21,908 --> 00:19:25,078
So we've identified a metric of how many experiments

301
00:19:25,114 --> 00:19:28,434
can we run per week, because we see this

302
00:19:28,472 --> 00:19:31,842
idea that the more experiments that we run, the more different

303
00:19:31,896 --> 00:19:35,922
combinations of things we can try, things going wrong, things going right,

304
00:19:36,056 --> 00:19:39,622
the more edge cases we'll encounter, and then the more issues

305
00:19:39,676 --> 00:19:43,254
that we'll detect before they go into production or before

306
00:19:43,292 --> 00:19:46,742
they are encountered in production. And this is also a place

307
00:19:46,796 --> 00:19:50,300
where the infrastructure, as code really comes into play.

308
00:19:50,830 --> 00:19:54,506
If we can spin up and down environments easily and we

309
00:19:54,528 --> 00:19:58,298
can modify their parameters easily, then this will help us

310
00:19:58,464 --> 00:20:01,854
increase this metric of experiments per week.

311
00:20:02,052 --> 00:20:05,102
In terms of testing, the tooling is also really

312
00:20:05,156 --> 00:20:08,874
important. So generic HTTP testing

313
00:20:08,922 --> 00:20:12,538
tools probably won't hit your edge cases,

314
00:20:12,634 --> 00:20:16,146
and they definitely don't know what the hot paths are for you

315
00:20:16,168 --> 00:20:19,346
in production, but you can probably identify those or at

316
00:20:19,368 --> 00:20:22,994
least make some guesses. And in our experience,

317
00:20:23,112 --> 00:20:26,546
it's really useful to build custom tooling.

318
00:20:26,738 --> 00:20:30,294
And so for us at fleet, thinking back to where

319
00:20:30,332 --> 00:20:33,858
we want to focus our efforts in terms of optimizing

320
00:20:33,954 --> 00:20:37,922
and mitigating risk, it's this agent server coordination.

321
00:20:38,066 --> 00:20:41,590
And so for the fleets server, we've created

322
00:20:41,670 --> 00:20:45,018
simulations of the OS query agent so that we can start up a

323
00:20:45,024 --> 00:20:48,454
lot of agents, users them in different scenarios

324
00:20:48,582 --> 00:20:51,694
and test how the server responds to these things,

325
00:20:51,892 --> 00:20:55,246
so that we've identified that hot path, that's the

326
00:20:55,268 --> 00:20:58,400
agent check ins and the processing of the received data.

327
00:20:58,930 --> 00:21:01,710
We can simulate these agents efficiently,

328
00:21:02,210 --> 00:21:05,422
meaning tens or hundreds of thousands

329
00:21:05,486 --> 00:21:09,538
of simulated agents running on a single actual

330
00:21:09,704 --> 00:21:13,026
virtual machine. And this is something that can

331
00:21:13,048 --> 00:21:16,886
be done pretty easily with amazing support

332
00:21:16,988 --> 00:21:20,498
for high concurrency HTTP

333
00:21:20,674 --> 00:21:24,230
when using go, the go programming language.

334
00:21:24,730 --> 00:21:29,346
Just for an example, here's a template

335
00:21:29,378 --> 00:21:32,774
that we use to kind of simulate the data that's returned

336
00:21:32,822 --> 00:21:36,534
from a host. And you can see that there are some templated

337
00:21:36,582 --> 00:21:40,278
parameters in here where it says cache, bring host name and

338
00:21:40,464 --> 00:21:43,310
uuid within the curly braces.

339
00:21:44,210 --> 00:21:47,870
These tests are opportunities to inject

340
00:21:48,210 --> 00:21:51,358
more randomness and see how the system

341
00:21:51,444 --> 00:21:54,794
responds. So what happens when we run a bunch

342
00:21:54,842 --> 00:21:57,986
of simulated agents that look very similar when we

343
00:21:58,008 --> 00:22:01,106
run a bunch that look very different when they all start up at the

344
00:22:01,128 --> 00:22:04,990
same time, or if they're all really spread out? These are all different factors

345
00:22:05,150 --> 00:22:08,754
in how the system will behave. What happens if our network

346
00:22:08,802 --> 00:22:13,202
access becomes compromised? If we can run these kind of simulations

347
00:22:13,346 --> 00:22:16,774
on a relatively cheap infrastructure, and we can have good control over

348
00:22:16,812 --> 00:22:20,410
those factors, then we hit more of those edge cases.

349
00:22:20,750 --> 00:22:24,380
And then debugging plays a really important,

350
00:22:24,750 --> 00:22:28,726
I call it dual role here between staging or load testing

351
00:22:28,758 --> 00:22:32,586
and production. Debugging helps. Having good debugging

352
00:22:32,618 --> 00:22:36,014
tooling helps us detect issues before we

353
00:22:36,052 --> 00:22:39,326
ship them or before they become actual incidents. And it

354
00:22:39,348 --> 00:22:43,154
also helps us CTO resolve incidents more quickly

355
00:22:43,352 --> 00:22:46,082
approach that we found CTO be useful for.

356
00:22:46,136 --> 00:22:49,842
This is what I like to call collect first,

357
00:22:49,976 --> 00:22:53,826
ask questions later. And this is something that we

358
00:22:53,848 --> 00:22:57,074
achieve with our fleet control debug archive

359
00:22:57,122 --> 00:23:00,326
command. So I'll just show an example of

360
00:23:00,348 --> 00:23:04,438
the execution of this command. With one command, we run

361
00:23:04,524 --> 00:23:08,914
all of these different profiles and debugging

362
00:23:08,962 --> 00:23:12,386
steps, finding out the allocations of memory,

363
00:23:12,578 --> 00:23:15,400
the errors that have been taking place,

364
00:23:15,850 --> 00:23:19,298
getting information about the concurrency profiling

365
00:23:19,314 --> 00:23:23,342
the cpu do, getting information

366
00:23:23,476 --> 00:23:27,118
from the database, including the

367
00:23:27,284 --> 00:23:30,426
status of the underlying database, the locking that's

368
00:23:30,458 --> 00:23:34,094
happening there. And then by doing this, when we simulate

369
00:23:34,142 --> 00:23:38,462
a scenario that we find causes undesirable

370
00:23:38,526 --> 00:23:42,258
behavior, we can easily get a whole snapshot of these things

371
00:23:42,344 --> 00:23:45,646
and then take some time later to analyze

372
00:23:45,678 --> 00:23:49,814
the data that was collected. On the other hand, we talked earlier about the

373
00:23:49,852 --> 00:23:53,366
difficulty of the feedback loops when debugging in

374
00:23:53,388 --> 00:23:57,350
a self managed environment. And this is also something

375
00:23:57,500 --> 00:24:00,890
that really mitigates that when we have a single

376
00:24:00,960 --> 00:24:04,234
command that we can ask our customers

377
00:24:04,352 --> 00:24:07,766
and our open source users to run, and that gives us a whole bunch

378
00:24:07,798 --> 00:24:11,514
of data that we can look at to minimize the round trips

379
00:24:11,562 --> 00:24:15,054
there. So, to kind of sum up what we've discussed here,

380
00:24:15,172 --> 00:24:19,182
I think it's really important to focus on where

381
00:24:19,236 --> 00:24:21,934
your highest impact work is going to be,

382
00:24:22,132 --> 00:24:25,794
get an intuitive sense of where your systems will break,

383
00:24:25,912 --> 00:24:29,810
what the highest risks are, and how you can

384
00:24:29,880 --> 00:24:33,394
exercise your systems and your code

385
00:24:33,592 --> 00:24:36,806
to trigger those things, understand them,

386
00:24:36,828 --> 00:24:40,550
and then have mitigations. Consistency is

387
00:24:40,620 --> 00:24:44,354
super important in infrastructure, and it's also challenged

388
00:24:44,402 --> 00:24:47,800
if you're in a self managed environment like we are.

389
00:24:48,350 --> 00:24:51,946
But I encourage using whatever tools are at your

390
00:24:51,968 --> 00:24:55,420
disposal, in particular infrastructure, as code,

391
00:24:55,870 --> 00:25:02,046
to try to achieve consistency and

392
00:25:02,068 --> 00:25:05,534
then testing automation, really important, because this will help

393
00:25:05,572 --> 00:25:09,082
you up that experiment per week metric,

394
00:25:09,146 --> 00:25:12,234
or whatever the similar metric is in your environment.

395
00:25:12,362 --> 00:25:15,886
The more tests you can run, and the more easily you can run those tests,

396
00:25:15,998 --> 00:25:19,582
the more you'll discover these strange edge cases,

397
00:25:19,646 --> 00:25:23,330
and the more you'll anticipate the problems before you run into them in production.

398
00:25:23,750 --> 00:25:27,750
And then debug tooling again is really important.

399
00:25:27,900 --> 00:25:32,070
This is something that helps in those dual roles of both during staging

400
00:25:32,410 --> 00:25:34,950
and testing and in production.

401
00:25:35,930 --> 00:25:39,446
So hopefully that was helpful. Today,

402
00:25:39,548 --> 00:25:43,222
CTO, understand about the work that we've been doing

403
00:25:43,276 --> 00:25:47,634
to reduce risk and improve performance at fleets.

404
00:25:47,762 --> 00:25:51,166
And thank you for listening to

405
00:25:51,188 --> 00:25:51,500
my talk.

