1
00:00:40,930 --> 00:00:44,434
Hello, everybody, and welcome to save the world with MongoDB

2
00:00:44,482 --> 00:00:47,826
data Lakes. My name is Joe Carlson.

3
00:00:47,858 --> 00:00:51,386
I'm a developer advocate for MongoDB, and I'm just glad

4
00:00:51,418 --> 00:00:55,054
you're here. Let's jump in. So, first of all, why should you care

5
00:00:55,092 --> 00:00:58,878
about this? Well, the goal of this talk is to a,

6
00:00:58,964 --> 00:01:02,506
save the world, great. And b, save some money,

7
00:01:02,628 --> 00:01:05,842
also great. And why is this a problem?

8
00:01:05,976 --> 00:01:09,682
Well, I'm sure you know that data needs

9
00:01:09,816 --> 00:01:14,110
online on every service is just growing

10
00:01:14,190 --> 00:01:17,442
and growing and growing. We're adding more and more features,

11
00:01:17,506 --> 00:01:21,270
we're saving more and more things, and we need to get access

12
00:01:21,340 --> 00:01:24,614
to them and scale these data needs faster than

13
00:01:24,652 --> 00:01:28,422
ever before. Right. Every application, we're just saving more and more

14
00:01:28,476 --> 00:01:31,766
data. Over time, as we're getting more and more users

15
00:01:31,798 --> 00:01:35,606
and more and more users are collecting more and more data, the data demands

16
00:01:35,638 --> 00:01:39,530
on our system just get bigger and bigger and bigger.

17
00:01:40,050 --> 00:01:43,610
Right. This even gets exacerbated further

18
00:01:43,770 --> 00:01:48,458
when we look data lakes scaling strategies. So, for example, with MongoDB,

19
00:01:48,554 --> 00:01:51,934
you can shard your databases horizontally so that you

20
00:01:51,972 --> 00:01:55,166
can easily scale these up. But if you're sharding these,

21
00:01:55,268 --> 00:01:59,294
you're also going to have to be supporting these databases with replicated

22
00:01:59,342 --> 00:02:02,962
servers, usually two of them, in case anything goes wrong with them.

23
00:02:03,016 --> 00:02:06,146
You have two backups on hand that you can recover up to if you need

24
00:02:06,168 --> 00:02:09,830
to. But these, of course, grow and grow and grow and grow too.

25
00:02:09,900 --> 00:02:14,098
It quickly become a big issue for our applications.

26
00:02:14,194 --> 00:02:17,270
Oh my gosh, right? Which I don't know if you know this, right?

27
00:02:17,340 --> 00:02:21,686
These data demands on our system require power and money

28
00:02:21,788 --> 00:02:24,842
to keep the lights on to keep those. Coding and data is the most important

29
00:02:24,896 --> 00:02:27,578
part of our application. We don't want to lose any of that, but we want

30
00:02:27,584 --> 00:02:31,034
to try to make sure that we're building and scaling these applications as

31
00:02:31,072 --> 00:02:34,646
quickly, as responsibly for the environment and as cheaply

32
00:02:34,678 --> 00:02:37,790
as possible. If we can get both those at the same time, that's a win.

33
00:02:37,860 --> 00:02:41,358
That's a win for us, right? So let's explore how to do that

34
00:02:41,444 --> 00:02:45,134
with data lakes. Now, before we do that, though, I just want to say

35
00:02:45,252 --> 00:02:49,106
hello. My name is Joe Karlsson. I'm a software engineer and

36
00:02:49,128 --> 00:02:52,190
a developer advocate, and I work for a company called MongoDB.

37
00:02:52,270 --> 00:02:54,622
Best way to get a hold of me is on Twitter. Otherwise, I'm on Twitch.

38
00:02:54,686 --> 00:02:58,022
Mongodb. Twitch too, if you want to check that out. I'm also making funny videos

39
00:02:58,076 --> 00:03:01,126
on TikTok and all my information is on my

40
00:03:01,148 --> 00:03:04,962
website@jocarlson.com. All of the resources, slides,

41
00:03:05,026 --> 00:03:08,886
and a recording of this video that you're watching right now can be found on

42
00:03:08,908 --> 00:03:11,720
my website. Excuse me, if you go to Jocarlson dev,

43
00:03:12,170 --> 00:03:15,122
artofmdb data lakes,

44
00:03:15,196 --> 00:03:18,346
or if you scan that QR code in the upper right hand corner at any

45
00:03:18,368 --> 00:03:21,178
time, that will take you there as well. Lastly, I just want to say,

46
00:03:21,264 --> 00:03:25,114
any opinions in this piece are of my own and don't reflect the opinions

47
00:03:25,162 --> 00:03:28,334
of my employer. All right, just saying. Don't fire me. I love my job.

48
00:03:28,452 --> 00:03:31,946
Okay, so we have one planet we need to protect. As data centers

49
00:03:31,978 --> 00:03:35,954
grow, they're using more power, more electricity, and this is

50
00:03:36,072 --> 00:03:39,342
very expensive for us and makes babies sad.

51
00:03:39,406 --> 00:03:42,494
Do you want to make babies sad? I don't want to make babies sad.

52
00:03:42,622 --> 00:03:45,726
Let's protect babies, please. Okay. We want to be leveraging

53
00:03:45,758 --> 00:03:48,914
something called a data lakes. But before we get to that, I want to discuss

54
00:03:49,112 --> 00:03:52,294
types of data that make a good use case for being saved in a data

55
00:03:52,332 --> 00:03:55,750
lakes. Hot data and cold data. And I'm coding to be using an example

56
00:03:55,820 --> 00:03:59,122
here today of IoT data. And with IoT

57
00:03:59,186 --> 00:04:02,458
data, typically we're saving in time series data to be displayed on

58
00:04:02,464 --> 00:04:05,994
a dashboard for whatever we want to understand trends over

59
00:04:06,032 --> 00:04:09,546
time with whatever our IoT data is collecting. But one of the downsides of

60
00:04:09,568 --> 00:04:12,698
IoT data is that it gets out of date or out

61
00:04:12,704 --> 00:04:15,934
of stale, and we don't care about as much. We still want that data for

62
00:04:15,972 --> 00:04:19,342
historical analysis or whatever, but that data is no longer

63
00:04:19,396 --> 00:04:22,794
hot. It's not being actively used a lot. Right? Just like think of your tweets

64
00:04:22,842 --> 00:04:26,254
from four years ago. It's good to still have those on hand, but you're probably

65
00:04:26,292 --> 00:04:30,094
not accessing those tweets very often. And we might want to do something differently

66
00:04:30,142 --> 00:04:33,266
with that data so that it gets saved, and it's cheaper for us to save

67
00:04:33,288 --> 00:04:37,074
that in the long run. So hot data. Hot data is data that is

68
00:04:37,112 --> 00:04:40,374
saved closer to the cpu. And you know this too. Like, if you've ever built

69
00:04:40,412 --> 00:04:44,406
your own pc, memory usage that is closer to the cpu is

70
00:04:44,428 --> 00:04:48,610
more expensive, right? We all know that motherboard is way more expensive than ram,

71
00:04:48,690 --> 00:04:52,054
and ram is way more expensive than hard drives.

72
00:04:52,182 --> 00:04:55,738
And if we can offload some of this data, some of this hot data that

73
00:04:55,744 --> 00:04:59,590
we don't need anymore, to cheaper forms of data storage,

74
00:04:59,670 --> 00:05:03,094
that's going to be a win for us. We're going to have lower electricity usage,

75
00:05:03,142 --> 00:05:05,726
which is going to save the planet, and that's going to be cheaper for us

76
00:05:05,748 --> 00:05:08,302
to save in the long run, and we can leverage that in the cloud.

77
00:05:08,436 --> 00:05:11,642
That's money, baby. Okay, so data lakes

78
00:05:11,706 --> 00:05:15,902
MongoDB actually has an amazing new service called the Data lake.

79
00:05:16,046 --> 00:05:19,250
The data lakes allows you to save MongoDB data.

80
00:05:19,320 --> 00:05:22,514
It can auto archive it into an AWS s

81
00:05:22,552 --> 00:05:26,434
three blob storage. That means for the long run, right? You can save things

82
00:05:26,472 --> 00:05:29,766
in a very cheap and efficient s three blob storage as

83
00:05:29,788 --> 00:05:33,222
a JSON, just a giant JSON text file, and you cold still

84
00:05:33,276 --> 00:05:36,614
query that data as if it was hot data, using your standard

85
00:05:36,732 --> 00:05:40,294
MongoDB queries to get that data out. But you're not

86
00:05:40,332 --> 00:05:44,070
paying for all that data to be saved actively in a MongoDB database.

87
00:05:44,150 --> 00:05:47,770
Again, that saves money and the planet all at once. Very cool.

88
00:05:47,840 --> 00:05:51,014
For those who don't know too, data lake is just a catch all term.

89
00:05:51,062 --> 00:05:53,706
You've probably seen it a lot in industry right now, but it allows you to

90
00:05:53,728 --> 00:05:57,738
save lots of different, unspecified, semi structured data altogether.

91
00:05:57,834 --> 00:06:00,478
This allows us to, like, we can pull a bunch of different things together and

92
00:06:00,484 --> 00:06:03,054
it's all going to be in one place and allow us to query that.

93
00:06:03,092 --> 00:06:06,290
With MongoDB data lakes, you can save things with different, or you could query things

94
00:06:06,360 --> 00:06:10,158
across databases, across collections, into s three blobs,

95
00:06:10,254 --> 00:06:13,874
whatever you need to do. We cold all put all that unstructured data together

96
00:06:13,992 --> 00:06:17,650
and make our lives a little bit easier. Okay, so in this talk today,

97
00:06:17,720 --> 00:06:21,222
I want to explore the MongoDB data lakes a little bit further. We'll be doing

98
00:06:21,276 --> 00:06:25,042
a function to save some data into our s three buckets,

99
00:06:25,106 --> 00:06:28,194
and then we'll be doing some queries on that. I also will be talking briefly

100
00:06:28,242 --> 00:06:31,638
about our new online archive, which makes this even easier to use.

101
00:06:31,724 --> 00:06:34,778
So MongoDB Atlas data Lake allows you to

102
00:06:34,784 --> 00:06:38,330
work with your data really easily. It scales automatically for you.

103
00:06:38,400 --> 00:06:41,914
We can auto put all of your MongoDB data into

104
00:06:41,952 --> 00:06:45,694
the data lake, and it's integrated with all the MongoDB cloud services,

105
00:06:45,812 --> 00:06:49,822
which makes working with that data just super easy. It's so

106
00:06:49,876 --> 00:06:53,246
fun and so easy to work with. It eliminates, because a lot of times what

107
00:06:53,268 --> 00:06:56,778
we're seeing is people are building their own data lakes solutions of MongoDB,

108
00:06:56,874 --> 00:06:59,934
and that's hard to do. We built it for you. You don't have to worry

109
00:06:59,982 --> 00:07:02,978
about archived that anymore. We want to make sure that it's easy to work with

110
00:07:02,984 --> 00:07:06,734
your data. It's easy to archive it, easy to get it out, and hopefully

111
00:07:06,782 --> 00:07:09,878
save you some money too. Also, as a developer, I just want it to

112
00:07:09,884 --> 00:07:13,686
be easy. We make that easy too. Yeah, you can still access your

113
00:07:13,708 --> 00:07:17,442
data just like you could with any other MongoDB database.

114
00:07:17,506 --> 00:07:20,806
You can access it with all of the drivers you're used to. It's going to

115
00:07:20,828 --> 00:07:24,146
feel just like you're playing with a MongoDB database. It's just going to be saved

116
00:07:24,178 --> 00:07:27,494
somewhere else and the queries might be a little slower. But again,

117
00:07:27,532 --> 00:07:30,398
if you're archiving data, that data doesn't need to be hot. You're going to be

118
00:07:30,404 --> 00:07:32,638
saving so much money for that. So it's up to you to determine what kind

119
00:07:32,644 --> 00:07:35,726
of data is going to work. Well, that you can archive, but we have a

120
00:07:35,748 --> 00:07:39,214
control plane that goes and accesses and points to where our queries need to go

121
00:07:39,252 --> 00:07:42,586
and where that data is saved and consolidates all that and then serves

122
00:07:42,618 --> 00:07:45,566
that up to you when you make those queries. Very cool. Right? And it can

123
00:07:45,588 --> 00:07:48,754
break it down to a bunch of different data sources. This isn't actually a live

124
00:07:48,792 --> 00:07:52,830
demo, but prerecorded anyways, I'm not really ready to live this dangerously

125
00:07:52,910 --> 00:07:57,150
yet. But firstly, what I want to do is set up a database

126
00:07:57,230 --> 00:08:01,042
with some hot and cold data and test it to make sure that it's

127
00:08:01,106 --> 00:08:04,274
going to work for our needs. What we're going to do, I wrote a Python

128
00:08:04,322 --> 00:08:07,762
script that imports a massive amount of lakes Iot

129
00:08:07,826 --> 00:08:10,494
data that we're going to be using. We're going to be using this fake IoT

130
00:08:10,562 --> 00:08:14,582
data to auto archived cold or old IoT

131
00:08:14,646 --> 00:08:18,074
data into our s three cluster, and we'll be able to query that.

132
00:08:18,112 --> 00:08:22,134
So you can see here every couple of seconds it's generating 2400 documents.

133
00:08:22,262 --> 00:08:25,630
It's a little slow, whatever. Every second inserts a couple,

134
00:08:25,700 --> 00:08:29,694
but. Great. I think I insert like 2400

135
00:08:29,892 --> 00:08:33,506
here. I can't remember. We'll find out in the next step. So you

136
00:08:33,528 --> 00:08:37,486
can see here on our MongoDB database,

137
00:08:37,598 --> 00:08:42,078
we've inserted 24,000 documents, or 52 megabytes

138
00:08:42,094 --> 00:08:45,518
of data into our collection. So we just have an IoT.

139
00:08:45,614 --> 00:08:48,846
This is just a generic IoT database

140
00:08:48,878 --> 00:08:51,414
that you might be saving some of your IoT data or whatever it is.

141
00:08:51,452 --> 00:08:55,282
We're just using IoT for this example. Right, but we have 2400 documents

142
00:08:55,346 --> 00:08:59,138
in here that we've just inserted with our script. Not a problem. And again,

143
00:08:59,164 --> 00:09:02,314
if you want to check out the script at all, check out that

144
00:09:02,352 --> 00:09:05,738
page with all the resources. Joecarlson dev

145
00:09:05,904 --> 00:09:08,714
mdb datalike all right,

146
00:09:08,752 --> 00:09:11,786
cool. So we got all of our data in there. Now we want to set

147
00:09:11,808 --> 00:09:14,554
up our data lakes so that we can connect it and query all this data

148
00:09:14,592 --> 00:09:17,886
and auto archive our data that we need in there. So how do we do

149
00:09:17,908 --> 00:09:21,006
that? If you're on atlas, you can see here in the,

150
00:09:21,108 --> 00:09:24,526
there's a data lakes portion on the right hand thing. And you can configure your

151
00:09:24,548 --> 00:09:28,018
data lakes pretty easily. It's actually easier than it is now.

152
00:09:28,184 --> 00:09:31,106
You just can click around in the GUI and connect whatever data sources they are,

153
00:09:31,128 --> 00:09:34,802
whether it's an s three cluster or a local one, which is pretty nice.

154
00:09:34,936 --> 00:09:38,110
You can also auto archive it too with our serverless functions with realm,

155
00:09:38,190 --> 00:09:41,526
but we're going to skip through that today. Okay, so let's assume that we have

156
00:09:41,548 --> 00:09:45,254
our data lakes set up. Wonderful. We also need to configure it with

157
00:09:45,292 --> 00:09:49,014
our third party services, which in this case is AWS s three.

158
00:09:49,132 --> 00:09:52,042
So let's see here. Okay, and we're back. Okay,

159
00:09:52,096 --> 00:09:55,766
so we're able to add s three in Mongodb.

160
00:09:55,878 --> 00:09:59,126
So basically we're just allowing the IM rule. So we've

161
00:09:59,158 --> 00:10:02,554
configured in s three that we have access to this. Then we're going to configure

162
00:10:02,602 --> 00:10:06,606
a new s three data service with our

163
00:10:06,788 --> 00:10:10,842
realm application, which is our serverless function. It allows us basically to automatically

164
00:10:10,906 --> 00:10:14,894
interact with our s three blob storage. So you're coding to put wherever your blob

165
00:10:14,942 --> 00:10:18,930
is and your secret key. I'm not revealing that here today.

166
00:10:19,080 --> 00:10:21,778
And you can define some rules for that. Cool.

167
00:10:21,944 --> 00:10:25,586
All right. So you can also set up rules too

168
00:10:25,768 --> 00:10:28,818
with everything of MongoDB. You can set up user based roles

169
00:10:28,914 --> 00:10:32,246
that have configurable access. And that's for all

170
00:10:32,268 --> 00:10:35,702
the services. You configure that as well. We're going to define rules in here.

171
00:10:35,756 --> 00:10:39,274
So what I'm going to use that service, the s three service we just set

172
00:10:39,312 --> 00:10:43,580
up, and we're going to add a rule that when data

173
00:10:44,030 --> 00:10:47,562
reaches a certain age, we want to automatically move that

174
00:10:47,616 --> 00:10:50,970
data over to our s three cluster.

175
00:10:51,790 --> 00:10:54,879
So let's see here. I think we can do that here. So we're going to

176
00:10:55,379 --> 00:10:57,678
write a function, this is a serverless function that lives in the cloud. You don't

177
00:10:57,684 --> 00:11:00,958
need to set up a server for this and call it realm retire. And this

178
00:11:00,964 --> 00:11:04,654
is coding to automatically, this is going to

179
00:11:04,692 --> 00:11:08,226
automatically save that data for us that we need. So this is what that looks

180
00:11:08,248 --> 00:11:11,746
like. Again, all the code is available on the page that

181
00:11:11,768 --> 00:11:15,230
I linked earlier, but we're just saying where the data is going to get saved

182
00:11:15,310 --> 00:11:18,514
and what data needs to go there. So we're finding a date range,

183
00:11:18,562 --> 00:11:21,782
right. That's going to be whatever we decide. It's going to be

184
00:11:21,836 --> 00:11:25,446
within two months ago. We want to check

185
00:11:25,468 --> 00:11:28,646
to see if there's any data that fits a query that we write,

186
00:11:28,748 --> 00:11:31,974
and it's going to automatically put all that data for us in our s three

187
00:11:32,012 --> 00:11:35,494
clusters. You can also do this on your own server.

188
00:11:35,542 --> 00:11:37,786
Here's how you might want to do that, or like what that query would look

189
00:11:37,808 --> 00:11:41,034
like if you're writing that in Python. Okay, so what we're going to do then

190
00:11:41,072 --> 00:11:44,414
is run this python script that automatically archives our data to

191
00:11:44,452 --> 00:11:50,318
an s three cluster. It's taking a date range from 2022

192
00:11:50,404 --> 00:11:54,046
to the third, so just the days it's coding to archive all that data for

193
00:11:54,068 --> 00:11:57,362
us. Okay, so it moves 240

194
00:11:57,416 --> 00:12:01,074
documents to our s three clusters, and this is actually in

195
00:12:01,112 --> 00:12:04,306
s three. Now we can see the data that we've just archived in there.

196
00:12:04,408 --> 00:12:07,454
These are just JSon. If you notice, they're all JSoN documents.

197
00:12:07,582 --> 00:12:11,778
And if we click into it, it's going to download

198
00:12:11,874 --> 00:12:14,678
for us and we can check that out. I'm going to open this up in

199
00:12:14,684 --> 00:12:17,990
visual studio code, but you can see that this is just our

200
00:12:18,060 --> 00:12:20,958
IoT data that we've archived in a JSON format.

201
00:12:21,074 --> 00:12:24,154
So what we're coding to be able to do now with our data lake is

202
00:12:24,192 --> 00:12:27,926
query this JSON text file as if it was a MongoDB

203
00:12:27,958 --> 00:12:30,380
database. Very cool, right?

204
00:12:31,390 --> 00:12:35,034
So we can actually see now that 240 documents have

205
00:12:35,072 --> 00:12:39,774
been moved, right? We had 24,000 to begin with. Now we have 23,520.

206
00:12:39,892 --> 00:12:43,086
Very cool. Okay, so we've confirmed that they've moved, we've confirmed that they're in the

207
00:12:43,108 --> 00:12:47,054
s three data clusters, and the world is happy now we're saving money.

208
00:12:47,172 --> 00:12:49,778
We don't have to pay for that storage space. We still have access to it,

209
00:12:49,784 --> 00:12:53,042
and we can still query for all that data if we need it. We also

210
00:12:53,096 --> 00:12:56,334
now have an online archive. So I wrote this from scratch.

211
00:12:56,382 --> 00:12:59,302
You do not need to do this. You can just configure online archive to follow

212
00:12:59,356 --> 00:13:03,314
rules through a GUI. It's a piece of cake. To save all this data automatically

213
00:13:03,362 --> 00:13:05,990
for you. I highly recommend checking out. It's incredible.

214
00:13:07,770 --> 00:13:11,114
Okay, we're going to try this one more time to sum up, we were

215
00:13:11,152 --> 00:13:14,278
able to archive data into an s three data cluster

216
00:13:14,374 --> 00:13:17,802
by moving data over programmatically. We still have access

217
00:13:17,856 --> 00:13:21,286
to query all that data we archived. Even though it's in an s three cluster

218
00:13:21,318 --> 00:13:24,334
and it's data with a JSON format, we can still query it like it was

219
00:13:24,372 --> 00:13:28,174
a normal MongoDB database. We're saving the world, and we're saving money by

220
00:13:28,212 --> 00:13:31,486
keeping cold data in a cheaper storage form

221
00:13:31,588 --> 00:13:35,006
that's more sustainable for growing data needs.

222
00:13:35,188 --> 00:13:38,622
That makes us happy, that makes babies happy. We're finally happy.

223
00:13:38,756 --> 00:13:42,106
Questions ip on Twitter if you have any questions. So what's next? If I've

224
00:13:42,138 --> 00:13:44,950
inspired you at all to want to explore this, I would encourage you to go

225
00:13:44,980 --> 00:13:47,446
out there and do it. I think watching a talk like this is a good

226
00:13:47,468 --> 00:13:50,086
way to learn, if it's worth your time to learn. But if you're working for

227
00:13:50,108 --> 00:13:53,186
a place or you have an application that has growing data demands

228
00:13:53,218 --> 00:13:56,770
that is costing you a lot of money. And if you're especially on MongoDB,

229
00:13:56,850 --> 00:13:59,846
definitely want to encourage you to check out how to do this. We have an

230
00:13:59,868 --> 00:14:03,010
always free dev tier with MongoDB Atlas, which is

231
00:14:03,020 --> 00:14:06,346
our cloud hosted database as a service. You should definitely check it out. It's the

232
00:14:06,368 --> 00:14:09,734
easiest way to use MongoDB. I swear it's great. There's not a free trial,

233
00:14:09,782 --> 00:14:13,370
it's free forever as long as you're under a certain size. Your database please.

234
00:14:13,440 --> 00:14:16,046
It's the best way to learn it. Join our community if you have any other

235
00:14:16,068 --> 00:14:19,546
questions, that's at community mongodb.com. We have courses

236
00:14:19,578 --> 00:14:23,802
on university mongodb.com which are also free and a great way to learn MongodB.

237
00:14:23,866 --> 00:14:27,726
And I probably write for developer mongodb.com which

238
00:14:27,748 --> 00:14:31,982
is all of our amazing developer resources are located there. If you want $100

239
00:14:32,036 --> 00:14:35,558
in free AWS credits, be sure to use code Joe when you

240
00:14:35,564 --> 00:14:38,790
sign up for atlas. All right, some more resources there,

241
00:14:38,860 --> 00:14:42,182
you can take a look at that. All of them are available at my website.

242
00:14:42,316 --> 00:14:45,958
And lastly, just want to say my name is Joe and please follow me on

243
00:14:45,964 --> 00:14:49,526
Twitter wherever else. And thank you so much. I had a blast and I

244
00:14:49,548 --> 00:14:52,498
love being here and y'all are so amazing. Have a great day y'all.

