1
00:00:22,010 --> 00:00:26,498
Hello everyone, and welcome to this session. I am Nidal Albeiruti.

2
00:00:26,594 --> 00:00:29,654
I am a solutions architect with Amazon Web Services.

3
00:00:29,852 --> 00:00:33,634
Our topic is about one of the important concepts in machine

4
00:00:33,682 --> 00:00:36,918
learning, which is feature engineering, but with a

5
00:00:36,924 --> 00:00:40,198
special focus on how to extract more

6
00:00:40,284 --> 00:00:44,114
from a specific type of sensors, which are binary

7
00:00:44,162 --> 00:00:48,230
sensors. We use them in the Internet of Things context.

8
00:00:48,810 --> 00:00:52,478
Therefore, in our agenda, I will take

9
00:00:52,564 --> 00:00:56,490
these concepts and discuss them, starting from the general topic

10
00:00:56,570 --> 00:00:59,934
down to the specific. So I will start first by

11
00:00:59,972 --> 00:01:03,454
discussing IoT solutions in general and

12
00:01:03,492 --> 00:01:06,690
their different challenges that must be thought of.

13
00:01:06,760 --> 00:01:09,954
To build such solutions. We will funnel down

14
00:01:09,992 --> 00:01:13,630
to the role of devices used there, mainly sensors.

15
00:01:13,790 --> 00:01:17,654
After that, binary sensors will be defined to

16
00:01:17,692 --> 00:01:21,814
clarify what these are and what make them different

17
00:01:21,932 --> 00:01:25,366
from other sensors. Then we will move on

18
00:01:25,388 --> 00:01:29,162
to introduce what features engineering is and

19
00:01:29,216 --> 00:01:31,820
by knowing what the feature is first.

20
00:01:32,270 --> 00:01:36,202
Later, we will discuss the additional features that

21
00:01:36,256 --> 00:01:39,546
can be created or extracted to help us to

22
00:01:39,568 --> 00:01:43,306
build more efficient machine learning models when dealing

23
00:01:43,338 --> 00:01:47,002
with binary sensors. Finally, we will introduce

24
00:01:47,066 --> 00:01:51,054
Amazon Sagemaker data wrangler as an efficient tool that

25
00:01:51,092 --> 00:01:54,802
can be used to make feature engineering tasks easier and

26
00:01:54,856 --> 00:01:57,842
repeatable. Let's make a start. So,

27
00:01:57,896 --> 00:02:01,806
first topic is Internet of Things IoT

28
00:02:01,998 --> 00:02:05,590
IoT solutions build on data received from

29
00:02:05,660 --> 00:02:09,158
the deployed sensors, and to receive this data,

30
00:02:09,324 --> 00:02:12,790
a connectivity solution in a way or another,

31
00:02:12,940 --> 00:02:16,454
must be established. The received data will then

32
00:02:16,492 --> 00:02:20,442
be analyzed to act on the different outcomes found in this

33
00:02:20,496 --> 00:02:24,022
data. The data or the actions can be forwarded

34
00:02:24,086 --> 00:02:27,606
to the integrated solutions to draw insights

35
00:02:27,638 --> 00:02:31,118
from them, and those insights can then support

36
00:02:31,204 --> 00:02:34,810
decision making or go back to the edge devices

37
00:02:34,970 --> 00:02:38,394
to trigger actions through the actuators.

38
00:02:38,522 --> 00:02:41,710
And that shows us two things. First,

39
00:02:41,860 --> 00:02:46,478
how IoT solutions can be complex and multidimensional.

40
00:02:46,654 --> 00:02:50,750
And second is how important is the role of sensors

41
00:02:50,830 --> 00:02:54,670
in IoT solutions? The explosive

42
00:02:54,750 --> 00:02:58,406
growth in IoT use cases and the sheer number

43
00:02:58,508 --> 00:03:02,834
and diversity of devices out there has been phenomenal

44
00:03:02,962 --> 00:03:06,882
in this slide. These are just a few examples

45
00:03:06,946 --> 00:03:10,394
of how AWS IoT is helping a IoT of

46
00:03:10,432 --> 00:03:14,780
customers solve their business problems, to mention a few.

47
00:03:15,710 --> 00:03:19,850
There is the use case of optimizing manufacturing.

48
00:03:20,530 --> 00:03:24,282
Then we have the use case of remotely monitoring

49
00:03:24,346 --> 00:03:28,046
patients and healthcare in

50
00:03:28,068 --> 00:03:31,038
the context of the medical field,

51
00:03:31,204 --> 00:03:35,262
tracking inventory levels and managing warehouse

52
00:03:35,326 --> 00:03:38,898
operations connecting homes in

53
00:03:38,904 --> 00:03:42,622
these context of ambient intelligence or connecting

54
00:03:42,686 --> 00:03:46,630
buildings or cities growing healthier crops

55
00:03:46,970 --> 00:03:50,822
with greater efficiencies, managing energy

56
00:03:50,956 --> 00:03:54,870
resources transforming transportation

57
00:03:55,690 --> 00:03:59,402
enhancing safety in working environments such

58
00:03:59,456 --> 00:04:02,618
as the worker safety and

59
00:04:02,704 --> 00:04:07,558
of course to monitor and manage electricity and water networks

60
00:04:07,734 --> 00:04:11,066
to achieve energy efficiency.

61
00:04:11,258 --> 00:04:15,162
AWS. You can see our customers have different use cases,

62
00:04:15,306 --> 00:04:19,006
but all use cases are taking data from

63
00:04:19,188 --> 00:04:22,662
sensors. The different types of IoT devices

64
00:04:22,746 --> 00:04:27,330
available are powered by embedded processors.

65
00:04:27,990 --> 00:04:31,362
We call them microprocessors in the case

66
00:04:31,416 --> 00:04:34,890
of sensors, which is at the middle of the slide,

67
00:04:35,070 --> 00:04:38,946
or microcontrollers in the case of actuators,

68
00:04:39,138 --> 00:04:43,670
which appear at the left hand side of this slide.

69
00:04:44,090 --> 00:04:47,886
Both of these devices have the necessary device software

70
00:04:48,018 --> 00:04:52,486
that enables them to integrate with AWS IoT.

71
00:04:52,678 --> 00:04:56,140
So where do we get these sensors and devices from?

72
00:04:56,750 --> 00:05:00,300
AWS has a device qualification program

73
00:05:00,850 --> 00:05:05,098
and qualified devices get listed in the AWS

74
00:05:05,194 --> 00:05:08,746
partner device catalog, and that helps

75
00:05:08,778 --> 00:05:12,502
you discover qualified hardware that worlds with AWS

76
00:05:12,586 --> 00:05:15,822
IoT services, so you never have to worry

77
00:05:15,886 --> 00:05:19,810
if your selected device will work with AWS IoT.

78
00:05:20,630 --> 00:05:26,840
The URL for this catalog is devices amazonaws.com.

79
00:05:27,770 --> 00:05:31,554
So binary sensors is part of the bigger group of sensors

80
00:05:31,602 --> 00:05:35,078
used in IoT solutions. They report

81
00:05:35,164 --> 00:05:38,826
the state of the monitored entities back to

82
00:05:38,848 --> 00:05:42,394
the IoT solutions. The peculiarity of

83
00:05:42,432 --> 00:05:46,060
their readings or values is that they can be only

84
00:05:46,510 --> 00:05:50,010
one of two mutually exclusive values,

85
00:05:50,450 --> 00:05:54,090
hence the name binary, which is demonstrated

86
00:05:54,170 --> 00:05:56,510
in the examples on the slide.

87
00:05:57,490 --> 00:06:01,354
So now here are the examples of binary

88
00:06:01,402 --> 00:06:04,958
sensors that are widely used in IoT

89
00:06:05,054 --> 00:06:08,194
environments. First, we have the

90
00:06:08,232 --> 00:06:11,710
passive infrared sensors, PIR sensors.

91
00:06:11,870 --> 00:06:15,734
These are motion detectors and they

92
00:06:15,772 --> 00:06:19,378
can report back movement or no movement.

93
00:06:19,554 --> 00:06:23,254
Similarly, we have pressure sensors and they can

94
00:06:23,292 --> 00:06:27,870
report pressure or no pressure connectivity sensors.

95
00:06:28,050 --> 00:06:32,358
They can report if there is a connection or no connection.

96
00:06:32,534 --> 00:06:35,786
Same for vibration. And finally, we have the

97
00:06:35,808 --> 00:06:39,142
example of smoke sensors, which are smoke

98
00:06:39,206 --> 00:06:42,862
detectors. They can report if there's a smoke or

99
00:06:42,916 --> 00:06:46,922
no smoke. Now we'll move on to the topic of feature

100
00:06:46,986 --> 00:06:50,702
engineering, which is under the context of

101
00:06:50,756 --> 00:06:54,014
machine learning topic. So feature

102
00:06:54,062 --> 00:06:58,254
engineering is part of the process to get data ready for machine

103
00:06:58,302 --> 00:07:01,700
learning modeling. After locating data,

104
00:07:02,070 --> 00:07:05,446
you first need to work

105
00:07:05,548 --> 00:07:09,490
on the various formats from the different identified sources,

106
00:07:09,650 --> 00:07:12,994
such Aws databases or data warehouses,

107
00:07:13,122 --> 00:07:16,258
which may require creating complex queries.

108
00:07:16,434 --> 00:07:20,694
Alternatively, data may exist AWS CSV or compressed

109
00:07:20,742 --> 00:07:23,978
format files on s three, for example,

110
00:07:24,064 --> 00:07:27,866
in data lakes. As part of

111
00:07:27,888 --> 00:07:30,522
this step exploratory data analysis,

112
00:07:30,586 --> 00:07:35,050
EDA is executed, and it's about exploring

113
00:07:35,130 --> 00:07:39,070
and analyzing the raw data even without

114
00:07:39,140 --> 00:07:42,834
domain knowledge. Once data is collected, it needs

115
00:07:42,872 --> 00:07:45,982
to be transformed into a usable format.

116
00:07:46,126 --> 00:07:49,618
Transforming your data requires you to write code

117
00:07:49,784 --> 00:07:53,474
to do these tedious tasks, for example,

118
00:07:53,672 --> 00:07:57,510
converting numbers into floating point dates into

119
00:07:57,580 --> 00:08:01,622
timestamps, or converting category text labels into

120
00:08:01,676 --> 00:08:05,654
integers. And all of those are some well

121
00:08:05,692 --> 00:08:08,278
known feature engineering tasks.

122
00:08:08,454 --> 00:08:12,266
After the data is transformed, you then write more code

123
00:08:12,368 --> 00:08:16,860
to create visualizations to inspect and analyze data

124
00:08:17,550 --> 00:08:21,374
such as quickly detecting outliers or

125
00:08:21,412 --> 00:08:25,482
extreme values within a data set, which is part of feature

126
00:08:25,546 --> 00:08:29,022
engineering as well. Once you have prepared your data

127
00:08:29,076 --> 00:08:33,198
in your development environment, you must make the data preparation

128
00:08:33,294 --> 00:08:37,710
work in production. This requires help from it operations

129
00:08:37,790 --> 00:08:41,810
team to schedule the data preparation to occur. AWS needed,

130
00:08:41,960 --> 00:08:45,134
such as on a regular calendar schedule or

131
00:08:45,192 --> 00:08:48,466
when the new data is available, or to translate

132
00:08:48,498 --> 00:08:52,790
the data preparation code into a more scalable language.

133
00:08:53,210 --> 00:08:56,918
Machine learning algorithms take a

134
00:08:56,924 --> 00:09:00,906
representation of the reality as vectors of

135
00:09:01,008 --> 00:09:04,918
features, which are aspects of the reality

136
00:09:05,014 --> 00:09:07,530
over which the algorithm operates.

137
00:09:07,950 --> 00:09:11,850
Pedro Domenigos says, some machine learning projects

138
00:09:11,930 --> 00:09:15,486
succeed and some fail. What makes the

139
00:09:15,508 --> 00:09:19,626
difference easily is the most important factor,

140
00:09:19,738 --> 00:09:23,338
which is these features used. So the

141
00:09:23,364 --> 00:09:27,842
definition of the feature you see here concurs with

142
00:09:27,896 --> 00:09:31,874
what Pedro has said. And on the right hand side of this

143
00:09:31,912 --> 00:09:35,658
slide you can see the different types of features.

144
00:09:35,854 --> 00:09:39,750
So what makes a good feature? A good feature

145
00:09:40,170 --> 00:09:44,374
must be informative, describes something

146
00:09:44,492 --> 00:09:48,026
that makes sense to a human. A good features must

147
00:09:48,048 --> 00:09:51,674
be available. You must have as many instances as

148
00:09:51,712 --> 00:09:55,626
possible or you need to deal with the missing data.

149
00:09:55,808 --> 00:09:59,398
A good feature must be discriminant, which divides

150
00:09:59,494 --> 00:10:02,650
instances into the different target classes

151
00:10:02,810 --> 00:10:06,526
or correlates with the target value.

152
00:10:06,708 --> 00:10:10,266
Good features allow a simple model to beat

153
00:10:10,458 --> 00:10:13,970
a complex model. You want features also to be

154
00:10:14,040 --> 00:10:18,002
as independent from each other and simple as

155
00:10:18,056 --> 00:10:21,474
possible. AWS better features usually means

156
00:10:21,592 --> 00:10:25,300
simpler models. We have to make the difference

157
00:10:25,930 --> 00:10:30,950
clear between features with hyperparameters.

158
00:10:31,290 --> 00:10:34,982
Hyperparameters are a set of parameters that

159
00:10:35,036 --> 00:10:38,810
are not determined by the learning algorithm, but rather

160
00:10:38,880 --> 00:10:42,614
specified as inputs to the learning algorithm.

161
00:10:42,742 --> 00:10:46,086
Hopefully the difference is clear. So, back to features.

162
00:10:46,118 --> 00:10:49,770
Now let's see some examples of features

163
00:10:50,290 --> 00:10:54,490
here. In this slide you can see the examples of features

164
00:10:54,650 --> 00:10:58,634
that can be created or extracted of your collected

165
00:10:58,682 --> 00:11:02,550
data, whether it's coming from sensors or other sources.

166
00:11:02,650 --> 00:11:06,734
In noniot solutions, the examples

167
00:11:06,782 --> 00:11:10,846
include in the case of images, we can extract

168
00:11:10,958 --> 00:11:14,778
the colors there, the texture, the contours.

169
00:11:14,974 --> 00:11:18,402
In the case of signals, the frequency, these phase,

170
00:11:18,466 --> 00:11:22,520
the samples, the spectrum, time series we have

171
00:11:23,930 --> 00:11:27,374
text and trends and self similarity

172
00:11:27,442 --> 00:11:30,534
between different time windows of the time series,

173
00:11:30,662 --> 00:11:34,890
biomedical context, the dna sequence and genes

174
00:11:35,310 --> 00:11:38,538
for text. One of these well known features to

175
00:11:38,544 --> 00:11:42,506
be extracted is POS tags, which refers

176
00:11:42,538 --> 00:11:46,158
to the parts of speech, which is the process to apply

177
00:11:46,244 --> 00:11:49,674
word classes to words within a sentence.

178
00:11:49,802 --> 00:11:52,678
For example, you take nouns,

179
00:11:52,874 --> 00:11:56,258
verbs, prepositions and tag them

180
00:11:56,344 --> 00:11:57,970
within statements.

181
00:11:58,870 --> 00:12:02,622
Great, now we know what the features

182
00:12:02,686 --> 00:12:06,150
is. So what is feature engineering?

183
00:12:06,570 --> 00:12:10,038
Feature engineering is the process of representing a problem

184
00:12:10,124 --> 00:12:13,830
domain to make it amenable for learning techniques.

185
00:12:14,490 --> 00:12:18,102
This process involves the initial discovery

186
00:12:18,166 --> 00:12:22,250
of features and their stepwise improvement,

187
00:12:22,910 --> 00:12:26,982
all based on domain knowledge and the observed performance

188
00:12:27,046 --> 00:12:30,526
of a given ML algorithm over specific

189
00:12:30,628 --> 00:12:34,346
training data. So features engineering

190
00:12:34,458 --> 00:12:38,734
helps improve results by modifying the

191
00:12:38,772 --> 00:12:42,454
data's features to better capture the nature

192
00:12:42,522 --> 00:12:45,966
of the problem. And feature engineering tends

193
00:12:45,998 --> 00:12:49,454
to bring out performance gains beyond tweaking

194
00:12:49,502 --> 00:12:52,818
the algorithms themselves in machine learning

195
00:12:52,904 --> 00:12:56,486
context. Sometimes feature engineering is

196
00:12:56,508 --> 00:12:59,782
referred to as data munging or

197
00:12:59,836 --> 00:13:02,946
data wrangling. Regardless,

198
00:13:03,138 --> 00:13:07,030
features engineering is a process to select and

199
00:13:07,100 --> 00:13:11,034
transform variables when creating a predictive model

200
00:13:11,152 --> 00:13:14,490
using machine learning or statistical modeling.

201
00:13:14,990 --> 00:13:18,538
And future engineering is an art like

202
00:13:18,624 --> 00:13:22,094
engineering is an art, like programming is can art,

203
00:13:22,212 --> 00:13:25,374
like medicine, is an art. There are well

204
00:13:25,412 --> 00:13:28,350
defined procedures in future engineering,

205
00:13:28,770 --> 00:13:33,022
and these defined procedures are methodical and

206
00:13:33,076 --> 00:13:36,154
provable, and they are widely

207
00:13:36,202 --> 00:13:39,758
known and understood, and future engineering

208
00:13:39,854 --> 00:13:43,300
is sensitive to these machine learning. Algorithm being used.

209
00:13:43,910 --> 00:13:47,750
AWS there are certain types of features, for example,

210
00:13:47,820 --> 00:13:51,254
categorical, that fare better with some

211
00:13:51,292 --> 00:13:54,530
algorithms, for example, decision trees,

212
00:13:54,690 --> 00:13:57,650
than others, such as, for example,

213
00:13:57,820 --> 00:14:02,102
svms. This slide lists the following

214
00:14:02,166 --> 00:14:05,930
subproblems, and they are as well known

215
00:14:06,590 --> 00:14:10,562
as subdomains or subproblems

216
00:14:10,646 --> 00:14:14,494
of feature engineering processes. I will

217
00:14:14,532 --> 00:14:18,590
go through some of them just giving examples.

218
00:14:19,010 --> 00:14:21,470
Starting by feature creation.

219
00:14:22,390 --> 00:14:26,130
Feature creation identifies these features in the data set

220
00:14:26,280 --> 00:14:29,540
that are relevant to the problem at hand.

221
00:14:30,150 --> 00:14:33,394
Moving on to feature importance, which is

222
00:14:33,432 --> 00:14:37,126
related to wrapper methods, it is possible to

223
00:14:37,148 --> 00:14:40,422
take advantage of some of the algorithms that do

224
00:14:40,476 --> 00:14:44,322
embedded feature selection to obtain a feature importance

225
00:14:44,386 --> 00:14:48,646
value as measured by the ML algorithm.

226
00:14:48,838 --> 00:14:52,650
For example, random forests produce

227
00:14:53,950 --> 00:14:57,514
an out of these feature importance score for each

228
00:14:57,552 --> 00:15:01,440
features as part of their training process.

229
00:15:02,130 --> 00:15:06,586
Feature transformation manages replacing missing features

230
00:15:06,698 --> 00:15:10,666
or features that are not valid. Some techniques

231
00:15:10,698 --> 00:15:13,890
include forming cartesian products of features,

232
00:15:14,230 --> 00:15:18,414
nonlinear transformations such as binning numeric variables

233
00:15:18,462 --> 00:15:21,822
into categories, and creating domain

234
00:15:21,886 --> 00:15:25,006
specific features. Moving on

235
00:15:25,048 --> 00:15:28,802
to feature extraction, it is the process of creating

236
00:15:28,866 --> 00:15:31,830
new features from existing features,

237
00:15:32,330 --> 00:15:36,134
typically with the goal of reducing the

238
00:15:36,332 --> 00:15:40,026
dimensionality of the features and

239
00:15:40,128 --> 00:15:44,134
in features engineering worlds the curse

240
00:15:44,182 --> 00:15:48,134
of dimensionality is a well known

241
00:15:48,182 --> 00:15:52,654
term referring to having numerous features and

242
00:15:52,772 --> 00:15:56,538
not restricting or reducing these dimensionality

243
00:15:56,634 --> 00:16:00,954
of these features that you have. While feature selection

244
00:16:01,082 --> 00:16:04,638
is the filtering of irrelevant

245
00:16:04,734 --> 00:16:08,478
or redundant features from your data sets,

246
00:16:08,654 --> 00:16:12,798
this is usually done by observing variance or correlation

247
00:16:12,894 --> 00:16:17,350
thresholds to determine which features to remove.

248
00:16:18,090 --> 00:16:21,762
So let's discuss now what are these suggested

249
00:16:21,826 --> 00:16:25,206
feature engineering techniques? And to

250
00:16:25,228 --> 00:16:29,274
do that, we'll introduce these first set of the

251
00:16:29,312 --> 00:16:33,500
well known feature engineering techniques, such AWS

252
00:16:33,950 --> 00:16:38,086
imputation handling outliers binning

253
00:16:38,278 --> 00:16:42,586
log transform one hot encoding grouping

254
00:16:42,618 --> 00:16:47,098
operations feature split scaling extracting

255
00:16:47,194 --> 00:16:50,714
dates there are other ways for feature

256
00:16:50,762 --> 00:16:54,942
engineering, but those are a good and well

257
00:16:54,996 --> 00:16:59,666
known examples. Feature feature feature engineering techniques

258
00:16:59,848 --> 00:17:03,330
binary IoT sensors now into the related features

259
00:17:03,400 --> 00:17:07,858
engineering techniques that we use with sensors.

260
00:17:08,034 --> 00:17:12,546
So what can we do with the data that we get from sensors

261
00:17:12,578 --> 00:17:16,150
in the IoT world? These slide shows

262
00:17:16,220 --> 00:17:19,878
an example of applying Fourier transform

263
00:17:20,054 --> 00:17:24,010
which is a mathematical transformation. It is

264
00:17:24,160 --> 00:17:27,126
widely used to take the sensors reading,

265
00:17:27,238 --> 00:17:31,186
which are distributed along the time domain, and transport

266
00:17:31,238 --> 00:17:34,686
them to a frequency domain. This is a

267
00:17:34,708 --> 00:17:38,026
very helpful transformation in the case of any sensors,

268
00:17:38,138 --> 00:17:41,790
including binary sensors. But what

269
00:17:41,860 --> 00:17:45,282
can we do for binary sensors? Is it more

270
00:17:45,336 --> 00:17:47,940
than on off indication only?

271
00:17:48,630 --> 00:17:52,750
Let's see specifically for binary sensors,

272
00:17:52,910 --> 00:17:56,706
what I'm suggesting here are these techniques

273
00:17:56,898 --> 00:18:00,150
in this slide and upcoming slides.

274
00:18:00,570 --> 00:18:04,214
First is a way to cut up

275
00:18:04,252 --> 00:18:08,330
the observations into a series of time windows.

276
00:18:08,830 --> 00:18:12,374
These inside each window you ignore

277
00:18:12,422 --> 00:18:15,786
the temporal parts of your sensor data.

278
00:18:15,968 --> 00:18:19,558
These technique can include same or different types

279
00:18:19,574 --> 00:18:23,086
of sensors in one bag and

280
00:18:23,188 --> 00:18:26,762
hence its name as in bag of features.

281
00:18:26,906 --> 00:18:30,990
Another way is for a series of sensor events

282
00:18:31,410 --> 00:18:35,074
can be time windowed and the challenge then

283
00:18:35,112 --> 00:18:39,170
is to select the appropriate length of this time window.

284
00:18:39,590 --> 00:18:43,566
So we are segmenting the series of sensor events.

285
00:18:43,758 --> 00:18:47,842
Please note that you should do some exploratory data analysis

286
00:18:47,906 --> 00:18:52,230
to find the typical sensor activity durations,

287
00:18:52,810 --> 00:18:56,214
and then you can select the time window length. It can

288
00:18:56,252 --> 00:19:00,090
be 5 seconds, 10 seconds or more after

289
00:19:00,160 --> 00:19:03,130
segmenting the series into time windows,

290
00:19:03,470 --> 00:19:06,826
which is out of the values, the available

291
00:19:06,928 --> 00:19:10,670
values that you have in the time window,

292
00:19:11,170 --> 00:19:14,858
you take each time window and then you can abstract

293
00:19:15,034 --> 00:19:19,102
the whole of it by representing it into

294
00:19:19,156 --> 00:19:23,266
a single value. For example, you can take the

295
00:19:23,288 --> 00:19:26,530
median of the values available within

296
00:19:26,600 --> 00:19:30,626
the time window, or in other cases you

297
00:19:30,648 --> 00:19:35,414
can see the presence of one value can

298
00:19:35,452 --> 00:19:38,870
be taken to represent the whole of the time window,

299
00:19:39,290 --> 00:19:44,070
for example by applying minimum or maximum operations.

300
00:19:44,890 --> 00:19:48,966
Let's see more techniques that we can apply to binary

301
00:19:49,078 --> 00:19:52,314
sensors. In this case as what we

302
00:19:52,352 --> 00:19:56,266
see in the slide. Binary sensors can be used to identify the

303
00:19:56,288 --> 00:20:00,434
location of the monitored entity by applying

304
00:20:00,502 --> 00:20:02,990
localizations in regions.

305
00:20:03,330 --> 00:20:07,370
Technique binary sensors can detect the presence

306
00:20:07,450 --> 00:20:11,054
or absence of a particular target in their

307
00:20:11,092 --> 00:20:14,946
sensing regions. They can be used to partition a

308
00:20:14,968 --> 00:20:19,730
monitored area and provide localization functionality.

309
00:20:20,230 --> 00:20:23,394
Even more, if many of these sensors are

310
00:20:23,432 --> 00:20:27,654
deployed to monitor an area, the area itself can

311
00:20:27,692 --> 00:20:30,710
be partitioned into subregions,

312
00:20:31,290 --> 00:20:35,202
and each subregion is characterized by the sensors

313
00:20:35,266 --> 00:20:39,226
detecting targets within that region. Let's see

314
00:20:39,328 --> 00:20:42,966
more techniques as well. Some of these new features

315
00:20:42,998 --> 00:20:46,170
that can be extracted to help your machine learning

316
00:20:46,240 --> 00:20:49,594
models. We have the location area,

317
00:20:49,712 --> 00:20:52,974
which may be associated with a unique sensor number

318
00:20:53,172 --> 00:20:56,426
within the ensemble of deployed sensors.

319
00:20:56,618 --> 00:21:00,538
The total elapsed time of each continuously

320
00:21:00,634 --> 00:21:04,190
happening event when these sensor has switched

321
00:21:04,270 --> 00:21:07,486
to the on state. Because we're speaking about binary

322
00:21:07,518 --> 00:21:12,382
sensors here, this will indicate the length of detecting continuous

323
00:21:12,526 --> 00:21:16,082
sensor state we spoke about time window,

324
00:21:16,226 --> 00:21:19,960
but time window aggregation where we group events together.

325
00:21:20,570 --> 00:21:24,178
We can then compare similar periods

326
00:21:24,274 --> 00:21:26,760
of the day, for example, together.

327
00:21:27,950 --> 00:21:31,430
Finally, merging the binary sensors events

328
00:21:31,590 --> 00:21:35,258
with other sensor events to add context to the

329
00:21:35,264 --> 00:21:39,778
binary sensor event to extract new features.

330
00:21:39,974 --> 00:21:43,770
For example, having a binary sensor associated

331
00:21:43,850 --> 00:21:48,154
with a light sensor, for example, to differentiate

332
00:21:48,202 --> 00:21:51,870
between an event happening in daytime and

333
00:21:51,940 --> 00:21:55,300
another event happening at night, for example.

334
00:21:55,990 --> 00:21:59,182
Now, taking the PIR sensor

335
00:21:59,326 --> 00:22:02,862
case specifically, and when detecting

336
00:22:02,926 --> 00:22:05,666
activity using passive infrared sensors,

337
00:22:05,698 --> 00:22:09,234
PIR sensors here are some examples

338
00:22:09,362 --> 00:22:12,834
of the features that can be extracted for PIR

339
00:22:12,962 --> 00:22:16,162
sensors. For example, these activity

340
00:22:16,226 --> 00:22:19,580
level within a region, an area

341
00:22:20,350 --> 00:22:24,218
which is the number of events firing or happening

342
00:22:24,304 --> 00:22:28,138
or coming from a PIR sensors can be one

343
00:22:28,224 --> 00:22:31,150
of the extracted features.

344
00:22:31,650 --> 00:22:35,754
For example, as well, we have the elapsed

345
00:22:35,802 --> 00:22:39,294
time between sensor events. So if we

346
00:22:39,332 --> 00:22:43,266
have two consecutive same value events from

347
00:22:43,288 --> 00:22:47,666
the same sensor, this means that the entity in

348
00:22:47,688 --> 00:22:50,994
the same region or subregion is

349
00:22:51,112 --> 00:22:55,578
active within the region. So those are two consecutive

350
00:22:55,694 --> 00:22:59,542
same value events from the same sensors. And the time

351
00:22:59,596 --> 00:23:03,174
between those two events can be extracted aws,

352
00:23:03,212 --> 00:23:06,566
well as inactivity time within the

353
00:23:06,588 --> 00:23:09,818
same region or subregion. In another

354
00:23:09,904 --> 00:23:13,050
way, if we have two consecutive same value

355
00:23:13,120 --> 00:23:15,690
events from different sensors,

356
00:23:16,270 --> 00:23:20,254
this means that the entity being monitored here is moving from

357
00:23:20,292 --> 00:23:24,426
one region to another, and this is active

358
00:23:24,458 --> 00:23:27,998
time or changing these region can be another

359
00:23:28,084 --> 00:23:31,930
feature and a very helpful feature to identify

360
00:23:32,090 --> 00:23:36,510
activity between different regions. The frequency

361
00:23:36,590 --> 00:23:40,514
of movement events within the same region, as we

362
00:23:40,552 --> 00:23:44,862
said, can be extracted as an activity time

363
00:23:45,016 --> 00:23:49,250
or no activity time. This is very useful

364
00:23:49,330 --> 00:23:53,170
in the case of ambient intelligent intelligence

365
00:23:53,250 --> 00:23:57,918
solutions, which is in the home monitoring

366
00:23:58,114 --> 00:24:01,226
contexts or environments. Finally,

367
00:24:01,328 --> 00:24:05,754
we have a very important event which I

368
00:24:05,872 --> 00:24:09,542
found it to be very helpful in ambient

369
00:24:09,606 --> 00:24:13,162
intelligence and behavioral modeling

370
00:24:13,226 --> 00:24:16,938
techniques is the no sensors events,

371
00:24:17,114 --> 00:24:20,954
which means that we don't have any PIR sensor

372
00:24:21,002 --> 00:24:24,606
firing coming from any deployed or ensemble

373
00:24:24,798 --> 00:24:28,900
of sensors. So any no

374
00:24:29,350 --> 00:24:32,786
sensor firings at all can be

375
00:24:32,808 --> 00:24:36,086
considered a features, which means

376
00:24:36,188 --> 00:24:39,814
that the monitored entity is not available as

377
00:24:39,852 --> 00:24:43,590
well, which helps behavioral modeling.

378
00:24:44,090 --> 00:24:47,202
So we spoke about feature

379
00:24:47,266 --> 00:24:50,902
engineering. We defined what a feature is, and we defined what feature

380
00:24:50,966 --> 00:24:55,334
engineering is. Now we're going to introduce Amazon

381
00:24:55,382 --> 00:24:59,034
Sagemaker data Wrangler as part of

382
00:24:59,152 --> 00:25:01,790
the Amazon Sagemaker studio.

383
00:25:02,930 --> 00:25:06,670
Amazon Sagemaker is a service with a lot of different

384
00:25:06,740 --> 00:25:10,810
features and capabilities in it. We typically

385
00:25:10,890 --> 00:25:14,290
talk about those capabilities as falling into four

386
00:25:14,360 --> 00:25:17,634
categories. We have on the left hand

387
00:25:17,672 --> 00:25:21,314
side of the slide the data preparation and

388
00:25:21,352 --> 00:25:25,414
then these model build phase. Then we move

389
00:25:25,452 --> 00:25:28,818
on to training and tuning and deployment

390
00:25:28,914 --> 00:25:32,600
and management or hosting of the model.

391
00:25:33,370 --> 00:25:36,934
These four categories really address the need that

392
00:25:36,972 --> 00:25:40,474
machine learning builders have when

393
00:25:40,512 --> 00:25:44,650
dealing with each stage of a model's lifecycle.

394
00:25:45,550 --> 00:25:49,414
Since we are discussing feature engineering, the highlighted

395
00:25:49,462 --> 00:25:53,630
column on the left shows the prepare these and

396
00:25:53,700 --> 00:25:56,640
available services that are used there.

397
00:25:57,330 --> 00:26:00,814
Related to that, we can see the

398
00:26:00,852 --> 00:26:04,722
data angler which is Sagemaker data angler and

399
00:26:04,856 --> 00:26:07,620
Sagemaker feature store.

400
00:26:08,950 --> 00:26:13,102
Amazon Sagemaker Data Wrangler is the fastest

401
00:26:13,166 --> 00:26:16,642
and easiest way to prepare data for machine

402
00:26:16,706 --> 00:26:20,214
learning. Sagemaker Data Wrangler gives

403
00:26:20,252 --> 00:26:24,134
you the ability to use a visual interface to

404
00:26:24,172 --> 00:26:28,662
access data, perform EDA exploratory

405
00:26:28,726 --> 00:26:32,650
data analysis and feature engineering,

406
00:26:32,990 --> 00:26:36,442
and seamlessly operationalize your data

407
00:26:36,576 --> 00:26:39,782
flow by exporting it

408
00:26:39,936 --> 00:26:43,310
into an Amazon sagemaker pipeline.

409
00:26:43,890 --> 00:26:47,134
Amazon Sagemaker Pipeline is

410
00:26:47,172 --> 00:26:50,474
one way of exporting your data, but you can export

411
00:26:50,602 --> 00:26:54,354
as well to Amazon Sagemaker data Wrangler job

412
00:26:54,552 --> 00:26:58,414
or a Python file, or to SageMaker

413
00:26:58,542 --> 00:27:01,714
feature group. Amazon Sagemaker Data

414
00:27:01,752 --> 00:27:04,962
Wrangler also provides you with over 300 built

415
00:27:05,016 --> 00:27:08,370
in transforms, custom transforms using

416
00:27:08,440 --> 00:27:12,466
Python Pyspark or Spark SQL runtime.

417
00:27:12,658 --> 00:27:16,870
Built in data analysis such as common charts,

418
00:27:17,390 --> 00:27:21,002
custom charts, and the useful model

419
00:27:21,056 --> 00:27:24,406
analysis capabilities such as feature importance,

420
00:27:24,598 --> 00:27:27,930
target leakage, and model explainability

421
00:27:28,750 --> 00:27:32,302
all are available for you as

422
00:27:32,356 --> 00:27:35,146
part of the SageMaker studio.

423
00:27:35,338 --> 00:27:39,034
Finally, SageMaker data Angular creates a dataflow

424
00:27:39,162 --> 00:27:42,646
file that can be versioned and shared

425
00:27:42,698 --> 00:27:46,530
across the teams for reproducibility.

426
00:27:47,110 --> 00:27:51,122
With SageMaker data angular data selection tool,

427
00:27:51,256 --> 00:27:55,414
you can quickly select data from multiple data sources such as

428
00:27:55,452 --> 00:27:58,946
Amazon Athena, Amazon Redshift,

429
00:27:59,138 --> 00:28:02,694
AWS, Lake Formation, Amazon S

430
00:28:02,732 --> 00:28:06,280
three, and Amazon Sagemaker feature store.

431
00:28:07,050 --> 00:28:10,746
Recently, Snowflake has been added as a

432
00:28:10,768 --> 00:28:14,890
data source for Amazon Sagemaker data Wrangler.

433
00:28:15,230 --> 00:28:19,178
You can now quickly and easily connect to Snowflake without

434
00:28:19,264 --> 00:28:23,006
writing a single line of code for

435
00:28:23,188 --> 00:28:27,226
other sources. You can write queries for data sources

436
00:28:27,338 --> 00:28:31,390
and import data directly into SageMaker from

437
00:28:31,460 --> 00:28:34,862
various file formats such as CSV

438
00:28:34,926 --> 00:28:38,850
files, Parquet files, and database tables.

439
00:28:39,750 --> 00:28:43,362
This data is imported into a secure central data

440
00:28:43,416 --> 00:28:47,158
preparation environment where users will have access

441
00:28:47,244 --> 00:28:50,838
to a variety of pre built tools to prepare their

442
00:28:50,924 --> 00:28:54,070
data. To transform your data,

443
00:28:54,140 --> 00:28:57,706
Sagemaker data Wrangler offers a rich selection of

444
00:28:57,728 --> 00:29:00,650
pre configured data transformed. For example,

445
00:29:00,720 --> 00:29:03,846
you can convert a text file column into a numerical

446
00:29:03,878 --> 00:29:08,054
column with a single click, or author

447
00:29:08,102 --> 00:29:11,950
custom transforms in Pyspark, SQL or

448
00:29:12,020 --> 00:29:15,486
pandas to provide flexibility across

449
00:29:15,588 --> 00:29:19,326
your organization. This streamlines the

450
00:29:19,348 --> 00:29:22,534
process of cleaning, verifying,

451
00:29:22,602 --> 00:29:26,594
and visualizing data without writing a single line of

452
00:29:26,632 --> 00:29:30,530
code. Once the data is transformed, Sagemaker data

453
00:29:30,600 --> 00:29:34,290
angle makes it easy to clean and explore the data

454
00:29:34,440 --> 00:29:38,150
with data visualization in SageMaker Studio.

455
00:29:38,650 --> 00:29:42,146
These visualizations allow you to quickly identify

456
00:29:42,258 --> 00:29:45,874
inconsistencies in data preparation workflow

457
00:29:46,002 --> 00:29:49,994
and diagnose issues before models are deployed into

458
00:29:50,112 --> 00:29:53,802
production. Finally, data Wrangler makes it easy

459
00:29:53,856 --> 00:29:56,970
to create a pipeline for data preparation.

460
00:29:57,470 --> 00:30:01,274
You can export your data preparation workflow to a notebook

461
00:30:01,402 --> 00:30:04,480
or code script with a single click,

462
00:30:05,170 --> 00:30:09,386
efficiently brings your data preparation workflows into production

463
00:30:09,498 --> 00:30:13,502
without manually sifting

464
00:30:13,566 --> 00:30:17,726
through and translating hundreds of lines of data preparation

465
00:30:17,838 --> 00:30:21,726
code. So this slide summarizes all the steps

466
00:30:21,758 --> 00:30:25,970
taken by Sagemaker data wrangler and provide details

467
00:30:26,050 --> 00:30:29,640
on what functions data Wrangler performs at each step.

468
00:30:30,250 --> 00:30:33,734
The key takeaway here is that these steps can

469
00:30:33,772 --> 00:30:37,206
be iterated on in whole or in part,

470
00:30:37,308 --> 00:30:41,594
to quickly build a strong set of data transformation code

471
00:30:41,792 --> 00:30:45,498
and features to improve the performance and quality

472
00:30:45,584 --> 00:30:47,930
of your machine learning models.

473
00:30:48,590 --> 00:30:52,382
As we mentioned before, you import from multiple data

474
00:30:52,436 --> 00:30:56,234
sources, but here we mentioned Amazon Sagemaker

475
00:30:56,282 --> 00:30:59,566
feature store, which is just mentioned as an

476
00:30:59,588 --> 00:31:02,866
example only at the right hand of the

477
00:31:02,888 --> 00:31:06,642
slide you can see that to operationalize your data

478
00:31:06,776 --> 00:31:10,340
you can integrate with these other Amazon services available,

479
00:31:11,030 --> 00:31:15,686
or you can export to pipeline your

480
00:31:15,868 --> 00:31:19,446
tasks that you have created in data Wrangler to pipeline them

481
00:31:19,548 --> 00:31:23,366
and convert them to a script or python file as what we

482
00:31:23,388 --> 00:31:24,840
have described before.

483
00:31:26,170 --> 00:31:30,306
So we have reached the end of this session.

484
00:31:30,498 --> 00:31:34,186
Thank you for taking the time to listen to

485
00:31:34,208 --> 00:31:37,430
this session. We have highlighted the different additional

486
00:31:37,510 --> 00:31:41,422
features that can be extracted and created based

487
00:31:41,476 --> 00:31:45,150
on binary sensors data, and of course all

488
00:31:45,220 --> 00:31:48,954
has happened utilizing different feature engineering

489
00:31:49,002 --> 00:31:53,374
techniques. So binary sensors are not about

490
00:31:53,492 --> 00:31:56,750
zeros or ones, only much more

491
00:31:56,820 --> 00:32:00,350
can be extrapolated and extracted to help

492
00:32:00,420 --> 00:32:03,982
your machine learning models. Please use these

493
00:32:04,036 --> 00:32:07,382
additional features to enhance your models and

494
00:32:07,436 --> 00:32:11,810
get more accurate results depending on the selected ML

495
00:32:11,890 --> 00:32:15,094
algorithm. Thank you very much.

496
00:32:15,292 --> 00:32:16,900
Have a nice time. Thank you.

