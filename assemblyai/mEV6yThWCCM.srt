1
00:00:38,850 --> 00:00:42,358
Hi everyone, and welcome to my talk today. When you

2
00:00:42,364 --> 00:00:45,654
need a data streaming platform, when you need to do real time

3
00:00:45,692 --> 00:00:49,526
analytics or real time data ingestion, what probably

4
00:00:49,628 --> 00:00:52,798
comes to mind is Kafka. And if

5
00:00:52,804 --> 00:00:56,062
you've been using Kafka for a while, you might be facing some

6
00:00:56,116 --> 00:00:59,818
limitations or pain points in your daily operations,

7
00:00:59,994 --> 00:01:03,474
like certain rigidity when you need

8
00:01:03,512 --> 00:01:07,010
to scale out right? And if you're using

9
00:01:07,080 --> 00:01:10,510
traditional message bus solutions like Private MQ,

10
00:01:10,670 --> 00:01:13,570
you might also be facing certain limits.

11
00:01:14,150 --> 00:01:18,510
But I have good news for you. There is an alternative that's gaining popularity

12
00:01:18,590 --> 00:01:22,002
right now. Apache Prosa. I'm Julian,

13
00:01:22,066 --> 00:01:25,686
developer advocate at stream native. You can see a QR code and

14
00:01:25,708 --> 00:01:28,838
a link on your screen. Feel free to scan this QR code.

15
00:01:28,924 --> 00:01:32,394
That's the best way to reach out to me and to get access to

16
00:01:32,432 --> 00:01:36,730
additional resources, for example, case studies or benchmarks.

17
00:01:37,070 --> 00:01:40,614
For those who don't know stream native stream native was founded

18
00:01:40,662 --> 00:01:43,994
by the original creators of Puruson. We are one

19
00:01:44,032 --> 00:01:48,174
of the largest contributors to Apache Puruson and the ecosystem surrounding it,

20
00:01:48,292 --> 00:01:51,914
and we provide a fully managed pulsar service with enterprise

21
00:01:51,962 --> 00:01:56,478
features. This service can be deployed on either our infrastructure

22
00:01:56,574 --> 00:02:00,126
or your infrastructure. So what is Apache

23
00:02:00,158 --> 00:02:03,726
Pulsar? Apache Pulsar is a cloud native

24
00:02:03,838 --> 00:02:06,420
messaging and data streaming platform.

25
00:02:07,110 --> 00:02:10,454
Cloud native means that Pulsar is designed for

26
00:02:10,492 --> 00:02:13,654
running in containerized environments and is

27
00:02:13,692 --> 00:02:17,378
designed to scale out, to scale horizontally. But it's

28
00:02:17,394 --> 00:02:21,690
not only about scalability, it's also about elasticity.

29
00:02:22,190 --> 00:02:25,580
Elasticity is about adapting to workload changes,

30
00:02:25,950 --> 00:02:29,402
and Prusan is both messaging and the data streaming platform,

31
00:02:29,536 --> 00:02:32,826
as we will see in this presentation. So Prusan

32
00:02:32,858 --> 00:02:36,910
is a messaging and streaming platform. But what is data

33
00:02:36,980 --> 00:02:41,562
streaming? Data streaming is a process of transmitting

34
00:02:41,626 --> 00:02:45,074
data continuously and in real time from

35
00:02:45,112 --> 00:02:48,306
a source to destination. The data is

36
00:02:48,328 --> 00:02:51,762
sent in small pieces and is processed or analyzed as

37
00:02:51,816 --> 00:02:55,874
received. So streaming works best when this data needs

38
00:02:55,912 --> 00:02:59,350
to be processed in the right order, the data,

39
00:02:59,420 --> 00:03:02,934
these events then can be internally together, so they

40
00:03:02,972 --> 00:03:06,406
need to be processed in the right order. This data may

41
00:03:06,428 --> 00:03:09,942
be persisted for a long time, but also be transformed,

42
00:03:10,006 --> 00:03:13,626
aggregated, or even replayed. And you

43
00:03:13,648 --> 00:03:17,258
may also need to read millions of records very quickly

44
00:03:17,344 --> 00:03:21,390
and perform big catchup reads, so you need throughput.

45
00:03:21,890 --> 00:03:25,182
Streaming platforms can handle massive amounts of data,

46
00:03:25,316 --> 00:03:28,430
so these platforms scale out horizontally.

47
00:03:29,010 --> 00:03:32,774
Typical use cases for data streaming include data ingestion.

48
00:03:32,922 --> 00:03:36,222
There's a process of collecting and importing data from various

49
00:03:36,286 --> 00:03:40,350
sources for storage, processing and analyzes,

50
00:03:40,510 --> 00:03:44,686
and other use cases can be about real time analytics.

51
00:03:44,878 --> 00:03:48,646
You will make the right decisions based on fresh data so

52
00:03:48,668 --> 00:03:52,518
you won't wait for a batch process to be run tomorrow. For example,

53
00:03:52,604 --> 00:03:55,862
you can implement a dashboard that's updating in real

54
00:03:55,916 --> 00:03:59,190
time, or you can implement fraud detection.

55
00:03:59,530 --> 00:04:03,030
The most common streaming platform is Kafka. I can also

56
00:04:03,100 --> 00:04:07,066
mention Amazon kinesis and of course pursuant is also a great

57
00:04:07,168 --> 00:04:11,286
streaming platform. Now let's talk about messaging.

58
00:04:11,398 --> 00:04:14,746
That's very different. Sometimes we

59
00:04:14,768 --> 00:04:18,586
need to assign time consuming tasks to a group of workers.

60
00:04:18,778 --> 00:04:22,366
One producer service needs tasks to be performed by

61
00:04:22,388 --> 00:04:26,162
a consumer service, that is a worker, and these

62
00:04:26,216 --> 00:04:29,826
tasks can take some time to process, or the

63
00:04:29,848 --> 00:04:32,994
consumer service can be unavailable for a couple of

64
00:04:33,032 --> 00:04:36,882
minutes. And we don't want these events to impact the producer

65
00:04:36,946 --> 00:04:40,406
service. We need decoupling to do

66
00:04:40,428 --> 00:04:42,950
that. You can set up a work queue.

67
00:04:43,290 --> 00:04:46,886
This queue contains messages from the producers, and each of

68
00:04:46,908 --> 00:04:50,490
these messages is a task to be performed by consumers.

69
00:04:50,990 --> 00:04:54,380
You don't need to perform this task in a strict order.

70
00:04:55,070 --> 00:04:59,462
For example, if the task consists in pushing notifications to Alice

71
00:04:59,526 --> 00:05:02,926
and Bob's iPhones, it doesn't matter if I

72
00:05:02,948 --> 00:05:06,558
notify Alice first or Bob first. I won't break anything.

73
00:05:06,644 --> 00:05:10,078
I won't break my system if I do that. A message

74
00:05:10,164 --> 00:05:13,994
broker manages this message queue, accepts the producer messages

75
00:05:14,042 --> 00:05:17,538
and delivers them to the consumers so they can perform

76
00:05:17,624 --> 00:05:21,534
those tasks. So there are some essential features

77
00:05:21,582 --> 00:05:24,766
for a message queue. Queues can grow faster

78
00:05:24,798 --> 00:05:28,166
than consumers can handle. So one expected feature is

79
00:05:28,188 --> 00:05:32,130
the ability to add new consumers to consume the queue faster

80
00:05:32,210 --> 00:05:35,590
and also remove consumers when the workload decreases.

81
00:05:36,170 --> 00:05:40,086
Another expectation is when a consumer is busy processing

82
00:05:40,118 --> 00:05:43,386
a message. You don't want to wait for the processing to

83
00:05:43,408 --> 00:05:46,986
be completed. The broker will deliver the next task to

84
00:05:47,008 --> 00:05:50,330
another consumer instance without waiting for the current

85
00:05:50,400 --> 00:05:53,966
task to be complete. When a consumer fails to

86
00:05:53,988 --> 00:05:57,694
handle a message because of an error or timeout, you may

87
00:05:57,732 --> 00:06:01,454
need the message broker to redeliver the message later to

88
00:06:01,492 --> 00:06:05,326
retry the task, and you also need to remove

89
00:06:05,438 --> 00:06:08,866
undeliverable messages from the queue and move them to

90
00:06:08,888 --> 00:06:12,130
what we call a dead letter queue. Because of that,

91
00:06:12,200 --> 00:06:16,198
messages may then be consumed in a slightly different

92
00:06:16,284 --> 00:06:19,606
order than produced. But as we said,

93
00:06:19,708 --> 00:06:23,400
the ordering is not a strong requirement here.

94
00:06:23,770 --> 00:06:27,710
RabbitMQ and Amazon SQs match these requirements

95
00:06:27,810 --> 00:06:31,434
because they implement this message queue semantic and we

96
00:06:31,472 --> 00:06:35,180
cannot expect these requirements from a swimming platform.

97
00:06:35,710 --> 00:06:38,620
These platforms are not designed for that.

98
00:06:38,990 --> 00:06:43,242
However, pulsar implements both the swimming requirements

99
00:06:43,386 --> 00:06:45,870
and the message queuing requirements,

100
00:06:46,610 --> 00:06:50,510
so you can see that data streaming and messaging require a different

101
00:06:50,580 --> 00:06:53,722
set of features on the data streaming

102
00:06:53,786 --> 00:06:57,614
world. The data streaming requirements are that these events

103
00:06:57,662 --> 00:07:01,330
need to be processed in order. That's essential.

104
00:07:02,070 --> 00:07:04,910
You need to ingest a large amount of data.

105
00:07:05,080 --> 00:07:08,520
That's not quite the same on the messaging world.

106
00:07:08,890 --> 00:07:12,694
You need data retention, you need to handle big

107
00:07:12,732 --> 00:07:16,466
catch up rates. And the most common platform that provides

108
00:07:16,498 --> 00:07:20,486
this is Kafka. However, message queuing requirements

109
00:07:20,598 --> 00:07:24,294
are quite different. It's about decoupling

110
00:07:24,342 --> 00:07:27,686
task execution. It's not about processing events

111
00:07:27,718 --> 00:07:31,366
or processing stream of data. It's about decoupling

112
00:07:31,398 --> 00:07:34,954
task executions. You need to be able to add a remove

113
00:07:35,002 --> 00:07:39,674
consumer dynamically so they can catch up with those backlog

114
00:07:39,722 --> 00:07:42,814
tasks. With the task with the backlog of task. Right.

115
00:07:43,012 --> 00:07:46,186
You don't want to block the queue when a consumer is busy

116
00:07:46,298 --> 00:07:49,906
or failed to consume the message. And you need to

117
00:07:49,928 --> 00:07:53,234
redeliver those failed messaging later to

118
00:07:53,272 --> 00:07:57,094
retry it. So one of the most common platform that provide this

119
00:07:57,132 --> 00:08:00,866
is RabbitMQ. So on one side you are in the Kafka

120
00:08:00,898 --> 00:08:04,840
world and on the other side you are in the RabbitMQ world.

121
00:08:05,290 --> 00:08:09,034
Okay, so does it mean that you have to choose between messaging and

122
00:08:09,072 --> 00:08:12,742
streaming, or do we have to manage two different platforms,

123
00:08:12,806 --> 00:08:16,090
one for messaging and another for data streaming?

124
00:08:16,750 --> 00:08:20,058
Well, this is not the dilemma with Prusan. With Prusa you

125
00:08:20,064 --> 00:08:23,262
can do both using the same platform, the same

126
00:08:23,316 --> 00:08:26,958
technology. So you have only one SDK to learn

127
00:08:27,044 --> 00:08:30,714
as a developer and you have only one broker

128
00:08:30,762 --> 00:08:34,080
to manage in production. That's pretty cool.

129
00:08:34,690 --> 00:08:38,734
This is why we see that person is a unified messaging

130
00:08:38,782 --> 00:08:42,162
and streaming platform. It can handle both

131
00:08:42,216 --> 00:08:45,746
messaging and streaming use cases and this is one of the key features of

132
00:08:45,768 --> 00:08:49,818
person. But now I want to talk about pulsar

133
00:08:49,854 --> 00:08:53,800
scalability and elasticity. And these are the very different.

134
00:08:54,570 --> 00:08:58,182
Elasticity means you can grow or shrink resources

135
00:08:58,246 --> 00:09:00,858
quickly to adapt to workload changes,

136
00:09:01,024 --> 00:09:04,742
so you can save on infrastructure costs by avoiding

137
00:09:04,806 --> 00:09:08,538
over provisioning. Some data streaming platforms like

138
00:09:08,624 --> 00:09:12,346
Kafka and Prusan can scale very well,

139
00:09:12,528 --> 00:09:17,230
but Prusan is both scalable and elastic.

140
00:09:17,650 --> 00:09:21,406
The scalability requirements are determined by the bottleneck you

141
00:09:21,428 --> 00:09:25,194
have to address. So if your bottleneck is the number of messages

142
00:09:25,242 --> 00:09:28,734
remaining to be consumed in a topic, then you need to scale.

143
00:09:28,782 --> 00:09:32,434
On the consumer side, when the bottleneck is the number of

144
00:09:32,472 --> 00:09:35,422
topics or the number of connections with clients,

145
00:09:35,566 --> 00:09:39,430
then you need to add more processing power to the pulsar cluster.

146
00:09:39,770 --> 00:09:42,834
And when the bottleneck is the storage capacity,

147
00:09:42,962 --> 00:09:46,310
then you need to add more storage capacity to the pulsar cluster.

148
00:09:46,730 --> 00:09:50,298
Let's delve into the first bottleneck, which is likely to

149
00:09:50,304 --> 00:09:54,246
be the most common one you will encounter. Suppose you have multiple

150
00:09:54,278 --> 00:09:57,946
consumers consuming a topic what happens if

151
00:09:57,968 --> 00:10:01,946
the number of messages to be consumed grows faster

152
00:10:02,058 --> 00:10:05,754
than the consumers can process them? With Prusa,

153
00:10:05,802 --> 00:10:09,054
you just have to add new consumers to what we call a

154
00:10:09,092 --> 00:10:13,066
shared or key shared subscription so you can increase the throughput

155
00:10:13,098 --> 00:10:15,490
by adding new consumers to the subscription.

156
00:10:15,910 --> 00:10:19,186
And that's it. With Kafka, when you

157
00:10:19,208 --> 00:10:23,026
need to add a new active consumer to a consumer group, then you need

158
00:10:23,048 --> 00:10:26,546
to add a new partition to the topic. For that, you need to

159
00:10:26,568 --> 00:10:29,942
perform an operation on the broker side and you will also

160
00:10:29,996 --> 00:10:33,830
perform a rebalance of the data across the partitions.

161
00:10:34,250 --> 00:10:37,538
This can be heavy and this can lead to performance loss or

162
00:10:37,564 --> 00:10:41,526
even downtime. So you have to anticipate and plan carefully.

163
00:10:41,638 --> 00:10:45,978
Right, but Prusar doesn't have this issue because

164
00:10:46,064 --> 00:10:49,174
no partitions. And of course when you have fewer

165
00:10:49,222 --> 00:10:52,878
messages, you may end up with underutilized consumers so you

166
00:10:52,884 --> 00:10:56,394
can remove them to save on infrastructure costs. And that's

167
00:10:56,442 --> 00:11:00,206
pretty straightforward. All of that while preserving the

168
00:11:00,228 --> 00:11:04,050
ordering guarantee, unlike traditional messaging brokers.

169
00:11:04,550 --> 00:11:08,910
Now that we delved into the most common bottleneck, let's explore

170
00:11:08,990 --> 00:11:12,466
how Puruson can handle a rapidly increasing number of

171
00:11:12,488 --> 00:11:15,666
consumer and producers. But first I need

172
00:11:15,688 --> 00:11:19,522
to explain the unique architecture of Puruson. This architecture

173
00:11:19,586 --> 00:11:22,774
is more sophisticated than other platforms, so it

174
00:11:22,812 --> 00:11:26,086
brings many benefits. In Prusan, there are two

175
00:11:26,108 --> 00:11:29,958
types of nodes, the broker nodes and the bookie nodes. The broker nodes

176
00:11:29,974 --> 00:11:33,386
are responsible for managing all the communication and the

177
00:11:33,408 --> 00:11:36,518
processing of the topics. So they are stateless,

178
00:11:36,614 --> 00:11:40,578
they don't store data. So a broker node

179
00:11:40,614 --> 00:11:42,880
deletion won't impact the data.

180
00:11:43,410 --> 00:11:46,654
In contrast, the bookie nodes are responsible for storage. They have

181
00:11:46,692 --> 00:11:49,918
state, they store the messages and the bookies are

182
00:11:49,924 --> 00:11:53,486
Apache bookkeeper nodes. So let's say that I need more

183
00:11:53,508 --> 00:11:57,650
processing power on my cluster, I'll add more brokers

184
00:11:58,070 --> 00:12:01,858
because their state is stored in the bookkeeper. Here I can add a

185
00:12:01,864 --> 00:12:05,106
new broker and that load can then be migrated to

186
00:12:05,128 --> 00:12:08,726
another broker. So some of the magic of Prusan is that it will

187
00:12:08,748 --> 00:12:12,838
take care of all the moving of connections and the moving is

188
00:12:12,924 --> 00:12:16,950
transparent to your application. If you compatible this

189
00:12:17,020 --> 00:12:20,346
to Kafka, then adding a new broker, I will have to

190
00:12:20,368 --> 00:12:24,774
manage the movement of all of the data to another broker to rebalance

191
00:12:24,902 --> 00:12:28,010
the data right across my cluster.

192
00:12:28,430 --> 00:12:31,646
But here with Prusa, there is no

193
00:12:31,668 --> 00:12:35,150
heavy partition rebalance, there is no data movement involved.

194
00:12:35,730 --> 00:12:39,146
Now when you need to store more data, then we'll

195
00:12:39,178 --> 00:12:42,846
just add more bookies. As soon as

196
00:12:42,868 --> 00:12:46,750
you add a new bookie, it's going to be eligible for getting new messages

197
00:12:46,830 --> 00:12:50,718
right away. It immediately becomes functional

198
00:12:50,894 --> 00:12:54,782
and there is no need to wait for any data rebalance

199
00:12:54,846 --> 00:12:58,398
here the node is instantly available. Pulsar does

200
00:12:58,424 --> 00:13:01,320
not have this issue and you need scalability on your data.

201
00:13:01,690 --> 00:13:05,366
So here is a recap of the two levels of elasticity and

202
00:13:05,388 --> 00:13:09,146
the benefits compared to other streaming platforms. When you

203
00:13:09,168 --> 00:13:12,566
do data streaming or messaging, your most common bottleneck

204
00:13:12,598 --> 00:13:17,126
is the consumer. The advantage of prusan is that scaling consumer

205
00:13:17,238 --> 00:13:21,386
doesn't require complex operations like adding partitions.

206
00:13:21,578 --> 00:13:25,562
If your bottleneck is a processing power on the cluster

207
00:13:25,626 --> 00:13:29,214
side, you just have to add new broker nodes without the need for

208
00:13:29,252 --> 00:13:32,526
data movement across nodes. And finally,

209
00:13:32,628 --> 00:13:35,722
if stretch capacity is the limiting factor,

210
00:13:35,866 --> 00:13:40,270
adding new bookie nodes resolve the issue and unlike other platforms,

211
00:13:40,350 --> 00:13:42,930
these nodes immediately receive new messages.

212
00:13:43,430 --> 00:13:46,814
And of course you can easily downscale to save manufacturer

213
00:13:46,862 --> 00:13:51,270
cost. To ease the use of pulsar and Kubernetes environment with

214
00:13:51,340 --> 00:13:55,106
remote, we developed kubernetes operators. The operators

215
00:13:55,138 --> 00:13:58,454
facilitate the deployment of pulsar clusters on kubernetes.

216
00:13:58,582 --> 00:14:02,566
You can define your desired cluster configuration using familiar

217
00:14:02,598 --> 00:14:05,946
kubernetes manifest files and this allows for

218
00:14:06,048 --> 00:14:09,734
seamless scaling and facilitates the installation of additional

219
00:14:09,782 --> 00:14:12,878
components as well. You can find the documentation on

220
00:14:12,884 --> 00:14:16,590
the stream native website, and stream native offers these operators under

221
00:14:16,660 --> 00:14:20,446
a simple and free to use community license. Okay,

222
00:14:20,468 --> 00:14:24,046
so I just presented how you can scale puruson with its multi

223
00:14:24,078 --> 00:14:27,726
layered architecture, which is the second key feature of Puruson.

224
00:14:27,918 --> 00:14:31,810
Now let's shift our focus to another critical aspect.

225
00:14:32,150 --> 00:14:35,714
How does Prusan safeguard the durability of your

226
00:14:35,752 --> 00:14:39,390
data? Pulsar topics are made of segments

227
00:14:39,550 --> 00:14:43,366
and these segments contain messaging. Pulsar can

228
00:14:43,388 --> 00:14:46,710
distribute the segments to separate bookies and this is how a single

229
00:14:46,780 --> 00:14:49,990
topic is distributed across several data nodes.

230
00:14:50,330 --> 00:14:54,074
It's important to note that the storage model is completely

231
00:14:54,192 --> 00:14:58,300
different from the Kafka storage model, which is a partition based model.

232
00:14:58,990 --> 00:15:02,826
So here you have a replication factor of three. So every segment

233
00:15:02,858 --> 00:15:05,934
is replicated on three different bookies I use,

234
00:15:05,972 --> 00:15:09,246
as you can see. So if I kill one bookie here,

235
00:15:09,268 --> 00:15:12,394
the bookie two, I still have all the segments

236
00:15:12,442 --> 00:15:15,440
in the other bookies, so I haven't lost any data.

237
00:15:15,990 --> 00:15:19,246
You may need to store a big amount of data and retain

238
00:15:19,278 --> 00:15:22,180
it for a long time. Depending on your use case,

239
00:15:22,630 --> 00:15:26,370
you need to read the old messages that were produced days

240
00:15:26,440 --> 00:15:30,258
or months ago. And with Prusa you can offload

241
00:15:30,354 --> 00:15:33,618
those messages to an external storage.

242
00:15:33,794 --> 00:15:37,206
So instead of using our fast and expensive disk in

243
00:15:37,228 --> 00:15:41,014
the cluster nodes, we can rather leverage the use of third party

244
00:15:41,062 --> 00:15:44,650
cloud storage systems, moving this data into a more

245
00:15:44,720 --> 00:15:48,314
cost effective storage tier and this is

246
00:15:48,352 --> 00:15:52,298
transparent for the consumers. So if you need to replay a topic,

247
00:15:52,474 --> 00:15:56,478
some messages will be read from the cloud storage and other

248
00:15:56,564 --> 00:15:59,806
will be read from the bookie. But the consumer doesn't see

249
00:15:59,828 --> 00:16:03,386
this, it's transparent. And this offloading

250
00:16:03,418 --> 00:16:06,838
is facilitated by the segment based architecture I presented.

251
00:16:06,954 --> 00:16:10,606
This architecture allows older data segments to be offloaded

252
00:16:10,638 --> 00:16:14,066
seamlessly. So we've seen what happens when you lose a

253
00:16:14,088 --> 00:16:17,858
node, but what if you lose a region

254
00:16:17,954 --> 00:16:22,162
or a data center? That's where georeplication

255
00:16:22,226 --> 00:16:26,578
steps in. Georeplication provides disaster recovery.

256
00:16:26,674 --> 00:16:30,122
So you have several clusters deployed in different regions or

257
00:16:30,176 --> 00:16:33,782
different data centers, and if you lose a region, you can recover

258
00:16:33,846 --> 00:16:37,510
from it. Pulsar can replicate the data to different regions

259
00:16:37,590 --> 00:16:41,294
automatically in a bi directional way. And this is

260
00:16:41,332 --> 00:16:45,150
a built in feature. So setting this up is basically about configuration.

261
00:16:45,730 --> 00:16:48,974
Now I'd like to introduce another very cool feature of

262
00:16:49,012 --> 00:16:52,926
prosor multi tenancy. Multitenancy allows

263
00:16:52,958 --> 00:16:56,562
different departments or teams within an organization to

264
00:16:56,616 --> 00:17:00,770
share a browser cluster while keeping their data isolated.

265
00:17:01,270 --> 00:17:05,066
So multitenancy helps applications work in a shared environment

266
00:17:05,198 --> 00:17:08,514
by providing structure, security and resource

267
00:17:08,562 --> 00:17:12,546
isolation. The benefits include easier management,

268
00:17:12,658 --> 00:17:17,266
as you only need to operate a single cluster for multiple teams.

269
00:17:17,458 --> 00:17:20,822
Plus, sharing resources can lead to a significant

270
00:17:20,886 --> 00:17:25,094
reduction in the number of nodes in your infrastructure,

271
00:17:25,222 --> 00:17:28,774
which can save on cost. This is not a hack

272
00:17:28,822 --> 00:17:32,062
or another layer on top of Prusa. Pulsar is

273
00:17:32,116 --> 00:17:35,550
designed for that. This is a built in feature.

274
00:17:36,050 --> 00:17:40,202
So now you could say, well, Julian Prusan

275
00:17:40,266 --> 00:17:44,146
has impressive features, really impressive features. But you know,

276
00:17:44,248 --> 00:17:47,630
in my company I have an existing software ecosystem,

277
00:17:47,710 --> 00:17:51,042
right? I'm sending or consuming messages with

278
00:17:51,096 --> 00:17:54,926
Kafka and RabbitMQ. I have a bunch of microservices

279
00:17:55,038 --> 00:17:58,854
with thousands or maybe millions of lines, and I can't rewrite all

280
00:17:58,892 --> 00:18:02,038
of them. Well, I have good news for you.

281
00:18:02,204 --> 00:18:06,134
Busan has a high level of compatible, and I'll explain that

282
00:18:06,172 --> 00:18:09,990
right now. Messaging and streaming involve clients

283
00:18:10,150 --> 00:18:14,330
and a broker, and they communicate using a protocol.

284
00:18:14,910 --> 00:18:18,166
Pulsar provides its own banner protocol,

285
00:18:18,358 --> 00:18:21,606
but with the addition of protocol handlers,

286
00:18:21,798 --> 00:18:24,670
Pulsar becomes compatible with scarca clients,

287
00:18:25,090 --> 00:18:28,590
RabbitMQ clients, and MQTT clients.

288
00:18:28,930 --> 00:18:32,286
So by leveraging your existing apps, you can avoid the

289
00:18:32,308 --> 00:18:36,030
need to rewrite everything and ensure a seamless migration

290
00:18:36,110 --> 00:18:39,618
path. Prusa also benefits from a great

291
00:18:39,704 --> 00:18:43,346
ecosystem throughout client applications using the

292
00:18:43,368 --> 00:18:46,766
Pulsar native protocol, and benefit from all the features.

293
00:18:46,878 --> 00:18:50,674
You have many Pulsar client libraries available. You will surely

294
00:18:50,722 --> 00:18:54,422
find one for your favorite language, and don't hesitate to check

295
00:18:54,476 --> 00:18:58,506
out hub streamnative IO, where you'll find a

296
00:18:58,528 --> 00:19:01,510
wide ecosystem of connectors, libraries,

297
00:19:01,590 --> 00:19:05,066
protocol handlers, et cetera. When it comes to

298
00:19:05,088 --> 00:19:08,780
choosing a technology, open source is key.

299
00:19:09,390 --> 00:19:12,942
Some of the benefits of choosing an open source technology are

300
00:19:13,076 --> 00:19:16,990
sustainability, avoiding, vendor locked in

301
00:19:17,140 --> 00:19:21,114
and community support. And Pulsar of course is open source

302
00:19:21,162 --> 00:19:24,966
because it's Apache person. All these features are presented

303
00:19:25,018 --> 00:19:28,834
are available in open source, so if you download Pulsar you

304
00:19:28,872 --> 00:19:32,626
have all of them. This is great because you don't depend on

305
00:19:32,648 --> 00:19:36,002
a specific vendor. You're free to call a vendor to provide

306
00:19:36,056 --> 00:19:40,226
a Prusa as a service like stream native, or you can manage a Prusa

307
00:19:40,258 --> 00:19:43,606
cluster by yourself. There is no vendor unlock in

308
00:19:43,788 --> 00:19:47,494
now some data on the Prusa open source community there is

309
00:19:47,532 --> 00:19:51,210
more than 600 contributors to PruSa and they are more making

310
00:19:51,280 --> 00:19:54,886
contributions to the ecosystem surrounding around perusal.

311
00:19:55,078 --> 00:19:59,034
The entire pursuit code base is growing year over year

312
00:19:59,232 --> 00:20:02,714
and you can discuss with the community on Slack. The number

313
00:20:02,752 --> 00:20:06,414
of Slack members reach 10,000. That's so many

314
00:20:06,452 --> 00:20:09,966
people who can help you. Additionally, there are now

315
00:20:10,068 --> 00:20:13,742
thousands of organizations using purson. And now

316
00:20:13,796 --> 00:20:17,658
let's take a look at the history of person at a glance.

317
00:20:17,834 --> 00:20:21,154
Person was developed by Yahoo in 2012 as their

318
00:20:21,192 --> 00:20:25,650
cloud messaging service. It then went open source in 2016.

319
00:20:26,250 --> 00:20:29,622
By 2018, Prusan has graduated to a top

320
00:20:29,676 --> 00:20:34,034
level appaship project and since 2019

321
00:20:34,162 --> 00:20:37,986
there's been a surge in the Prusa currency growth with rapid

322
00:20:38,018 --> 00:20:42,022
adoption and an increasing number of contributors. It's worth

323
00:20:42,076 --> 00:20:45,258
mentioning that Prusa has been in production for over

324
00:20:45,344 --> 00:20:49,190
ten years. This means that it's tested,

325
00:20:49,270 --> 00:20:52,858
it's proven to be robust and mature. So here is a

326
00:20:52,864 --> 00:20:56,842
quick recap. Prusan is a unified messaging and streaming

327
00:20:56,906 --> 00:21:00,318
platform handling both patterns, both semantics at the

328
00:21:00,324 --> 00:21:04,180
same time, so you can have only one platform to manage.

329
00:21:04,550 --> 00:21:07,870
Prusan is doing great at both scaling and being elastic,

330
00:21:07,950 --> 00:21:10,850
and provides three levels of elasticity.

331
00:21:11,270 --> 00:21:14,322
Prusa ensures the durability of the data and can

332
00:21:14,376 --> 00:21:18,130
offload to external, cheap and unlimited storage.

333
00:21:18,630 --> 00:21:21,894
Prusan has durable application built in which is great for

334
00:21:21,932 --> 00:21:25,074
disaster recovery. Pulsar is natively

335
00:21:25,122 --> 00:21:29,090
multi tenant, pursuant is compatible with the software ecosystem

336
00:21:29,170 --> 00:21:32,310
and all these great features are available as open

337
00:21:32,380 --> 00:21:35,480
source. So you have no vendor unlock in.

338
00:21:35,930 --> 00:21:39,538
All right, that's the end of my talk. I hope you enjoyed

339
00:21:39,714 --> 00:21:43,166
watching for attending. If you have any question, I'll be

340
00:21:43,188 --> 00:21:47,006
very happy to answer. Feel free to scan this QR code on the left to

341
00:21:47,028 --> 00:21:50,090
contact me and to get access to additional resources.

342
00:21:50,250 --> 00:21:54,826
Feel free to try out pulsar by downloading it or by installing the operators.

343
00:21:54,938 --> 00:21:58,590
And you're welcome to join the Apache Pulsar Slack channel.

344
00:21:58,740 --> 00:21:59,260
See you there.

