1
00:02:01,140 --> 00:02:04,720
Hi. Welcome, everyone. In this session today, we'll discuss

2
00:02:04,790 --> 00:02:08,256
about building confidence through chaos engineering on AWS. So,

3
00:02:08,278 --> 00:02:12,180
we'll learn what chaos engineering is and what it isn't

4
00:02:12,340 --> 00:02:15,844
and what value is of chaos engineering.

5
00:02:15,972 --> 00:02:19,864
And how can we get started with chaos engineering within your own

6
00:02:19,902 --> 00:02:23,304
firms? But more importantly, I will show you

7
00:02:23,342 --> 00:02:26,812
how you can combine the power of chaos engineering and

8
00:02:26,866 --> 00:02:30,076
continuous resilience and build a process that you

9
00:02:30,098 --> 00:02:34,636
can scale chaos engineering across your organization in

10
00:02:34,658 --> 00:02:38,300
a controlled and secure way so that your developers

11
00:02:38,380 --> 00:02:42,092
and engineers with secure,

12
00:02:42,156 --> 00:02:45,932
reliable, and robust workloads that ultimately

13
00:02:46,076 --> 00:02:48,850
lead to great customer experience. Right.

14
00:02:49,560 --> 00:02:53,376
My name is Narender Gakka. I'm a solutions architect at AWS,

15
00:02:53,488 --> 00:02:57,204
and my area of expertise is resilience as well.

16
00:02:57,322 --> 00:03:01,110
So, let's get started. So, the first,

17
00:03:01,480 --> 00:03:04,904
let's look at our agenda. So, first, I'll introduce you to

18
00:03:04,942 --> 00:03:08,744
chaos engineering, and let's see what it

19
00:03:08,782 --> 00:03:12,536
is and more importantly, what it isn't. Also, and I

20
00:03:12,558 --> 00:03:16,252
will also take you through the various aspects when you are

21
00:03:16,386 --> 00:03:19,832
thinking about prerequisites for chaos

22
00:03:19,896 --> 00:03:23,724
engineering and what you need to get started in your own

23
00:03:23,762 --> 00:03:27,184
workloads and in your environments. We'll then dive deep into

24
00:03:27,222 --> 00:03:30,784
the continuous resilience and why continuous resilience is

25
00:03:30,822 --> 00:03:34,844
so important. When we are thinking about resilient applications

26
00:03:34,892 --> 00:03:40,496
on AWS and combined

27
00:03:40,528 --> 00:03:44,372
with chaos engineering and continuous resilience, I will also

28
00:03:44,426 --> 00:03:48,192
take you through our Chaos Engineering and continuous resilience

29
00:03:48,256 --> 00:03:52,392
program that we users to help our customers to build

30
00:03:52,446 --> 00:03:55,720
chaos engineering practices and programs

31
00:03:57,020 --> 00:03:59,880
that can scale across their organizations.

32
00:04:00,940 --> 00:04:04,428
And at last, I will also share with you some

33
00:04:04,594 --> 00:04:07,804
resources and great workshops, which we have so

34
00:04:07,842 --> 00:04:11,804
that you can get started with Chaos Engineering on AWS on

35
00:04:11,842 --> 00:04:13,710
your own, basically.

36
00:04:15,680 --> 00:04:19,344
So when we are thinking about Chaos Engineering, it's not

37
00:04:19,382 --> 00:04:22,944
really a new concept. It has been

38
00:04:22,982 --> 00:04:26,176
there for over a decade now. And there are many companies

39
00:04:26,278 --> 00:04:29,536
that have already successfully adopted

40
00:04:29,568 --> 00:04:32,884
and embraced the Chaos engineering and have taken the

41
00:04:32,922 --> 00:04:37,716
mechanisms in trying to find out what

42
00:04:37,738 --> 00:04:40,704
we call as known unknowns.

43
00:04:40,832 --> 00:04:44,456
I mean, these are things that we are aware of,

44
00:04:44,558 --> 00:04:48,500
but not don't fully understand in our systems.

45
00:04:48,580 --> 00:04:52,728
It could be weaknesses within our system or resilience issues and also

46
00:04:52,814 --> 00:04:56,604
chase the unknown unknowns, which are the things that we

47
00:04:56,642 --> 00:04:59,710
neither are aware of nor do we fully understand.

48
00:05:01,920 --> 00:05:05,804
And through chaos engineering, these various companies were basically able

49
00:05:05,842 --> 00:05:08,988
to find deficiencies within their environments

50
00:05:09,164 --> 00:05:12,956
and prevent the users on what we call it AWS,

51
00:05:13,068 --> 00:05:16,780
large scale events, and therefore ultimately

52
00:05:16,860 --> 00:05:19,090
have a better experience for their customers.

53
00:05:21,700 --> 00:05:25,520
And yet, when we are talking about or when we are thinking about chaos engineering,

54
00:05:25,600 --> 00:05:28,736
in many ways, it's not how we see Chaos engineering.

55
00:05:28,768 --> 00:05:32,632
Right? There is still a perception that chaos engineering is that thing

56
00:05:32,686 --> 00:05:36,232
which blows up production and

57
00:05:36,286 --> 00:05:40,072
nor where we randomly just shun down things within

58
00:05:40,126 --> 00:05:43,992
an environment. That is not what chaos engineering is about.

59
00:05:44,046 --> 00:05:47,868
Right. It's not just about blowing up production or

60
00:05:47,954 --> 00:05:51,292
randomly stopping things or removing things.

61
00:05:51,426 --> 00:05:54,552
But when we are thinking about chaos engineering at AWS,

62
00:05:54,616 --> 00:05:57,680
we should look at it from a much different perspective.

63
00:05:58,820 --> 00:06:03,116
And many of you probably have seen shared

64
00:06:03,148 --> 00:06:06,172
responsibility or shared responsibility model for AWS,

65
00:06:06,236 --> 00:06:09,860
for security. This basically is for

66
00:06:10,010 --> 00:06:12,832
resilience. And there are two sections,

67
00:06:12,896 --> 00:06:16,596
the blue and orange pit in the

68
00:06:16,618 --> 00:06:19,904
resilience of cloud. We at AWS

69
00:06:19,952 --> 00:06:23,576
are responsible for the resilience of

70
00:06:23,598 --> 00:06:26,676
the facilities, for example the network, the storage,

71
00:06:26,868 --> 00:06:30,584
the networking or the database aspects. These are

72
00:06:30,622 --> 00:06:33,996
basically the services which you consume, but you as a

73
00:06:34,018 --> 00:06:37,452
customer, you are responsible of how and what

74
00:06:37,506 --> 00:06:40,750
services you use, where you place them.

75
00:06:41,440 --> 00:06:44,616
Think for example for your workloads. Think about zonal

76
00:06:44,648 --> 00:06:48,352
services like EC two, where you place your data and

77
00:06:48,406 --> 00:06:51,600
how you fail over if something happens within your environment.

78
00:06:51,940 --> 00:06:55,776
But also think about the challenges that come up when you are

79
00:06:55,878 --> 00:06:58,930
looking at a shared responsibility model.

80
00:07:00,040 --> 00:07:04,932
So how can you make sure if the service fails that you are consuming that

81
00:07:05,066 --> 00:07:08,740
in an orange space that your workload is resilient?

82
00:07:09,400 --> 00:07:13,444
Right. What happens if an availability zone

83
00:07:13,492 --> 00:07:17,496
goes down at AWS? Is your workload or an application able

84
00:07:17,518 --> 00:07:21,384
to recover from those things? How do you know if your

85
00:07:21,582 --> 00:07:24,456
workloads can fail over to another AZ?

86
00:07:24,568 --> 00:07:27,644
And this is where chaos engineering comes

87
00:07:27,682 --> 00:07:31,116
into play and help you with those aspects. So when

88
00:07:31,138 --> 00:07:34,780
you are thinking about workloads that you are running in the blue,

89
00:07:38,980 --> 00:07:42,496
what you can influence in the primary dependency that you're consuming in

90
00:07:42,518 --> 00:07:45,296
AWS, if you're using EC two,

91
00:07:45,478 --> 00:07:48,636
if you're using lambda, if you're using sqs, if you're

92
00:07:48,668 --> 00:07:52,320
using services like caching, services like elasticache,

93
00:07:52,480 --> 00:07:56,928
these are the services that you can impact with chaos engineering

94
00:07:56,944 --> 00:08:01,380
in a safe and controlled way. And you can also figure out mechanisms

95
00:08:01,720 --> 00:08:05,172
on how your components within your application can gracefully

96
00:08:05,236 --> 00:08:07,050
fail over to another service.

97
00:08:10,540 --> 00:08:14,204
Sorry. So when we are thinking about

98
00:08:14,242 --> 00:08:17,644
chaos engineering, what it provides you is

99
00:08:17,762 --> 00:08:21,160
more like improved operational

100
00:08:21,240 --> 00:08:24,776
readiness. Because your teams with chaos engineering

101
00:08:24,808 --> 00:08:29,280
will get trained on what to do if a service fails.

102
00:08:29,620 --> 00:08:33,024
And you will have mechanisms in place to be able to

103
00:08:33,062 --> 00:08:37,040
fail over automatically. And you will also have great

104
00:08:37,110 --> 00:08:41,110
observability in place because you will realize that

105
00:08:41,480 --> 00:08:44,736
by doing case engineering you will realize what is missing

106
00:08:44,768 --> 00:08:48,676
within your observability, which you currently have and what

107
00:08:48,698 --> 00:08:52,244
you haven't seen. And when you are running these experiments

108
00:08:52,292 --> 00:08:56,116
in a controlled way, you'll continuously

109
00:08:56,148 --> 00:08:59,640
improve the observability part as well. And ultimately

110
00:09:00,140 --> 00:09:03,688
you will build resilience so that

111
00:09:03,774 --> 00:09:07,740
the workloads which you build will have more resilient on AWS.

112
00:09:08,480 --> 00:09:11,550
And when you're thinking about all of this put together,

113
00:09:12,080 --> 00:09:15,372
what it does lead to is of course, a happy customer

114
00:09:15,506 --> 00:09:19,324
and a better application. Right? So that's what chaos

115
00:09:19,372 --> 00:09:23,020
engineering is about, that it's all about building resilient

116
00:09:23,180 --> 00:09:27,090
workloads that ultimately leads to great customer experience.

117
00:09:29,640 --> 00:09:32,848
And so when you think about chaos engineering,

118
00:09:33,024 --> 00:09:37,264
it's all about building controlled experiments.

119
00:09:37,392 --> 00:09:40,596
And if we already know that an

120
00:09:40,618 --> 00:09:44,036
experiment will fail, we're not going to run it at experiment because we

121
00:09:44,058 --> 00:09:47,812
already know why that fails. And there is no point of running that experiment.

122
00:09:47,876 --> 00:09:51,928
It's rather you invest time in fixing that issue. So we're

123
00:09:51,934 --> 00:09:55,724
not going to run that experiment. And if we know that

124
00:09:55,762 --> 00:09:59,496
we're going to inject a fault and that fault will trigger

125
00:09:59,528 --> 00:10:03,448
a bug that brings

126
00:10:03,464 --> 00:10:07,004
our system down, we're also not going to run the experiments because we already

127
00:10:07,042 --> 00:10:10,496
know what happens when you trigger that bug. It's better to go and

128
00:10:10,518 --> 00:10:13,936
fix that bug. So what we want to

129
00:10:13,958 --> 00:10:16,892
make sure is if we have an experiment,

130
00:10:17,036 --> 00:10:20,064
that by definition that experiment

131
00:10:20,192 --> 00:10:23,956
should be tolerated by the system and also should

132
00:10:23,978 --> 00:10:28,084
be fail safe, that it doesn't lead you

133
00:10:28,122 --> 00:10:29,830
to issues.

134
00:10:33,640 --> 00:10:38,672
And many of you might have a similar workload

135
00:10:38,736 --> 00:10:42,656
with a similar architecture wherein you have the external

136
00:10:42,768 --> 00:10:46,456
DNS pointing to your load balancer, where you have a service running which is

137
00:10:46,478 --> 00:10:50,356
getting data and customer data from either cache

138
00:10:50,388 --> 00:10:53,704
or a database, depending on your data

139
00:10:53,742 --> 00:10:57,064
freshness, et cetera. But when you're thinking about it,

140
00:10:57,102 --> 00:11:02,300
let's say you're using redis on EC two or elasticache,

141
00:11:02,880 --> 00:11:06,750
what your confidence level if the

142
00:11:07,120 --> 00:11:10,456
redis fails? Right? What happens if the redis

143
00:11:10,488 --> 00:11:14,224
fails? Do you have mechanisms in place to make sure that your

144
00:11:14,262 --> 00:11:18,176
database does not get fully overrun by all

145
00:11:18,198 --> 00:11:21,840
these requests which are not being served from the cache suddenly?

146
00:11:23,000 --> 00:11:26,356
Or what

147
00:11:26,378 --> 00:11:29,952
if you think about the latency that suddenly gets injected

148
00:11:30,096 --> 00:11:34,084
between your two

149
00:11:34,122 --> 00:11:37,604
microservices and you create a retry

150
00:11:37,652 --> 00:11:38,730
storm? Right?

151
00:11:42,460 --> 00:11:45,930
Did you have mechanisms to mitigate such an issue?

152
00:11:47,020 --> 00:11:50,024
What about the back off and jitter,

153
00:11:50,072 --> 00:11:52,830
et cetera? And also,

154
00:11:54,160 --> 00:11:57,192
let's assume that you have, let's say, cascading failure,

155
00:11:57,256 --> 00:12:01,116
that everything in an availability goes down. Are you

156
00:12:01,138 --> 00:12:04,460
confident that you can either fail over to a different availability

157
00:12:04,540 --> 00:12:08,496
zone to one another and think about impacts that

158
00:12:08,518 --> 00:12:11,856
you might have on a regional service? That what

159
00:12:11,878 --> 00:12:15,344
is your confidence if the whole region, the entire region

160
00:12:15,392 --> 00:12:19,030
basically goes down.

161
00:12:19,400 --> 00:12:22,500
Is your service able to recover

162
00:12:23,720 --> 00:12:27,512
in another region within the given

163
00:12:27,566 --> 00:12:31,172
sla of your application? Right. So what is your confidence

164
00:12:31,236 --> 00:12:35,160
level with the current architecture that you have?

165
00:12:35,230 --> 00:12:39,230
Basically, do you have those runbooks, playbooks which will let you

166
00:12:39,760 --> 00:12:43,768
do this cross region or a cross easy failover seamlessly?

167
00:12:43,864 --> 00:12:46,350
And can you run through them? Right.

168
00:12:48,160 --> 00:12:51,820
And so when you're thinking about like chaos engineering,

169
00:12:54,340 --> 00:12:57,436
when we are thinking about the services that we build on a daily

170
00:12:57,468 --> 00:13:01,232
basis, they're all based on trade offs that we have

171
00:13:01,286 --> 00:13:04,100
every single day, right? So those trade offs,

172
00:13:07,560 --> 00:13:10,470
we basically all want to build great,

173
00:13:11,000 --> 00:13:12,580
awesome workloads.

174
00:13:14,360 --> 00:13:17,640
But the reality is we are under pressure that

175
00:13:17,710 --> 00:13:21,480
a certain budget, we can only users certain budget,

176
00:13:22,380 --> 00:13:25,816
and that there are certain time that we need to deliver. We have

177
00:13:25,838 --> 00:13:29,116
a time constraints as well, and we

178
00:13:29,138 --> 00:13:32,424
need to also maintain and get those certain features released

179
00:13:32,472 --> 00:13:35,292
on time. But in a distributed system,

180
00:13:35,426 --> 00:13:39,544
there is no way that every single person understands

181
00:13:39,592 --> 00:13:43,452
the hundreds or many microservices

182
00:13:43,516 --> 00:13:46,892
that we communicate with each other. And ultimately,

183
00:13:47,036 --> 00:13:50,352
what happens if think

184
00:13:50,406 --> 00:13:54,196
that I'm depending on a soft dependency, where someone suddenly changes a

185
00:13:54,218 --> 00:13:58,068
code, that becomes a hard dependency. And what happens

186
00:13:58,234 --> 00:14:01,910
is that you suddenly have an event.

187
00:14:03,720 --> 00:14:07,684
And when you're thinking about these events, usually they happen. You're somewhere

188
00:14:07,732 --> 00:14:11,368
in a restaurant or maybe somewhere outside, and you get called

189
00:14:11,454 --> 00:14:15,290
at an odd hour and everybody runs and tries to fix that

190
00:14:16,140 --> 00:14:19,852
issue and bring the system back up. And the challenge with

191
00:14:19,986 --> 00:14:23,500
such a system is that

192
00:14:23,570 --> 00:14:27,484
once you go back to business as usual, you might get

193
00:14:27,522 --> 00:14:30,270
that same challenge again. Right?

194
00:14:35,140 --> 00:14:38,368
And then it's not because we don't want

195
00:14:38,374 --> 00:14:42,016
to fix it, but because the

196
00:14:42,038 --> 00:14:45,472
good intentions, they don't work. That's what we say at aws,

197
00:14:45,536 --> 00:14:48,836
right? You need mechanisms which come

198
00:14:48,858 --> 00:14:52,944
into play, and those mechanisms can be built using chaos

199
00:14:52,992 --> 00:14:55,220
engineering and the continuous resilience.

200
00:14:58,460 --> 00:15:01,784
Now, as I mentioned in the beginning, that there are

201
00:15:01,822 --> 00:15:05,636
many companies that have already adopted chaos engineering,

202
00:15:05,668 --> 00:15:09,352
and there are so many verticals,

203
00:15:09,496 --> 00:15:12,956
these companies that have adopted chaos engineering, and some of

204
00:15:12,978 --> 00:15:16,300
them already started quite

205
00:15:16,370 --> 00:15:20,216
early and have seen tremendous benefits in the overall improvement of resilience

206
00:15:20,248 --> 00:15:23,796
within their workloads. These are some of the industries

207
00:15:23,848 --> 00:15:27,884
what we see on the screen, and there are many case studies

208
00:15:27,932 --> 00:15:31,104
or customer stories which are in that link. So please feel

209
00:15:31,142 --> 00:15:34,916
free to go through them on how they were adopted if you belong to

210
00:15:34,938 --> 00:15:38,356
those industries, how they have leveraged the chaos engineering and improved their

211
00:15:38,378 --> 00:15:39,780
architectures overall.

212
00:15:43,400 --> 00:15:47,204
There are many customers that will adopt chaos engineering

213
00:15:47,332 --> 00:15:50,712
in the next years to come by. And there is a great study

214
00:15:50,766 --> 00:15:53,956
by Gartner that was done for the infrastructure

215
00:15:53,988 --> 00:15:57,208
and operations leaders guide. That said that

216
00:15:57,374 --> 00:16:01,256
40% of companies will adopt chaos engineering

217
00:16:01,288 --> 00:16:05,052
in next year alone. And they are doing that because

218
00:16:05,106 --> 00:16:08,812
when they think they can increase customer experience by almost

219
00:16:08,866 --> 00:16:12,688
20%, and think about how many more happy customers

220
00:16:12,854 --> 00:16:16,204
you're going to have with such a number. It's a significant

221
00:16:16,252 --> 00:16:20,288
number, this 20% is. So let's get the prerequisites now

222
00:16:20,374 --> 00:16:24,580
on how you can get started with chaos engineering.

223
00:16:27,720 --> 00:16:30,948
Okay, let's get some of the prerequisites on how you

224
00:16:30,954 --> 00:16:33,984
can get started. So first you need like basic

225
00:16:34,032 --> 00:16:38,132
monitoring, and if you already have observability

226
00:16:38,276 --> 00:16:42,296
already, which is a great starting point, and then you

227
00:16:42,318 --> 00:16:46,250
need to have organizational awareness as well.

228
00:16:47,020 --> 00:16:50,476
And third is that you need to think about what sort

229
00:16:50,498 --> 00:16:53,916
of real world events or faults we

230
00:16:53,938 --> 00:16:58,060
are injecting into our environment. And then

231
00:16:58,130 --> 00:17:02,184
fourth is then, of course, once we find those faults

232
00:17:02,232 --> 00:17:05,880
within the environment, we find a deficiency,

233
00:17:06,040 --> 00:17:09,164
right. We remediate, we actively commit

234
00:17:09,212 --> 00:17:12,892
ourselves, have the resources to basically go and fix those

235
00:17:13,046 --> 00:17:16,916
so that it improves either security or the resiliency of

236
00:17:16,938 --> 00:17:20,004
your workloads. There's no point finding it, but not fixing it, right?

237
00:17:20,042 --> 00:17:23,540
So that is the fourth prerequisite.

238
00:17:25,560 --> 00:17:29,120
So when you're thinking about metrics,

239
00:17:29,200 --> 00:17:32,076
many of you really have great metrics.

240
00:17:32,208 --> 00:17:35,512
If you're using AWS already, you have the Cloudwatch integration and you

241
00:17:35,566 --> 00:17:38,940
already have the metrics coming into. But in chaos engineering,

242
00:17:39,280 --> 00:17:43,052
we call metrics as known unknowns. So these are the things

243
00:17:43,106 --> 00:17:46,270
that we already aware and fully understand.

244
00:17:47,760 --> 00:17:50,944
So basically we call them known knowns, right? So these are the things

245
00:17:50,982 --> 00:17:54,976
that we already are aware of and we fully understand. And when

246
00:17:54,998 --> 00:17:58,268
you're thinking about metrics, for example, like it's

247
00:17:58,284 --> 00:18:00,560
cpu percentage, it's memory,

248
00:18:01,460 --> 00:18:05,316
and it's all great, but in distributed system,

249
00:18:05,498 --> 00:18:09,376
you're going to look at many, many different dashboards and metrics

250
00:18:09,408 --> 00:18:12,992
to figure out what's going on within the environment,

251
00:18:13,056 --> 00:18:16,948
because it doesn't give you a comprehensive view, each gives its own

252
00:18:17,114 --> 00:18:20,500
view, but you're going to look at many, many different dashboards.

253
00:18:20,580 --> 00:18:24,696
So when you are starting with chaos engineering, many times when

254
00:18:24,718 --> 00:18:28,316
we are running, like, first experiments, even if we are trying to make sure

255
00:18:28,338 --> 00:18:31,944
that we are seeing everything, we realize we can't

256
00:18:31,992 --> 00:18:35,100
see it. And this is what leads to the observability.

257
00:18:35,680 --> 00:18:39,036
So observability helps us find basically

258
00:18:39,138 --> 00:18:42,824
that needle in a haystack. By collating

259
00:18:42,872 --> 00:18:46,210
all the information, we start looking at like highest level

260
00:18:46,660 --> 00:18:50,416
at our baseline instead of looking

261
00:18:50,438 --> 00:18:53,664
at a particular graph. And even

262
00:18:53,702 --> 00:18:57,236
if we have absolutely no idea what's going on,

263
00:18:57,338 --> 00:19:01,396
we're going to understand where we are. So basically at a high level we know

264
00:19:01,578 --> 00:19:05,060
what is our application health, what sort of customer

265
00:19:05,130 --> 00:19:08,856
interaction we are having, et cetera, so that we can drill down all

266
00:19:08,878 --> 00:19:12,564
the way to tracing. Like we can use the services like AWS

267
00:19:12,612 --> 00:19:16,170
X ray and understand it. But there are also options,

268
00:19:17,340 --> 00:19:21,116
there are many open source options and if

269
00:19:21,138 --> 00:19:25,068
you already use them, that's perfectly fine. Aws, well right. So when

270
00:19:25,074 --> 00:19:28,236
you're thinking about observability, this is the

271
00:19:28,258 --> 00:19:31,792
key. Observability is based on what we say,

272
00:19:31,926 --> 00:19:37,136
there are three key pillars and you

273
00:19:37,158 --> 00:19:40,988
have, as we already mentioned the metrics, then you have the logging

274
00:19:41,084 --> 00:19:44,708
and then you have the tracing. Now what is important

275
00:19:44,874 --> 00:19:48,276
is why these three key are

276
00:19:48,458 --> 00:19:52,310
important is because we want to make sure that you embed for example

277
00:19:53,560 --> 00:19:56,884
metrics within your logs, so that if you're looking

278
00:19:56,922 --> 00:19:59,928
at a high level steady state that you might have,

279
00:20:00,094 --> 00:20:03,560
you want to drill in. And as soon as you get into a stage from

280
00:20:03,630 --> 00:20:07,240
tracing to log that you see what's going on

281
00:20:07,310 --> 00:20:10,936
and can also correlate between all those components

282
00:20:10,968 --> 00:20:14,380
end to end. And so at that point you can understand

283
00:20:14,450 --> 00:20:16,110
where your application is.

284
00:20:21,310 --> 00:20:25,146
So if we take can example of the observability.

285
00:20:25,338 --> 00:20:27,790
So when we are looking at this graph,

286
00:20:29,010 --> 00:20:32,334
even for example any person who has

287
00:20:32,372 --> 00:20:35,950
absolutely no idea about what this workload is

288
00:20:36,100 --> 00:20:40,306
and sees there are few issues here, like if

289
00:20:40,328 --> 00:20:43,506
you look at the spikes there and

290
00:20:43,528 --> 00:20:46,818
you're going to say okay, something happened here basically.

291
00:20:46,984 --> 00:20:50,610
And if we would drill down we would see that we have a process

292
00:20:50,680 --> 00:20:54,034
which can out of control or there is a cpu spike,

293
00:20:54,082 --> 00:20:54,680
right?

294
00:20:56,730 --> 00:21:00,102
And every one of you is

295
00:21:00,156 --> 00:21:03,290
able to look at the graph down here and say wait a minute,

296
00:21:04,350 --> 00:21:07,946
why did the disk utilization drop? And if you

297
00:21:07,968 --> 00:21:11,626
drill down you will realize that it

298
00:21:11,648 --> 00:21:15,870
had an issue with my Kubernetes cluster and the pod suddenly,

299
00:21:16,450 --> 00:21:20,270
right, the number of nodes suddenly start restarting

300
00:21:25,250 --> 00:21:28,960
and that leads to lot of

301
00:21:29,570 --> 00:21:33,346
500 errors. And as you know

302
00:21:33,368 --> 00:21:37,394
HTTP 500 obviously is not a good thing to do. So if we can

303
00:21:37,432 --> 00:21:41,222
correlate this, that is a good observability. That because

304
00:21:41,276 --> 00:21:44,760
of such and such issue, this is my end user experience.

305
00:21:48,090 --> 00:21:51,274
And if you want to provide developers with the

306
00:21:51,312 --> 00:21:54,822
aspects of understanding the interactions

307
00:21:54,886 --> 00:21:58,406
within the microservices and especially when you're

308
00:21:58,438 --> 00:22:01,606
thinking about like chaos engineering and experiments,

309
00:22:01,718 --> 00:22:05,854
you want them to understand what is the impact of

310
00:22:05,892 --> 00:22:09,630
this experiment is. And we shouldn't forget in the

311
00:22:09,700 --> 00:22:13,166
user experience and what users sees when

312
00:22:13,188 --> 00:22:16,202
you are running these experiments because if you're thinking about the baseline

313
00:22:16,266 --> 00:22:19,390
and we are running an experiment and the baseline

314
00:22:19,470 --> 00:22:22,962
doesn't move means that the customer is super happy because everything

315
00:22:23,016 --> 00:22:26,846
is green, it's all working. Even if when you are doing experiment,

316
00:22:26,958 --> 00:22:30,198
if everything is fine from the end user's perspective, that is a

317
00:22:30,204 --> 00:22:33,800
successful application or a reliable or zilliant application, right?

318
00:22:35,210 --> 00:22:38,946
And now that we understand the observability aspects,

319
00:22:38,978 --> 00:22:42,314
now basically we have seen what basic monitoring and observability is.

320
00:22:42,352 --> 00:22:46,426
Now let's actually move on to

321
00:22:46,608 --> 00:22:50,250
the next prerequisite, which is the organizational awareness.

322
00:22:53,310 --> 00:22:56,606
What we found is that when you are starting with

323
00:22:56,628 --> 00:23:00,698
a small team and you enable small team on chaos engineering,

324
00:23:00,794 --> 00:23:04,830
and they build common faults that can be injected across the organization

325
00:23:05,590 --> 00:23:09,234
and then able to decentralize the

326
00:23:09,272 --> 00:23:12,430
development teams on chaos engineering,

327
00:23:12,510 --> 00:23:17,506
that works fairly well. Now why

328
00:23:17,528 --> 00:23:20,870
is that? Why does a small team work? Well, if you're thinking about that,

329
00:23:20,940 --> 00:23:24,802
you have hundreds of, maybe depending on the scale

330
00:23:24,866 --> 00:23:28,326
and size of your organization, you might have hundreds if

331
00:23:28,348 --> 00:23:31,740
not thousands of development teams who are trying to

332
00:23:32,510 --> 00:23:36,426
build the application. There's no way that

333
00:23:36,608 --> 00:23:40,118
the central team will understand every single workload

334
00:23:40,294 --> 00:23:44,006
that is around you. And there is

335
00:23:44,048 --> 00:23:47,374
also no way that the central team will get the power to basically

336
00:23:47,492 --> 00:23:51,518
inject failures everywhere. But those development teams already

337
00:23:51,604 --> 00:23:55,374
have like IAM permission to access their environments and

338
00:23:55,412 --> 00:23:58,546
do things their own environments, rather than the central team doing the other way

339
00:23:58,568 --> 00:24:02,686
around. So it's much easier to help them run experiments

340
00:24:02,718 --> 00:24:06,466
than having a central team running it for. Right. So you decentralize the

341
00:24:06,488 --> 00:24:09,878
case engineering so that they can embrace it part of

342
00:24:09,884 --> 00:24:14,294
the development cycles itself. So that

343
00:24:14,332 --> 00:24:17,874
also helps basically with building customized experiments

344
00:24:17,922 --> 00:24:21,866
which is suitable for their own workload, which they are designing and building.

345
00:24:22,048 --> 00:24:25,450
And the key to all of this to work

346
00:24:25,520 --> 00:24:29,686
is having that executive sponsorship, the management sponsorship,

347
00:24:29,878 --> 00:24:34,006
that helps you make those resilience part of the journey of the software development

348
00:24:34,038 --> 00:24:37,614
lifecycle, and also shift the responsibility for

349
00:24:37,652 --> 00:24:41,198
resilience to those development teams who know their own application,

350
00:24:41,284 --> 00:24:44,078
their own piece of code better than anybody else.

351
00:24:44,244 --> 00:24:47,200
And then we think that these real world,

352
00:24:48,070 --> 00:24:51,906
they can also think about the real world failures and faults which this application

353
00:24:52,008 --> 00:24:55,780
can suffer or have dependency on.

354
00:25:00,330 --> 00:25:03,814
Now, what we see the real world when

355
00:25:03,852 --> 00:25:07,400
we say real world experiments, is that

356
00:25:10,490 --> 00:25:14,280
some of the key experiments are

357
00:25:14,990 --> 00:25:18,854
code and configuration errors. So think about the faults,

358
00:25:18,982 --> 00:25:22,390
the common faults you can inject when you are thinking about deployments,

359
00:25:22,470 --> 00:25:25,902
or think about experiments that you can cause and say, okay,

360
00:25:25,956 --> 00:25:29,722
well, do we even realize that we have a faulty deployment?

361
00:25:29,786 --> 00:25:33,418
Or do we see it within observability if my deployment fails,

362
00:25:33,514 --> 00:25:37,406
or it is leading to a customer's customer transaction to fail,

363
00:25:37,438 --> 00:25:40,946
et cetera, et cetera. So how

364
00:25:40,968 --> 00:25:44,306
do we do that? Experiments. And second

365
00:25:44,408 --> 00:25:47,090
is that when we are thinking about infrastructure,

366
00:25:48,710 --> 00:25:52,258
what if you have an EC two instance that fails within your environment?

367
00:25:52,434 --> 00:25:55,926
Suddenly in a microservices deployment you have an

368
00:25:55,948 --> 00:25:59,046
eks cluster where a load balancer doesn't pass the traffic to

369
00:25:59,068 --> 00:26:03,110
your, sorry, doesn't pass traffic,

370
00:26:03,930 --> 00:26:08,054
or able to mitigate such events? Are you able to mitigate such infrastructure

371
00:26:08,102 --> 00:26:11,514
events within your architecture? And what about

372
00:26:11,552 --> 00:26:14,974
the data and state? Right, this is also a critical resource for your

373
00:26:15,012 --> 00:26:18,350
application. This is not just about cache drift,

374
00:26:18,850 --> 00:26:22,880
but what if suddenly your database runs out of,

375
00:26:23,330 --> 00:26:27,170
let's say disk space or out of memory? Do we have

376
00:26:27,320 --> 00:26:30,722
mechanisms to not only just

377
00:26:30,776 --> 00:26:34,626
first detect it and inform you that this happened, but also how

378
00:26:34,648 --> 00:26:38,886
do you automatically mitigate that so that your

379
00:26:38,908 --> 00:26:42,374
application is working resiliently, right? And then of

380
00:26:42,412 --> 00:26:45,800
course you have dependency where we have seen

381
00:26:46,490 --> 00:26:50,166
that. Do you basically understand

382
00:26:50,268 --> 00:26:53,434
the dependencies of your

383
00:26:53,472 --> 00:26:57,242
application with any third parties which you have? It could be maybe

384
00:26:57,296 --> 00:27:00,506
an identity provider or a third party API which

385
00:27:00,528 --> 00:27:03,946
your application consumes every time

386
00:27:03,968 --> 00:27:07,902
a user logs in, or let's say, does a transaction that

387
00:27:07,956 --> 00:27:11,886
you do understand those dependencies? And what happens if those

388
00:27:12,068 --> 00:27:15,326
suffer any issues? Do you have a mechanisms to

389
00:27:15,348 --> 00:27:19,154
first test it and also prove that your application is resilient enough

390
00:27:19,192 --> 00:27:22,740
to tolerate and can work without them as well?

391
00:27:23,350 --> 00:27:27,086
And then of course, although highly

392
00:27:27,198 --> 00:27:31,502
unlikely but technically feasible,

393
00:27:31,566 --> 00:27:35,266
that natural disasters, when we are thinking about

394
00:27:35,448 --> 00:27:38,962
maybe human errors, that something happened, the user does

395
00:27:39,016 --> 00:27:42,886
something, how do you, you know, how can you fail

396
00:27:42,918 --> 00:27:46,586
over, or how can you simulate those events and

397
00:27:46,608 --> 00:27:50,154
that too in a controlled way through

398
00:27:50,192 --> 00:27:54,106
the chaos engineering. Right. So these are some

399
00:27:54,128 --> 00:27:58,026
of the real world experiments which you can do with

400
00:27:58,048 --> 00:28:01,834
the chaos engineering. And then the last prerequisite,

401
00:28:01,882 --> 00:28:05,406
of course, is about making sure that when we

402
00:28:05,428 --> 00:28:09,220
are building a deficiency within our systems that

403
00:28:10,310 --> 00:28:14,766
could be related to the security or resilience, that we can go and basically remediate

404
00:28:14,798 --> 00:28:18,722
it, because it's basically

405
00:28:18,776 --> 00:28:22,038
worth nothing if you build new features, but our services

406
00:28:22,124 --> 00:28:25,634
is not available. Right. So we need to have that executive

407
00:28:25,682 --> 00:28:29,682
sponsorship as well, that we need to be able to prioritize

408
00:28:29,746 --> 00:28:33,290
these issues which come up through chaos engineering

409
00:28:34,190 --> 00:28:38,054
and basically fix them and improve the resilience of the architecture

410
00:28:38,182 --> 00:28:41,674
in a continuous fashion. So that basically

411
00:28:41,792 --> 00:28:45,710
now brings us to the continuous resolution,

412
00:28:46,450 --> 00:28:50,446
continuous resilience. So when

413
00:28:50,468 --> 00:28:53,070
we are thinking about the continuous resilience,

414
00:28:54,210 --> 00:28:57,714
resilience is not just one time thing, because every

415
00:28:57,752 --> 00:29:01,854
day you're building new features, releasing them to your customers and your architecture

416
00:29:01,902 --> 00:29:05,620
changes. So we need to think

417
00:29:07,030 --> 00:29:10,310
it should be part of everyday life when we are thinking about building

418
00:29:10,380 --> 00:29:14,182
resilient workloads right

419
00:29:14,236 --> 00:29:17,126
from the bottom to all the way to the application itself.

420
00:29:17,308 --> 00:29:20,706
And so continuous resilience is basically a lifecycle that

421
00:29:20,748 --> 00:29:24,460
helps us think about workload from a steady state point

422
00:29:25,870 --> 00:29:29,898
and steady state point of view, and work towards

423
00:29:29,984 --> 00:29:33,854
mitigating events like we just went through,

424
00:29:33,892 --> 00:29:37,310
from code configuration all the way to the very unlikely

425
00:29:37,650 --> 00:29:41,646
events of natural disasters, et cetera. And also we

426
00:29:41,668 --> 00:29:45,470
need to build safe experimentation of these within our pipelines,

427
00:29:46,370 --> 00:29:49,454
within our pipelines, and also actually outside our pipelines,

428
00:29:49,502 --> 00:29:53,860
because errors happen all the time. And not just when we provision new code

429
00:29:54,630 --> 00:29:58,586
and making sure that we also learn from the faults that surface

430
00:29:58,638 --> 00:30:03,080
during those experiments as well. And so

431
00:30:03,450 --> 00:30:07,046
when you take continuous resilience and chaos engineering and

432
00:30:07,068 --> 00:30:10,786
you put them together, that's what leads us to the Chaos Engineering

433
00:30:10,818 --> 00:30:12,680
and continuous Resilience program,

434
00:30:15,050 --> 00:30:16,040
which is,

435
00:30:19,550 --> 00:30:23,354
and the program that we have built over the last few years at AWS and

436
00:30:23,392 --> 00:30:26,574
have helped many customers run

437
00:30:26,612 --> 00:30:30,298
through it, which enabled them to basically, as I was saying earlier,

438
00:30:30,394 --> 00:30:33,946
to build a chaos engineering program with their own firm and scale

439
00:30:33,978 --> 00:30:37,426
it across various organizations and develop teams so that

440
00:30:37,448 --> 00:30:40,914
they can build controlled experiments within

441
00:30:40,952 --> 00:30:43,250
their environment and also improve resilience.

442
00:30:46,950 --> 00:30:50,610
Usually when we are talking about or when we are starting on this journey,

443
00:30:53,530 --> 00:30:57,718
we start with a game day that

444
00:30:57,724 --> 00:31:03,894
we prepare for, to start with game

445
00:31:03,932 --> 00:31:07,974
day, as you might think, where we are just running it for 2 hours session

446
00:31:08,022 --> 00:31:11,434
and we are checking if something was fine or not, especially when we are starting

447
00:31:11,472 --> 00:31:15,086
out. With chaos engineering, it's important to truly plan what

448
00:31:15,108 --> 00:31:19,002
we want to execute. So it's setting expectations

449
00:31:19,066 --> 00:31:21,200
as a big part of it.

450
00:31:22,210 --> 00:31:25,454
So the key to that, because you're going to need quite

451
00:31:25,492 --> 00:31:29,258
a few people that you want to invite, is project planning.

452
00:31:29,434 --> 00:31:32,754
And usually the first time when we do this, it might be between a week

453
00:31:32,792 --> 00:31:35,666
or three weeks that we are planning the game day and the various people that

454
00:31:35,688 --> 00:31:38,770
we want in a game day, like the KS champion,

455
00:31:39,370 --> 00:31:42,040
that will advocate the game day throughout the company.

456
00:31:42,970 --> 00:31:47,234
It could be the development teams. If there are site reliability

457
00:31:47,282 --> 00:31:50,262
engineers, sres, we're going to bring them in as well,

458
00:31:50,316 --> 00:31:54,082
observability and incident response teams. And then

459
00:31:54,156 --> 00:31:57,866
once we all have all the roles and responsibilities for the game day,

460
00:31:57,888 --> 00:32:01,526
we're going to think about what is it that we want to run experiments

461
00:32:01,558 --> 00:32:05,146
on. And when we are thinking about chaos engineering,

462
00:32:05,178 --> 00:32:08,880
it's not just about resilience. It can be about security

463
00:32:09,410 --> 00:32:13,310
or other aspects of the architecture as well. And so contribution

464
00:32:13,730 --> 00:32:17,874
is a list of what's important to

465
00:32:17,912 --> 00:32:20,862
you. That can be resilience, that can be availability,

466
00:32:21,006 --> 00:32:24,162
or that can be a security. It could also be

467
00:32:24,216 --> 00:32:27,602
durability for some of the customers. That's something

468
00:32:27,656 --> 00:32:30,886
which you can define. And then of

469
00:32:30,908 --> 00:32:34,166
course we want to make sure that there is a

470
00:32:34,188 --> 00:32:38,274
clear outcome of what we want to achieve with this chaos

471
00:32:38,322 --> 00:32:42,330
experiment. In our case, when we are starting

472
00:32:42,400 --> 00:32:46,426
out, what we actually prove to

473
00:32:46,448 --> 00:32:50,186
the organization and the sponsors is that we can run an experiment in a

474
00:32:50,208 --> 00:32:53,420
safe and controlled way without impacting the customers.

475
00:32:53,950 --> 00:32:57,598
That's the key. And then we take these learnings and share it,

476
00:32:57,684 --> 00:33:00,814
either if we found that something or not within our

477
00:33:00,852 --> 00:33:05,118
customers, to be able to make sure that the businesses unit understand

478
00:33:05,284 --> 00:33:09,074
how to mitigate these failures. If we found something

479
00:33:09,272 --> 00:33:12,670
or have the confidence that we are resilient to the faults,

480
00:33:12,750 --> 00:33:14,020
we inject it.

481
00:33:16,630 --> 00:33:19,974
So then we basically go to the next type where

482
00:33:20,012 --> 00:33:26,054
we select a workload for

483
00:33:26,092 --> 00:33:30,166
this presentation here. So let's have can example application.

484
00:33:30,348 --> 00:33:33,706
And this is basically could be

485
00:33:33,728 --> 00:33:37,286
because we are talking about the bank. So this can be like a payments

486
00:33:37,398 --> 00:33:40,490
workload. And it's running on eks,

487
00:33:41,070 --> 00:33:44,482
where eks is deployed across multiple availability zones

488
00:33:44,646 --> 00:33:48,378
and there is a route 53 and there are application load

489
00:33:48,394 --> 00:33:52,334
balancer which is taking in the traffic, et cetera. And also

490
00:33:52,372 --> 00:33:56,318
there is can aurora database and Kafka for managed

491
00:33:56,334 --> 00:34:00,370
streaming, et cetera. It's important

492
00:34:00,440 --> 00:34:03,326
that when you are choosing a workload,

493
00:34:03,438 --> 00:34:06,734
making sure that we are not starting out and not choosing

494
00:34:06,782 --> 00:34:10,558
a critical workload that you already have and then

495
00:34:10,584 --> 00:34:14,806
impact it. And then obviously everyone would be happy if you start with such

496
00:34:14,828 --> 00:34:20,294
a critical system and something goes wrong. So choose something which is not critical so

497
00:34:20,332 --> 00:34:22,250
that even if it is degraded,

498
00:34:24,350 --> 00:34:28,634
if it has some customer impact, then it is still flying because

499
00:34:28,672 --> 00:34:31,786
it's not critical. And usually

500
00:34:31,888 --> 00:34:35,194
we have metrics that allow that when you're

501
00:34:35,322 --> 00:34:38,686
thinking about slos for your service. So once

502
00:34:38,708 --> 00:34:43,498
you have chosen a workload, we're going to make sure that our chaos experiments

503
00:34:43,674 --> 00:34:47,506
that we want to run are safe. And we

504
00:34:47,528 --> 00:34:49,970
do that through a discovery phase of the workload.

505
00:34:51,750 --> 00:34:55,746
And that discovery phase will involve quite a bit of architecture in

506
00:34:55,768 --> 00:34:59,362
it, right? So we're going to dive deep into it

507
00:34:59,416 --> 00:35:03,186
all. You know, that well architected review. It helps

508
00:35:03,218 --> 00:35:07,670
customers build secure, high performing, resilient and efficient

509
00:35:08,410 --> 00:35:11,850
workloads on AWS,

510
00:35:12,350 --> 00:35:15,750
which has six pillars like operational excellence,

511
00:35:15,830 --> 00:35:20,010
security, reliability, performance, efficiency and cost optimization,

512
00:35:20,510 --> 00:35:24,298
as well as newly added security sustainability as well.

513
00:35:24,464 --> 00:35:27,790
And so when we are thinking about the well architected review, it's just not about

514
00:35:27,860 --> 00:35:31,182
clicking the buttons in the tool. But we are talking

515
00:35:31,236 --> 00:35:34,538
about through the various designs

516
00:35:34,554 --> 00:35:37,934
of the architecture and we want to understand how the architecture and

517
00:35:37,972 --> 00:35:41,330
the workloads and the components within your workloads speak to one another.

518
00:35:41,400 --> 00:35:44,850
Right. And what mechanisms do you have in place?

519
00:35:44,920 --> 00:35:48,894
Like can one component, when it fails, can it retry again

520
00:35:48,952 --> 00:35:52,566
as well or not? And what mechanisms do

521
00:35:52,588 --> 00:35:56,406
they have in regards to circuit breakers or have

522
00:35:56,428 --> 00:36:00,006
you implemented them? Have you tested it, et cetera? And do

523
00:36:00,028 --> 00:36:03,542
you have like run books or playbooks in place in case

524
00:36:03,596 --> 00:36:06,646
we have to roll back a particular experiment?

525
00:36:06,838 --> 00:36:10,026
And we want to make sure that you have the observability in place. And for

526
00:36:10,048 --> 00:36:13,274
example, health checks as well when we execute something so that

527
00:36:13,312 --> 00:36:16,494
your system automatically can recover from it.

528
00:36:16,612 --> 00:36:20,750
And if we have all that information, we can see that there is a deficiency

529
00:36:21,090 --> 00:36:24,462
that might impact internal or external customer.

530
00:36:24,596 --> 00:36:28,654
That's where we basically stop. When we see an impact

531
00:36:28,702 --> 00:36:32,786
to customers, then we basically stop that experiment. And if

532
00:36:32,808 --> 00:36:36,386
we have known issues, we're going

533
00:36:36,408 --> 00:36:40,054
to have to basically fix these first before we move

534
00:36:40,092 --> 00:36:43,958
on within that process. Right now, once and only if

535
00:36:44,044 --> 00:36:47,094
that everything is fine, we're going to say, okay,

536
00:36:47,132 --> 00:36:50,814
let's basically move on to the definition

537
00:36:50,882 --> 00:36:54,780
of an experiment, right? So the next phase is basically

538
00:36:55,150 --> 00:36:58,842
defining experiment. So when you are thinking about your

539
00:36:58,896 --> 00:37:02,794
system that we just, the sample application which we just

540
00:37:02,832 --> 00:37:07,454
saw before, we can think about what

541
00:37:07,492 --> 00:37:11,230
can go wrong within this environment, right? So if we already

542
00:37:11,300 --> 00:37:14,478
have or have not mechanisms in place,

543
00:37:14,644 --> 00:37:18,434
for example, if you have a third party identity provider, in our

544
00:37:18,472 --> 00:37:21,966
case, do we have a breaklass

545
00:37:21,998 --> 00:37:25,874
account wherein I can prove that I can still log in if

546
00:37:25,912 --> 00:37:29,650
something happens, right? If that identity provides goes down,

547
00:37:29,720 --> 00:37:33,462
can I still log in with a break glass account? And let's say,

548
00:37:33,516 --> 00:37:37,222
what about my eks cluster? If I have a node that fails within

549
00:37:37,276 --> 00:37:41,434
that cluster, do I have my code which

550
00:37:41,472 --> 00:37:44,700
builds on the other node itself? Right?

551
00:37:45,470 --> 00:37:49,706
Do I know how long does it take or what

552
00:37:49,728 --> 00:37:53,100
would be my end customer impact if it happens?

553
00:37:53,950 --> 00:37:57,166
Or it could

554
00:37:57,188 --> 00:38:00,814
be someone misconfigured an auto scaling group and

555
00:38:00,852 --> 00:38:04,238
health checks and which suddenly marks most of the

556
00:38:04,244 --> 00:38:07,546
instances in that zone unhealthy. So do we have

557
00:38:07,588 --> 00:38:10,882
mechanisms to detect that? And what does that mean again

558
00:38:10,936 --> 00:38:14,706
for customers and the teams that operate the environment as

559
00:38:14,728 --> 00:38:17,794
well? Right? And think about the scenario where someone

560
00:38:17,832 --> 00:38:22,120
pushed a configuration change and the ECR and

561
00:38:22,970 --> 00:38:26,786
your container registry is no longer accessible anymore.

562
00:38:26,898 --> 00:38:30,070
So that means you cannot basically launch new containers.

563
00:38:30,670 --> 00:38:34,634
Do we have mechanisms to detect that

564
00:38:34,672 --> 00:38:39,194
and recover from that? And what

565
00:38:39,232 --> 00:38:42,646
about the issues with the Kafka which is managing

566
00:38:42,678 --> 00:38:46,410
our streamers. So are we going to lose any active messages?

567
00:38:46,490 --> 00:38:50,046
What would be the data loss there? What if it

568
00:38:50,068 --> 00:38:53,662
loses a partition or it loses its connectivity, or basically

569
00:38:53,716 --> 00:38:57,566
it may reboot, et cetera. So do we have mechanisms

570
00:38:57,598 --> 00:39:01,294
to mitigate that? And what about our aurora database?

571
00:39:01,342 --> 00:39:04,994
What if the writer instance is not accessible or has gone

572
00:39:05,032 --> 00:39:08,414
down for whatever reason? Can it automatically

573
00:39:08,462 --> 00:39:12,262
and seamlessly fail over to the other

574
00:39:12,316 --> 00:39:15,078
node? And meanwhile all of this is happening.

575
00:39:15,164 --> 00:39:19,126
What happens to the latency or the jitter offer the application

576
00:39:19,308 --> 00:39:21,160
when you implement all this?

577
00:39:23,790 --> 00:39:24,540
Yeah,

578
00:39:28,590 --> 00:39:32,326
and with basically the fault injection and controlled experiments,

579
00:39:32,438 --> 00:39:36,206
we are able to do all of this. And then lastly, think about challenges that

580
00:39:36,228 --> 00:39:39,518
your clients might have connect to your environment while all of

581
00:39:39,524 --> 00:39:43,454
this is happening. So for our experiment, we wanted

582
00:39:43,492 --> 00:39:46,894
to achieve, what we wanted to achieve

583
00:39:46,942 --> 00:39:51,134
is that we can execute and understand a brownout

584
00:39:51,182 --> 00:39:54,546
scenario. So what

585
00:39:54,568 --> 00:39:58,894
a brownout scenario is that our client that connects

586
00:39:58,942 --> 00:40:02,550
to us expects a response in a certain

587
00:40:02,620 --> 00:40:05,990
amount of, let's say milliseconds or depending on the environment.

588
00:40:06,570 --> 00:40:10,006
And if it do not provide that, the client just going

589
00:40:10,028 --> 00:40:14,060
to go and back off. But the challenge is

590
00:40:14,430 --> 00:40:16,250
when you have a brownout,

591
00:40:19,950 --> 00:40:23,742
that your server is still trying to compute whatever

592
00:40:23,796 --> 00:40:28,014
they requested for, but the client is not

593
00:40:28,052 --> 00:40:31,582
there, and those are the wasted cycle. So that

594
00:40:31,636 --> 00:40:35,150
inflection point is basically called the brownout.

595
00:40:37,830 --> 00:40:41,794
Now, before we think about an experiment to go

596
00:40:41,832 --> 00:40:45,714
ahead, before we can

597
00:40:45,752 --> 00:40:49,154
actually think about an experiment to simulate a brownout within

598
00:40:49,192 --> 00:40:53,174
our eks environments, we need to understand the steady state

599
00:40:53,292 --> 00:40:56,550
and what the steady state is and what it isn't.

600
00:40:57,130 --> 00:41:00,262
So when you're thinking about defining a steady state for our

601
00:41:00,316 --> 00:41:04,054
workload, that's the high level top

602
00:41:04,092 --> 00:41:06,906
metric, right? That you're thinking about your service.

603
00:41:07,008 --> 00:41:10,858
So for example, for a payment system, it could be transactions per second,

604
00:41:10,944 --> 00:41:15,114
for a retail system it can be orders per second, for streaming

605
00:41:15,242 --> 00:41:18,574
stream starts per second, et cetera. And when you're looking at that

606
00:41:18,612 --> 00:41:22,846
line, you see very quickly you

607
00:41:22,868 --> 00:41:26,094
have an order drop or a transaction

608
00:41:26,142 --> 00:41:30,094
drop, that something that you injected within the environment caused

609
00:41:30,142 --> 00:41:33,506
probably that to drop. So we need to have

610
00:41:33,528 --> 00:41:36,142
that steady state metric,

611
00:41:36,206 --> 00:41:40,306
or already available, so that when we run these case experiments,

612
00:41:40,338 --> 00:41:43,560
we would immediately know something happened.

613
00:41:46,090 --> 00:41:49,382
So the hypothesis is key as well when we are thinking

614
00:41:49,436 --> 00:41:53,058
about the experiment, because the hypothesis will define at the end.

615
00:41:53,164 --> 00:41:56,902
Did your experiment turn out to be a turnout

616
00:41:56,966 --> 00:42:00,214
as expected, or did you learn something new that you didn't

617
00:42:00,262 --> 00:42:03,580
expect? And so the important here is, as you see,

618
00:42:04,210 --> 00:42:07,562
we are saying that we are expecting a transaction

619
00:42:07,626 --> 00:42:11,438
rate. So 300 transactions per second and

620
00:42:11,524 --> 00:42:14,922
we think that even 40% of our nodes will fail

621
00:42:14,986 --> 00:42:18,478
within our environment. Still, 99% of all requests

622
00:42:18,494 --> 00:42:22,222
to our API should be successful. So the 99th

623
00:42:22,286 --> 00:42:26,306
percentile and return a

624
00:42:26,488 --> 00:42:30,334
response within 100 milliseconds. So what we also

625
00:42:30,392 --> 00:42:33,926
would want to define is because we know our systems, we're going to

626
00:42:33,948 --> 00:42:37,398
say, okay, based on we have our experience,

627
00:42:37,484 --> 00:42:41,014
the node should come back within five minutes and

628
00:42:41,132 --> 00:42:44,522
the part will get scheduled and process tropic within the

629
00:42:44,576 --> 00:42:47,370
eight minutes after the initiation of experiment.

630
00:42:48,510 --> 00:42:51,866
And once again we are all agreeing on that hypothesis, then we're going

631
00:42:51,888 --> 00:42:55,550
to go out and fill out can experiment template.

632
00:42:56,850 --> 00:43:00,906
And so when you're thinking about an experiment template, experiment template

633
00:43:00,938 --> 00:43:04,286
itself, we're going to make sure that very clearly defining what we want to

634
00:43:04,308 --> 00:43:08,450
run, we're going to have the definition of the workload itself

635
00:43:08,600 --> 00:43:11,380
and what experiment and action we want to run.

636
00:43:12,870 --> 00:43:16,066
And you might want to run the experiment where you say, I'm going to

637
00:43:16,088 --> 00:43:19,426
run for 30 minutes with five minutes intervals to make

638
00:43:19,448 --> 00:43:22,298
sure that you can look at the graphs and on the experiments,

639
00:43:22,334 --> 00:43:26,086
staggering experiments you are running to understand the impact of the

640
00:43:26,108 --> 00:43:29,746
experiments. And then of course, because we want to do this in a controlled

641
00:43:29,778 --> 00:43:33,430
way, we need to be very clear on what the fault isolation

642
00:43:33,510 --> 00:43:37,222
boundary is for our experiment.

643
00:43:37,286 --> 00:43:40,620
And we're going to clearly define that as well.

644
00:43:41,550 --> 00:43:45,006
So we're going to have the alarms that are in place that would trigger the

645
00:43:45,028 --> 00:43:48,638
experiment to roll back if it gets

646
00:43:48,804 --> 00:43:51,966
out of control or if it causes any issues with

647
00:43:51,988 --> 00:43:55,294
the customer transactions. And that's the key

648
00:43:55,332 --> 00:43:58,230
because we want to make sure that we are practicing safeguards,

649
00:43:58,330 --> 00:44:02,834
engineering experiments, right? And we also make sure that

650
00:44:02,952 --> 00:44:06,434
we understand what is the observability and what we are looking

651
00:44:06,472 --> 00:44:09,078
at when we are running the experiment. So we need to keep an eye on

652
00:44:09,084 --> 00:44:13,000
the observability and the key steady state metrics. And then

653
00:44:14,650 --> 00:44:18,040
you would add the I hypothesis again to the template as well.

654
00:44:20,170 --> 00:44:24,566
Yeah, aws. You see on the right side we have the two empty

655
00:44:24,598 --> 00:44:25,580
lines for that.

656
00:44:28,670 --> 00:44:31,180
When we are thinking about the experiment itself,

657
00:44:32,510 --> 00:44:36,720
whether good or bad, we are always going to have an end report

658
00:44:37,170 --> 00:44:40,670
where we might celebrate that our system is resilient enough

659
00:44:40,820 --> 00:44:44,894
to such failure, or we might celebrate that we find something that

660
00:44:44,932 --> 00:44:48,706
we didn't know before and we have just helped our application

661
00:44:48,808 --> 00:44:52,546
and the organization that we have mitigated an issue or an

662
00:44:52,568 --> 00:44:55,698
event which could have happened in the

663
00:44:55,704 --> 00:44:59,526
future. Right? So once we have that experiment ready,

664
00:44:59,628 --> 00:45:03,506
we're going to think about basically preparing or priming the environment

665
00:45:03,538 --> 00:45:07,510
for our experiment. But before we go there, I just want to touch upon

666
00:45:09,450 --> 00:45:13,350
how do you go through that entire cycle of how we execute an

667
00:45:13,420 --> 00:45:17,050
experiment, because that's also critical on how we execute that experiment.

668
00:45:17,630 --> 00:45:20,842
So the execution flow is like. So first we have to check if the system

669
00:45:20,896 --> 00:45:24,614
is actually in a healthy state. Because if you remember in the beginning,

670
00:45:24,662 --> 00:45:28,378
I was saying that if we already know the system is unhealthy or it's

671
00:45:28,394 --> 00:45:31,886
going to fail, we're not going to run that experiment. So we immediately stop that.

672
00:45:31,988 --> 00:45:36,074
So once the system is healthy, we'll see if the experiment is still valid,

673
00:45:36,122 --> 00:45:40,082
because the issue or the test we are doing might have been already fixed before.

674
00:45:40,136 --> 00:45:43,726
So you don't want to run that experiment because the developer

675
00:45:43,758 --> 00:45:47,998
might have fixed those bugs or improved that resilience.

676
00:45:48,174 --> 00:45:52,578
And if we see this, then we're

677
00:45:52,594 --> 00:45:56,342
going to create a controlled experiment group where we're going to make sure

678
00:45:56,396 --> 00:46:00,486
that defined, and I'm going to go into that in a few seconds. And if

679
00:46:00,508 --> 00:46:04,042
we see that the control and experiment group is

680
00:46:04,096 --> 00:46:08,614
there and defined and which is up and running, then we start generating

681
00:46:08,662 --> 00:46:12,730
load against the control and experiment group in our environment.

682
00:46:13,870 --> 00:46:17,834
And we are checking again, is the steady state that we have, is intolerance

683
00:46:17,882 --> 00:46:20,958
that we think it should be or not.

684
00:46:21,124 --> 00:46:24,846
If it is tolerant, then now, and finally we can go

685
00:46:24,868 --> 00:46:29,826
ahead and run the experiment against the target, and then we

686
00:46:29,848 --> 00:46:32,882
check if it is intolerance based

687
00:46:32,936 --> 00:46:36,018
on what we think. And if it isn't, then we stop. Condition is going

688
00:46:36,024 --> 00:46:39,730
to kick in and it's going to roll back. Um,

689
00:46:46,060 --> 00:46:50,956
so as I was saying in the, in the previous slide that I

690
00:46:50,978 --> 00:46:54,592
mentioned the aspects of control and experiment group. So when you're thinking about

691
00:46:54,646 --> 00:46:59,724
chaos engineering and like running experiments, the goal

692
00:46:59,772 --> 00:47:03,856
always is that it's controlled, and two, that you

693
00:47:03,878 --> 00:47:08,404
have a minimal or no

694
00:47:08,442 --> 00:47:12,544
impact to your customers when you're running it. So weighing

695
00:47:12,592 --> 00:47:15,590
how you can do that is, as we call it,

696
00:47:16,760 --> 00:47:21,380
not just having synthetic load that you generate, but also synthetic resources.

697
00:47:21,540 --> 00:47:24,996
For example, you spin a new key case cluster, a synthetic

698
00:47:25,028 --> 00:47:29,016
one, one that you have and inject a

699
00:47:29,038 --> 00:47:32,670
fault, and the other one which is healthy and still serving your customers,

700
00:47:34,240 --> 00:47:38,284
right. So you're not impacting an existing resources that

701
00:47:38,322 --> 00:47:41,836
is already being used by the customers, but new resource with exactly the

702
00:47:41,858 --> 00:47:45,052
same code base and the other ones

703
00:47:45,106 --> 00:47:48,592
where you understand what happens in a certain failure scenario. So once

704
00:47:48,646 --> 00:47:51,964
we prime the experiment and we see that control and experiment

705
00:47:52,012 --> 00:47:55,244
group are healthy and we see a steady

706
00:47:55,292 --> 00:47:58,992
state, we can then move on and think about running the experiment

707
00:47:59,056 --> 00:48:02,944
itself. Now, running a chaos engineering

708
00:48:02,992 --> 00:48:06,384
experiment requires great tools that are safe

709
00:48:06,432 --> 00:48:10,324
to experiment. So when we are thinking about tools,

710
00:48:10,372 --> 00:48:13,876
there are various tools out there how you can use and consume.

711
00:48:13,988 --> 00:48:17,240
In AWS, we have fault injection simulator

712
00:48:17,740 --> 00:48:21,308
with which, when you're thinking about one of the first slide with

713
00:48:21,314 --> 00:48:25,324
the shared responsibility model for resilience, fault injection simulator helps you

714
00:48:25,442 --> 00:48:28,956
quite a bit with that because the faults that you can

715
00:48:28,978 --> 00:48:32,620
inject with fis are

716
00:48:32,690 --> 00:48:35,936
running against the AWS APIs directly. And you

717
00:48:35,958 --> 00:48:39,136
can inject these faults against your primary dependency to make

718
00:48:39,158 --> 00:48:42,704
sure that you can create mechanisms that you can

719
00:48:42,742 --> 00:48:45,760
survive a component failure within your system, et cetera.

720
00:48:46,120 --> 00:48:49,392
Now second is that faults and actions

721
00:48:49,456 --> 00:48:54,116
that I want to highlight are the

722
00:48:54,138 --> 00:48:57,428
highlighted parts are basically integration with litmus chaos and the

723
00:48:57,434 --> 00:49:00,868
chaos mesh. And the great thing about this is that it provides

724
00:49:00,884 --> 00:49:04,312
you with a widened scope of faults that you can inject, for example,

725
00:49:04,446 --> 00:49:08,852
in our example architecture into your kubernetes

726
00:49:08,916 --> 00:49:12,616
cluster to fault injection simulator via

727
00:49:12,648 --> 00:49:15,020
single plane of glass.

728
00:49:16,560 --> 00:49:19,740
And then it also has various integrations.

729
00:49:20,320 --> 00:49:23,636
Now, if you want to run experiments

730
00:49:23,688 --> 00:49:27,792
against, let's say, EC two systems, you have the capability to run these

731
00:49:27,926 --> 00:49:31,840
through AWS systems manager via the SSM agent.

732
00:49:31,990 --> 00:49:35,264
Now think about where these

733
00:49:35,302 --> 00:49:39,156
come into play. So when we are thinking about running experiments, these are

734
00:49:39,178 --> 00:49:42,868
the ways on how you can create disruptions within the system.

735
00:49:43,034 --> 00:49:46,752
Let's say you have various microservices that run and consume a database.

736
00:49:46,816 --> 00:49:51,124
Now you might say how

737
00:49:51,162 --> 00:49:55,332
can we create a fault within the database without having impact to all those microservices,

738
00:49:55,396 --> 00:49:58,696
right? And the answer to that is that you can inject faults within these

739
00:49:58,718 --> 00:50:01,896
microservices itself, for example, packet laws, or that they

740
00:50:01,918 --> 00:50:05,384
would result in exactly the same as application not

741
00:50:05,422 --> 00:50:08,924
being able to talk to or write to the database, because it's not going to,

742
00:50:08,962 --> 00:50:12,716
right. It's not going to get there and reach the database without you bringing down

743
00:50:12,738 --> 00:50:16,076
the database itself. And so it's important that to widen

744
00:50:16,108 --> 00:50:20,096
the scope and think about the experiments that you can run and

745
00:50:20,198 --> 00:50:23,216
see what actions you have on how

746
00:50:23,238 --> 00:50:26,640
you can simulate those various experiment failures.

747
00:50:27,060 --> 00:50:30,516
So in our example case, because we want to do that brown out that I

748
00:50:30,538 --> 00:50:33,968
showed before we use the eks

749
00:50:34,064 --> 00:50:38,084
action, that we can terminate a certain number of

750
00:50:38,122 --> 00:50:41,496
nodes, a percentage of nodes within our cluster, and we

751
00:50:41,518 --> 00:50:46,936
would run them, right? So if you go to the

752
00:50:46,958 --> 00:50:50,648
tool itself, the way it runs, if you use

753
00:50:50,814 --> 00:50:53,740
the tool, we can trust the fis that we're going to make sure that something

754
00:50:53,810 --> 00:50:57,340
goes wrong, that it can alert automatically

755
00:50:57,680 --> 00:51:00,952
and also helps us roll back the experiment,

756
00:51:01,016 --> 00:51:04,256
right? And the fault injection simulator has these

757
00:51:04,278 --> 00:51:07,808
mechanisms wherein. So when you build an experiment with

758
00:51:07,894 --> 00:51:11,184
fis, you can define what are my key alarms are,

759
00:51:11,222 --> 00:51:14,704
which define those steady state. And they

760
00:51:14,742 --> 00:51:18,144
should kick in that if they find

761
00:51:18,182 --> 00:51:21,792
any deviance. Right. And if something goes wrong during that experiment,

762
00:51:21,936 --> 00:51:25,632
it should basically stop and then roll back the whole experiment.

763
00:51:25,776 --> 00:51:29,544
So in our case everything was fine and we said that, okay, well, now we

764
00:51:29,582 --> 00:51:34,116
have confidence based on our observability that we have for this experiment.

765
00:51:34,308 --> 00:51:36,760
Now let's move to the next environment,

766
00:51:39,340 --> 00:51:42,620
which is obviously taking this into the production.

767
00:51:42,960 --> 00:51:46,556
So you have to think about the guardrails that are important in your

768
00:51:46,578 --> 00:51:49,640
production environment. So when you are running ks,

769
00:51:49,720 --> 00:51:53,230
experiment in production, especially when you are thinking about

770
00:51:53,840 --> 00:51:57,260
running them for the first time, please don't run on a peak hours.

771
00:51:57,410 --> 00:52:00,576
It's probably not the best idea to run on a peak hours. And also make

772
00:52:00,598 --> 00:52:04,096
sure that in many ways, when you're running these experiments in lower and

773
00:52:04,118 --> 00:52:08,132
ever environments, your permissions are also quite permissive that you have

774
00:52:08,186 --> 00:52:11,316
when it compared to production. And you got to make sure that you have the

775
00:52:11,338 --> 00:52:15,236
observability in

776
00:52:15,258 --> 00:52:18,476
place that you have permissions to execute these various

777
00:52:18,528 --> 00:52:22,136
experiments. Aws. Well, because in production it's always the

778
00:52:22,158 --> 00:52:26,090
restricted permissions. And also key to understand is that

779
00:52:26,780 --> 00:52:30,440
the fault isolation boundary changed because we are in production now.

780
00:52:30,510 --> 00:52:34,430
So we need to make sure that we understand that as well.

781
00:52:36,800 --> 00:52:40,648
And also we understand the risk of running them in production

782
00:52:40,824 --> 00:52:44,450
environment because we need to

783
00:52:45,060 --> 00:52:48,688
understand and make sure that we are not impacting our customers.

784
00:52:48,854 --> 00:52:52,464
That's the key. So once we

785
00:52:52,502 --> 00:52:56,312
understand this and have the runbook and plays books

786
00:52:56,476 --> 00:52:59,716
which are up to date, we are finally at a stage where we

787
00:52:59,738 --> 00:53:03,172
can think about moving to production. And here again,

788
00:53:03,226 --> 00:53:06,710
we want to think about, you know,

789
00:53:07,240 --> 00:53:08,150
you know,

790
00:53:10,200 --> 00:53:13,780
think about like, you know, experiment in production with a cannery.

791
00:53:14,840 --> 00:53:17,984
We'll check that in a second. So, you know,

792
00:53:18,042 --> 00:53:21,320
as you have seen this picture before, in our lower environment,

793
00:53:21,660 --> 00:53:24,876
we're going to do the same thing in production. But we

794
00:53:24,898 --> 00:53:28,508
don't have a mirrored environment, right? So that some customers

795
00:53:28,594 --> 00:53:32,108
do where they split traffic. And we have a chaos engineering environment in

796
00:53:32,114 --> 00:53:36,076
our production and another environment. So what we use in this

797
00:53:36,098 --> 00:53:39,628
is a cannery to say that we're going to take the real traffic

798
00:53:39,804 --> 00:53:43,650
a tiny bit of percentage into our.

799
00:53:44,500 --> 00:53:48,056
We're going to start bringing that real user traffic into the controlled

800
00:53:48,108 --> 00:53:51,364
and experiment group. Now keep in mind at this

801
00:53:51,402 --> 00:53:55,332
point, nothing should go wrong. We have the control and experiment group

802
00:53:55,386 --> 00:53:59,044
here as well. We haven't injected the fault

803
00:53:59,172 --> 00:54:03,140
yet. And we should be able to see from can observability

804
00:54:03,220 --> 00:54:06,328
perspective that everything is good,

805
00:54:06,414 --> 00:54:09,764
because we haven't created any experiments

806
00:54:09,812 --> 00:54:13,416
yet. And once we see that truly happen,

807
00:54:13,518 --> 00:54:17,612
that's where we start. That's where we kick in the

808
00:54:17,666 --> 00:54:21,790
experiment. Right. So we're going to

809
00:54:23,540 --> 00:54:26,320
get running the experiment in production.

810
00:54:27,700 --> 00:54:30,864
But when we are thinking about running this

811
00:54:30,902 --> 00:54:35,104
in production, we want to make sure that we have all the

812
00:54:35,302 --> 00:54:38,624
workload experts in terms of engineering teams,

813
00:54:38,672 --> 00:54:41,824
observability operators, incident response teams,

814
00:54:41,872 --> 00:54:45,104
everybody in a room before we actually do this in production.

815
00:54:45,152 --> 00:54:48,776
So that if something goes wrong or if you have seen

816
00:54:48,798 --> 00:54:52,056
any unforeseen incidents during that engineering experience,

817
00:54:52,158 --> 00:54:55,336
you can quickly roll back and make sure that the system is back up and

818
00:54:55,358 --> 00:54:58,490
running within no time. Right.

819
00:55:00,300 --> 00:55:03,660
And the final stage is basically going into

820
00:55:03,730 --> 00:55:07,736
the correction of error state where we are basically listing

821
00:55:07,768 --> 00:55:11,676
out all the key findings, learnings from

822
00:55:11,698 --> 00:55:15,600
that experiment which we have run, and then we'll see,

823
00:55:15,670 --> 00:55:19,404
okay, how did we communicate between the teams?

824
00:55:19,532 --> 00:55:22,930
Did we have any persons or people whom we needed

825
00:55:23,380 --> 00:55:27,404
in the room, but they were not there? Was there any documentation

826
00:55:27,452 --> 00:55:31,236
missing, et cetera. How can we improve the overall process? How do

827
00:55:31,258 --> 00:55:35,044
we then basically take these learnings and share that

828
00:55:35,082 --> 00:55:38,528
across the organization so that they can further improve the

829
00:55:38,554 --> 00:55:40,920
overall workloads, et cetera?

830
00:55:43,020 --> 00:55:47,080
So that is the final

831
00:55:47,150 --> 00:55:51,324
phase that the next take is basically the automation part

832
00:55:51,362 --> 00:55:54,572
because we are not running this just for once.

833
00:55:54,626 --> 00:55:59,112
Right. So we want to basically take this learnings

834
00:55:59,176 --> 00:56:04,556
and automate that so

835
00:56:04,578 --> 00:56:08,176
that we can bring them and run them in pipelines. So we need

836
00:56:08,198 --> 00:56:11,324
to make sure that experiments also run in the pipeline

837
00:56:11,372 --> 00:56:14,880
and also outside the pipeline because the faults happen all the time.

838
00:56:15,030 --> 00:56:18,416
So they don't just happen by pushing a code,

839
00:56:18,518 --> 00:56:22,260
they happen like day in and day out within the production environment as well sometimes.

840
00:56:22,330 --> 00:56:26,580
Right. And then we can also use game days to

841
00:56:26,650 --> 00:56:31,000
bring in the teams together to help

842
00:56:31,070 --> 00:56:34,840
them understand the overall architecture and recover the apps, et cetera,

843
00:56:35,180 --> 00:56:39,076
and test those processes work. And are people alerted

844
00:56:39,108 --> 00:56:43,404
in a way that if something goes wrong, they're able to work together

845
00:56:43,602 --> 00:56:47,176
and then bring that resilience,

846
00:56:47,288 --> 00:56:54,576
continuous resilience culture to

847
00:56:54,598 --> 00:56:58,288
make it easier for our customers. We have built in a

848
00:56:58,294 --> 00:57:01,568
lot of templates and handbooks that

849
00:57:01,574 --> 00:57:04,432
we are going to the experiments with them. So we share,

850
00:57:04,486 --> 00:57:08,500
like chaos Engineering Handbook

851
00:57:09,720 --> 00:57:13,216
that shows the business value of chaos engineering and how it helps

852
00:57:13,248 --> 00:57:16,800
with resilience. The chaos engineering templates

853
00:57:16,880 --> 00:57:20,436
as well as correction of error templates we have, and also various aspects

854
00:57:20,468 --> 00:57:24,200
of the reports that we share with customers when we are running to the program

855
00:57:24,350 --> 00:57:27,704
now. Next, I just want to share some resources, which we

856
00:57:27,742 --> 00:57:32,076
have, but we

857
00:57:32,098 --> 00:57:35,310
have the workshops with which you can,

858
00:57:37,200 --> 00:57:41,768
for example, in the screen we see that you basically start with an observability

859
00:57:41,864 --> 00:57:45,916
workshop. And then the reason that is because the workshop

860
00:57:45,948 --> 00:57:49,360
builds an entire system that provides you with everything

861
00:57:49,430 --> 00:57:53,472
in the stack of observability. And then you have to absolutely nothing

862
00:57:53,526 --> 00:57:57,156
to get out of pressing a button, right? And once we

863
00:57:57,178 --> 00:58:00,964
have that and we have the observability, from top down to tracing to

864
00:58:01,002 --> 00:58:05,460
logging to metrics, then go for the chaos engineering workshop

865
00:58:06,360 --> 00:58:09,156
and looking at the various experiments there,

866
00:58:09,258 --> 00:58:12,144
and start with some database fault,

867
00:58:12,192 --> 00:58:15,336
injection the containers and EC two and it shows you how

868
00:58:15,358 --> 00:58:18,948
you can do that in the pipeline as well. And you can take those experiments

869
00:58:18,964 --> 00:58:22,196
and you run it against a sample application within the observability

870
00:58:22,388 --> 00:58:25,724
workshop and it gives you a great view of what's going on within

871
00:58:25,762 --> 00:58:29,288
your system. And if you inject these failures or faults,

872
00:58:29,384 --> 00:58:32,876
you'll see them right away within those dashboards with no effort at

873
00:58:32,898 --> 00:58:36,696
all. So these are the QR codes

874
00:58:36,728 --> 00:58:40,064
for those workshops. Please do

875
00:58:40,102 --> 00:58:43,948
get started and reach out to any of your AWS

876
00:58:44,124 --> 00:58:47,490
representatives or contacts for further information

877
00:58:47,940 --> 00:58:51,808
on these. You can also reach me

878
00:58:51,894 --> 00:58:55,376
on my Twitter account with that I just

879
00:58:55,398 --> 00:58:59,352
want to thank you for your time. I know it's been long

880
00:58:59,406 --> 00:59:01,972
session, but I hope you found it insightful.

881
00:59:02,036 --> 00:59:05,624
Please do share your feedback and let me know

882
00:59:05,662 --> 00:59:08,950
if you want more information on this. Thank you.

