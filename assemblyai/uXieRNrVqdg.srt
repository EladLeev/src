1
00:00:20,170 --> 00:00:24,042
Hey everyone, and welcome to this talk on floating

2
00:00:24,106 --> 00:00:27,574
point challenges and how we can overcome this in

3
00:00:27,612 --> 00:00:31,442
Rustlang. My name is Prabhat

4
00:00:31,506 --> 00:00:33,560
and I'm just 23 years old.

5
00:00:35,770 --> 00:00:39,834
I'm just getting started in this industry and I

6
00:00:39,872 --> 00:00:43,834
came across lots of problems while I was working in

7
00:00:43,872 --> 00:00:47,306
Rustlank, specifically with floating point numbers. But before all of

8
00:00:47,328 --> 00:00:50,966
this talks, let's get some

9
00:00:51,008 --> 00:00:55,278
more info about myself. So that's me here as you can see.

10
00:00:55,364 --> 00:00:58,766
So talk about some of my interests. They are like I

11
00:00:58,788 --> 00:01:02,474
love Linux and I like systems programming.

12
00:01:02,602 --> 00:01:06,466
Also I like to music and dj a lot. Additionally for some

13
00:01:06,488 --> 00:01:10,466
physical activities I like cycling as well. Apart from that,

14
00:01:10,568 --> 00:01:14,686
talking about my professional experience, I have done multiple programming

15
00:01:14,718 --> 00:01:18,438
jobs, freelancing ones and otherwise, during my college years,

16
00:01:18,524 --> 00:01:22,386
specifically in the domain of systems programming which involved

17
00:01:22,498 --> 00:01:25,750
dealing with c rusts directly,

18
00:01:26,170 --> 00:01:29,754
and I've contributed to some open source projects in

19
00:01:29,792 --> 00:01:33,194
rust. Some minors contributions are there, but yeah, I did

20
00:01:33,232 --> 00:01:37,034
that as well. And recently I completed my internship as

21
00:01:37,072 --> 00:01:41,182
a rust developer for a startup known as Afroshoot. It's one of the

22
00:01:41,316 --> 00:01:44,986
world's most thriving AI

23
00:01:45,018 --> 00:01:49,018
photography startup in the domain of photography,

24
00:01:49,114 --> 00:01:52,494
and that was some details about myself.

25
00:01:52,612 --> 00:01:56,782
Also, if you want to reach out to me, you can have my LinkedIn

26
00:01:56,846 --> 00:02:00,546
is in the right top hand corner and along with

27
00:02:00,568 --> 00:02:04,194
my GitHub so you can scan it. Reach out to me if you have any

28
00:02:04,232 --> 00:02:07,974
questions about this talk. On that note, let's proceed to the

29
00:02:08,012 --> 00:02:11,478
next slide. So these are the

30
00:02:11,484 --> 00:02:13,480
things that I will be covering in my talk.

31
00:02:14,490 --> 00:02:17,846
Before designing the content for my

32
00:02:17,868 --> 00:02:21,306
talk, I kept in mind that the approach that I kept in mind was

33
00:02:21,408 --> 00:02:25,418
I won't be getting too much into the details about every single thing

34
00:02:25,584 --> 00:02:28,854
because you can easily find it out in the web somehow.

35
00:02:28,982 --> 00:02:32,442
But my approach would be to instead

36
00:02:32,576 --> 00:02:36,174
just give you a sort of like indicator on where

37
00:02:36,212 --> 00:02:39,886
you can look out in case if you get stuck or what

38
00:02:39,908 --> 00:02:43,182
are the issues that can come along the way if you are

39
00:02:43,236 --> 00:02:46,210
working with floating point numbers,

40
00:02:46,280 --> 00:02:50,062
specifically if you are in rust? And all of these concepts

41
00:02:50,126 --> 00:02:54,050
would still be applicable for the other languages as well. That being said. But yeah,

42
00:02:54,120 --> 00:02:58,566
I have especially kept rust in mind and

43
00:02:58,588 --> 00:03:00,840
how we can use rust to mitigate these issues.

44
00:03:01,530 --> 00:03:04,662
So going back to the, I'm sorry,

45
00:03:04,796 --> 00:03:08,122
proceeding to the another slide, I would first like

46
00:03:08,176 --> 00:03:12,810
to give you a sense of idea on why the

47
00:03:12,960 --> 00:03:16,694
importance of floating point numbers is so much. So here's

48
00:03:16,742 --> 00:03:20,746
one basic example on what happened in

49
00:03:20,768 --> 00:03:24,190
the year 1991. So there was a patriot missile system.

50
00:03:24,260 --> 00:03:27,866
We all know that us has lots of different kind of weapons

51
00:03:27,898 --> 00:03:31,262
in their arsenals so the patriot missile failure which

52
00:03:31,316 --> 00:03:34,974
occurred in the Gulf War, was again the result of some floating

53
00:03:35,022 --> 00:03:38,500
point calculations errors. And this was interesting because

54
00:03:39,350 --> 00:03:42,850
instead of like having f 64, some other type

55
00:03:42,920 --> 00:03:45,410
in their embedded architecture,

56
00:03:45,830 --> 00:03:49,046
which drives the missile, which guides it from one direction to

57
00:03:49,068 --> 00:03:52,134
the other, and it

58
00:03:52,172 --> 00:03:55,494
guides them on which location, target location of the region it should

59
00:03:55,532 --> 00:03:58,940
drop, there was some calculations there in that,

60
00:03:59,790 --> 00:04:02,490
and this error was in the one 10th of a second.

61
00:04:02,560 --> 00:04:06,246
So, since a missile

62
00:04:06,358 --> 00:04:10,006
also carries an embedded hardware itself that needs some programming,

63
00:04:10,118 --> 00:04:13,882
and there was an inappropriate use of data type which couldn't accommodate

64
00:04:14,026 --> 00:04:17,802
the precise value of the one 10th of a second, which resulted

65
00:04:17,866 --> 00:04:21,454
in gradual accumulation of the rounding errors over time because

66
00:04:21,492 --> 00:04:25,166
of the limited precision which we'll take a look about in

67
00:04:25,188 --> 00:04:29,166
the next slides. Because of

68
00:04:29,188 --> 00:04:32,626
that rounding, error propagation came in picture. We'll also take

69
00:04:32,648 --> 00:04:36,606
a look on this one as well. So the patriots tracking

70
00:04:36,638 --> 00:04:40,770
with DAR system relied on time calculations, precise time calculations.

71
00:04:40,850 --> 00:04:43,858
Because of such a high degree of precision,

72
00:04:43,954 --> 00:04:47,574
about one 10th of a second has to be maintained to make it drop to

73
00:04:47,692 --> 00:04:51,418
desired location. That didn't happen. And the result

74
00:04:51,504 --> 00:04:55,226
was there was casualty about 22. We came

75
00:04:55,248 --> 00:04:58,554
out with casualty of 22 soldiers or something.

76
00:04:58,592 --> 00:05:02,074
I don't know the exact numbers here, but that was a result like

77
00:05:02,112 --> 00:05:05,678
it dropped to totally another region, it drift off from the

78
00:05:05,684 --> 00:05:10,494
target region out of somewhere else. So this

79
00:05:10,532 --> 00:05:13,946
can be the significant impact if things are not fall correctly,

80
00:05:13,978 --> 00:05:17,726
because implicitly we might tend to think, okay, why not just use integers?

81
00:05:17,758 --> 00:05:22,226
Why not to just use any other data types? But skipping something

82
00:05:22,408 --> 00:05:25,460
necessarily doesn't means it's a solution as well.

83
00:05:25,910 --> 00:05:29,854
So let's go to the slides.

84
00:05:29,982 --> 00:05:33,738
So these are some of the plain

85
00:05:33,774 --> 00:05:37,158
memories we need to know before we get started to

86
00:05:37,164 --> 00:05:39,954
know about it. Just kind of like a quick note.

87
00:05:40,082 --> 00:05:42,986
So, real world representation, I mean,

88
00:05:43,008 --> 00:05:46,554
the point of the floatingpoint numbers is to provide a real world representation of the

89
00:05:46,592 --> 00:05:50,590
real numbers. So I mean, the numbers which contains decimal place,

90
00:05:50,740 --> 00:05:53,966
again, that's the fundamental purpose of any

91
00:05:54,068 --> 00:05:57,246
floating point number in any language in PI. And they

92
00:05:57,268 --> 00:05:59,150
are used for scientific notations.

93
00:06:00,850 --> 00:06:04,450
They are represented in a form of scientific, through the scientific notations.

94
00:06:05,190 --> 00:06:09,410
Also, they have precision and the range thing

95
00:06:09,480 --> 00:06:12,866
in them. So floating point offers like a trade off

96
00:06:12,888 --> 00:06:15,906
between the precision and the range, suitably for a wide spectrum of numerical

97
00:06:15,938 --> 00:06:19,222
calculations. We'll understand this as well in the first

98
00:06:19,276 --> 00:06:21,526
slides, like why does it happen?

99
00:06:21,628 --> 00:06:24,854
Specifically, why there's a trade off between

100
00:06:24,892 --> 00:06:26,550
the precision and the range.

101
00:06:28,510 --> 00:06:32,730
And also we'll take a look on limited,

102
00:06:33,550 --> 00:06:37,414
we covered about limited precisions here, and also we'll

103
00:06:37,542 --> 00:06:41,114
try to understand mitigation strategies, like how developers

104
00:06:41,162 --> 00:06:44,826
can apply mitigation strategies to manage rounding errors

105
00:06:44,858 --> 00:06:48,606
and ensure reliable results are being there in

106
00:06:48,628 --> 00:06:51,070
their code and doesn't cause any havoc.

107
00:06:51,810 --> 00:06:55,902
So let's start like how is the floating

108
00:06:55,966 --> 00:07:00,066
point numbers are standardized? So IEEE or

109
00:07:00,088 --> 00:07:04,274
the Institute for Electrical and Electronics Engineering seven five four standard

110
00:07:04,392 --> 00:07:07,974
is like the widely used specifications for how if floating point

111
00:07:08,012 --> 00:07:11,110
numbers should be represented using the scientific notations.

112
00:07:11,690 --> 00:07:15,234
So all the things like operations or its method that are being carried

113
00:07:15,282 --> 00:07:19,030
out is being defined by the standard.

114
00:07:19,180 --> 00:07:23,194
Also, the standard defines several formats for representing numbers

115
00:07:23,312 --> 00:07:26,746
in the precision thing. So we have different kind

116
00:07:26,768 --> 00:07:30,734
of precision range depending upon the powers of two. So two par five

117
00:07:30,772 --> 00:07:34,414
or 32 bits here represents a single precision, and two par

118
00:07:34,452 --> 00:07:38,490
six is reserved for double precision.

119
00:07:38,650 --> 00:07:42,754
We'll understand this in the next slides. And extended precision is

120
00:07:42,792 --> 00:07:46,434
like for more higher these

121
00:07:46,472 --> 00:07:50,030
are more specifically used with supercomputers where we require

122
00:07:50,110 --> 00:07:52,450
more higher degree of precision values.

123
00:07:53,190 --> 00:07:56,886
So let's start by taking a look onto the

124
00:07:56,908 --> 00:07:57,670
precision.

125
00:07:59,770 --> 00:08:03,506
So, single precision, or 32 bit in IEEE

126
00:08:03,538 --> 00:08:07,554
74 single precision format is

127
00:08:07,612 --> 00:08:11,340
composed of like, or rather I should say is

128
00:08:12,510 --> 00:08:16,214
divided in summation

129
00:08:16,262 --> 00:08:19,866
of different sets of bit. So the first bit here represents the

130
00:08:19,888 --> 00:08:23,406
sign. If the first bit is zero, it is

131
00:08:23,428 --> 00:08:26,846
for positive, and if it's one, then it's negative according to

132
00:08:26,868 --> 00:08:30,314
the scientific notation. And eight bits here represents

133
00:08:30,362 --> 00:08:34,686
the exponent. So the power of tens that's

134
00:08:34,718 --> 00:08:38,050
responsible for representing the decimal point

135
00:08:38,120 --> 00:08:41,554
in the entire numbers. Another is this

136
00:08:41,592 --> 00:08:45,554
one is the most critical one, and this is significant bit.

137
00:08:45,672 --> 00:08:49,134
So 23 bits here are the ones that actually represents

138
00:08:49,182 --> 00:08:52,934
a value in it. So two part 22 is the range that

139
00:08:52,972 --> 00:08:56,454
which you can have your numbers represented along with the help of the sign

140
00:08:56,492 --> 00:08:59,594
bit that represents whether it's assigned value or

141
00:08:59,632 --> 00:09:02,966
non signed value. So we don't

142
00:09:02,998 --> 00:09:07,114
have a binary division of the two part 23 from

143
00:09:07,152 --> 00:09:10,522
two, which takes up a divided range. We don't have that,

144
00:09:10,576 --> 00:09:14,074
we have just positive values from the significant coming, and then we take

145
00:09:14,112 --> 00:09:17,902
a signed bit to represent actually whether this negative or not.

146
00:09:18,036 --> 00:09:21,098
So here's a formula. I won't get into the details of the formula,

147
00:09:21,114 --> 00:09:26,194
but just for the sake of thing, I've put it here. So going

148
00:09:26,232 --> 00:09:30,046
to the next one. So also we need to take a look into double precision

149
00:09:30,078 --> 00:09:34,514
as well, because these two are the most used one in

150
00:09:34,552 --> 00:09:39,000
any programming job. So double precision in the IEEE 74

151
00:09:39,370 --> 00:09:43,462
contains similar thing, but with

152
00:09:43,516 --> 00:09:47,320
enlarged precision and range. So of course,

153
00:09:48,590 --> 00:09:52,122
in any floating point number, the first

154
00:09:52,176 --> 00:09:56,438
bit will always be reserved for representing the signed or the unsigned

155
00:09:56,534 --> 00:10:00,694
status of the number the exponent bit here will going to represent the exponent

156
00:10:00,742 --> 00:10:03,918
part of the floating point number, and the significant bit

157
00:10:04,004 --> 00:10:07,786
will represent the fractional, I mean the significant part or the Mantis,

158
00:10:07,898 --> 00:10:12,230
which we better know as. So here you get more range

159
00:10:12,410 --> 00:10:13,570
and precision.

160
00:10:16,230 --> 00:10:20,574
So coming to the actual visual

161
00:10:20,622 --> 00:10:27,074
representation. So as you can see, this is therefore this

162
00:10:27,112 --> 00:10:30,226
format here is for the single precision, that is 32 bits

163
00:10:30,258 --> 00:10:34,450
one. So as you can see that as per the previous slides,

164
00:10:34,530 --> 00:10:39,000
the first bit is reserved as the sign and the rest.

165
00:10:39,370 --> 00:10:43,146
And the exponent one is used to represent the exponent part, which is

166
00:10:43,168 --> 00:10:45,786
the power of the ten in the decimal number system.

167
00:10:45,968 --> 00:10:50,246
And the fractions here represent the actual number

168
00:10:50,368 --> 00:10:53,402
which will be represented from it. So the significant

169
00:10:53,466 --> 00:10:57,230
component here will be represented by the fraction part,

170
00:10:57,380 --> 00:11:00,026
which is composed of 23 bits.

171
00:11:00,138 --> 00:11:03,266
Multiply that with the exponent with the base of

172
00:11:03,288 --> 00:11:07,730
ten, and you get a scientific notation where

173
00:11:07,800 --> 00:11:11,714
a decimal real number in a number

174
00:11:11,752 --> 00:11:15,010
in a decimal system can represent using

175
00:11:15,080 --> 00:11:17,830
a number in a base two system.

176
00:11:17,980 --> 00:11:21,174
So that's exactly the structure of how it has been

177
00:11:21,292 --> 00:11:25,014
devised by the IEEE San five four standard. So that's where

178
00:11:25,052 --> 00:11:29,046
we're going to take a look at mostly. So again, we covered

179
00:11:29,078 --> 00:11:32,506
this as well. I'll just go through

180
00:11:32,528 --> 00:11:36,074
it quickly. So first of all, there's a couple of

181
00:11:36,112 --> 00:11:39,366
things you need to have about floating

182
00:11:39,398 --> 00:11:42,974
point numbers. That is important things,

183
00:11:43,092 --> 00:11:46,254
or rather the issues that can come up while you're coding it out.

184
00:11:46,292 --> 00:11:49,790
So this is a drawback. Somehow we can say that

185
00:11:49,860 --> 00:11:53,466
floating point numbers are like they have a limited precision

186
00:11:53,498 --> 00:11:56,834
and rounding errors problems. So since the number of

187
00:11:56,872 --> 00:12:00,926
bits which are available is finite, so representation

188
00:12:00,958 --> 00:12:03,940
of the real numbers often cannot be done exactly,

189
00:12:05,990 --> 00:12:10,054
it can be done to a certain range only because we

190
00:12:10,092 --> 00:12:13,218
don't have infinite bits, we have limited number of bits, we have finite bits,

191
00:12:13,234 --> 00:12:16,658
so we can only represent it on a certain range of values,

192
00:12:16,754 --> 00:12:20,418
which we'll take a look on that in the examples in the slides.

193
00:12:20,514 --> 00:12:23,978
So this inherit limitation reach rounding error. So what we need to

194
00:12:23,984 --> 00:12:27,386
do is like, since let's say any number is

195
00:12:27,488 --> 00:12:31,178
way more long in the decimal

196
00:12:31,194 --> 00:12:34,506
number system, then we need to round it up to represent it to the closest

197
00:12:34,538 --> 00:12:38,286
value possible in

198
00:12:38,308 --> 00:12:42,334
the given number format. Like if we have 32

199
00:12:42,372 --> 00:12:44,786
bit single precision floating point number,

200
00:12:44,888 --> 00:12:48,322
then we have to make sure that we round it

201
00:12:48,456 --> 00:12:52,354
from this significant component so that it can

202
00:12:52,392 --> 00:12:56,166
be represented properly and we try

203
00:12:56,188 --> 00:12:58,680
to minimize the loss. This is like inherently present.

204
00:12:59,130 --> 00:13:03,426
So there are certain addresses

205
00:13:03,618 --> 00:13:05,800
regarding this limited precision thing.

206
00:13:06,890 --> 00:13:10,518
So let's start to talk about the problems,

207
00:13:10,684 --> 00:13:14,214
or rather issues one by one. So limited

208
00:13:14,262 --> 00:13:17,514
precisions and rounding errors. Right? So every 14 point

209
00:13:17,552 --> 00:13:21,174
numbers, as I told you previously in the previous slides, have a finite number

210
00:13:21,312 --> 00:13:25,290
or a finite number of bits to represent a significant

211
00:13:25,450 --> 00:13:28,606
or a mantisa here. Now, rounding errors will

212
00:13:28,628 --> 00:13:31,614
happen when the real number cannot be represented exactly.

213
00:13:31,732 --> 00:13:35,274
So the term real number here must be understood as

214
00:13:35,332 --> 00:13:39,298
like this applies both rational numbers and irrational numbers.

215
00:13:39,464 --> 00:13:42,946
So something that terminates well, the numbers which

216
00:13:42,968 --> 00:13:46,446
make sense are the ones that rational. They're usually finite and terminating,

217
00:13:46,558 --> 00:13:50,550
whereas irrational numbers are non terminating. Like one prominent example

218
00:13:50,620 --> 00:13:54,406
is PI. So PI is an irrational number which is non terminating and

219
00:13:54,428 --> 00:13:58,406
non repeating mostly. So here, as you can see in the illustration

220
00:13:58,518 --> 00:14:01,786
given below, is like the

221
00:14:01,808 --> 00:14:06,182
green boxes, represents the finite number of bits we have in any given precision

222
00:14:06,326 --> 00:14:09,594
in any given precision type. And the

223
00:14:09,632 --> 00:14:13,360
rest of the red boxes here might indicate that

224
00:14:17,090 --> 00:14:20,640
those particular bits can't be accommodated here.

225
00:14:22,690 --> 00:14:26,434
So the entire bits given here in the top are the

226
00:14:26,472 --> 00:14:30,050
actual numbers. But since we have limited number of bits,

227
00:14:31,910 --> 00:14:35,394
we have to round it to represent since the red boxes is the ones

228
00:14:35,432 --> 00:14:38,674
that we don't have in our precision range, so we can't represent

229
00:14:38,722 --> 00:14:42,774
them. Now here's one

230
00:14:42,812 --> 00:14:46,150
common problem with it. So let's say if you have two floating point numbers,

231
00:14:46,220 --> 00:14:49,734
this is quite common. I'm sure many of you might have come across it already.

232
00:14:49,852 --> 00:14:53,690
So if you have two floating point numbers, the chances are

233
00:14:53,760 --> 00:14:57,562
that it might not output you zero three. Exactly. It might

234
00:14:57,616 --> 00:15:01,086
output you zero three up

235
00:15:01,108 --> 00:15:04,462
to nine, something like that. So because

236
00:15:04,516 --> 00:15:07,742
of this, lots of issues they happen. So to address it,

237
00:15:07,796 --> 00:15:11,630
we have fixed point arithmetic so we can

238
00:15:11,700 --> 00:15:15,486
use numbers to represent them as a scale integers to avoid precision

239
00:15:15,518 --> 00:15:18,754
loss. One step could be taken like this. Another could be like

240
00:15:18,792 --> 00:15:22,654
we can perform arithmetic operations on scale integers to maintain the precision.

241
00:15:22,702 --> 00:15:26,226
So it's like you do not

242
00:15:26,248 --> 00:15:29,046
need to directly use the floatingpoint numbers in the first place. You can rather use

243
00:15:29,068 --> 00:15:32,374
arithmetic operations on the scale integers, which will always be

244
00:15:32,412 --> 00:15:35,654
precise and return you the results. And you can apply some

245
00:15:35,692 --> 00:15:39,494
operations which will result to you which

246
00:15:39,532 --> 00:15:43,030
will help you preserve the accuracy of your calculations.

247
00:15:43,190 --> 00:15:46,666
Another could be to use a big decimal libraries. So this is also

248
00:15:46,768 --> 00:15:49,962
an advanced approach, but it is quite useful. You can

249
00:15:50,016 --> 00:15:54,654
google up more about the big decimal here. I won't be covering it in

250
00:15:54,692 --> 00:15:58,090
lots of detail here, but you can use like numb big integer

251
00:15:58,170 --> 00:16:02,750
crate and rust deck crates for arbitrary precision decimal arithmetic.

252
00:16:03,490 --> 00:16:07,486
And obviously one more choice which we have left here is a trade

253
00:16:07,518 --> 00:16:10,722
off between the memory and the precision kind of thing.

254
00:16:10,856 --> 00:16:14,146
So you have to make a choice between f 64 and f 32.

255
00:16:14,168 --> 00:16:17,718
Wisely depending upon what your requirements are.

256
00:16:17,884 --> 00:16:21,574
So you have to go through that. Coming up to the another

257
00:16:21,612 --> 00:16:24,994
one is the rounding error. So since we have limited

258
00:16:25,042 --> 00:16:28,566
number of bits, as we talked about, because of

259
00:16:28,588 --> 00:16:31,766
that diff comes another interesting issue is of rounding

260
00:16:31,798 --> 00:16:35,770
error. So we try to round it to represent to the closest value possible,

261
00:16:35,840 --> 00:16:40,186
just like let's say we have 0322

262
00:16:40,288 --> 00:16:44,186
and non terminating, but we have to represent it up to the fourth decimal

263
00:16:44,218 --> 00:16:47,774
place only after the point. So we might

264
00:16:47,812 --> 00:16:51,598
write like as 0322 and five

265
00:16:51,684 --> 00:16:55,234
something, or maybe zero could be there. So that's what

266
00:16:55,272 --> 00:16:58,978
the meaning of rounding is. So precision loss can happen because of that,

267
00:16:59,064 --> 00:17:02,420
because of the rounding errors, since not real numbers

268
00:17:03,190 --> 00:17:06,710
cannot be expressed exactly using the limited precision here.

269
00:17:06,860 --> 00:17:11,410
And because of this, rounding errors can accumulate as a result of approximations

270
00:17:11,490 --> 00:17:15,126
made during the arithmetic operations. So let's say you

271
00:17:15,148 --> 00:17:18,634
have a complicated expression statement which has

272
00:17:18,752 --> 00:17:22,842
lots of different mathematical operations present

273
00:17:22,976 --> 00:17:26,202
with itself, and what will happen is certain

274
00:17:26,256 --> 00:17:29,318
expressions, certain components of those mathematical expressions,

275
00:17:29,414 --> 00:17:33,466
we're going to accumulate more error than you

276
00:17:33,488 --> 00:17:37,002
might think of. So hence what the consequences

277
00:17:37,066 --> 00:17:40,714
could be that it can lead to discrepancies

278
00:17:40,762 --> 00:17:44,026
between the expected mathematical results and the actual results obtained

279
00:17:44,058 --> 00:17:47,758
when using floating point numbers. So you can have redundant

280
00:17:47,854 --> 00:17:53,182
error being thrown out in your calculations, which is very undesirable

281
00:17:53,246 --> 00:17:57,350
considering if you have some kind of applications where you desire precision.

282
00:17:58,250 --> 00:18:01,062
So here's one example of it. As you can see,

283
00:18:01,116 --> 00:18:04,934
there's two number x and y, and y specifically being represented using

284
00:18:04,972 --> 00:18:08,634
the exponent part e to the power -15

285
00:18:08,752 --> 00:18:10,300
and when we sum it up,

286
00:18:12,270 --> 00:18:16,070
you can see that this thing is not very suitable,

287
00:18:16,150 --> 00:18:19,322
I mean, not giving out the results which we actually

288
00:18:19,456 --> 00:18:22,080
desired for. So,

289
00:18:23,410 --> 00:18:28,366
to solve this, we have something like we

290
00:18:28,388 --> 00:18:32,446
can use the decimal types to work with. So rust decimal crate. Why? It's that

291
00:18:32,468 --> 00:18:36,386
it's similar to the number big crates that we previously mentioned here.

292
00:18:36,488 --> 00:18:40,206
So with that, we can work with the decimal based arithmetic,

293
00:18:40,238 --> 00:18:42,900
which can minimize the rounding errors. Also,

294
00:18:44,710 --> 00:18:48,758
we can try to round only when necessary. Another thing is,

295
00:18:48,924 --> 00:18:52,806
avoid cumulative rounding. So you

296
00:18:52,828 --> 00:18:56,646
can structure your calculations in such a way, if it's long, in such

297
00:18:56,668 --> 00:19:00,374
a way that it doesn't result in a cumulative errors.

298
00:19:00,502 --> 00:19:04,458
Rather, it's like done in a way that avoids it in

299
00:19:04,464 --> 00:19:08,102
the first place. Also, one very good technique to overcome

300
00:19:08,166 --> 00:19:11,370
it is avoiding divisions.

301
00:19:11,530 --> 00:19:15,278
So mostly it's been seen that it tends to

302
00:19:15,284 --> 00:19:18,110
happen more with the divisions, because division,

303
00:19:18,690 --> 00:19:22,626
we have more

304
00:19:22,648 --> 00:19:25,986
degree of numbers represented, we have more

305
00:19:26,088 --> 00:19:29,198
degree of numbers generated while we are applying division operations.

306
00:19:29,294 --> 00:19:32,962
So rather it can be instead done

307
00:19:33,016 --> 00:19:36,310
using with multiply and then carrying out

308
00:19:36,380 --> 00:19:40,114
the end result, in other words, trying to minimize the division

309
00:19:40,162 --> 00:19:43,734
here directly if possible. Another good

310
00:19:43,772 --> 00:19:48,110
approach to rounding error prowess is to use interval arithmetic.

311
00:19:48,290 --> 00:19:51,500
You can google it up to learn more about it.

312
00:19:52,350 --> 00:19:56,060
So using interval arithmetic libraries like

313
00:19:57,790 --> 00:20:01,346
there was one INRI. Yeah, so INRI

314
00:20:01,398 --> 00:20:06,154
crate in rust provides you with the interval

315
00:20:06,202 --> 00:20:09,550
arithmetic operations that you can apply in your calculations

316
00:20:10,130 --> 00:20:13,694
so that you can have some loss

317
00:20:13,742 --> 00:20:17,220
prevented coming to the next point here.

318
00:20:18,310 --> 00:20:21,906
Or the next issue is the loss of significance. So the

319
00:20:21,928 --> 00:20:25,314
loss of significance is because of the consequences in

320
00:20:25,352 --> 00:20:28,370
the limited precision of the floating point representation,

321
00:20:28,450 --> 00:20:31,906
again, where the available bits

322
00:20:31,938 --> 00:20:35,222
are allocated to the most significant bit. So the difference between

323
00:20:35,276 --> 00:20:38,986
rounding error and the loss of significance is that we

324
00:20:39,008 --> 00:20:42,694
just take in the most significant

325
00:20:42,742 --> 00:20:46,186
digits, most significant sorry bits in

326
00:20:46,208 --> 00:20:49,642
a given binary number, and because of that we have

327
00:20:49,696 --> 00:20:53,066
something like loss of significance. They are quite close with each other, rounding errors

328
00:20:53,098 --> 00:20:56,238
and loss of significance. But rounding errors are something which

329
00:20:56,244 --> 00:20:59,914
we do explicitly, whereas loss of significance

330
00:20:59,962 --> 00:21:02,806
is something which is, again, not in our hands directly.

331
00:21:02,858 --> 00:21:07,214
So it can take place, let's say, due to nearly

332
00:21:07,262 --> 00:21:11,634
equal numbers, like if they are being subtracted and

333
00:21:11,672 --> 00:21:15,334
hence the cancellation or

334
00:21:15,372 --> 00:21:19,042
the difference here can be completely avoided,

335
00:21:19,106 --> 00:21:22,562
resulting in something which is not very precise.

336
00:21:22,706 --> 00:21:26,406
So, as you can see on an illustration on this slide that we

337
00:21:26,428 --> 00:21:29,498
have two different bit blocks here.

338
00:21:29,664 --> 00:21:33,242
The first one is in the orange and another

339
00:21:33,296 --> 00:21:36,906
one is in blue, and the third row is something which we

340
00:21:36,928 --> 00:21:40,618
see as a result, that after subtracting

341
00:21:40,634 --> 00:21:44,640
it, if we had

342
00:21:45,810 --> 00:21:49,678
more numbers than we expected, then the rest of the ones which are in

343
00:21:49,684 --> 00:21:53,278
the three red boxes would get discarded, simply because we don't have range to

344
00:21:53,284 --> 00:21:57,086
accommodate it. And we'll take up the most significant bits in the given binary

345
00:21:57,118 --> 00:22:00,642
numbers for the significant or the mantisa part.

346
00:22:00,696 --> 00:22:04,162
So this can result in that. So, as you can see,

347
00:22:04,296 --> 00:22:08,146
here's an example illustrating this. So we have two numbers which

348
00:22:08,168 --> 00:22:11,478
are quite close with each other. Like the difference is not huge,

349
00:22:11,644 --> 00:22:14,886
but what you will see in the output here in the difference is that it

350
00:22:14,908 --> 00:22:18,090
will have something like 00899

351
00:22:18,240 --> 00:22:21,642
or something. So again,

352
00:22:21,696 --> 00:22:24,010
it won't be direct and very precise.

353
00:22:26,670 --> 00:22:30,294
So solutions for mitigating

354
00:22:30,342 --> 00:22:34,458
this one is again quite clever, like reordering the operations.

355
00:22:34,554 --> 00:22:38,154
So let's say if you have subtraction in some early inner

356
00:22:38,202 --> 00:22:41,454
complex calculations, you can restructure it down

357
00:22:41,492 --> 00:22:44,180
so that it can be done later on, if not later.

358
00:22:45,270 --> 00:22:48,340
If it cannot be eliminated, then at least it can be done later.

359
00:22:48,950 --> 00:22:52,210
You can use alternative formulas to rewrite your entire

360
00:22:52,280 --> 00:22:55,922
expressions. One of my favorite personal favorite ones

361
00:22:55,976 --> 00:22:59,478
is something which is taught to engineering students in their first year of the college.

362
00:22:59,564 --> 00:23:03,094
Like Taylor's expansion. So Taylor's expansion is something you can

363
00:23:03,132 --> 00:23:07,862
learn more about. I won't be able to cover it here. So basically,

364
00:23:07,916 --> 00:23:11,994
Tyler's expansion is a mechanism through which you

365
00:23:12,032 --> 00:23:16,250
can precisely compute

366
00:23:16,750 --> 00:23:20,234
the number of arithmetic, I'm sorry, the number

367
00:23:20,272 --> 00:23:23,518
of decimal digits you

368
00:23:23,524 --> 00:23:27,502
would need. It can go as long

369
00:23:27,556 --> 00:23:31,070
as you might want it to be. So it's kind of a formula in itself.

370
00:23:31,220 --> 00:23:35,310
And this can be used to represent any complex calculations

371
00:23:36,370 --> 00:23:39,346
in a discrete fashion. So let's say you want to approximate sine x,

372
00:23:39,368 --> 00:23:42,354
you can do it with the Taylor's expansion, you want to approximate cos x,

373
00:23:42,392 --> 00:23:45,522
you can still do it with the Taylor's expansion, you want to

374
00:23:45,576 --> 00:23:48,978
approximate tan inverse of x, you can still do it

375
00:23:48,984 --> 00:23:51,670
with the Taylor's expansion.

376
00:23:53,290 --> 00:23:56,982
So yeah, this is one of the very clever techniques that we can have for

377
00:23:57,036 --> 00:23:58,810
loss of significance to be avoided.

378
00:24:00,030 --> 00:24:01,180
Excuse me.

379
00:24:02,910 --> 00:24:06,374
And another one is to use arbitrary precision arithmetic,

380
00:24:06,422 --> 00:24:09,786
the one which we saw in the case of limited precision ones. So you can

381
00:24:09,808 --> 00:24:12,880
still use D squids as well if you require it.

382
00:24:13,970 --> 00:24:17,582
Now, coming up to the associativity, the issue

383
00:24:17,636 --> 00:24:21,418
of associativity and the order of operations.

384
00:24:21,594 --> 00:24:23,200
Now this is quite interesting.

385
00:24:25,750 --> 00:24:30,290
Most of us are quite aware about the rule of commutativeity

386
00:24:31,030 --> 00:24:34,866
for summation and multiplication. So most

387
00:24:34,888 --> 00:24:38,434
of us might not notice the difference here, that for

388
00:24:38,472 --> 00:24:42,326
the result one and result two, the output must be the

389
00:24:42,348 --> 00:24:45,654
same, right? Most of us might assume that, and looking at it in the first

390
00:24:45,692 --> 00:24:49,674
place. But the thing that is behind the scenes is applicable is

391
00:24:49,712 --> 00:24:53,494
like flowing point operations are not always associative.

392
00:24:53,622 --> 00:24:56,406
In fact, because of the issues which you talked about previously,

393
00:24:56,438 --> 00:25:00,198
like limited precision, rounding errors, and loss of significance.

394
00:25:00,374 --> 00:25:03,702
So changing orders can lead to different results.

395
00:25:03,766 --> 00:25:07,198
It's quite possible. So you can run this code, you can find it

396
00:25:07,204 --> 00:25:11,038
by yourself, that it might result. In the first case, it will result you,

397
00:25:11,204 --> 00:25:15,074
though we are doing the exact same thing mathematically, but the

398
00:25:15,112 --> 00:25:18,514
output which we're getting in the result one variable is

399
00:25:18,552 --> 00:25:22,498
one, and for the result two, it's zero. So this is

400
00:25:22,584 --> 00:25:26,066
very much undesirable and something we want to avoid

401
00:25:26,098 --> 00:25:28,600
at all costs. Now,

402
00:25:30,410 --> 00:25:34,854
overcoming this issue could be done using the

403
00:25:34,892 --> 00:25:38,566
solution to this is pretty straightforward, so we

404
00:25:38,588 --> 00:25:42,026
can use parentheses and explicit groupings, just like in the

405
00:25:42,048 --> 00:25:45,334
case where we do it. In the case of pointers

406
00:25:45,382 --> 00:25:49,146
rereferencing, where we have the scope of the evaluation to be decided using the

407
00:25:49,168 --> 00:25:52,634
help of parentheses, we can still do

408
00:25:52,672 --> 00:25:56,510
it using here as well. But we have to be

409
00:25:56,660 --> 00:26:00,030
little cautious about it because still it might not give you the exact

410
00:26:00,100 --> 00:26:03,578
results. But if you are aware about the order of calculations in your

411
00:26:03,604 --> 00:26:06,914
expression, then this can be a good

412
00:26:06,952 --> 00:26:10,670
mitigation technique. And another one is reordering operation,

413
00:26:10,750 --> 00:26:14,686
not just using parentheses, but you can also reorder your operations so that the loss

414
00:26:14,718 --> 00:26:17,938
of significance, I mean the parts of the calculations where the loss of

415
00:26:17,944 --> 00:26:21,654
significance is the least can come up before

416
00:26:21,692 --> 00:26:24,310
the rest of the part where the loss of significance could be the most.

417
00:26:24,460 --> 00:26:28,454
So reordering your operations can be another good way to deal with that.

418
00:26:28,572 --> 00:26:32,470
Another thing here is this is very advanced,

419
00:26:32,550 --> 00:26:36,026
and honestly I couldn't able to find lots of material about it yet,

420
00:26:36,048 --> 00:26:39,574
because probably this is still in the RFC stage of the rusts

421
00:26:39,622 --> 00:26:42,934
compilers. So you can use

422
00:26:42,992 --> 00:26:46,474
fuse, multiply and add operations. So it's an SIMD operation.

423
00:26:46,522 --> 00:26:50,158
Specifically. If you don't know about it, you can still google it

424
00:26:50,164 --> 00:26:54,210
up to learn more about it. But FMA operations can paralyze

425
00:26:54,630 --> 00:27:00,162
the certain components of the certain

426
00:27:00,216 --> 00:27:03,362
calculations of the component of the expression in a way that

427
00:27:03,416 --> 00:27:05,886
reduces overall rounding errors.

428
00:27:05,918 --> 00:27:09,366
So you can take a look into this more in

429
00:27:09,388 --> 00:27:12,966
depth if you wish to. Now, coming up to

430
00:27:12,988 --> 00:27:17,186
the another interesting issue here with floatingpoint numbers is comparison

431
00:27:17,218 --> 00:27:19,420
and the equality testing. Trust me,

432
00:27:20,430 --> 00:27:25,386
this one is the one which most of you will come across if

433
00:27:25,408 --> 00:27:29,434
you ever will work with the floating point numbers in your code. So I

434
00:27:29,472 --> 00:27:32,060
came across first time when I was in my high school,

435
00:27:32,590 --> 00:27:36,046
I couldn't compare the floating point numbers because the result was like it

436
00:27:36,068 --> 00:27:39,582
was so arbitrary that it was not possible for me to

437
00:27:39,636 --> 00:27:43,330
write a code logic at that time. But coming to the point here,

438
00:27:43,400 --> 00:27:45,410
if you, let's say, want to compare,

439
00:27:47,990 --> 00:27:51,554
if you want to compare two same floating point

440
00:27:51,592 --> 00:27:55,250
numbers, then it's better idea to use equality.

441
00:27:55,590 --> 00:27:59,666
Instead of using direct equality comparison, you can use epsilon

442
00:27:59,698 --> 00:28:03,350
based comparison for the approximate equality. Now, what does it mean?

443
00:28:03,500 --> 00:28:06,806
Basically, it means that as you can see in

444
00:28:06,828 --> 00:28:10,878
the function approximately equal, we have two distinct floating

445
00:28:10,914 --> 00:28:14,474
point numbers and we are first subtracting it and we are taking

446
00:28:14,512 --> 00:28:17,980
the absolute of them because we don't want the negative results to be,

447
00:28:18,830 --> 00:28:22,990
we only care about the magnitude of the result, not that the sign,

448
00:28:23,060 --> 00:28:25,918
whether it's positive or negative real number.

449
00:28:26,084 --> 00:28:29,294
So the thing here is we can compare it

450
00:28:29,492 --> 00:28:33,022
with the minimal difference

451
00:28:33,076 --> 00:28:37,022
of the epsilon, which is defined using the constant epsilon.

452
00:28:37,166 --> 00:28:40,354
By the way, standard library in rust provides you with

453
00:28:40,472 --> 00:28:43,790
all of these constants. I've just written it explicitly

454
00:28:43,870 --> 00:28:47,720
for the sake of giving an idea how exactly

455
00:28:49,370 --> 00:28:52,786
it is represented, but it's already there in the standard library, so you can directly

456
00:28:52,818 --> 00:28:57,158
import it from the standard crate. So you

457
00:28:57,164 --> 00:29:00,698
can compare the result of the magnitude of the result.

458
00:29:00,864 --> 00:29:04,518
If it's less than the epsilon, then it's fine. If it's greater

459
00:29:04,534 --> 00:29:07,978
than the error, I mean, sorry, the difference

460
00:29:08,064 --> 00:29:12,234
is greater than the epsilon one, then it's possibly not approximately

461
00:29:12,282 --> 00:29:15,854
equal to each other. So this

462
00:29:15,892 --> 00:29:20,014
one technique can be employed to test your code. I mean to test like

463
00:29:20,052 --> 00:29:23,454
a floatingpoint comparison. So getting into more

464
00:29:23,492 --> 00:29:27,698
specifics here, when we saw that you can use epsilon based comparison instead of using

465
00:29:27,864 --> 00:29:30,580
equality operator in the first place.

466
00:29:31,110 --> 00:29:34,878
Another good technique here is relative error comparison.

467
00:29:35,054 --> 00:29:38,690
This is very similar to the epsilon based comparison, but except

468
00:29:38,770 --> 00:29:41,922
the threshold of the comparison is something which we decide

469
00:29:41,986 --> 00:29:45,174
as developer, having the knowledge of the system that how much

470
00:29:45,212 --> 00:29:48,566
can there be an acceptable range

471
00:29:48,598 --> 00:29:52,214
of error difference, sorry, the acceptable

472
00:29:52,262 --> 00:29:55,930
range of the subtractive difference from two numbers here,

473
00:29:56,080 --> 00:29:58,700
which will be fine to describe it.

474
00:30:00,370 --> 00:30:03,898
Another technique we can use is units and last piece comparison.

475
00:30:04,074 --> 00:30:07,866
So we can use ULPL equal

476
00:30:07,898 --> 00:30:11,870
functions to compare them in case we

477
00:30:11,940 --> 00:30:15,246
only want equality. This comes from the float compare

478
00:30:15,278 --> 00:30:18,434
crate in rust, so you can use this one. It's quite good

479
00:30:18,472 --> 00:30:22,354
as well. I've used it personally and

480
00:30:22,392 --> 00:30:25,974
comparing with some specific tolerance value. So this is again

481
00:30:26,092 --> 00:30:29,266
similar to the relative error comparison.

482
00:30:29,298 --> 00:30:32,962
But relative errors can be dynamic, whereas this can be static.

483
00:30:33,026 --> 00:30:36,966
So this technique can be used

484
00:30:36,988 --> 00:30:41,254
as well to avoid having some errors in the equality

485
00:30:41,302 --> 00:30:45,162
comparison. Now another case, this is not a problem.

486
00:30:45,296 --> 00:30:46,940
Rather this is like,

487
00:30:50,270 --> 00:30:53,674
I should say, the specification given in the

488
00:30:53,712 --> 00:30:57,102
seven, five, four standard of the IEEe that we can have not a number

489
00:30:57,156 --> 00:31:00,638
and infinity handling in the floating point numbers. Let's say that you want to

490
00:31:00,644 --> 00:31:04,478
divide two numbers and those two floatingpoint

491
00:31:04,574 --> 00:31:08,466
numbers. Floating point numbers happen to be zero, absolute zero. Then in

492
00:31:08,488 --> 00:31:10,820
that result, mathematically it should be nothing.

493
00:31:11,590 --> 00:31:15,460
So you get certain methods which

494
00:31:16,470 --> 00:31:20,134
is not a number method, which will result you whether

495
00:31:20,172 --> 00:31:23,960
it's number or not. If that's true, then result is an n

496
00:31:24,330 --> 00:31:27,846
and result if it's infinite, then you can check for that condition as

497
00:31:27,868 --> 00:31:32,202
well. So these two methods are given to you so that you can check if,

498
00:31:32,256 --> 00:31:37,050
let's say, certain parts of your calculation might throw you undesirable

499
00:31:37,550 --> 00:31:41,354
condition and you don't want to write your own technique

500
00:31:41,402 --> 00:31:45,466
for testing it out directly. So you can have this inbuilt methods

501
00:31:45,578 --> 00:31:48,510
to test it out for you as a safety check. Now,

502
00:31:48,580 --> 00:31:52,270
since these operations arise from division like,

503
00:31:52,340 --> 00:31:55,614
I mean, all these conditions can arise from having a square root of a negative

504
00:31:55,662 --> 00:31:58,978
number and something like dividing zero by zero. So all

505
00:31:58,984 --> 00:32:01,970
these conditions where we can't express anything correctly,

506
00:32:03,910 --> 00:32:06,680
or the calculation might get into complex numbers.

507
00:32:08,650 --> 00:32:11,846
We should use these APIs to

508
00:32:11,868 --> 00:32:15,000
test whether this one is actually an odd number.

509
00:32:16,010 --> 00:32:19,718
Coming up to another one here is compilers

510
00:32:19,734 --> 00:32:23,546
optimizations now this is not

511
00:32:23,568 --> 00:32:26,842
an issue, but this is rather given to you

512
00:32:26,896 --> 00:32:30,018
as a safety net so that you can deal with your compiler

513
00:32:30,054 --> 00:32:34,762
directly and instructed explicitly not to optimize

514
00:32:34,826 --> 00:32:36,000
or do some,

515
00:32:37,890 --> 00:32:42,042
or to do some kind of premature

516
00:32:42,106 --> 00:32:45,682
optimizations during the phase of compilation of your code

517
00:32:45,736 --> 00:32:49,454
to execute both. So compilers can optimize

518
00:32:49,582 --> 00:32:53,026
floatingpoint calculations for performance range. So this can

519
00:32:53,048 --> 00:32:57,250
happen quite a lot of time and may reorder or combine operations

520
00:32:57,410 --> 00:33:01,590
which may affect precision and could result in an

521
00:33:01,740 --> 00:33:05,510
associativity of the orders problem which we saw in the previous slides.

522
00:33:06,490 --> 00:33:10,278
So to overcome that you can use table comparison

523
00:33:10,294 --> 00:33:13,946
flags to control those optimizations. So for example here

524
00:33:13,968 --> 00:33:17,622
in this code you can see that his own function perform calculations

525
00:33:17,686 --> 00:33:20,460
which take two distinct swimming point numbers.

526
00:33:20,910 --> 00:33:24,254
And what can happen end up is it could be written as such

527
00:33:24,292 --> 00:33:27,440
as like it could be computed in a way such as

528
00:33:28,450 --> 00:33:31,886
b times 2.0 and then it could be added, or it could

529
00:33:31,908 --> 00:33:35,918
be previously added and then it could be like multiplied

530
00:33:35,934 --> 00:33:39,554
with the two. I'm not saying that

531
00:33:39,592 --> 00:33:42,834
the order of evaluation is completely ignored, but how

532
00:33:42,872 --> 00:33:46,146
compiler is going to deal with such a complex expression is up to

533
00:33:46,168 --> 00:33:49,350
it. So you can't predict it almost every time. So it's better

534
00:33:49,420 --> 00:33:53,046
to use appropriate compiler flags to help it ignore it.

535
00:33:53,068 --> 00:33:57,074
So one of them is like inline never. So you are explicitly telling your instructing

536
00:33:57,122 --> 00:34:00,826
compiler to never inline this function call and always let it

537
00:34:00,848 --> 00:34:05,210
be computed once there is a value passed to it, because otherwise

538
00:34:06,270 --> 00:34:10,910
this can lead to undesirable output and accuracy errors.

539
00:34:12,370 --> 00:34:16,026
So some few compiler optimization techniques

540
00:34:16,058 --> 00:34:19,674
include like using compilers attribute. So rust

541
00:34:19,722 --> 00:34:23,614
provides you with the attributes that lets you control the behavior of the floating

542
00:34:23,662 --> 00:34:26,990
point operations. One such attribute is target feature,

543
00:34:27,070 --> 00:34:31,262
and within this target feature you can have a parameter enabled

544
00:34:31,326 --> 00:34:35,522
that is a fast math. So this will going to do

545
00:34:35,576 --> 00:34:39,130
some internal instructions,

546
00:34:39,230 --> 00:34:44,614
it will release some internal instructions while compiling that this

547
00:34:44,652 --> 00:34:48,578
piece of code should have fast math support enabled.

548
00:34:48,754 --> 00:34:53,274
You can learn more about fast math like

549
00:34:53,312 --> 00:34:56,678
from the web. I'm not going to get into lots of detail, but fast math

550
00:34:56,694 --> 00:35:00,518
is basically something that allows you to overcome

551
00:35:00,614 --> 00:35:04,350
the calculations that are arising from the compilers optimization.

552
00:35:04,690 --> 00:35:08,314
Another thing you can use is use compiler flags directly

553
00:35:08,362 --> 00:35:11,742
in the phase of compilation rather than in the code base.

554
00:35:11,796 --> 00:35:14,866
So you can have a target cpu given,

555
00:35:14,968 --> 00:35:18,770
and you can use LLVM's

556
00:35:19,350 --> 00:35:22,514
arguments to enable fast math in your build

557
00:35:22,552 --> 00:35:25,338
file. So that since rust is built on LLVM,

558
00:35:25,374 --> 00:35:28,854
so we can have a direct optimization option available from

559
00:35:28,892 --> 00:35:31,960
LLVM as well for the fast math thing.

560
00:35:33,050 --> 00:35:35,990
Now, coming to the accumulative errors,

561
00:35:37,290 --> 00:35:41,098
this one is pretty interesting. So quite sometimes,

562
00:35:41,184 --> 00:35:44,586
let's say that you might have to deal with some kind of stories of a

563
00:35:44,608 --> 00:35:47,914
floating point numbers, and in that scenario it

564
00:35:47,952 --> 00:35:51,658
becomes quite necessary to know that there can be lots of,

565
00:35:51,744 --> 00:35:56,542
as you saw with the previous slides, that there can be lots of issues with

566
00:35:56,596 --> 00:35:59,978
the limited number of bits, there can be rounding error

567
00:35:59,994 --> 00:36:03,178
problems, there can be loss of significance, associativity, et cetera,

568
00:36:03,194 --> 00:36:06,446
et cetera. So because of all this, there can be an accumulation

569
00:36:06,478 --> 00:36:09,970
of the rounding errors. So it's like you're cursively calling a function

570
00:36:10,040 --> 00:36:13,582
whether it's iterative or cursive or iterative

571
00:36:13,646 --> 00:36:17,426
whatever. And because of a given set of calculations, that error

572
00:36:17,458 --> 00:36:20,982
will get amplified over the entire period of

573
00:36:21,036 --> 00:36:24,150
the calculation.

574
00:36:26,010 --> 00:36:31,206
So to minimize that, we have something like Cahan's

575
00:36:31,238 --> 00:36:34,758
summation algorithm and compensated summation algorithm.

576
00:36:34,854 --> 00:36:39,034
So this one here is given to the

577
00:36:39,072 --> 00:36:42,522
code shown to you on your slides is a Cahan's summation problem.

578
00:36:42,656 --> 00:36:46,314
It takes in like array of floating point numbers, which is series of floating

579
00:36:46,362 --> 00:36:49,886
point numbers, and the output might not be the one. But since we

580
00:36:49,908 --> 00:36:53,754
are using Cahan's summation algorithm, we get a definitive

581
00:36:53,802 --> 00:36:57,358
result here. So you can use Cahan summation as well if in case you're

582
00:36:57,374 --> 00:37:00,626
dealing with some kind of iterative algorithm which

583
00:37:00,728 --> 00:37:04,162
does a calculation of floating point numbers, or with the help of floating point numbers,

584
00:37:04,296 --> 00:37:06,120
could be recursive as well.

585
00:37:08,810 --> 00:37:12,054
So we covered lots of things in

586
00:37:12,092 --> 00:37:13,640
this talk here.

587
00:37:17,790 --> 00:37:21,642
These two slides will be about summarizing it like

588
00:37:21,696 --> 00:37:25,466
what we just saw. Excuse me. So first

589
00:37:25,488 --> 00:37:28,858
of all, actually choose

590
00:37:28,944 --> 00:37:32,854
the appropriate data type depending upon the requirements precision

591
00:37:32,902 --> 00:37:36,522
requirements. So go with f 32 if you know that your range of calculations

592
00:37:36,586 --> 00:37:39,758
won't going to cross that boundary. Otherwise you have to switch to f 34 if

593
00:37:39,764 --> 00:37:41,710
you need more range and precision.

594
00:37:42,530 --> 00:37:45,714
Now leverage some numerical computation crates for

595
00:37:45,752 --> 00:37:49,474
enhanced accuracy, such as rug num, big into

596
00:37:49,512 --> 00:37:53,202
dick and INRI. You can take a look on

597
00:37:53,256 --> 00:37:56,486
these crates. These are grape crates. If in case you need to deal with some

598
00:37:56,508 --> 00:37:59,586
kind of 40 point operations, you can use them directly.

599
00:37:59,618 --> 00:38:03,154
These are material ones. Also avoid

600
00:38:03,202 --> 00:38:06,994
direct equality comparison and instead use epsilon

601
00:38:07,042 --> 00:38:10,860
based comparisons like relative ones and tolerance based ones.

602
00:38:11,230 --> 00:38:15,094
Also perform error propagation analysis to estimate potential

603
00:38:15,142 --> 00:38:19,066
inaccuracies. This is quite advanced. It doesn't employ use

604
00:38:19,168 --> 00:38:22,586
of any library or crate or something. But what

605
00:38:22,608 --> 00:38:26,658
you can do is you can use some weight

606
00:38:26,694 --> 00:38:30,170
to estimate how much error is going to throw out if you deliberately

607
00:38:30,330 --> 00:38:33,946
put in some errors in your calculations so that you can know how much error

608
00:38:33,978 --> 00:38:37,714
is going to accumulate. Now, coming to part two, you can consider using

609
00:38:37,752 --> 00:38:41,166
arbitrary precision libraries for critical computation of your code. So arbitrary

610
00:38:41,198 --> 00:38:44,686
precision libraries gives you more range to compute

611
00:38:44,798 --> 00:38:48,774
and more precision since they rely on

612
00:38:48,972 --> 00:38:53,206
decimal based calculations, so they will give you more specific

613
00:38:53,388 --> 00:38:57,122
results. Also, minimize accumulated errors in iterative

614
00:38:57,186 --> 00:39:00,406
algorithms. Use techniques like Cahan summation, which we

615
00:39:00,428 --> 00:39:04,150
saw this couple of slides previously, and also

616
00:39:04,220 --> 00:39:07,750
use stable compiled flags to control floatingpoint behaviors,

617
00:39:07,830 --> 00:39:11,646
which can happen unknowingly, which can be done unknowingly by

618
00:39:11,668 --> 00:39:15,840
your compiler for you. So you must want to avoid that.

619
00:39:16,930 --> 00:39:20,846
Also, thoroughly test numerical code with diverse input set this

620
00:39:20,868 --> 00:39:25,806
is more or less we are talking about fuzzing

621
00:39:25,838 --> 00:39:29,266
here specifically. So thoroughly test numerical codes with

622
00:39:29,288 --> 00:39:33,282
diverse sets of inputs to identify adverse inaccuracies. So if you

623
00:39:33,336 --> 00:39:36,418
are not sure about certain edge cases, go ahead and put it there in your

624
00:39:36,424 --> 00:39:40,114
test cases, just to be ensure that your

625
00:39:40,152 --> 00:39:43,410
test case is that once you're shipping out something to your production,

626
00:39:43,490 --> 00:39:46,902
it's tested and test proof and you don't have something

627
00:39:46,956 --> 00:39:51,046
like a patriot missile crisis happening in

628
00:39:51,068 --> 00:39:54,520
case you have to work on something like that. So that's all from my side.

629
00:39:56,650 --> 00:39:58,600
I would thank you, all of you,

630
00:39:59,450 --> 00:40:04,078
for listening to me till here at this end. And yeah,

631
00:40:04,244 --> 00:40:07,006
in case you have some questions, you can always reach out to me. I will

632
00:40:07,028 --> 00:40:10,446
be more than happy to answer whatever, and I will

633
00:40:10,468 --> 00:40:14,000
be more than happy to help you as well. So yeah,

634
00:40:14,690 --> 00:40:15,340
take care guys.

