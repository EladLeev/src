1
00:02:01,060 --> 00:02:04,716
Hi everyone. First of all, I would like to give a big shout

2
00:02:04,748 --> 00:02:07,888
out to all the organizers of Conf 42. Thanks a

3
00:02:07,894 --> 00:02:11,012
lot for this smooth conduct. It has been a very fantastic,

4
00:02:11,036 --> 00:02:14,584
fantastic experience for me. Amazing job, guys.

5
00:02:14,702 --> 00:02:18,776
Hi, thanks for joining me. In this talk,

6
00:02:18,878 --> 00:02:22,100
I'll be talking about experimentation platform at scale.

7
00:02:22,180 --> 00:02:25,996
I'm Hrithik Shwastav. I work as a lead software engineer at

8
00:02:26,018 --> 00:02:31,144
Kojek in data engineering team, currently in a stream called experimentation.

9
00:02:31,272 --> 00:02:34,508
In this talk, I'll be walking you through my experience

10
00:02:34,594 --> 00:02:38,256
of building a b testing platform at Gojek Tech. Challenges which

11
00:02:38,278 --> 00:02:42,316
we faced, right. And the changes in architecture which we did to cater

12
00:02:42,348 --> 00:02:46,160
the new throughput. The new scale, which is right now

13
00:02:46,310 --> 00:02:49,860
is around 2.1 or 2.2 million

14
00:02:50,010 --> 00:02:53,636
rpm. Let's proceed. Yeah, so this

15
00:02:53,658 --> 00:02:57,104
is the title of my talk, a B testing platform at scale.

16
00:02:57,232 --> 00:03:00,964
Let's move to the next slide. Yeah, so the

17
00:03:01,082 --> 00:03:04,244
agenda of this talk will be like. I'll be explaining about

18
00:03:04,282 --> 00:03:07,384
what a b testing is, and then we will

19
00:03:07,422 --> 00:03:10,968
talk about the objective, what the audience of this

20
00:03:11,054 --> 00:03:14,692
talk can learn from this. Then I'll be explaining about the litmus,

21
00:03:14,756 --> 00:03:18,444
the name of a b testing platform at Gojek Tech.

22
00:03:18,482 --> 00:03:21,996
Give a light on the old architecture from where we started

23
00:03:22,098 --> 00:03:25,468
building the platform. Then we will talk about the

24
00:03:25,554 --> 00:03:28,864
current scale or the scale, how the number of

25
00:03:28,902 --> 00:03:32,252
experiments, number of clients, the adoption experimentation

26
00:03:32,316 --> 00:03:36,492
has increased in kosher. And then we will talk about the new architecture,

27
00:03:36,636 --> 00:03:40,148
how we have made changes in the new architecture to cater the

28
00:03:40,234 --> 00:03:43,556
new scale. So what is a b testing? A B

29
00:03:43,578 --> 00:03:47,700
testing is basically a randomized experimentation process

30
00:03:47,850 --> 00:03:51,632
wherein two or more versions of features are shown to

31
00:03:51,706 --> 00:03:55,432
different segment of users at the same time. After running the

32
00:03:55,486 --> 00:03:58,916
experiment for some days or weeks, we analyze

33
00:03:58,948 --> 00:04:02,324
the data. And then this data helps us determine

34
00:04:02,372 --> 00:04:06,060
which version leaves the maximum impact and drive the business

35
00:04:06,130 --> 00:04:09,020
metrics. Let's for an example,

36
00:04:09,090 --> 00:04:12,584
understand this concept with an example. The allocation

37
00:04:12,632 --> 00:04:16,984
algorithm, right? The allocation algorithm is

38
00:04:17,042 --> 00:04:20,256
used for allocating drivers to customers. Suppose you

39
00:04:20,278 --> 00:04:23,856
have an existing algorithm which is allocating drivers to customers.

40
00:04:23,958 --> 00:04:27,456
Now you have a hypothesis that by changing few of the

41
00:04:27,478 --> 00:04:29,940
parameters in your allocation algorithm,

42
00:04:30,440 --> 00:04:34,864
you will be able to allocate your driver efficiently

43
00:04:34,912 --> 00:04:38,724
to your customers, which will impact your business metric, which is

44
00:04:38,762 --> 00:04:42,280
booking conversion metric. And this metric will

45
00:04:42,350 --> 00:04:45,784
increase by making this change in the algorithm. So what you will

46
00:04:45,822 --> 00:04:50,084
do, instead of directly rolling this out algorithm to production,

47
00:04:50,212 --> 00:04:53,896
which can be very critical and

48
00:04:53,918 --> 00:04:57,768
dangerous, because you do not know whether how this will perform in

49
00:04:57,854 --> 00:05:01,656
production. Right? So you will be running an experiment with the existing algorithm

50
00:05:01,688 --> 00:05:05,308
and the new algorithm which you are proposing, you will run this experiment for

51
00:05:05,314 --> 00:05:08,992
a few weeks and collect the data, and then you will analyze the data and

52
00:05:09,046 --> 00:05:12,384
see for which variation the booking conversion rate has

53
00:05:12,422 --> 00:05:16,370
increased. If it is a new variation, then you will be rolling that out

54
00:05:16,980 --> 00:05:20,656
to all the users in a staggered

55
00:05:20,688 --> 00:05:24,580
manner. If it is not, then you are sure that whatever

56
00:05:24,650 --> 00:05:28,324
you thought was not right and the current algorithm is performing better

57
00:05:28,362 --> 00:05:32,120
than the suggestion the new algorithm.

58
00:05:32,540 --> 00:05:36,040
Right? So let's move to the next slide.

59
00:05:36,620 --> 00:05:39,736
Yeah, so these days a

60
00:05:39,758 --> 00:05:43,144
lot of companies are investing in experimentation. Not only

61
00:05:43,182 --> 00:05:46,620
big giants, even small startups are also

62
00:05:46,770 --> 00:05:50,268
doing experimentation on daily basis. So whatever

63
00:05:50,354 --> 00:05:53,052
apps which you are using right now, these days,

64
00:05:53,106 --> 00:05:57,112
right, it must be running at least few

65
00:05:57,266 --> 00:06:00,448
hundreds of experiments on each app.

66
00:06:00,614 --> 00:06:04,256
Because this makes sense as well, right? Because it is

67
00:06:04,358 --> 00:06:08,130
the data which is leading you to make the decision, right? Not just that

68
00:06:09,860 --> 00:06:13,316
your manager or some high paying people are coming and saying that

69
00:06:13,418 --> 00:06:16,564
this is how we should do it and this will perform better and we are

70
00:06:16,602 --> 00:06:20,564
rolling this out to all the users. This decision is

71
00:06:20,682 --> 00:06:24,228
backend by data, right? So I'll quote one of

72
00:06:24,234 --> 00:06:27,544
the quote by Jeff Bezos. What he said

73
00:06:27,582 --> 00:06:31,464
is that if you double the number of experiments per year, you are going

74
00:06:31,502 --> 00:06:35,372
to double your inventiveness. He also

75
00:06:35,426 --> 00:06:39,436
knew that how critical experimentation is for

76
00:06:39,458 --> 00:06:43,320
a particular form. So Amazon

77
00:06:43,400 --> 00:06:46,784
also must be running pretty huge number of

78
00:06:46,822 --> 00:06:49,872
experiments for its different,

79
00:06:49,926 --> 00:06:50,850
different services.

80
00:06:53,540 --> 00:06:57,504
Let's move to the next slide. Yeah, Litmus. What is

81
00:06:57,542 --> 00:07:01,044
Litmus? So Litmus is basically the name

82
00:07:01,082 --> 00:07:04,820
of experimentation platform at Gojek

83
00:07:05,880 --> 00:07:09,284
Tech. Actually has two

84
00:07:09,402 --> 00:07:13,108
core features. One is a b testing, which we call experiment,

85
00:07:13,204 --> 00:07:16,584
where you come and define your experiment with

86
00:07:16,702 --> 00:07:19,928
two or more variants and you can do the

87
00:07:19,934 --> 00:07:23,716
A B testing. Second one is a staggered rollout, right? Once you

88
00:07:23,758 --> 00:07:28,664
have the result of your experiment, you can promote

89
00:07:28,712 --> 00:07:32,124
your winning variant to a release, like rolling that

90
00:07:32,162 --> 00:07:36,120
out to all the users in a staggered way. So that is

91
00:07:36,290 --> 00:07:39,584
called release. These are the two

92
00:07:39,622 --> 00:07:43,504
features currently supported by Litmus. The third one is that

93
00:07:43,542 --> 00:07:49,320
it also supports the targeting rules. I am purposefully

94
00:07:49,420 --> 00:07:53,172
mentioning this here because if you see here, right,

95
00:07:53,226 --> 00:07:58,324
we have a kind of rules like app version newer than 1.23.12.

96
00:07:58,362 --> 00:08:02,036
Sorry. So all the users who will have

97
00:08:02,058 --> 00:08:05,332
the app version newer than this will be part of this experiments.

98
00:08:05,396 --> 00:08:08,904
So these are some rules which we support.

99
00:08:09,102 --> 00:08:12,708
So I'm mentioning this because for each experiments

100
00:08:12,724 --> 00:08:16,252
we need to pass the rules for the experiments, right? Which makes

101
00:08:16,306 --> 00:08:20,332
this application as a cpu intensive. I want to highlight this

102
00:08:20,386 --> 00:08:23,868
word because it will be used in

103
00:08:23,874 --> 00:08:27,264
the system design. Also, it will help

104
00:08:27,302 --> 00:08:31,410
us design the system better because we know the expectation from this applications,

105
00:08:32,260 --> 00:08:35,696
right? So these are the key

106
00:08:35,718 --> 00:08:39,776
features supported by Litmus, which is the A

107
00:08:39,798 --> 00:08:43,348
B testing platform of Kojek. So what is

108
00:08:43,354 --> 00:08:46,736
the objective of this talk? Right. So I'll

109
00:08:46,768 --> 00:08:50,630
be walking you through my experience of building the

110
00:08:50,940 --> 00:08:56,568
litmus from scratch, to also

111
00:08:56,654 --> 00:09:00,456
the changes which we made to the system to cater the

112
00:09:00,638 --> 00:09:04,436
hydropole. So what are the non functional

113
00:09:04,468 --> 00:09:08,350
requirements which we will need to build the system?

114
00:09:08,720 --> 00:09:12,940
So these are the core non functional requirements. First is the low latency.

115
00:09:13,680 --> 00:09:17,168
Low latency is required because

116
00:09:17,334 --> 00:09:21,504
if you see, right, experimentation is a new

117
00:09:21,542 --> 00:09:25,570
call in your flow, right? Suppose if you are running an

118
00:09:26,260 --> 00:09:30,704
algorithm for driver allocation, right now you have only one algorithm,

119
00:09:30,752 --> 00:09:34,324
so you know which algorithm to use. If you are running

120
00:09:34,362 --> 00:09:37,936
an experiment, you added one more branching

121
00:09:37,968 --> 00:09:42,216
there, right. You will talk to the experimentation service, get to know which

122
00:09:42,398 --> 00:09:46,084
algorithm to use and then you will use that algorithm.

123
00:09:46,212 --> 00:09:49,624
So this low latency is very critical here because it is

124
00:09:49,662 --> 00:09:53,624
going to impact the overall flow, right. Second one

125
00:09:53,662 --> 00:09:57,784
is the highly available, which I think is given for all the systems.

126
00:09:57,912 --> 00:10:01,356
The system has to be highly available because it

127
00:10:01,378 --> 00:10:03,980
is being used in lot of flows,

128
00:10:05,120 --> 00:10:09,232
a high throughput. A high throughput in the sense, because in

129
00:10:09,366 --> 00:10:13,010
Gojek pretty much every services are using this

130
00:10:13,620 --> 00:10:17,136
platform for running experiments. So we are expected to get the

131
00:10:17,158 --> 00:10:20,016
high throughput, weak consistency.

132
00:10:20,128 --> 00:10:23,364
Basically the data should be

133
00:10:23,402 --> 00:10:26,740
eventually consistent. Like if anyone making some changes

134
00:10:26,810 --> 00:10:30,950
in the experiments, it should eventually be reflected on the client side.

135
00:10:32,380 --> 00:10:36,456
Yeah. These are the objectives. Let's go to

136
00:10:36,638 --> 00:10:38,650
the architecture, how we started.

137
00:10:39,420 --> 00:10:43,224
So let's look into the initial architecture, how the idea

138
00:10:43,262 --> 00:10:46,430
of litmus started, how we build the

139
00:10:46,960 --> 00:10:49,500
version one of Litmus.

140
00:10:50,720 --> 00:10:54,990
So we have this Litmus server, right? So Litmus server is basically

141
00:10:55,680 --> 00:10:59,056
a b testing platform. The application which have all the

142
00:10:59,238 --> 00:11:02,892
functional requirements like supporting experiments,

143
00:11:02,956 --> 00:11:05,680
releases rules, et cater,

144
00:11:07,300 --> 00:11:11,264
which is backed by postgres. We have a primary database

145
00:11:11,312 --> 00:11:15,024
postgres. Right. So how the initial

146
00:11:15,072 --> 00:11:19,124
flow was, we had a portal actually. So the

147
00:11:19,162 --> 00:11:22,608
experimenters can come to portal, can create experiments,

148
00:11:22,784 --> 00:11:26,216
update their created experiments. Basically can do any kind

149
00:11:26,238 --> 00:11:29,896
of crud on their experiments. So how the flow was.

150
00:11:29,998 --> 00:11:33,680
Whenever they are making any changes on the portal, portal will talk to the litmus

151
00:11:33,700 --> 00:11:37,180
server and litmus server will reflect that changes to the DB.

152
00:11:38,240 --> 00:11:41,756
And we have clients. Clients are basically the,

153
00:11:41,858 --> 00:11:45,372
you can say microservices or mobile clients, any clients which

154
00:11:45,426 --> 00:11:49,132
needs to talk to experimentation platform to decide which variants

155
00:11:49,196 --> 00:11:52,770
to use, which variation of an experiment to use.

156
00:11:53,780 --> 00:11:56,770
So how does this flow work?

157
00:11:58,440 --> 00:12:01,968
Clients talks to the Litmus server and litmus

158
00:12:01,984 --> 00:12:06,256
server talks to the DB, fetches all the valid experiments

159
00:12:06,288 --> 00:12:09,920
and return those to clients. So this is the initial

160
00:12:10,080 --> 00:12:13,396
architecture. You must

161
00:12:13,418 --> 00:12:16,696
be thinking that this is pretty knife and there are a

162
00:12:16,718 --> 00:12:19,944
couple of limitations here. Yeah, and we will be talking about

163
00:12:19,982 --> 00:12:23,272
those limitations in the next slide. Let's go to next

164
00:12:23,326 --> 00:12:27,004
slide. Yeah, so limitations. First one is the

165
00:12:27,042 --> 00:12:30,270
single cluster handling the request of read and write.

166
00:12:30,640 --> 00:12:33,470
Right? So what is the problem here?

167
00:12:34,560 --> 00:12:38,112
So the problem is the read throughput and

168
00:12:38,166 --> 00:12:42,572
write throughput are very different for litmus.

169
00:12:42,636 --> 00:12:46,428
The read throughput is around six to seven x larger

170
00:12:46,444 --> 00:12:51,060
than the write throughput. Right. So the tuning,

171
00:12:51,480 --> 00:12:55,412
the scaling required for the read will be different

172
00:12:55,466 --> 00:12:58,800
from the write. Right. So if we are using a single cluster,

173
00:12:58,880 --> 00:13:03,544
and suppose read is using

174
00:13:03,582 --> 00:13:06,890
a lot of resources, it might affect the write.

175
00:13:07,420 --> 00:13:11,396
Suppose if there is some outage happened on that cluster,

176
00:13:11,508 --> 00:13:15,292
read and write, both will be affected by those. So this is

177
00:13:15,346 --> 00:13:19,836
one of the potential problem in this

178
00:13:20,018 --> 00:13:23,256
whole architecture. The second one is primary database

179
00:13:23,288 --> 00:13:27,376
being used for read and write. So we

180
00:13:27,398 --> 00:13:33,644
have a primary and recovery

181
00:13:33,692 --> 00:13:36,944
database. Primary is basically which

182
00:13:36,982 --> 00:13:40,704
will handle all the writes and read ports. Other replicas

183
00:13:40,752 --> 00:13:45,270
are basically where you can read the data. If we had the

184
00:13:47,080 --> 00:13:50,928
read clients directly read from the

185
00:13:51,114 --> 00:13:54,650
application server through directly read through

186
00:13:55,260 --> 00:13:58,570
replica database via litmus application,

187
00:13:59,020 --> 00:14:02,650
it will be better in the performance that even if the

188
00:14:03,420 --> 00:14:06,750
primary database is down, they can read through the

189
00:14:07,440 --> 00:14:11,176
replica database, which is like reading the data is a critical flow

190
00:14:11,208 --> 00:14:15,196
here because a lot of the microservices will be talking to

191
00:14:15,378 --> 00:14:18,624
the experimentation platform at

192
00:14:18,662 --> 00:14:22,624
API level, right? So even if the primary database is

193
00:14:22,662 --> 00:14:26,204
not performing well or running some kind of maintenance,

194
00:14:26,332 --> 00:14:31,404
they can read from the recovery

195
00:14:31,452 --> 00:14:35,344
database. The third one is a single deployment.

196
00:14:35,472 --> 00:14:38,836
Single deployment as in like if you are making any changes

197
00:14:38,938 --> 00:14:42,676
related to write APIs, the deployment will

198
00:14:42,698 --> 00:14:46,232
go for both the read and write. Right. If you are making any changes

199
00:14:46,286 --> 00:14:49,540
related to read APIs,

200
00:14:49,700 --> 00:14:53,284
like any optimization you have done, you will have to make the deployment.

201
00:14:53,332 --> 00:14:56,908
And that deployment time deployment will affect both read and write,

202
00:14:57,074 --> 00:15:00,684
right? So the separation for

203
00:15:00,722 --> 00:15:04,540
read and write is not here, which we will talk about in the next slide.

204
00:15:06,180 --> 00:15:09,424
So what we did next, we separated the read and write

205
00:15:09,462 --> 00:15:13,040
cluster to scale it

206
00:15:13,110 --> 00:15:16,880
better because the throughput difference was quite

207
00:15:17,030 --> 00:15:20,260
large. So let's go to the architecture.

208
00:15:20,920 --> 00:15:23,540
We have Litmus write servers,

209
00:15:24,200 --> 00:15:26,900
we have litmus read servers,

210
00:15:27,800 --> 00:15:31,788
we have primary database, and we have replica,

211
00:15:31,984 --> 00:15:35,816
which will be replicating all

212
00:15:35,838 --> 00:15:39,176
the data from primary on

213
00:15:39,198 --> 00:15:42,936
its site. So how the portal will

214
00:15:42,958 --> 00:15:46,540
be talking right now, portal will talk to the write servers.

215
00:15:46,880 --> 00:15:50,124
And similar to the previous fashion, the write

216
00:15:50,162 --> 00:15:54,764
server will make those changes in the DB and

217
00:15:54,802 --> 00:15:58,812
all the clients now will be talking to read servers and

218
00:15:58,866 --> 00:16:02,476
read servers will be talking, fetching the data from replica and returning

219
00:16:02,508 --> 00:16:05,904
them to claims. This is how the system will look like

220
00:16:05,942 --> 00:16:08,480
once we segregate the read and write clusters.

221
00:16:09,640 --> 00:16:13,300
Let's go to the pros and cons of this architecture.

222
00:16:14,920 --> 00:16:18,804
Pros read and write cluster can

223
00:16:18,842 --> 00:16:22,216
scale independently, which is correct, right. If your

224
00:16:22,238 --> 00:16:25,768
read throughput is increasing insanely, you can just

225
00:16:25,854 --> 00:16:29,672
scale your read cluster. And even

226
00:16:29,726 --> 00:16:33,224
if your read cluster is for

227
00:16:33,262 --> 00:16:36,936
some time affecting the response time,

228
00:16:37,118 --> 00:16:40,492
right. Because of the high throughput, your write cluster or

229
00:16:40,546 --> 00:16:44,344
your portal will not face any kind of issue unless

230
00:16:44,392 --> 00:16:46,270
the issue is on the database side.

231
00:16:47,540 --> 00:16:51,212
Decoupled deployment the deployments will be faster,

232
00:16:51,276 --> 00:16:54,770
right? Because the read changes and the write changes

233
00:16:55,460 --> 00:16:59,968
are independent raw, right. The deployment on read is not affecting

234
00:17:00,064 --> 00:17:03,444
the write and

235
00:17:03,482 --> 00:17:06,916
vice versa. Because anytime on a high

236
00:17:06,938 --> 00:17:10,464
throughput service, if you are doing a deployment on day, right, so it will affect

237
00:17:10,512 --> 00:17:14,200
all the clients that time. So if we have a read and write client separately,

238
00:17:16,540 --> 00:17:19,816
we will be able to deploy the write cluster at any time

239
00:17:19,838 --> 00:17:22,490
because it is a low throughput service.

240
00:17:24,400 --> 00:17:27,448
Third one is the master slave setup,

241
00:17:27,544 --> 00:17:31,352
basically primary or replica setup.

242
00:17:31,496 --> 00:17:35,164
Here the write

243
00:17:35,202 --> 00:17:38,604
cluster is talking to the primary

244
00:17:38,652 --> 00:17:42,880
database and the read cluster is talking to the Replica database.

245
00:17:44,020 --> 00:17:48,064
So you can tune the replica database to increase the number

246
00:17:48,102 --> 00:17:51,060
of connections for your applications.

247
00:17:51,400 --> 00:17:54,896
And there won't be much load on the master

248
00:17:55,088 --> 00:17:59,124
because it will only be serving the right

249
00:17:59,162 --> 00:18:02,730
throughput. So there are some cons as well,

250
00:18:03,180 --> 00:18:06,776
which I think we will understand later in the

251
00:18:06,878 --> 00:18:11,092
talk. The cons are all the clients are sharing the resources,

252
00:18:11,156 --> 00:18:14,628
right? Means there are a lot of clients

253
00:18:14,724 --> 00:18:17,916
which are talking to experimentation platform. They all have

254
00:18:17,938 --> 00:18:21,404
a common resource. Like the read cluster has some amount of

255
00:18:21,442 --> 00:18:25,908
cpu, right? That cpu is being shared by all the clients requests,

256
00:18:26,104 --> 00:18:29,552
right? It is a problem. I'll explain this in the

257
00:18:29,606 --> 00:18:33,116
later section. Not horizontally scalable

258
00:18:33,148 --> 00:18:37,776
is the second con or the limitation. I would say it

259
00:18:37,798 --> 00:18:41,264
is because the underlying

260
00:18:41,312 --> 00:18:44,790
database is a postgres, right? You can

261
00:18:45,480 --> 00:18:49,236
have certain amount of connections there, right. If you

262
00:18:49,258 --> 00:18:52,408
are going to do the horizontal scaling, each server will

263
00:18:52,414 --> 00:18:56,152
be making a connection to database, right? So you need to keep in mind that

264
00:18:56,206 --> 00:18:59,380
as well, it's not easily horizontal scalable.

265
00:18:59,540 --> 00:19:03,796
Whenever you are reaching the max connection configured

266
00:19:03,828 --> 00:19:07,516
on the DB side, your server will be crashing, right? Because it will not be

267
00:19:07,538 --> 00:19:10,876
able to get the ideal connections with the DB. So this

268
00:19:10,898 --> 00:19:15,352
is one of the limitations in the real write segregation

269
00:19:15,416 --> 00:19:18,652
crash. Yeah. So let's

270
00:19:18,716 --> 00:19:22,224
talk about scale over time, which is important to

271
00:19:22,262 --> 00:19:26,450
understand the changes in the architecture needs to be done to

272
00:19:27,860 --> 00:19:31,764
support the new scale. Right? So let's see how

273
00:19:31,802 --> 00:19:35,424
the number of clients, number of experiments or the number of religious

274
00:19:35,472 --> 00:19:38,836
has changed over the course of time. Let's see the

275
00:19:38,858 --> 00:19:42,052
number of clients over time. So from

276
00:19:42,106 --> 00:19:45,640
2019 to 2022, I would say

277
00:19:45,790 --> 00:19:48,890
see the number of clients, how it has increased, right.

278
00:19:49,660 --> 00:19:52,936
Every quarter it is increasing somewhere it

279
00:19:52,958 --> 00:19:56,556
is increasing slow and somewhere it has increased by

280
00:19:56,578 --> 00:20:00,604
a big margin. But the trend is increasing, right. It is not anytime at

281
00:20:00,642 --> 00:20:04,716
constant trend. So this is one

282
00:20:04,738 --> 00:20:08,944
of the major part in the scale that the

283
00:20:08,982 --> 00:20:12,048
different clients will be talking to your systems now,

284
00:20:12,214 --> 00:20:15,452
right? And each client can have different requirements.

285
00:20:15,516 --> 00:20:19,316
Some clients can run hundred of experiments, some will be running two or

286
00:20:19,338 --> 00:20:23,844
three experiments, right? So there

287
00:20:23,882 --> 00:20:27,444
is a diversity in the clients as well. We'll talk about

288
00:20:27,482 --> 00:20:31,140
that later. Let's see the number of active experiment at a time

289
00:20:31,210 --> 00:20:35,476
over a course of period from 2019 to 2022,

290
00:20:35,498 --> 00:20:38,824
right? If you see, we started with 178, we reached to

291
00:20:38,862 --> 00:20:42,200
739, and now in 2020, and we

292
00:20:42,270 --> 00:20:46,024
were about 1.8k. These are the active

293
00:20:46,072 --> 00:20:49,336
experiments. Like running experiments, we have excluded

294
00:20:49,368 --> 00:20:52,936
the completed or stopped experiment. Stopped experiments.

295
00:20:53,128 --> 00:20:56,572
So this actually is the distribution

296
00:20:56,636 --> 00:21:00,636
of clients experiments. The number of experiments

297
00:21:00,748 --> 00:21:03,932
running for a particular client, you see it's pretty uneven,

298
00:21:03,996 --> 00:21:07,856
right? So the expectation from

299
00:21:07,878 --> 00:21:11,236
the clients is also very different. Right. Few clients will be

300
00:21:11,258 --> 00:21:14,790
running hundreds of experiments. They are okay with the

301
00:21:15,400 --> 00:21:19,104
large response time. Few clients are running only two and three experiments,

302
00:21:19,152 --> 00:21:22,432
but they are not okay with the large latencies.

303
00:21:22,496 --> 00:21:26,468
Right. Because the response time is proportional

304
00:21:26,484 --> 00:21:30,036
to the number of experiments. Right. Because we have the rule configured

305
00:21:30,068 --> 00:21:33,944
for the experiments, and we have to parse a rule for each experiment,

306
00:21:33,992 --> 00:21:37,624
right? So somehow this response

307
00:21:37,672 --> 00:21:42,284
time is proportional to the number

308
00:21:42,322 --> 00:21:46,060
of experiment running for a client, right? So here, if you see

309
00:21:46,210 --> 00:21:49,904
it is not only the overall skills, it is

310
00:21:49,942 --> 00:21:54,012
also the diversity in the client. Right. What is the expectation

311
00:21:54,076 --> 00:21:57,936
from each client if you are sharing the resources? Right.

312
00:21:58,038 --> 00:22:01,896
Suppose some clients is okay with large latency.

313
00:22:01,948 --> 00:22:05,424
Suppose 50 ms as 99 percentile.

314
00:22:05,472 --> 00:22:08,564
But some clients are not okay with it because they're only running one or two

315
00:22:08,602 --> 00:22:12,264
experiments. Why would they want their 99 percentile to be this high?

316
00:22:12,302 --> 00:22:16,440
Right. So this was one of the other concern.

317
00:22:16,780 --> 00:22:19,800
Let's look the similar graph for releases.

318
00:22:20,220 --> 00:22:23,848
This is the trend for the active releases over course of time.

319
00:22:24,014 --> 00:22:27,420
And this is the distribution of number of releases

320
00:22:28,000 --> 00:22:31,436
per client. This is similar to what we saw in the experiment. And the

321
00:22:31,458 --> 00:22:35,116
problem is also the similar. Right? And if you see

322
00:22:35,138 --> 00:22:40,128
the numbers, numbers are also similar. We had around 1.8k

323
00:22:40,294 --> 00:22:43,312
experiments at the end of 2022,

324
00:22:43,366 --> 00:22:46,672
built in releases, we had 1.6k.

325
00:22:46,726 --> 00:22:49,648
Now, you see, from 2022 to 2023,

326
00:22:49,734 --> 00:22:53,284
we have already contributed, like increased the

327
00:22:53,322 --> 00:22:57,364
active releases by two k. So you see the

328
00:22:57,402 --> 00:22:59,700
scale, right? How is it increasing?

329
00:23:01,420 --> 00:23:05,560
So let's see what changes we made in the architecture to

330
00:23:05,710 --> 00:23:09,140
cater this release, right? Cater this scale.

331
00:23:09,220 --> 00:23:12,776
Sorry. So let's look

332
00:23:12,798 --> 00:23:16,376
at some of the points here that every year Litmus is receiving a

333
00:23:16,398 --> 00:23:19,180
new scale, right? At the end of 2019,

334
00:23:19,920 --> 00:23:23,576
the max throughput was coming around 100k rpm.

335
00:23:23,768 --> 00:23:27,020
These all numbers are rpm. I forgot to mention in the slide,

336
00:23:27,840 --> 00:23:31,164
at the end of 2020, it was around 500k

337
00:23:31,202 --> 00:23:34,924
rpm. At the end of 2021, it was around 1.1 million rpm.

338
00:23:34,972 --> 00:23:38,240
At the end of 2022, it reached to 2 million rpm.

339
00:23:40,820 --> 00:23:43,520
So the last two architectures we saw it,

340
00:23:43,670 --> 00:23:47,280
the very basic and naive architecture with a single server

341
00:23:48,500 --> 00:23:52,052
and the second one with a segregated read and right cluster,

342
00:23:52,196 --> 00:23:55,896
were able to handle around 500k throughput within the

343
00:23:55,998 --> 00:23:59,640
committed SlA. Beyond that, it was like

344
00:23:59,790 --> 00:24:02,904
the responses were frustrated and we

345
00:24:02,942 --> 00:24:06,110
were seeing some errors as well because of the

346
00:24:06,720 --> 00:24:10,012
resource scarcity, right?

347
00:24:10,146 --> 00:24:14,104
So, looking at the adoption and the growth of experimentation

348
00:24:14,152 --> 00:24:17,264
per year, which we saw in the scale slide, right.

349
00:24:17,302 --> 00:24:20,930
How the number of experiments and releases are increasing over a course of

350
00:24:21,300 --> 00:24:24,928
time, we adopted a sidecar pattern to optimize the

351
00:24:24,934 --> 00:24:28,872
latency, right? Why sidecar pattern?

352
00:24:28,956 --> 00:24:32,644
Sidecar pattern, because by doing sidecar, if you are running

353
00:24:32,682 --> 00:24:36,592
the sidecars on the client server,

354
00:24:36,736 --> 00:24:41,052
you are distributing the resources, right? So resources are distributed.

355
00:24:41,136 --> 00:24:45,252
So if some clients want to run thousand experiments,

356
00:24:45,316 --> 00:24:47,976
right? Say thousand is a random number,

357
00:24:48,078 --> 00:24:51,016
say 100 experiments want to run, right?

358
00:24:51,118 --> 00:24:54,344
So they will be configuring that

359
00:24:54,382 --> 00:24:57,996
much of resources to the sidecar, right? If some clients are running only

360
00:24:58,018 --> 00:25:01,544
one or two experiments, why would they bother to give good resources?

361
00:25:01,592 --> 00:25:04,696
They will just be giving one code to that sidecar,

362
00:25:04,728 --> 00:25:08,404
right? Or less, for running the sidecar.

363
00:25:08,552 --> 00:25:12,284
So this came to our mind because of the diversity

364
00:25:12,332 --> 00:25:16,076
in the clients, right? We thought that let's make it distributed,

365
00:25:16,188 --> 00:25:19,860
let's make the resources distributed and

366
00:25:19,930 --> 00:25:24,884
build the platform so that it can handle the

367
00:25:24,922 --> 00:25:28,544
scale as well as the diversity in the scale.

368
00:25:28,672 --> 00:25:32,324
Let's look at the architecture diagram. Now. How does it look like

369
00:25:32,442 --> 00:25:36,868
we have a. Right servers, right. We have the primary database.

370
00:25:37,044 --> 00:25:40,330
So portal flow will be similar to what it was, right.

371
00:25:41,500 --> 00:25:45,644
All the portal requests will go to the right server, and write server will be

372
00:25:45,842 --> 00:25:49,070
reflecting those changes in the primary database. Right.

373
00:25:49,920 --> 00:25:53,736
Now, how the clients will be connecting. So these yellow

374
00:25:53,768 --> 00:25:57,296
boxes are client servers and these are

375
00:25:57,318 --> 00:26:01,104
the client processes in gray, right? So we

376
00:26:01,142 --> 00:26:05,312
have deployed the sidecar processes along

377
00:26:05,366 --> 00:26:09,128
with the client main process, right? These sidecars

378
00:26:09,244 --> 00:26:13,456
will be syncing the data from right servers

379
00:26:13,648 --> 00:26:17,830
periodically, let's say 5 seconds every five,

380
00:26:18,840 --> 00:26:22,364
which is a configurable value. Every, let's say t seconds

381
00:26:22,432 --> 00:26:25,924
they will fetch all the experiments for that particular client

382
00:26:26,052 --> 00:26:29,348
and they will store it in patcher tv. Sidecar uses Patcher

383
00:26:29,364 --> 00:26:32,904
tv. So periodically they are syncing the

384
00:26:33,022 --> 00:26:36,316
data from write server and write server has

385
00:26:36,338 --> 00:26:40,076
an API exposed which is basically fetch all the

386
00:26:40,098 --> 00:26:43,240
data and will return it to sidecar processes.

387
00:26:43,400 --> 00:26:46,684
So every t second they will

388
00:26:46,722 --> 00:26:50,332
be fetching the data from write server and store that in the badger Db.

389
00:26:50,396 --> 00:26:53,856
So whenever a client make a request to sitecast, Sidecar will read

390
00:26:53,878 --> 00:26:57,792
the data from the badger DB and do all the rule parsing and everything

391
00:26:57,846 --> 00:27:01,360
and return the valid experiments to client.

392
00:27:02,120 --> 00:27:05,156
So if you see this right here,

393
00:27:05,338 --> 00:27:08,612
all the computations are distributed over

394
00:27:08,666 --> 00:27:12,228
each servers, right? Rather than doing all the computations

395
00:27:12,324 --> 00:27:15,588
on a particular set of servers, it has been distributed.

396
00:27:15,764 --> 00:27:19,770
So by this architecture it can scale to

397
00:27:20,700 --> 00:27:24,092
any value, right? Because here we are not making

398
00:27:24,146 --> 00:27:27,944
any direct connection with the database, we are syncing the data via API.

399
00:27:28,072 --> 00:27:32,252
The connections are maintained by the right server with the database and

400
00:27:32,306 --> 00:27:36,036
also the client's

401
00:27:36,088 --> 00:27:39,008
response time of.

402
00:27:39,094 --> 00:27:42,544
Sorry, the response time of one of the client is not getting affected by

403
00:27:42,582 --> 00:27:46,704
others, right. Let's see the key

404
00:27:46,742 --> 00:27:50,656
points for the sidecar pattern. So given Sidecar is distributed,

405
00:27:50,688 --> 00:27:55,124
it is designed with the availability and consistency. So using this

406
00:27:55,322 --> 00:27:58,740
pace theorem,

407
00:27:59,800 --> 00:28:03,796
basically if we have a partition,

408
00:28:03,988 --> 00:28:07,530
we will have to choose between availability and

409
00:28:07,980 --> 00:28:11,160
consistency. Else we have to choose between latency and

410
00:28:11,230 --> 00:28:14,684
consistency. So sidecar has been

411
00:28:14,722 --> 00:28:18,280
built with choosing the availability

412
00:28:18,360 --> 00:28:22,472
over consistency in case of network partition and consistency

413
00:28:22,536 --> 00:28:25,676
over latency if

414
00:28:25,778 --> 00:28:30,304
there is no network partition, right? So we

415
00:28:30,342 --> 00:28:34,016
decided to go ahead with this. We had a survey with our

416
00:28:34,198 --> 00:28:37,776
clients and we decided to go ahead with this. We do not have right now

417
00:28:37,798 --> 00:28:41,268
the options to configure the sidecar to support

418
00:28:41,354 --> 00:28:45,136
both consistency

419
00:28:45,248 --> 00:28:49,172
and latency with a configurable value. Right now

420
00:28:49,226 --> 00:28:53,316
sidecar just support availability in case of network partition,

421
00:28:53,428 --> 00:28:56,852
else consistency second one is horizontally

422
00:28:56,916 --> 00:29:00,392
scalable, which I already talked about, right? Since it is not making

423
00:29:00,446 --> 00:29:04,164
direct connection to the database, right? It is horizontally scalable.

424
00:29:04,292 --> 00:29:08,076
Like you can literally spun up like hundreds of thousands of servers which

425
00:29:08,098 --> 00:29:11,612
will talk to the litmus servers periodically, right?

426
00:29:11,666 --> 00:29:15,020
Because the throughput on the sidecar is not directly proportional to the

427
00:29:15,090 --> 00:29:18,544
throughput on the right application.

428
00:29:18,662 --> 00:29:22,112
Because if you see here, the call

429
00:29:22,166 --> 00:29:25,952
between sidecar process and write servers is happening periodically every

430
00:29:26,006 --> 00:29:29,696
t second. So irrespective of this throughput.

431
00:29:29,728 --> 00:29:31,910
Right. The client process to sidecar process.

432
00:29:33,160 --> 00:29:36,912
This throughput is not getting affected. So it is horizontally

433
00:29:36,976 --> 00:29:40,624
scalable. So the third part is resources are distributed,

434
00:29:40,672 --> 00:29:44,116
which I already explained. Right. The response time of a client will not be affected

435
00:29:44,148 --> 00:29:48,056
by the experiments of other clients. Because the resources are distributed and

436
00:29:48,078 --> 00:29:51,908
their clients will be running the sidecar on their own resources.

437
00:29:52,084 --> 00:29:55,556
And also we have a central monitoring for all the sidescar on the new relic

438
00:29:55,588 --> 00:29:59,272
and Grafana. Because as an experimentation

439
00:29:59,336 --> 00:30:02,956
platform, if we are providing a sidecar, we need to check the health of all

440
00:30:02,978 --> 00:30:06,796
the sidescars as well. Right. If someone is complaining that my sidecar is

441
00:30:06,818 --> 00:30:10,316
not working, you need to have the observability.

442
00:30:10,428 --> 00:30:13,936
Right? What is the issue? Is it working fine? Since when it

443
00:30:13,958 --> 00:30:17,344
is not working? So we have monitoring as well for all

444
00:30:17,382 --> 00:30:20,736
those sidecars. Yeah. So this brings

445
00:30:20,768 --> 00:30:24,468
us to the last section of the talk.

446
00:30:24,554 --> 00:30:28,404
Basically how the throughput and response times

447
00:30:28,602 --> 00:30:32,180
look like right now. This is one of the screenshot I have took

448
00:30:32,250 --> 00:30:35,944
while making this slide at a particular time.

449
00:30:35,982 --> 00:30:39,704
So if you see this is. This is reaching up to 1.61

450
00:30:39,742 --> 00:30:43,224
million rpm, the throughput and if you see the 99

451
00:30:43,262 --> 00:30:45,960
percentile, right. It is pretty efficient.

452
00:30:46,040 --> 00:30:48,632
9.52 ms.

453
00:30:48,696 --> 00:30:52,830
Right. You see the scale and the response time. This is the

454
00:30:53,600 --> 00:30:57,232
most optimized thing which we were targeting for.

455
00:30:57,366 --> 00:31:01,500
And using this sidecar pattern, we were able to deliver

456
00:31:01,660 --> 00:31:05,490
this, right. And make the system

457
00:31:06,580 --> 00:31:10,724
more available and

458
00:31:10,922 --> 00:31:14,148
more diversified according to support

459
00:31:14,234 --> 00:31:18,356
the requirement of different clients. So yeah,

460
00:31:18,458 --> 00:31:22,584
this is the overall learning for me while

461
00:31:22,622 --> 00:31:25,544
building the experimentation platform.

462
00:31:25,742 --> 00:31:29,048
Yes. So currently we are

463
00:31:29,214 --> 00:31:32,772
focusing mostly on the building analytics systems.

464
00:31:32,916 --> 00:31:36,616
But I think that is not the part of this talk that is out

465
00:31:36,638 --> 00:31:39,672
of scope of this talk. So yeah,

466
00:31:39,726 --> 00:31:42,490
that's all from my side. Thanks a lot.

467
00:31:43,100 --> 00:31:47,520
Please reach out to me on Twitter or LinkedIn for any questions you have.

468
00:31:47,670 --> 00:31:50,848
I have written a few blogs as well on this.

469
00:31:51,014 --> 00:31:54,368
If you are interested, you can go on media man. Thanks a lot. Have a

470
00:31:54,374 --> 00:31:54,736
nice day,

