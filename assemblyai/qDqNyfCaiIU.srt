1
00:00:26,050 --> 00:00:30,066
Good morning. Hello. My name is Pavos

2
00:00:30,098 --> 00:00:34,038
Kripek, and today with Anna Warno, we will tell a

3
00:00:34,044 --> 00:00:37,250
little bit more about the time series forecasting.

4
00:00:37,330 --> 00:00:41,030
We will focus on advanced ensembling methods.

5
00:00:41,450 --> 00:00:45,042
As you probably know, we are working with the time series

6
00:00:45,106 --> 00:00:48,646
for quite a long time using the

7
00:00:48,668 --> 00:00:52,798
most advanced method, and today we want to briefly tell

8
00:00:52,884 --> 00:00:56,206
about ensembling. So why the

9
00:00:56,228 --> 00:00:59,978
ensembling is important? Why to use the ensembling?

10
00:01:00,154 --> 00:01:03,902
How the ensembling is used by the leading forecasting

11
00:01:03,966 --> 00:01:07,454
methods. I will start with the brief

12
00:01:07,502 --> 00:01:11,570
introduction with some slides to introduce the topic and

13
00:01:11,640 --> 00:01:15,050
to provide a little bit of theoretical background.

14
00:01:15,230 --> 00:01:19,590
And then Anya will make a hands on session and

15
00:01:19,740 --> 00:01:23,042
present the different ensembling methods.

16
00:01:23,106 --> 00:01:27,250
The results on the real data sets using different ensembling

17
00:01:27,330 --> 00:01:31,354
method. And Anya also will present

18
00:01:31,552 --> 00:01:35,434
the advanced ensembling method based on the

19
00:01:35,472 --> 00:01:38,810
machine learning algorithms neural networks.

20
00:01:39,890 --> 00:01:43,198
The methods has been prepared by her.

21
00:01:43,284 --> 00:01:47,482
So it's a very interesting and unique approach

22
00:01:47,546 --> 00:01:51,390
for the ensembling. Okay, so starting

23
00:01:51,460 --> 00:01:55,026
from the beginning, what is ensembling? Ensembling is

24
00:01:55,048 --> 00:01:59,730
just a simply used outcome from the multiple forecasters

25
00:02:00,070 --> 00:02:03,278
to achieve better accuracy of prediction,

26
00:02:03,374 --> 00:02:07,250
better robustness and stability of the results.

27
00:02:08,070 --> 00:02:11,430
It's nothing new. The ensembling has been used

28
00:02:11,500 --> 00:02:13,320
for the very long time.

29
00:02:16,730 --> 00:02:20,778
Mostly it was based in the simple form. I will present

30
00:02:20,864 --> 00:02:24,140
that, but also

31
00:02:24,910 --> 00:02:28,502
there were present some additional more advanced

32
00:02:28,566 --> 00:02:32,062
algorithms of assembling. I will tell a bit

33
00:02:32,116 --> 00:02:35,710
more about that in the presentation.

34
00:02:36,370 --> 00:02:39,886
Why disassembly is important, as you know,

35
00:02:39,988 --> 00:02:43,806
because we already told about that on the previous editions

36
00:02:43,838 --> 00:02:47,518
of the data Science Summit.

37
00:02:47,694 --> 00:02:51,230
The real breakthrough in the time series

38
00:02:51,310 --> 00:02:55,150
forecasting has been achieved during

39
00:02:55,240 --> 00:02:58,646
the M four and M five competition. It is

40
00:02:58,828 --> 00:03:02,402
probably the most prestigious competition related

41
00:03:02,466 --> 00:03:05,574
to time serious forecasting organized by

42
00:03:05,612 --> 00:03:09,382
the professor Sparrows Macridiakis,

43
00:03:09,446 --> 00:03:13,370
the legend in the time series forecasting.

44
00:03:13,790 --> 00:03:17,622
It is very interested that both winning

45
00:03:17,686 --> 00:03:21,246
method for the first and for the second place on

46
00:03:21,268 --> 00:03:25,818
the M four were using ensembling

47
00:03:25,914 --> 00:03:30,174
in the different way, but both in the very innovative way.

48
00:03:30,292 --> 00:03:34,142
In my presentation, I will briefly say about the winning method,

49
00:03:34,206 --> 00:03:38,158
about the ES hybrid, how the ES hybrid

50
00:03:38,254 --> 00:03:41,918
used ensembling, and also how the ensembling

51
00:03:42,014 --> 00:03:45,254
was extended in the NBeats method. It is

52
00:03:45,292 --> 00:03:49,000
some kind of the successor or

53
00:03:50,010 --> 00:03:53,282
next method related to the time series

54
00:03:53,346 --> 00:03:56,642
forecasting, and in the hands on session,

55
00:03:56,706 --> 00:04:00,442
Anya will tell more about other methods and

56
00:04:00,496 --> 00:04:02,860
show how it works live.

57
00:04:05,150 --> 00:04:08,842
The most important thing about ensembling is

58
00:04:08,896 --> 00:04:12,094
that improves the accuracy and also the

59
00:04:12,132 --> 00:04:15,834
stability, because having multiple ensembles,

60
00:04:15,882 --> 00:04:19,710
multiple forecasters allow to avoid some

61
00:04:19,780 --> 00:04:23,722
negative effect of the individual forecaster

62
00:04:23,786 --> 00:04:27,780
in a particular period of the time.

63
00:04:30,310 --> 00:04:34,242
As I said, the winning methods from the m four and

64
00:04:34,296 --> 00:04:38,902
m five competition use very heavily the

65
00:04:38,956 --> 00:04:42,086
ensembling. What are the types of the

66
00:04:42,108 --> 00:04:45,640
ensembling? There are some, let's say simple

67
00:04:46,010 --> 00:04:49,510
types of the ensembling, like voting, stacking,

68
00:04:49,590 --> 00:04:52,502
bugging and combination of ensembling.

69
00:04:52,566 --> 00:04:56,186
So it simply means that, for example, in the

70
00:04:56,208 --> 00:05:00,640
voting, we are selecting the prediction which

71
00:05:01,170 --> 00:05:05,434
is provided by most of the forecasters,

72
00:05:05,562 --> 00:05:09,054
because assembling, of course could be used for the

73
00:05:09,172 --> 00:05:12,930
regression and classification problem. Voting is of course

74
00:05:13,000 --> 00:05:16,914
useful for the classification problem. And for

75
00:05:16,952 --> 00:05:20,594
example, if we want to use voting, then we are

76
00:05:20,792 --> 00:05:24,770
selecting the class which is

77
00:05:24,840 --> 00:05:28,230
most voted. So most of the forecasters

78
00:05:29,450 --> 00:05:33,266
predict that class. Of course, we can use additional rules

79
00:05:33,298 --> 00:05:37,094
for that, like some

80
00:05:37,132 --> 00:05:41,818
kind of the majority rules, or to

81
00:05:41,984 --> 00:05:45,830
exclude the forecasters with no predictions

82
00:05:45,910 --> 00:05:49,722
and so on. But general idea is quite

83
00:05:49,776 --> 00:05:53,374
simple. It is also very similar for the regression. We can

84
00:05:53,412 --> 00:05:57,326
use the average of the prediction. Of course we

85
00:05:57,348 --> 00:06:00,974
can add additional rules like removing outliers and

86
00:06:01,012 --> 00:06:03,550
so on, and other combination.

87
00:06:04,210 --> 00:06:08,550
That's the first category of the ensembling. So this simple ensembling,

88
00:06:08,650 --> 00:06:12,050
and there are also advanced ensembling approaches,

89
00:06:12,790 --> 00:06:16,342
about two of them I will tell a little bit more

90
00:06:16,396 --> 00:06:20,262
describing the methods, and one

91
00:06:20,316 --> 00:06:23,990
even more advanced will be presented by Anya during

92
00:06:24,060 --> 00:06:26,230
the hands on session.

93
00:06:27,210 --> 00:06:30,646
Very important thing is that these ensembling methods

94
00:06:30,678 --> 00:06:34,294
are currently very extensively developed.

95
00:06:34,342 --> 00:06:38,730
So new concepts, new ideas appearing.

96
00:06:39,550 --> 00:06:42,922
It looks that it's a very useful approach

97
00:06:42,986 --> 00:06:45,390
for the time series forecasters.

98
00:06:46,530 --> 00:06:50,442
How the assembling was used in the ES hybrid

99
00:06:50,506 --> 00:06:54,610
method, it is the winning methods from the m four competition.

100
00:06:55,110 --> 00:06:59,378
In this methods there are many innovative solutions or

101
00:06:59,464 --> 00:07:03,202
concepts. I told about that more

102
00:07:03,256 --> 00:07:07,270
on my previous session on

103
00:07:07,340 --> 00:07:10,310
previous edition of data Science summit.

104
00:07:10,970 --> 00:07:13,346
But from the ensembling perspective,

105
00:07:13,458 --> 00:07:17,710
the ES hybrid method uses very innovative

106
00:07:17,810 --> 00:07:21,638
approach for the ensembling. So grouping

107
00:07:21,734 --> 00:07:25,782
different models to the particular time series.

108
00:07:25,926 --> 00:07:29,980
So during the training process, each model is

109
00:07:30,450 --> 00:07:34,590
rated, the metrics are calculated,

110
00:07:36,850 --> 00:07:40,346
the used metrics is SMEP. The metrics

111
00:07:40,378 --> 00:07:43,890
are calculated and based on the value of the metrics,

112
00:07:44,470 --> 00:07:48,082
each model is assigned to one or more time

113
00:07:48,136 --> 00:07:51,966
series on which the results

114
00:07:51,998 --> 00:07:55,446
are the best. And per each time series, the set of

115
00:07:55,468 --> 00:07:59,350
the models, the best models from the whole training

116
00:07:59,500 --> 00:08:03,240
is keep kept. So after each

117
00:08:04,410 --> 00:08:08,186
of the epoch, the model is evaluated against

118
00:08:08,368 --> 00:08:12,234
all of the time series and assigned to the

119
00:08:12,272 --> 00:08:15,574
best one. Of course, one model could be assigned to the multiple

120
00:08:15,622 --> 00:08:16,730
time series.

121
00:08:20,910 --> 00:08:24,254
Final prediction of the test set is done by

122
00:08:24,292 --> 00:08:28,170
the set of the best model for the given time series

123
00:08:28,330 --> 00:08:32,346
selected during the trade. So that was

124
00:08:32,468 --> 00:08:35,618
a very unique approach to the assembling and

125
00:08:35,704 --> 00:08:38,210
also very successful.

126
00:08:40,070 --> 00:08:44,130
And the second method, which was not used

127
00:08:44,200 --> 00:08:47,638
in the m four competition, it was some kind of

128
00:08:47,804 --> 00:08:51,542
successor. Or the method which claims to

129
00:08:51,596 --> 00:08:55,890
have better results than es hybrid, is the nbeats method.

130
00:08:56,050 --> 00:09:00,262
And the NBIT method go even more with

131
00:09:00,316 --> 00:09:04,346
the assembling. I do not go into the details of

132
00:09:04,368 --> 00:09:07,914
the NBIT method because I described that on the previous edition of

133
00:09:07,952 --> 00:09:11,660
the data science summit. But generally speaking,

134
00:09:12,510 --> 00:09:16,778
from the ensembling point of view, the NBeats

135
00:09:16,954 --> 00:09:20,606
uses ensembled up to 100

136
00:09:20,708 --> 00:09:24,018
different models. So it ensembled 100 different

137
00:09:24,104 --> 00:09:30,062
model, and models are trained

138
00:09:30,126 --> 00:09:33,566
threefold. There are models trained on the subset

139
00:09:33,598 --> 00:09:37,160
of the training data with the different loss function

140
00:09:37,770 --> 00:09:41,574
and with the different horizon. Thanks to

141
00:09:41,612 --> 00:09:45,090
that, the outcome of the prediction

142
00:09:45,170 --> 00:09:47,790
is very diversified.

143
00:09:47,970 --> 00:09:51,750
So it increased the chances

144
00:09:51,830 --> 00:09:55,610
that one single model could give the wrong

145
00:09:55,680 --> 00:10:00,380
prediction, increased the chances that the one wrong

146
00:10:00,850 --> 00:10:04,510
model will not destroy the effect

147
00:10:04,580 --> 00:10:08,254
of the predictions. So in bits, there are up

148
00:10:08,292 --> 00:10:12,670
to 100 different model based for the predictions.

149
00:10:14,470 --> 00:10:17,758
Okay, that's all from my theoretical

150
00:10:17,854 --> 00:10:21,790
part. Just for summary, there is a dynamical

151
00:10:21,950 --> 00:10:24,850
development of the new ensembling method,

152
00:10:25,430 --> 00:10:29,810
and ensembling could give us could improve the efficiency

153
00:10:29,890 --> 00:10:34,242
of the predictions and especially increased

154
00:10:34,386 --> 00:10:37,506
robustness. We are working in

155
00:10:37,548 --> 00:10:41,034
the financial time series forecasters, and for the

156
00:10:41,072 --> 00:10:42,810
financial time series,

157
00:10:43,390 --> 00:10:47,062
ensembling significantly improves accuracy

158
00:10:47,126 --> 00:10:51,578
and robustness. So that's all about the theoretical introduction.

159
00:10:51,674 --> 00:10:55,710
I do my best to keep it short. And now

160
00:10:55,780 --> 00:10:59,006
anya will present hands on session how to

161
00:10:59,028 --> 00:11:03,806
use this ensembling and what are the results in practice.

162
00:11:03,998 --> 00:11:05,540
Thank you very much.

163
00:11:06,790 --> 00:11:10,450
So after the theoretical introduction, I will present

164
00:11:10,520 --> 00:11:14,210
some real case study from our work. I will be talking

165
00:11:14,280 --> 00:11:18,146
about the real time series ensembling on cloud resources

166
00:11:18,178 --> 00:11:22,038
predictions. We have a project

167
00:11:22,124 --> 00:11:25,670
where we are forecasting cloud resources usage,

168
00:11:26,410 --> 00:11:30,282
and based on the predictions we can make a decision whether to

169
00:11:30,336 --> 00:11:34,122
change, for example, number of instances or not. The whole

170
00:11:34,176 --> 00:11:38,074
process looks as follow different models for

171
00:11:38,112 --> 00:11:41,254
time series forecasting are trained independently

172
00:11:41,382 --> 00:11:44,838
of each other, so they are trained

173
00:11:44,854 --> 00:11:48,814
as separated models, and predictions from

174
00:11:48,852 --> 00:11:52,698
each of the trained method is sent with the

175
00:11:52,724 --> 00:11:55,970
same frequency. So for example, every 10 seconds.

176
00:11:56,710 --> 00:12:01,166
In this way we obtain the m predictions, where m is the number of forecasting

177
00:12:01,198 --> 00:12:04,958
methods for each timestamp. But it's

178
00:12:04,974 --> 00:12:08,658
the most convenient to have only one number and make decisions

179
00:12:08,754 --> 00:12:12,214
based on it. So we use

180
00:12:12,252 --> 00:12:15,794
an ensembler that not only returns a more convenient format

181
00:12:15,842 --> 00:12:19,146
of our prediction, but can also correct it for

182
00:12:19,168 --> 00:12:23,062
us. There are several challenges

183
00:12:23,126 --> 00:12:26,586
that we have to face. First of all, as I mentioned before,

184
00:12:26,688 --> 00:12:30,558
the predictions are made in real time, and for this reason there

185
00:12:30,564 --> 00:12:33,742
are often missing predictions. It may happen

186
00:12:33,796 --> 00:12:37,454
because there was some delay or the methods is not ready

187
00:12:37,492 --> 00:12:38,880
for predictions yet.

188
00:12:40,690 --> 00:12:44,082
We also need to ensure the versatility of our solution so

189
00:12:44,136 --> 00:12:48,094
that the final predictions are accurate for different types of applications

190
00:12:48,142 --> 00:12:51,218
and presented metrics and also robust to

191
00:12:51,224 --> 00:12:55,254
poorly predicted forecasters that sometimes happen large

192
00:12:55,292 --> 00:12:59,090
predictions error can lead to bad decisions and be very costly

193
00:12:59,170 --> 00:13:01,990
in cloud computing environment.

194
00:13:04,010 --> 00:13:08,106
And now I will show how we can use ensembling for this problem on

195
00:13:08,128 --> 00:13:11,766
an examples data set our predicted

196
00:13:11,798 --> 00:13:16,042
metric, our target metric is cpu usage and

197
00:13:16,096 --> 00:13:20,102
our data contains 6000 rows and

198
00:13:20,176 --> 00:13:24,586
these 6000 rows are tp

199
00:13:24,618 --> 00:13:28,522
usage predicted by five forecasters

200
00:13:28,586 --> 00:13:29,840
on a test set.

201
00:13:31,650 --> 00:13:35,086
Amongst the forecasters we can find methods

202
00:13:35,198 --> 00:13:39,314
like TFT or nbeats and here

203
00:13:39,352 --> 00:13:42,834
we have plotted real value here and predictions for

204
00:13:42,872 --> 00:13:49,030
each of the forecasting methods. We can zoom and

205
00:13:49,100 --> 00:13:53,334
as we can see, some of the forecasters actually look

206
00:13:53,532 --> 00:13:56,946
quite good. The predictions look accurate,

207
00:13:57,058 --> 00:14:00,060
for example prediction two or predictions six,

208
00:14:01,230 --> 00:14:04,810
but we still try to improve them with ensembling.

209
00:14:06,430 --> 00:14:11,150
We start with fast, easy to implement standard methods.

210
00:14:11,890 --> 00:14:15,520
So of course we have here methods like nave mean

211
00:14:17,170 --> 00:14:21,082
over the best subset. We are taking all possible subsets

212
00:14:21,146 --> 00:14:24,546
and calculate which set of features of

213
00:14:24,648 --> 00:14:28,254
predictions has the best. For example, mean absolute error

214
00:14:28,302 --> 00:14:32,274
on a train set, mean over the best n

215
00:14:32,312 --> 00:14:35,858
methods on Kalas time steps. So it's similar

216
00:14:35,944 --> 00:14:39,606
from the to the previous method, but we are

217
00:14:39,788 --> 00:14:43,942
calculating error only on

218
00:14:44,076 --> 00:14:48,380
the last k steps. You can also find

219
00:14:48,830 --> 00:14:52,570
weights for each forecasters, for example with linear programming.

220
00:14:53,230 --> 00:14:57,146
With linear programming it's easy to impose constraints on

221
00:14:57,168 --> 00:15:01,600
the weights. We want them to be positive and sum to one.

222
00:15:02,850 --> 00:15:07,630
We can also find forecaster weights depending on metric scores.

223
00:15:08,050 --> 00:15:12,320
So the better the results of a method, the more weight we give to

224
00:15:12,870 --> 00:15:16,018
it. And now I will show how

225
00:15:16,104 --> 00:15:19,220
these simple methods work on our data set.

226
00:15:20,230 --> 00:15:23,986
And now I will show how these simple methods work on

227
00:15:24,008 --> 00:15:25,220
our data set.

228
00:15:32,570 --> 00:15:35,974
So we have a simple visualization. Here we

229
00:15:36,012 --> 00:15:40,570
have five predictors, five forecasters and

230
00:15:40,720 --> 00:15:44,582
four of the simple ensembling

231
00:15:44,646 --> 00:15:48,394
methods. The predictions are sent in real time

232
00:15:48,512 --> 00:15:52,366
and assembled predictions are also produced in

233
00:15:52,388 --> 00:15:55,694
real time. In this

234
00:15:55,732 --> 00:15:59,550
table we can see how good are our

235
00:15:59,620 --> 00:16:03,342
methods. We have metrics,

236
00:16:03,406 --> 00:16:07,266
five metrics here, for example, mean absolute error and the

237
00:16:07,288 --> 00:16:09,860
best scores are highlighted with green.

238
00:16:10,870 --> 00:16:14,594
So we can see that linear programming is

239
00:16:14,632 --> 00:16:18,658
the winner method here, however not

240
00:16:18,744 --> 00:16:23,080
its nave mean over best subset is

241
00:16:24,410 --> 00:16:28,166
often the best on

242
00:16:28,268 --> 00:16:31,946
smapy metrics. And here we

243
00:16:31,968 --> 00:16:35,910
can see what's the gain of the best ensembling

244
00:16:35,990 --> 00:16:39,690
method over the best single method single forecaster.

245
00:16:45,650 --> 00:16:49,694
These light green rectangles means that

246
00:16:49,732 --> 00:16:53,042
this specific method achieves the best

247
00:16:53,096 --> 00:16:56,290
scores on this period.

248
00:16:58,310 --> 00:17:02,414
To sum up, ensembling methods achieve

249
00:17:02,462 --> 00:17:06,354
usually better scores than the single forecasters.

250
00:17:06,482 --> 00:17:10,294
However, there is no it's hard

251
00:17:10,332 --> 00:17:15,798
to say which of the Nsemzik methods is the best because

252
00:17:15,884 --> 00:17:18,700
it vary across the data set.

253
00:17:20,350 --> 00:17:23,914
So after the simple methods, it's time to present something more

254
00:17:23,952 --> 00:17:28,054
advanced. We will be training neural networks

255
00:17:28,102 --> 00:17:31,230
with the given input predictions,

256
00:17:31,890 --> 00:17:36,190
historical real values and historical errors.

257
00:17:37,970 --> 00:17:41,546
Each input will consist of two parts,

258
00:17:41,658 --> 00:17:45,266
past and future. In the

259
00:17:45,288 --> 00:17:48,866
past we have predictions error for each of

260
00:17:48,888 --> 00:17:50,530
the forecasting methods,

261
00:17:53,110 --> 00:17:57,138
and by forecasting error I mean real value minus

262
00:17:57,234 --> 00:18:01,106
predicted values. We have also columns

263
00:18:01,138 --> 00:18:05,078
indicating that we are in the past real

264
00:18:05,164 --> 00:18:09,254
value so the original target

265
00:18:09,302 --> 00:18:14,074
metrics, values and time index in

266
00:18:14,112 --> 00:18:18,650
a future. We have predicted values for each of the forecasters.

267
00:18:19,230 --> 00:18:22,366
Columns are indicating that we are in the future

268
00:18:22,548 --> 00:18:26,526
and we

269
00:18:26,548 --> 00:18:31,566
cannot impute real values here because they

270
00:18:31,588 --> 00:18:35,906
are real values here because they

271
00:18:35,928 --> 00:18:39,234
are not known at the moment. And you

272
00:18:39,272 --> 00:18:43,230
also have time index. The train architectures

273
00:18:43,310 --> 00:18:48,082
are fully connected. Neural networks convolutional

274
00:18:48,146 --> 00:18:52,274
neural networks one dimensional conclusion

275
00:18:52,322 --> 00:18:56,150
neural networks one dimensional plus residual connections

276
00:18:57,550 --> 00:19:01,850
convolutional neural networks one dimensional plus attention

277
00:19:02,190 --> 00:19:05,894
and convolutional neural networks one dimensional

278
00:19:05,942 --> 00:19:09,450
plus phase dual connection plus attention and the best methods

279
00:19:11,630 --> 00:19:14,894
were conclusion neural networks one

280
00:19:14,932 --> 00:19:19,914
dimensional plus size dual connections and convolutional

281
00:19:19,962 --> 00:19:23,310
neural networks one dimensional plus size dwell connections plus

282
00:19:23,380 --> 00:19:26,706
attention tension it's hard

283
00:19:26,728 --> 00:19:30,398
to say which of those two top methods

284
00:19:30,494 --> 00:19:34,338
was better as this attention model did not

285
00:19:34,504 --> 00:19:38,402
have a large impact on the performance

286
00:19:38,546 --> 00:19:42,470
in this case. As I mentioned before,

287
00:19:42,620 --> 00:19:46,310
we have issue with missing values

288
00:19:47,130 --> 00:19:50,826
in our problem. So we decided to train

289
00:19:50,928 --> 00:19:54,938
one more model that would be robots to potential future

290
00:19:55,104 --> 00:19:58,330
data gaps for some predictive models.

291
00:19:58,670 --> 00:20:02,720
For this purpose, we randomly removed some columns during training.

292
00:20:04,210 --> 00:20:08,014
Input for such networks looks very similar

293
00:20:08,132 --> 00:20:09,870
to the previous task,

294
00:20:12,050 --> 00:20:15,970
but we are randomly removing some

295
00:20:16,040 --> 00:20:19,460
of the columns from the input. So for example,

296
00:20:20,630 --> 00:20:24,562
here we removed values from the

297
00:20:24,616 --> 00:20:28,870
prediction zero and replaced it with zeros.

298
00:20:29,610 --> 00:20:33,590
And for each of the forecasters method we have mask

299
00:20:34,250 --> 00:20:38,762
which indicates which

300
00:20:38,896 --> 00:20:42,506
of the forecasters methods are not present at the

301
00:20:42,528 --> 00:20:45,946
moment, and the feature has similar

302
00:20:46,048 --> 00:20:50,070
structure. Instead of standard softmax at

303
00:20:50,080 --> 00:20:53,594
the end of neural networks, we used based

304
00:20:53,642 --> 00:20:57,594
Softmax for this problem. The experiments confirmed

305
00:20:57,642 --> 00:21:01,902
that the network trained in this way is immune to potential data

306
00:21:01,956 --> 00:21:05,730
failures and continues to return good results.

307
00:21:06,790 --> 00:21:10,738
How performs an ensembling method based on deep learning

308
00:21:10,904 --> 00:21:13,170
compare to standard techniques.

309
00:21:19,610 --> 00:21:23,510
So we have one more plot here. The best neural network

310
00:21:24,970 --> 00:21:28,606
it was hard to choose the best method from standard ensembled

311
00:21:28,658 --> 00:21:32,860
techniques because it

312
00:21:33,230 --> 00:21:36,810
changed a lot and it was dependent on the part

313
00:21:36,880 --> 00:21:40,060
of the time series we are currently looking at.

314
00:21:44,690 --> 00:21:48,366
We can see here that none of the

315
00:21:48,388 --> 00:21:52,294
method is the best on all of the metrics

316
00:21:52,442 --> 00:21:56,850
and that it also varies across

317
00:21:56,920 --> 00:21:57,940
data set.

318
00:22:02,950 --> 00:22:06,742
But here in this case where we have this

319
00:22:06,796 --> 00:22:09,670
best neural network ensembling method,

320
00:22:10,090 --> 00:22:12,040
it does not change at all.

321
00:22:13,770 --> 00:22:18,378
Deep learning method is constantly the best here

322
00:22:18,544 --> 00:22:22,010
and the improvement in mean absolute error

323
00:22:24,350 --> 00:22:27,578
is significantly better because here we have

324
00:22:27,664 --> 00:22:32,062
around four and here around

325
00:22:32,196 --> 00:22:34,880
here is usually less than one.

326
00:22:38,210 --> 00:22:42,974
And also the plot here looks significantly

327
00:22:43,022 --> 00:22:47,330
better than the plots of other

328
00:22:47,400 --> 00:22:54,880
methods you.

329
00:22:54,950 --> 00:22:58,244
So to sum up, ensembling can help

330
00:22:58,282 --> 00:23:02,608
a lot. Even simple methods can produce satisfactory results.

331
00:23:02,784 --> 00:23:06,452
Ensembling based on neural network may

332
00:23:06,506 --> 00:23:10,164
give much better results but requires also more data to

333
00:23:10,202 --> 00:23:14,192
start because it requires reasonable

334
00:23:14,256 --> 00:23:17,904
amount of data to be trained on and properly

335
00:23:17,952 --> 00:23:21,724
trained neural networks can be resistant to the deficiency of

336
00:23:21,762 --> 00:23:26,376
some predictions and still give better predictions results

337
00:23:26,408 --> 00:23:28,088
than classical methods.

