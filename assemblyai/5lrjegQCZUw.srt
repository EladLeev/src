1
00:02:14,850 --> 00:02:18,302
What do you do with your metrics? You either

2
00:02:18,356 --> 00:02:21,760
save them and mostly visualize them.

3
00:02:22,450 --> 00:02:25,866
The talk is about a tool called Openmetrics

4
00:02:25,898 --> 00:02:29,242
exporter. I'm Pivot Sharma,

5
00:02:29,306 --> 00:02:32,510
I'm CTO and co founder at lastcent IO.

6
00:02:33,090 --> 00:02:36,262
And is this talk about yet

7
00:02:36,316 --> 00:02:40,294
another exporter? Because openmetrics in

8
00:02:40,332 --> 00:02:44,086
favor of hotel and lot of other formats have probably fallen out of

9
00:02:44,108 --> 00:02:48,006
favor. Let me walk you through the journey of how

10
00:02:48,108 --> 00:02:51,500
and why instrumentation and metrics are important.

11
00:02:52,990 --> 00:02:57,610
All modern cloud components are built on complex layers

12
00:02:58,670 --> 00:03:02,202
before you even get to instrumenting

13
00:03:02,346 --> 00:03:05,806
your application code. The application

14
00:03:05,908 --> 00:03:09,482
code that you deploy is deployed across layers

15
00:03:09,546 --> 00:03:12,894
of infrastructure, layers like it probably is

16
00:03:12,932 --> 00:03:16,674
deployed on a kubernetes. That kubernetes is running on a set

17
00:03:16,712 --> 00:03:20,546
of virtual machines which are supplied by the cloud and

18
00:03:20,648 --> 00:03:23,330
maybe which is connected to a disk as well underneath.

19
00:03:24,070 --> 00:03:27,462
And all of these components that are complex layers within

20
00:03:27,516 --> 00:03:30,946
themselves are speaking with each other. Before a request reaches

21
00:03:30,978 --> 00:03:34,294
your application, it probably passes through a load balancer which

22
00:03:34,332 --> 00:03:38,590
reaches an ingress controller. Then it reaches your pod

23
00:03:38,770 --> 00:03:41,834
over there, there may be another Nginx running and then

24
00:03:41,872 --> 00:03:45,434
it hits the application and each one of these

25
00:03:45,552 --> 00:03:49,030
complex layers and handshakes are breaking

26
00:03:49,110 --> 00:03:52,310
almost all the time. To stay on top of it,

27
00:03:52,400 --> 00:03:55,694
you need instrumentation. One of the industrial ways of

28
00:03:55,732 --> 00:03:58,442
instrumenting and observing is metrics.

29
00:03:58,586 --> 00:04:02,538
And one of the formats that have been the pioneer here is the open metrics

30
00:04:02,554 --> 00:04:06,782
format. But how do you observe such dynamic

31
00:04:06,846 --> 00:04:11,486
components and infrastructures these days? A typical landscape

32
00:04:11,598 --> 00:04:15,300
of observability in the modern world looks like this.

33
00:04:15,670 --> 00:04:19,206
You probably would have a can somewhere. And this is a typical example

34
00:04:19,308 --> 00:04:22,818
of any web application. There would be a CDN.

35
00:04:22,914 --> 00:04:26,194
There are instances, instances which are running some stateful

36
00:04:26,242 --> 00:04:29,762
workload, there are pods. Almost everybody runs kubernetes

37
00:04:29,826 --> 00:04:33,466
these days. There is some reverse proxy in form of ambassador or a

38
00:04:33,488 --> 00:04:37,254
traffic, maybe there's an NgInx as well as an ingress controller.

39
00:04:37,382 --> 00:04:41,482
And each one of these layers in the name of instrumentation are

40
00:04:41,536 --> 00:04:45,402
either emitting data in form of metrics or in form of logs.

41
00:04:45,546 --> 00:04:48,554
If you have metrics, well and good. If you have logs, you got to transform

42
00:04:48,602 --> 00:04:52,794
them into openmetrics. The way to export these metric,

43
00:04:52,922 --> 00:04:56,322
if the provider is already servicing these

44
00:04:56,376 --> 00:05:00,002
components like cdNs, like load balancers, you might

45
00:05:00,056 --> 00:05:03,086
get the data natively into the metric storage

46
00:05:03,118 --> 00:05:06,706
layer which is provided by the cloud. But if it is not, and it's a

47
00:05:06,728 --> 00:05:09,558
managed infrastructure that you are managing by yourself,

48
00:05:09,724 --> 00:05:12,834
you would probably be running some kind of exporter somewhere,

49
00:05:12,882 --> 00:05:17,074
a metric exporter, which all of it is unifying

50
00:05:17,122 --> 00:05:21,014
into a metric sync now, these metric syncs are also diverse.

51
00:05:21,142 --> 00:05:25,414
The cloud provider's metric sync is different from your metric sync.

52
00:05:25,542 --> 00:05:29,670
And consistently you're trying to build a correlation

53
00:05:29,830 --> 00:05:34,122
because they are effectively talking about the same component

54
00:05:34,266 --> 00:05:38,170
or the layer underneath. All of this consolidation

55
00:05:38,250 --> 00:05:41,994
and aggregation is mostly done at runtime using a visual

56
00:05:42,042 --> 00:05:45,806
tool, which very famously is grafana, almost as

57
00:05:45,828 --> 00:05:49,326
a de facto standard. And there's a human trying to interpret

58
00:05:49,358 --> 00:05:52,958
a dashboard, a dashboard which is speaking to the diverse

59
00:05:52,974 --> 00:05:56,260
layers, which are effectively talking about the same thing here.

60
00:05:56,650 --> 00:06:00,710
Now, what do we observe here? And these are certain

61
00:06:00,780 --> 00:06:04,054
challenges that I want to raise, among many other challenges of

62
00:06:04,092 --> 00:06:06,070
these metric syncs and data lakes.

63
00:06:07,450 --> 00:06:11,174
The first metric here is just an example. It's a namesake Go GC

64
00:06:11,222 --> 00:06:14,918
duration seconds. How often would you have seen this metric?

65
00:06:15,014 --> 00:06:18,518
Almost all the time. Every metric page that is emitted

66
00:06:18,534 --> 00:06:21,894
by Prometheus. You see this metric somewhere. But have you ever

67
00:06:21,952 --> 00:06:25,150
really alerted on this metric? If not,

68
00:06:25,220 --> 00:06:29,434
what is it doing there? If your scrape interval is 15 seconds,

69
00:06:29,562 --> 00:06:32,814
which is the default of Prometheus, you're emitting this

70
00:06:32,852 --> 00:06:36,674
metric four times. And every such emitter is

71
00:06:36,872 --> 00:06:40,354
all other Golang codes are bringing in such

72
00:06:40,392 --> 00:06:44,210
metrics everywhere. You would be surprised to know is

73
00:06:44,280 --> 00:06:47,746
something that we observed across our customers is that 40% of your

74
00:06:47,768 --> 00:06:51,334
metrics might not even be accessed ever. But they

75
00:06:51,372 --> 00:06:54,950
are all sitting there in that metric storage layers

76
00:06:55,610 --> 00:06:59,586
and you would have heard it@your.org as well. I have heard it@many.org

77
00:06:59,698 --> 00:07:03,286
where the very common component is about, hey, our dashboards

78
00:07:03,318 --> 00:07:07,354
are getting slower. Our dashboards are getting slower because there's too much

79
00:07:07,392 --> 00:07:10,666
data in it or there are too many requests in there. Now what

80
00:07:10,688 --> 00:07:14,154
is that data in there? Is data that you don't even know? Is that

81
00:07:14,192 --> 00:07:17,486
data that you don't even use because it's coming free from

82
00:07:17,508 --> 00:07:21,486
the exporters that we actually use? If we take a step back and

83
00:07:21,508 --> 00:07:24,666
say, how did we even get here? How do you observe,

84
00:07:24,778 --> 00:07:28,034
for example, an eks cluster? There are some

85
00:07:28,072 --> 00:07:32,018
metrics about this eks cluster that Cloudwatch is providing you.

86
00:07:32,184 --> 00:07:35,794
But there are some metrics which you are emitting by yourself in form

87
00:07:35,832 --> 00:07:39,202
of kubestate, openmetrics or a c advisor. Very likely

88
00:07:39,266 --> 00:07:43,426
that there is a Prometheus sitting in their free, which is also being observed

89
00:07:43,538 --> 00:07:47,330
via somebody's grafana. She's emitting metrics

90
00:07:47,490 --> 00:07:50,762
in two formats about the exact same thing.

91
00:07:50,896 --> 00:07:54,170
Isn't that a problem? At the same time,

92
00:07:54,240 --> 00:07:57,994
do you even know if the exporter is lagging somewhere or

93
00:07:58,032 --> 00:08:01,514
has it crashed? Or because it is creating data from

94
00:08:01,552 --> 00:08:04,806
Cloudwatch in a pursuit to unify the metric.

95
00:08:04,918 --> 00:08:08,494
It's just raking up the Cloudwatch bill every single run of its

96
00:08:08,532 --> 00:08:11,822
own, every single time where it pulls data,

97
00:08:11,956 --> 00:08:16,446
the bill just keeps increasing. And it is probably dumping

98
00:08:16,638 --> 00:08:20,290
a plethora of metrics that we don't even have any control over.

99
00:08:20,440 --> 00:08:23,060
If I was to ask a very standard question here,

100
00:08:24,070 --> 00:08:28,298
if, for example, I want to emit and observe an SQL database,

101
00:08:28,494 --> 00:08:31,634
what metrics are even being emitted by the Prometheus

102
00:08:31,682 --> 00:08:35,446
exporter, you're going to have a tough time even figuring out

103
00:08:35,468 --> 00:08:39,174
what metrics are in there. So if I was to summarize these key

104
00:08:39,212 --> 00:08:42,562
challenges, sources are consistently changing.

105
00:08:42,706 --> 00:08:45,942
We're in an infrastructure world where everything is ephemeral.

106
00:08:46,086 --> 00:08:49,834
Servers go in pods, die in by the hour. Infrastructure is

107
00:08:49,872 --> 00:08:53,790
changed. The functions, the lambdas, which we literally don't even have any control

108
00:08:53,860 --> 00:08:57,342
over. Fundamentally, there is no correlation between

109
00:08:57,396 --> 00:09:00,714
the entities that we speak. At the same time, there's a metric

110
00:09:00,762 --> 00:09:04,306
explosion happening because there's a lot of unused information.

111
00:09:04,408 --> 00:09:08,094
And there's literally no way of prioritizing urgent versus

112
00:09:08,142 --> 00:09:12,686
important versus unused metrics. And every single new source

113
00:09:12,878 --> 00:09:16,806
of a type of a component that you have to observe, it's a new

114
00:09:16,828 --> 00:09:19,320
binary, a new exporter that needs to run.

115
00:09:20,170 --> 00:09:24,550
If only we could actually make this declarative infrastructure.

116
00:09:26,010 --> 00:09:29,654
The cloud component creation world went through a transformation

117
00:09:29,702 --> 00:09:32,826
like this in form of tools like terraform. They made

118
00:09:32,928 --> 00:09:36,074
infrastructure as code. We actually,

119
00:09:36,192 --> 00:09:38,890
openmetrics exporter proposes to make.

120
00:09:38,960 --> 00:09:42,234
Observability was a code. It's a three step process.

121
00:09:42,432 --> 00:09:46,298
Step one, you declare your observability.

122
00:09:46,474 --> 00:09:50,110
There are certain metrics that matter, and there are other metrics that do not

123
00:09:50,180 --> 00:09:53,726
at all points of time. And this is very subjective. This is domain specific as

124
00:09:53,748 --> 00:09:57,694
well. The same Nginx could mean different in different domains. You declare

125
00:09:57,742 --> 00:10:01,486
these, you declare what type of components

126
00:10:01,518 --> 00:10:05,282
do you want to observe. You run a plan locally on your computer.

127
00:10:05,416 --> 00:10:08,694
You validate if the data is coming in. If everything looks

128
00:10:08,732 --> 00:10:12,274
well, you dispatch it. Once you dispatch

129
00:10:12,322 --> 00:10:15,526
it, openmetrics exporter will export all

130
00:10:15,548 --> 00:10:19,580
the metrics on a slash openmetrics page along with the timestamp of it.

131
00:10:20,030 --> 00:10:24,060
Once it is there any standard Prometheus agent,

132
00:10:24,830 --> 00:10:28,058
be it form of Prometheus agent, a Grafana agent, or a

133
00:10:28,064 --> 00:10:31,974
VM agent, can carry these highly precise selected

134
00:10:32,022 --> 00:10:35,386
metrics into the metric lake, where what is sitting

135
00:10:35,418 --> 00:10:38,718
in your metric lake is now something that you duly alert on,

136
00:10:38,884 --> 00:10:42,414
is important to you, and more importantly is what you have

137
00:10:42,452 --> 00:10:46,046
selected. An anatomy of an openmetrics

138
00:10:46,078 --> 00:10:49,534
exporter looks like this. You declare a scraper,

139
00:10:49,662 --> 00:10:53,410
a scraper is a first class entity which can connect to a cloud

140
00:10:53,480 --> 00:10:57,342
source and has certain attributes to it. These attributes

141
00:10:57,486 --> 00:11:01,030
are what form the metrics. There are different kinds of

142
00:11:01,100 --> 00:11:04,246
gauges, vectors and histograms, which are

143
00:11:04,268 --> 00:11:07,858
pretty much vector is kind of a summary as well. There's a dynamic label

144
00:11:07,874 --> 00:11:11,658
to it. Every definition of this metric type

145
00:11:11,744 --> 00:11:14,822
carries a query inside it which is specific to a source.

146
00:11:14,966 --> 00:11:18,620
Once it speaks to a source, it can pull that data,

147
00:11:19,070 --> 00:11:22,640
and more importantly, it can use this as an engine.

148
00:11:23,090 --> 00:11:26,846
If you pay attention, there's a resources each load balancer line.

149
00:11:26,948 --> 00:11:31,258
What it effectively does is that once that I have written this file,

150
00:11:31,434 --> 00:11:35,178
it is enough for it to run across any selection

151
00:11:35,194 --> 00:11:38,850
of load balancers that I may want to run. So I write this code

152
00:11:38,920 --> 00:11:42,450
once, but I run it multiple times for each load balancer.

153
00:11:43,030 --> 00:11:45,886
We've spoken multiple times about correlations,

154
00:11:46,078 --> 00:11:49,334
but what exactly do we mean by correlation? Let's take

155
00:11:49,372 --> 00:11:52,646
the same example here. There's load balancer data that

156
00:11:52,668 --> 00:11:55,986
I get from Cloudwatch, which I observed

157
00:11:56,018 --> 00:11:59,770
to say namesake is at throughput right now, but at the same time

158
00:11:59,840 --> 00:12:02,250
the latency is being tracked elsewhere.

159
00:12:03,150 --> 00:12:06,730
Assume that this latency was being emitted into Prometheus.

160
00:12:07,470 --> 00:12:11,334
And once it is in Prometheus, I effectively want to emit

161
00:12:11,382 --> 00:12:14,846
them with the same label sets, but a different metric name. How do I do

162
00:12:14,868 --> 00:12:18,346
that? As a part of the same metric file is what openmetrics literally

163
00:12:18,538 --> 00:12:22,666
unlocks. I just add a new stanza called gauge

164
00:12:22,698 --> 00:12:26,078
latency. But this time around, the source will be PromQL.

165
00:12:26,174 --> 00:12:29,682
It's a standard promql query with certain functions written here,

166
00:12:29,736 --> 00:12:33,362
which are manipulations of data being done. And this

167
00:12:33,416 --> 00:12:37,350
entire scraper runs together. So when it runs, it pulls data

168
00:12:37,420 --> 00:12:40,674
from Cloudwatch and from PromQL and emits

169
00:12:40,722 --> 00:12:44,178
it together. What it means is that if it fails,

170
00:12:44,354 --> 00:12:47,714
it fails both of them equally. There is unification,

171
00:12:47,762 --> 00:12:50,666
there's a correlation of data happening here. And when it does,

172
00:12:50,768 --> 00:12:54,042
it emits it with the same label set. So if there's any

173
00:12:54,096 --> 00:12:57,770
change happening, the change is uniformly being applied in both places.

174
00:12:59,710 --> 00:13:03,206
Many a times there are sources. And this is one of the fundamental

175
00:13:03,238 --> 00:13:06,814
powerful features of open networks, is that the data at

176
00:13:06,852 --> 00:13:10,446
source can constantly change. And this

177
00:13:10,468 --> 00:13:13,918
is applicable in, let's say a stateful system where you're

178
00:13:13,934 --> 00:13:17,058
saving your openmetrics like SQL, where the

179
00:13:17,064 --> 00:13:20,802
data might alter because there is

180
00:13:20,856 --> 00:13:24,494
look back and lag, which are the fundamental first class properties

181
00:13:24,542 --> 00:13:27,974
of any scraper. What it means is that you can actually stay

182
00:13:28,012 --> 00:13:32,134
on top of this data changing. Very rarely you would see uses them

183
00:13:32,172 --> 00:13:36,114
together, but they can powerfully

184
00:13:36,162 --> 00:13:40,018
be used in conjunction with each other. What look back means is how

185
00:13:40,044 --> 00:13:43,754
much time should I look back into the

186
00:13:43,792 --> 00:13:46,838
time frame, so that if there's anything that is changing

187
00:13:46,854 --> 00:13:50,042
in the past, can be emitted with the same timestamp. Now,

188
00:13:50,096 --> 00:13:53,146
most native exporters, what they assume is that

189
00:13:53,168 --> 00:13:56,858
the time of scrape is assumed as the time of insertion.

190
00:13:57,034 --> 00:14:01,114
But at openmetrics, we retain that timestamp across all sources.

191
00:14:01,242 --> 00:14:04,698
So what it effectively means is that if any point in the

192
00:14:04,724 --> 00:14:08,482
future a historical timestamp has changed, it can actually get

193
00:14:08,536 --> 00:14:11,460
converted into your metric, sync with the corrected value.

194
00:14:11,910 --> 00:14:15,694
In certain situation, you may not want to process any pipelines

195
00:14:15,822 --> 00:14:18,918
until the value is converted, and in such cases, lag is

196
00:14:18,924 --> 00:14:22,614
your friend. You can apply a lag and the whole

197
00:14:22,652 --> 00:14:26,166
pipeline will not emit fresh data until that point

198
00:14:26,188 --> 00:14:30,230
of time. One of the lessons that we learned

199
00:14:30,390 --> 00:14:33,286
from cloud systems,

200
00:14:33,398 --> 00:14:37,030
and trying to automate a lot of cloud operations

201
00:14:37,110 --> 00:14:40,982
that we do via terraform, the fundamental patience

202
00:14:41,046 --> 00:14:44,666
test that happens is the feedback loop is extremely painful

203
00:14:44,698 --> 00:14:47,786
and small. We want to change that with openmetrics exporter.

204
00:14:47,818 --> 00:14:51,310
So what we did was we added native Grafana support here.

205
00:14:51,460 --> 00:14:55,178
So every open exporter file, every run

206
00:14:55,204 --> 00:14:59,054
of it, is resulting in a new dashboard

207
00:14:59,182 --> 00:15:02,686
which you can debug later. It's the same example here where we're

208
00:15:02,718 --> 00:15:05,758
observing an RDS. If we run here with two parameters,

209
00:15:05,854 --> 00:15:09,518
the first parameter being the endpoint of the grafana, where I'm

210
00:15:09,534 --> 00:15:12,358
running my local grafana, it could be a hosted grafana as well, and along with

211
00:15:12,364 --> 00:15:15,810
it, an API token, it actually emits a dashboard.

212
00:15:15,890 --> 00:15:19,238
Once I visit that dashboard, I can visually see if everything

213
00:15:19,324 --> 00:15:22,634
that I had emitted looks correct or not. If all

214
00:15:22,672 --> 00:15:25,866
the statistics are correct, I may have not accidentally uses an

215
00:15:25,888 --> 00:15:29,414
average in case of max or a sum or a query is not misbehaving.

216
00:15:29,542 --> 00:15:33,434
And once I'm confirmed, I'm happy to dispatch

217
00:15:33,482 --> 00:15:34,910
this into production.

218
00:15:36,530 --> 00:15:40,250
Software engineering's fundamental principle is about reusability.

219
00:15:40,410 --> 00:15:44,334
We want to write code once and reuse it as many times as possible.

220
00:15:44,532 --> 00:15:48,206
We brought the concept of reusability and modules

221
00:15:48,238 --> 00:15:51,614
as first class citizens into openmetrics exporter.

222
00:15:51,742 --> 00:15:55,086
If we look at the same module, the same scraper files

223
00:15:55,118 --> 00:15:58,166
that we have been writing for a while, all we need

224
00:15:58,188 --> 00:16:01,942
to change is one single keyword. Instead of

225
00:16:01,996 --> 00:16:04,770
identifying the scraper uniquely,

226
00:16:04,930 --> 00:16:08,690
we replace the word as module. This very simple

227
00:16:08,780 --> 00:16:12,474
and a small fix here, what it does is it makes the

228
00:16:12,512 --> 00:16:16,134
entire selection of metrics that I've carefully

229
00:16:16,182 --> 00:16:19,834
crafted be available as a blueprint to be run

230
00:16:19,872 --> 00:16:23,614
every time. What it means is that once

231
00:16:23,652 --> 00:16:27,034
I've identified the key metrics that are extremely

232
00:16:27,082 --> 00:16:30,554
important for me. I can save them, I can create a blueprint,

233
00:16:30,682 --> 00:16:33,902
and elsewhere where the next time I have to run

234
00:16:33,956 --> 00:16:37,810
this, I can refer to this as a remote URL.

235
00:16:38,310 --> 00:16:41,554
Every single scraper that I have to write once can

236
00:16:41,592 --> 00:16:45,218
actually be Prometheus to be used across the community

237
00:16:45,384 --> 00:16:49,458
or my subsequent deployments. And before that, binding blueprint

238
00:16:49,554 --> 00:16:53,138
or a guardrail, which ensures that everything is observed

239
00:16:53,154 --> 00:16:56,614
homogeneously. On the left is an example where instead

240
00:16:56,652 --> 00:17:00,280
of scraper I call it extends. And that's it. Boom. Job done.

241
00:17:01,210 --> 00:17:05,146
What I'd also do is we create a catalog of all

242
00:17:05,168 --> 00:17:09,382
the important openmetrics that we seem to be relevant

243
00:17:09,526 --> 00:17:12,966
across. All of these components that are available across clouds.

244
00:17:13,078 --> 00:17:16,058
This is a registry that you can actually visit.

245
00:17:16,154 --> 00:17:19,838
We have a default catalog that is consistently being updated, but you

246
00:17:19,844 --> 00:17:23,070
can always contribute as well. It's open source.

247
00:17:24,130 --> 00:17:28,058
All of this is only meaningful

248
00:17:28,234 --> 00:17:31,858
if they could speak with multiple sources of data

249
00:17:32,024 --> 00:17:34,994
and sources, just like terraform has.

250
00:17:35,032 --> 00:17:38,978
Providers is a first class entity here. Openmetrics exporter

251
00:17:38,994 --> 00:17:42,994
can help you pull data from Prometheus, from Google, Stackdriver, from Amazon,

252
00:17:43,042 --> 00:17:46,614
Cloudwatch, Redshift, or any other SQL power database. It can also

253
00:17:46,652 --> 00:17:51,082
speak with other Openmetrics backend along

254
00:17:51,136 --> 00:17:55,206
the lines we actually saw these syntaxes, they may have looked familiar,

255
00:17:55,318 --> 00:17:58,538
the syntax is written in etc. A lot of

256
00:17:58,544 --> 00:18:02,134
people ask me, why is it not Yaml? Well, here's a joke.

257
00:18:02,182 --> 00:18:06,282
Yaml provides you job security, etc. Lets you concentrate.

258
00:18:06,426 --> 00:18:09,566
But again, why not Yaml? If we look

259
00:18:09,588 --> 00:18:12,926
at a smallish example here, what I'm trying

260
00:18:12,948 --> 00:18:16,654
to do is I'm trying to run over a resource all cluster I'm joining,

261
00:18:16,702 --> 00:18:20,418
splitting, and then doing a format. Any such

262
00:18:20,504 --> 00:18:24,942
manipulation of data is almost impossible

263
00:18:25,006 --> 00:18:28,146
to be done as a first class language construct.

264
00:18:28,258 --> 00:18:31,782
And you know, every DSL as it evolves wants

265
00:18:31,836 --> 00:18:35,234
to add some programming logic to it, some iterators,

266
00:18:35,282 --> 00:18:38,930
some conditionals, some ternary operators, some boolean expressions.

267
00:18:39,090 --> 00:18:42,382
Now, because all of these are available as first class in HCL,

268
00:18:42,466 --> 00:18:45,734
what it means is you can add some logic,

269
00:18:45,782 --> 00:18:48,822
not obnoxiously complex logic, but some logic,

270
00:18:48,966 --> 00:18:52,154
into the configuration templates itself,

271
00:18:52,352 --> 00:18:56,046
which means that that is less code, which is actually to be

272
00:18:56,068 --> 00:18:59,754
written. Now imagine if you had an existing

273
00:18:59,802 --> 00:19:02,974
exporter that you were using, which was off the shelf, and all

274
00:19:03,012 --> 00:19:07,106
you had to do was add a certain formatter of a string, or a

275
00:19:07,128 --> 00:19:11,278
certain split, or a certain join, or a certain regular expression.

276
00:19:11,374 --> 00:19:14,670
You would have to go edit the code and maintain a fork.

277
00:19:14,830 --> 00:19:19,350
But if the language spec becomes powerful where you can freely run these expressions,

278
00:19:20,570 --> 00:19:24,326
it only provides for more reusability, and that

279
00:19:24,348 --> 00:19:28,002
is the fundamental principle behind etc. It's human readable.

280
00:19:28,066 --> 00:19:31,674
Humans can interact with it better. It effectively means

281
00:19:31,712 --> 00:19:35,510
that it is less learning curve. It will fit all existing editors

282
00:19:35,590 --> 00:19:38,694
as you have seen with terraform console or nomad,

283
00:19:38,822 --> 00:19:42,486
all the editors will continue to work. Your existing Gitops pipeline

284
00:19:42,518 --> 00:19:46,686
will continue to work. Now, if we take a recap here

285
00:19:46,788 --> 00:19:50,874
and we try to understand how is this different from the existing Prometheus

286
00:19:50,922 --> 00:19:54,526
exporters that we would have actually used in case we

287
00:19:54,548 --> 00:19:57,954
were trying to build this as well. I want to go scenario by

288
00:19:57,992 --> 00:20:01,794
scenario. If there was a new source of data

289
00:20:01,992 --> 00:20:05,646
in an existing exporter approach, we would have to dispatch

290
00:20:05,678 --> 00:20:09,586
a new binary. Every single new binary that gets dispatched

291
00:20:09,618 --> 00:20:13,730
onto production is another point of failure, which needs to be tracked further.

292
00:20:13,890 --> 00:20:18,130
Now, who keeps a track of the binary? Probably another binary,

293
00:20:18,210 --> 00:20:21,886
and that's kind of a recursion that keeps going on. With openmetrics exporter,

294
00:20:21,938 --> 00:20:24,794
you just have to write a scraper file. Now,

295
00:20:24,832 --> 00:20:28,106
open matrix exporter has been written for massive concurrency. It's backed by

296
00:20:28,128 --> 00:20:31,210
Golang. We are on a single machine which is probably

297
00:20:31,280 --> 00:20:35,210
six cores and some 6gb of ram.

298
00:20:35,290 --> 00:20:38,862
We've been able to scale it to 500 scrapers and up to three to 4

299
00:20:38,916 --> 00:20:40,750
million data points being emitted.

300
00:20:41,970 --> 00:20:45,482
We also touch the part of what metrics do you have control over

301
00:20:45,556 --> 00:20:49,374
in existing exporters, let alone controlling at times finding

302
00:20:49,422 --> 00:20:53,310
it becomes really hard with openmetrics exporter because it's observability

303
00:20:53,390 --> 00:20:57,286
as code. We get to pick and choose the metrics that are important

304
00:20:57,388 --> 00:21:00,066
to us and what should reside in our metrics.

305
00:21:00,098 --> 00:21:03,382
Lake building correlation is

306
00:21:03,436 --> 00:21:06,854
a job for post processing. Post processing is that

307
00:21:06,892 --> 00:21:10,146
you would carry this data into another data lake,

308
00:21:10,258 --> 00:21:13,530
and thereafter we will write complex pieces of code using

309
00:21:13,600 --> 00:21:17,350
some sort of another language which can help us extract

310
00:21:17,510 --> 00:21:21,034
some knowledge out of it, so that we can see the data together from two

311
00:21:21,072 --> 00:21:24,254
things. Or it would just be a visual post processing in

312
00:21:24,292 --> 00:21:27,946
form of a grafana dashboard. With openmetrics exporter,

313
00:21:27,978 --> 00:21:31,774
it becomes a native support because you can club these metrics together and

314
00:21:31,812 --> 00:21:35,314
emit them as unified, homogeneous under the same

315
00:21:35,352 --> 00:21:38,622
label set, logic manipulation is almost impossible.

316
00:21:38,686 --> 00:21:41,934
We are at the mercy of editing

317
00:21:41,982 --> 00:21:45,986
the code which is a part of the exporter itself, but because it is

318
00:21:46,088 --> 00:21:49,442
a first class function via etc, the native

319
00:21:49,506 --> 00:21:53,586
expressions allow very easy manipulation inside openmetrics exporter.

320
00:21:53,698 --> 00:21:57,622
Now, how does this all fit together? Let's take an example. The step

321
00:21:57,676 --> 00:22:00,886
one is the sub and ship cycle. You would download the

322
00:22:00,908 --> 00:22:04,886
binary from an artifactory as a practitioner

323
00:22:04,918 --> 00:22:08,566
you would write certain etc files or you would reuse existing modules

324
00:22:08,598 --> 00:22:12,518
that are available. You run them, you plan them, you run a grafana

325
00:22:12,534 --> 00:22:16,570
on them. Once everything is sorted looks good you commit it to a repository.

326
00:22:16,730 --> 00:22:20,362
From the repository you would probably create an artifactory of the etc

327
00:22:20,426 --> 00:22:24,222
files on the deploy side which is the step two the same

328
00:22:24,276 --> 00:22:28,526
open metrics binary which is a flavor specific to the deployment server

329
00:22:28,718 --> 00:22:32,206
tries to download these etc files and then emits

330
00:22:32,238 --> 00:22:36,126
the metrics. These metrics are exposed on a slash metric page which any

331
00:22:36,168 --> 00:22:39,858
Prometheus running in agent mode or a VM agent or a grafana

332
00:22:39,874 --> 00:22:42,950
agent can then ship it to the Openmetrics receiver.

333
00:22:44,650 --> 00:22:47,974
Openmetrics exporter was actually born at last nine.

334
00:22:48,092 --> 00:22:51,474
That's a company where I work at and we allow people to ship

335
00:22:51,522 --> 00:22:55,398
reliable software and the very first step of

336
00:22:55,564 --> 00:22:59,442
building this reliability stack is building a consolidated

337
00:22:59,506 --> 00:23:03,306
metric layer. You can visit OpenMetrics exporter

338
00:23:03,338 --> 00:23:06,890
at our last nine page and well happy for feedback.

