1
00:00:00,250 --> 00:00:01,630
Are you an SRE,

2
00:00:03,570 --> 00:00:04,830
a developer,

3
00:00:06,610 --> 00:00:10,474
a quality engineer who wants to tackle the challenge of improving

4
00:00:10,522 --> 00:00:14,254
reliability in your DevOps? You can enable your DevOps for

5
00:00:14,292 --> 00:00:17,854
reliability with chaos native. Create your free

6
00:00:17,892 --> 00:01:17,142
account at Chaos native Litmus Cloud hello

7
00:01:17,196 --> 00:01:20,614
everyone. In today's session I am going to talk about

8
00:01:20,652 --> 00:01:24,534
a streaming and near real time analytics this

9
00:01:24,572 --> 00:01:27,720
near real time analytics, not a real time one.

10
00:01:29,710 --> 00:01:32,666
So if you see in today's world,

11
00:01:32,848 --> 00:01:36,220
most of the companies are looking for a

12
00:01:36,590 --> 00:01:40,522
near real time analytics or real time analysing, which are of course

13
00:01:40,576 --> 00:01:44,462
challenge to achieve most of the time. So gone

14
00:01:44,516 --> 00:01:47,502
are the days when you would get the data,

15
00:01:47,556 --> 00:01:51,374
next day you analyze for the next one week and try to figure out what

16
00:01:51,412 --> 00:01:54,866
we need to do, how to react to a situation like a

17
00:01:54,888 --> 00:01:58,034
fraud or anything like that. So things have

18
00:01:58,072 --> 00:02:01,554
changed. To create value, companies must

19
00:02:01,592 --> 00:02:05,814
drive real time insights from a variety of data sources that

20
00:02:05,852 --> 00:02:10,038
are producing high velocity data and

21
00:02:10,124 --> 00:02:13,942
of course in huge volumes. So having

22
00:02:13,996 --> 00:02:18,134
analytics solution in place would enable faster

23
00:02:18,182 --> 00:02:21,500
reaction in real time to events which are affecting business.

24
00:02:23,310 --> 00:02:27,174
So the need for analyzing heterogeneous data from multiple

25
00:02:27,222 --> 00:02:31,118
sources, whether it's internal, external, doesn't matter, are more

26
00:02:31,204 --> 00:02:34,842
than ever, thereby making the analytics

27
00:02:34,906 --> 00:02:39,002
landscapes ever evolving with numerous technologies

28
00:02:39,066 --> 00:02:43,250
and tools and making the platform more and more complex.

29
00:02:44,710 --> 00:02:48,194
So building a futuristic solution is not only

30
00:02:48,232 --> 00:02:52,610
time consuming, but costly because it involves

31
00:02:53,510 --> 00:02:56,934
quite a lot of things like selection of

32
00:02:56,972 --> 00:03:00,578
right technologies, acquiring the right talent pool,

33
00:03:00,754 --> 00:03:04,374
ongoing platform management and operations and

34
00:03:04,412 --> 00:03:08,234
monitoring. So what

35
00:03:08,272 --> 00:03:12,380
I'm going to talk is how to make this platform build

36
00:03:12,990 --> 00:03:17,370
bit easier and less expensive.

37
00:03:19,630 --> 00:03:23,306
Keeping in mind that most of the time building analytics solution is

38
00:03:23,328 --> 00:03:26,682
an afterthought or other. Okay, you first build the features,

39
00:03:26,826 --> 00:03:30,302
core features, and then we'll talk about analytics. So when

40
00:03:30,356 --> 00:03:33,380
that is the time, you probably have less budget left.

41
00:03:34,550 --> 00:03:38,146
So in this session we'll discuss and demo how you

42
00:03:38,168 --> 00:03:41,602
can leverage AWS producing and services to create

43
00:03:41,656 --> 00:03:45,010
a near real time analysing solution with

44
00:03:45,080 --> 00:03:47,570
minimum to no coding.

45
00:03:48,070 --> 00:03:51,922
Note, minimum to no coding for

46
00:03:51,976 --> 00:03:55,254
an e commerce website. Of course it's a demo website. I've just

47
00:03:55,292 --> 00:03:58,726
built a one pager website just to showcase how the data flows from one end

48
00:03:58,748 --> 00:04:02,282
to the other end at the same time, and how

49
00:04:02,336 --> 00:04:06,746
you can integrate with the preexisting data sources if

50
00:04:06,768 --> 00:04:09,786
you need to. And most of the time you probably need to because you would

51
00:04:09,808 --> 00:04:13,162
probably need to integrate with the other back end systems

52
00:04:13,226 --> 00:04:17,354
and make a joint of analysing

53
00:04:17,402 --> 00:04:20,846
and reporting. So of

54
00:04:20,868 --> 00:04:24,586
course the solution need to have set of advantages which is no

55
00:04:24,628 --> 00:04:28,066
different than any other one. In this case, it is easy

56
00:04:28,088 --> 00:04:31,614
to build AWS, I said there's no coding or a very minimum

57
00:04:31,662 --> 00:04:36,166
coding depending on how exhaustive your feature set

58
00:04:36,348 --> 00:04:39,766
you want to build. Then it is

59
00:04:39,788 --> 00:04:43,778
elastic and fully managed. So it is auto

60
00:04:43,874 --> 00:04:47,094
scalable horizontally and vertically and

61
00:04:47,132 --> 00:04:52,234
fully managed by AWS. It is pretty much serverless solution and

62
00:04:52,272 --> 00:04:56,266
then it is as always, any other AWS product and services

63
00:04:56,368 --> 00:04:59,050
is highly available and durable.

64
00:05:00,270 --> 00:05:04,170
Seamless integration with other AWS services like Lambda

65
00:05:04,510 --> 00:05:10,110
ECS, Fargate or EKS RDS

66
00:05:10,450 --> 00:05:14,210
S three which is again a core aspect of the whole solution,

67
00:05:14,550 --> 00:05:18,162
which is the data lake. And last but

68
00:05:18,216 --> 00:05:22,242
not the least is the cost.

69
00:05:22,376 --> 00:05:25,182
It is pay as you go.

70
00:05:25,336 --> 00:05:28,680
Like if you don't use it, you don't pay it.

71
00:05:31,290 --> 00:05:34,738
So let's quickly go over the agenda.

72
00:05:34,914 --> 00:05:38,694
So over the next 1015 minutes, I'll quickly go

73
00:05:38,732 --> 00:05:42,614
over why real time data streaming

74
00:05:42,662 --> 00:05:46,330
and analytics principle of data streaming

75
00:05:46,750 --> 00:05:51,170
and near real time streaming on AWS.

76
00:05:51,350 --> 00:05:56,014
What are the options we have in hand? And at the end I'll go

77
00:05:56,052 --> 00:06:00,186
over one use case and the demo covering

78
00:06:00,218 --> 00:06:01,920
that use case end to end.

79
00:06:03,110 --> 00:06:09,890
So let's quickly turn our attention to the why

80
00:06:09,960 --> 00:06:15,082
real time analytics, as I briefly covered

81
00:06:15,246 --> 00:06:20,120
earlier, is companies

82
00:06:20,890 --> 00:06:24,230
must drive the real time insights from a variety of data sources.

83
00:06:25,050 --> 00:06:28,442
And Gartner in 2019 emphasized that,

84
00:06:28,576 --> 00:06:32,314
saying data integration requirement, which these

85
00:06:32,352 --> 00:06:35,942
days demands more of a real time streaming,

86
00:06:36,006 --> 00:06:38,758
replication and virtualized capabilities.

87
00:06:38,934 --> 00:06:42,526
Gone are the days when you do

88
00:06:42,628 --> 00:06:46,318
offline processing in days or weeks or months,

89
00:06:46,484 --> 00:06:51,386
right? So I think that pretty much sets

90
00:06:51,418 --> 00:06:54,866
the scene. Now, before I go to the details, I just

91
00:06:54,888 --> 00:06:58,946
wanted to take you through a quick case study about this.

92
00:06:59,128 --> 00:07:03,026
Epic Games Fortnite. So real time

93
00:07:03,048 --> 00:07:07,278
data streaming analysing guarantees in this game that gamers

94
00:07:07,294 --> 00:07:10,710
are engaged, resulting in the most successful game currently

95
00:07:10,780 --> 00:07:14,134
in the market. So for those who are not really that

96
00:07:14,172 --> 00:07:17,834
familiar with, what is this all about? Fortnite is set in a world

97
00:07:17,872 --> 00:07:21,078
where players can coordinate among,

98
00:07:21,254 --> 00:07:24,762
cooperated rather on various missions, on fight

99
00:07:24,816 --> 00:07:28,698
back against a mysterious storm, or attempt to be the last person

100
00:07:28,784 --> 00:07:32,954
standing in the game's battle royal mode. It has

101
00:07:32,992 --> 00:07:37,402
become a phenomenon, attracting more than 125,000,000 players

102
00:07:37,466 --> 00:07:41,326
in less than a year. So it is quite popular in that sense.

103
00:07:41,428 --> 00:07:45,218
So what is challenge here? So it is a free to play game

104
00:07:45,304 --> 00:07:48,622
with revenue coming entirely from in game microservice

105
00:07:48,686 --> 00:07:52,846
transactions, meaning its revenue depends on continuously

106
00:07:52,878 --> 00:07:56,790
capturing the attention of the gamers through the new content and

107
00:07:56,860 --> 00:08:00,470
continuous innovation. So to operate this way,

108
00:08:00,540 --> 00:08:04,214
Epic Games needs an up to date or up to the

109
00:08:04,252 --> 00:08:07,454
minute understanding on the gamer satisfaction.

110
00:08:07,602 --> 00:08:11,082
Helping guarantee the experience is one that

111
00:08:11,136 --> 00:08:14,918
keeps them engaged so that's

112
00:08:14,934 --> 00:08:18,170
a challenge, right? Because you need to understand every gamers,

113
00:08:19,310 --> 00:08:22,966
how are they reacting, how make

114
00:08:23,008 --> 00:08:26,606
sure they're happy all the time. Their experience is seamless. At the same

115
00:08:26,628 --> 00:08:30,638
time, collecting data, sort of the solution solution was so

116
00:08:30,724 --> 00:08:34,158
Epic collects billions of records on a daily basis,

117
00:08:34,334 --> 00:08:38,862
tracking virtually everything happening in the game, how players interact,

118
00:08:39,006 --> 00:08:42,926
how often they use certain weapons, and even the strategies

119
00:08:42,958 --> 00:08:46,606
they use to navigate the game universe. More than

120
00:08:46,648 --> 00:08:49,986
14 petabytes of data are stored in, in the data lake,

121
00:08:50,098 --> 00:08:53,634
powered by Amazon S three. So Amazon S three plays a significant

122
00:08:53,682 --> 00:08:57,714
role here, growing two petabytes per month.

123
00:08:57,852 --> 00:08:59,820
It's a massive amount of information.

124
00:09:02,590 --> 00:09:06,138
So as you can see, data loses value over time.

125
00:09:06,224 --> 00:09:09,546
So it is published by Forrester. And if you

126
00:09:09,568 --> 00:09:11,100
see from left to the right,

127
00:09:12,110 --> 00:09:15,486
the time critical decisions are

128
00:09:15,508 --> 00:09:19,038
made within minutes. And as you move towards the right,

129
00:09:19,124 --> 00:09:22,754
it becomes more of a historical how it should be used for

130
00:09:22,792 --> 00:09:27,486
batch processing for business intelligence report or machine

131
00:09:27,518 --> 00:09:31,122
learning training data. So the value

132
00:09:31,176 --> 00:09:34,594
of data diminishes over time. To get most

133
00:09:34,792 --> 00:09:38,470
value from the data, it must be processed at the velocity

134
00:09:38,970 --> 00:09:43,042
in which it is created at the source. So organization,

135
00:09:43,106 --> 00:09:46,982
in pursuit of better customer experience will inevitably need to

136
00:09:47,036 --> 00:09:49,286
start driving towards more reactive,

137
00:09:49,398 --> 00:09:52,422
intelligent and real time experiences.

138
00:09:52,566 --> 00:09:56,038
They just can't wait for data to be batch processed.

139
00:09:56,134 --> 00:09:59,846
And this making decisions and taking actions too late.

140
00:10:00,038 --> 00:10:03,086
So reactivity will differentiate your business.

141
00:10:03,188 --> 00:10:07,246
To achieve a better customer experience, organs need to work with the

142
00:10:07,268 --> 00:10:10,880
freshest data possible. So I think that's pretty much,

143
00:10:12,390 --> 00:10:14,660
pretty clear here. Now,

144
00:10:15,910 --> 00:10:19,442
if I go further down and

145
00:10:19,496 --> 00:10:22,980
analyze the different use cases of the data,

146
00:10:23,830 --> 00:10:28,486
and what kind of use cases would

147
00:10:28,508 --> 00:10:32,002
you have in organization, in terms of timeline,

148
00:10:32,066 --> 00:10:36,354
of using the different data? As you can see here, the messaging between microservice

149
00:10:36,402 --> 00:10:41,382
is a classic example where you need a millisecond of delay.

150
00:10:41,446 --> 00:10:45,386
You can't afford to have minutes here. So response analytics, it's a way

151
00:10:45,408 --> 00:10:48,826
of an application and mobile application notification, they need to

152
00:10:48,848 --> 00:10:52,346
happen within milliseconds. When things are happening at the back end of the front end,

153
00:10:52,368 --> 00:10:56,378
the micro interactions are happening, then long ingestion

154
00:10:56,554 --> 00:11:00,202
that could you consider IoT, device maintenance or CDC.

155
00:11:00,266 --> 00:11:03,854
When you capture data from source to the destination

156
00:11:03,902 --> 00:11:07,634
database, those scenarios you can think of having in

157
00:11:07,672 --> 00:11:12,286
seconds delay, whereas in a typical ETL,

158
00:11:12,478 --> 00:11:16,102
in a data lake or data scenario, you can have minutes or hours

159
00:11:16,156 --> 00:11:19,570
or days of delays in terms of analysing.

160
00:11:19,730 --> 00:11:23,400
So again, this clearly articulates the need

161
00:11:24,250 --> 00:11:27,590
or the importance of data over time

162
00:11:27,660 --> 00:11:31,530
as we move forward. So what is the trend?

163
00:11:32,190 --> 00:11:36,106
So, one of the great things about data stream is that many customers find they

164
00:11:36,128 --> 00:11:40,026
can be used for messaging and they enable the

165
00:11:40,048 --> 00:11:43,758
development team of real time analytics application down the road.

166
00:11:43,844 --> 00:11:47,166
As a result, we are seeing customers replace message queues with

167
00:11:47,188 --> 00:11:50,558
data streams to provide an immediate boost in the

168
00:11:50,564 --> 00:11:54,094
capability of their architecture. So they're

169
00:11:54,142 --> 00:11:57,906
effectively, they're moving away from batch workflows to

170
00:11:57,928 --> 00:12:01,714
a lower latency streaming build application and then

171
00:12:01,752 --> 00:12:05,720
data streams are event spinal cord for services. So streams have become

172
00:12:07,610 --> 00:12:10,930
the backbone of eventually Microsoft

173
00:12:11,010 --> 00:12:14,514
service interaction. Their messaging is also there, but it's slowly

174
00:12:14,562 --> 00:12:18,186
getting into the real time stream kind of mode. And of

175
00:12:18,208 --> 00:12:21,846
course we have KmS. We'll talk about a little bit around one of the AWS

176
00:12:21,878 --> 00:12:25,020
services, which is managed Kafka service.

177
00:12:25,470 --> 00:12:29,014
Again, we talked about CDC change stream

178
00:12:29,062 --> 00:12:32,894
database and any stream machine learning, any ideal time

179
00:12:32,932 --> 00:12:35,946
automation that is also slowly getting becoming popular.

180
00:12:36,058 --> 00:12:40,334
So the fundamental message is here

181
00:12:40,452 --> 00:12:43,826
the world is moving towards stream based, which is

182
00:12:43,848 --> 00:12:48,050
near real time or real time stream based interaction

183
00:12:48,390 --> 00:12:52,034
across systems. So what

184
00:12:52,072 --> 00:12:56,310
happens here? So effectively, you ingest

185
00:12:56,650 --> 00:12:58,950
data as they're generated,

186
00:13:00,410 --> 00:13:03,430
you process without interrupting the stream.

187
00:13:03,770 --> 00:13:06,838
That is important because when you are processing data,

188
00:13:07,004 --> 00:13:09,880
your ingestion should not,

189
00:13:14,190 --> 00:13:17,622
whole streaming process should not get disturbed,

190
00:13:17,686 --> 00:13:21,890
right? Or the delayed in the person. And so you have ingest

191
00:13:22,070 --> 00:13:25,710
and then your ingestion, then you are processing the data,

192
00:13:25,780 --> 00:13:28,926
and then you are creating analysing, which will be real

193
00:13:28,948 --> 00:13:32,854
time, near real time, or completely a batch based analysing.

194
00:13:33,002 --> 00:13:36,418
So the idea is to decouple each one

195
00:13:36,424 --> 00:13:40,322
of them. They're making sure they're all frictionless at the same

196
00:13:40,376 --> 00:13:44,100
time. Create that real time, near real time

197
00:13:45,050 --> 00:13:48,546
experience for the consumers,

198
00:13:48,658 --> 00:13:52,166
right? So fundamentally, if you

199
00:13:52,188 --> 00:13:54,950
look at the principle of data streaming,

200
00:13:57,050 --> 00:14:01,978
there are producers which

201
00:14:02,144 --> 00:14:07,210
data can be produced, captured and processed in millisecond data,

202
00:14:07,360 --> 00:14:10,814
then data buffered, enabling parallel and

203
00:14:10,932 --> 00:14:14,766
independent I o. And data must be captured and

204
00:14:14,788 --> 00:14:18,986
processed in order that they're produced. So there are three fundamental

205
00:14:19,098 --> 00:14:22,658
need. So number one, in order to

206
00:14:22,664 --> 00:14:24,770
be real time, data needs to be procured,

207
00:14:25,830 --> 00:14:29,106
sorry, produced, captured and processed in

208
00:14:29,128 --> 00:14:32,494
milliseconds. If not, you can't react

209
00:14:32,542 --> 00:14:36,274
in real time. That is important. So you have to be able to produce,

210
00:14:36,322 --> 00:14:39,650
capture and process in milliseconds in seconds.

211
00:14:39,810 --> 00:14:43,414
Second, you need a system that scales up to support the

212
00:14:43,452 --> 00:14:47,506
ingestion needs of your business, but also allows you to build

213
00:14:47,548 --> 00:14:51,466
your own application on top of the data collected. Otherwise, you'll need

214
00:14:51,488 --> 00:14:55,580
to be chaining data fits together

215
00:14:56,270 --> 00:15:00,106
and which adds latency and erodes your ability to

216
00:15:00,128 --> 00:15:02,640
react in real time. Third,

217
00:15:03,090 --> 00:15:06,574
ordering is critical because your application need to be able to tell the

218
00:15:06,612 --> 00:15:09,838
story of what happened, when it happened,

219
00:15:09,924 --> 00:15:13,662
how it happened, related to other events in the pipeline.

220
00:15:13,726 --> 00:15:17,314
So that is also while you're talking about real

221
00:15:17,352 --> 00:15:21,122
time, the sequence of event

222
00:15:21,256 --> 00:15:25,090
or sequence of record process also is extremely important.

223
00:15:25,160 --> 00:15:28,854
Otherwise you lose track of when things happen within the

224
00:15:28,972 --> 00:15:31,734
when things are happening in really fast.

225
00:15:31,932 --> 00:15:35,160
So all three are equally important.

226
00:15:36,010 --> 00:15:39,498
AWS I articulated. Now moving on.

227
00:15:39,584 --> 00:15:43,846
So what are the challenges of data streaming? So organizations

228
00:15:43,878 --> 00:15:48,710
face many challenges as they attempt to build out real time data streaming capabilities

229
00:15:48,870 --> 00:15:51,850
and embark on generated real time analytics.

230
00:15:52,350 --> 00:15:56,494
Data stream are difficult to set up, tricky to

231
00:15:56,532 --> 00:16:00,446
scale, hard to achieve, high availability, complex to

232
00:16:00,468 --> 00:16:04,062
integrate into broader ecosystems, error prone,

233
00:16:04,206 --> 00:16:07,982
complex to manage. Over time, they can become very expensive

234
00:16:08,046 --> 00:16:11,486
to maintain. As I mentioned earlier as a part of my introduction,

235
00:16:11,678 --> 00:16:15,026
these challenges have often been enough of a

236
00:16:15,048 --> 00:16:18,706
reason for many companies to shy away from such projects.

237
00:16:18,738 --> 00:16:22,642
In fact, they always get pushed for those various

238
00:16:22,706 --> 00:16:26,002
reasons. The good news is, at AWS,

239
00:16:26,146 --> 00:16:30,074
it has been our core focus of the last five plus years to build

240
00:16:30,112 --> 00:16:32,730
a solution that removes those challenges.

241
00:16:36,910 --> 00:16:40,438
So let's talk about a little bit of what is

242
00:16:40,464 --> 00:16:43,962
there for real time, near real time streaming

243
00:16:44,106 --> 00:16:47,934
on AWS. So how do you address those in

244
00:16:47,972 --> 00:16:51,454
AWS? The AWS solution is easy to set

245
00:16:51,492 --> 00:16:55,490
up and use, has high availability and durability

246
00:16:56,150 --> 00:16:59,714
default being across three regions is

247
00:16:59,752 --> 00:17:03,326
fully managed and scalable, producing the complexity of managing

248
00:17:03,358 --> 00:17:06,962
the systems over time and scaling as demands

249
00:17:07,106 --> 00:17:11,042
increase, and also comes with seamless

250
00:17:11,106 --> 00:17:15,014
integration with other core AWS services such as

251
00:17:15,132 --> 00:17:18,710
elasticsearch for log analytics, s three for

252
00:17:18,860 --> 00:17:23,158
data lag storage, redshift for data housing purposes,

253
00:17:23,334 --> 00:17:26,634
lambda for serverless processing, et cetera. Finally,

254
00:17:26,752 --> 00:17:29,862
with AWS, you only

255
00:17:29,936 --> 00:17:33,534
pay for what you use, making the solution very

256
00:17:33,572 --> 00:17:37,262
cost effective. So basically

257
00:17:37,316 --> 00:17:41,550
what I'm saying, you pay only

258
00:17:41,620 --> 00:17:44,926
for the part you use. If you don't use, you do not pay for

259
00:17:44,948 --> 00:17:48,386
it. So that makes the whole solution is

260
00:17:48,408 --> 00:17:52,162
also very cost effective. It's of course one of the biggest criteria for

261
00:17:52,216 --> 00:17:55,570
making a decision for building analytics how much it's going to cost

262
00:17:55,720 --> 00:17:59,106
right on a first time, as well as on an ongoing

263
00:17:59,138 --> 00:18:02,920
basis in terms of maintenance. So let's talk about

264
00:18:04,010 --> 00:18:07,094
streaming data architecture. So,

265
00:18:07,132 --> 00:18:11,350
data streaming technology lets customer systems ingest,

266
00:18:11,430 --> 00:18:15,034
process and analyze high volumes of high

267
00:18:15,072 --> 00:18:18,554
velocity data from variety of sources in real time. We have been talking

268
00:18:18,592 --> 00:18:22,442
about it. So you enable the ingestion

269
00:18:22,506 --> 00:18:25,486
and capturing of real time streaming data,

270
00:18:25,588 --> 00:18:28,270
store it based on your processing requirements.

271
00:18:28,770 --> 00:18:32,094
And this is essentially what differentiates this from MQ type

272
00:18:32,132 --> 00:18:36,254
of setup and process. To tap the real time insight,

273
00:18:36,382 --> 00:18:40,210
you can set alerts, email notification, trigger other

274
00:18:40,360 --> 00:18:44,354
event driven application and finally move the data to

275
00:18:44,392 --> 00:18:47,510
a persistence layer. So what are those steps?

276
00:18:49,450 --> 00:18:52,114
So your data sources,

277
00:18:52,242 --> 00:18:56,294
devices and or application that produce real time data

278
00:18:56,332 --> 00:19:00,338
at high velocity. Then your

279
00:19:00,444 --> 00:19:03,786
stream ingestion data from tens of thousands of data sources can

280
00:19:03,808 --> 00:19:07,466
be written into a single stream. You need to have a pipe where

281
00:19:07,648 --> 00:19:11,050
you can push the data through. And then

282
00:19:11,200 --> 00:19:15,358
once you push the data through, you should be able to store that. So data

283
00:19:15,444 --> 00:19:18,974
stored in order, in the order received for

284
00:19:19,012 --> 00:19:22,490
a set duration, and can be replayed indifferently during this time.

285
00:19:22,580 --> 00:19:25,010
So some of the AWS,

286
00:19:27,030 --> 00:19:30,900
not some of the AWS, the kinesis products like

287
00:19:31,430 --> 00:19:34,994
Kinesis data stream you're talking about. You can

288
00:19:35,032 --> 00:19:38,418
store those streaming data for

289
00:19:38,424 --> 00:19:42,198
a year, so you can effectively replay as many times you want and if you

290
00:19:42,204 --> 00:19:45,590
need it in future debts. But it can be up to one.

291
00:19:45,740 --> 00:19:49,942
You can decide to keep it for a month or one day or any duration,

292
00:19:50,006 --> 00:19:53,500
but it is up to three years, up to up to 365 days.

293
00:19:54,590 --> 00:19:59,082
And then once you store it, then you

294
00:19:59,136 --> 00:20:02,686
then process that data so records are read in the order they

295
00:20:02,708 --> 00:20:07,150
are produced, enabling real time analytics or streaming ETL.

296
00:20:07,890 --> 00:20:11,946
So that is the time we'll again cover this as a part of Kinesis

297
00:20:11,978 --> 00:20:15,346
data firewalls, the product we'll be using for our demo.

298
00:20:15,528 --> 00:20:18,946
And then at the end you store the data for

299
00:20:18,968 --> 00:20:22,900
a longer duration. It could be data like s three or database or

300
00:20:23,590 --> 00:20:29,718
any other solution as you might think of what

301
00:20:29,724 --> 00:20:34,150
are those near real time or real time streaming producing

302
00:20:34,220 --> 00:20:37,720
within AWS. So this is the kinesis set of products.

303
00:20:40,810 --> 00:20:44,774
We made it very easy. Customers can collect, process and analyze

304
00:20:44,822 --> 00:20:48,042
data and video stream in real time without having to deal

305
00:20:48,096 --> 00:20:51,662
with the many complex cities mentioned

306
00:20:51,796 --> 00:20:55,438
before. The Kinesis services works together and provide

307
00:20:55,524 --> 00:20:59,630
flexibility and options to tailor your streaming architecture to

308
00:20:59,700 --> 00:21:03,322
your specific use cases. We have Kinesis

309
00:21:03,386 --> 00:21:07,294
data stream allows you to collect and store streaming

310
00:21:07,342 --> 00:21:11,390
data in a scale on demand. The firehouse Kinesis data firehouse

311
00:21:11,470 --> 00:21:15,574
firehouse is a fast and simply and simple way to stream data into

312
00:21:15,612 --> 00:21:19,506
data lakes or other end destination again at scale,

313
00:21:19,538 --> 00:21:22,962
with the ability to execute serverless data transformation

314
00:21:23,106 --> 00:21:26,578
as required. And then you have data analysing

315
00:21:26,674 --> 00:21:30,186
allows you to build, integrate and execute applications in

316
00:21:30,208 --> 00:21:33,574
SQL and Java. These three services work together to enable

317
00:21:33,622 --> 00:21:37,500
customers to stream, process and deliver data in real time.

318
00:21:37,870 --> 00:21:41,390
Then you have MSK

319
00:21:41,730 --> 00:21:45,534
AWS managed streaming for Apache Kafka, which is

320
00:21:45,572 --> 00:21:49,066
fully managed service for customers who prefer Apache Kafka

321
00:21:49,178 --> 00:21:52,866
or use Apache Kafka alongside Kinesis to enable specific use

322
00:21:52,888 --> 00:21:56,626
cases. The fully managed service and then at the end you have

323
00:21:56,728 --> 00:22:00,834
the Kinesis video stream allows customers to

324
00:22:00,872 --> 00:22:04,382
capture process store media streams for playback,

325
00:22:04,446 --> 00:22:06,200
analytics and machine learning.

326
00:22:08,570 --> 00:22:11,974
Out of this file, we'll be using two of them. The first two Kinesis data

327
00:22:12,012 --> 00:22:15,960
stream and Kinesis data firewalls as a part of the demo.

328
00:22:16,890 --> 00:22:21,414
Moving on, so little more about Kinesis

329
00:22:21,462 --> 00:22:24,998
data stream and the firewalls

330
00:22:25,094 --> 00:22:28,602
I'll cover. So KDS is popularly known as

331
00:22:28,656 --> 00:22:32,522
Kinesis data stream is massively scalable and durable real

332
00:22:32,576 --> 00:22:36,586
time data streaming service. It's continuously capture gigabytes

333
00:22:36,618 --> 00:22:40,270
of data per second from hundreds of thousands of sources

334
00:22:40,850 --> 00:22:43,918
such as website click streaming. That is the one we'll be

335
00:22:43,924 --> 00:22:47,282
using. Website click streaming as a part of the demo database event

336
00:22:47,336 --> 00:22:51,314
stream, financial transactions, social media feeds, it logs and

337
00:22:51,352 --> 00:22:55,130
location tracking events. The data collected is available in milliseconds

338
00:22:55,150 --> 00:22:58,754
to enable real time analytics use cases such as real time dashboard,

339
00:22:58,802 --> 00:23:02,050
real time anomaly detection, dynamic producing

340
00:23:02,130 --> 00:23:06,326
and many more. So make

341
00:23:06,348 --> 00:23:10,140
your streaming data available to multiple real time analytics application

342
00:23:10,830 --> 00:23:15,370
to s three or to AWS lambda in events

343
00:23:15,440 --> 00:23:18,166
milliseconds of the data being collected.

344
00:23:18,278 --> 00:23:21,958
That is fast, right? You can't probably get

345
00:23:21,984 --> 00:23:25,226
better than that and getting the data from which is being ingested

346
00:23:25,258 --> 00:23:29,386
and pushing it to the s three or lambda within 70 milliseconds.

347
00:23:29,578 --> 00:23:33,410
It is durable and it is secure,

348
00:23:34,310 --> 00:23:38,386
easy to use. It has got call it

349
00:23:38,408 --> 00:23:41,198
KCL Kinesis client libraries, connectors,

350
00:23:41,294 --> 00:23:45,854
agents and it is easily integrated

351
00:23:45,902 --> 00:23:49,254
with lambda and kinesis data analytics and

352
00:23:49,292 --> 00:23:52,806
data firehouse. It is elastic, dynamically scale your

353
00:23:52,828 --> 00:23:56,466
application currency data sync can scale from megabytes

354
00:23:56,498 --> 00:23:59,800
to terabytes of data per hour.

355
00:24:00,490 --> 00:24:04,394
Scale from thousands to millions of put records per

356
00:24:04,432 --> 00:24:08,202
second. You can dynamically adjust the throughput of your stream at any time

357
00:24:08,256 --> 00:24:11,898
based on the volume of your data input data and of course low cost

358
00:24:11,984 --> 00:24:15,326
can see detectives has no upfront cost. You only pay for the

359
00:24:15,348 --> 00:24:19,280
resources you use for as little as zero point

360
00:24:19,650 --> 00:24:23,822
15 events. Zero point $15

361
00:24:23,956 --> 00:24:27,246
per hour of course. For the latest

362
00:24:27,278 --> 00:24:31,506
pricing you can go to the website AWS website and get

363
00:24:31,528 --> 00:24:36,630
the information in more details then moving

364
00:24:36,700 --> 00:24:41,670
to the firehose.

365
00:24:46,010 --> 00:24:49,590
All right, data firehose

366
00:24:50,090 --> 00:24:54,954
again is the fully managed service is

367
00:24:54,992 --> 00:24:58,534
the easiest way to reliably data load streaming

368
00:24:58,582 --> 00:25:01,966
data into data lakes again. We'll be using this as

369
00:25:01,988 --> 00:25:05,200
a part of the demo to load the data in S three.

370
00:25:05,810 --> 00:25:09,402
It captures transfer and load streaming data into s three redshift

371
00:25:09,466 --> 00:25:12,654
elasticsearch splank, enabling near real

372
00:25:12,692 --> 00:25:15,906
time analysing with existing business intelligence tools. That's what

373
00:25:15,928 --> 00:25:19,522
you are already using today. It is fully managed service that

374
00:25:19,576 --> 00:25:23,314
automatically scales to match the throughput of your data and

375
00:25:23,352 --> 00:25:27,046
requires no ongoing administration. It can

376
00:25:27,068 --> 00:25:30,710
also batch, compress, transform, encrypt data before

377
00:25:30,780 --> 00:25:34,166
loading it, minimize the amount of storage used at

378
00:25:34,188 --> 00:25:38,614
the destination and increasing security. You can easily create firewalls

379
00:25:38,662 --> 00:25:42,666
delivery stream from AWS console, configure it

380
00:25:42,688 --> 00:25:46,122
with few clicks and again, we'll cover this. How easy

381
00:25:46,176 --> 00:25:49,706
to configure it because we'll be literally finishing the demo

382
00:25:49,808 --> 00:25:53,440
end to end. For an ecommerce website,

383
00:25:53,810 --> 00:25:57,214
click streaming data collection in 15 to 20 minutes time

384
00:25:57,332 --> 00:26:01,226
and within that we will be setting up Kinesis data stream.

385
00:26:01,338 --> 00:26:05,054
Kinesis data firewalls lambda s

386
00:26:05,092 --> 00:26:09,158
three and then at the end Amazon quicksite.

387
00:26:09,274 --> 00:26:12,754
All can be done within the demo time of 20

388
00:26:12,792 --> 00:26:16,534
minutes. So in realtime you can spend few hours to set

389
00:26:16,572 --> 00:26:20,086
it up and get the data for the first time. When you

390
00:26:20,108 --> 00:26:24,630
attend for the first time with ekinesis

391
00:26:24,970 --> 00:26:28,618
data firehose, you only pay for the amount of data we

392
00:26:28,784 --> 00:26:32,570
transmit through the service and is applicable for

393
00:26:32,640 --> 00:26:36,022
data format conversion.

394
00:26:36,086 --> 00:26:39,130
There is no minimum fee or setup fee as such,

395
00:26:39,280 --> 00:26:42,400
like many other AWS services.

396
00:26:48,000 --> 00:26:51,804
So what we'll do, we'll now

397
00:26:52,002 --> 00:26:53,710
see the different.

398
00:26:55,780 --> 00:27:00,044
We have seen the five steps

399
00:27:00,092 --> 00:27:03,584
of data stream architecture and we see

400
00:27:03,622 --> 00:27:07,292
how those steps can be aligned with AWS product

401
00:27:07,446 --> 00:27:11,044
or how

402
00:27:11,082 --> 00:27:14,596
they fall within the AWS product we have been talking about.

403
00:27:14,778 --> 00:27:18,710
So this is an example. Like from the left, if you see there are

404
00:27:19,740 --> 00:27:23,892
data sources which producing

405
00:27:23,956 --> 00:27:27,304
data in millions of

406
00:27:27,342 --> 00:27:30,936
records from different devices, different applications, and which

407
00:27:30,958 --> 00:27:34,844
is getting streamed. And then

408
00:27:34,882 --> 00:27:40,124
the stream is being stored in

409
00:27:40,162 --> 00:27:44,296
KDS, which is kinesis data stream. And then the stream

410
00:27:44,328 --> 00:27:48,540
is being processed using Kinesis data analytics

411
00:27:48,700 --> 00:27:52,320
or kinesis data firewalls. Or you can actually use

412
00:27:52,470 --> 00:27:55,472
Kinesis video stream which is not shown on the screen here.

413
00:27:55,606 --> 00:28:00,180
And at the end you

414
00:28:00,330 --> 00:28:04,500
store those data for a longer term analytics.

415
00:28:05,640 --> 00:28:09,316
For a longer term analytics. So that's pretty much I

416
00:28:09,338 --> 00:28:13,064
wanted to discuss and take you through the

417
00:28:13,102 --> 00:28:17,050
AWS streaming solution before getting into

418
00:28:18,940 --> 00:28:22,090
the use cases and the demo.

419
00:28:24,720 --> 00:28:28,508
Now let's start the demo. But before I actually go to the

420
00:28:28,674 --> 00:28:32,828
browser console and take you through the entire demo, I just wanted to

421
00:28:32,994 --> 00:28:36,384
spend few minutes to explain you the use case

422
00:28:36,422 --> 00:28:40,208
and the architecture behind the entire demo.

423
00:28:40,374 --> 00:28:44,400
So it's a very simple use case. I have a demo website

424
00:28:44,550 --> 00:28:48,416
which is listing list of products and it has got two actions

425
00:28:48,608 --> 00:28:52,736
is a buy and view details. The objective

426
00:28:52,768 --> 00:28:56,710
of this demo would be to capture the

427
00:28:57,080 --> 00:29:00,552
user actions like how many people are clicking on buy and which

428
00:29:00,606 --> 00:29:04,104
product they're buying. As simple as that.

429
00:29:04,302 --> 00:29:07,864
Now quickly. So from the left,

430
00:29:07,902 --> 00:29:10,984
if you see the users logging in or

431
00:29:11,022 --> 00:29:14,700
signing up. Then they browse the product, then they view

432
00:29:14,770 --> 00:29:18,076
the product details, then they decide to go ahead

433
00:29:18,098 --> 00:29:21,564
to buy or not. That's it. Now let's move into

434
00:29:21,602 --> 00:29:23,980
the actual architecture.

435
00:29:24,640 --> 00:29:28,160
So the simple website is hosted in s three bucket

436
00:29:28,500 --> 00:29:31,808
and the website is being accessed through the cloud front. And in front of

437
00:29:31,814 --> 00:29:35,324
the cloud front you have the web application firewall WAF,

438
00:29:35,372 --> 00:29:39,776
popularly known as now. Every time

439
00:29:39,878 --> 00:29:43,524
user clicks on something, action HTTP request goes

440
00:29:43,562 --> 00:29:47,684
to the cloud front and that

441
00:29:47,802 --> 00:29:51,076
click is streamed through the click. Information is streamed

442
00:29:51,108 --> 00:29:53,960
through the Kinesis data stream.

443
00:29:54,540 --> 00:29:57,572
And then from Kinesis data stream,

444
00:29:57,716 --> 00:30:00,772
it is consumed by kinesis data firewalls.

445
00:30:00,916 --> 00:30:04,016
And once it is consumed by data firewalls,

446
00:30:04,148 --> 00:30:07,708
for every record which is being consuming by data fire, you can trigger a

447
00:30:07,714 --> 00:30:11,016
lambda to do additional processing of the data which is being consumed

448
00:30:11,048 --> 00:30:14,968
by data firewalls. And then

449
00:30:15,154 --> 00:30:18,416
once it is processed by lambda, it goes to

450
00:30:18,438 --> 00:30:22,176
the s three. Of course, kinesis data firehose can

451
00:30:22,278 --> 00:30:26,096
send data to many destinations, including s three, redshift and

452
00:30:26,118 --> 00:30:29,584
other dynodB, et cetera. So in this

453
00:30:29,622 --> 00:30:33,684
case we are actually pushing the data, all those click string information from

454
00:30:33,722 --> 00:30:37,744
cloud, from, through the Kinesis data string to the firehose,

455
00:30:37,792 --> 00:30:41,944
to the s three. And once it goes to s three, we're using the

456
00:30:41,982 --> 00:30:45,908
serverless ETL platform of AWS

457
00:30:46,004 --> 00:30:49,988
glue and crawler and accessing

458
00:30:50,164 --> 00:30:53,436
further, creating a data model for the

459
00:30:53,458 --> 00:30:57,480
whole analysing platform using that glue and crawler.

460
00:30:57,640 --> 00:31:01,324
And then we are seeing the data through QuickSight and

461
00:31:01,362 --> 00:31:05,016
Athena integration. Again, everything is serverless, few clicks away from

462
00:31:05,058 --> 00:31:08,860
actual data coming through the quicksight, the entire operation

463
00:31:09,020 --> 00:31:12,624
could take up to, depending on how

464
00:31:12,662 --> 00:31:16,752
you automated the whole setup would take up to

465
00:31:16,886 --> 00:31:19,350
few minutes, less than five minutes, definitely.

466
00:31:21,160 --> 00:31:24,660
Here in the kinesis data stream, there's a minimum

467
00:31:25,240 --> 00:31:28,644
buffer time of 1 minute. So you have to have a buffer of 1

468
00:31:28,682 --> 00:31:32,176
minute. Means when the data comes from cloud front to the kinesis

469
00:31:32,208 --> 00:31:35,476
data stream to fire up, there's a minimum delay of 1 minute because it accumulates

470
00:31:35,508 --> 00:31:38,856
for a minute and then pushes to the next level. So by

471
00:31:38,878 --> 00:31:42,024
the time it comes here in s three, maybe a couple of minutes, and then

472
00:31:42,062 --> 00:31:45,732
you do the crawler trigger and the workflow

473
00:31:45,796 --> 00:31:49,532
goes through, puts the data and you can do data

474
00:31:49,586 --> 00:31:53,304
integration with your other systems. And then whole integration

475
00:31:53,352 --> 00:31:56,828
might take few more minutes, and then eventually in a quick site it will appear.

476
00:31:56,924 --> 00:32:00,160
So let's say in five minutes, you will get the data from the

477
00:32:00,230 --> 00:32:04,016
time somebody clicks on a product and by

478
00:32:04,038 --> 00:32:07,072
the time you see in a quick site. So idea is to,

479
00:32:07,126 --> 00:32:11,060
in this case, to see which maybe you can see end of the day

480
00:32:11,130 --> 00:32:14,564
or maybe every hour, which product, when you have launched a product,

481
00:32:14,682 --> 00:32:17,060
which product possibly is more popular,

482
00:32:17,720 --> 00:32:21,092
which one people are buying more,

483
00:32:21,146 --> 00:32:24,884
reviewing more those kind of information. So you don't need to build any other analytics

484
00:32:24,932 --> 00:32:28,584
platform, but just use the Clickstream data to see in your

485
00:32:28,622 --> 00:32:31,320
Quicksight which could be used by the business users.

486
00:32:32,240 --> 00:32:35,692
Now with that, I will now move

487
00:32:35,746 --> 00:32:38,844
to the browser to

488
00:32:38,882 --> 00:32:42,424
show you the entire architecture.

489
00:32:42,472 --> 00:32:45,820
So we'll start with kinesis data stream.

490
00:32:46,560 --> 00:32:49,792
We'll create that I've already pre created, but I'll show you how to create

491
00:32:49,846 --> 00:32:53,184
it. And then we'll move to cloud from and see

492
00:32:53,222 --> 00:32:57,016
how the cloud from is binding to the kinesis data stream.

493
00:32:57,148 --> 00:33:00,384
So let me switch to browser

494
00:33:00,432 --> 00:33:03,270
now. Ah,

495
00:33:06,680 --> 00:33:10,964
all right. So I have kinesis

496
00:33:11,012 --> 00:33:15,240
data stream here. I've already created a data

497
00:33:15,310 --> 00:33:18,584
stream here. Conf fourty two website demo. I can

498
00:33:18,622 --> 00:33:22,264
create one, but it takes few minutes to deploy. So it's very

499
00:33:22,302 --> 00:33:26,156
simple, straightforward. It basically asks you what is the capacity of

500
00:33:26,178 --> 00:33:29,512
your kinesis data stream and how is the capacity is calculated,

501
00:33:29,576 --> 00:33:33,228
is depending on what is the average size of

502
00:33:33,234 --> 00:33:36,668
the records and what is the number of records per second is coming. So let's

503
00:33:36,684 --> 00:33:40,348
say you have ten records

504
00:33:40,444 --> 00:33:43,884
per second coming in and you have only one string to capture.

505
00:33:43,932 --> 00:33:47,808
And then it will calculate how many shards do you need. So for more information

506
00:33:47,894 --> 00:33:51,488
about the shard and everything, you can go to the edible documentation

507
00:33:51,584 --> 00:33:55,012
when you have time. So I've created that.

508
00:33:55,066 --> 00:33:59,092
Now let's move on to cloud front. So I have created a cloud front

509
00:33:59,146 --> 00:34:02,984
distribution which is pointing to my s three origin.

510
00:34:03,102 --> 00:34:06,408
So this is my s three origin. Basically they all pointing to

511
00:34:06,414 --> 00:34:09,832
the same s three origin, which is basically a single

512
00:34:09,886 --> 00:34:13,404
page website like this. As simple

513
00:34:13,442 --> 00:34:17,084
as that. I have buy and buy buttons and

514
00:34:17,122 --> 00:34:19,070
I have view details button.

515
00:34:21,280 --> 00:34:25,792
That's pretty much now

516
00:34:25,846 --> 00:34:30,304
let's move into cloud from once I have created, let's work

517
00:34:30,422 --> 00:34:34,770
with this. Once coding with

518
00:34:35,380 --> 00:34:39,700
QY. So what I've done is I have just gone to the logs.

519
00:34:40,600 --> 00:34:43,780
In the logs you have real time configuration.

520
00:34:44,680 --> 00:34:48,148
Real time basically takes the click stream, which whatever

521
00:34:48,234 --> 00:34:51,368
request goes through the cloud front, it can capture that.

522
00:34:51,534 --> 00:34:54,330
Now let me create one,

523
00:34:56,300 --> 00:34:59,176
but I'll not save that because I already created one. But I'll show you what

524
00:34:59,198 --> 00:35:02,780
are the information you need to provide when you

525
00:35:02,850 --> 00:35:06,536
create the configuration.

526
00:35:06,568 --> 00:35:10,204
So give a name, give a sampling rate. This is nothing but what

527
00:35:10,242 --> 00:35:16,412
percentage of the click stream data you want to capture. Is it 50%?

528
00:35:16,466 --> 00:35:20,016
Is it 100%? Let's say I put 100% and

529
00:35:20,038 --> 00:35:23,660
then which field of the request you want to capture.

530
00:35:23,820 --> 00:35:26,636
All these fields you can capture which is coming as a part of the HTTP

531
00:35:26,668 --> 00:35:29,988
request. But most important bit for us would be

532
00:35:30,154 --> 00:35:33,344
to capture this uri query

533
00:35:33,392 --> 00:35:36,676
parameters. Apart from that you can capture other information like

534
00:35:36,698 --> 00:35:40,580
the country, the port and IP addresses, et cetera.

535
00:35:41,400 --> 00:35:45,284
And then what is the endpoint, where will the data get pushed?

536
00:35:45,332 --> 00:35:48,644
So remember I created KDS, which is kinesis data stream.

537
00:35:48,692 --> 00:35:52,732
Confront conf 42 website demo which is nothing.

538
00:35:52,786 --> 00:35:56,156
But here in the other tab, if I can

539
00:35:56,178 --> 00:35:59,660
go to other tab, yeah, this one, I'm connecting to this

540
00:35:59,730 --> 00:36:03,870
kinesis data stream now. Once I connect it,

541
00:36:06,400 --> 00:36:10,912
okay, now back to cloud

542
00:36:10,966 --> 00:36:14,930
font, real time configuration as I showed you. You can pick up

543
00:36:15,380 --> 00:36:18,924
whatever information you want from the request

544
00:36:18,972 --> 00:36:22,404
which is coming in now. We have decided to go with 100%

545
00:36:22,442 --> 00:36:26,276
subliminates every request we considered coming in. And of course it

546
00:36:26,298 --> 00:36:30,250
is getting delivered to this kinesis data stream which we already

547
00:36:30,620 --> 00:36:34,170
created earlier. Now let's go

548
00:36:35,180 --> 00:36:38,616
to the architecture bank and see how much

549
00:36:38,638 --> 00:36:39,720
you have covered.

550
00:36:43,500 --> 00:36:46,830
I'll just pull together the architecture slides. Here you go.

551
00:36:47,520 --> 00:36:51,196
So here we have already have the s three bucket with the

552
00:36:51,218 --> 00:36:54,976
website. We have configured the cloud front, we have configured the

553
00:36:55,078 --> 00:36:58,690
data stream. So now the requests are all coming from,

554
00:37:00,500 --> 00:37:04,576
every request is going to the cloud front. The stream is also getting

555
00:37:04,678 --> 00:37:08,028
delivered to kinesis data stream. Now what we'll

556
00:37:08,044 --> 00:37:11,936
do, we'll set up this so that it can consume the record, those records,

557
00:37:11,968 --> 00:37:15,204
click stream records from Kinesis data stream and

558
00:37:15,242 --> 00:37:18,564
then process by lambda and then pushes to

559
00:37:18,602 --> 00:37:23,784
s three bucket. Let's go back to the

560
00:37:23,822 --> 00:37:24,680
browser.

561
00:37:29,900 --> 00:37:34,216
So we have kinesis data files, I've already created

562
00:37:34,248 --> 00:37:37,310
one, let's see, what information do you have in that?

563
00:37:37,760 --> 00:37:41,324
So as always,

564
00:37:41,522 --> 00:37:44,616
this is the source, which is, as we have seen, the architecture

565
00:37:44,648 --> 00:37:48,936
diagram. We are getting the source from kinesis data stream.

566
00:37:49,128 --> 00:37:52,764
Then we are consuming that, transforming the records

567
00:37:52,812 --> 00:37:56,352
using a lambda function. And the lambda function is nothing but

568
00:37:56,486 --> 00:37:59,896
just taking the input, see all those attributes

569
00:37:59,948 --> 00:38:03,604
which are coming from the header from the request and then messaging it,

570
00:38:03,642 --> 00:38:07,092
taking the product id, product name, status, et cetera. Very simple,

571
00:38:07,146 --> 00:38:11,636
straightforward, but you can have of course much more complicated lambda

572
00:38:11,668 --> 00:38:15,976
function which can create different alert, et cetera, whatever if

573
00:38:15,998 --> 00:38:19,544
you want coming back. And then I'm putting that into

574
00:38:19,582 --> 00:38:23,640
the destination bucket which is conf 42 website demo

575
00:38:23,710 --> 00:38:27,016
September 2021. And then I'm

576
00:38:27,048 --> 00:38:30,750
just creating a prefix for the bucket so that I know which day, what month,

577
00:38:31,280 --> 00:38:34,816
which month, year and day and hour the request came through

578
00:38:34,838 --> 00:38:38,096
for my further analysis. And I have some

579
00:38:38,198 --> 00:38:41,756
other configuration which, for the timing

580
00:38:41,788 --> 00:38:45,164
you can ignore like encryption,

581
00:38:45,212 --> 00:38:48,436
et cetera. Now I also have a set of

582
00:38:48,458 --> 00:38:51,536
a bucket for all the error. In case there is a processing

583
00:38:51,568 --> 00:38:54,724
error happens, it will go into that

584
00:38:54,762 --> 00:38:58,320
bucket. Now let's

585
00:38:58,400 --> 00:39:01,832
go to s three. Since I've already sent

586
00:39:01,886 --> 00:39:05,464
some request. My s three bucket is already populated with

587
00:39:05,662 --> 00:39:10,424
some of the records. If you see here, the website is my year

588
00:39:10,462 --> 00:39:14,350
2021, and then I have month

589
00:39:14,960 --> 00:39:18,616
and then I have day ten and day eleven events

590
00:39:18,648 --> 00:39:21,992
of September. And I had at 01:00

591
00:39:22,056 --> 00:39:25,616
UTC I have request came in and then I

592
00:39:25,638 --> 00:39:29,072
had 02:00 UTC the records came

593
00:39:29,126 --> 00:39:29,730
in.

594
00:39:32,820 --> 00:39:36,416
So now data is all there in s three. Once it goes to s

595
00:39:36,438 --> 00:39:40,260
three, if you remember, we have the glue and crawler.

596
00:39:40,760 --> 00:39:44,390
So I have a workflow already created here.

597
00:39:45,080 --> 00:39:48,452
And what it does, it actually goes

598
00:39:48,506 --> 00:39:52,628
through the s three, creates a data catalog, database catalog.

599
00:39:52,804 --> 00:39:56,810
So I have the database called Conf 42 demo,

600
00:39:57,740 --> 00:40:00,776
and I have the table which it

601
00:40:00,798 --> 00:40:04,652
has created called website. Because the

602
00:40:04,706 --> 00:40:09,404
prefix of this s

603
00:40:09,442 --> 00:40:13,724
three bucket is website here. So it has picked up that and

604
00:40:13,762 --> 00:40:17,360
then back to console here. If you see in this table,

605
00:40:17,860 --> 00:40:21,692
it has actually picked up all those attributes

606
00:40:21,756 --> 00:40:24,988
which are there, part of the request, it's all picked

607
00:40:25,004 --> 00:40:28,372
up and it has done a partitioning based on near month,

608
00:40:28,426 --> 00:40:32,244
day and hour, so that the analysing your

609
00:40:32,282 --> 00:40:35,492
analytics can run faster and does

610
00:40:35,546 --> 00:40:38,740
the processing based on those partition. And those are unique.

611
00:40:39,580 --> 00:40:41,610
Now I have the data here,

612
00:40:43,500 --> 00:40:45,560
I have the tables created.

613
00:40:46,940 --> 00:40:50,136
Then I just need to trigger this workflow. What it

614
00:40:50,158 --> 00:40:54,012
does, it is again crawl through the latest data

615
00:40:54,066 --> 00:40:57,180
which has come in s three bucket and puts it into the table.

616
00:40:58,240 --> 00:41:02,248
So as the visitors visits the website, the data keeps

617
00:41:02,264 --> 00:41:06,610
coding in and you can schedule your

618
00:41:06,980 --> 00:41:10,864
workflow to run as

619
00:41:10,902 --> 00:41:15,328
frequently as you want. And then every time the

620
00:41:15,494 --> 00:41:19,128
workflow runs, your s three data lake

621
00:41:19,164 --> 00:41:22,484
will get populated with the latest data and

622
00:41:22,522 --> 00:41:26,784
then the same data. We can pull it from the quicksight.

623
00:41:26,832 --> 00:41:30,168
But before I go to quicksight, I just want to show you how

624
00:41:30,254 --> 00:41:33,976
the workflow looks like. Right. So this is typically workflow. It is crawling and then

625
00:41:33,998 --> 00:41:37,690
it's populating the database within s three,

626
00:41:38,060 --> 00:41:39,770
the tables within s three.

627
00:41:41,180 --> 00:41:45,550
So I can see that show that there are multiple run happened

628
00:41:45,920 --> 00:41:49,900
for this workflow. And if I open one of these and it tells you

629
00:41:49,970 --> 00:41:54,284
every steps has gone through successfully, and for last time, I ran here.

630
00:41:54,482 --> 00:41:57,960
All right, now let's go to Quicksight.

631
00:41:58,040 --> 00:42:01,820
So, quicksite you can create quicksighted dashboard

632
00:42:01,900 --> 00:42:04,944
from different sources. As you can see on the screen

633
00:42:04,982 --> 00:42:08,804
out of the box, we could have done from s three, but only

634
00:42:08,842 --> 00:42:12,992
thing in that case, we couldn't have done the data integration from different sources,

635
00:42:13,136 --> 00:42:15,700
whereas in our case, in Athena,

636
00:42:16,200 --> 00:42:20,280
you can get data from different sources, you can create

637
00:42:20,350 --> 00:42:23,960
a workflow, and then

638
00:42:24,110 --> 00:42:28,170
using the workflow, you can integrate that and create a real

639
00:42:29,180 --> 00:42:32,232
data set for your analytics tools to pick up.

640
00:42:32,366 --> 00:42:36,060
So I'm not going to create, because I already created the

641
00:42:36,130 --> 00:42:40,220
data set, and I'll show you the data set which I created.

642
00:42:40,800 --> 00:42:43,260
This is the data set I created.

643
00:42:45,540 --> 00:42:48,672
It has already 34 rows in it.

644
00:42:48,806 --> 00:42:52,464
Just refreshed it

645
00:42:52,662 --> 00:42:56,076
into last twelve minutes, twelve minutes back. So let's

646
00:42:56,108 --> 00:42:59,904
see whether doing a refresh again,

647
00:42:59,942 --> 00:43:03,392
it increases the number count. I am not sure whether

648
00:43:03,446 --> 00:43:06,950
I did any more request in between, but while that is happening,

649
00:43:08,440 --> 00:43:11,384
what I want to do is I want to show you how the data is

650
00:43:11,422 --> 00:43:15,352
actually coming into the

651
00:43:15,486 --> 00:43:16,970
quicksight. Right?

652
00:43:19,180 --> 00:43:22,588
So this is the data which you

653
00:43:22,594 --> 00:43:26,364
can see in the quicksight by

654
00:43:26,402 --> 00:43:32,284
day and hour. You can see the

655
00:43:32,322 --> 00:43:35,810
dark blue is the data on 10th, which came

656
00:43:36,420 --> 00:43:40,844
10 September, which is at 09:00, 10:00 twelve and 01:00

657
00:43:40,972 --> 00:43:45,676
as in 11th, it is at 01:00

658
00:43:45,708 --> 00:43:49,204
and 02:00 in the afternoon. And then you can do

659
00:43:49,242 --> 00:43:52,996
whatever dissect, dissection of the data you

660
00:43:53,018 --> 00:43:56,852
want. If you want product name, you can get the product name,

661
00:43:56,906 --> 00:44:00,516
which product was clicked more. You can

662
00:44:00,538 --> 00:44:02,310
do by hour as you have seen,

663
00:44:04,300 --> 00:44:07,576
or you can do by month. But in this case, there's only one month of

664
00:44:07,598 --> 00:44:10,676
data available on the 10th and 11th.

665
00:44:10,708 --> 00:44:13,804
You can see that. So that's pretty

666
00:44:13,842 --> 00:44:18,220
much. So if I go back to the presentation

667
00:44:18,800 --> 00:44:19,870
for a moment,

668
00:44:25,040 --> 00:44:28,816
it. So in summary, I think we have covered pretty much end to

669
00:44:28,838 --> 00:44:31,810
end. If you take a look from the left,

670
00:44:32,420 --> 00:44:35,568
the user is accessing the website which is

671
00:44:35,574 --> 00:44:39,072
hosted in s three through cloud front. Every request

672
00:44:39,136 --> 00:44:43,440
which goes through Cloudfront. Also the request

673
00:44:43,520 --> 00:44:47,156
log stream goes to the Kinesis data stream. And the

674
00:44:47,178 --> 00:44:50,400
Kinesis data stream is connected to firehose

675
00:44:50,560 --> 00:44:54,744
as the Kinesis data firehose as a consumer. And when

676
00:44:54,782 --> 00:44:58,296
the records comes to firehose, the lambda gets kicked in

677
00:44:58,318 --> 00:45:01,704
for everyday record. And you can do anything you

678
00:45:01,742 --> 00:45:05,060
want with that record using the lambda.

679
00:45:05,220 --> 00:45:08,552
And you can use that lambda to communicate further downstream

680
00:45:08,616 --> 00:45:12,444
systems if you want. For any specific scenario, if you have or

681
00:45:12,482 --> 00:45:16,496
you can send a notification, SNS or email, you can trigger email, anything you

682
00:45:16,518 --> 00:45:20,544
want as such. So let's say you have a very high

683
00:45:20,582 --> 00:45:24,864
value product which people are clicking quite a lot but not

684
00:45:24,902 --> 00:45:28,630
buying. So you can have a scenario where you can count those

685
00:45:29,240 --> 00:45:32,704
and keep counting it and keep the count in a dynamodb

686
00:45:32,752 --> 00:45:35,430
database. When the count reaches a certain number,

687
00:45:35,880 --> 00:45:39,648
lambda can send a notification raising concern, right, so you

688
00:45:39,674 --> 00:45:43,224
can do all those different scenarios and then after that

689
00:45:43,262 --> 00:45:47,684
it goes to the s three. And again we did a prefix

690
00:45:47,732 --> 00:45:51,464
which is website near month, day and

691
00:45:51,502 --> 00:45:55,096
time. And you can do whatever you want to do. You can define

692
00:45:55,128 --> 00:45:59,230
that. Then that is picked up the

693
00:46:00,240 --> 00:46:04,572
crawler glue job and the crawler picks up the data from s three based

694
00:46:04,626 --> 00:46:08,572
on the frequency fix which has been set

695
00:46:08,626 --> 00:46:12,068
up. And then eventually through Athena Quicksight

696
00:46:12,104 --> 00:46:15,504
integration, the data is available in the quicksight. So if you see the

697
00:46:15,542 --> 00:46:19,156
entire end to end architecture, there was no coding involved, only the lambda was

698
00:46:19,178 --> 00:46:23,236
used just to manipulate the

699
00:46:23,338 --> 00:46:27,284
click stream, which is coming for a better for the clarity of the

700
00:46:27,322 --> 00:46:30,824
data which goes through the s three to the quicksite. Apart from that, there's nothing

701
00:46:30,862 --> 00:46:34,324
else effectively, you know the user

702
00:46:34,372 --> 00:46:37,592
behavior, how they are clicking the different

703
00:46:37,646 --> 00:46:40,650
products in a quick site in minutes.

704
00:46:42,880 --> 00:46:46,664
That's pretty much. Thank you for joining

705
00:46:46,712 --> 00:46:49,550
in. Good to have you guys.

706
00:46:50,320 --> 00:46:55,080
And if you want to learn more about AWS

707
00:46:55,160 --> 00:46:58,348
analytics platform, there are quick site, of course those are all available.

708
00:46:58,434 --> 00:47:01,676
You can go to AWS website and based

709
00:47:01,698 --> 00:47:05,276
on interest which area you want to focus, you can get additional information.

710
00:47:05,458 --> 00:47:08,130
Thank you once again. Have a wonderful day.

