1
00:00:25,570 --> 00:00:29,174
You all right. Welcome everybody.

2
00:00:29,292 --> 00:00:33,106
Pleasure to be here. And today I'm going to show you some strategies

3
00:00:33,138 --> 00:00:36,786
to implement observability in your company without acquiring

4
00:00:36,818 --> 00:00:40,390
engineers efforts. If you are doing observability using

5
00:00:40,460 --> 00:00:44,454
open source solutions, as I am, you probably think

6
00:00:44,492 --> 00:00:48,050
about do the same thing that most of the observability

7
00:00:48,130 --> 00:00:51,738
vendors are doing out instrument maintain. They are just deploying

8
00:00:51,754 --> 00:00:55,198
an agent in your host and starts collecting your

9
00:00:55,284 --> 00:00:59,166
metrics, logs and traces. So the idea of this talk is to

10
00:00:59,188 --> 00:01:03,122
show you some strategies using things that you probably already have

11
00:01:03,176 --> 00:01:07,214
on your infrastructure to start collecting all those signos

12
00:01:07,342 --> 00:01:10,980
and improve your developers experience and without

13
00:01:11,590 --> 00:01:15,534
requiring engineering effort. So the idea is to let developers

14
00:01:15,582 --> 00:01:19,334
focus on what really matters for them. Okay? So I hope you

15
00:01:19,372 --> 00:01:23,190
enjoy and let's get started. Cool. So before

16
00:01:23,260 --> 00:01:26,754
we start talking about observability strategies, let me introduce

17
00:01:26,802 --> 00:01:30,166
myself. My name is Nicolas Takashi. I'm a brazilian software

18
00:01:30,198 --> 00:01:34,060
engineer living in Portugal for the last seven years.

19
00:01:34,670 --> 00:01:38,262
I'm open source contributor especially on the observability

20
00:01:38,406 --> 00:01:42,042
ecosystem for projects such as Prometheus, operator,

21
00:01:42,186 --> 00:01:46,670
opentelemetry and many other projects on the observability ecosystem.

22
00:01:47,010 --> 00:01:50,922
I'm currently working at quarter logs it analytics

23
00:01:50,986 --> 00:01:54,754
platform for logs, metrics, traces and also security

24
00:01:54,872 --> 00:01:58,114
data. And you can find me on my

25
00:01:58,152 --> 00:02:01,986
social media networks such as Twitter, LinkedIn, GitHub by

26
00:02:02,008 --> 00:02:05,002
my name. I'm usually talking about Kubernetes,

27
00:02:05,086 --> 00:02:08,834
observability, Githubs distributed

28
00:02:08,882 --> 00:02:12,262
system and also of course personal life and

29
00:02:12,316 --> 00:02:16,214
changing experience. So I hope to see you there and let's move

30
00:02:16,252 --> 00:02:20,282
forward. Okay, cool. So now

31
00:02:20,336 --> 00:02:24,138
everybody know who I am, let's move forward. Let's start talking about

32
00:02:24,224 --> 00:02:28,170
observability strategies, folks. So before

33
00:02:28,240 --> 00:02:32,418
we start talking about the strategies, see the things in action.

34
00:02:32,614 --> 00:02:35,866
Let's ensure that everybody's on the same page. Let's ensure

35
00:02:35,898 --> 00:02:40,122
that everybody has the same knowledge and the same understanding about observability

36
00:02:40,266 --> 00:02:43,586
and the use cases. Okay? I know this may be very

37
00:02:43,688 --> 00:02:47,858
basic, but this is very important to have the understanding about

38
00:02:47,944 --> 00:02:51,406
the strategies that we're going to see here. So folks,

39
00:02:51,518 --> 00:02:55,490
in the context of software engineers, observability is crucial

40
00:02:55,570 --> 00:02:59,270
for understanding how system is behaving. So given

41
00:02:59,340 --> 00:03:03,554
external inputs and what, I mean external inputs, I mean users

42
00:03:03,602 --> 00:03:07,158
using your system, like if you are running

43
00:03:07,244 --> 00:03:10,534
ecommerce, users buying, adding things to

44
00:03:10,572 --> 00:03:13,714
their checkout bag, doing payments

45
00:03:13,762 --> 00:03:17,418
and all those things, okay? And you start collecting telemetry data.

46
00:03:17,504 --> 00:03:20,746
So you start collecting traces and you start collecting logs,

47
00:03:20,778 --> 00:03:24,222
you start collecting metrics, profiling and many other things,

48
00:03:24,276 --> 00:03:27,966
okay. And getting all those information

49
00:03:28,068 --> 00:03:32,110
this huge amount of information, because observability is very

50
00:03:32,180 --> 00:03:35,554
easy to start handling a huge

51
00:03:35,592 --> 00:03:39,470
amount of data. You can identify

52
00:03:39,630 --> 00:03:43,314
issues, you can identify bot on acts on your system where

53
00:03:43,352 --> 00:03:47,858
your system can be improved, okay? And you can troubleshooting

54
00:03:47,954 --> 00:03:51,446
problems very quickly. But when

55
00:03:51,468 --> 00:03:55,510
we are talking about observability, it's very common. People start talking

56
00:03:55,580 --> 00:03:59,002
about infrastructure observability, which is true

57
00:03:59,056 --> 00:04:01,260
and which is very important actually,

58
00:04:01,710 --> 00:04:05,100
because if we are running a health

59
00:04:05,470 --> 00:04:08,954
infrastructure, your resilience is

60
00:04:08,992 --> 00:04:12,474
better, your reliability is better, and your customers are

61
00:04:12,512 --> 00:04:15,726
happy. And usually when you are talking

62
00:04:15,748 --> 00:04:20,074
about infrastructure observability, we are talking about monitoring kubernetes

63
00:04:20,122 --> 00:04:23,762
system, for example, if your system is scaling or not,

64
00:04:23,816 --> 00:04:27,026
if you have new pods or for example if

65
00:04:27,048 --> 00:04:30,606
you're running kafka broker,

66
00:04:30,718 --> 00:04:33,826
you may pay attention on disk size,

67
00:04:34,008 --> 00:04:36,360
disk throughput and many other things,

68
00:04:37,850 --> 00:04:41,542
which is important, as I said. But when you're talking

69
00:04:41,596 --> 00:04:45,910
about observability, we need to talk about also application observability,

70
00:04:46,970 --> 00:04:50,986
which is a little bit more complex because most of

71
00:04:51,008 --> 00:04:54,186
the things on the infrastructure side is

72
00:04:54,288 --> 00:04:57,338
made. We have metrics, we have logs by

73
00:04:57,344 --> 00:05:00,778
default because logs is the most common

74
00:05:00,944 --> 00:05:03,994
observability data type.

75
00:05:04,192 --> 00:05:07,910
But when you're talking about application observability is

76
00:05:07,920 --> 00:05:11,246
a little bit more complex because some systems is

77
00:05:11,268 --> 00:05:14,482
not prepared yet to export all the things that we need.

78
00:05:14,536 --> 00:05:18,386
Okay, because sometimes we

79
00:05:18,408 --> 00:05:22,046
need technical metrics

80
00:05:22,158 --> 00:05:25,426
or technical information like logs, traces and metrics.

81
00:05:25,458 --> 00:05:28,230
But from the application perspective,

82
00:05:28,810 --> 00:05:32,374
you want to know what is the p 99 of

83
00:05:32,412 --> 00:05:36,006
latency for a specific application. You may want to

84
00:05:36,028 --> 00:05:39,690
know how many messages this specific

85
00:05:39,760 --> 00:05:43,194
application is producing for your message broker. If you're

86
00:05:43,232 --> 00:05:45,850
using Kafka, RabbitMQ,

87
00:05:46,190 --> 00:05:49,834
anyone is the same concept more or

88
00:05:49,872 --> 00:05:53,598
less. And on the application

89
00:05:53,684 --> 00:05:57,118
side you may

90
00:05:57,204 --> 00:06:01,102
want to understand also some business metrics. So as

91
00:06:01,156 --> 00:06:05,038
given the ecommerce example that I gave you in the beginning,

92
00:06:05,134 --> 00:06:08,402
you may want to know how many orders your customers is doing

93
00:06:08,456 --> 00:06:11,794
by second, by minute, what is the

94
00:06:11,832 --> 00:06:16,040
click path on your system, how users navigate on your platform,

95
00:06:16,890 --> 00:06:20,550
and collecting all those information. You can start

96
00:06:20,620 --> 00:06:23,878
thinking about like places in your system that

97
00:06:24,044 --> 00:06:27,622
you want to add

98
00:06:27,676 --> 00:06:30,962
a lot of focus to improve resilience, to improve

99
00:06:31,026 --> 00:06:34,870
performance, to reduce or hate, and so on and so forth.

100
00:06:34,950 --> 00:06:39,418
Okay? But collecting all those information sometimes

101
00:06:39,584 --> 00:06:43,486
is not easier, especially if this is not build in your framework that

102
00:06:43,508 --> 00:06:47,342
you are using. And it lead us to a work that

103
00:06:47,396 --> 00:06:51,566
most of the engineers don't like to do even less

104
00:06:51,668 --> 00:06:55,774
the product manager, which is instrumenting code to get

105
00:06:55,812 --> 00:06:58,020
in the required information. Okay,

106
00:06:59,270 --> 00:07:02,546
so if the idea of this talk is to show some strategies to

107
00:07:02,568 --> 00:07:06,870
let engineers putting their focus on the things that really matters

108
00:07:07,210 --> 00:07:10,994
for them, like delivering features, measure user

109
00:07:11,042 --> 00:07:15,320
experience, getting business information.

110
00:07:15,930 --> 00:07:19,382
Let's avoid engineers spending time adding

111
00:07:19,446 --> 00:07:23,366
telemetry to collecting technical things like HTTP requests,

112
00:07:23,398 --> 00:07:24,730
Kafka, throughput,

113
00:07:26,750 --> 00:07:29,530
DB latence, and so on and so forth. Okay,

114
00:07:29,600 --> 00:07:32,974
so this is what we're going to show you today, how we

115
00:07:33,012 --> 00:07:36,842
can collecting standard metrics without acquiring

116
00:07:36,906 --> 00:07:41,422
engineers efforts. And this is useful to make

117
00:07:41,476 --> 00:07:45,522
some standard proxy across your company and ensure that doesn't matter

118
00:07:45,576 --> 00:07:48,754
the language, the language that you are writing your system, you are

119
00:07:48,792 --> 00:07:52,354
collecting the same kind of information using the

120
00:07:52,392 --> 00:07:56,130
same structure. Okay, pay cool

121
00:07:56,200 --> 00:07:59,902
folks. And we are talking about instrumenting

122
00:07:59,966 --> 00:08:03,362
code and instrumenting code. It's collect

123
00:08:03,426 --> 00:08:06,806
as much as we can. So as an engineer, you want

124
00:08:06,828 --> 00:08:10,714
to know every single information from

125
00:08:10,752 --> 00:08:14,060
your system because everything is kind of available,

126
00:08:15,630 --> 00:08:18,714
but do this kind of job can

127
00:08:18,912 --> 00:08:22,320
quickly become overwhelming for you and for your team because

128
00:08:23,010 --> 00:08:26,714
you need to use engineering

129
00:08:26,762 --> 00:08:29,998
time to add metrics that maybe

130
00:08:30,084 --> 00:08:34,078
might be provided by platform. If you

131
00:08:34,084 --> 00:08:37,586
have a platform team, for example, a DevOps team that can do

132
00:08:37,688 --> 00:08:41,218
all the automations and the strategies that

133
00:08:41,224 --> 00:08:45,380
you're going to see today on your company and may give you such

134
00:08:45,990 --> 00:08:49,634
information, and you can use your engineers

135
00:08:49,682 --> 00:08:53,346
time to do instrumentation to collect things that really matters

136
00:08:53,378 --> 00:08:57,238
for your system and for your product engineering and so on. And for

137
00:08:57,244 --> 00:09:01,126
example, we have a meme here. And of course, this is just a joke,

138
00:09:01,238 --> 00:09:04,586
but a funny one, because when

139
00:09:04,608 --> 00:09:07,994
you're talking to product engineers that we need to instrument our code,

140
00:09:08,032 --> 00:09:11,982
we need to spending engineering time instrumenting codes instead

141
00:09:12,036 --> 00:09:15,486
of delivering features, it automatically get a

142
00:09:15,508 --> 00:09:19,066
low priority. Okay? And as I said, this is just a joke,

143
00:09:19,098 --> 00:09:23,262
folks, because of course we are talking about two different professionals

144
00:09:23,326 --> 00:09:27,170
looking for two different perspectives for the same problem,

145
00:09:27,320 --> 00:09:30,866
okay? And when you are talking about

146
00:09:30,968 --> 00:09:34,690
when you are engineer, you're trying to push as

147
00:09:34,840 --> 00:09:38,870
much as you can, the better system to production you want to build,

148
00:09:38,940 --> 00:09:42,418
the more scalable, the more performance system. When you're

149
00:09:42,434 --> 00:09:45,974
a product engineers you want to push to production, the better product you

150
00:09:46,012 --> 00:09:49,514
can with the better features, the basics, user experience

151
00:09:49,632 --> 00:09:52,874
and so on and so forth. So this is

152
00:09:52,912 --> 00:09:56,762
a trade off. We need to talk to each other. And of course, if your

153
00:09:56,816 --> 00:10:01,034
platform team is providing you some

154
00:10:01,072 --> 00:10:05,054
basic information on the platform side, you just

155
00:10:05,092 --> 00:10:08,398
need to instrument your system, your code for the

156
00:10:08,404 --> 00:10:11,742
information that really matters for your product, for your teams,

157
00:10:11,796 --> 00:10:16,066
and you can use the information to

158
00:10:16,088 --> 00:10:20,482
improve your product. And your product engineers can use the same information because

159
00:10:20,536 --> 00:10:24,226
it's attack and business

160
00:10:24,328 --> 00:10:28,134
information and observability information so

161
00:10:28,252 --> 00:10:31,654
let's move forward and let's see what are the

162
00:10:31,772 --> 00:10:34,920
strategies that we're going to see today. Those are three.

163
00:10:35,530 --> 00:10:39,414
Okay, cool folks. So those are the strategies

164
00:10:39,462 --> 00:10:43,574
to not overload your engineer team, the proxy strategy, the open telemetry

165
00:10:43,622 --> 00:10:46,570
strategies, and the EBPF strategies.

166
00:10:47,390 --> 00:10:50,646
I have a blog post for every one of those

167
00:10:50,688 --> 00:10:54,798
strategies on my medium account. You can check this information

168
00:10:54,884 --> 00:10:58,254
there as well and feel free to reach me out and provide any

169
00:10:58,292 --> 00:11:02,302
feedback that you may want. And people,

170
00:11:02,436 --> 00:11:06,402
the idea of these strategies is to design and give

171
00:11:06,536 --> 00:11:10,798
to your engineering team a solid foundation

172
00:11:10,974 --> 00:11:15,026
of observability without having any code change.

173
00:11:15,128 --> 00:11:19,618
And what I mean by this is simple. You as an engineers,

174
00:11:19,794 --> 00:11:23,654
you as an engineer, when you want to deploy a service on your

175
00:11:23,692 --> 00:11:28,150
company infrastructure, you don't want to change your code to start collecting

176
00:11:29,230 --> 00:11:31,862
common metrics such as HTTP classes,

177
00:11:31,926 --> 00:11:34,598
GRPC streams,

178
00:11:34,694 --> 00:11:38,410
Kafka consumers and Kafka producers.

179
00:11:38,910 --> 00:11:43,162
You don't want to instrument your code to collecting latency

180
00:11:43,226 --> 00:11:46,762
metrics. You don't want to instrument your code to start collecting

181
00:11:46,826 --> 00:11:50,782
basic tracing information and

182
00:11:50,836 --> 00:11:54,494
so on, because you want to leverage your platform.

183
00:11:54,692 --> 00:11:58,240
You want to consume observability as a service.

184
00:11:58,790 --> 00:12:02,306
I like to say that because we are offering many

185
00:12:02,488 --> 00:12:05,700
services as a service, like CI as a service,

186
00:12:06,470 --> 00:12:09,874
kubernetes as a service, deployment as service, but you

187
00:12:09,912 --> 00:12:14,038
also want to have observability as a service. You want to deploy your system

188
00:12:14,124 --> 00:12:17,538
without any coaching and you want to start collecting

189
00:12:17,714 --> 00:12:23,066
telemetry data. Okay? So in

190
00:12:23,088 --> 00:12:27,206
the end, the idea of this talk is to provide some useful insights

191
00:12:27,238 --> 00:12:30,850
that you may use in separate each one or combine

192
00:12:30,950 --> 00:12:35,102
those strategies together to get the information that

193
00:12:35,236 --> 00:12:38,574
you may want. Okay? After I

194
00:12:38,612 --> 00:12:42,314
show you the live demo of each strategies,

195
00:12:42,362 --> 00:12:45,534
we are going to see table of comparison

196
00:12:45,662 --> 00:12:49,326
so we can compare each strategies,

197
00:12:49,358 --> 00:12:52,500
the benefits, the pros and cons of each one.

198
00:12:53,110 --> 00:12:56,434
And this may help you understand when you

199
00:12:56,472 --> 00:13:00,470
choose one and when you choose another one and so on.

200
00:13:00,620 --> 00:13:04,134
Okay, so folks, I hope you enjoy, like this is going to be

201
00:13:04,172 --> 00:13:08,022
very fun right now because it's live demo. You're going to see things

202
00:13:08,076 --> 00:13:11,446
in action or you're going to see open source solutions.

203
00:13:11,558 --> 00:13:15,494
And yeah, let's go, let's move forward and see the proxy strategy

204
00:13:15,542 --> 00:13:19,274
in action. Okay, cool folks. So let's go for

205
00:13:19,312 --> 00:13:22,990
the first strategies, the proxy strategy.

206
00:13:24,210 --> 00:13:27,882
We are going to leverage existing piece of infrastructure

207
00:13:27,946 --> 00:13:31,406
that you probably already have on your company, which is your

208
00:13:31,508 --> 00:13:35,374
web proxies, okay? If you're running HTTP

209
00:13:35,422 --> 00:13:39,250
applications, you probably have something like Nginx or Ha

210
00:13:39,320 --> 00:13:43,970
proxies, which are very common solutions

211
00:13:44,550 --> 00:13:48,974
when we need this kind of strategy.

212
00:13:49,022 --> 00:13:52,582
But this is not coupled for

213
00:13:52,636 --> 00:13:56,754
any technology. Okay, I'm going to use Nginx

214
00:13:56,802 --> 00:13:59,974
as an example just because I'm familiar with Nginx.

215
00:14:00,022 --> 00:14:03,754
But you can do the same with ATA proxy or any other web

216
00:14:03,792 --> 00:14:07,114
server that you know better. Okay?

217
00:14:07,312 --> 00:14:11,454
And the concept is the same. So as we can see

218
00:14:11,652 --> 00:14:15,962
on this slide, we have a diagram showing

219
00:14:16,026 --> 00:14:19,790
the flow. So we have an ingress proxy. The ingress proxy is

220
00:14:19,860 --> 00:14:23,714
responsible to handle the HTTP request that's coming

221
00:14:23,832 --> 00:14:27,954
from outside your platform to

222
00:14:27,992 --> 00:14:32,174
inside your platform. And then it's hedirecting

223
00:14:32,222 --> 00:14:35,354
to the proper service. So service a or service cb.

224
00:14:35,502 --> 00:14:39,202
And on the left side of the diagram

225
00:14:39,346 --> 00:14:42,550
we have the three telemetry

226
00:14:43,290 --> 00:14:47,362
backends, okay? We have Prometheus for metrics, yeager for traces,

227
00:14:47,426 --> 00:14:52,182
and open source for logs. And those backends

228
00:14:52,326 --> 00:14:56,234
are going to store the opentelemetry data produced by

229
00:14:56,272 --> 00:15:00,598
the ingress proxy. Okay, this is a very simple one.

230
00:15:00,784 --> 00:15:04,542
And the idea of this strategy is if you are using web

231
00:15:04,596 --> 00:15:08,830
applications and this strategy is very web

232
00:15:08,900 --> 00:15:12,398
specific, you can ensure that you are going

233
00:15:12,404 --> 00:15:16,434
to produce standard telemetry data like

234
00:15:16,632 --> 00:15:21,662
traces, metrics and logs independent

235
00:15:21,726 --> 00:15:25,874
of the technology that you are using to build your serve. So let's imagine

236
00:15:26,002 --> 00:15:31,014
that the Serfca is using Java and

237
00:15:31,052 --> 00:15:34,114
the Serfcb is using Golang.

238
00:15:34,242 --> 00:15:38,206
You can ensure that the opentelemetry data that we are collecting

239
00:15:38,338 --> 00:15:42,234
is using the same standard and doesn't care about the

240
00:15:42,272 --> 00:15:45,642
technology that the service is

241
00:15:45,696 --> 00:15:49,210
using. Okay? So let's move,

242
00:15:49,280 --> 00:15:52,974
let's move to the VS code and see the

243
00:15:53,012 --> 00:15:55,360
very simple strategy we have.

244
00:15:57,090 --> 00:16:01,280
So quick spoiler. First thing that you see here is

245
00:16:01,730 --> 00:16:04,918
a make file. I'm just using this to abstract a few combs

246
00:16:04,954 --> 00:16:07,460
and type a little bit less.

247
00:16:07,990 --> 00:16:11,666
But on the app folder we have a very simple

248
00:16:11,768 --> 00:16:16,666
Golang application where we are mimicking ecommerce

249
00:16:16,718 --> 00:16:20,178
checkout process. So when user is doing a checkout,

250
00:16:20,274 --> 00:16:23,606
the checkout service is going to call the payment service to do

251
00:16:23,628 --> 00:16:27,222
the user payment. Okay, very simple.

252
00:16:27,356 --> 00:16:30,346
And here you can see a few lines of go code,

253
00:16:30,448 --> 00:16:34,694
not important for us. And then we have a docker

254
00:16:34,742 --> 00:16:37,340
file, simple as well known especially.

255
00:16:38,990 --> 00:16:43,342
And then we have docker compose where we have a

256
00:16:43,476 --> 00:16:47,166
few containers running here. And I'm going

257
00:16:47,188 --> 00:16:51,006
to tell you about it a little bit.

258
00:16:51,188 --> 00:16:55,170
So first we have two proxy containers.

259
00:16:55,910 --> 00:16:59,890
The first one is the ingress proxy, the ones that I told you,

260
00:17:00,040 --> 00:17:03,860
it's handling the requests coming from outside your platform.

261
00:17:04,470 --> 00:17:08,338
And then we have the egress container

262
00:17:08,434 --> 00:17:11,874
which is acting as ambassador container.

263
00:17:11,922 --> 00:17:16,178
So which is handling the request that's

264
00:17:16,354 --> 00:17:19,818
going out from one service to another service.

265
00:17:19,904 --> 00:17:24,230
Okay. And having those containers,

266
00:17:24,310 --> 00:17:28,810
those two proxies, we can collecting

267
00:17:29,390 --> 00:17:32,630
and connect points between service a and service

268
00:17:32,720 --> 00:17:36,782
b with distributed traces. Okay. It's very similar what

269
00:17:36,836 --> 00:17:40,474
we have when we are using serfs mesh in kubernetes.

270
00:17:40,522 --> 00:17:44,210
Okay. We have like containers handling

271
00:17:44,710 --> 00:17:48,290
in front of every application to do this magic.

272
00:17:48,790 --> 00:17:51,906
Cool. Besides that we have checkout and the

273
00:17:51,928 --> 00:17:56,390
payment serves and then we have the exporter

274
00:17:58,650 --> 00:18:01,894
for both proxies. For the ingress and the

275
00:18:01,932 --> 00:18:05,906
ingress I'm using Prometheus exporter

276
00:18:05,938 --> 00:18:10,170
which is collecting, it's creating metrics using

277
00:18:10,240 --> 00:18:13,914
the NginX HTTP logs which

278
00:18:13,952 --> 00:18:17,386
are very useful and we already have a lot of information there. So we

279
00:18:17,408 --> 00:18:20,694
are getting the logs and creating metrics from the existing

280
00:18:20,742 --> 00:18:23,290
logs to understand latency,

281
00:18:23,870 --> 00:18:27,854
quests, hates and so on and so forth. And then we

282
00:18:27,892 --> 00:18:31,258
have Prometheus and also Yeager.

283
00:18:31,354 --> 00:18:35,780
Okay, so before we move to the next step folks, let me

284
00:18:36,710 --> 00:18:40,430
come back for the proxy configuration.

285
00:18:40,510 --> 00:18:44,926
And I would like to highlight that we are using a

286
00:18:44,968 --> 00:18:48,406
very specific image for this container which is

287
00:18:48,428 --> 00:18:52,294
the NgInX open tracing and this

288
00:18:52,332 --> 00:18:56,102
docker image already have all the required modules to

289
00:18:56,156 --> 00:18:59,974
starting spawns when a request is received and

290
00:19:00,012 --> 00:19:03,930
then export the spawns and the traces

291
00:19:05,630 --> 00:19:09,020
to the tracing backend in our case,

292
00:19:10,530 --> 00:19:14,222
Yeager. Okay, so an important thing

293
00:19:14,276 --> 00:19:17,774
here is I know that we already

294
00:19:17,812 --> 00:19:21,360
have opentelemetry version that was released very

295
00:19:23,110 --> 00:19:26,818
in this week. I didn't update this demo yet,

296
00:19:26,904 --> 00:19:30,290
but if you are starting doing this strategy right now,

297
00:19:30,440 --> 00:19:34,814
I really recommend you using the Opentelemetry version

298
00:19:34,862 --> 00:19:38,440
and not the open tracing. Okay cool.

299
00:19:38,970 --> 00:19:42,374
So for the ingress configuration we

300
00:19:42,412 --> 00:19:46,402
have something very simple. For the proxy configuration,

301
00:19:46,466 --> 00:19:50,522
it's like a forward proxy. We are just getting

302
00:19:50,576 --> 00:19:54,314
the request and forward this to the serves and

303
00:19:54,352 --> 00:19:57,926
we are just leveraging the proxy to collecting

304
00:19:57,958 --> 00:20:01,514
the opentelemetry data without needing

305
00:20:01,642 --> 00:20:05,198
to change anything on the code as

306
00:20:05,284 --> 00:20:08,974
we saw the go application. It's very simple and

307
00:20:09,012 --> 00:20:12,586
we don't have any instrumentation to

308
00:20:12,628 --> 00:20:16,180
collecting HTTP metrics. Okay cool.

309
00:20:16,630 --> 00:20:20,098
So we already know the basics, we already

310
00:20:20,264 --> 00:20:24,260
know all the things and then we can

311
00:20:24,630 --> 00:20:27,686
just start generating a

312
00:20:27,708 --> 00:20:31,606
few load on those services and see the things happening. So I'll be

313
00:20:31,628 --> 00:20:35,586
open the terminal, let's get the make file just to understand what's

314
00:20:35,618 --> 00:20:38,946
happening behind the scenes. And the first thing that I'm

315
00:20:38,978 --> 00:20:43,006
going to do is run make setup. Make setup

316
00:20:43,058 --> 00:20:46,330
is going to start all the containers using

317
00:20:46,400 --> 00:20:50,026
Docker compose up. And also I have all

318
00:20:50,048 --> 00:20:53,662
the containers up and running. As we can see here.

319
00:20:53,716 --> 00:20:57,022
All those logs is here are here we can

320
00:20:57,076 --> 00:21:01,150
go to the web browser. Let me switch to the web browser

321
00:21:01,730 --> 00:21:06,000
and oh actually let me just fix

322
00:21:06,770 --> 00:21:10,386
for the seminar nokijit. And now on the

323
00:21:10,408 --> 00:21:14,274
web browser we can access localhost nine

324
00:21:14,312 --> 00:21:18,550
it's nine it which is Prometheus web

325
00:21:18,620 --> 00:21:22,914
interface. And we can see the Prometheus targets

326
00:21:22,962 --> 00:21:26,674
here, the two Nginx products, the ingress and the grass

327
00:21:26,722 --> 00:21:28,940
and Prometheus itself. Okay,

328
00:21:30,270 --> 00:21:33,660
so backing to the vs code,

329
00:21:34,270 --> 00:21:38,010
let me see which is the port

330
00:21:38,160 --> 00:21:42,080
that Yeager is running because I don't know by heart

331
00:21:42,450 --> 00:21:45,680
and it's 6006. Eight six.

332
00:21:46,050 --> 00:21:49,326
And then we are going to move back to the

333
00:21:49,348 --> 00:21:53,074
browser and we are going to

334
00:21:53,192 --> 00:21:56,420
access localhost and then the Yeager port.

335
00:21:57,110 --> 00:22:00,690
So we don't have anything here. Okay. The only service

336
00:22:00,760 --> 00:22:04,546
we have is Yeager itself because Yeager is collecting its

337
00:22:04,728 --> 00:22:08,182
outer traces and

338
00:22:08,236 --> 00:22:11,622
now we have all those things running as

339
00:22:11,676 --> 00:22:15,314
expected. We can start producing a few loads

340
00:22:15,362 --> 00:22:19,226
to this infrastructure. So let's go back

341
00:22:19,408 --> 00:22:22,954
to vs code and then

342
00:22:23,072 --> 00:22:26,746
we are going to open a new terminal and using a

343
00:22:26,768 --> 00:22:29,938
make comment we have here which is maketest.

344
00:22:30,134 --> 00:22:34,378
What maketest is doing is just creating a few load

345
00:22:34,474 --> 00:22:38,046
on the checkout serves using Vegeta. Vegeta is

346
00:22:38,068 --> 00:22:40,670
a very simple loading test in Cli.

347
00:22:41,330 --> 00:22:44,930
I think this is just amazing for this kind of workload. Okay.

348
00:22:45,080 --> 00:22:48,594
And I'm going to run load test against

349
00:22:48,712 --> 00:22:52,318
the checkout API during 6 seconds.

350
00:22:52,414 --> 00:22:54,500
Okay. So let's go.

351
00:22:55,370 --> 00:22:58,546
And it's producing, it's making a lot of HTTP requests.

352
00:22:58,578 --> 00:23:02,246
So if we can go to the logs, we may see a

353
00:23:02,268 --> 00:23:06,166
few logs happening here and

354
00:23:06,268 --> 00:23:11,098
I think we may already have a few data

355
00:23:11,184 --> 00:23:14,410
available. So let's go back to browser.

356
00:23:14,750 --> 00:23:19,034
And then in the browser the first thing that we're going to see is,

357
00:23:19,152 --> 00:23:22,442
are the metrics that we are collecting. So we have a few

358
00:23:22,496 --> 00:23:26,178
metrics, name it Nginx. Let me hit fresh

359
00:23:26,214 --> 00:23:29,678
because the metrics might not be available. Yeah, we already

360
00:23:29,764 --> 00:23:32,950
have it. And then we have the NgInX

361
00:23:33,050 --> 00:23:36,260
HTTP request count total.

362
00:23:37,030 --> 00:23:40,498
So we have few things

363
00:23:40,584 --> 00:23:43,634
here, we can see all

364
00:23:43,672 --> 00:23:47,846
those things. And then if we run an expression like

365
00:23:47,948 --> 00:23:52,774
hate HTTP request and

366
00:23:52,972 --> 00:23:56,102
sum this by, I don't know,

367
00:23:56,236 --> 00:24:00,070
serfs, URi and stats

368
00:24:00,150 --> 00:24:04,762
code we can see all

369
00:24:04,816 --> 00:24:08,202
those things. And then

370
00:24:08,336 --> 00:24:12,160
we can see this increasing over time,

371
00:24:12,690 --> 00:24:16,266
which is very cool. And since we are doing linear requests

372
00:24:16,298 --> 00:24:19,630
we are not going to see these ups going up and down.

373
00:24:19,780 --> 00:24:24,306
But here we can already know the information

374
00:24:24,488 --> 00:24:28,050
that we need. Okay. We can measure

375
00:24:28,470 --> 00:24:31,618
the amount of HTTP requests for a given service

376
00:24:31,784 --> 00:24:34,500
and that's cool. That's pretty cool.

377
00:24:35,270 --> 00:24:38,926
And using that information we can see the amount of HTTP

378
00:24:38,958 --> 00:24:42,854
requests we can use in these two building slos, for example,

379
00:24:42,972 --> 00:24:46,470
like ahorhates, we can use that information

380
00:24:46,540 --> 00:24:50,026
to measure HTTP responses because we should have

381
00:24:50,128 --> 00:24:54,138
probably, I'm not finding right now,

382
00:24:54,224 --> 00:24:58,970
but we should have a few metrics about latency.

383
00:24:59,310 --> 00:25:03,166
But we may know which is for

384
00:25:03,188 --> 00:25:07,834
example the response

385
00:25:07,962 --> 00:25:11,422
size for each class, which is also

386
00:25:11,476 --> 00:25:14,834
very useful information. And using the

387
00:25:14,872 --> 00:25:18,158
NgInx exporter, you can build any metric

388
00:25:18,254 --> 00:25:21,486
you want using the NgInX logs.

389
00:25:21,518 --> 00:25:25,226
Okay. So you can get in the logs and you can build all the required

390
00:25:25,278 --> 00:25:28,530
metrics that you may need. Those are just a few examples.

391
00:25:28,610 --> 00:25:32,006
And as I said, those are

392
00:25:32,108 --> 00:25:35,686
standard metrics. Doesn't matter which technology you are using

393
00:25:35,788 --> 00:25:39,894
behind the scenes. The next telemetry

394
00:25:39,942 --> 00:25:43,706
data that we can see are the

395
00:25:43,888 --> 00:25:47,434
distributed tracings, okay. And then we can see that

396
00:25:47,472 --> 00:25:51,214
we have two services here. The first one is the

397
00:25:51,252 --> 00:25:55,466
checkout. And if we find traces from checkout

398
00:25:55,658 --> 00:25:59,630
services, we can see that the checkout

399
00:25:59,970 --> 00:26:03,834
go to payments and so on.

400
00:26:03,892 --> 00:26:08,114
So we have two hope, we have two hopes for every service.

401
00:26:08,232 --> 00:26:11,058
This is Nginx internal, okay.

402
00:26:11,224 --> 00:26:14,514
And we can see in

403
00:26:14,552 --> 00:26:18,342
that way we can look in for system architecture and we can

404
00:26:18,396 --> 00:26:19,800
see for example,

405
00:26:22,970 --> 00:26:26,918
the service is not zooming again. Let me see if it's better on the service.

406
00:26:27,004 --> 00:26:30,394
Yeah, we can see the checkouts. It's using the

407
00:26:30,432 --> 00:26:34,122
payments API. And if you have many services, you'll be able to see

408
00:26:34,256 --> 00:26:36,970
this diagram architecture,

409
00:26:37,870 --> 00:26:41,818
which is nice because if you are using

410
00:26:41,904 --> 00:26:47,498
a microservices solution, whether you have many microservices

411
00:26:47,594 --> 00:26:51,422
talking to each other, it's very hard to know only by

412
00:26:51,476 --> 00:26:55,502
knowledge what are the service communication

413
00:26:55,566 --> 00:26:59,074
flow. Okay. One thing that's very

414
00:26:59,112 --> 00:27:02,722
important, those distributed information are

415
00:27:02,776 --> 00:27:06,102
useful, but it doesn't give much detail about

416
00:27:06,156 --> 00:27:10,514
the service internally. So you cannot identify performance

417
00:27:10,562 --> 00:27:14,360
issues inside your services using that information. Okay.

418
00:27:14,730 --> 00:27:19,914
This is useful to understand the network hopes on

419
00:27:19,952 --> 00:27:23,222
your platform, but not useful

420
00:27:23,286 --> 00:27:26,634
to identify internal problems. Okay.

421
00:27:26,752 --> 00:27:30,494
But giving that information, giving that telemetry data,

422
00:27:30,612 --> 00:27:34,398
your teams can start looking and see, okay, we are talking to these herbs and

423
00:27:34,404 --> 00:27:37,934
these herbs and then they can see,

424
00:27:37,972 --> 00:27:42,250
okay, on this payment flow,

425
00:27:42,330 --> 00:27:45,906
I want to improve the details. And then

426
00:27:45,928 --> 00:27:50,222
the team can go and add telemetry for the flow and the path

427
00:27:50,286 --> 00:27:53,966
they really want to know. Okay. And they can reduce

428
00:27:53,998 --> 00:27:56,280
the amount of work they need to do.

429
00:27:56,730 --> 00:28:01,106
Cool. So this is the proxy strategy,

430
00:28:01,218 --> 00:28:04,582
folks. This is very simple. As I said,

431
00:28:04,636 --> 00:28:08,840
the idea is not like any rocket science. Just using

432
00:28:09,450 --> 00:28:13,290
a piece of infrastructure steps that you already have on your company

433
00:28:13,440 --> 00:28:16,010
and start collecting a few telemetry date.

434
00:28:16,080 --> 00:28:19,974
Okay, so before we finish just talking about

435
00:28:20,032 --> 00:28:27,054
the logs, logs are available on

436
00:28:27,092 --> 00:28:31,178
your host. So for example, you can see all the logs produced

437
00:28:31,274 --> 00:28:35,150
here. We are producing a lot because we are generating

438
00:28:35,230 --> 00:28:37,570
a bunch of requests.

439
00:28:38,070 --> 00:28:40,740
We don't have any errors in that case,

440
00:28:41,590 --> 00:28:45,414
but we could mimic some errors, for example, and also we

441
00:28:45,452 --> 00:28:49,414
have this, those access logs. You can use

442
00:28:49,532 --> 00:28:53,154
some log sheeper like Opentelemetry fluent

443
00:28:53,202 --> 00:28:56,486
beat to collecting those logs and ship to

444
00:28:56,588 --> 00:29:00,086
open source solution. And my advice

445
00:29:00,198 --> 00:29:03,322
is since most of the information you

446
00:29:03,376 --> 00:29:07,654
have on the logs are now available on metrics

447
00:29:07,702 --> 00:29:10,910
like stats code path,

448
00:29:11,570 --> 00:29:15,578
you can just choose to drop

449
00:29:15,674 --> 00:29:19,520
some of those logs, especially the 200

450
00:29:19,890 --> 00:29:23,762
stats codes. Okay, this is just another device that might save

451
00:29:23,816 --> 00:29:28,270
you some money in terms of storage and also networking.

452
00:29:28,430 --> 00:29:30,850
So this is the first strategies,

453
00:29:31,350 --> 00:29:34,718
we are going to move to the next one now,

454
00:29:34,904 --> 00:29:38,834
which is the open telemetry strategy. So let's

455
00:29:38,882 --> 00:29:42,086
go. Okay, cool folks, this is the

456
00:29:42,188 --> 00:29:45,554
second strategy, the opentelemetry strategies.

457
00:29:45,602 --> 00:29:49,850
And in my opinion this is the coolest one because

458
00:29:50,000 --> 00:29:53,706
this is using the Opentelemetry, which is an

459
00:29:53,728 --> 00:29:57,146
amazing project maintained by amazing people,

460
00:29:57,328 --> 00:30:00,750
offering a lot of integration and

461
00:30:00,900 --> 00:30:03,680
wow, this is very interesting. Okay,

462
00:30:04,450 --> 00:30:08,302
and this strategies folks also aims to rely in the

463
00:30:08,356 --> 00:30:13,394
infrastructure piece that you will deploy on your company

464
00:30:13,512 --> 00:30:17,554
and it will start collecting metrics and traces out

465
00:30:17,592 --> 00:30:21,186
of the box for you. Okay, so the Opentelemetry project,

466
00:30:21,288 --> 00:30:25,334
it's a huge project composed by many different parts such

467
00:30:25,372 --> 00:30:29,190
as the open telemetry specification, the opentelemetry collector,

468
00:30:29,770 --> 00:30:34,242
also the opentelemetry EBPF

469
00:30:34,306 --> 00:30:37,986
out instrumentation operator and so on and so forth.

470
00:30:38,098 --> 00:30:42,694
We are going to using today the open telemetry, open instrumentation and the open telemetry

471
00:30:42,742 --> 00:30:46,060
collector to

472
00:30:46,510 --> 00:30:50,366
generating, auto generating and collecting the

473
00:30:50,388 --> 00:30:53,822
traces and generating the metrics. Okay, so looking

474
00:30:53,876 --> 00:30:57,406
for the diagram, we have an auto agent, opentelemetry agent

475
00:30:57,508 --> 00:31:01,680
running your host and receiving the

476
00:31:02,050 --> 00:31:05,620
tracings that will be produced by your application.

477
00:31:06,230 --> 00:31:11,154
The opentelemetry agent is going to process the

478
00:31:11,192 --> 00:31:14,462
traces and creating metrics and then expose

479
00:31:14,526 --> 00:31:17,922
metrics and traces to the back end

480
00:31:17,976 --> 00:31:21,170
for metrics to promote traces to Yeager,

481
00:31:22,250 --> 00:31:26,114
it's like kind of language agnostic. So it doesn't matter which language

482
00:31:26,162 --> 00:31:30,294
you're using, if you're using Python, if you're using Java

483
00:31:30,342 --> 00:31:34,042
and so on and so forth. Okay, so let's move to the

484
00:31:34,096 --> 00:31:37,834
vs code. On vs code we have

485
00:31:38,032 --> 00:31:41,614
almost the same thing. Before I

486
00:31:41,652 --> 00:31:45,402
show you the solution, I'm just going to run make setup

487
00:31:45,466 --> 00:31:49,434
to ensure that I have everything running and I will start products

488
00:31:49,562 --> 00:31:52,926
a few loads on the

489
00:31:52,948 --> 00:31:56,322
seRps. It's using the same vegeta comment that I showed you

490
00:31:56,376 --> 00:31:59,074
on the previous demo.

491
00:31:59,272 --> 00:32:03,586
So we will looking for the configs we have here.

492
00:32:03,768 --> 00:32:07,650
So first I have the app folder. On the app folder

493
00:32:07,730 --> 00:32:11,554
we have a very simple python application mimicking

494
00:32:11,602 --> 00:32:14,822
the same behavior, checkouts and payments doing the same

495
00:32:14,876 --> 00:32:18,762
flow. Okay? And folks, as you can see

496
00:32:18,816 --> 00:32:22,554
here, we don't have any open telemetry or Prometheus or

497
00:32:22,592 --> 00:32:26,374
Jaeger code. We are not creating traces, we are not creating

498
00:32:26,422 --> 00:32:29,900
metrics. This is a plain and standard

499
00:32:30,670 --> 00:32:34,366
flask Python server. Okay, this is just to

500
00:32:34,388 --> 00:32:37,950
bear in mind all the things that you're going to see. It's auto generated.

501
00:32:38,610 --> 00:32:41,806
So looking to the pipe to the docker file, this is

502
00:32:41,828 --> 00:32:45,506
where the magic starts to happen, because we have to

503
00:32:45,528 --> 00:32:51,266
install a few python packages on

504
00:32:51,288 --> 00:32:55,038
the host, which has a dependence like the opentelemetry distro,

505
00:32:55,134 --> 00:32:58,434
the exporter of the opentelemetry format.

506
00:32:58,482 --> 00:33:02,006
And also we need to run a few bootstrap comments to

507
00:33:02,108 --> 00:33:06,038
configure everything that we need on the host. And then

508
00:33:06,204 --> 00:33:09,914
we have the opentelemetry instrument, some rounds in the

509
00:33:09,952 --> 00:33:13,562
Python command. It's like running

510
00:33:13,616 --> 00:33:17,354
the opentelemetry instruments. And this is starting a process of

511
00:33:17,392 --> 00:33:20,682
Python when we are doing these folks.

512
00:33:20,746 --> 00:33:24,606
And this is where the magic starts to happening because the

513
00:33:24,628 --> 00:33:27,982
out instrumentation project of opentelemetry is

514
00:33:28,036 --> 00:33:31,774
doing the same thing that the

515
00:33:31,972 --> 00:33:35,642
vendor agent is doing. It's changing your codes

516
00:33:35,786 --> 00:33:40,180
in Python casing during hung time to

517
00:33:40,630 --> 00:33:44,018
adding opentelemetry code. So the

518
00:33:44,104 --> 00:33:48,086
opentelemetry out instrumentation is gamechanging the Python code to

519
00:33:48,268 --> 00:33:52,322
initialize tracing context and ensure that we are propagating

520
00:33:52,386 --> 00:33:56,354
the tracing headers when we are doing HTTP requests, when you're producing

521
00:33:56,402 --> 00:34:00,438
a Kafka message, reading the trace context when we are receiving

522
00:34:00,454 --> 00:34:03,834
a request or consuming a Kafka message, and those are just

523
00:34:03,872 --> 00:34:07,254
example, this is doing a lot of things behind the scenes.

524
00:34:07,302 --> 00:34:11,310
Okay, cool. The traces are being produced

525
00:34:11,650 --> 00:34:15,406
by this application will be sent

526
00:34:15,508 --> 00:34:19,054
by the opentelemetry collector. Before I

527
00:34:19,092 --> 00:34:23,594
show you the opentelemetry collector configuration

528
00:34:23,642 --> 00:34:26,738
to you, let me show you the containers. We have running

529
00:34:26,824 --> 00:34:31,170
side the Docker compose. We have Prometheus and then we have Yeager,

530
00:34:31,750 --> 00:34:35,314
and then we have the open telemetry. And then we

531
00:34:35,352 --> 00:34:39,286
have the services containers where we have the checkouts API and

532
00:34:39,308 --> 00:34:42,582
also the payments. As you can see, I have for

533
00:34:42,636 --> 00:34:46,530
both containers a few environment variables defined.

534
00:34:46,610 --> 00:34:49,990
We have the opentelemetry traces exporter,

535
00:34:51,130 --> 00:34:54,654
which is going to be the OTLP.

536
00:34:54,722 --> 00:34:58,170
What is the format that I'm going to export traces,

537
00:34:58,510 --> 00:35:02,622
which is the serfs name, in this case the checkout. And what is

538
00:35:02,676 --> 00:35:05,982
the opentelemetry collector inch point where

539
00:35:06,036 --> 00:35:09,786
this application should push traces, which is hotel

540
00:35:09,898 --> 00:35:13,150
and on the port 43, 117. Okay,

541
00:35:13,220 --> 00:35:16,746
hotel is the container. Running the open telemetry

542
00:35:16,778 --> 00:35:20,546
collector. Cool. Pretty simple. Moving to the open

543
00:35:20,568 --> 00:35:24,606
telemetry collector we have the open telemetry

544
00:35:24,638 --> 00:35:28,402
collector. And the open telemetry collector is a piece of software

545
00:35:28,466 --> 00:35:32,120
responsible to receive opentelemetry data,

546
00:35:32,730 --> 00:35:36,534
process the opentelemetry data and then export the

547
00:35:36,572 --> 00:35:40,582
opentelemetry data. This is literally

548
00:35:40,726 --> 00:35:44,230
a software pipeline. Okay, so you can receive,

549
00:35:44,310 --> 00:35:47,994
process and export. And we can

550
00:35:48,032 --> 00:35:51,382
see we have a pipeline section on the

551
00:35:51,456 --> 00:35:55,630
opentelemetry code collector config. And we have two pipelines running here.

552
00:35:55,700 --> 00:35:59,722
The first one is the receiver, which is the OTLP.

553
00:35:59,786 --> 00:36:04,702
So the application is producing the traces for this

554
00:36:04,756 --> 00:36:08,734
receiver. And then we have a batch processing. We are just

555
00:36:08,852 --> 00:36:12,190
batching the spans and exporting those

556
00:36:12,260 --> 00:36:15,794
spans in batch to the back end which is export

557
00:36:15,842 --> 00:36:19,766
another OtlEp and then the OTLP it's sending to

558
00:36:19,788 --> 00:36:23,446
the Jaeger end point. And on the

559
00:36:23,468 --> 00:36:26,610
exporters we are also sending to something named

560
00:36:26,690 --> 00:36:30,246
Spun matrix. And what is spun

561
00:36:30,278 --> 00:36:33,526
matrix? Spun matrix is a connector.

562
00:36:33,638 --> 00:36:37,382
And connector is a part of opentelemetry

563
00:36:37,446 --> 00:36:40,880
collector that acts as

564
00:36:41,250 --> 00:36:45,360
a receiver and also exporter. So it's like

565
00:36:45,730 --> 00:36:49,710
literally a connector, it can receive and export

566
00:36:51,250 --> 00:36:55,074
on the same time. Okay, so for every

567
00:36:55,192 --> 00:36:58,494
spun we are exporting

568
00:36:58,542 --> 00:37:02,398
we are sending to the spun matrix. The spun matrix is receiving

569
00:37:02,494 --> 00:37:05,906
the spuns. Processing those spuns is

570
00:37:06,008 --> 00:37:09,714
basically creating metrics from

571
00:37:09,752 --> 00:37:13,080
the spuns. And later we are using

572
00:37:13,530 --> 00:37:17,202
another pipeline named matrix, spun matrix.

573
00:37:17,266 --> 00:37:20,922
And we are getting data from the spun matrix since we can

574
00:37:21,056 --> 00:37:24,730
receive and export data. And then we are getting

575
00:37:24,800 --> 00:37:28,694
from metrics from spun matrix and we are exporting to Prometheus

576
00:37:28,742 --> 00:37:32,582
and vote write which is remote writing to our Prometheus server.

577
00:37:32,646 --> 00:37:35,530
Okay, this is basically that folks.

578
00:37:35,690 --> 00:37:39,402
And the part of code is responsible

579
00:37:39,466 --> 00:37:44,450
to creating metrics from traces is the spam matrix connector.

580
00:37:45,270 --> 00:37:48,690
So the open telemetry out instrumentation

581
00:37:50,070 --> 00:37:53,154
on the docker image is gamechanging the python code.

582
00:37:53,272 --> 00:37:56,674
To initialize the tracing context and ensure

583
00:37:56,722 --> 00:38:00,194
that we are propagating headers and reading headers. The collector

584
00:38:00,242 --> 00:38:03,714
is receiving processing and export to the metrics.

585
00:38:03,762 --> 00:38:06,658
Okay, just one note.

586
00:38:06,834 --> 00:38:10,434
The way that I did on the docker is the simplest

587
00:38:10,482 --> 00:38:13,978
way that you can do to have out instrumentation running.

588
00:38:14,064 --> 00:38:16,330
Okay, there is another approaches.

589
00:38:17,390 --> 00:38:22,414
If you are running kubernetes you can use an open

590
00:38:22,452 --> 00:38:26,458
telemetry operator and you can inject sidecars

591
00:38:26,634 --> 00:38:30,080
on your pods based on

592
00:38:31,810 --> 00:38:35,034
your technology. So if you're using Java,

593
00:38:35,082 --> 00:38:38,666
you just need to notate your pods with Java

594
00:38:38,778 --> 00:38:42,386
notation, open telemetry, Java notation and opentelemetry collector is

595
00:38:42,408 --> 00:38:46,210
going to do all the magic to you. The opentelemetry operator.

596
00:38:46,290 --> 00:38:49,510
Okay, but I'm not show you this today

597
00:38:49,580 --> 00:38:53,334
because it's a little bit more complex. So we

598
00:38:53,372 --> 00:38:55,320
already have this running for a while.

599
00:38:57,710 --> 00:39:01,210
Let me see, probably for a few

600
00:39:01,280 --> 00:39:03,260
times from now.

601
00:39:05,230 --> 00:39:09,206
Let me just ensure one thing. I guess I forget

602
00:39:09,318 --> 00:39:11,600
to run make test.

603
00:39:12,610 --> 00:39:15,870
Yeah, but let running and let's switch to browser.

604
00:39:16,210 --> 00:39:19,902
Now we are on the browser. We can go

605
00:39:20,036 --> 00:39:23,586
first on the Yeager view and we

606
00:39:23,608 --> 00:39:27,198
can see the traces from the services. We have checkout

607
00:39:27,294 --> 00:39:30,820
and payments. If we look to

608
00:39:31,270 --> 00:39:34,754
one tracing in specific, we can see that

609
00:39:34,792 --> 00:39:38,246
we have three spans. We have the first spawn which is

610
00:39:38,268 --> 00:39:42,178
the checkout. So when the checkout service receives

611
00:39:42,354 --> 00:39:46,018
the request and then we have a checkout

612
00:39:46,114 --> 00:39:49,482
action. Get where the checkout service

613
00:39:49,536 --> 00:39:52,954
is doing an HTTP request to the payment service and

614
00:39:52,992 --> 00:39:58,310
then we have the payment service receiving

615
00:39:58,390 --> 00:40:01,470
the request. Since the payment service is doing nothing,

616
00:40:01,540 --> 00:40:05,306
we don't have any kind of continuation

617
00:40:05,418 --> 00:40:08,526
spun here. But if we're consuming a Kafka master job,

618
00:40:08,628 --> 00:40:12,414
another thing, we're going to see this spun as

619
00:40:12,452 --> 00:40:16,406
well. Okay, we can look to those spuns

620
00:40:16,458 --> 00:40:20,578
and see that we have a few useful information like user agent.

621
00:40:20,664 --> 00:40:24,546
In this case vegeta is written in go.

622
00:40:24,648 --> 00:40:28,594
So the user agent is go client. We have, which is the host

623
00:40:28,642 --> 00:40:30,470
port, the prip,

624
00:40:32,250 --> 00:40:36,086
the opentelemetry library name. We also have a few process

625
00:40:36,188 --> 00:40:39,610
tags such as the SDK version.

626
00:40:41,390 --> 00:40:44,630
It's also showing auto version, auto instrumentation

627
00:40:44,710 --> 00:40:48,154
version and the same is true for

628
00:40:48,272 --> 00:40:51,994
payment serves. The difference here is the user agent is

629
00:40:52,032 --> 00:40:55,274
Python because checkout serves is a Python server

630
00:40:55,322 --> 00:40:58,478
and not goling. Okay, cool.

631
00:40:58,644 --> 00:41:02,614
As we can see on the open telemetry strategy

632
00:41:02,682 --> 00:41:07,010
we have more details about the service internals.

633
00:41:07,670 --> 00:41:11,614
The thing that we didn't have on the proxy strategy,

634
00:41:11,662 --> 00:41:15,714
because we are collecting on the proxy strategy, we are collecting opentelemetry

635
00:41:15,762 --> 00:41:18,520
data from the layer above. Okay,

636
00:41:19,450 --> 00:41:22,694
well we also have the system architecture as we can

637
00:41:22,732 --> 00:41:27,526
see here, very useful as on

638
00:41:27,548 --> 00:41:31,046
the other demo. But since we are using

639
00:41:31,148 --> 00:41:34,502
Opentelemetry in this puns metrics and we are creating

640
00:41:34,566 --> 00:41:38,918
metrics from traces, we can leverage another Yeager feature,

641
00:41:39,014 --> 00:41:42,430
which is the monitor feature. We can just

642
00:41:42,500 --> 00:41:45,678
see an APM view like by

643
00:41:45,764 --> 00:41:50,174
service and operation. Under these serves we can see

644
00:41:50,372 --> 00:41:53,894
the request hate the P 99 matters,

645
00:41:53,962 --> 00:41:57,794
we can see the action and we can also

646
00:41:57,912 --> 00:42:01,266
see the impact of this action on the services.

647
00:42:01,368 --> 00:42:05,218
So this is related. So if this

648
00:42:05,304 --> 00:42:07,700
action is most used or less used,

649
00:42:08,650 --> 00:42:11,906
this is very nice and very useful for quick and troubleshoot.

650
00:42:11,938 --> 00:42:15,318
If you want to see this on your Yeager page.

651
00:42:15,404 --> 00:42:19,050
Yeager is reading data from Prometheus to build

652
00:42:19,120 --> 00:42:21,994
this screen, which is kind of nice. Okay,

653
00:42:22,192 --> 00:42:24,250
so moving to Prometheus,

654
00:42:25,070 --> 00:42:29,546
and if we go to Prometheus on the 99th part,

655
00:42:29,728 --> 00:42:33,454
we have two metrics here. The first one is the cost,

656
00:42:33,652 --> 00:42:37,850
which is basically a hate of actions.

657
00:42:37,930 --> 00:42:41,838
Okay, in this case, since we are using an HTTP request, this is

658
00:42:41,844 --> 00:42:45,700
a hate of HTTP request. And then we can

659
00:42:46,070 --> 00:42:50,260
run Hm five minutes by

660
00:42:50,950 --> 00:42:54,706
HTTP metal stats code and then service

661
00:42:54,808 --> 00:42:58,982
name and we can see this going up.

662
00:42:59,116 --> 00:43:03,000
Okay, let me just reduce this a bit.

663
00:43:03,690 --> 00:43:07,014
Well, this is it. So if we are

664
00:43:07,052 --> 00:43:11,114
doing Kafka producers, we're going to see the

665
00:43:11,152 --> 00:43:15,020
same thing, Kafka consumer, the same thing

666
00:43:15,950 --> 00:43:19,258
under the calls. So it's important

667
00:43:19,344 --> 00:43:22,494
to understand this kind of operation. We can

668
00:43:22,532 --> 00:43:26,558
also of course have in the span name,

669
00:43:26,644 --> 00:43:30,874
for example, to help you understanding what is the action

670
00:43:31,002 --> 00:43:34,754
is being executed on this part

671
00:43:34,792 --> 00:43:37,460
of code. But it's basically that, okay,

672
00:43:38,070 --> 00:43:42,206
another useful metric we have is the buck duration bucket

673
00:43:42,318 --> 00:43:46,070
where we can measure p 99,

674
00:43:46,140 --> 00:43:49,974
p 95 as we saw on the

675
00:43:50,012 --> 00:43:53,702
Yeager screen. So let's do sun

676
00:43:53,836 --> 00:43:57,490
by Le and service name,

677
00:43:57,660 --> 00:44:02,460
and we are going to use a histogram quantile p 99

678
00:44:02,910 --> 00:44:06,858
for these. And we can see like

679
00:44:06,944 --> 00:44:10,506
last five minutes, the p 99 for

680
00:44:10,528 --> 00:44:14,426
the checkout service is around eight milliseconds and for

681
00:44:14,448 --> 00:44:17,598
the payment service is around one millisecond and a

682
00:44:17,604 --> 00:44:22,042
half. Okay, well those are very useful metrics.

683
00:44:22,186 --> 00:44:25,474
Again, we are doing all those things without any

684
00:44:25,512 --> 00:44:29,154
code change. We are just deploying an agent on

685
00:44:29,192 --> 00:44:32,978
the host or changing the docker image. So this is very

686
00:44:33,064 --> 00:44:37,382
simple to be executed by someone from

687
00:44:37,436 --> 00:44:41,318
your platform team and which is nice. We are

688
00:44:41,404 --> 00:44:44,914
ensure that those metrics are being produced

689
00:44:45,042 --> 00:44:48,890
using open standard, which is the opentelemetry specification.

690
00:44:49,550 --> 00:44:54,090
Most of the vendors are supporting opentelemetry

691
00:44:54,990 --> 00:44:58,874
data. Open source solutions are supporting open

692
00:44:58,912 --> 00:45:02,366
telemetry data as well. And if you are

693
00:45:02,388 --> 00:45:05,754
using a vendor which is not supporting open telemetry format,

694
00:45:05,802 --> 00:45:09,038
I do recommend you move away from this vendor because

695
00:45:09,204 --> 00:45:15,410
it's not good for you. Keep using proprietary

696
00:45:16,150 --> 00:45:19,474
opentelemetry date, okay, which is nice as

697
00:45:19,512 --> 00:45:23,026
well, because doesn't matter which technology you are using to

698
00:45:23,048 --> 00:45:27,934
build your service, if you're using python, if you're using Golang,

699
00:45:27,982 --> 00:45:31,222
if you're using Java, all those metrics will have

700
00:45:31,276 --> 00:45:34,502
the same standard, the same labels and so on.

701
00:45:34,556 --> 00:45:38,614
So this is very useful, especially if you want to build dynamic dashboards,

702
00:45:38,742 --> 00:45:41,738
dynamic slos and so on.

703
00:45:41,824 --> 00:45:45,500
Okay folks, so I hope you enjoy

704
00:45:46,110 --> 00:45:49,820
this demo. For me, this is one of the coolest one.

705
00:45:50,450 --> 00:45:54,618
And then we are now switching to the 31, to the EBPF

706
00:45:54,714 --> 00:45:58,734
strategies. All right folks, so this is the

707
00:45:58,772 --> 00:46:02,234
30 observability, observability strategies to not

708
00:46:02,282 --> 00:46:06,278
overload engineering teams. And this strategy

709
00:46:06,314 --> 00:46:09,842
is like very interesting

710
00:46:09,976 --> 00:46:13,902
because it's using EBPF. So EBPF is an emerging

711
00:46:13,966 --> 00:46:18,440
technology, especially on the cloud native space.

712
00:46:19,210 --> 00:46:23,330
You may see a lot of products using EBPF

713
00:46:23,410 --> 00:46:26,774
for observability, security, networking and

714
00:46:26,812 --> 00:46:30,682
so on. So EVPF, for those that

715
00:46:30,736 --> 00:46:34,742
don't know what this means, it stands for extended

716
00:46:34,806 --> 00:46:38,986
Barclay packets filter, BPF. It's very common on

717
00:46:39,008 --> 00:46:42,282
the Linux kernel and EBPF,

718
00:46:42,346 --> 00:46:46,026
it's like BPF with some tuning

719
00:46:46,138 --> 00:46:49,194
extra features and really cool extra features

720
00:46:49,242 --> 00:46:52,582
actually. And the idea of EBPF

721
00:46:52,666 --> 00:46:56,834
is extending your Linux kernel to

722
00:46:56,872 --> 00:47:00,366
trace, monitor and analyze system performances

723
00:47:00,398 --> 00:47:04,050
and behavior. So you can collecting things that's happening

724
00:47:04,120 --> 00:47:07,842
on the kernel level and providing sites

725
00:47:07,906 --> 00:47:10,470
and provide those information to the user space.

726
00:47:10,540 --> 00:47:14,514
Okay. And EBPF

727
00:47:14,562 --> 00:47:18,330
is not such a new technology, but you can see a few

728
00:47:18,400 --> 00:47:21,994
products leveraging EBPF. We start

729
00:47:22,032 --> 00:47:25,686
seeing a few more nowadays, but it's not highly

730
00:47:25,718 --> 00:47:28,774
adopted yet. Okay. For many reasons,

731
00:47:28,822 --> 00:47:32,546
people still not like discovering

732
00:47:32,598 --> 00:47:35,790
and so on. So the idea of this

733
00:47:35,940 --> 00:47:39,646
demo folks is basically the same idea. It's like we are

734
00:47:39,668 --> 00:47:43,406
going to having an agent that will be able to collecting

735
00:47:43,438 --> 00:47:47,170
all the signos that we need, like the metrics, trace logs

736
00:47:48,310 --> 00:47:51,666
from the application level, not only from the

737
00:47:51,688 --> 00:47:55,570
infrastructure, but as we did for the other demos,

738
00:47:56,330 --> 00:47:59,734
we're going to use the same

739
00:47:59,932 --> 00:48:03,138
concept of collecting application level observability.

740
00:48:03,234 --> 00:48:06,742
Okay, wow. So let's move

741
00:48:06,876 --> 00:48:10,634
to the VS code. And this

742
00:48:10,672 --> 00:48:15,414
demo is going to be a little bit different because I'm

743
00:48:15,462 --> 00:48:18,954
really focusing on kubernetes strategies right

744
00:48:18,992 --> 00:48:23,360
now because I'm going to be using a solution which is Kubernetes based.

745
00:48:23,970 --> 00:48:27,742
But we already have many other options to the ones that not

746
00:48:27,796 --> 00:48:31,406
using kubernetes. Okay, so what

747
00:48:31,428 --> 00:48:35,634
we have here, it's like pretty simple. We have a

748
00:48:35,672 --> 00:48:39,666
cluster and then we are going to starting a

749
00:48:39,688 --> 00:48:43,262
minikube cluster and then we are going to install on the cluster

750
00:48:43,326 --> 00:48:48,214
cilian. What is cilian? Celine is

751
00:48:48,252 --> 00:48:52,406
a networking interface for

752
00:48:52,508 --> 00:48:56,386
Kubernetes. Okay. So we have many, like Falco,

753
00:48:56,498 --> 00:48:59,978
we have some cloud specific

754
00:49:00,064 --> 00:49:03,702
CNIs container network interface like AWS,

755
00:49:03,766 --> 00:49:07,302
CNI and Cylin is another CNI,

756
00:49:07,366 --> 00:49:11,630
okay. And Celine is fully built

757
00:49:11,700 --> 00:49:14,974
on top of EBPF and it's using EBPF for

758
00:49:15,012 --> 00:49:18,382
networking, for loading, balance and many other

759
00:49:18,436 --> 00:49:21,694
things. And now so using EBPF to

760
00:49:21,732 --> 00:49:25,710
provide observability inside your cluster. So using

761
00:49:25,780 --> 00:49:29,870
Celine and its EBPF agent, we can collect

762
00:49:30,030 --> 00:49:32,798
metrics like TCP,

763
00:49:32,894 --> 00:49:36,774
HTTP networking metrics and so

764
00:49:36,812 --> 00:49:40,694
on. And we can also understanding what

765
00:49:40,732 --> 00:49:43,750
is the networking flow inside our cluster.

766
00:49:45,450 --> 00:49:48,826
Very similar with the information we have

767
00:49:48,928 --> 00:49:52,950
on the Yeager diagram architecture,

768
00:49:53,030 --> 00:49:57,018
but it's build not using traces, it's build

769
00:49:57,104 --> 00:49:59,450
using network flows.

770
00:50:00,190 --> 00:50:03,806
But the concept is the same. It's just another sign or another kind

771
00:50:03,828 --> 00:50:06,958
of information that we can use to get the same

772
00:50:07,044 --> 00:50:10,398
site. Okay, so I have all the comments that

773
00:50:10,404 --> 00:50:13,790
I need to install celeb to install the monitoring,

774
00:50:15,090 --> 00:50:19,246
the monitoring stack on the Kubernetes cluster. And another thing like

775
00:50:19,428 --> 00:50:23,774
we are leveraging some applications from Star wars to

776
00:50:23,812 --> 00:50:27,142
start collecting traffic from there. Okay, I will not

777
00:50:27,196 --> 00:50:31,522
cover each comment as I didn't on the previous

778
00:50:31,586 --> 00:50:35,400
one, but you are free to check this out later.

779
00:50:35,850 --> 00:50:39,690
All the source code will be available on the GitHub and

780
00:50:39,840 --> 00:50:43,930
you have access for that. Okay folks, so meanwhile

781
00:50:44,830 --> 00:50:48,426
let's start creating, make setup as we

782
00:50:48,448 --> 00:50:52,234
did for the other ones. We are going to start in a Kubernetes cluster

783
00:50:52,362 --> 00:50:57,034
and then we will start installing

784
00:50:57,082 --> 00:50:59,802
all the things that we need like Celian, Grafana,

785
00:50:59,866 --> 00:51:03,140
Prometheus and so on. Okay?

786
00:51:03,510 --> 00:51:07,220
So as soon as it's finished, we will be back here.

787
00:51:07,590 --> 00:51:10,674
Okay, cool. Now, we already have all

788
00:51:10,712 --> 00:51:15,018
the components running on the cluster. So if we run kubectl,

789
00:51:15,134 --> 00:51:18,774
get pods a to get pods from all

790
00:51:18,812 --> 00:51:22,310
namespace, we're going to see all the

791
00:51:22,380 --> 00:51:26,098
pods that we need to have on our clusters.

792
00:51:26,194 --> 00:51:29,446
Like we have the celine operator,

793
00:51:29,558 --> 00:51:33,754
which is the operator that's going to ensure that

794
00:51:33,792 --> 00:51:39,526
we have each agent has ceiling

795
00:51:39,558 --> 00:51:43,222
agent running there, that the cylinder is running health,

796
00:51:43,296 --> 00:51:46,042
it's collecting all the metrics and so on and so forth.

797
00:51:46,186 --> 00:51:50,426
So we have few kubernetes pods

798
00:51:50,458 --> 00:51:54,580
like core, DNS, etcD. It's pretty straightforward. Those one, we have

799
00:51:56,310 --> 00:52:00,850
rubble and UI. Okay, I have other Kubernetes

800
00:52:02,230 --> 00:52:06,058
pods as well. So the thing here is like cylinder

801
00:52:06,094 --> 00:52:09,618
by default is not providing any kind of observability.

802
00:52:09,714 --> 00:52:13,174
Okay, the ceiling project, it's working in a very

803
00:52:13,212 --> 00:52:17,074
specific way, which is Kubernetes CNI

804
00:52:17,202 --> 00:52:20,790
networking and loading balancing.

805
00:52:20,870 --> 00:52:24,250
Sure that when you create a new pod,

806
00:52:24,590 --> 00:52:28,502
the pods getting IP, the nodes getting IP,

807
00:52:28,646 --> 00:52:32,538
it's like doing the communication with your cloud providers

808
00:52:32,634 --> 00:52:36,762
to getting ips from your network

809
00:52:36,826 --> 00:52:39,982
and so on. But the

810
00:52:40,036 --> 00:52:43,902
Celian project has another sub project named

811
00:52:43,966 --> 00:52:47,490
Hubble. So Hubble is an observability

812
00:52:49,670 --> 00:52:54,574
solution, if I can say that, that it's

813
00:52:54,622 --> 00:52:58,534
leveraging cilium to getting network flows from

814
00:52:58,572 --> 00:53:03,042
your pod communications and then extracting metrics

815
00:53:03,106 --> 00:53:07,014
and providing network visibility from

816
00:53:07,052 --> 00:53:10,854
your cluster and the applications that you are running. Okay, so this

817
00:53:10,892 --> 00:53:14,474
is what is celebrated and this is what Hubble and we are going to see

818
00:53:14,512 --> 00:53:18,346
this right now. For this we

819
00:53:18,368 --> 00:53:22,142
need to have port forward few components on

820
00:53:22,196 --> 00:53:25,534
our machine. Let me show you the pods again

821
00:53:25,572 --> 00:53:29,454
and explain other thing. So we have each agent with

822
00:53:29,492 --> 00:53:33,330
a ceiling agent et Kubernetes nodes with a ceiling agent running

823
00:53:33,400 --> 00:53:37,310
there, watching every network

824
00:53:37,390 --> 00:53:39,780
communication inside those nodes. Okay,

825
00:53:40,470 --> 00:53:44,270
and then rubble, it's getting the network

826
00:53:44,350 --> 00:53:47,410
flows and using all the observability that we need,

827
00:53:47,480 --> 00:53:50,660
creating metrics and so on.

828
00:53:51,590 --> 00:53:55,270
And we have a few other things here, which is like

829
00:53:55,420 --> 00:53:59,100
we have a Grafana and then we have also prometheus because we need

830
00:53:59,790 --> 00:54:03,354
store the time series that

831
00:54:03,552 --> 00:54:07,926
rebel is creating. And we have Death

832
00:54:07,958 --> 00:54:11,306
Star and now tie fighter

833
00:54:11,418 --> 00:54:15,086
X wing to provide

834
00:54:15,188 --> 00:54:18,670
some loads inside the cluster. And then we can mimic like

835
00:54:18,740 --> 00:54:21,550
services communicating to each other. Okay,

836
00:54:21,620 --> 00:54:25,700
so the tie fighter and X wing is like

837
00:54:26,070 --> 00:54:29,634
doing some HTTP request to death Star. And we're going to see

838
00:54:29,672 --> 00:54:33,170
this in action like right now. Cool.

839
00:54:33,240 --> 00:54:36,546
So let's port forward a few components

840
00:54:36,658 --> 00:54:39,878
like make port forward and then

841
00:54:39,964 --> 00:54:43,926
relay because we are running all

842
00:54:43,948 --> 00:54:47,598
those things from our machine. And then this is like ceiling

843
00:54:47,634 --> 00:54:51,846
requirement because cylinder

844
00:54:51,878 --> 00:54:55,546
UI needs to talk to cylinder relay to getting the information

845
00:54:55,648 --> 00:54:59,130
that we need. So now I'm going to run get

846
00:54:59,200 --> 00:55:03,210
port make port forward Ui.

847
00:55:03,630 --> 00:55:07,630
Okay. Meanwhile we can switch to browser

848
00:55:08,130 --> 00:55:11,546
and then on the browser, let me switch to browser

849
00:55:11,578 --> 00:55:15,266
as well. We can. Localhost is 2000,

850
00:55:15,368 --> 00:55:18,100
I guess it's 12,000.

851
00:55:18,630 --> 00:55:22,366
Okay, this is the rubble

852
00:55:22,478 --> 00:55:25,734
home page. Okay, so you can see all

853
00:55:25,772 --> 00:55:29,400
the namespace we have inside the cluster. And then

854
00:55:30,730 --> 00:55:34,326
if we click on the namespace, we can see all

855
00:55:34,348 --> 00:55:38,358
the applications is running inside this namespace and the traffic

856
00:55:38,534 --> 00:55:41,658
flowing inside each application.

857
00:55:41,824 --> 00:55:45,370
Let me show you another one. Like Kube systems, the same idea.

858
00:55:45,520 --> 00:55:49,020
We have a rubber UI and a rubble running there.

859
00:55:50,510 --> 00:55:54,080
What else on Celia monitoring we have

860
00:55:54,530 --> 00:55:58,058
Grafana. Let me see if it's load.

861
00:55:58,234 --> 00:56:01,646
No flows found for now because we don't

862
00:56:01,668 --> 00:56:05,874
have any trafficking happening inside this

863
00:56:05,912 --> 00:56:09,842
namespace, but we can back to default and then

864
00:56:09,896 --> 00:56:14,178
we can see that x wing and tie fighter is

865
00:56:14,344 --> 00:56:17,860
talking to Devstar. Okay. And we can see

866
00:56:20,250 --> 00:56:24,630
all the action happening right now, like the post for

867
00:56:24,700 --> 00:56:27,762
the v one requested landing. Okay.

868
00:56:27,836 --> 00:56:31,194
And we can see like forward. And if we click on

869
00:56:31,232 --> 00:56:35,002
those things, let me see maybe

870
00:56:35,136 --> 00:56:37,420
down below, if we click here,

871
00:56:38,350 --> 00:56:42,186
I just missed this. We can see few details,

872
00:56:42,218 --> 00:56:45,520
like when this communication is happening,

873
00:56:46,770 --> 00:56:50,106
we see if it's a track, that action, if it's in grass

874
00:56:50,138 --> 00:56:54,434
or aggress, what else we

875
00:56:54,472 --> 00:56:57,934
know what is the source pods. So if you have many pods,

876
00:56:57,982 --> 00:57:01,220
you can see from where this

877
00:57:01,670 --> 00:57:04,020
network action is coming from.

878
00:57:05,450 --> 00:57:08,898
And you have a few labels, we have the ip,

879
00:57:08,994 --> 00:57:12,162
we have the destination pods, we have destination

880
00:57:12,226 --> 00:57:15,174
labels, like a lot of useful information.

881
00:57:15,372 --> 00:57:18,714
Okay? And then we can run

882
00:57:18,832 --> 00:57:22,490
a few filters here,

883
00:57:22,560 --> 00:57:25,994
like we can filter by name. Let me

884
00:57:26,032 --> 00:57:29,562
see, kubernetes, I'm not seeing this,

885
00:57:29,616 --> 00:57:31,600
but maybe clicking here,

886
00:57:32,210 --> 00:57:36,110
namespace default. It's already, you see,

887
00:57:36,180 --> 00:57:40,126
label equals namespace equals default. This is how

888
00:57:40,308 --> 00:57:43,426
we are filtering. And this is the same thing.

889
00:57:43,448 --> 00:57:47,540
If we click here on the

890
00:57:47,910 --> 00:57:51,266
service pod, we can see a few labels from

891
00:57:51,288 --> 00:57:54,100
the pod, from the destination, same thing.

892
00:57:55,930 --> 00:57:59,042
And that's basically that. From Cedar,

893
00:57:59,186 --> 00:58:02,614
we can know a few network information,

894
00:58:02,732 --> 00:58:04,280
but not very special.

895
00:58:06,490 --> 00:58:10,026
But this is more like a UI view because

896
00:58:10,128 --> 00:58:13,606
from that application you cannot create any alerts,

897
00:58:13,638 --> 00:58:17,446
you cannot create any dashboard. Okay? This is more information regarding

898
00:58:17,558 --> 00:58:20,746
ceiling and rubble than like a properly

899
00:58:20,778 --> 00:58:24,814
observability solution. But how?

900
00:58:24,852 --> 00:58:28,814
Look, we are, because ceiling and rubble exports all

901
00:58:28,852 --> 00:58:32,826
those information, especially metrics, is being created

902
00:58:32,858 --> 00:58:36,546
to a time series database such as Prometheus. And then we

903
00:58:36,568 --> 00:58:39,890
can move back to vs code. Okay,

904
00:58:39,960 --> 00:58:44,002
so let's go back to vs code and then let's create a

905
00:58:44,056 --> 00:58:49,030
new tab here and let's port forward Grafana.

906
00:58:49,450 --> 00:58:53,094
Now we have grafana running. Let's go back to the

907
00:58:53,132 --> 00:58:56,840
browser and open localhost 3000.

908
00:58:57,630 --> 00:59:01,574
And we have few dashboards on Grafana. The first dashboard

909
00:59:01,622 --> 00:59:04,010
is like about ceiling operator.

910
00:59:04,990 --> 00:59:08,614
This is not what we need, it's more related to ceiling

911
00:59:08,662 --> 00:59:10,170
operator healthiness.

912
00:59:12,350 --> 00:59:15,390
What else? We have ceiling metrics,

913
00:59:15,730 --> 00:59:19,278
which is useful as well, but not what we need.

914
00:59:19,364 --> 00:59:22,910
Okay, we may see like how much the

915
00:59:22,980 --> 00:59:26,430
BPF memory has been using, if we have any

916
00:59:26,500 --> 00:59:29,778
ABPF, air horse or not,

917
00:59:29,944 --> 00:59:33,426
system calls, maps and so on. But this is

918
00:59:33,448 --> 00:59:37,110
not what we want, right? Let's see again

919
00:59:37,180 --> 00:59:40,866
what else we have. We have two another dashboards,

920
00:59:40,898 --> 00:59:44,614
which are more related to rubble and the metrics. Rubble is

921
00:59:44,652 --> 00:59:47,842
empowering based on the network flows.

922
00:59:47,986 --> 00:59:51,594
So we have Hubble dashboard by itself.

923
00:59:51,712 --> 00:59:55,190
We may see the amount of flows and the flows,

924
00:59:55,270 --> 00:59:58,842
folks, we are talking about here. It's all those things is happening

925
00:59:58,896 --> 01:00:02,682
on this tab down below where I'm hovering

926
01:00:02,746 --> 01:00:07,134
the mouse. So each communication between a

927
01:00:07,172 --> 01:00:10,926
search and a destination is considered a flow. So we

928
01:00:10,948 --> 01:00:14,354
may see the amount of flows we have, we may see the type

929
01:00:14,392 --> 01:00:18,222
of flows we have, like it's trace or if it's

930
01:00:18,286 --> 01:00:21,902
Lsl seven network

931
01:00:21,966 --> 01:00:26,562
flow. So in this case we have a few l seven flows

932
01:00:26,626 --> 01:00:29,394
because we are doing HTTP quests,

933
01:00:29,442 --> 01:00:32,646
okay. And so on. So we may see if

934
01:00:32,668 --> 01:00:36,182
we are losing any package. And that's it

935
01:00:36,236 --> 01:00:39,482
from this dashboard, which is nice because we

936
01:00:39,536 --> 01:00:43,062
start already seeing a few HTTP

937
01:00:43,126 --> 01:00:46,934
metrics that are being created on top of the network

938
01:00:46,982 --> 01:00:50,858
flow. Celian is collecting DNS

939
01:00:50,954 --> 01:00:54,414
and so on. But what is nice here is also the

940
01:00:54,452 --> 01:00:57,710
Hubble L seven HTTP metrics by workload,

941
01:00:58,610 --> 01:01:02,754
where we can see what

942
01:01:02,792 --> 01:01:06,126
is the source workload,

943
01:01:06,238 --> 01:01:09,582
for example x wing or tire fighter,

944
01:01:09,726 --> 01:01:13,314
and then we can see the destination in this case

945
01:01:13,352 --> 01:01:16,322
is only death star. We don't have any other destination,

946
01:01:16,466 --> 01:01:20,146
but we can see metrics

947
01:01:20,258 --> 01:01:24,786
like by stats code, by source

948
01:01:24,818 --> 01:01:28,738
and destination and so on. So let's explore.

949
01:01:28,834 --> 01:01:32,154
We may see also latency, as we can see here, like the p

950
01:01:32,192 --> 01:01:35,834
99 and the p 95. We can build like

951
01:01:35,872 --> 01:01:39,894
slos using these metrics. We can build in alerts for error,

952
01:01:39,942 --> 01:01:43,774
h latence and so on. So just to show you

953
01:01:43,972 --> 01:01:47,770
we can see few metrics to explore.

954
01:01:47,850 --> 01:01:51,502
Labels, let me see, five minutes.

955
01:01:51,636 --> 01:01:55,026
And we can see a few labels here. Like we have the

956
01:01:55,048 --> 01:01:58,702
destination, the namespace and the workload,

957
01:01:58,846 --> 01:02:01,060
the destination IP as well.

958
01:02:02,630 --> 01:02:06,438
We have which protocol we are using because

959
01:02:06,604 --> 01:02:10,534
we may use like Kafka protocol, we may

960
01:02:10,572 --> 01:02:14,134
use HTTP or any other kind

961
01:02:14,172 --> 01:02:17,734
of things. Okay, we may

962
01:02:17,772 --> 01:02:21,582
use HTTP two and so on. We have the source workload

963
01:02:21,666 --> 01:02:25,434
and the source namespace, which is the

964
01:02:25,472 --> 01:02:29,382
method. And I guess we also have the status

965
01:02:29,446 --> 01:02:32,910
code in someplace here. I'm not

966
01:02:32,980 --> 01:02:36,942
100% sure. Yeah, we have the status and also

967
01:02:36,996 --> 01:02:40,814
we have the method. So we

968
01:02:40,852 --> 01:02:43,426
may create inquiries like,

969
01:02:43,528 --> 01:02:47,138
okay, on the last five

970
01:02:47,224 --> 01:02:50,900
minutes, I want by

971
01:02:53,510 --> 01:02:57,720
this, in a source workload code

972
01:02:59,370 --> 01:03:03,320
and method, and we can see something like that.

973
01:03:04,250 --> 01:03:07,990
I guess it's status maybe. Yeah, it's status.

974
01:03:09,790 --> 01:03:12,906
So we may build some slos like all the

975
01:03:12,928 --> 01:03:16,934
requests, all the requests by the band requests,

976
01:03:16,982 --> 01:03:23,038
and then we can status something

977
01:03:23,204 --> 01:03:24,400
like that.

978
01:03:25,810 --> 01:03:29,774
Okay, I just have typo here.

979
01:03:29,892 --> 01:03:32,606
Oh, it's the opposing. Yeah,

980
01:03:32,788 --> 01:03:36,062
and then we may have something like that.

981
01:03:36,116 --> 01:03:39,540
Yeah, I broken the query, but I don't know why

982
01:03:41,910 --> 01:03:45,714
because it's zero. Yeah, this is the reason because we don't have

983
01:03:45,752 --> 01:03:48,790
enough questions with the errors. But yeah, you got the idea.

984
01:03:48,860 --> 01:03:52,566
Okay, so you can use all those metrics on

985
01:03:52,588 --> 01:03:55,958
the same standard that we have. On the other, doesn't matter which technology

986
01:03:56,044 --> 01:03:59,926
you're using, if it's java, if it's golang, if it's ruby,

987
01:04:00,038 --> 01:04:03,302
we don't care. The metrics we are collecting

988
01:04:03,366 --> 01:04:06,922
are the same. The network flow we are collecting are the same.

989
01:04:07,056 --> 01:04:10,366
We don't need to instrument our codes to get all

990
01:04:10,388 --> 01:04:13,806
those metrics, which is nice. Again, this is

991
01:04:13,828 --> 01:04:17,706
on the application level, it's not on the load balancer

992
01:04:17,738 --> 01:04:21,840
level. Okay. It's pretty close to your service.

993
01:04:23,090 --> 01:04:26,198
And then this is the idea of ceiling,

994
01:04:26,314 --> 01:04:28,900
EBPF and so on. Okay,

995
01:04:29,270 --> 01:04:32,402
so back into the

996
01:04:32,536 --> 01:04:36,210
slides. I would like to add a few things

997
01:04:36,280 --> 01:04:41,974
here is like, I've been using Celine as

998
01:04:42,092 --> 01:04:45,240
the solution for this demo, but the truth is,

999
01:04:45,610 --> 01:04:49,530
when I was building this demo, Celine was the most mature

1000
01:04:50,030 --> 01:04:53,270
technology for observability using EBPF.

1001
01:04:53,430 --> 01:04:56,474
But nowadays, we already have a few

1002
01:04:56,512 --> 01:05:00,590
more technologies that I didn't test yet. Like we have

1003
01:05:00,740 --> 01:05:04,526
opentelemetry, EBPF agent, which is providing a

1004
01:05:04,548 --> 01:05:08,618
few metrics. Not very detailed as Celine

1005
01:05:08,714 --> 01:05:12,586
is doing. But I do believe this is a project that's

1006
01:05:12,618 --> 01:05:16,210
going to be very mature in a few

1007
01:05:16,280 --> 01:05:19,650
weeks and a few months maybe. We are still working

1008
01:05:19,720 --> 01:05:23,486
to have helm shards to make this agent installation

1009
01:05:23,598 --> 01:05:27,394
easily, and the opentelemetry team is working to improve

1010
01:05:27,442 --> 01:05:31,174
this. I know that community is

1011
01:05:31,212 --> 01:05:34,994
building a few other services using EBPF, but I didn't

1012
01:05:35,042 --> 01:05:38,326
test yet. Okay, so for sure,

1013
01:05:38,428 --> 01:05:41,980
Celium is not providing all the signals we need.

1014
01:05:42,750 --> 01:05:46,278
We are talking about more related

1015
01:05:46,374 --> 01:05:50,354
to maybe we are not including traces

1016
01:05:50,422 --> 01:05:53,854
and logs. Okay, we are only talking about metrics. But this

1017
01:05:53,892 --> 01:05:59,290
is a starting. EBPF, as I said, is continuous

1018
01:05:59,370 --> 01:06:02,754
growing technology. So day after

1019
01:06:02,792 --> 01:06:06,354
day, we seek the community involving the

1020
01:06:06,392 --> 01:06:09,246
observability and security solutions using EBPF.

1021
01:06:09,358 --> 01:06:12,930
So I do believe this is a technology that you

1022
01:06:13,080 --> 01:06:16,694
must watch for

1023
01:06:16,732 --> 01:06:20,226
your observability systems, not only for metrics

1024
01:06:20,258 --> 01:06:24,434
and traces, but also for profiling. We see many profiling

1025
01:06:24,482 --> 01:06:28,354
solutions like park doing cpu and memory profiling

1026
01:06:28,402 --> 01:06:30,918
using EBPF. Okay, folks,

1027
01:06:31,014 --> 01:06:34,678
so this is the third strategy,

1028
01:06:34,694 --> 01:06:38,282
the EVPF strategies. Again, the idea is to not change

1029
01:06:38,336 --> 01:06:42,222
any line of code to include instrumentation. And we

1030
01:06:42,276 --> 01:06:45,150
collecting as much as we can using platform.

1031
01:06:45,300 --> 01:06:49,198
Okay, so this was the last one.

1032
01:06:49,284 --> 01:06:53,120
Let's now looking for a table comparing both of them.

1033
01:06:54,230 --> 01:06:58,238
Okay, now you might be asking what is the best solution?

1034
01:06:58,334 --> 01:07:01,810
What is the best strategy? To not overload my team.

1035
01:07:01,960 --> 01:07:05,382
And this is the answer that engineers usually

1036
01:07:05,436 --> 01:07:09,782
to hate. But it depends, of course, it depends of your

1037
01:07:09,836 --> 01:07:13,334
infrastructure. It depends your team knowledge, it depends how many

1038
01:07:13,372 --> 01:07:16,994
people you have working on platform abstracting

1039
01:07:17,042 --> 01:07:20,426
features provide things as service inside your company.

1040
01:07:20,608 --> 01:07:25,066
So of course it depends always. But I

1041
01:07:25,168 --> 01:07:28,540
list a few things that I think that might be important,

1042
01:07:28,990 --> 01:07:32,966
like technology agnostic, context propagation

1043
01:07:33,078 --> 01:07:36,874
environment agnostic, and also MOOC opentelemetry data.

1044
01:07:37,072 --> 01:07:40,174
So from the technical agnostic, what I mean by this,

1045
01:07:40,212 --> 01:07:44,338
if I have different implementations based on the technology

1046
01:07:44,504 --> 01:07:48,590
I've been using, and for proxy and EBPF

1047
01:07:48,670 --> 01:07:52,466
solutions, this is completely agnostic. It doesn't matter the

1048
01:07:52,488 --> 01:07:55,746
technology you are using, the implementation will be the same like on the

1049
01:07:55,768 --> 01:07:59,526
proxy we are collecting on the proxy level, EBPF is doing all

1050
01:07:59,548 --> 01:08:04,390
the magic on the kernel level for you, the opentelemetry instrumentation,

1051
01:08:05,290 --> 01:08:07,960
it depends on the technology that you are using.

1052
01:08:08,990 --> 01:08:11,020
The example that I show you,

1053
01:08:11,950 --> 01:08:15,834
it's a python solution. So if

1054
01:08:15,872 --> 01:08:19,370
you are using, I don't know, C sharp, this is another

1055
01:08:19,440 --> 01:08:23,358
way to implement like the same concept, but another

1056
01:08:23,444 --> 01:08:26,746
way if you are using ROS, another way if you're

1057
01:08:26,778 --> 01:08:29,406
using nodes as well.

1058
01:08:29,588 --> 01:08:34,274
So open telemetry instrumentation depends on your

1059
01:08:34,312 --> 01:08:38,754
technology. So you might have different implementation based on

1060
01:08:38,792 --> 01:08:42,194
the solution you have. But in the end

1061
01:08:42,392 --> 01:08:46,494
it's worth, you saw the power of opentelemetry,

1062
01:08:46,542 --> 01:08:50,434
of instrumentation and the things we can do with opentelemetry

1063
01:08:50,482 --> 01:08:53,622
collector. I do believe that you should give a try for that.

1064
01:08:53,756 --> 01:08:58,018
Okay, so the things that ensures context propagation,

1065
01:08:58,114 --> 01:09:00,678
like the proxy, it kind of shows,

1066
01:09:00,764 --> 01:09:05,062
okay, it only ensures context propagation

1067
01:09:05,126 --> 01:09:08,998
on our demo because we have an ingress proxy and an egress

1068
01:09:09,094 --> 01:09:12,746
proxy. So I'm listening to all the traffic that's

1069
01:09:12,778 --> 01:09:16,414
incoming from my platform and all the traffic that's going out from

1070
01:09:16,452 --> 01:09:20,046
my services. And the

1071
01:09:20,068 --> 01:09:23,502
other things is we only see context

1072
01:09:23,566 --> 01:09:26,946
between proxies. We don't see things happening sides of the

1073
01:09:26,968 --> 01:09:30,366
application. Okay? So we cannot

1074
01:09:30,478 --> 01:09:32,734
using this information to Rio,

1075
01:09:32,782 --> 01:09:36,358
troubleshooting the problems inside the application.

1076
01:09:36,524 --> 01:09:39,174
This is what I mean regarding traces, okay,

1077
01:09:39,292 --> 01:09:43,154
regarding the metrics, well, we have the highest

1078
01:09:43,282 --> 01:09:47,586
level possible and as

1079
01:09:47,628 --> 01:09:51,158
much closer from your customers, your proxies.

1080
01:09:51,334 --> 01:09:54,826
More realistic will be the latents and the error rates that

1081
01:09:54,848 --> 01:09:57,580
you're going to collect from your system. Okay,

1082
01:09:57,950 --> 01:10:01,222
the opentelemetry and the EBPF,

1083
01:10:01,286 --> 01:10:05,454
yeah, it ensures context propagation. The opentelemetry saw the

1084
01:10:05,492 --> 01:10:09,118
things as well, going through one serfs in another.

1085
01:10:09,204 --> 01:10:12,606
And they use inside the serfs, the EBPF as

1086
01:10:12,628 --> 01:10:16,050
well. It depends, of course, the solution you are using. Sealum is not

1087
01:10:16,120 --> 01:10:20,414
talking about trace, but if you're trying another solutions, that is EBPF

1088
01:10:20,462 --> 01:10:23,220
trace solution, it's going to work.

1089
01:10:24,070 --> 01:10:27,910
All the three options are environment agnostic. So it doesn't matter

1090
01:10:27,980 --> 01:10:31,794
if you're running kubernetes,

1091
01:10:31,842 --> 01:10:35,218
if you're running virtual machines, if you are running bare metal. So it's

1092
01:10:35,234 --> 01:10:39,370
going to work. The ceiling is

1093
01:10:39,440 --> 01:10:43,034
only working for kubernetes because

1094
01:10:43,072 --> 01:10:46,538
it's a Kubernetes CNI. But the EBPF solution is not

1095
01:10:46,704 --> 01:10:51,126
kubernetes based technology. So you can use Opentelemetry

1096
01:10:51,158 --> 01:10:54,410
EBPF collector on your Linux machines.

1097
01:10:54,490 --> 01:10:57,962
Okay. All those are providing

1098
01:10:58,026 --> 01:11:02,240
different kind of telemetry data. We have logs, from there we have

1099
01:11:02,770 --> 01:11:06,050
traces and now we also

1100
01:11:06,120 --> 01:11:09,810
have metrics for sure for all those three options.

1101
01:11:09,880 --> 01:11:13,790
Okay folks. And as I said, it depends.

1102
01:11:13,870 --> 01:11:17,342
You need to understand your user case, you need to understand your requirements

1103
01:11:17,406 --> 01:11:20,870
and your capabilities to choose the best fit for you.

1104
01:11:20,940 --> 01:11:24,646
One of them is going to provide for you what we need and the

1105
01:11:24,668 --> 01:11:27,898
metrics and the telemetry data that you are looking for.

1106
01:11:27,984 --> 01:11:31,626
Okay, cool. But there is another

1107
01:11:31,728 --> 01:11:35,034
option and if I'm able

1108
01:11:35,072 --> 01:11:39,110
to implement every solution, why not do this and collecting

1109
01:11:39,270 --> 01:11:42,394
metrics in different levels, okay, like using

1110
01:11:42,432 --> 01:11:45,994
the proxy strategies to collecting metrics on the proxy

1111
01:11:46,042 --> 01:11:49,614
level, the proxies that are closer to my customers and then I

1112
01:11:49,652 --> 01:11:53,630
can measure pretty close customers

1113
01:11:53,780 --> 01:11:58,180
latency. Okay, why not using auto instrumentation to

1114
01:11:59,510 --> 01:12:02,674
start collecting traces from my applications without any

1115
01:12:02,712 --> 01:12:06,514
code change and providing all the information that I need

1116
01:12:06,552 --> 01:12:10,274
and then I can use an open telemetry collector to processor and enrich

1117
01:12:10,322 --> 01:12:14,082
this telemetry data and why not using EBPF

1118
01:12:14,146 --> 01:12:17,554
as well? So then I can collecting network information and also

1119
01:12:17,612 --> 01:12:21,494
application metrics from the kernel level because the kernel

1120
01:12:21,542 --> 01:12:25,222
is the best place ever that we can use to collecting

1121
01:12:25,366 --> 01:12:28,906
observability and security data. And then you

1122
01:12:28,928 --> 01:12:32,394
can have different point of views and you can decide which

1123
01:12:32,432 --> 01:12:35,870
metric and which it's better for each level

1124
01:12:35,940 --> 01:12:39,882
you are looking for. If you're looking for network levels, probably EBPF.

1125
01:12:39,946 --> 01:12:43,822
And the Celia solution is going to be better for you. If you're only looking

1126
01:12:43,876 --> 01:12:47,918
for application level metrics, the opentelemetry

1127
01:12:47,934 --> 01:12:50,786
is going to be better and so on and so forth. And then you can

1128
01:12:50,808 --> 01:12:54,386
use all those strategies, provide as much insight as you can for

1129
01:12:54,408 --> 01:12:57,942
your engineer teams. They can build alerts, they can do whatever they

1130
01:12:57,996 --> 01:13:01,554
want, or you using all those standard metrics,

1131
01:13:01,602 --> 01:13:05,446
you can automatically providing them dashboards and

1132
01:13:05,468 --> 01:13:09,434
now alerting out of the box, okay? And this is

1133
01:13:09,552 --> 01:13:13,194
the great thing from those strategies to ensure that

1134
01:13:13,232 --> 01:13:17,210
the teams are going to getting default observability

1135
01:13:17,790 --> 01:13:21,262
for their services without any code changes. They can

1136
01:13:21,316 --> 01:13:24,974
focus on delivery features and make the product owner happen,

1137
01:13:25,012 --> 01:13:28,446
the customer happen as well, increase revenue. And then

1138
01:13:28,548 --> 01:13:32,142
when they really need to adding effort about

1139
01:13:32,196 --> 01:13:35,962
observability, they're going to adding observability for their context

1140
01:13:36,026 --> 01:13:39,658
for their specific use case cool. So folks,

1141
01:13:39,754 --> 01:13:43,662
that's it. We can also try to join all those

1142
01:13:43,716 --> 01:13:47,654
three. So that's it from my

1143
01:13:47,692 --> 01:13:51,046
side. I really hope you enjoyed it. If you have

1144
01:13:51,068 --> 01:13:53,922
any questions, feel free to ping me out. Will be my pleasure.

1145
01:13:53,986 --> 01:13:57,590
Talk to you and have a nice conversation about cloud

1146
01:13:57,660 --> 01:14:01,698
native, about observability and many other subjects.

1147
01:14:01,874 --> 01:14:05,174
And thank you for being here and thank

1148
01:14:05,212 --> 01:14:07,298
you for listening. So see you folks.

