1
00:00:25,570 --> 00:00:29,174
You. Hi, I'm Ivan Merrill, and this

2
00:00:29,212 --> 00:00:32,520
talk is incident response. Let's do science instead.

3
00:00:32,890 --> 00:00:35,718
And this talk is an evolution, really,

4
00:00:35,804 --> 00:00:39,814
of my thinking around incidents. Having spent 15 years in

5
00:00:39,852 --> 00:00:43,286
the financial sector, working primarily in kind of monitoring and

6
00:00:43,308 --> 00:00:46,982
observability, rolling tools out, educating people

7
00:00:47,036 --> 00:00:50,618
on how to use them, and getting involved in incidents.

8
00:00:50,714 --> 00:00:53,982
And as you can imagine, working with some fairly big

9
00:00:54,036 --> 00:00:57,630
financial institutions, some of those incidents can be

10
00:00:57,700 --> 00:01:01,726
pretty scary and have quite a large impact. So what

11
00:01:01,748 --> 00:01:04,946
I've tried to do is look at some things that have gone

12
00:01:05,048 --> 00:01:08,674
really not so good that I've seen before,

13
00:01:08,792 --> 00:01:12,402
some ideas around what I think can do and we can do better,

14
00:01:12,536 --> 00:01:15,782
and also add in some research that

15
00:01:15,836 --> 00:01:19,490
other people, far cleverer than me, have undertaken

16
00:01:19,650 --> 00:01:23,654
to help you, hopefully with some really practical information that

17
00:01:23,692 --> 00:01:26,550
can maybe help you with incidents.

18
00:01:27,550 --> 00:01:31,222
So first things first. Incident response,

19
00:01:31,286 --> 00:01:35,302
I really strongly believe can learn a lot from safety engineering

20
00:01:35,366 --> 00:01:39,258
in other domains. Right? The tech industry is

21
00:01:39,344 --> 00:01:42,474
still pretty young. And as someone

22
00:01:42,512 --> 00:01:46,222
who's spent a lot, all my career, really on the kind of operation side,

23
00:01:46,276 --> 00:01:49,982
I know that it's great to see now that

24
00:01:50,116 --> 00:01:53,680
sres are getting more focused and DevOps really

25
00:01:54,050 --> 00:01:58,206
kind of brought another bit of focus onto ops kind of roles and ops functions

26
00:01:58,238 --> 00:02:01,662
and everything else like that. But actually big incidents,

27
00:02:01,726 --> 00:02:05,414
complex incidents on big systems and stuff, we're still

28
00:02:05,452 --> 00:02:08,774
pretty new at it. We're still pretty young at it. And when we

29
00:02:08,812 --> 00:02:13,042
look at aviation, health, the emergency

30
00:02:13,106 --> 00:02:16,930
services, they've been dealing with really, really important incidents

31
00:02:17,010 --> 00:02:20,860
for an awful lot longer than us. And their incidents actually

32
00:02:21,470 --> 00:02:25,578
are often much more impactful. They are quite literally life

33
00:02:25,664 --> 00:02:29,082
and death. So it's absolutely right that

34
00:02:29,136 --> 00:02:33,054
we look at these industries, look at these domains, and understand why

35
00:02:33,092 --> 00:02:35,120
they're doing what they're doing. Right.

36
00:02:36,370 --> 00:02:40,270
I would feel horrified if I saw some firefighters turn up and

37
00:02:40,340 --> 00:02:43,406
they were just kind of, just started all, everyone kind of went for

38
00:02:43,428 --> 00:02:47,602
themselves and crazily started like using their hose and spraying water

39
00:02:47,656 --> 00:02:51,458
onto random bits, right? That's not what they do. They have

40
00:02:51,624 --> 00:02:54,798
clear kind of structure to what they're doing,

41
00:02:54,984 --> 00:02:58,102
some thought being their decision making.

42
00:02:58,236 --> 00:03:01,686
And so let's make sure that we can

43
00:03:01,868 --> 00:03:05,702
take these ideas into the tech industry as well,

44
00:03:05,836 --> 00:03:09,706
because we really need to. And too often

45
00:03:09,808 --> 00:03:11,770
it can descend into chaos.

46
00:03:14,190 --> 00:03:18,038
So firstly, a definition, right? I love a definition. And John Osborne

47
00:03:18,054 --> 00:03:21,598
and Richard Cook are far better to give one than

48
00:03:21,684 --> 00:03:25,454
I am. So I've relied on their kind of authoritative views here

49
00:03:25,492 --> 00:03:28,650
to take this definition,

50
00:03:28,730 --> 00:03:32,266
which is that incidents are a set of activities

51
00:03:32,378 --> 00:03:36,046
bounded in time that are related to an undesirable

52
00:03:36,158 --> 00:03:39,778
system behavior. And there's a couple of

53
00:03:39,784 --> 00:03:43,378
things I'd like to take away from this. One is that it says an

54
00:03:43,464 --> 00:03:47,046
undesirable system behavior. It doesn't just say

55
00:03:47,148 --> 00:03:50,498
something broke. Right? So that's

56
00:03:50,514 --> 00:03:54,626
.1 and .2 is it says a set of activities,

57
00:03:54,818 --> 00:03:58,666
not just this one thing. So it's not just

58
00:03:58,688 --> 00:04:02,358
one thing, and it didn't just break. It's a set of activities

59
00:04:02,534 --> 00:04:06,086
bounded in time that are related to an undesirable

60
00:04:06,198 --> 00:04:09,826
system behaviour. So we have a definition.

61
00:04:09,958 --> 00:04:12,894
Right. And why is all this important?

62
00:04:13,012 --> 00:04:17,066
Well, the fact is that catastrophe

63
00:04:17,178 --> 00:04:20,314
is always around the corner. So says Richard

64
00:04:20,362 --> 00:04:24,178
looks, who was an amazing person, and I strongly suggest you

65
00:04:24,264 --> 00:04:27,858
seek out his work. And he wrote about

66
00:04:27,944 --> 00:04:31,346
how complex system fails. And this is one of

67
00:04:31,368 --> 00:04:33,940
the items in that kind of list, really.

68
00:04:35,370 --> 00:04:38,262
We are never far away from catastrophe. And in fact,

69
00:04:38,316 --> 00:04:42,546
it's actually worse than that. Right. The complex systems

70
00:04:42,578 --> 00:04:44,120
that we are building. Right.

71
00:04:46,090 --> 00:04:49,802
One of the hallmarks of a complex system

72
00:04:49,936 --> 00:04:53,222
is the potential for a catastrophic outcome.

73
00:04:53,366 --> 00:04:57,226
So just the fact that we are building these complex systems means that

74
00:04:57,248 --> 00:05:00,554
we are creating the potential for catastrophe. We can never

75
00:05:00,672 --> 00:05:04,430
escape it, right. And we can build systems

76
00:05:05,490 --> 00:05:09,214
as resilient as we can, and we can really invest an awful lot of

77
00:05:09,252 --> 00:05:12,458
time in making them resilient, a lot of effort, a lot of

78
00:05:12,484 --> 00:05:16,034
money. But the fact is that there will

79
00:05:16,072 --> 00:05:19,266
always be a way for it to reach a

80
00:05:19,288 --> 00:05:22,354
catastrophic outcome. Some things are completely

81
00:05:22,472 --> 00:05:26,262
beyond our control, and so we need to work out

82
00:05:26,316 --> 00:05:28,870
how to actually deal with this catastrophe.

83
00:05:31,130 --> 00:05:34,838
And the fact is that incidents aren't actually

84
00:05:34,924 --> 00:05:38,522
very easy. They're generally pretty difficult. If they

85
00:05:38,576 --> 00:05:42,250
were easy, I would not be doing this talk and

86
00:05:42,320 --> 00:05:46,042
sharing some experience and hopefully some useful information with you.

87
00:05:46,176 --> 00:05:49,930
So as a result of them being pretty hard,

88
00:05:50,000 --> 00:05:53,906
in fact, there are a number of common pitfalls that people generally

89
00:05:53,958 --> 00:05:57,626
seem to fall into that I've certainly seen anyway. So I'd

90
00:05:57,658 --> 00:06:01,258
like to just kind of COVID these, and I'm sure some of them will appear

91
00:06:01,354 --> 00:06:05,250
quite familiar to you. So the first thing is

92
00:06:05,400 --> 00:06:08,562
an over reliance on dashboards and runbooks. Right.

93
00:06:08,696 --> 00:06:12,882
And what happens here that I've seen quite a lot is

94
00:06:12,936 --> 00:06:16,454
that people generate dashboards and they are

95
00:06:16,492 --> 00:06:20,002
a predetermined set of signals, which can be great signals,

96
00:06:20,066 --> 00:06:23,862
whatever. That's a whole other subject. But they

97
00:06:23,996 --> 00:06:27,430
have got these signals and they're trying to understand

98
00:06:27,500 --> 00:06:31,638
what's going on. This incident, based on these predetermined set of signals,

99
00:06:31,814 --> 00:06:35,034
and they might not be able to work

100
00:06:35,072 --> 00:06:38,554
out. Right. What's going on. This incident doesn't map nicely to

101
00:06:38,592 --> 00:06:42,446
their dashboard. And particularly when you

102
00:06:42,468 --> 00:06:45,966
have kind of several teams that all have their own dashboards and everything else like

103
00:06:45,988 --> 00:06:49,280
that, everyone's looking and their dashboards are all green,

104
00:06:49,650 --> 00:06:53,306
but the incidents is still occurring.

105
00:06:53,498 --> 00:06:56,930
And I found out that this is actually called the watermelon effect,

106
00:06:57,000 --> 00:07:00,194
which I really like, whereby everything

107
00:07:00,232 --> 00:07:04,002
is green on the surface, but if you scratch it at that surface, everything is

108
00:07:04,056 --> 00:07:07,666
red underneath. Right. And it

109
00:07:07,688 --> 00:07:11,686
can easily lead to this kind of very low meantime to innocence for

110
00:07:11,708 --> 00:07:14,934
every team, because everyone's looking at their dashboard, their dashboard is

111
00:07:14,972 --> 00:07:18,346
green. They're all saying they're fine, but the incident is going on,

112
00:07:18,528 --> 00:07:22,426
and runbooks aren't actually necessarily any

113
00:07:22,448 --> 00:07:26,474
better. This is not saying runbooks aren't great. They can absolutely be

114
00:07:26,512 --> 00:07:30,022
great. But we mustn't only rely on our runbooks,

115
00:07:30,086 --> 00:07:34,030
right. Because if all that ever happens is that we

116
00:07:34,100 --> 00:07:37,726
only have teams that know how to respond to

117
00:07:37,748 --> 00:07:41,006
incidents by looking for a relevant runbook, then all they

118
00:07:41,028 --> 00:07:44,850
can do is if situation x occurs, they do y.

119
00:07:45,000 --> 00:07:49,138
Right. There is no room

120
00:07:49,224 --> 00:07:53,074
really, for these people to build their troubleshooting skills that are really so

121
00:07:53,112 --> 00:07:56,950
important just in incidents, right. And they're not gaining any experience

122
00:07:57,100 --> 00:08:01,426
in how the system works. They're not rebuilding

123
00:08:01,458 --> 00:08:05,042
their mental model of their complex system with every incident.

124
00:08:05,106 --> 00:08:08,886
Right. There is no learning going on here. And so we

125
00:08:08,908 --> 00:08:12,858
need a way of to investigate incidents that allow us to build

126
00:08:13,024 --> 00:08:16,874
our unnecessary experience and learn what works and what doesn't work and

127
00:08:16,912 --> 00:08:20,462
help us kind of upgrade our mental models because our systems change.

128
00:08:20,516 --> 00:08:24,110
Right. And another

129
00:08:24,180 --> 00:08:27,486
thing that I have seen that, I mean, hopefully, quite obviously

130
00:08:27,588 --> 00:08:31,086
seems bad is guesswork. We know we need to

131
00:08:31,108 --> 00:08:35,186
do something, right? But we don't have a clue what to do. And so

132
00:08:35,288 --> 00:08:39,266
we sometimes guess, and there are many forms of

133
00:08:39,288 --> 00:08:42,562
guessing. Some things are just really big gambles, some things

134
00:08:42,616 --> 00:08:46,834
maybe a bit more educated guess, like just immediately

135
00:08:46,882 --> 00:08:50,566
failing over to a second site or anything else like that, right.

136
00:08:50,748 --> 00:08:53,894
And the fact is that sometimes it does actually work.

137
00:08:54,012 --> 00:08:57,898
Right. But that's luck, and luck is not

138
00:08:57,984 --> 00:09:01,830
a reliable kind of strategy.

139
00:09:01,910 --> 00:09:06,102
Right. We can't build up a troubleshooting

140
00:09:06,166 --> 00:09:09,558
skill based on luck. There's no learning opportunities

141
00:09:09,654 --> 00:09:13,046
for it. There is no hypothesis that's

142
00:09:13,078 --> 00:09:16,078
being built. There's no context to our decision making,

143
00:09:16,244 --> 00:09:19,374
and it's certainly not able to. We're certainly not able to build

144
00:09:19,412 --> 00:09:22,830
a runbook or anything else like that based on luck.

145
00:09:23,590 --> 00:09:27,522
Roll the dice and take option five.

146
00:09:27,576 --> 00:09:30,802
In this case, that's not going to work,

147
00:09:30,856 --> 00:09:34,642
right. So we need a way to structure our thinking

148
00:09:34,776 --> 00:09:38,534
and to help us move forward when information is actually low because

149
00:09:38,652 --> 00:09:41,702
we do find ourselves in situations where it is really not

150
00:09:41,756 --> 00:09:45,446
clear from the information that we have what to do. So how do we

151
00:09:45,468 --> 00:09:46,520
deal with that?

152
00:09:49,070 --> 00:09:52,698
Spending a long time on the wrong hypothesis is a

153
00:09:52,784 --> 00:09:56,822
massive time sink and can be really, really costly for organizations.

154
00:09:56,886 --> 00:10:00,346
Right. The amount of

155
00:10:00,368 --> 00:10:04,078
times we see we've got a high error rate,

156
00:10:04,244 --> 00:10:07,854
customers are complaining, things are breaking, and we look

157
00:10:07,892 --> 00:10:11,598
at whatever's giving us the most errors and we follow that through

158
00:10:11,684 --> 00:10:14,938
and we spend an hour or so

159
00:10:15,044 --> 00:10:18,642
investigating and everything is looking like it's absolutely going to be this

160
00:10:18,696 --> 00:10:22,622
thing. And suddenly this little bit of information appears

161
00:10:22,766 --> 00:10:26,342
and it's absolutely blown our hypothesis out,

162
00:10:26,396 --> 00:10:30,390
right. It cannot be the thing that we've just spent an hour investigating

163
00:10:31,130 --> 00:10:34,534
and we're human, right. And when

164
00:10:34,572 --> 00:10:37,640
we kind of have an idea of what's going on,

165
00:10:38,890 --> 00:10:42,266
we suffer from confirmation bias, right? We look for things that are

166
00:10:42,288 --> 00:10:45,786
going to reinforce our hypothesis. We want to

167
00:10:45,808 --> 00:10:49,034
believe in ourselves. We feel this is what it's going to be.

168
00:10:49,072 --> 00:10:52,874
We get this feeling. And so we surround ourselves

169
00:10:52,922 --> 00:10:56,926
with information. We seek out the information that's going to say,

170
00:10:57,108 --> 00:10:59,982
that's going to agree with what we think, right?

171
00:11:00,036 --> 00:11:04,020
This is confirmation bias. And so we need a way to prevent that

172
00:11:04,550 --> 00:11:07,986
as much as possible, right. Because we do not want to be spending a lot

173
00:11:08,008 --> 00:11:10,420
of time investigating the wrong things.

174
00:11:12,710 --> 00:11:16,040
Fear of failure is. Yeah,

175
00:11:16,490 --> 00:11:19,794
it's a really big thing and it's really important during incidents,

176
00:11:19,842 --> 00:11:24,054
right. Because incidents, no matter how

177
00:11:24,092 --> 00:11:27,682
much we can apply best practice and everything, they are high stress

178
00:11:27,746 --> 00:11:31,890
situations, right. And quite often good practices

179
00:11:31,970 --> 00:11:36,214
and good decision making can go out the way. And if we're

180
00:11:36,262 --> 00:11:39,834
scared of doing the wrong thing, we may never take action. In fact,

181
00:11:39,872 --> 00:11:43,114
that's one of the kind of the key factors

182
00:11:43,162 --> 00:11:46,346
in procrastination, right. We are generally just scared of what's

183
00:11:46,378 --> 00:11:49,760
going to happen if we get it wrong. And so we just never start.

184
00:11:50,770 --> 00:11:53,838
We need psychological safety in incidents.

185
00:11:53,934 --> 00:11:57,650
We need to feel good in the situations,

186
00:11:58,230 --> 00:12:01,300
in the decisions that we're making, right?

187
00:12:01,750 --> 00:12:06,722
And we can be in a situation where we are 99%

188
00:12:06,776 --> 00:12:10,662
certain that this is the thing or we can have all of the evidence and

189
00:12:10,716 --> 00:12:13,846
everything else like that done everything, right. But we

190
00:12:13,868 --> 00:12:16,870
are fearful of the consequences. And that, again,

191
00:12:16,940 --> 00:12:20,066
can delay resolving

192
00:12:20,098 --> 00:12:23,286
the incident. But also, it's really not nice for the people involved

193
00:12:23,318 --> 00:12:27,130
in incidents, right. It doesn't make people want to get involved in incidents either,

194
00:12:27,200 --> 00:12:30,070
which we need them to. So, yeah,

195
00:12:30,160 --> 00:12:33,326
fear of failure is a really important thing. We need to provide a

196
00:12:33,348 --> 00:12:37,006
way to have psychological safety and

197
00:12:37,108 --> 00:12:40,000
in our decision making. So, yeah,

198
00:12:43,650 --> 00:12:47,726
and so I've covered a few things that I've

199
00:12:47,758 --> 00:12:51,294
seen quite a few times in incidents, and I really like this quote.

200
00:12:51,342 --> 00:12:55,102
Right. History doesn't repeat itself, but it often rhymes.

201
00:12:55,246 --> 00:12:58,534
And I think this applies to incidents as

202
00:12:58,572 --> 00:13:02,390
well, because we shouldn't really ever be seeing the same incident over

203
00:13:02,460 --> 00:13:05,654
and over and over again. Right. But we can quite

204
00:13:05,692 --> 00:13:08,962
often see patterns of incidents.

205
00:13:09,106 --> 00:13:12,474
And so it's really important that we learn and we

206
00:13:12,512 --> 00:13:16,634
improve with every incident that we have. And many organizations are

207
00:13:16,672 --> 00:13:20,330
now performing some kind of post mortem, post incident analysis,

208
00:13:20,670 --> 00:13:24,938
whatever you want to call it, but there are also some kind of pitfalls

209
00:13:25,034 --> 00:13:28,686
that we can fall into here, too. So just going to take another moment just

210
00:13:28,708 --> 00:13:30,240
to look at a couple of those.

211
00:13:31,650 --> 00:13:35,714
And the thing is that it seems easy to look

212
00:13:35,752 --> 00:13:39,470
back at an incident and determine what went wrong. The difficulty

213
00:13:39,550 --> 00:13:43,026
is understanding what actually happened and how to

214
00:13:43,048 --> 00:13:46,870
learn from it. Hindsight is 2020,

215
00:13:47,020 --> 00:13:50,182
right? It's not actually very good to look back and go,

216
00:13:50,316 --> 00:13:52,440
oh, this thing happened.

217
00:13:54,090 --> 00:13:57,222
There is no understanding when we point out something that just seems

218
00:13:57,276 --> 00:14:01,066
really obvious afterwards because it didn't seem obvious at the

219
00:14:01,088 --> 00:14:04,602
time. If it was obvious, people would have,

220
00:14:04,736 --> 00:14:08,458
would have made that course of action. They would have seen that,

221
00:14:08,624 --> 00:14:12,218
oh, it was a bad deployment, we redeployed, everything was better,

222
00:14:12,304 --> 00:14:16,254
da da. Well, that's quite a kind of simplistic view of

223
00:14:16,292 --> 00:14:19,786
an event, right? And if it was that simple, people would have just resolved

224
00:14:19,818 --> 00:14:23,146
it straight away. But it wasn't. So it didn't

225
00:14:23,178 --> 00:14:26,626
feel that simple. And if we take that kind of

226
00:14:26,648 --> 00:14:30,610
approach, then we are preventing learning, right? And actually,

227
00:14:30,680 --> 00:14:34,066
so one of the things that I think we can understand from this is that

228
00:14:34,088 --> 00:14:37,326
we need to record the context of why decisions

229
00:14:37,358 --> 00:14:41,042
were made, right? So that if it seems so obvious afterwards,

230
00:14:41,106 --> 00:14:44,662
why didn't it at a time, right? Why were people thinking it was this?

231
00:14:44,716 --> 00:14:47,640
What is the context of that decision that they were making?

232
00:14:48,590 --> 00:14:52,490
And that's something that's Quite often missing in

233
00:14:52,640 --> 00:14:54,330
post incident analysis.

234
00:14:56,430 --> 00:14:59,626
And we can talk about it in terms of

235
00:14:59,648 --> 00:15:03,114
normative language, right? So when reviewing an incident,

236
00:15:03,242 --> 00:15:06,622
normative language kind of says that a judgment is being made,

237
00:15:06,676 --> 00:15:10,702
right? And that's often based on someone's perception. And it's really,

238
00:15:10,756 --> 00:15:14,490
really easy to do. I'm quite certain that I've done

239
00:15:14,500 --> 00:15:18,786
it before. I'm sure most people here have. But the

240
00:15:18,808 --> 00:15:21,682
implication, right, is that if people had done this thing,

241
00:15:21,736 --> 00:15:24,020
then everything would have been resolved much better.

242
00:15:26,310 --> 00:15:29,974
Everything would have been fine, it would have been resolved quicker. Da da. But they

243
00:15:30,012 --> 00:15:33,094
didn't, you know, and that's just silly them. Right? The team

244
00:15:33,132 --> 00:15:36,040
missed this obvious error, which they ought to have seen.

245
00:15:36,570 --> 00:15:40,546
Well, what if they missed it because there were a sea of alerts

246
00:15:40,578 --> 00:15:43,660
that made it impossible to see any single alert? Right?

247
00:15:44,030 --> 00:15:47,626
What if they had seen the alert, but they didn't know it was important

248
00:15:47,808 --> 00:15:51,580
or knew it was important, but they didn't know what to do?

249
00:15:52,050 --> 00:15:55,742
Normative language doesn't help us. Right? And in actual

250
00:15:55,796 --> 00:15:59,214
fact, it's removed an opportunity for learning because this

251
00:15:59,252 --> 00:16:02,706
person has made this judgment that actually this was just kind of this

252
00:16:02,728 --> 00:16:05,570
silly person, this human error,

253
00:16:05,910 --> 00:16:09,554
and that was the cause. Right. But that's really

254
00:16:09,672 --> 00:16:13,460
not helpful and doesn't help us improve in the future.

255
00:16:15,290 --> 00:16:19,190
Human error, to be clear, should never

256
00:16:19,260 --> 00:16:23,154
really be considered as a major factor

257
00:16:23,202 --> 00:16:26,806
of an incident. Right. It's rarely ever the case, and it just

258
00:16:26,908 --> 00:16:29,580
removes every opportunity for us to learn.

259
00:16:30,910 --> 00:16:34,362
It's very easy to say, just don't make that mistake next time.

260
00:16:34,416 --> 00:16:37,674
Right? But how is that person allowed to make that mistake by

261
00:16:37,712 --> 00:16:41,326
the system? So, yeah, we need to think of a way that we

262
00:16:41,348 --> 00:16:44,526
can explain things in a more factual way

263
00:16:44,628 --> 00:16:47,310
and avoid normative language.

264
00:16:50,370 --> 00:16:52,560
Mechanistic resolving. Right.

265
00:16:53,730 --> 00:16:56,926
I kind of almost like to think of this in a

266
00:16:56,948 --> 00:17:00,514
kind of Scooby Doo type way, right. And bear with

267
00:17:00,552 --> 00:17:04,146
me because this is a little bit stretching it, but we

268
00:17:04,168 --> 00:17:07,666
would have gotten away with it, too, if it hadn't been for that meddling regional

269
00:17:07,698 --> 00:17:11,238
failure on our database service. Right? That's my

270
00:17:11,324 --> 00:17:14,840
Scooby Doo kind of villain impression. Sorry.

271
00:17:15,370 --> 00:17:19,066
But what I'm trying to get at, right, is that there

272
00:17:19,088 --> 00:17:23,260
is a temptation to reduce really complex failures to simple

273
00:17:23,710 --> 00:17:27,354
outcomes, right? So this happened

274
00:17:27,472 --> 00:17:31,274
because of that, right. Everything failed simply

275
00:17:31,322 --> 00:17:34,894
because of our regional database failure. Right?

276
00:17:34,932 --> 00:17:38,078
And everything can be explained by this simple thing.

277
00:17:38,244 --> 00:17:41,806
And this kind of leads us to fall into this

278
00:17:41,828 --> 00:17:45,346
trap of there being a single root cause. But we know from our

279
00:17:45,368 --> 00:17:48,770
definition, right, that it's often a set of actions.

280
00:17:49,830 --> 00:17:53,154
So, yeah, we need to avoid that,

281
00:17:53,192 --> 00:17:57,238
because very, very rarely, if ever, I'm not sure I've ever seen one.

282
00:17:57,324 --> 00:18:00,470
Is there actually a root cause?

283
00:18:00,620 --> 00:18:04,470
So, mechanistic reasoning, right? It simplifies the issues

284
00:18:04,540 --> 00:18:07,734
faced. It's almost impossible for there

285
00:18:07,772 --> 00:18:11,194
to be a single root cause. Yet this can often lead to that

286
00:18:11,232 --> 00:18:14,934
thinking. Normally, there are a series of contributing

287
00:18:14,982 --> 00:18:18,826
factors, right? And again, it's removing the opportunity

288
00:18:18,928 --> 00:18:22,800
for learning because we've got that root cause. It was that.

289
00:18:23,170 --> 00:18:26,670
But if we know it was that, then, okay, fine,

290
00:18:26,740 --> 00:18:29,722
solve that. But there are normally, as I said, contributing factors.

291
00:18:29,786 --> 00:18:33,566
So much else going on around it. So we

292
00:18:33,588 --> 00:18:36,418
need to be able to think of those as well and learn on all of

293
00:18:36,424 --> 00:18:40,430
those things. So, yeah, we need to be exploratory

294
00:18:40,510 --> 00:18:44,274
in our approach, right? We need to have a way to evolve our thinking as

295
00:18:44,312 --> 00:18:47,746
new information arrives and understand that fundamentally,

296
00:18:47,778 --> 00:18:50,722
we may never actually know all of the causes,

297
00:18:50,786 --> 00:18:54,166
right? But hopefully we can learn from as many as

298
00:18:54,188 --> 00:18:54,920
we can.

299
00:18:57,950 --> 00:19:02,010
And Richard Cook, who I've mentioned

300
00:19:02,080 --> 00:19:06,202
a couple of times already, and again, strongly recommend looking

301
00:19:06,256 --> 00:19:10,326
out at, kind of described this as above the line and below

302
00:19:10,368 --> 00:19:13,694
the line. And this is a really, really important concept when

303
00:19:13,732 --> 00:19:16,942
we think about our systems and stuff,

304
00:19:16,996 --> 00:19:20,686
right? Because the complex system isn't just the technology,

305
00:19:20,868 --> 00:19:24,274
right? It's actually everything involved, and that's the

306
00:19:24,312 --> 00:19:28,162
humans as well. So the technology

307
00:19:28,296 --> 00:19:31,586
is below the line, and that's here.

308
00:19:31,608 --> 00:19:35,490
I've got the code, the infrastructure, other tech stuff. This is my

309
00:19:35,560 --> 00:19:39,714
simplified version of it, by the way. And we are above

310
00:19:39,762 --> 00:19:43,430
the line, and we don't actually see the code

311
00:19:43,500 --> 00:19:46,886
running on the computer. We don't kind of see

312
00:19:46,988 --> 00:19:50,794
zeros and ones traveling between network interfaces and

313
00:19:50,832 --> 00:19:54,890
everything else like that, right? Or deployments going

314
00:19:54,960 --> 00:19:58,586
from our CI CD platform to our

315
00:19:58,608 --> 00:20:01,200
infrastructure. We don't actually see that.

316
00:20:01,970 --> 00:20:05,166
We interact with that, right? And we

317
00:20:05,188 --> 00:20:08,510
can only infer what's going on beneath that, right?

318
00:20:08,580 --> 00:20:12,094
Based on our interactions. And the fact is that

319
00:20:12,132 --> 00:20:15,746
we are actually the adaptive part

320
00:20:15,768 --> 00:20:19,266
of the system. We are the ones that introduce change,

321
00:20:19,368 --> 00:20:22,830
right? We introduce change to our code by making releases.

322
00:20:22,910 --> 00:20:26,274
We introduce change to our architecture. But we

323
00:20:26,312 --> 00:20:29,506
make new deployments, we make new releases, we introduce

324
00:20:29,538 --> 00:20:32,934
new features, everything else like that, right? We are the ones

325
00:20:32,972 --> 00:20:36,966
that are introducing change. We are the adaptive parts of our system, and we

326
00:20:36,988 --> 00:20:41,786
are above the line, okay? And this

327
00:20:41,808 --> 00:20:45,100
is kind of an ongoing thing, and it's all

328
00:20:45,950 --> 00:20:50,206
changing the way that our systems are

329
00:20:50,228 --> 00:20:54,362
being. Changing how our catastrophe occurs

330
00:20:54,426 --> 00:20:58,720
or can occur. And also,

331
00:20:59,570 --> 00:21:03,338
what should I say? Well, I think the best way to unthink about

332
00:21:03,364 --> 00:21:06,654
this, right, is that if everyone stopped

333
00:21:06,702 --> 00:21:10,130
working on your system, right, there was no more human

334
00:21:10,200 --> 00:21:14,146
interaction whatsoever in terms of supporting it or anything else like that.

335
00:21:14,248 --> 00:21:17,858
Deployments, how long would that system survive?

336
00:21:18,034 --> 00:21:21,986
That service, that thing? How long before users

337
00:21:22,018 --> 00:21:25,670
would be complaining? So it's a really important

338
00:21:25,820 --> 00:21:30,042
being to understand is that we are actually part of the system and our

339
00:21:30,096 --> 00:21:35,130
behavior is

340
00:21:35,200 --> 00:21:38,860
impacting the system. And even more than that,

341
00:21:39,230 --> 00:21:42,942
we are the ones that define what an incident is. Again,

342
00:21:42,996 --> 00:21:47,370
I mentioned our definition of an incident, right? Undesirable behavior.

343
00:21:47,450 --> 00:21:50,554
Who defines what is undesirable

344
00:21:50,602 --> 00:21:54,334
behavior? We define what is undesirable

345
00:21:54,382 --> 00:21:57,906
behavior. But also, as we are part of

346
00:21:57,928 --> 00:22:01,506
the system, if we improve our behavior, then we are

347
00:22:01,528 --> 00:22:04,498
actually improving the system's behavior. Because again,

348
00:22:04,664 --> 00:22:06,760
we are part of the system.

349
00:22:08,890 --> 00:22:12,790
And I said we introduce change,

350
00:22:12,860 --> 00:22:16,280
we are the adaptive part. But as we introduce change,

351
00:22:17,050 --> 00:22:19,770
we introduce new forms of failure.

352
00:22:20,830 --> 00:22:24,870
And as we resolving one incident

353
00:22:25,030 --> 00:22:28,982
or one type of failure, then we are potentially introducing

354
00:22:29,046 --> 00:22:32,634
a new one, right? A new weird and wonderful

355
00:22:32,682 --> 00:22:34,910
way in which our service can fail.

356
00:22:35,330 --> 00:22:38,798
And again, this is another richard looks thing from

357
00:22:38,964 --> 00:22:42,574
how complex systems fail. It's even worse than

358
00:22:42,612 --> 00:22:45,794
that, right. In that as we make our systems more

359
00:22:45,832 --> 00:22:49,140
resilient to the types of failures that we've already seen,

360
00:22:49,910 --> 00:22:53,694
it actually takes more for the system to fail,

361
00:22:53,822 --> 00:22:57,010
right? So the next failure is likely to be even

362
00:22:57,080 --> 00:23:00,294
more catastrophic because each time we're kind of making

363
00:23:00,332 --> 00:23:03,734
it more resilient, we're fixing it, we're preventing all

364
00:23:03,772 --> 00:23:07,142
of these kind of smaller failures. It's just going to take a

365
00:23:07,196 --> 00:23:10,602
bigger thing for it to go wrong. But the fact

366
00:23:10,656 --> 00:23:15,514
is that we can't stop change. We need to

367
00:23:15,552 --> 00:23:19,914
have change, right? It's part of our

368
00:23:19,952 --> 00:23:23,146
jobs and for good reason, right? We do need to

369
00:23:23,168 --> 00:23:26,446
release new features, we need to stay ahead of the competition. We need to do

370
00:23:26,468 --> 00:23:30,206
all these things. We have to introduce change. Change cannot stop,

371
00:23:30,308 --> 00:23:33,586
but it is introducing new forms of failure. And it's important that

372
00:23:33,608 --> 00:23:34,900
we're aware of that.

373
00:23:37,750 --> 00:23:40,900
So when we think about these things,

374
00:23:41,830 --> 00:23:45,582
been looking at research and actually how people go around troubleshooting,

375
00:23:45,646 --> 00:23:49,478
right? And the fact is that some people just seem to be understand

376
00:23:49,564 --> 00:23:53,782
what's going, seem to be able to understand what's going on much

377
00:23:53,836 --> 00:23:57,506
more effectively than others. They seem to

378
00:23:57,548 --> 00:24:00,922
kind of smell what's going on,

379
00:24:00,976 --> 00:24:04,650
right. They've got this

380
00:24:04,720 --> 00:24:08,474
evolved approach to troubleshooting. Right? And the

381
00:24:08,512 --> 00:24:12,078
fact is that they are not working out

382
00:24:12,244 --> 00:24:15,774
how things are breaking in the same way as people who are new

383
00:24:15,812 --> 00:24:19,422
to a service or new to a technology have,

384
00:24:19,476 --> 00:24:22,734
right. When we first

385
00:24:22,852 --> 00:24:27,358
start out troubleshooting, we rely on our system and domain

386
00:24:27,454 --> 00:24:30,466
knowledge, right? We know the technology, we have,

387
00:24:30,488 --> 00:24:34,514
our mental model and we kind of try to work out what's broken based

388
00:24:34,552 --> 00:24:38,214
on these things. Well, this thing is calling that thing.

389
00:24:38,332 --> 00:24:41,734
So let's look at that. And we kind of build

390
00:24:41,772 --> 00:24:46,694
a base hypothesis on what

391
00:24:46,732 --> 00:24:50,950
we think can be going on based on these understandings

392
00:24:51,030 --> 00:24:54,042
of the technology, right,

393
00:24:54,096 --> 00:24:57,526
and the service. But actually, as we evolve

394
00:24:57,638 --> 00:25:01,566
our troubleshooting experience and let's become more experienced with

395
00:25:01,588 --> 00:25:05,102
our system, we actually move lets from our

396
00:25:05,156 --> 00:25:08,766
understanding of these things and more to our experience

397
00:25:08,868 --> 00:25:10,880
in what we've actually seen before,

398
00:25:11,970 --> 00:25:15,698
we can start to build hypotheses based on how similar

399
00:25:15,784 --> 00:25:18,846
the symptoms are that we see here to symptoms

400
00:25:18,878 --> 00:25:22,734
that we've seen in other incidents. Right. So what's

401
00:25:22,782 --> 00:25:26,434
actually happening is that we're able to build and remove and

402
00:25:26,472 --> 00:25:30,194
such a sedentary cycle through hypotheses much faster.

403
00:25:30,322 --> 00:25:32,998
Right. Because we can start to say, okay,

404
00:25:33,084 --> 00:25:36,918
well, I've seen these signals, these are similar

405
00:25:37,004 --> 00:25:41,130
to these incidents over here. How can I

406
00:25:41,280 --> 00:25:44,806
remove this hypothesis or this hypothesis

407
00:25:44,838 --> 00:25:49,222
that I've seen before from this incident, right. They are literally cycling

408
00:25:49,286 --> 00:25:53,182
through these hypotheses really quickly until

409
00:25:53,236 --> 00:25:56,990
they get to one that seems to fit. And that is how people

410
00:25:57,060 --> 00:26:00,206
are evolving their troubleshooting experience. Right.

411
00:26:00,308 --> 00:26:03,918
But it's clear that this takes a

412
00:26:03,924 --> 00:26:07,422
lot of time and effort and experience. Right. These people haven't.

413
00:26:07,566 --> 00:26:11,410
You can't just turn up to a system and gain this kind of smell and

414
00:26:11,560 --> 00:26:14,370
understanding of what's been before in it. Right.

415
00:26:14,520 --> 00:26:17,720
Experience is really hard one. Right.

416
00:26:19,050 --> 00:26:22,694
But transferring this experience is

417
00:26:22,732 --> 00:26:26,342
really, really hard in most cases in many companies, you actually

418
00:26:26,396 --> 00:26:29,706
need to have been involved in that incident directly to

419
00:26:29,728 --> 00:26:33,260
have any real knowledge of what's going on with it. Right.

420
00:26:34,670 --> 00:26:37,674
About how to apply it in this particular case.

421
00:26:37,792 --> 00:26:40,974
And that's quite difficult to do. And again,

422
00:26:41,172 --> 00:26:44,526
we are part of the system, but there is change on the adaptive part as

423
00:26:44,548 --> 00:26:47,840
well. People come and go. So what do we do?

424
00:26:50,530 --> 00:26:53,954
Well, I think that we can bring

425
00:26:53,992 --> 00:26:58,126
more science into this. Right. And again, I love a definition. So let's

426
00:26:58,158 --> 00:27:01,858
take the ever reliable Wikipedia, its view

427
00:27:01,944 --> 00:27:05,714
on science. So science is a systematic enterprise

428
00:27:05,762 --> 00:27:09,314
that builds and organizes knowledge in the form of testable

429
00:27:09,362 --> 00:27:12,470
explanations and predictions about the universe.

430
00:27:14,250 --> 00:27:17,566
Lovely. Worth noting, we're not talking about the universe,

431
00:27:17,618 --> 00:27:21,482
we're talking about our service or system. So we can reduce the scope quite

432
00:27:21,536 --> 00:27:23,610
drastically, which is fortunate.

433
00:27:24,590 --> 00:27:28,230
And something specific is that scientific

434
00:27:28,390 --> 00:27:32,202
understanding is quite often now based on Karl Popper's theory

435
00:27:32,266 --> 00:27:35,600
of falsifiability, which is really hard to say.

436
00:27:36,610 --> 00:27:39,120
Try and say it three times. And yeah,

437
00:27:39,730 --> 00:27:43,922
anyway, no matter how many observations are

438
00:27:43,976 --> 00:27:46,530
made which confirm a theory,

439
00:27:46,870 --> 00:27:50,878
there is always the possibility that a future observation

440
00:27:50,974 --> 00:27:54,546
could refute it. Right? And there is

441
00:27:54,568 --> 00:27:58,546
a quote here, right. Induction cannot yield certainty.

442
00:27:58,738 --> 00:28:02,694
Science progresses when a theory is shown to be wrong and

443
00:28:02,732 --> 00:28:07,190
a new theory is introduced which better explains the phenomena.

444
00:28:07,630 --> 00:28:11,350
So essentially we learn with each theory

445
00:28:11,430 --> 00:28:16,566
that we disprove, right. And we've

446
00:28:16,598 --> 00:28:20,630
kind of got this thing here that you can see this kind of classic scientific

447
00:28:20,710 --> 00:28:23,950
experiment thing going on, that we've got an observation question,

448
00:28:24,100 --> 00:28:27,966
research hypothesis. We test with an experiment, we analyze the

449
00:28:27,988 --> 00:28:31,294
data and we report the conclusions, but it doesn't end there. We keep

450
00:28:31,332 --> 00:28:34,362
going, right? We can never truly know.

451
00:28:34,516 --> 00:28:37,886
And this is the theory of false viability from Karl Popper.

452
00:28:37,998 --> 00:28:41,726
And I think actually this is something that we can apply to creating

453
00:28:41,758 --> 00:28:45,286
hypotheses within incidents. I think this is

454
00:28:45,308 --> 00:28:48,870
something that actually we can apply that will help us.

455
00:28:49,020 --> 00:28:52,642
So I think it's quite straightforward

456
00:28:52,706 --> 00:28:57,042
actually to convert this scientific method into incident resolution

457
00:28:57,106 --> 00:29:00,554
behaviors. So an observation is our

458
00:29:00,592 --> 00:29:04,662
system is exhibiting an undesirable behavior

459
00:29:04,806 --> 00:29:08,106
brain, based on our definition. And so we do

460
00:29:08,128 --> 00:29:12,298
some research, right? We look at our monitoring and observability systems

461
00:29:12,474 --> 00:29:16,634
and based on what we see, we create a hypothesis on what is most likely

462
00:29:16,682 --> 00:29:20,414
happening. We think

463
00:29:20,452 --> 00:29:23,650
it's this particular thing that's going on because we've seen it in our system,

464
00:29:23,720 --> 00:29:27,426
right? And so we do an experiment and this is

465
00:29:27,448 --> 00:29:31,374
important, we attempt to disprove the hypothesis,

466
00:29:31,502 --> 00:29:34,350
right? Disprove, not prove.

467
00:29:34,510 --> 00:29:38,466
Again, thinking about having the wrong hypothesis

468
00:29:38,498 --> 00:29:42,658
and losing a lot of time, we attempt to disprove our hypothesis.

469
00:29:42,834 --> 00:29:47,282
In our analysis we say, are we able to disprove this hypothesis?

470
00:29:47,426 --> 00:29:50,882
If so, great, okay, that wasn't the thing.

471
00:29:51,036 --> 00:29:54,554
But we learned something, right? And now we can move on to the next

472
00:29:54,592 --> 00:29:58,154
most likely hypothesis and we can repeat it and we

473
00:29:58,192 --> 00:30:01,774
can keep going through this cycle until we're unable to

474
00:30:01,812 --> 00:30:05,754
disprove a hypothesis, at which point this becomes

475
00:30:05,882 --> 00:30:09,566
our most likely working theory, obviously until such time as we

476
00:30:09,588 --> 00:30:12,320
learn anything that disproves it.

477
00:30:13,030 --> 00:30:16,546
And we can either narrow the hypothesis or we can start

478
00:30:16,568 --> 00:30:19,890
to take action based on the hypothesis.

479
00:30:23,430 --> 00:30:28,930
This is kind of a way that we can apply this structured scientific

480
00:30:29,010 --> 00:30:31,750
thinking to an incidents.

481
00:30:32,170 --> 00:30:35,750
And here is a theory,

482
00:30:36,410 --> 00:30:39,834
here is my hypothesis. In fact a

483
00:30:39,872 --> 00:30:44,502
more scientificbased scientific, in fact a more scientific hypothesis

484
00:30:44,566 --> 00:30:48,454
driven approach to how humans perform and document

485
00:30:48,502 --> 00:30:51,950
incident investigations can improve reliability.

486
00:30:53,970 --> 00:30:57,278
Because I'm talking about not just

487
00:30:57,444 --> 00:31:00,894
creating these hypotheses in this way, but ideally doing some

488
00:31:00,932 --> 00:31:04,862
kind of, kind of documentation, writing down, providing some context

489
00:31:04,926 --> 00:31:09,518
because we want to learn from all these things, right? And so here's

490
00:31:09,534 --> 00:31:12,866
one I made earlier. A possible explanation for

491
00:31:12,968 --> 00:31:16,430
high error rate is that there is a high database latency,

492
00:31:16,510 --> 00:31:19,954
blah, blah, blah. I'm not going to read it all out, but we can disprove

493
00:31:20,002 --> 00:31:20,920
this by,

494
00:31:22,570 --> 00:31:26,246
and you might be thinking kind of, why would I want to write all this

495
00:31:26,268 --> 00:31:29,818
stuff out? Why would I want to consider this structured thinking? This seems a lot

496
00:31:29,824 --> 00:31:33,130
of effort. And why indeed,

497
00:31:34,510 --> 00:31:38,826
you know, fair question, this is my hypothesis and

498
00:31:39,008 --> 00:31:40,460
maybe I've got it wrong.

499
00:31:42,370 --> 00:31:46,458
Well, let's have a look at some of the things we've

500
00:31:46,474 --> 00:31:50,458
thought about before, right? Well, but because if we're using Karl Popper's

501
00:31:50,474 --> 00:31:53,582
theory of falsifiability, we're removing bad

502
00:31:53,636 --> 00:31:57,086
avenues of investigation as soon as we can, right? We're attempting

503
00:31:57,118 --> 00:32:00,674
to disprove something rather than just go on improving it and

504
00:32:00,712 --> 00:32:04,642
keeping on proving it, improving it, improving it and proving it until such

505
00:32:04,696 --> 00:32:08,614
time that we don't prove it, right? So we're removing quickly bad avenues of

506
00:32:08,652 --> 00:32:12,854
investigation because we're looking to disprove it

507
00:32:12,892 --> 00:32:16,534
allows for changes in incident changes, right? We do get more information

508
00:32:16,652 --> 00:32:20,182
as incidents go on and they evolve. And so,

509
00:32:20,236 --> 00:32:24,406
like science, it's only ever our current understanding,

510
00:32:24,518 --> 00:32:28,282
right? Until such time that it becomes disproven. And that might happen

511
00:32:28,336 --> 00:32:31,200
because as I said, incidents do change over time.

512
00:32:32,850 --> 00:32:36,154
This can formalize the language used to explain decision

513
00:32:36,202 --> 00:32:39,934
making, right? And this is really, really super important.

514
00:32:40,052 --> 00:32:43,310
We're creating a way to communicate to others in a clear

515
00:32:43,380 --> 00:32:46,814
way, right? We can record our hypothesis,

516
00:32:46,862 --> 00:32:49,870
we can record the outcomes of our experiments,

517
00:32:50,030 --> 00:32:53,698
and this can really improve learning because we have much more information

518
00:32:53,784 --> 00:32:57,474
to work from, right? We have the context of what's going

519
00:32:57,512 --> 00:33:01,542
on. And that provides us with a level

520
00:33:01,596 --> 00:33:05,480
of psychological safety because there is a clear kind of

521
00:33:06,650 --> 00:33:10,246
documented reasons as to why we're doing what we're doing, right? We have the

522
00:33:10,268 --> 00:33:13,706
reasoning behind the decisions. We have the proof of why we did what

523
00:33:13,728 --> 00:33:17,594
we did, right? And there is safety in that because

524
00:33:17,792 --> 00:33:21,274
we can show exactly how we got here. And that's really,

525
00:33:21,312 --> 00:33:24,554
really important. And also, as I said, it avoids

526
00:33:24,602 --> 00:33:28,174
normative language. We're talking about kind of a very scientificbased approach here.

527
00:33:28,212 --> 00:33:31,710
We're talking about things that are based on facts, hypotheses.

528
00:33:32,530 --> 00:33:36,206
We are looking at this hypothesis until such time as it's

529
00:33:36,398 --> 00:33:40,066
disproven. We've got our little kind of. There you go,

530
00:33:40,248 --> 00:33:43,582
our little hypothesis there. It's factual language,

531
00:33:43,646 --> 00:33:48,310
right? So we're avoiding all our normative language and our mechanistical reasoning.

532
00:33:49,210 --> 00:33:52,802
And yeah, hindsight bias is reduced

533
00:33:52,866 --> 00:33:56,614
as there is context, right? If we are going through

534
00:33:56,652 --> 00:34:00,074
all of these hypotheses, people are able to see after the

535
00:34:00,112 --> 00:34:03,194
event why we did what we did.

536
00:34:03,312 --> 00:34:06,582
There is context, okay? This was their hypothesis.

537
00:34:06,646 --> 00:34:09,994
They thought it. Because of this, they disproved it. Okay?

538
00:34:10,032 --> 00:34:13,200
It makes sense that they went on to there. We can see their thinking.

539
00:34:14,770 --> 00:34:18,014
So you might be thinking, okay, that sounds actually pretty

540
00:34:18,052 --> 00:34:20,794
good, but how do I even create a hypothesis?

541
00:34:20,922 --> 00:34:24,420
Well, John Osborne, again,

542
00:34:24,870 --> 00:34:27,460
very great person in this field,

543
00:34:28,550 --> 00:34:32,002
his actual thesis that he did, looked into

544
00:34:32,056 --> 00:34:36,194
this and failures in kind of Internet

545
00:34:36,242 --> 00:34:40,502
services. Right. And so he

546
00:34:40,556 --> 00:34:43,862
produced these steps. The first being

547
00:34:43,916 --> 00:34:47,794
is to look for changes. And I think this is something that actually we've

548
00:34:47,842 --> 00:34:51,574
all seen quite often, and certainly I've seen it before in incidents,

549
00:34:51,702 --> 00:34:55,414
an incident is created and the first thing is what's

550
00:34:55,462 --> 00:34:59,126
changed. And we know that change is a very common source of failure.

551
00:34:59,158 --> 00:35:01,660
And so it's a really great place to start.

552
00:35:03,550 --> 00:35:07,322
If that doesn't help you, if maybe you've created some hypotheses

553
00:35:07,386 --> 00:35:10,494
based on what's changed and you've disproven them all,

554
00:35:10,612 --> 00:35:14,298
then you go wide, but you don't go deep. So what

555
00:35:14,324 --> 00:35:18,062
this means is think of lots and lots of different hypotheses.

556
00:35:18,126 --> 00:35:21,634
Maybe every team that's involved is asked to think of

557
00:35:21,672 --> 00:35:25,170
maybe one or two hypotheses. Right. Keep it high level,

558
00:35:25,240 --> 00:35:28,354
though, and don't go too deep. Try and disprove

559
00:35:28,402 --> 00:35:31,480
as many of these as possible. Right.

560
00:35:32,010 --> 00:35:35,606
You can always zoom into these later. As I said earlier, once you've got

561
00:35:35,628 --> 00:35:38,882
a hypothesis and you can't disprove it, maybe you can narrow the scope,

562
00:35:38,946 --> 00:35:42,310
but for the moment, widen the net, think of lots of different things,

563
00:35:42,380 --> 00:35:43,800
try and disprove them,

564
00:35:45,850 --> 00:35:48,706
and then don't always forget Occam's razor.

565
00:35:48,738 --> 00:35:52,686
Right. Simplest is often the

566
00:35:52,708 --> 00:35:55,822
most common. Right. So when going through these things,

567
00:35:55,876 --> 00:35:59,470
when starting to think of more hypotheses, think of Occam's razor.

568
00:36:00,770 --> 00:36:04,754
And that is kind of my talk, really. And hopefully that's given

569
00:36:04,792 --> 00:36:07,986
you some food for thought on how you can

570
00:36:08,088 --> 00:36:11,726
improve or start or whatever your incident

571
00:36:11,838 --> 00:36:15,794
response. And I'll leave you with this, another Richard

572
00:36:15,842 --> 00:36:18,710
Cook quote, because he is so influential.

573
00:36:19,530 --> 00:36:23,106
All practitioner acts are a gamble.

574
00:36:23,218 --> 00:36:27,142
Right. With science, there is no 100%

575
00:36:27,196 --> 00:36:30,634
guarantee, there is just a hypothesis that we

576
00:36:30,672 --> 00:36:34,166
can't disprove. Right. We have to accept

577
00:36:34,198 --> 00:36:38,186
this, particularly during incidents. Right. We never know for absolute

578
00:36:38,288 --> 00:36:41,774
certain that what we're going to do really is going to fix something. There is

579
00:36:41,812 --> 00:36:45,946
always the opportunity for another level of catastrophe

580
00:36:46,058 --> 00:36:49,694
because the actions that we take are above the line and

581
00:36:49,732 --> 00:36:53,434
we're interacting with things that we don't understand about. They exist

582
00:36:53,482 --> 00:36:57,498
below the line. So we don't know for certain. And therefore it's

583
00:36:57,514 --> 00:37:01,242
a gamble. But hopefully by introducing more scientific

584
00:37:01,306 --> 00:37:05,542
method, by recording our actions, by providing more

585
00:37:05,676 --> 00:37:08,966
structured response, we can reduce the size of this

586
00:37:08,988 --> 00:37:12,614
gamble. And also, if not, or whatever happens,

587
00:37:12,732 --> 00:37:16,246
we have these actions recorded that allow us to learn for the

588
00:37:16,268 --> 00:37:19,142
future. So that's it.

589
00:37:19,196 --> 00:37:22,690
That's me. Thank you very much and have a great conference.

