1
00:00:25,570 --> 00:00:29,606
You. Hello everyone. Thank you for having me.

2
00:00:29,788 --> 00:00:33,282
Today we're going to talk about ambience Mesh. My name is Abdel.

3
00:00:33,426 --> 00:00:36,870
I'm a cloud developer advocate with Google. I work

4
00:00:36,940 --> 00:00:39,954
on Kubernetes and service mesh.

5
00:00:40,082 --> 00:00:42,966
I've been with the company for almost ten years. I'm also a co host of

6
00:00:42,988 --> 00:00:46,646
a Kubernetes podcast called the Kubernetes podcast from Google. And I'm also

7
00:00:46,668 --> 00:00:50,606
a CNCF ambassador and that's my Twitter x if

8
00:00:50,628 --> 00:00:54,494
you want to reach out about anything related to this topic. So today

9
00:00:54,532 --> 00:00:57,310
I'm going to do an introduction to ambience mesh. But before I get going,

10
00:00:57,460 --> 00:01:00,654
we have to understand what istio is and what

11
00:01:00,692 --> 00:01:03,970
does it do. So Istio is a service mesh tool.

12
00:01:04,040 --> 00:01:07,700
It's open source, it is part of the CNCF landscape. It's actually

13
00:01:08,070 --> 00:01:10,610
a graduated project in the CNCF.

14
00:01:11,830 --> 00:01:15,246
Like any Kubernetes tool, it basically follows the same architecture

15
00:01:15,278 --> 00:01:18,502
which is based on a control plane and data plane. In the

16
00:01:18,556 --> 00:01:22,178
case of istio, the control plane is literally called Istio,

17
00:01:22,274 --> 00:01:25,314
which is a deployment. If you are familiar with Kubernetes,

18
00:01:25,362 --> 00:01:29,174
it's essentially a deployment that runs in the istio

19
00:01:29,222 --> 00:01:32,662
system namespace and it's completely stateless,

20
00:01:32,726 --> 00:01:36,006
so it can be scaled up and down depending on traffic.

21
00:01:36,118 --> 00:01:39,478
The data plane for Istio is what we call the proxies.

22
00:01:39,654 --> 00:01:43,706
These are based on an open source proxy called Invoi.

23
00:01:43,898 --> 00:01:47,466
So Invoi is C proxy, which was written and open sourced

24
00:01:47,498 --> 00:01:51,086
by Lyft and they have just made it available for everybody to

25
00:01:51,108 --> 00:01:54,734
use. And the way issue works is

26
00:01:54,772 --> 00:01:57,874
essentially you would deploy the control plane, then you

27
00:01:57,912 --> 00:02:01,298
would typically label namespaces in kubernetes to

28
00:02:01,304 --> 00:02:04,638
say, I want this namespace to be part of the service mesh. And I'm

29
00:02:04,654 --> 00:02:08,454
going to explain why we say a service mesh later. And what

30
00:02:08,492 --> 00:02:13,030
would happen is the proxy will be automatically injected next to your workloads

31
00:02:13,370 --> 00:02:16,530
and it will be set up such a way that it transparently

32
00:02:16,610 --> 00:02:20,554
intercept traffic coming in and out of your application. So in the

33
00:02:20,592 --> 00:02:24,426
example you see on the screen, the little

34
00:02:24,528 --> 00:02:27,850
rectangle, the gray rectangle is the pod. Inside the pod you have

35
00:02:27,920 --> 00:02:31,790
the service, which is one container, and then the proxy is a second container.

36
00:02:33,410 --> 00:02:36,026
It has a name. In Kubernetes we call it sidecar.

37
00:02:36,138 --> 00:02:39,902
Sidecar is not a Kubernetes native object per se.

38
00:02:39,956 --> 00:02:43,490
It's more like, well, that's not correct.

39
00:02:43,560 --> 00:02:47,422
Since Kubernetes 1.28 kubernetes

40
00:02:47,486 --> 00:02:50,990
have implemented a way to handle sidecars

41
00:02:51,070 --> 00:02:54,366
in a more native way. But sidecars

42
00:02:54,398 --> 00:02:57,686
is just a common term. It's just like an agreed on kind of

43
00:02:57,708 --> 00:03:01,554
pattern where we decided that inside a pod, if you have two containers

44
00:03:01,602 --> 00:03:05,362
and one container is providing extra features,

45
00:03:05,426 --> 00:03:09,446
then we would call it a sidecar. So when

46
00:03:09,468 --> 00:03:12,934
your pod boots, the proxy would boot, it will connect to the istio control plane,

47
00:03:12,982 --> 00:03:16,586
download all its configuration, including any policies to

48
00:03:16,608 --> 00:03:19,306
enforce, any routing to do, where to send,

49
00:03:19,328 --> 00:03:23,046
telemetry, et cetera, et cetera. It also downloads the entire routing table,

50
00:03:23,158 --> 00:03:26,718
so it keeps in memory a list of all the other pods, pods in

51
00:03:26,724 --> 00:03:29,806
kubernetes that are represented by an API port. So the proxy is aware of

52
00:03:29,828 --> 00:03:33,034
all the other pods and then any certificates.

53
00:03:33,082 --> 00:03:36,846
And one of the key things that people use Kubernetes istio for is mtls,

54
00:03:36,878 --> 00:03:40,862
so mutual tls where you have certificates on both the client and server.

55
00:03:41,006 --> 00:03:44,340
So this is how istio stands today.

56
00:03:45,830 --> 00:03:49,362
It has also like a set of, we call them special proxies.

57
00:03:49,426 --> 00:03:52,866
One of them is the ingress gateway and the other one is the egress gateway.

58
00:03:52,978 --> 00:03:56,642
They are just standalone invoice proxies that also are configured

59
00:03:56,706 --> 00:04:00,086
through the control plane and they are used, one of them is

60
00:04:00,108 --> 00:04:03,098
used for ingress, so ill traffic coming from outside the service mesh to inside the

61
00:04:03,104 --> 00:04:06,538
service mesh, and you can also use it to enforce some policies on

62
00:04:06,544 --> 00:04:10,694
the perimeter. And then the egress gateway is used for traffic leaving

63
00:04:10,742 --> 00:04:13,838
the service mesh, which also can be used to implement some policy

64
00:04:13,924 --> 00:04:17,358
enforcement. Now why do we call this a

65
00:04:17,364 --> 00:04:20,986
service mesh? Where is the term mesh coming from? Well, if you're familiar

66
00:04:21,018 --> 00:04:25,086
with Kubernetes in Kubernetes space, you would use a service capital s

67
00:04:25,268 --> 00:04:28,786
as a way to implement service discovery and load balancing. So if you have two

68
00:04:28,808 --> 00:04:31,870
applications, application a, application b inside the Kubernetes cluster,

69
00:04:31,950 --> 00:04:35,586
you would create a service for application b, and then that service would

70
00:04:35,608 --> 00:04:39,266
create a DNS entry in kubedns or whatever DNS you're using

71
00:04:39,288 --> 00:04:43,366
core DNS or whatever. And then you would use that sqdn of the service in

72
00:04:43,388 --> 00:04:46,774
order to a discover all the pods behind the service and b have

73
00:04:46,812 --> 00:04:49,800
a single point of entry toward the service.

74
00:04:50,330 --> 00:04:53,462
Typically a service gives you a stable vip virtual ip,

75
00:04:53,606 --> 00:04:56,906
and then regardless of where you are in the cluster, if service a

76
00:04:56,928 --> 00:05:00,362
is talking to service b or application a is talking to application b. So application

77
00:05:00,416 --> 00:05:03,898
a would use service b for service discovery,

78
00:05:03,994 --> 00:05:07,866
it will get an IP address, in return, it will send traffic to that IP

79
00:05:07,898 --> 00:05:11,434
address, and then some magic behind which is typically

80
00:05:11,482 --> 00:05:14,778
implemented through IP tables or other mechanisms would implement load

81
00:05:14,794 --> 00:05:18,094
balancing, which is typically a route robin in a service mesh scenario,

82
00:05:18,142 --> 00:05:21,266
that's completely different. Services which are again capital s

83
00:05:21,288 --> 00:05:25,202
services are still used for service discovery, so you still implement service

84
00:05:25,256 --> 00:05:28,598
discovery from an application perspective. The same way if you have service b, you have

85
00:05:28,604 --> 00:05:31,766
to create a Kubernetes service for it. And then you would use that

86
00:05:31,788 --> 00:05:35,560
Kubernetes service to call it from service a or from application a.

87
00:05:36,570 --> 00:05:40,382
The communication path is completely different because since the proxies

88
00:05:40,466 --> 00:05:43,930
know each other, and since every proxy know every other proxy,

89
00:05:44,270 --> 00:05:47,898
the service discovery is implemented the same way. But at the

90
00:05:47,904 --> 00:05:51,366
moment when service a application a sends traffic, that traffic is intercepted

91
00:05:51,398 --> 00:05:55,086
by the proxy and the proxy sends traffic directly to

92
00:05:55,108 --> 00:05:58,558
the other proxies that represent service b directly to

93
00:05:58,564 --> 00:06:02,190
their IP address port. So the VIP is not used for traffic routing.

94
00:06:02,610 --> 00:06:05,946
That cluster IP created by the service that you have created

95
00:06:05,978 --> 00:06:08,686
is not used in this case. And that's why we call it service mesh,

96
00:06:08,718 --> 00:06:12,366
because you have basically a mesh of communication. You have every proxy

97
00:06:12,398 --> 00:06:14,530
or every pod talks to every other pod.

98
00:06:17,530 --> 00:06:25,350
So explained a little bit how

99
00:06:25,420 --> 00:06:28,698
istio works. Istio is used for a lot of

100
00:06:28,704 --> 00:06:32,454
things, including policy enforcement. You can do things like timeout

101
00:06:32,502 --> 00:06:36,090
and retries and circuit breakers. You can do things like authorization,

102
00:06:36,750 --> 00:06:40,166
using jot tokens, or using Spiffey,

103
00:06:40,198 --> 00:06:42,650
which is one of the protocols inside of kubernetes.

104
00:06:43,070 --> 00:06:46,458
You can do a lot of traffic shaping like content routing, canneries a,

105
00:06:46,464 --> 00:06:49,870
b testing, et cetera. So all of these things are implemented in

106
00:06:49,940 --> 00:06:53,838
the infrastructure layer. That's the key point with the service mesh like istio, is that

107
00:06:53,924 --> 00:06:56,578
if you want to implement any of these things as a developer, you will have

108
00:06:56,584 --> 00:06:59,780
to write code for it. With istio, you basically let

109
00:07:01,590 --> 00:07:04,994
the network layer, if you want, handle all these things for you. So the app

110
00:07:05,032 --> 00:07:07,986
itself doesn't have to be even aware that there is a timeout or there is

111
00:07:08,008 --> 00:07:11,302
a retry policy, or there is a circuit breaker or any of these things,

112
00:07:11,356 --> 00:07:14,726
right? The concept of service mesh is

113
00:07:14,748 --> 00:07:18,006
not really new. It existed for a very long time. People had to do

114
00:07:18,028 --> 00:07:21,446
this kind of traffic routing, et cetera.

115
00:07:21,478 --> 00:07:25,034
And it used to be implemented through proxies. Typically what really a service

116
00:07:25,072 --> 00:07:28,700
mesh introduced is just this concept of sidecar that we have talked about, right?

117
00:07:30,690 --> 00:07:34,010
So sidecars

118
00:07:34,090 --> 00:07:35,360
give us a lot of things.

119
00:07:37,810 --> 00:07:41,354
They allowed us to implement kind of network smart

120
00:07:41,402 --> 00:07:44,526
feature in the network, in the infrastructure layer without having

121
00:07:44,548 --> 00:07:46,100
to implement them in the code.

122
00:07:47,510 --> 00:07:51,058
And while they are useful and important, sidecars have

123
00:07:51,064 --> 00:07:54,386
some complications. One of them is that they are very invasive. What do we

124
00:07:54,408 --> 00:07:58,214
mean by invasive? Well, imagine that you have a scenario where you

125
00:07:58,252 --> 00:08:01,878
don't have istio, and the steps in order for you to

126
00:08:01,884 --> 00:08:05,926
implement istio is that you would start by deploying the

127
00:08:06,028 --> 00:08:09,846
control plane. That's just typically just a

128
00:08:09,868 --> 00:08:12,858
deployment. There are a bunch of crds that you have to deploy because in the

129
00:08:12,864 --> 00:08:15,850
istio world it has its own objects for traffic routing.

130
00:08:17,150 --> 00:08:21,306
So all these crds get created and then in

131
00:08:21,328 --> 00:08:24,918
order to add existing applications to the service mesh.

132
00:08:25,014 --> 00:08:27,946
Here I'm talking about scenario where you're going from I don't have a service mesh

133
00:08:27,978 --> 00:08:31,566
to I want to have istio. If you're starting fresh, this is probably not

134
00:08:31,588 --> 00:08:35,066
a problem for you, but then what you would do is you would tag

135
00:08:35,098 --> 00:08:38,466
or label namespaces and then you will have to restart your pods. And that's why

136
00:08:38,488 --> 00:08:42,066
we say it's invasive, because it requires restarting workloads in order for

137
00:08:42,088 --> 00:08:45,640
the proxy to be able to be injected and used.

138
00:08:46,010 --> 00:08:49,800
And that's typically not a problem,

139
00:08:52,810 --> 00:08:56,662
but typically not a problem. Depending on the scenario. If you

140
00:08:56,716 --> 00:09:00,242
don't want to reload your workloads then it would be hard.

141
00:09:00,316 --> 00:09:03,706
And typically what people do is that they would wait until next time they

142
00:09:03,728 --> 00:09:07,894
do upgrades for their kubernetes clusters and then they would install istio,

143
00:09:08,022 --> 00:09:11,866
which is fine, except that you are basically implementing

144
00:09:11,898 --> 00:09:15,738
too many changes and that's typically not recommended

145
00:09:15,754 --> 00:09:18,030
from a change management perspective.

146
00:09:18,770 --> 00:09:22,826
It also doesn't work with some implementations like istio sidecar istio

147
00:09:22,858 --> 00:09:25,866
doesn't implement TCP, doesn't implement TCP,

148
00:09:25,898 --> 00:09:29,342
sorry, it doesn't implement websockets. There are a bunch of things that are not implementable.

149
00:09:29,486 --> 00:09:32,866
Only HTTP communication is implementable. And then

150
00:09:32,888 --> 00:09:35,846
the last thing, and this is like a contention point really, if people have been

151
00:09:35,868 --> 00:09:39,926
using Istio for the last five years, is the

152
00:09:39,948 --> 00:09:43,586
resource requirements in the last benchmark executed

153
00:09:43,618 --> 00:09:47,758
on Istio, I think 1.18 the benchmark

154
00:09:47,794 --> 00:09:51,706
is something like a 0.3 or

155
00:09:51,728 --> 00:09:55,034
0.4 virtual cpu and around 40

156
00:09:55,072 --> 00:09:58,186
or 50 megabytes of memory per sidecar for a

157
00:09:58,208 --> 00:10:02,060
service which is serving 1000 requests per second.

158
00:10:02,590 --> 00:10:06,522
Again, don't forget that there are sidecars per pod. So for each pod

159
00:10:06,586 --> 00:10:10,606
there is your container plus the sidecar. So zero point 35

160
00:10:10,628 --> 00:10:13,806
or 0.4 VCPU and 50GB of memory might not

161
00:10:13,828 --> 00:10:17,122
sound like a lot, but if you are running inside a cluster that contains 1000

162
00:10:17,176 --> 00:10:20,258
containers or 2000 containers, that could add up.

163
00:10:20,424 --> 00:10:24,418
Essentially the moment you add istio, you're doubling up the number of containers in your

164
00:10:24,504 --> 00:10:28,638
cluster and that's an issue. So the community and

165
00:10:28,664 --> 00:10:32,006
the maintainers of istio got together and tried to

166
00:10:32,188 --> 00:10:35,414
figure out a way to solve this. And they came up with this idea

167
00:10:35,452 --> 00:10:39,186
of ambience mesh. So the whole idea of ambient mesh

168
00:10:39,218 --> 00:10:42,554
is to change the data path. The control plane will remain the same and is

169
00:10:42,592 --> 00:10:46,950
the same. The data path, the way we insert intelligence

170
00:10:47,030 --> 00:10:50,746
into the network has

171
00:10:50,768 --> 00:10:53,326
to be implemented through a set of requirements. One of them, it has to be

172
00:10:53,348 --> 00:10:56,638
nondisruptive to workloads. In other terms, adding or

173
00:10:56,644 --> 00:10:59,982
removing the proxies or whatever is going to replace the proxies should be

174
00:11:00,116 --> 00:11:03,954
not transparent, at least

175
00:11:03,992 --> 00:11:08,334
for a while. Ambient mesh should have compatibility with sidecar

176
00:11:08,382 --> 00:11:12,498
based istio, because we are aware that the way people will

177
00:11:12,584 --> 00:11:16,774
implement ambient mesh will be through a migration process of

178
00:11:16,812 --> 00:11:20,246
existing istio workloads. And that's like a very complicated thing

179
00:11:20,268 --> 00:11:23,890
to do. So one of the requirements is traffic interoperability

180
00:11:23,970 --> 00:11:27,542
between traditional sidecars and no

181
00:11:27,596 --> 00:11:31,000
sidecars, which is what ambient mesh is aiming to do.

182
00:11:31,390 --> 00:11:34,474
And then in order to enable it, to disable it, they wanted to implement it

183
00:11:34,512 --> 00:11:37,898
through a simple way. So in the new architecture for

184
00:11:37,904 --> 00:11:41,166
ambient mesh, sidecars are gone and they

185
00:11:41,188 --> 00:11:43,630
are replaced by two types of proxies.

186
00:11:45,010 --> 00:11:48,506
Those proxies try to treat the mesh as two different layers,

187
00:11:48,618 --> 00:11:51,690
secure layer and the layer seven processing layer.

188
00:11:51,850 --> 00:11:55,834
The secured layer is implemented through a proxy per node.

189
00:11:55,962 --> 00:11:59,150
So it's a multitenant per node cluster. So there is no more

190
00:11:59,220 --> 00:12:02,670
per pod proxy anymore. It's per node called

191
00:12:02,740 --> 00:12:06,146
ztunnel. ZTunnel runs as a demon, so it runs one proxy

192
00:12:06,178 --> 00:12:09,846
per node. It's completely stateless, which means it can be scaled up

193
00:12:09,868 --> 00:12:13,746
and down. It has built in authentication

194
00:12:13,778 --> 00:12:17,234
and encryption, and it implements some of the layer four policies

195
00:12:17,282 --> 00:12:21,002
and telemetry. If we want full

196
00:12:21,056 --> 00:12:25,050
layer seven policies, like authorization policies for example,

197
00:12:25,200 --> 00:12:29,190
which require something to look at the HTTP header

198
00:12:29,270 --> 00:12:33,262
to implement the authorization. Then they added another

199
00:12:33,316 --> 00:12:36,414
thing called the waypoint proxy. This is a per

200
00:12:36,532 --> 00:12:40,078
namespace proxy which still uses invoi. So ztunnel is a

201
00:12:40,084 --> 00:12:43,546
new developed proxy in Rust, but the waypoint

202
00:12:43,578 --> 00:12:47,214
proxy implements is

203
00:12:47,252 --> 00:12:50,334
based on invoi. And then they used this new protocol

204
00:12:50,382 --> 00:12:53,534
called Hborn for encryption and authentication.

205
00:12:53,662 --> 00:12:57,314
Now in this new architecture, there are a bunch of things that

206
00:12:57,352 --> 00:13:01,238
we have managed to solve. One of them is if people only want to

207
00:13:01,244 --> 00:13:04,982
do mtls, then you don't have to implement the layer seven

208
00:13:05,036 --> 00:13:08,454
process layer, you don't need it. You can just disable it and just

209
00:13:08,492 --> 00:13:11,930
have the overlay, a discure overlay layer through the ztunnel.

210
00:13:12,830 --> 00:13:16,646
If you want some basic traffic management like through TCP routing,

211
00:13:16,678 --> 00:13:20,230
et cetera, et cetera you can also do that. By the

212
00:13:20,240 --> 00:13:23,946
way, I said earlier that issue doesn't support TCP.

213
00:13:23,978 --> 00:13:27,726
That was wrong. It doesn't support UDP, not TCP. And then

214
00:13:27,748 --> 00:13:31,406
if you want some advanced traffic management or

215
00:13:31,428 --> 00:13:34,878
security with authorization policies, then you can implement the layer

216
00:13:34,894 --> 00:13:38,974
seven processing layer. And through this new architecture, the aim

217
00:13:39,022 --> 00:13:42,706
also is to try to make adopting service mesh as easy as

218
00:13:42,728 --> 00:13:45,970
possible. So this is how the ztunnel looks like.

219
00:13:46,040 --> 00:13:49,698
I talked about the fact that ztunnel runs per node.

220
00:13:49,874 --> 00:13:53,206
You can consider the little purple squares as the

221
00:13:53,228 --> 00:13:56,200
node. Each node has a ztunnel running in it as a demon set,

222
00:13:57,130 --> 00:14:00,774
completely scalable up and down. If there is a lot of traffic, all the containers

223
00:14:00,822 --> 00:14:04,522
in the pod which now don't have sidecars anymore send

224
00:14:04,576 --> 00:14:08,230
traffic to ztunnel. And then the Ztunnel implements HTTP

225
00:14:08,310 --> 00:14:12,730
tunneling as an overlay to basically encrypt traffic as it goes between two nodes.

226
00:14:13,070 --> 00:14:16,446
One of the things also ztunnel does is that it keeps the identity of the

227
00:14:16,468 --> 00:14:19,754
pod. So if you have container

228
00:14:19,882 --> 00:14:22,942
c one or pod c one sending traffic to pod s one,

229
00:14:23,076 --> 00:14:26,754
then s one will see the traffic coming as the identity of pod c

230
00:14:26,792 --> 00:14:30,690
one. So it will see the service account essentially. That's what I'm trying to say.

231
00:14:30,840 --> 00:14:33,614
Then if you want to add those layer seven policies,

232
00:14:33,742 --> 00:14:37,286
then we create the waypoint proxy for you, or you will have to deploy it

233
00:14:37,308 --> 00:14:40,534
manually. And then if

234
00:14:40,572 --> 00:14:43,506
there are any policies to be enforced, then they will be enforced by the waypoint

235
00:14:43,538 --> 00:14:44,310
proxy.

236
00:14:46,890 --> 00:14:50,054
Again, the waypoint proxy ran per namespace, so there is no more sidecars.

237
00:14:50,102 --> 00:14:53,926
Again, so it's just a special proxy that runs somewhere and it's

238
00:14:53,958 --> 00:14:57,578
responsible for one namespace, completely scalable as well if there

239
00:14:57,584 --> 00:14:59,210
is more traffic, because it's stateless.

240
00:15:00,830 --> 00:15:04,566
So we talked quickly about how we'd traditionally

241
00:15:04,598 --> 00:15:08,266
deploy issue service mesh in the traditional deployment

242
00:15:08,298 --> 00:15:11,966
model, which uses sidecars. So you would deploy the control plane and

243
00:15:11,988 --> 00:15:16,126
you would tag namespaces and then restart

244
00:15:16,158 --> 00:15:19,714
them to inject the sidecar in

245
00:15:19,752 --> 00:15:22,866
the new mode with ambient mesh. You don't have to do any of that stuff.

246
00:15:22,888 --> 00:15:26,414
You deploy the control plane, obviously, and then you can just enable

247
00:15:26,462 --> 00:15:29,958
the ztunnel or enable the ztunnel and then the

248
00:15:29,964 --> 00:15:34,050
ctunnel will be implemented through the network CNI, because istio does have a CNI.

249
00:15:34,130 --> 00:15:37,946
So they basically took the CNI and made it work better with the

250
00:15:37,968 --> 00:15:38,890
ztunnel.

251
00:15:41,310 --> 00:15:44,886
So what is Hbond? So traditionally

252
00:15:44,998 --> 00:15:48,822
with istio based proxies in istio sidecar,

253
00:15:48,966 --> 00:15:52,198
every connection from the client creates a new TCP connection

254
00:15:52,294 --> 00:15:55,374
between the proxies. So you see here I have two containers, c,

255
00:15:55,412 --> 00:15:59,546
one and s one container, c one talks to three different ports,

256
00:15:59,738 --> 00:16:03,486
and then for each of those ports there is a new TCP tunnel,

257
00:16:03,518 --> 00:16:05,810
or TCP connection created between the proxies.

258
00:16:08,230 --> 00:16:12,302
So with Hbone, one of the things this protocol

259
00:16:12,366 --> 00:16:16,370
can do is that it can tunnel through the connection through a single MTLs

260
00:16:16,530 --> 00:16:18,360
connection using HTTP connect.

261
00:16:19,770 --> 00:16:22,998
So it's actually better performance than

262
00:16:23,084 --> 00:16:26,374
sidecars. And although this is what

263
00:16:26,412 --> 00:16:30,530
Hbone is able to do, by the way, this is actually not visually correct

264
00:16:30,620 --> 00:16:33,878
because there are no sidecars. It's the ztunnels

265
00:16:33,894 --> 00:16:37,706
talking to each other. And the ztunnels will have a single MTLs connection and

266
00:16:37,728 --> 00:16:40,330
they will tunnel all traffic through that connection.

267
00:16:41,710 --> 00:16:45,134
I don't have a demo, so I just want to quickly talk about

268
00:16:45,172 --> 00:16:49,482
some stuff that are important to keep in mind in istio traffic

269
00:16:49,626 --> 00:16:53,070
management or in the existing sidecar based proxy.

270
00:16:53,650 --> 00:16:57,342
This is typically how you would do traffic management. So if you're familiar with Kubernetes,

271
00:16:57,406 --> 00:17:00,526
you know that you create deployments and services and stuff. But if you add istio,

272
00:17:00,638 --> 00:17:04,098
then you remember all the crds I talked about.

273
00:17:04,184 --> 00:17:07,506
All these CRDs gives you objects that allows you to do

274
00:17:07,528 --> 00:17:11,154
traffic management in istio. So one of them, for example, here is an example.

275
00:17:11,192 --> 00:17:14,374
I have a virtual service and a destination route. So let's take an example.

276
00:17:14,412 --> 00:17:16,918
We have service a on one side and then we have service b on the

277
00:17:16,924 --> 00:17:21,114
other side, and then we added service b

278
00:17:21,232 --> 00:17:24,714
version two. And I want to send part of the traffic from

279
00:17:24,752 --> 00:17:27,980
service a to service b, in this case, 5%.

280
00:17:28,430 --> 00:17:31,018
I can do that with Kubernetes natively. So what I have to do is I

281
00:17:31,024 --> 00:17:34,426
have to deploy what we call destination rules. The destination

282
00:17:34,458 --> 00:17:38,686
rule, essentially what they do is create like virtual services in a way not

283
00:17:38,708 --> 00:17:42,846
to be confused with the actual objects called virtual service, but they basically take

284
00:17:42,948 --> 00:17:46,618
service b v one and service b v two, and make them look

285
00:17:46,644 --> 00:17:50,386
like two different destinations. And then with the virtual service, then you can

286
00:17:50,568 --> 00:17:54,146
say, I want 5% to be able to send to v two, and then I

287
00:17:54,168 --> 00:17:58,246
want 95% to be sent to v one. Right? And because of

288
00:17:58,268 --> 00:18:01,634
the mesh concept I talked about earlier, the sidecar

289
00:18:01,682 --> 00:18:05,126
on the service a side is able to do that fine grained tuning of

290
00:18:05,148 --> 00:18:08,806
sending traffic between a and b. What happened over

291
00:18:08,828 --> 00:18:12,774
the last few years or so is that Kubernetes, or the Kubernetes

292
00:18:12,822 --> 00:18:16,598
community have worked on a new API, a new open source

293
00:18:16,614 --> 00:18:20,330
API called the Gateway API. So the gateway API is essentially a set of APIs

294
00:18:20,910 --> 00:18:24,298
that are going to be the next generation ingress.

295
00:18:24,474 --> 00:18:27,978
They will eventually replace Ingress as an API.

296
00:18:28,074 --> 00:18:31,706
And the Gateway API was implemented with a bunch of kind of lesson learned

297
00:18:31,738 --> 00:18:36,210
from the Ingress API. One of them is being able to do things natively

298
00:18:37,510 --> 00:18:40,946
in the API itself instead of just relying on extra crds or

299
00:18:40,968 --> 00:18:44,546
extra annotations. If you have implemented ingress in

300
00:18:44,568 --> 00:18:47,922
istio in a service mesh, sorry, in Kubernetes,

301
00:18:47,986 --> 00:18:52,114
you would know that it can get very long, because the Ingress

302
00:18:52,162 --> 00:18:55,922
API in Kubernetes solves the most common denominator

303
00:18:55,986 --> 00:18:59,638
across all cloud providers and across all the open source tools that exist.

304
00:18:59,814 --> 00:19:03,606
And it is up to each cloud provider

305
00:19:03,638 --> 00:19:07,434
and each open source tool, each gateway API, each whatever,

306
00:19:07,552 --> 00:19:11,306
to add that layer of

307
00:19:11,408 --> 00:19:15,054
customization they need. And those annotations that you

308
00:19:15,092 --> 00:19:18,958
see in an ingress object, they are typically not compatible with each other.

309
00:19:19,124 --> 00:19:23,246
The Gateway API had in mind to be able to have a single

310
00:19:23,348 --> 00:19:26,530
standard mode of implementing most of what people care about,

311
00:19:26,600 --> 00:19:30,674
things like routing rules and path based and

312
00:19:30,712 --> 00:19:36,626
host based routing rules and those kind of things, right? And so the

313
00:19:36,648 --> 00:19:39,510
Gateway API comes in three different objects.

314
00:19:40,010 --> 00:19:43,394
So you would have what we call a gateway class, a gateway and an HTTP

315
00:19:43,442 --> 00:19:46,194
route. There are also TCP routes and there are TLS routes.

316
00:19:46,322 --> 00:19:50,202
But a gateway class essentially is something that

317
00:19:50,256 --> 00:19:53,610
the cloud provider implements for you or installs for you

318
00:19:53,680 --> 00:19:57,514
that defines the type of load balancer the gateway objects create. An actual

319
00:19:57,552 --> 00:20:00,902
load balancer. The HTTP route is what maps

320
00:20:00,966 --> 00:20:04,350
the actual service or the backend toward that load balancer.

321
00:20:05,330 --> 00:20:09,054
You can have multiple personas deploying these things in the ingress space.

322
00:20:09,172 --> 00:20:12,526
It's up to the service owner inside the namespace to

323
00:20:12,548 --> 00:20:16,238
deploy the ingress object to expose their application to the outside of the cluster.

324
00:20:16,414 --> 00:20:20,238
Here you can have a platform admin implement the load balancers

325
00:20:20,254 --> 00:20:24,014
for you, and then it's up to each service owner to implement their own routing

326
00:20:24,062 --> 00:20:27,550
rules. Also, one of the key implementation details of

327
00:20:27,560 --> 00:20:31,314
the Gateway API is the fact that you can do cross namespace

328
00:20:31,442 --> 00:20:35,222
routing, which you couldn't do with ingress. This is just an example

329
00:20:35,356 --> 00:20:38,754
where you would have the gateway object called Foo,

330
00:20:38,882 --> 00:20:42,666
which is using a gateway class provided by the cloud provider or

331
00:20:42,688 --> 00:20:46,826
by the infrastructure provider. In that foo gateway object,

332
00:20:46,928 --> 00:20:50,026
which deploys the load balancer, you can decide what the domain is, you can have

333
00:20:50,048 --> 00:20:53,766
TLS certificates, you can have policies, et cetera, and then you can allow the store

334
00:20:53,808 --> 00:20:57,326
developer and the site developer. These are two different namespaces, two different apps to

335
00:20:57,348 --> 00:21:00,894
use HTTP route to route how traffic gets from the load balancer to their back

336
00:21:00,932 --> 00:21:04,674
end. This is how an object looks like.

337
00:21:04,712 --> 00:21:08,738
And there is a reason why I'm talking about this because

338
00:21:08,904 --> 00:21:12,050
in the context of ambient mesh.

339
00:21:12,710 --> 00:21:16,146
So this is an example where you have an HTTP route that says if I

340
00:21:16,168 --> 00:21:19,966
have hostname foo.com, if I have a rule that matches

341
00:21:19,998 --> 00:21:23,398
the header cannery, then I would send it to the cannery version of the service.

342
00:21:23,564 --> 00:21:27,830
Otherwise I just want to send to existing version.

343
00:21:28,250 --> 00:21:31,994
You can also do things like weight based bumping, like 80%,

344
00:21:32,032 --> 00:21:36,122
20%, stuff like that. So what's happening right now is that

345
00:21:36,176 --> 00:21:39,386
the istio community have decided that they are going

346
00:21:39,408 --> 00:21:42,666
to take the Gateway API and use it as

347
00:21:42,688 --> 00:21:45,520
a way to do traffic management. There are multiple reasons for this.

348
00:21:46,690 --> 00:21:50,346
The main reason, the straight point reason, is no one needs more crds.

349
00:21:50,378 --> 00:21:53,774
So we're trying to get rid of crds. That's reason number one. Reason two,

350
00:21:53,812 --> 00:21:58,302
since the Gateway API comes with already a bunch of those routing rules

351
00:21:58,366 --> 00:22:02,382
natively implemented through the API itself, Istio have decided

352
00:22:02,446 --> 00:22:05,954
well, if this is the way forward for kubernetes, and eventually at

353
00:22:05,992 --> 00:22:09,286
some point all Kubernetes clusters will have the Gateway API installed out of the

354
00:22:09,308 --> 00:22:12,882
box. Because it is an upstream API like ingress

355
00:22:12,946 --> 00:22:16,166
is, then we might just leverage it, we might just use it

356
00:22:16,268 --> 00:22:19,766
to be able to implement this. And so as of today, there is Istio

357
00:22:19,798 --> 00:22:23,578
and Linkerd. Both actually support the gateway API. So you can

358
00:22:23,744 --> 00:22:27,622
just use the Gateway API to define all your route

359
00:22:27,686 --> 00:22:31,066
and your routed rules management instead of

360
00:22:31,088 --> 00:22:34,114
using the built in crds of Istio or Linkerd.

361
00:22:34,182 --> 00:22:37,534
And there are more to come down the path. That's it.

362
00:22:37,572 --> 00:22:41,450
I hope this was useful as a basic introduction to Mitmesh.

363
00:22:41,530 --> 00:22:44,446
I know I talk a lot and I talk very fast, so maybe you can

364
00:22:44,468 --> 00:22:47,706
just go back and slow it down a little bit in the slides.

365
00:22:47,738 --> 00:22:51,354
There are a bunch of links in the show, notes to material

366
00:22:51,402 --> 00:22:54,474
to read and I hope that was useful.

367
00:22:54,522 --> 00:22:57,814
Don't follow to don't forget to reach

368
00:22:57,852 --> 00:23:00,886
out to me on Twitter if you need any, have any questions, or if

369
00:23:00,908 --> 00:23:03,860
you need any help and subscribe to the podcast. Thank you.

