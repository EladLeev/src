1
00:01:40,130 --> 00:01:43,406
You. Hi everyone, my name

2
00:01:43,428 --> 00:01:47,322
is Nashant and I'm really excited to be here today at the Con 42 incident

3
00:01:47,386 --> 00:01:51,450
management event to talk to you about that very topic, incident management.

4
00:01:51,610 --> 00:01:54,686
So after this session, you should have a good idea of how to run can

5
00:01:54,708 --> 00:01:57,934
effective incident management process within your team or company,

6
00:01:58,132 --> 00:02:01,358
how to minimize the impact to your users or customers,

7
00:02:01,524 --> 00:02:05,120
and finally, how to continue to learn and improve from past experience.

8
00:02:07,130 --> 00:02:10,360
So before we get started, here's a little about myself.

9
00:02:10,730 --> 00:02:14,482
I'm the engineering manager for the ad services platform team at Pinterest

10
00:02:14,626 --> 00:02:18,194
and our team owns multiple critical systems that power a multibillion

11
00:02:18,242 --> 00:02:21,706
dollar ad business in that role as well.

12
00:02:21,728 --> 00:02:25,558
Ads. Through my involvement with our various incident manager on call rotations,

13
00:02:25,734 --> 00:02:29,658
I've experienced several high severity incidents and learned a lot from that process.

14
00:02:29,824 --> 00:02:32,622
So I'm hoping these learnings can be of use to others who are in similar

15
00:02:32,676 --> 00:02:35,726
positions and help you run an effective incident response process of

16
00:02:35,748 --> 00:02:36,480
your own.

17
00:02:39,170 --> 00:02:42,526
So to start off, let's define what an incident is.

18
00:02:42,708 --> 00:02:46,278
There is a few different situations that we may classify as an incident.

19
00:02:46,474 --> 00:02:50,514
For example, if a systems is not behaving as expected, let's say a particular system

20
00:02:50,552 --> 00:02:53,986
has higher latency than normal. Perhaps when users aren't able

21
00:02:54,008 --> 00:02:57,446
to access our service, they can't log in, they can't get to the front page,

22
00:02:57,628 --> 00:03:01,714
or when employees aren't able to do their work, if employees can't submit pull requests,

23
00:03:01,762 --> 00:03:04,870
can't comment on other people's code, so on and so forth.

24
00:03:06,570 --> 00:03:10,266
So we can say an incident is an event that is not part of the

25
00:03:10,288 --> 00:03:14,186
regular operations of a service that are causing an interruption in

26
00:03:14,208 --> 00:03:16,780
availability or a reduction in its quality.

27
00:03:18,270 --> 00:03:21,766
Taken, you might say, well, the same also applies to most bugs,

28
00:03:21,798 --> 00:03:26,330
right? They're also a manner in which a service may experience interruptions or quality degradations.

29
00:03:26,490 --> 00:03:29,680
And you'd be completely right. That is also what a bug usually is.

30
00:03:30,210 --> 00:03:33,230
So then we ask ourselves, what is an incident really?

31
00:03:33,380 --> 00:03:36,610
How do we differentiate one from a bug? And especially

32
00:03:36,680 --> 00:03:40,834
since we're saying we need a process for handling incidents, we also therefore need

33
00:03:40,872 --> 00:03:44,158
an actionable definition. So here's

34
00:03:44,174 --> 00:03:47,474
how I define an incident. I say an incident is when we need

35
00:03:47,512 --> 00:03:50,774
one or more people to stop everything else that they're doing Andor come help

36
00:03:50,812 --> 00:03:54,754
us fix the bleeding immediately. This is also what differentiates

37
00:03:54,802 --> 00:03:57,906
an incident from a bug, which typically will also have an SLA

38
00:03:57,938 --> 00:04:01,530
for response and may be treated urgently as well.

39
00:04:01,680 --> 00:04:05,418
But for instance, a bug may only require a response during business hours,

40
00:04:05,584 --> 00:04:09,126
and typically only from the team that owns the component that is facing

41
00:04:09,158 --> 00:04:09,930
the bug.

42
00:04:13,070 --> 00:04:16,126
So now that we've defined what an incident is and talked about how it's different

43
00:04:16,148 --> 00:04:20,000
from a bug, let's talk about what incident management looks like.

44
00:04:20,450 --> 00:04:23,842
There's typically five main phases. First of all,

45
00:04:23,896 --> 00:04:27,678
detection. The detection might happen through automated alerts

46
00:04:27,774 --> 00:04:31,970
from our observability or monitoring systems, or through manual reports.

47
00:04:32,870 --> 00:04:35,394
After that, triage the situation,

48
00:04:35,592 --> 00:04:39,174
identify which component is responsible, and identify how

49
00:04:39,212 --> 00:04:42,358
severe the impact is. Next, we go

50
00:04:42,364 --> 00:04:45,814
to mitigate things means we have to stop the bleeding as soon as possible,

51
00:04:45,932 --> 00:04:49,430
minimizing the impact to our users, to our employees, so on and so forth.

52
00:04:49,930 --> 00:04:53,286
After that, we can talk about prevention. How do we figure out what the root

53
00:04:53,318 --> 00:04:56,506
cause was? Andor actually solve it so that we're confident that this issue is not

54
00:04:56,528 --> 00:04:58,922
going to reoccur. Andor then finally,

55
00:04:59,056 --> 00:05:02,542
remediation. What are the follow up action items that we need

56
00:05:02,596 --> 00:05:06,640
to make sure that our systems are more resilient to such situations in the future?

57
00:05:09,970 --> 00:05:13,354
So the first thing to discuss is the process of actually declaring

58
00:05:13,402 --> 00:05:17,378
an incident. So given our definition above, we expect someone to

59
00:05:17,384 --> 00:05:20,706
drop everything and respond, which means we need a clear set of criteria for

60
00:05:20,728 --> 00:05:24,114
defining an incident. So let's talk about

61
00:05:24,152 --> 00:05:27,606
when. And to talk about this, I'm going to borrow a

62
00:05:27,628 --> 00:05:31,000
story from the famous Toyota manufacturing line.

63
00:05:32,330 --> 00:05:35,734
Toyota had something called an Andor cord, which was a rope that

64
00:05:35,772 --> 00:05:39,382
any employee on their manufacturing line could pull whenever they found a problem

65
00:05:39,436 --> 00:05:42,934
with production. Pulling this cord would immediately halt

66
00:05:42,982 --> 00:05:46,586
production. The team leader would come over and ask why the cord was

67
00:05:46,608 --> 00:05:51,370
pulled, and then the teams would work together to solve the problem before resuming production.

68
00:05:52,030 --> 00:05:56,378
So in software engineering, we typically don't need to pause all of production.

69
00:05:56,474 --> 00:06:00,014
There may be some really severe cases where we do. However, typically we just

70
00:06:00,052 --> 00:06:03,674
ask one team or a handful of team members to respond to the incident,

71
00:06:03,802 --> 00:06:07,566
pause others work, respond to the incident, andor help production resume

72
00:06:07,598 --> 00:06:10,946
as normal. So based on this, we can say

73
00:06:10,968 --> 00:06:14,180
that we should create an incidents when we need immediate support.

74
00:06:14,630 --> 00:06:18,002
And what that means will be different between

75
00:06:18,056 --> 00:06:22,034
different components of an organization, different teams, so on and so forth. For instance,

76
00:06:22,082 --> 00:06:25,554
an ads team may declare their incidents criteria based on revenue impact.

77
00:06:25,682 --> 00:06:28,970
Another team may use login failures, app crashes,

78
00:06:29,310 --> 00:06:33,510
still user content, high latency, so on and so forth, content safety,

79
00:06:33,590 --> 00:06:37,498
et cetera. So the next questions is,

80
00:06:37,584 --> 00:06:41,290
who should be declaring the incident? And the answer here is very easy.

81
00:06:41,440 --> 00:06:44,986
Anybody who notices the impact should be empowered

82
00:06:45,018 --> 00:06:49,390
to declare an incident. And it's extremely, extremely important to empower everyone

83
00:06:49,460 --> 00:06:53,294
and anyone in your team or organization to declare an incident and

84
00:06:53,332 --> 00:06:56,546
make it easy to refer to and understand the incident criteria so

85
00:06:56,568 --> 00:06:57,620
they can do so.

86
00:06:59,670 --> 00:07:03,186
The last question is, how do we declare an incident? Going back

87
00:07:03,208 --> 00:07:06,414
to the Toyota story, all they had to do was pull this cord to declare

88
00:07:06,462 --> 00:07:09,960
that something was wrong. And we need to make it just as easy

89
00:07:10,730 --> 00:07:14,406
so different teams stop different processes. We may have a Google form that you fill

90
00:07:14,428 --> 00:07:17,906
out and immediately file an incident. You may file a certain type of Jira ticket

91
00:07:17,938 --> 00:07:20,982
under a project which declares an incident, so on, so forth.

92
00:07:21,126 --> 00:07:24,394
Whatever the preferred method is, it just needs to be extremely easy.

93
00:07:24,592 --> 00:07:28,300
And when we file that ticket or fill out that form,

94
00:07:28,670 --> 00:07:32,106
basically when a new incident is declared, we need to taken sure that someone is

95
00:07:32,128 --> 00:07:36,186
notified. Someone knows that something is wrong. This may be the system owner.

96
00:07:36,298 --> 00:07:39,130
Perhaps in the form you can select what system is impacted.

97
00:07:39,290 --> 00:07:42,454
Could be the person on call for that system, the incident

98
00:07:42,522 --> 00:07:44,260
manager, at the team or at the company.

99
00:07:45,430 --> 00:07:47,730
Doesn't matter. Someone needs to be notified.

100
00:07:51,270 --> 00:07:54,914
So now that we know what an incident is and how to declare one,

101
00:07:55,032 --> 00:07:59,000
how do we tell them apart which incidents are more severe than others?

102
00:08:00,170 --> 00:08:03,618
So we know that all incidents are not the same. Even though we're

103
00:08:03,634 --> 00:08:07,250
saying that incidents require one or more people to drops everything and respond,

104
00:08:07,410 --> 00:08:10,786
the scope of how many people are expected to respond may vary

105
00:08:10,818 --> 00:08:14,106
greatly based on the impact. For instance, let's say you're a

106
00:08:14,128 --> 00:08:17,306
company or team that has an Android app, and you

107
00:08:17,328 --> 00:08:21,078
have 1% of Android users reporting that the app occasionally crashes

108
00:08:21,254 --> 00:08:24,974
every couple of days, versus we say that 20%

109
00:08:25,012 --> 00:08:28,430
of your Android users cannot log into the app at all.

110
00:08:28,580 --> 00:08:31,982
Those are two vastly different scenarios, and the number of people

111
00:08:32,036 --> 00:08:35,300
or teams expected to respond may be vastly different as well.

112
00:08:36,950 --> 00:08:40,014
Which means, as we said before, every team, component,

113
00:08:40,062 --> 00:08:44,254
or service within your company's architecture needs some customized criteria

114
00:08:44,302 --> 00:08:45,490
for incidents.

115
00:08:48,150 --> 00:08:51,526
The general framework we can adopt is how many people need to respond or

116
00:08:51,548 --> 00:08:54,322
need to know, since we're kind of using this framework to determine what an incident

117
00:08:54,386 --> 00:08:57,954
is anyway. So typically at most places,

118
00:08:58,002 --> 00:09:01,858
we think about incident severity levels as starting at Seph

119
00:09:01,874 --> 00:09:04,778
three or set four for the lowest severity incidents, and then going all the way

120
00:09:04,784 --> 00:09:08,326
up to sev zero, which is typically complete outage all hands on deck.

121
00:09:08,358 --> 00:09:11,626
We need to fix the situation immediately. So one

122
00:09:11,648 --> 00:09:14,250
way to think about this is how many people are expected to respond?

123
00:09:14,330 --> 00:09:17,598
Or how many people, or how many levels up do we need

124
00:09:17,604 --> 00:09:21,198
to notify about the impact? So again, going back to some of

125
00:09:21,204 --> 00:09:25,066
these examples, we said, let's say if there's only an elevation

126
00:09:25,098 --> 00:09:28,894
in latency, there's a regression in latency. Your home page is loading slower

127
00:09:28,942 --> 00:09:32,146
than it usually does, that's probably a

128
00:09:32,168 --> 00:09:35,202
low severity incidents. Let's say it's a step three or step four.

129
00:09:35,336 --> 00:09:39,014
Typically only the team that owns that component, their manager and a few others

130
00:09:39,052 --> 00:09:41,160
might need to know respond to the situation.

131
00:09:41,530 --> 00:09:44,966
However, if user login is down and no one can get

132
00:09:44,988 --> 00:09:48,566
into your site or your app, typically company execs would like to

133
00:09:48,588 --> 00:09:52,330
know. And that sort of gets us into the sev zero or sev one criteria.

134
00:09:52,990 --> 00:09:56,886
So these are two frameworks to think about how to define your severity levels.

135
00:09:56,998 --> 00:10:00,666
Based on that, then you will define certain thresholds for different parts of

136
00:10:00,688 --> 00:10:04,518
your stack, your architecture, different components.

137
00:10:04,614 --> 00:10:07,518
You'll come up with different thresholds to define what an incident is.

138
00:10:07,604 --> 00:10:11,166
So if there's $1 in revenue loss, that is one level of

139
00:10:11,188 --> 00:10:14,498
severity. If there's $10 in revenue loss, that's a higher level. So on and

140
00:10:14,504 --> 00:10:15,250
so forth.

141
00:10:17,510 --> 00:10:20,942
So now that we understand when to declare can incident,

142
00:10:21,086 --> 00:10:23,140
what do we actually do? Once that happens,

143
00:10:25,590 --> 00:10:28,710
the first thing to do is some administrative work to take care of.

144
00:10:28,860 --> 00:10:32,438
Open up your communication channels. This may include starting a

145
00:10:32,444 --> 00:10:35,890
chat room or a slack channel, setting up a video conference

146
00:10:35,970 --> 00:10:39,894
if you're in the office taken, marking out a physical room

147
00:10:39,932 --> 00:10:43,254
that you're going to use to respond to this incident, do all your communication,

148
00:10:43,302 --> 00:10:46,746
so on and so forth. Optionally, you may also choose to create

149
00:10:46,768 --> 00:10:49,910
an investigation doc to keep notes. Things may involve

150
00:10:50,070 --> 00:10:53,546
what steps were taken, who all got involved, what the impact is, et cetera,

151
00:10:53,578 --> 00:10:57,402
et cetera. This can sometimes be helpful if things incidents

152
00:10:57,466 --> 00:11:00,906
response requires many parallel tracks of investigation. If we're

153
00:11:00,938 --> 00:11:04,786
expecting more and more people to jump in over time, this document can become the

154
00:11:04,808 --> 00:11:08,738
source of truth for everyone to quickly ramp up and figure out

155
00:11:08,904 --> 00:11:10,740
what the situation currently is.

156
00:11:13,110 --> 00:11:16,450
Next, we need to define and assign some critical roles.

157
00:11:17,430 --> 00:11:20,774
So the first one we have here is the incident runner and

158
00:11:20,812 --> 00:11:24,434
this person is responsible for the outcome of the incident response.

159
00:11:24,562 --> 00:11:28,422
That means they're responsible for helping drive this incident to a

160
00:11:28,476 --> 00:11:32,058
resolution. This may involve communication with the

161
00:11:32,064 --> 00:11:34,310
various folks involved in the investigation,

162
00:11:34,470 --> 00:11:38,294
escalating to other teams or other folks on the team, and actually supporting

163
00:11:38,342 --> 00:11:40,620
the investigation and the debugging process.

164
00:11:42,190 --> 00:11:45,726
The other role is the incident manager role and this person is

165
00:11:45,748 --> 00:11:49,342
responsible for coordinating the incident response. So what that means

166
00:11:49,396 --> 00:11:52,554
is they're here to help the team understand the impact,

167
00:11:52,602 --> 00:11:56,142
determine the severity, identify can appropriate incident

168
00:11:56,206 --> 00:11:59,614
runner. This may be the subject matter expert, the system owner,

169
00:11:59,662 --> 00:12:03,214
the on caller, so on. They're also here to support the incident runner,

170
00:12:03,262 --> 00:12:06,810
handle broader communications. For example, status updates to a broader audience,

171
00:12:06,910 --> 00:12:11,090
cross team escalations, so on and so forth. The incident manager

172
00:12:11,170 --> 00:12:15,234
is also overseeing the process, asking the right questions and helping the team prioritize

173
00:12:15,282 --> 00:12:19,106
between next steps. They're also here to keep the team calm and

174
00:12:19,148 --> 00:12:22,666
like I said, prioritize those next steps. And then finally, it's really important that

175
00:12:22,688 --> 00:12:25,814
this incident manager is someone who's a confident decision

176
00:12:25,862 --> 00:12:28,730
maker with good problem solving skills.

177
00:12:31,950 --> 00:12:36,330
So let's talk about how do we do this impact assessment?

178
00:12:36,410 --> 00:12:40,206
How do we figure out what the severity is? Because this is extremely important

179
00:12:40,388 --> 00:12:44,186
to understand. If we need to escalate to other teams, what channels

180
00:12:44,218 --> 00:12:48,014
of communication do we need to engage, if this is something that is user or

181
00:12:48,052 --> 00:12:51,738
potentially partner facing? If you have other businesses, your customers, for instance,

182
00:12:51,834 --> 00:12:55,346
you may need to send out external communications as well to inform a fear

183
00:12:55,378 --> 00:12:59,586
services availability, outage or degradation

184
00:12:59,618 --> 00:13:03,346
in service quality. Finally, assessing the impact

185
00:13:03,378 --> 00:13:06,598
is also really important to determine what your resolution process

186
00:13:06,684 --> 00:13:10,326
is. For instance, let's say you have an incident late at

187
00:13:10,348 --> 00:13:13,814
night, it's two in the morning, or an incidents that's

188
00:13:13,862 --> 00:13:16,666
middle of the day when everyone is around, your whole team and your company are

189
00:13:16,688 --> 00:13:19,260
around to help with the response process.

190
00:13:21,310 --> 00:13:24,954
Based on the severity of the incident, we may really choose different processes

191
00:13:25,002 --> 00:13:28,398
to resolve this incident. So at some point, if it's middle of

192
00:13:28,404 --> 00:13:32,426
the day, we might be okay with a hot fix or even rolling forward late

193
00:13:32,458 --> 00:13:35,626
at night. We may prefer to be extra cautious and only roll back.

194
00:13:35,668 --> 00:13:38,846
So we're not introducing any new changes. We're not introducing

195
00:13:38,878 --> 00:13:42,690
any new potential sources of instability to make the situation worse.

196
00:13:43,990 --> 00:13:47,442
Once we've done this, once we've figured out the impact and determine the severity,

197
00:13:47,586 --> 00:13:51,570
the incidents runner and incident manager can start to take different responsibilities.

198
00:13:51,730 --> 00:13:55,474
The incident runner can start to play a more active role with actively

199
00:13:55,522 --> 00:13:58,834
debugging escalating to relevant people or teams for support.

200
00:13:58,972 --> 00:14:02,698
While the incident manager takes care of internal comms to

201
00:14:02,864 --> 00:14:06,886
teams, to the overall company, to execs if necessary, and external

202
00:14:06,918 --> 00:14:09,370
comps. Like we said, if partners are impacted,

203
00:14:11,550 --> 00:14:15,182
the first priority of the incident response process here is always

204
00:14:15,236 --> 00:14:18,766
to stop the bleeding first. The root cause can wait if

205
00:14:18,788 --> 00:14:22,426
it's not immediately clear, oftentimes it is, but if it's

206
00:14:22,458 --> 00:14:26,146
not, we need to first treat the symptoms. So this

207
00:14:26,168 --> 00:14:29,506
may involve rolling back suspicious changes. If some

208
00:14:29,528 --> 00:14:33,118
services were deployed, if some experiments were rolled out, if some config flags

209
00:14:33,134 --> 00:14:36,786
were changed during this time, even if we don't feel,

210
00:14:36,968 --> 00:14:40,530
based on our knowledge of the systems, that these changes should cause this incident,

211
00:14:40,690 --> 00:14:44,166
if the timeline aligns, it's still worth rolling them back just to

212
00:14:44,188 --> 00:14:47,714
rule them out. The other option here is to just put up a quick patch

213
00:14:47,762 --> 00:14:51,260
to alleviate the symptom if we don't know the true root cause,

214
00:14:52,110 --> 00:14:54,970
or if it'll take longer to fix the underlying root cause.

215
00:14:55,120 --> 00:14:59,274
One example of this is if you have a data dependency and

216
00:14:59,312 --> 00:15:02,966
the system you're calling is suddenly returning corrupt data or empty data,

217
00:15:03,088 --> 00:15:07,002
and your service is crashing, we could potentially just put up a quick patch

218
00:15:07,066 --> 00:15:09,360
to add a fallback path for that missing data.

219
00:15:10,450 --> 00:15:13,070
Therefore treating the system, stopping the bleeding,

220
00:15:13,510 --> 00:15:16,850
and minimizing the impact to our downstream users.

221
00:15:17,430 --> 00:15:21,150
Once we've done this, once we've stopped the bleeding, cured the symptoms,

222
00:15:21,310 --> 00:15:25,170
we can update our incidents status to mitigated, informing everyone

223
00:15:25,240 --> 00:15:28,774
that the impact is now taken care of. We can now focus on the next

224
00:15:28,812 --> 00:15:32,614
steps. The next step, naturally, is to

225
00:15:32,652 --> 00:15:36,246
resolve the root cause, understand what caused the system to get into a

226
00:15:36,268 --> 00:15:40,402
state of an incident, and ensure that we're completing any action items needed

227
00:15:40,476 --> 00:15:44,422
to resolve this, or guarantee that the issue is not going to reoccur

228
00:15:44,566 --> 00:15:48,326
in the near future. What that near future horizon

229
00:15:48,358 --> 00:15:52,198
looks like is sort of dependent on your component or team. But for instance,

230
00:15:52,294 --> 00:15:55,326
you may say that as long as we have the confidence that this incident or

231
00:15:55,348 --> 00:15:58,766
this issue is not going to reoccur for a month, we're okay with saying

232
00:15:58,788 --> 00:16:02,794
that the incidents is resolved. The second part of resolution

233
00:16:02,842 --> 00:16:06,290
is ensuring that all your systems are restored to regular operations.

234
00:16:06,630 --> 00:16:10,238
So, for example, if you chose to pause your continuous deploys,

235
00:16:10,334 --> 00:16:14,046
if you rolled back any live experiments, if you made any config changes, Andor undid

236
00:16:14,078 --> 00:16:17,666
them, making sure all those are back to normal are a

237
00:16:17,688 --> 00:16:21,282
key part of ensuring that you can say that the incident is truly resolved

238
00:16:21,346 --> 00:16:23,830
because your systems are now back to regular operations.

239
00:16:25,130 --> 00:16:28,674
And then lastly, once we've gone through this process of resolution,

240
00:16:28,802 --> 00:16:32,086
sending out a final update to internal and external parties to

241
00:16:32,108 --> 00:16:35,674
inform them, perhaps you might want to inform them what the root cause was,

242
00:16:35,792 --> 00:16:39,066
but most importantly, communicate that it has been resolved, our systems are back

243
00:16:39,088 --> 00:16:42,942
to normal, and we are confident that our systems are stable for a certain

244
00:16:42,996 --> 00:16:44,000
amount of time.

245
00:16:47,650 --> 00:16:51,486
So at this point, we are confident that our incinerative resolved, there's no more

246
00:16:51,508 --> 00:16:54,820
impact. We can now start to think about the postmortem process.

247
00:16:55,990 --> 00:16:59,442
So what do you do afterwards? First off,

248
00:16:59,576 --> 00:17:02,706
why do we need a postmortem process? Our goal is to

249
00:17:02,728 --> 00:17:07,014
continue to make our systems more resilient. Like we said earlier, we want to

250
00:17:07,132 --> 00:17:09,510
learn and improve from our past experiences.

251
00:17:10,330 --> 00:17:14,306
So we need to first define and follow a rigorous postmortem

252
00:17:14,338 --> 00:17:17,986
process. This may involve creating a postmortem

253
00:17:18,018 --> 00:17:21,526
document template so that we know that everyone is following a fixed

254
00:17:21,558 --> 00:17:25,034
template, including the same level and detail of

255
00:17:25,072 --> 00:17:28,330
information that the broader team needs to identify

256
00:17:28,750 --> 00:17:32,522
gaps and critical remediation items. The second

257
00:17:32,576 --> 00:17:35,934
step for a postmortem process is to typically have an in person review

258
00:17:36,132 --> 00:17:39,886
once the postmortem document is complete. Getting the team together to

259
00:17:39,908 --> 00:17:43,502
ask questions and discuss areas of improvements is a critical part of this process

260
00:17:43,556 --> 00:17:46,810
as well. The attendees for

261
00:17:46,820 --> 00:17:50,382
things postmortem review are typically the people who responded to the incident, the incident

262
00:17:50,446 --> 00:17:53,586
runner, the incident manager, and any other key stakeholders such

263
00:17:53,608 --> 00:17:57,494
as the system owners, the on callers, sres who have knowledge about

264
00:17:57,532 --> 00:18:00,726
generally how to improve stability overall. All these

265
00:18:00,748 --> 00:18:03,960
folks should be required, or at least encouraged to attend this review.

266
00:18:04,890 --> 00:18:09,010
And then finally, it's really important to make sure that we have an SLA

267
00:18:09,170 --> 00:18:12,726
to complete this postpartum process and any remediation ideas

268
00:18:12,838 --> 00:18:16,266
that we identified as part of it. This helps us as a team

269
00:18:16,288 --> 00:18:19,962
and as can organization ensure that we're holding a high bar for quality

270
00:18:20,016 --> 00:18:23,198
and stability, and that none of these tasks are slipping through the

271
00:18:23,204 --> 00:18:24,030
cracks.

272
00:18:26,610 --> 00:18:29,966
So let's talk a little bit about what this postmortem document might look

273
00:18:29,988 --> 00:18:33,166
like. I find that these are sort of five things that are really important to

274
00:18:33,188 --> 00:18:36,418
cover. You may choose to add more. These are the five things that

275
00:18:36,424 --> 00:18:40,126
I strongly recommend. First of all, talk about the impact,

276
00:18:40,238 --> 00:18:44,558
what happened? For example, if we were suddenly charging advertisers

277
00:18:44,574 --> 00:18:48,626
too much for their ad insertions ad impressions, we may need to process refunds,

278
00:18:48,738 --> 00:18:52,338
so on and so forth. This also involves describing

279
00:18:52,354 --> 00:18:55,478
the root cause that caused this impact. Next,

280
00:18:55,644 --> 00:18:59,682
providing a detailed timeline of events, including when the incident began.

281
00:18:59,746 --> 00:19:03,302
So when the symptoms first started, what time did we detect the incidents?

282
00:19:03,446 --> 00:19:06,906
When was the incident mitigated to? When do we fix the systems? And then when

283
00:19:06,928 --> 00:19:10,666
did we actually resolve the root cause? Based on how long

284
00:19:10,688 --> 00:19:13,882
it took to detect, mitigate and resolve our incident,

285
00:19:14,026 --> 00:19:17,886
we may be able to identify action items to

286
00:19:17,908 --> 00:19:21,994
reduce that time. For instance, if it took really long to detect an incidents,

287
00:19:22,122 --> 00:19:25,330
a very clear action item here is that we need better alerting.

288
00:19:26,390 --> 00:19:30,526
So then the last part andor follows is to write down our remediation

289
00:19:30,558 --> 00:19:34,174
ideas to improve these metrics above and identify owners

290
00:19:34,222 --> 00:19:35,910
for these remediation items.

291
00:19:39,210 --> 00:19:42,294
So one thing I want to call out is that it's really important to keep

292
00:19:42,332 --> 00:19:46,022
our postmortem process blameless. Focus on the what Andor

293
00:19:46,076 --> 00:19:49,574
why not the who? Humans make mistakes.

294
00:19:49,702 --> 00:19:52,666
It's natural, it's expected. We learn from them,

295
00:19:52,848 --> 00:19:56,502
systems need to handle them. For instance, if we're

296
00:19:56,566 --> 00:19:59,210
seeing our system crash due to a null pointer exception,

297
00:19:59,870 --> 00:20:03,134
the questions we might ask here are, why did that happen? Why was the change

298
00:20:03,172 --> 00:20:07,246
not detected by unit teams by integration tests through an

299
00:20:07,268 --> 00:20:10,606
incremental rollout process causing failures in canary first on and so

300
00:20:10,628 --> 00:20:14,398
forth? Or if someone accidentally causes a

301
00:20:14,404 --> 00:20:17,634
corruption or deletion of a prod database. Table why

302
00:20:17,672 --> 00:20:20,514
is such access available in the first place? Why were they able to make such

303
00:20:20,552 --> 00:20:23,746
a change? Asking the

304
00:20:23,768 --> 00:20:26,854
right questions are really going to help us get to the point,

305
00:20:26,972 --> 00:20:28,280
get to the root cause,

306
00:20:29,770 --> 00:20:33,734
the underlying root cause, and identify what we need to do to fix

307
00:20:33,772 --> 00:20:37,286
it going forward. A really

308
00:20:37,308 --> 00:20:40,746
helpful technique, also coming from Toyota, is this idea of the

309
00:20:40,768 --> 00:20:44,634
five whys, so we can ask the question why five times,

310
00:20:44,752 --> 00:20:47,900
give or take, to determine the root cause of a problem.

311
00:20:48,830 --> 00:20:53,162
There's two important guidelines to follow here. Make sure you're never identifying

312
00:20:53,226 --> 00:20:55,520
a person or team as a root cause.

313
00:20:56,050 --> 00:20:58,880
Secondly, five is a guidance. It may be more or less.

314
00:21:01,170 --> 00:21:04,766
So let's take an example. The what here is that users can log into

315
00:21:04,788 --> 00:21:08,318
their accounts. So we ask the first why.

316
00:21:08,404 --> 00:21:11,410
Andor we decide that the API is rejecting the login request.

317
00:21:12,390 --> 00:21:15,858
Okay, that's clearly a problem, but not really our root cause.

318
00:21:16,024 --> 00:21:19,720
Second, why tells us that the API can talk to our authentication service.

319
00:21:20,970 --> 00:21:24,486
Next, we say that it can't do this because it doesn't have the

320
00:21:24,508 --> 00:21:27,814
right SSL cert. Next, why did this

321
00:21:27,852 --> 00:21:31,094
happen? Because someone copied the wrong config to prod. So now we're getting

322
00:21:31,132 --> 00:21:34,582
somewhere. But based on our guideline, we said we never want to identify

323
00:21:34,646 --> 00:21:37,830
a person or a team as the root cause. So we're going to ask ourselves

324
00:21:37,910 --> 00:21:42,134
why one more time. Here we come to the answer that there's no validation

325
00:21:42,182 --> 00:21:45,406
for the API config. So clearly, as we

326
00:21:45,428 --> 00:21:49,178
went through this process, we dove deeper and deeper, peeled back the layers,

327
00:21:49,274 --> 00:21:52,410
and identified that our root cause here is that there's no validation.

328
00:21:52,570 --> 00:21:55,758
So a very clear fix, very clear remediation item,

329
00:21:55,854 --> 00:21:59,934
is to ensure that we have such validation in place so that no one accidentally

330
00:21:59,982 --> 00:22:02,820
or maliciously can cause it to change in the future.

331
00:22:07,030 --> 00:22:10,520
All right, now I'm going to dive into some lessons that I've learned over time.

332
00:22:11,770 --> 00:22:15,554
First off, it's really, really important to destigmatize

333
00:22:15,602 --> 00:22:19,698
incidents within your team and within your organization. Going back to Toyota's

334
00:22:19,714 --> 00:22:23,194
and on card story, someone asked them about this process where

335
00:22:23,232 --> 00:22:27,066
any employee had the right to pull the cord, and Toyota took issue with that

336
00:22:27,088 --> 00:22:30,826
wording. They said that employees just didn't just have

337
00:22:30,848 --> 00:22:34,062
the right to pull that cord. They were obligated to pull the cord if they

338
00:22:34,116 --> 00:22:37,790
saw anything wrong. It was part of their responsibilities as an employee.

339
00:22:38,690 --> 00:22:42,234
So it's really important to reward our incident runners

340
00:22:42,282 --> 00:22:45,410
and our incident reporters identifying incidents as a learning

341
00:22:45,480 --> 00:22:49,566
opportunity. So celebrating our incident responders

342
00:22:49,598 --> 00:22:53,102
Andor reporters will help continue to encourage this good behavior.

343
00:22:53,246 --> 00:22:56,862
We should empower our employees to speak up when they see some impact.

344
00:22:57,006 --> 00:23:00,326
It's really important as team managers and as leaders to

345
00:23:00,348 --> 00:23:03,090
publicly praise those employees who identified the impact,

346
00:23:03,170 --> 00:23:06,550
declared the incidents, as well as the ones who actually spent time

347
00:23:06,620 --> 00:23:09,814
resolving them. Oftentimes incidents happen late at night.

348
00:23:09,852 --> 00:23:13,598
People stay up all night resolving these incidents. It's really important to recognize

349
00:23:13,634 --> 00:23:17,306
that hard work. It's also really important to reward them.

350
00:23:17,328 --> 00:23:20,598
Employees who spend these long hours could receive rewards. These may be in the forms

351
00:23:20,614 --> 00:23:23,914
of shoutouts, in terms of improved visibility, perhaps in terms

352
00:23:23,952 --> 00:23:27,566
of dollar bonuses, so on and so forth, and also encourage them

353
00:23:27,588 --> 00:23:30,160
to take time off to recover when they're up late at night.

354
00:23:31,730 --> 00:23:35,674
My opinion is that false positives here are significantly better than false negatives.

355
00:23:35,722 --> 00:23:38,754
Obviously, there's a trade off. You don't want to get to a point where

356
00:23:38,792 --> 00:23:41,726
everyone's declaring an incidents at the top of the drop of hat,

357
00:23:41,838 --> 00:23:45,026
which is why we talked about earlier. We need a really clear set of

358
00:23:45,048 --> 00:23:48,626
incident criteria. However, it is always better to file an

359
00:23:48,648 --> 00:23:51,414
incident prematurely, get immediate support,

360
00:23:51,532 --> 00:23:54,946
resolve the issue, and then later decide that the impact actually didn't

361
00:23:54,978 --> 00:23:58,374
end up being significant enough to warrant a full incident andor

362
00:23:58,412 --> 00:24:01,622
a full postmortem process. At that point, we can just

363
00:24:01,756 --> 00:24:05,338
lower the priority, lower the severity, and call it a bug or something like that.

364
00:24:05,504 --> 00:24:08,666
The alternative, though, is that employees are not sure whether the

365
00:24:08,688 --> 00:24:12,490
impact is big enough yet to declare an incident hold off,

366
00:24:12,640 --> 00:24:16,238
and by the time they decide that it is, the impact might be really big

367
00:24:16,404 --> 00:24:19,626
and balloon sort of out of control and require

368
00:24:19,658 --> 00:24:23,486
a lot more people to jump into support and require just a lot

369
00:24:23,508 --> 00:24:27,140
longer till we actually resolve the issue, leading to a much higher impact than we

370
00:24:27,750 --> 00:24:31,650
necessarily would have had a person that had just filed an incident earlier.

371
00:24:33,910 --> 00:24:37,186
Okay, the next part is specifically

372
00:24:37,218 --> 00:24:40,726
for the incident manager. The guiding principle for

373
00:24:40,748 --> 00:24:44,102
an incident manager to follow is to minimize risk and drive

374
00:24:44,156 --> 00:24:48,070
resolution. A key part of this is being confident.

375
00:24:49,210 --> 00:24:52,794
So let's take a few examples. Let's say we have

376
00:24:52,832 --> 00:24:56,794
an event where an ad system is misbehaving and causing our ad

377
00:24:56,832 --> 00:24:59,946
load, which we define as the percentage of ads users see

378
00:24:59,968 --> 00:25:03,274
compared to non ads posts, to be much higher than normal.

379
00:25:03,402 --> 00:25:07,774
So users are used to seeing only two

380
00:25:07,812 --> 00:25:11,118
out of ten posts being ads. Suddenly that number goes up to six out of

381
00:25:11,124 --> 00:25:14,442
ten. This could also be causing

382
00:25:14,506 --> 00:25:17,986
a drop in ad relevance, users suddenly seeing ads that are no longer relevant to

383
00:25:18,008 --> 00:25:22,050
them. In this case, is it better to turn off ads entirely

384
00:25:22,550 --> 00:25:26,302
and taken that revenue hit, or show irrelevant ads for a period and potentially

385
00:25:26,366 --> 00:25:30,498
impact our user experience, maybe even causing them to not come back to our site?

386
00:25:30,664 --> 00:25:34,038
There's no easy answer here and the team is going to struggle with this question.

387
00:25:34,204 --> 00:25:37,634
So this is one example where it's really important for the incident

388
00:25:37,682 --> 00:25:41,250
manager to step in and help the team confidently make a decision.

389
00:25:41,410 --> 00:25:44,870
This doesn't mean you're solely responsible for that decision

390
00:25:44,950 --> 00:25:48,362
or making that decision. It means you have to be the one to

391
00:25:48,416 --> 00:25:51,350
find the right information, whether that is predocumented,

392
00:25:51,430 --> 00:25:55,054
or identify the right person to make that calm, and just guide the

393
00:25:55,092 --> 00:25:57,200
team through that whole process.

394
00:25:58,530 --> 00:26:01,818
Another example, like we talked about before, there's many ways we may choose to resolve

395
00:26:01,834 --> 00:26:04,782
an incident. We may need to pick between these three options.

396
00:26:04,836 --> 00:26:08,242
Do we roll back a change? Do we fix forward, which would include all changes

397
00:26:08,296 --> 00:26:11,730
since the last time we deployed, or do we hot fix

398
00:26:11,880 --> 00:26:14,850
while cherry picking a patch on top of our latest deploy?

399
00:26:15,590 --> 00:26:18,994
Again, going back to our guiding principle, we said we need to minimize risks.

400
00:26:19,042 --> 00:26:22,518
So factors like time of day, availability of other teams, et cetera, play a

401
00:26:22,524 --> 00:26:26,070
factor and the incident manager can help make this decision.

402
00:26:27,770 --> 00:26:31,250
Another example is if the incident runner or any

403
00:26:31,260 --> 00:26:34,826
of the incident responders are sort of panicked, they're not able

404
00:26:34,848 --> 00:26:38,566
to make progress. The incidents manager can step in and help them collect

405
00:26:38,598 --> 00:26:40,998
themselves, calm down Andor refocus.

406
00:26:41,174 --> 00:26:45,086
Alternatively, if we feel like the incident runner hasn't quite

407
00:26:45,108 --> 00:26:48,270
had the experiences yet, they need to effectively run this incident,

408
00:26:49,250 --> 00:26:52,366
identify the next best person to do so. Andor encourage this

409
00:26:52,388 --> 00:26:55,986
person to stick around and learn from the experience so that they can be

410
00:26:56,008 --> 00:26:58,020
an effective incident runner the next time.

411
00:26:59,430 --> 00:27:04,050
One last example is for these late night and after hours incidents,

412
00:27:04,550 --> 00:27:07,602
the incident manager can help make a call on this question,

413
00:27:07,656 --> 00:27:11,366
like should we pause our investigation till business hours when other teams are

414
00:27:11,388 --> 00:27:14,470
available, or should we go ahead and wake other people up right now?

415
00:27:14,620 --> 00:27:18,214
This involves understanding the impact, the severity, Andor just

416
00:27:18,252 --> 00:27:21,930
being confident in making this decision, ensuring that the team also,

417
00:27:22,000 --> 00:27:24,860
in turn feels confident with the decision that they've made.

418
00:27:26,590 --> 00:27:30,202
So how do we do this? As an incident manager, it's really,

419
00:27:30,256 --> 00:27:33,500
really important to remember that it's okay to ask for help.

420
00:27:34,590 --> 00:27:37,838
Most of the times, you will likely not be the subject matter expert in

421
00:27:37,844 --> 00:27:41,294
the room. You might not even have ever worked with the system that is

422
00:27:41,332 --> 00:27:44,698
encountering this incidents. So rely on your incidents runner

423
00:27:44,794 --> 00:27:48,738
or the subject matter expert. These may sometimes be the same person, sometimes they

424
00:27:48,744 --> 00:27:52,606
may not, but rely on them to help make this call, or rely

425
00:27:52,638 --> 00:27:55,730
on them to at least give you the information that you need to make difficult

426
00:27:55,800 --> 00:27:59,490
calls. The other thing is sometimes

427
00:27:59,560 --> 00:28:03,062
when you're the incident manager, there may be several incidents ongoing at the same

428
00:28:03,116 --> 00:28:06,786
time. Sloop in other folks to help loop in other folks on the rotation.

429
00:28:06,818 --> 00:28:10,246
It's completely okay to ask for help because the alternative is

430
00:28:10,268 --> 00:28:13,594
that you're just juggling too many balls. You don't know what's happening in each

431
00:28:13,632 --> 00:28:16,762
incident response process and things sort of slip through.

432
00:28:16,896 --> 00:28:20,458
And we may be in a situation where we're taking much longer to

433
00:28:20,464 --> 00:28:24,590
resolve the incident than is good or than could have been avoided.

434
00:28:27,890 --> 00:28:31,454
So in order to ask for help, we need to know who to ask

435
00:28:31,492 --> 00:28:35,006
for help. So make a list of these key contacts who can help make

436
00:28:35,028 --> 00:28:38,626
these hard decisions. If we decide that we want to turn off all ads for

437
00:28:38,648 --> 00:28:42,546
our site, typically you might not be the one who

438
00:28:42,568 --> 00:28:46,098
is responsible or has the power to make that decision. So loop in

439
00:28:46,104 --> 00:28:50,118
the folks who can. It's really important to document those so that it's easy

440
00:28:50,204 --> 00:28:53,910
to pull them in when you need. The second one is

441
00:28:53,980 --> 00:28:57,366
who to reach out to for external support. In today's world, a lot of

442
00:28:57,388 --> 00:29:01,542
services are running on hosted infrastructure from

443
00:29:01,596 --> 00:29:04,914
partners in Amazon, Google, et cetera, or for instance,

444
00:29:05,042 --> 00:29:08,422
relying on third party services like pagerduty, so on and so forth,

445
00:29:08,486 --> 00:29:11,686
for learning, for other communications, so on, Andor, so forth.

446
00:29:11,878 --> 00:29:16,974
If we rely on some of these services for critical parts of our

447
00:29:17,012 --> 00:29:20,398
own applications, who do we reach out to for external support?

448
00:29:20,564 --> 00:29:23,950
You don't want to be in a situation where one of these third party services

449
00:29:24,020 --> 00:29:27,566
is down and you're scrambling at that point to figure out who to reach out

450
00:29:27,588 --> 00:29:30,834
to. So it's really important to make this list ahead

451
00:29:30,872 --> 00:29:34,850
of time and continue to update it as and when you find gaps.

452
00:29:38,230 --> 00:29:41,906
The other lesson I learned is that it's really important to measure. Our goal

453
00:29:41,938 --> 00:29:45,878
as part of an incident management process is ultimately to reduce downtime for

454
00:29:45,884 --> 00:29:49,218
our services. However, how do we do this if we're

455
00:29:49,234 --> 00:29:52,274
not measuring this in the first place? So what are some things that we can

456
00:29:52,332 --> 00:29:56,118
measure? There's three metrics that I found to be very useful.

457
00:29:56,294 --> 00:29:59,866
First off, MTTR or mean time to recovery. How long

458
00:29:59,888 --> 00:30:03,390
did it take us from when the incident began till we resolved the incident?

459
00:30:04,370 --> 00:30:06,830
That's a very easy one to measure.

460
00:30:07,810 --> 00:30:10,974
Typically, you will document this as part of your

461
00:30:11,012 --> 00:30:14,334
post mortem document and you can continue to monitor this over time.

462
00:30:14,452 --> 00:30:17,634
If there are certain systems that are seeing longer time to recovery than

463
00:30:17,672 --> 00:30:21,890
others, you can focus some efforts to improve the stability of those systems.

464
00:30:22,790 --> 00:30:26,290
Second one is downtime between failures. Again, if a particular

465
00:30:26,360 --> 00:30:30,038
system is failing very frequently, we know that as an organization or as a

466
00:30:30,044 --> 00:30:32,838
team, we should focus our efforts there.

467
00:30:33,004 --> 00:30:36,406
And then lastly, we talked about this in one of our examples, but meantime to

468
00:30:36,428 --> 00:30:40,118
detection. If we notice time after time that

469
00:30:40,284 --> 00:30:43,898
we're not aware that there's an incident till much later,

470
00:30:44,064 --> 00:30:47,622
that means that there's a gap in our alerting systems, our monitoring systems.

471
00:30:47,766 --> 00:30:51,434
So if this metric is high, then we can clearly identify one

472
00:30:51,472 --> 00:30:54,350
easy action item is to improve our alerts.

473
00:30:55,650 --> 00:30:58,842
Only once we start measuring these can we identify these action items,

474
00:30:58,906 --> 00:31:01,440
iterate, improve them and measure again.

475
00:31:02,530 --> 00:31:06,126
This will also really help motivate the team because we can see these metrics improve

476
00:31:06,158 --> 00:31:10,354
over time, giving them the faith that this incident management process things

477
00:31:10,392 --> 00:31:13,330
incident response process is leading to some improvements.

478
00:31:15,510 --> 00:31:18,766
And then finally, it's really helpful to use error budgets

479
00:31:18,798 --> 00:31:22,278
when you need to trade off innovation work versus KTLo or keep the

480
00:31:22,284 --> 00:31:25,794
lights on work. So error budgets can be thought about. Ads the maximum

481
00:31:25,842 --> 00:31:29,666
amount of time that a system can fail without violating its SLA.

482
00:31:29,858 --> 00:31:33,274
So as an example here, if we say that our SLA is three

483
00:31:33,312 --> 00:31:36,954
nines, 99.9%, our error budget works out to a little under

484
00:31:36,992 --> 00:31:40,986
9 hours. If you're monitoring this on a team level or a

485
00:31:41,008 --> 00:31:44,526
service level, we can very easily say once the system is out

486
00:31:44,548 --> 00:31:48,174
of its error budget. So once it's breaking SLA, we clearly need

487
00:31:48,212 --> 00:31:51,902
to deprioritize some of our product innovation work and focus

488
00:31:51,956 --> 00:31:55,186
more on system stability so that we can continue to offer our

489
00:31:55,208 --> 00:31:58,546
services with high availability and a high quality to our

490
00:31:58,568 --> 00:31:59,620
users and customers.

491
00:32:02,230 --> 00:32:05,506
So to wrap up, we answered four questions here

492
00:32:05,528 --> 00:32:09,054
today. Firstly, we established a clear definition

493
00:32:09,102 --> 00:32:12,454
for incidents and how they're different from bug. We went

494
00:32:12,492 --> 00:32:15,846
over how to declare an incident and defined a process to

495
00:32:15,868 --> 00:32:19,202
respond to an incidents, to minimizing the impact to our users,

496
00:32:19,346 --> 00:32:22,780
communicate with our stakeholders Andor get the support that we need.

497
00:32:23,230 --> 00:32:26,826
Then we talked about a postmortem process that we can use to reflect on

498
00:32:26,848 --> 00:32:30,678
what went wrong and extract learnings to continue improving our systems

499
00:32:30,694 --> 00:32:34,198
in the future. And then lastly, we discussed ways to

500
00:32:34,224 --> 00:32:37,566
measure and improve on our incident handling, as well as ways to

501
00:32:37,588 --> 00:32:41,246
reward and motivate our teams to take this work seriously and continue to

502
00:32:41,268 --> 00:32:44,574
keep our system stable Andor.

503
00:32:44,612 --> 00:32:47,550
That's all. Thank you so much for having me here today.

504
00:32:47,700 --> 00:32:50,766
I hope you found the session helpful and if you have any questions or if

505
00:32:50,788 --> 00:32:53,360
you'd like to chat, you can find me at my email address.

506
00:32:53,970 --> 00:32:57,282
I wish you all the very best with your own incident management and programs,

507
00:32:57,346 --> 00:32:59,538
and I hope you enjoy the rest of the conference.

