1
00:00:25,410 --> 00:00:28,962
You strategies for working with data as it grows.

2
00:00:29,106 --> 00:00:32,610
Hello everyone, my name is Marco Carranza. I'm an entrepreneur

3
00:00:32,690 --> 00:00:36,054
and also the technical cofounder of Teamcore Solutions. I have

4
00:00:36,092 --> 00:00:39,446
been a user of Python for more than 15 years and I have

5
00:00:39,468 --> 00:00:42,982
had the opportunity to use it extensively to develop many

6
00:00:43,036 --> 00:00:46,358
technological solutions for the retail industry on a

7
00:00:46,364 --> 00:00:50,126
global scale. In teamcore solutions, we process sales information

8
00:00:50,268 --> 00:00:53,850
through machine learning algorithms, giving companies visibility

9
00:00:53,930 --> 00:00:58,062
into the execution of their product at each store, generating insights and

10
00:00:58,116 --> 00:01:01,390
specific actions for the office and the field teams.

11
00:01:02,210 --> 00:01:05,426
The objective of this talk is to view alternative for

12
00:01:05,448 --> 00:01:09,042
processing data using different types of technologies and data

13
00:01:09,096 --> 00:01:12,846
frames. First we will take a look to different techniques

14
00:01:12,878 --> 00:01:16,526
to keep pandas memory usage under control and

15
00:01:16,568 --> 00:01:19,766
to allow us to process larger files. Then we

16
00:01:19,788 --> 00:01:23,634
will take a look in how we can vertical scale our pandas loads

17
00:01:23,682 --> 00:01:27,046
using Jupyter notebooks. Next we will learn

18
00:01:27,228 --> 00:01:30,534
about this amazing Python library Bayx so

19
00:01:30,572 --> 00:01:34,486
we can process a large amount of data that cannot fit in our memory.

20
00:01:34,678 --> 00:01:38,826
Then we will try Modin so we can speed up our pandas code with

21
00:01:38,848 --> 00:01:41,914
a minimal change in our Python source code.

22
00:01:42,032 --> 00:01:45,406
And finally, we will take a look to Pyspark and

23
00:01:45,428 --> 00:01:48,190
understand in which cases is a great alternative.

24
00:01:49,170 --> 00:01:52,458
Introduction nowadays, data is getting bigger

25
00:01:52,474 --> 00:01:56,210
and bigger, making it almost impossible to process it in

26
00:01:56,280 --> 00:01:59,698
regular desktop machines. To solve this problem, during the

27
00:01:59,704 --> 00:02:03,266
last years a lot of new technologies have emerged to press all the

28
00:02:03,288 --> 00:02:07,174
data using multiple cluster of computers. The challenge is that

29
00:02:07,212 --> 00:02:11,826
you will need to build your own solution on top of these technologies requiring designing

30
00:02:11,938 --> 00:02:15,634
data processing pipelines and in some cases combining multiple

31
00:02:15,682 --> 00:02:19,466
technologies. However, in some cases we don't have enough time or

32
00:02:19,488 --> 00:02:23,446
resources to learn to use and set up a full infrastructure

33
00:02:23,558 --> 00:02:27,174
to run a couple of experiments. Maybe you are a researcher

34
00:02:27,222 --> 00:02:30,694
with very limited resources or a startup with a tight

35
00:02:30,742 --> 00:02:34,266
schedule to launch a product to the market. Usually the software

36
00:02:34,298 --> 00:02:38,074
that process data works fine when it's tested with a small sample file,

37
00:02:38,202 --> 00:02:41,722
but when you load the real data, the program crashes.

38
00:02:41,866 --> 00:02:45,122
In some cases, some simple optimization could help to process

39
00:02:45,176 --> 00:02:50,594
the data, but when the data is much larger than

40
00:02:50,632 --> 00:02:54,382
the memory available, the problem is harder to solve.

41
00:02:54,526 --> 00:02:57,986
In this talk, we will show you multiple strategies

42
00:02:58,018 --> 00:03:01,266
to process data locally and review some alternative tools

43
00:03:01,378 --> 00:03:04,406
that could help us to process large data sets using a

44
00:03:04,428 --> 00:03:06,230
distributed environment.

45
00:03:08,410 --> 00:03:11,978
Pandas is the de facto tool when we are working with

46
00:03:12,064 --> 00:03:15,206
data on python environments. Now we're

47
00:03:15,238 --> 00:03:18,698
going to see a couple of tricks that will help us to control

48
00:03:18,864 --> 00:03:22,060
the memory of our workloads in a better way.

49
00:03:22,750 --> 00:03:26,970
Trick number one, spurse data structures.

50
00:03:27,130 --> 00:03:31,038
Sometimes we have data sets that came with many,

51
00:03:31,124 --> 00:03:34,946
many empty values. Usually these empty values are

52
00:03:34,968 --> 00:03:38,446
represented as non values and using an sparse

53
00:03:38,478 --> 00:03:41,570
column representation could help us to save some memory.

54
00:03:41,910 --> 00:03:45,054
In pandas, sparse objects uses much less

55
00:03:45,112 --> 00:03:49,462
memory, especially if we save these

56
00:03:49,516 --> 00:03:53,206
objects as pickle on dask and when we are

57
00:03:53,228 --> 00:03:55,874
using them inside a Python interpreter.

58
00:03:56,002 --> 00:03:58,620
Let's take a quick look to a small example.

59
00:04:00,590 --> 00:04:03,530
As we can see in this data frame,

60
00:04:04,750 --> 00:04:08,010
when we list the content of the column name

61
00:04:08,080 --> 00:04:11,374
education 2003 revision, we'll see that there

62
00:04:11,412 --> 00:04:15,018
are a lot of rows with non values.

63
00:04:15,194 --> 00:04:18,814
And then if we take a deeper look to

64
00:04:18,852 --> 00:04:21,966
how much memory we are using, we realize that

65
00:04:21,988 --> 00:04:25,860
we are like consuming a lot of memory something cloud to

66
00:04:26,630 --> 00:04:30,098
19 megabytes. So with

67
00:04:30,264 --> 00:04:33,842
a very simple command we can change the

68
00:04:33,896 --> 00:04:37,382
data type of that column and tell pandas to

69
00:04:37,436 --> 00:04:39,720
use a spurse type object.

70
00:04:40,650 --> 00:04:44,406
So after doing that, if we take a look again to

71
00:04:44,428 --> 00:04:47,698
the memory usage, we'll see that it has reduced. It has

72
00:04:47,724 --> 00:04:51,498
been reduced a lot. So basically after

73
00:04:51,584 --> 00:04:55,094
changing only the data type, we have reduced the memory

74
00:04:55,142 --> 00:04:58,714
in 41%. This maybe doesn't look

75
00:04:58,752 --> 00:05:02,090
too much, but it's very useful, especially when you have

76
00:05:02,160 --> 00:05:05,806
very large data sets that come with a lot of

77
00:05:05,828 --> 00:05:06,990
empty values.

78
00:05:09,090 --> 00:05:12,846
Trick number two, sampling. Sampling is a very interesting and

79
00:05:12,868 --> 00:05:16,830
useful technique and will help us to create a smaller

80
00:05:16,910 --> 00:05:20,402
data set from a larger one, and if it's dont

81
00:05:20,456 --> 00:05:24,174
in the right weight, will help us to run a faster analysis

82
00:05:24,222 --> 00:05:27,510
without sacrificing the quality of the results.

83
00:05:28,570 --> 00:05:32,390
Pan has a special function for that named sample

84
00:05:33,770 --> 00:05:35,480
and let's see an example.

85
00:05:37,050 --> 00:05:41,110
In this example we have one large data frame,

86
00:05:41,270 --> 00:05:45,050
so we are creating a sample of 1000

87
00:05:45,120 --> 00:05:48,170
rows. But before running a sample,

88
00:05:48,510 --> 00:05:52,366
you need to be careful because a common mistake is that

89
00:05:52,548 --> 00:05:56,062
a lot of people think that if they only pick up

90
00:05:56,196 --> 00:05:58,750
the first thousand rows,

91
00:05:59,650 --> 00:06:03,746
that will be a right sample. But in reality the

92
00:06:03,768 --> 00:06:07,950
correct way is to use this function because you will get a more uniform sample

93
00:06:08,110 --> 00:06:11,570
that is better for further

94
00:06:12,230 --> 00:06:16,280
analysis. For example, if we run later

95
00:06:17,130 --> 00:06:20,530
the function describe pandas,

96
00:06:20,610 --> 00:06:23,974
we'll get some instagrams and also

97
00:06:24,012 --> 00:06:27,994
some descriptive statistics. And if you can see if

98
00:06:28,032 --> 00:06:32,362
we compare the result of the original data frame with

99
00:06:32,496 --> 00:06:36,042
the sample one, the results are pretty similar.

100
00:06:36,176 --> 00:06:39,494
Also, if we take a look to the histograms,

101
00:06:39,542 --> 00:06:42,320
we'll see that both are very similar.

102
00:06:42,850 --> 00:06:46,654
But if we make the mistake of only picking the

103
00:06:46,692 --> 00:06:51,040
first n grows, you will get a completely different result.

104
00:06:53,650 --> 00:06:57,682
Okay, trick number three, cloud only the columns that you need.

105
00:06:57,816 --> 00:07:01,266
In some cases we have very large data sets that

106
00:07:01,368 --> 00:07:04,546
comes with many columns. In some cases you can

107
00:07:04,568 --> 00:07:07,970
have hundreds of columns. So there's no point to cloud

108
00:07:08,050 --> 00:07:12,198
all these columns into memory. So the basic rule is

109
00:07:12,284 --> 00:07:15,926
less columns, less memories. So let's take a

110
00:07:15,948 --> 00:07:19,482
quick look to an example. In this

111
00:07:19,536 --> 00:07:23,820
small example we have a large text file that is

112
00:07:24,350 --> 00:07:28,346
3.8gb on disk. So basically after reading it

113
00:07:28,448 --> 00:07:32,654
we realize that we have 77 columns and

114
00:07:32,772 --> 00:07:36,686
if we analyze the memory usage we

115
00:07:36,708 --> 00:07:40,490
will realize that we're using 4.5gb of memory.

116
00:07:40,650 --> 00:07:44,414
So a quick way to reduce the memory

117
00:07:44,462 --> 00:07:47,778
usage is only to select

118
00:07:47,864 --> 00:07:51,378
the columns that we are going to work with. So after

119
00:07:51,464 --> 00:07:54,974
selecting these four columns that you can see in the example, we realize

120
00:07:55,022 --> 00:07:59,874
that we have reduced the memory from 4.5gb

121
00:07:59,922 --> 00:08:03,698
to a little bit more of 300 megabytes.

122
00:08:03,874 --> 00:08:07,830
Also this could be done directly when reading the CSV file

123
00:08:09,950 --> 00:08:13,334
number four use smaller numerical

124
00:08:13,382 --> 00:08:17,462
data types. There are like multiple data types

125
00:08:17,526 --> 00:08:20,954
in pandas and according to the type we can

126
00:08:20,992 --> 00:08:24,446
store more or less information. For example, if we

127
00:08:24,468 --> 00:08:27,706
want to store the age of a person there's

128
00:08:27,738 --> 00:08:31,614
no point of using the data type in 64

129
00:08:31,652 --> 00:08:34,798
view because we are going to waste a lot of memory. In that case it's

130
00:08:34,814 --> 00:08:38,740
much better to use a smaller data type as int eight.

131
00:08:39,110 --> 00:08:41,460
Let's take a quick look to an example.

132
00:08:43,430 --> 00:08:47,302
For example, in this data frame we can see the column name detail h

133
00:08:47,356 --> 00:08:51,126
type and we realize that all the

134
00:08:51,148 --> 00:08:54,646
values are between one and nine. But when we analyze the

135
00:08:54,668 --> 00:08:58,054
data type of that specific column we realize that we are

136
00:08:58,092 --> 00:09:01,834
using in 64. So the recommendation is to change that data

137
00:09:01,872 --> 00:09:06,826
type and we realize that we are using 196

138
00:09:06,928 --> 00:09:10,650
megabytes of memory. But after changing it to a smaller

139
00:09:10,730 --> 00:09:15,214
data type we will be using only 24

140
00:09:15,252 --> 00:09:18,782
megabytes. That means that we could be reducing with only

141
00:09:18,916 --> 00:09:22,874
one line, 87 dont 5%

142
00:09:22,932 --> 00:09:24,370
of the memory consumption.

143
00:09:27,510 --> 00:09:31,202
Trick number five using categorical data

144
00:09:31,256 --> 00:09:35,258
types in some cases it's possible to shrink non numerical

145
00:09:35,294 --> 00:09:38,710
data and reduce the total memory footprint.

146
00:09:39,050 --> 00:09:42,178
For these cases pan has a custom categorical

147
00:09:42,274 --> 00:09:45,494
data type. So let's take a quick

148
00:09:45,532 --> 00:09:47,000
look to an example.

149
00:09:49,790 --> 00:09:53,046
In this example we can see that there is a column

150
00:09:53,078 --> 00:09:56,950
named sex that could have only two categorical values,

151
00:09:57,030 --> 00:10:00,986
f and m. So when we analyze the type of

152
00:10:01,008 --> 00:10:03,120
data that we are using, it's an object.

153
00:10:04,930 --> 00:10:08,650
If you remember the objects can consume a lot of memory.

154
00:10:08,730 --> 00:10:12,154
So when we look deeply we realize

155
00:10:12,202 --> 00:10:15,618
that we are using more than 142

156
00:10:15,624 --> 00:10:19,314
megabytes of memory only for that column. But then if we

157
00:10:19,352 --> 00:10:23,138
change the data type as a categorical type we will see

158
00:10:23,304 --> 00:10:27,526
a huge memory reduction. We will reduce from 142

159
00:10:27,548 --> 00:10:31,446
megabytes to only 2.4. That means that we are reducing the

160
00:10:31,468 --> 00:10:34,760
total memory of to 98%.

161
00:10:38,570 --> 00:10:41,686
Trick number six reading data by chunks attempting

162
00:10:41,718 --> 00:10:44,794
to read a large file can lead to a crash if

163
00:10:44,832 --> 00:10:48,474
there's not enough memory for the entire file to be read at

164
00:10:48,512 --> 00:10:52,382
once. Reading the file in chunks make it possible

165
00:10:52,436 --> 00:10:56,206
to access a very large files by reading one part of

166
00:10:56,228 --> 00:10:59,534
the file at a time. Let's take a look to a

167
00:10:59,572 --> 00:11:00,480
small example.

168
00:11:02,690 --> 00:11:06,222
In this example we have a very large file

169
00:11:06,286 --> 00:11:09,646
that is almost 4gb on disk.

170
00:11:09,758 --> 00:11:13,582
So basically what we are doing in pandas, we are reading the CSV

171
00:11:13,646 --> 00:11:17,238
file, but we are adding a parameter named chunk size. So in

172
00:11:17,244 --> 00:11:21,826
this case we are iterating over the file and reading

173
00:11:22,018 --> 00:11:27,400
the grows in blocks of 500,000.

174
00:11:28,250 --> 00:11:31,814
So every time we read a part of

175
00:11:31,932 --> 00:11:35,354
the CSV file we start to apply and

176
00:11:35,392 --> 00:11:38,886
count all the values and then all the results are beginning

177
00:11:38,918 --> 00:11:42,042
storing a different variable. So after looping,

178
00:11:42,106 --> 00:11:45,614
every time we continue looping, we will release the

179
00:11:45,652 --> 00:11:49,194
memory and read the next chunk. So after finishing

180
00:11:49,242 --> 00:11:53,046
all the process, we will get a result with the desired

181
00:11:53,098 --> 00:11:54,050
calculation.

182
00:11:58,310 --> 00:12:02,674
Vertical scaling with Jupyter on the cloud the

183
00:12:02,712 --> 00:12:06,198
easiest solution to not having enough ram is to throw money to

184
00:12:06,204 --> 00:12:10,086
the problem. That's basically vertical scaling. Thanks to

185
00:12:10,108 --> 00:12:12,946
cloud computing, this is a very easy task.

186
00:12:13,138 --> 00:12:16,914
Vertical scaling is the ability to increase the capacity of existing

187
00:12:16,962 --> 00:12:20,262
hardware or software by adding resources, cpu,

188
00:12:20,406 --> 00:12:23,594
memory, dask, et cetera. In the other hand,

189
00:12:23,632 --> 00:12:26,934
we have horizontal scaling that involves adding machines

190
00:12:26,982 --> 00:12:29,290
to the pool of existing resources.

191
00:12:31,070 --> 00:12:34,446
Jupyter is a very popular tool that helps us

192
00:12:34,468 --> 00:12:38,238
to create documents with live code. Thanks to this tool.

193
00:12:38,324 --> 00:12:41,886
It's very easy to run this code on the cloud, and there

194
00:12:41,908 --> 00:12:45,410
are plenty of large and cheap machines

195
00:12:45,990 --> 00:12:48,050
all around the cloud providers.

196
00:12:49,190 --> 00:12:51,982
What is the advantage of this approach?

197
00:12:52,126 --> 00:12:54,820
Basically no code change is needed.

198
00:12:55,690 --> 00:12:59,650
It's very easy if you are using the right cloud tools.

199
00:12:59,730 --> 00:13:02,290
There are plenty of options like binder,

200
00:13:02,450 --> 00:13:05,970
kaggle kernels, Google Collab, Azure notebooks,

201
00:13:06,050 --> 00:13:09,670
et cetera. It's very good for testing and

202
00:13:09,820 --> 00:13:13,194
cleaning data and visualizing it.

203
00:13:13,392 --> 00:13:16,954
And the good thing is that you only pay for what you use. Of course,

204
00:13:16,992 --> 00:13:21,070
if you forget to turn off your machine, you'll have to pay that

205
00:13:21,140 --> 00:13:24,414
too. Which are the disadvantages of this

206
00:13:24,452 --> 00:13:28,174
approach is that in the long run it's very

207
00:13:28,212 --> 00:13:31,758
expensive because you haven't really optimized your code,

208
00:13:31,844 --> 00:13:35,774
you're only throwing more money to the problem. It does not scale

209
00:13:35,822 --> 00:13:39,518
very well and also it's

210
00:13:39,534 --> 00:13:43,278
not production ready. Normally your code is not optimized

211
00:13:43,374 --> 00:13:44,450
for production.

212
00:13:47,670 --> 00:13:51,818
Speeding up pandas with Modin Modin

213
00:13:52,014 --> 00:13:56,418
is a multi process data frame library with an identical API

214
00:13:56,434 --> 00:14:00,074
to pandas. That means that you don't need to change your code because

215
00:14:00,112 --> 00:14:03,798
the syntax is the same. Modin will allow users

216
00:14:03,814 --> 00:14:07,206
to speed up their pandas workflows because it will unlock

217
00:14:07,238 --> 00:14:10,746
all the cases of your machine. Also, it's very easy to

218
00:14:10,768 --> 00:14:14,320
install and only requires to change a single line of code.

219
00:14:15,490 --> 00:14:19,482
You have to change panda import to import Modin pandas

220
00:14:19,546 --> 00:14:21,840
as PD, so it's very easy.

221
00:14:24,870 --> 00:14:28,558
Pandas implementation is single threaded,

222
00:14:28,734 --> 00:14:32,446
so this means that only one of your cpu

223
00:14:32,478 --> 00:14:36,286
cases can be utilized at any given point of

224
00:14:36,328 --> 00:14:39,670
time. But if you use modin implementation,

225
00:14:40,090 --> 00:14:43,858
you will be able to use all the available cores of your machine,

226
00:14:44,034 --> 00:14:47,350
or maybe all the cores available in the entire cluster.

227
00:14:48,510 --> 00:14:52,134
The modin advantage of Modin is that unlocks

228
00:14:52,182 --> 00:14:54,490
all the cpu power of your machine.

229
00:14:55,230 --> 00:14:59,114
Only one import is needed, so no changes in the code are needed

230
00:14:59,312 --> 00:15:03,022
and it's really fast when reading data. Also has

231
00:15:03,076 --> 00:15:07,098
multiple compute engines available to distribute the calculation of clusters.

232
00:15:07,274 --> 00:15:10,990
These clusters could be implemented with dask or ray

233
00:15:11,730 --> 00:15:15,086
and the main disadvantages of modin is that you need to

234
00:15:15,108 --> 00:15:18,686
expend x ray for depending on the combining gen setup,

235
00:15:18,878 --> 00:15:22,066
dask array plus the

236
00:15:22,088 --> 00:15:25,220
configuration of the clusters. Also,

237
00:15:25,610 --> 00:15:28,934
distributed systems are complex so debugging could

238
00:15:28,972 --> 00:15:33,430
be a little bit tricky. And finally, Modin requires a lot of memory

239
00:15:34,010 --> 00:15:39,938
also as pandas processing

240
00:15:40,034 --> 00:15:43,998
large data sets with by Biax

241
00:15:44,034 --> 00:15:47,274
is a python library with a similar syntax to pandas that

242
00:15:47,312 --> 00:15:50,774
help us work with large data sets in machines with limited resources

243
00:15:50,822 --> 00:15:54,060
where the only limitations is the size of a hard drive.

244
00:15:54,510 --> 00:15:58,142
By provides memory mapping so it will never touch or copy that data

245
00:15:58,196 --> 00:16:01,150
to memory unless it's explicitly requested.

246
00:16:01,650 --> 00:16:04,814
Bix also provides some cool features

247
00:16:04,862 --> 00:16:08,862
like virtual columns and calculate statistics such as mean sum

248
00:16:08,926 --> 00:16:12,514
count standard deviation, with a blazing speed of

249
00:16:12,552 --> 00:16:16,120
up to billion object rows per second.

250
00:16:17,850 --> 00:16:21,346
For the best performance, bikes recommends using the HDF

251
00:16:21,378 --> 00:16:25,126
five format. HDF five is able

252
00:16:25,148 --> 00:16:28,722
to store a large amount of numerical data set with their metadata.

253
00:16:28,866 --> 00:16:32,674
But what do I do if my data is on a CSV format?

254
00:16:32,802 --> 00:16:36,262
No problem. Bikes include the functionality to read a large CSV

255
00:16:36,326 --> 00:16:40,106
file by chunks and convert them to HDF files on the

256
00:16:40,128 --> 00:16:43,130
fly. To do this, it's very easy,

257
00:16:43,200 --> 00:16:46,254
we only need to add a parameters convert equals true to the

258
00:16:46,292 --> 00:16:47,790
function from csV.

259
00:16:49,490 --> 00:16:53,246
Also, Bix provides a data frame server so calculation and

260
00:16:53,268 --> 00:16:57,438
aggregation could run on a different computer. Bix provides

261
00:16:57,454 --> 00:17:01,294
a python API with websockets and a rest API to communicate

262
00:17:01,342 --> 00:17:03,330
with a biox data frame server.

263
00:17:06,490 --> 00:17:09,882
The advantages of bikes are it

264
00:17:09,936 --> 00:17:13,946
helps control the memory usage with the memory mapping and also there are

265
00:17:14,048 --> 00:17:17,750
amazing examples in their website. Also, Bix allows

266
00:17:17,830 --> 00:17:20,890
computing on the fly with a lazy and virtual columns.

267
00:17:21,050 --> 00:17:24,494
It's possible and easy to build visualization with a data set larger than

268
00:17:24,532 --> 00:17:28,654
memory. Machine learning is also available through

269
00:17:28,692 --> 00:17:32,550
the Biax ML package and data can be exported

270
00:17:32,570 --> 00:17:36,690
to pandas data frame if you need a feature that is only available on pandas.

271
00:17:37,190 --> 00:17:41,746
Also, the disadvantages of BIax are you

272
00:17:41,768 --> 00:17:45,954
need to do some modification to your code, but the syntax is slightly

273
00:17:46,002 --> 00:17:49,894
similar to pandas. Also, Bix is

274
00:17:49,932 --> 00:17:53,640
not as mature as pandas but is improving every day.

275
00:17:54,650 --> 00:17:58,454
And also it's a little bit tricky to work with some of the data types

276
00:17:58,582 --> 00:18:04,682
with HDF five all

277
00:18:04,736 --> 00:18:08,074
in with Pyspark. When you need to

278
00:18:08,112 --> 00:18:11,434
work with a very large scale data, it's mandatory to distribute

279
00:18:11,482 --> 00:18:15,374
both the data and the computation to a cluster. This cannot be achieved with

280
00:18:15,412 --> 00:18:18,974
pandas. Spark is an analytic engine used

281
00:18:19,012 --> 00:18:22,866
for large scale data processing. It lets you spread both data and

282
00:18:22,888 --> 00:18:26,574
computation over clusters to achieve a substantial performance

283
00:18:26,702 --> 00:18:27,650
increase.

284
00:18:29,350 --> 00:18:33,294
Pyspark is a Python API for Spark. It combines

285
00:18:33,342 --> 00:18:37,462
the simplicity of python with the high performance of Spark also

286
00:18:37,516 --> 00:18:41,350
provides the Pyspark shell for interactively analyzing your data

287
00:18:41,420 --> 00:18:45,350
in a distributed environment. PySpark support most of

288
00:18:45,420 --> 00:18:48,842
Spark features such as Pyspark, SQL data

289
00:18:48,896 --> 00:18:52,342
frames, streaming machine learning libraries, and spark

290
00:18:52,406 --> 00:18:56,198
core. So the advantages

291
00:18:56,294 --> 00:18:59,646
of Pyspark are first, you will get a

292
00:18:59,668 --> 00:19:02,974
great speed with a large data set.

293
00:19:03,172 --> 00:19:06,606
Also, it's a very rich and mature ecosystem with

294
00:19:06,628 --> 00:19:10,222
a lot of libraries for machine learning, feature extractions, and data

295
00:19:10,276 --> 00:19:13,040
transformation. Also,

296
00:19:13,570 --> 00:19:17,214
spark runs on top of Hadoop so it can access other tools

297
00:19:17,262 --> 00:19:21,042
in the Hadoop ecosystem. In the other hand,

298
00:19:21,096 --> 00:19:23,940
we have some disadvantages. For example,

299
00:19:24,490 --> 00:19:28,002
the first thing that you will notice is that you need to modify

300
00:19:28,146 --> 00:19:32,242
your source code because the syntax of Pyspark is very different to pandas

301
00:19:32,306 --> 00:19:34,520
syntaxes. Also,

302
00:19:36,010 --> 00:19:39,562
Pyspark could have a very bad performance if the data

303
00:19:39,616 --> 00:19:42,970
set is very small. In that case, it's much better to keep using

304
00:19:43,040 --> 00:19:46,886
pandas. In Spark we have the machine

305
00:19:46,918 --> 00:19:50,734
learning library, but at this moment it

306
00:19:50,772 --> 00:19:54,906
has fewer algorithms. Also, Spark requires

307
00:19:54,938 --> 00:19:58,222
a huge amount of memory to process all the data,

308
00:19:58,356 --> 00:20:01,870
so in some cases this could not be very cost effective.

309
00:20:05,110 --> 00:20:08,610
Final notes there are multiple options

310
00:20:08,680 --> 00:20:12,654
to scale up your workloads. The easiest way is to vertical scale

311
00:20:12,702 --> 00:20:16,050
your resources with Jupiter and a cloud provider. But first,

312
00:20:16,120 --> 00:20:19,640
don't forget to optimize your data frame or you will be wasting money.

313
00:20:20,090 --> 00:20:23,814
Also, there are some powerful alternative to work with large data

314
00:20:23,852 --> 00:20:27,858
sets like bikes, so it's worth giving it a try if

315
00:20:27,884 --> 00:20:31,846
you need to process a huge amount of data. You can use Modin with ray

316
00:20:31,878 --> 00:20:35,594
or dask to distribute your workload, but this will add some

317
00:20:35,632 --> 00:20:38,666
extra complexity. Finally,

318
00:20:38,768 --> 00:20:41,610
you can rewrite your pandas logic to make it run over,

319
00:20:41,680 --> 00:20:45,898
spark data frames, and take advantage of many platforms available

320
00:20:45,984 --> 00:20:48,570
in almost all the cloud providers.

321
00:20:51,070 --> 00:20:53,070
Thank you very much for your attention.

