1
00:00:41,010 --> 00:00:43,866
Hello everyone, glad to be here at Cloud Native 2021.

2
00:00:43,868 --> 00:00:47,362
And today I'd like to talk to you about monitoring microservices

3
00:00:47,426 --> 00:00:51,226
the right way. But first, a word about myself off my name is Dotan

4
00:00:51,258 --> 00:00:55,166
Horowitz. I'm a developer advocate at Logs IO. I've been a

5
00:00:55,188 --> 00:00:59,306
developer, a solutions architect, a product manager. I'm an advocate

6
00:00:59,338 --> 00:01:03,242
of open source software and open communities in general and the CNCF,

7
00:01:03,306 --> 00:01:07,234
the cloud Native Computing foundation in particular. I'm a

8
00:01:07,272 --> 00:01:10,974
co organizer of the local chapter of CNCF in Israel,

9
00:01:11,102 --> 00:01:15,002
CNCF Tel Aviv. So if you're around, do join us for the meetups.

10
00:01:15,086 --> 00:01:18,838
I run a podcast on open source observability and

11
00:01:18,924 --> 00:01:21,270
DevOps open observability talks,

12
00:01:21,690 --> 00:01:25,734
and generally you can find me everywhere at Horovits quick

13
00:01:25,772 --> 00:01:29,314
word about logs IO. We provide SaaS

14
00:01:29,362 --> 00:01:33,018
platform for cloud native observability, which essentially means that you

15
00:01:33,024 --> 00:01:36,454
can just send your logs, your metrics, your track to a managed

16
00:01:36,502 --> 00:01:39,302
service for storing, indexing,

17
00:01:39,366 --> 00:01:42,926
analytics and visualization. And the nice thing

18
00:01:42,948 --> 00:01:46,650
is that it's all based on popular open source such as kibana,

19
00:01:46,730 --> 00:01:49,370
elasticsearch, Prometheus, Yeager,

20
00:01:49,450 --> 00:01:52,766
Opentelemetry and so on. So if you're interested, you have the link here

21
00:01:52,788 --> 00:01:56,146
for the conference. And with that, enough vendor talk. Let's talk about

22
00:01:56,248 --> 00:01:59,250
monitoring microservices. But before we go there,

23
00:01:59,320 --> 00:02:03,422
let's recap on how we used to do monitoring.

24
00:02:03,486 --> 00:02:07,554
Until that point, the popular combination was statsd

25
00:02:07,602 --> 00:02:11,222
and graphite, two popular open source tools. We had simple

26
00:02:11,276 --> 00:02:15,042
apps that pushed metrics to statsd over UDP.

27
00:02:15,106 --> 00:02:18,854
StatsD server aggregated the metrics and sent them over TCP

28
00:02:18,902 --> 00:02:22,310
to graphite backend for storing and visualization.

29
00:02:22,390 --> 00:02:26,170
A metric would look something like what you can see here on the screen.

30
00:02:26,320 --> 00:02:29,530
It's graphite's hierarchy notation,

31
00:02:29,870 --> 00:02:33,562
essentially representing the way that the app is in deployment.

32
00:02:33,626 --> 00:02:36,942
So in this case, you have prod production environment inside it web

33
00:02:36,996 --> 00:02:40,986
server inside it, the HTML service and these respective metrics

34
00:02:41,018 --> 00:02:44,370
for that service. And it all worked very fine and well for

35
00:02:44,440 --> 00:02:48,242
quite some time. But then came microservices and cloud

36
00:02:48,296 --> 00:02:52,158
native architectures, and things started getting messy. So I'd

37
00:02:52,174 --> 00:02:56,162
like to look at these new paradigm shifts and

38
00:02:56,296 --> 00:02:59,686
the new challenges that they brought in monitoring first, obviously,

39
00:02:59,788 --> 00:03:03,234
is microservices, which brought an explosion

40
00:03:03,282 --> 00:03:06,918
in the number of discrete entities that we need to monitor. Here on

41
00:03:06,924 --> 00:03:10,454
the screen, you can see the famous death stars by Amazon

42
00:03:10,502 --> 00:03:14,090
and Netflix. The microservice architecture diagrams with

43
00:03:14,160 --> 00:03:18,070
thousands of proprietary, interacting proprietary microservices,

44
00:03:18,150 --> 00:03:21,390
but even much more modest deployments. What used to be a single

45
00:03:21,460 --> 00:03:25,258
monolith is now dozens or more microservices,

46
00:03:25,354 --> 00:03:29,722
each one a cluster with multiple instances of course, each one potentially

47
00:03:29,786 --> 00:03:32,506
running its own programming language and databases,

48
00:03:32,618 --> 00:03:36,142
each one independently deployed and scaled and upgraded,

49
00:03:36,206 --> 00:03:39,566
which means a very dynamic and ephemeral environment.

50
00:03:39,678 --> 00:03:43,406
So from monitoring perspective, not only are we talking about an explosion

51
00:03:43,438 --> 00:03:47,014
in the number of entities that we need to monitor, but also a very,

52
00:03:47,052 --> 00:03:50,326
very diverse and dynamic environment to

53
00:03:50,348 --> 00:03:53,926
monitor, which means of course high volume of metrics and

54
00:03:54,028 --> 00:03:57,394
many dimensions. So microservices is one trend

55
00:03:57,442 --> 00:04:01,222
and the other one, the other paradigm shift is the shift to cloud native

56
00:04:01,286 --> 00:04:05,382
architecture. And with that, now we need to monitor applications spanning

57
00:04:05,446 --> 00:04:09,542
multiple containers, multiple nodes on multiple

58
00:04:09,606 --> 00:04:13,690
namespaces, deployment versions, potentially over fleets of clusters.

59
00:04:13,770 --> 00:04:17,326
And this introduces many additional dimensions to

60
00:04:17,348 --> 00:04:20,778
my metrics, what's called high cardinality metrics,

61
00:04:20,874 --> 00:04:24,514
because now I can ask about my microservice performance per

62
00:04:24,552 --> 00:04:28,302
pod, per node, per version and so on. In addition,

63
00:04:28,446 --> 00:04:32,526
the container, runtime, docker or other, and the Kubernetes

64
00:04:32,638 --> 00:04:36,822
are themselves additional critical systems that I now have

65
00:04:36,956 --> 00:04:40,246
that I need to monitor. In this example, you can see

66
00:04:40,348 --> 00:04:45,010
various Kubernetes services in this diagram, like Kubelet Kproxy

67
00:04:45,090 --> 00:04:48,578
on the node itself or on the control plane, the ETCD

68
00:04:48,674 --> 00:04:51,970
and the scheduler and so on. So each and every one of those I need

69
00:04:51,980 --> 00:04:55,366
to now monitor as well. So cloud native is the second paradigm

70
00:04:55,398 --> 00:04:58,762
shift and the next one that I would like to talk about is open

71
00:04:58,816 --> 00:05:02,286
source and cloud services. Now it's not so new obviously,

72
00:05:02,388 --> 00:05:06,014
and it doesn't specifically talk about microservices and cloud

73
00:05:06,052 --> 00:05:09,646
native, but it definitely got boost by

74
00:05:09,668 --> 00:05:13,646
them. And any typical system these days has multiple

75
00:05:13,758 --> 00:05:17,154
libraries and tools and frameworks for building

76
00:05:17,272 --> 00:05:20,962
the system itself end to end, including web servers and

77
00:05:21,016 --> 00:05:25,010
databases and NoSQL databases and API gateways and message

78
00:05:25,080 --> 00:05:28,674
brokers and queues and you name it. For example, here on these diagram

79
00:05:28,722 --> 00:05:32,662
you can see a typical data processing pipeline and just see

80
00:05:32,716 --> 00:05:36,626
how many potential third party tools, whether open source

81
00:05:36,658 --> 00:05:40,130
or cloud services, you have to implement this. And this is

82
00:05:40,140 --> 00:05:43,258
only the data processing of this part of the system, and we need to

83
00:05:43,264 --> 00:05:47,146
monitor all these third parts. So monitoring systems now need

84
00:05:47,168 --> 00:05:50,438
to provide integration with large and dynamic

85
00:05:50,534 --> 00:05:54,026
ecosystem of third parts platforms to provide complete observability.

86
00:05:54,138 --> 00:05:57,354
And let's face it, this tech track keeps updating

87
00:05:57,402 --> 00:06:01,374
in an increasing pace. The chase after the latest tech stack is becoming crazy,

88
00:06:01,492 --> 00:06:04,866
and so is keeping the monitoring up to speed with the latest and

89
00:06:04,888 --> 00:06:09,214
greatest. So just to recap, seen several challenges.

90
00:06:09,342 --> 00:06:13,006
First is the shift to microservices that introduced an explosion

91
00:06:13,038 --> 00:06:16,898
in the number of entities, high volume of metrics with many dimensions.

92
00:06:16,994 --> 00:06:19,970
Then cloud native introduced more layers,

93
00:06:20,050 --> 00:06:24,070
more virtualization, adding many additional dimensions like

94
00:06:24,140 --> 00:06:27,474
per pod, per node, per version and so on. And also new critical

95
00:06:27,522 --> 00:06:31,286
systems to monitor. And thirdly, the large and dynamic ecosystem

96
00:06:31,318 --> 00:06:34,922
of third party platforms that we use in our system, whether or

97
00:06:35,056 --> 00:06:38,346
open source or cloud services, which we need to

98
00:06:38,368 --> 00:06:42,358
monitoring. So new challenges obviously call for new capabilities.

99
00:06:42,454 --> 00:06:45,866
And I would like now to look at the requirements that you'd

100
00:06:45,898 --> 00:06:49,166
be looking for when designing your monitoring system to make

101
00:06:49,188 --> 00:06:52,698
sure that it can meet these challenges. The first requirement would be

102
00:06:52,804 --> 00:06:56,770
flexible query. Now we saw before this

103
00:06:56,840 --> 00:07:00,750
hierarchy notation for metrics in graphite

104
00:07:00,830 --> 00:07:04,622
that worked well for static machine centric metrics.

105
00:07:04,686 --> 00:07:08,582
But as we saw, these modern systems are much more

106
00:07:08,636 --> 00:07:12,662
dynamic and much more distributed and have many, many more

107
00:07:12,716 --> 00:07:16,438
dimensions, high cardinality. And it's becoming quite

108
00:07:16,604 --> 00:07:20,378
restrictive to try and represent that with this hierarchy model.

109
00:07:20,464 --> 00:07:24,540
For example, if we take the same example of this web server now,

110
00:07:25,390 --> 00:07:29,206
these days, this is probably going to be a service spreading

111
00:07:29,238 --> 00:07:33,258
across, deployed across multiple pods on multiple nodes. And then I

112
00:07:33,264 --> 00:07:36,602
may want to look at this metric on a specific pod,

113
00:07:36,666 --> 00:07:39,886
or maybe on all the pods on a specific node. Or then again I may

114
00:07:39,908 --> 00:07:43,322
want to look at it across the service which spans the entire cluster.

115
00:07:43,386 --> 00:07:47,022
And in the dot notation, introducing a new dimension

116
00:07:47,086 --> 00:07:50,686
effectively creates a new metric which makes it difficult to expose highly

117
00:07:50,718 --> 00:07:54,274
dimensional data, and it also is difficult to do query based

118
00:07:54,312 --> 00:07:57,894
aggregations. So if I want to group by some

119
00:07:57,932 --> 00:08:01,718
dimensions that I haven't pre aggregated in advance, like I don't

120
00:08:01,724 --> 00:08:05,126
know, getting all the 500 code out

121
00:08:05,148 --> 00:08:08,806
of a bunch of servers, how do I do that? With a hierarchy

122
00:08:08,838 --> 00:08:13,158
model, it's quite difficult. So this restriction

123
00:08:13,254 --> 00:08:16,230
drove the shift to a more flexible query,

124
00:08:16,310 --> 00:08:19,418
namely from hierarchy model to a labeling model.

125
00:08:19,504 --> 00:08:23,174
This was nicely introduced with the PromQl query

126
00:08:23,222 --> 00:08:27,086
language by Prometheus open source project. Since then, by the way,

127
00:08:27,108 --> 00:08:30,254
it's been adopted by many others of course. And if we look at the same

128
00:08:30,292 --> 00:08:34,194
example, what it means is essentially that each metric comes

129
00:08:34,312 --> 00:08:37,726
with a set of labels, a list of labels. Each label

130
00:08:37,758 --> 00:08:40,462
is essentially a key value pair.

131
00:08:40,526 --> 00:08:44,498
And once there I can just query on any

132
00:08:44,664 --> 00:08:48,814
dimension, any label, or any combination of labels that

133
00:08:48,872 --> 00:08:52,370
interests me. In this example, for example, if I want per pod, I just specify

134
00:08:52,450 --> 00:08:56,182
server equals pod seven. Or if I want on the entire

135
00:08:56,236 --> 00:08:59,706
service, I'll say service equals engines. And of course I can combine. And this

136
00:08:59,728 --> 00:09:03,114
is a pretty basic example. If you think about real

137
00:09:03,152 --> 00:09:06,742
life web applications that have the web server software

138
00:09:06,806 --> 00:09:10,534
and the environment and the HTTP method and the error

139
00:09:10,582 --> 00:09:15,114
code and the HTTP response code and the endpoints

140
00:09:15,162 --> 00:09:18,686
and so on. The number of ad hoc queries that I

141
00:09:18,788 --> 00:09:22,334
may want to create is almost endless. So if I want to

142
00:09:22,452 --> 00:09:26,802
ask questions like what's the total number of results per web

143
00:09:26,856 --> 00:09:30,574
server pod in production? Or what's the number of HTTP errors

144
00:09:30,622 --> 00:09:35,130
using the engine server on a specific endpoint in staging

145
00:09:35,230 --> 00:09:36,840
or things like that,

146
00:09:38,330 --> 00:09:41,938
it becomes easy to query that with the labeling

147
00:09:41,954 --> 00:09:45,734
model. So flexible querying is definitely the first

148
00:09:45,772 --> 00:09:49,366
requirement that we would like to have. The next up is metric scraping

149
00:09:49,398 --> 00:09:53,702
and auto discovery. As we've seen in stats these days, apps pushed

150
00:09:53,766 --> 00:09:57,340
metrics, but systems these days, as we talked about it,

151
00:09:58,190 --> 00:10:01,754
has many microservices, many third parts,

152
00:10:01,882 --> 00:10:05,374
tools, frameworks and how do you considered each

153
00:10:05,412 --> 00:10:08,942
and every microservice and especially third parties that you don't control

154
00:10:08,996 --> 00:10:12,426
the code to push metrics to your monitoring back end?

155
00:10:12,548 --> 00:10:16,418
It's rather impractical. What we would like in the new approach is

156
00:10:16,504 --> 00:10:19,986
for the monitoring system itself to

157
00:10:20,168 --> 00:10:24,174
discover the services or the components automatically and essentially

158
00:10:24,302 --> 00:10:28,630
pull the metrics off of them. Prometheus project actually introduced this

159
00:10:28,780 --> 00:10:33,174
notion exactly. Prometheus can detect the services

160
00:10:33,292 --> 00:10:36,386
automatically called targets in Prometheus

161
00:10:36,418 --> 00:10:39,766
terms and then pull the metrics off these targets,

162
00:10:39,878 --> 00:10:43,194
scrapes the metrics in Prometheus terms. We can

163
00:10:43,232 --> 00:10:46,518
do that thanks to open metrics. And open metrics

164
00:10:46,534 --> 00:10:50,086
is an open standard for transmitting metrics. That's come

165
00:10:50,208 --> 00:10:53,230
the standard for exposing metrics in the industry.

166
00:10:54,290 --> 00:10:57,994
In the CNCF sandbox, it's based on Prometheus format.

167
00:10:58,042 --> 00:11:01,690
It's proposed as a standard these days to the IETF.

168
00:11:01,770 --> 00:11:05,246
But most importantly is that it's widely adopted.

169
00:11:05,358 --> 00:11:08,642
Many common tools and frameworks expose out of the box metrics using

170
00:11:08,696 --> 00:11:11,874
this format. So whether you use kafka or

171
00:11:11,912 --> 00:11:16,034
RabbitmQ or MongoDB or MysQL or Apache,

172
00:11:16,082 --> 00:11:18,710
NgInX, Jenkins, GitHub,

173
00:11:19,210 --> 00:11:22,534
cloud services, you name it. They are

174
00:11:22,652 --> 00:11:26,614
more than probably that they expose their metrics using

175
00:11:26,652 --> 00:11:29,846
this format. And by the way, it's very easy to check. You just

176
00:11:29,868 --> 00:11:33,174
go to metrics typically and endpoint.

177
00:11:33,222 --> 00:11:36,582
You can even see that on the browser because it's textual

178
00:11:36,646 --> 00:11:40,494
and it's very, very popular. And this large

179
00:11:40,532 --> 00:11:43,806
ecosystem is key when dealing with such

180
00:11:43,908 --> 00:11:47,246
diverse and dynamic systems. As we talked about it,

181
00:11:47,348 --> 00:11:51,066
another nice thing about Prometheus is the native integration

182
00:11:51,098 --> 00:11:54,894
with Kubernetes and the CNCF ecosystem, which is not surprising

183
00:11:54,942 --> 00:11:57,742
because Prometheus is part of the CNCF.

184
00:11:57,806 --> 00:12:01,390
It's a secondly graduated project there after Kubernetes

185
00:12:01,470 --> 00:12:05,486
and Prometheus can pull from kubernetes both the discovery,

186
00:12:05,518 --> 00:12:09,554
so it can discover the services running on kubernetes through kubernetes

187
00:12:09,602 --> 00:12:13,074
and also fetch the metrics with plugins to the Kubernetes

188
00:12:13,122 --> 00:12:16,646
API Kubernetes metrics. It can also use console and

189
00:12:16,668 --> 00:12:20,742
others. So a very nice integration that makes it very seamless.

190
00:12:20,806 --> 00:12:24,858
If you run on cloud native and kubernetes, it would be very

191
00:12:24,944 --> 00:12:29,130
easy to start with Prometheus. Also, if you run on cloud services,

192
00:12:29,280 --> 00:12:33,182
Prometheus can plug into the service discovery of

193
00:12:33,316 --> 00:12:36,734
different cloud environments and pull from there.

194
00:12:36,772 --> 00:12:39,918
So very nice integration and ecosystem there.

195
00:12:40,004 --> 00:12:43,730
And the next requirement is core scalability.

196
00:12:44,950 --> 00:12:48,194
As we've seen, systems emit massive amounts of

197
00:12:48,232 --> 00:12:51,378
high cardinality metrics, which means high volume of

198
00:12:51,384 --> 00:12:54,938
time series data that we need to store and query effectively.

199
00:12:55,054 --> 00:12:59,078
Prometheus, with all its virtues that we've seen, is still

200
00:12:59,164 --> 00:13:02,726
by design a single node installation, which means that it

201
00:13:02,748 --> 00:13:06,982
doesn't scale horizontally. One option of scaling Prometheus is

202
00:13:07,116 --> 00:13:10,626
to compose Prometheus instances into a federated

203
00:13:10,738 --> 00:13:14,266
architecture, shard the matrix metrics and so

204
00:13:14,288 --> 00:13:17,900
on, but that can become quite complicated to manage.

205
00:13:18,430 --> 00:13:22,362
Another option is to write to a long term storage backend.

206
00:13:22,506 --> 00:13:26,206
Prometheus has a built in capability to remote write to a

207
00:13:26,228 --> 00:13:30,698
backend store. And again, there is a very rich ecosystem

208
00:13:30,794 --> 00:13:34,846
of time series databases that can serve

209
00:13:34,878 --> 00:13:38,626
as long term storage for Prometheus, whether proprietary or

210
00:13:38,728 --> 00:13:42,514
open source tools, some of them also under

211
00:13:42,552 --> 00:13:45,658
the CNCF itself like thanos and cortex.

212
00:13:45,774 --> 00:13:49,942
So you have quite a variety to choose from, both ones

213
00:13:49,996 --> 00:13:53,560
that will be self managed and ones that are

214
00:13:54,090 --> 00:13:57,254
cloud services like logs IO to choose

215
00:13:57,292 --> 00:14:00,874
from with different trade offs. So to summarize, we've seen

216
00:14:00,912 --> 00:14:04,566
that monitoring microservices and cloud native systems

217
00:14:04,598 --> 00:14:08,454
introduces challenges with massive

218
00:14:08,502 --> 00:14:12,270
amounts of instances, high volume diversity,

219
00:14:12,690 --> 00:14:16,106
highly distributed, the high dimensionality

220
00:14:16,218 --> 00:14:20,830
in cardinality, many third parts involved,

221
00:14:22,050 --> 00:14:27,010
and in order to monitor that, you'd look for first,

222
00:14:27,160 --> 00:14:31,266
flexible querying with high cardinality, with the labeling model

223
00:14:31,368 --> 00:14:34,938
being the prime model for these sorts of ad hoc queries,

224
00:14:35,054 --> 00:14:38,482
then efficient metric scraping with auto discovery.

225
00:14:38,546 --> 00:14:44,386
So we don't need to bother about each and every piece shipping

226
00:14:44,418 --> 00:14:48,578
the metrics to your system, and thirdly, these scalability

227
00:14:48,674 --> 00:14:52,794
to handle large volumes of metrics. As an open

228
00:14:52,832 --> 00:14:56,202
source fan, I'm glad to say that open source is taking the lead here.

229
00:14:56,256 --> 00:14:59,754
We talked about Prometheus as a popular tool

230
00:14:59,792 --> 00:15:03,198
for that, Openmetrics as the emerging open standard

231
00:15:03,284 --> 00:15:06,538
for that many time series databases that can serve

232
00:15:06,554 --> 00:15:11,294
as long term storage to scale. Prometheus very

233
00:15:11,492 --> 00:15:14,874
good integration with Kubernetes and the CNCF ecosystem.

234
00:15:14,922 --> 00:15:18,142
And by the way, also beyond CNCF, the general ecosystem,

235
00:15:18,286 --> 00:15:22,274
open source is definitely a driver here and is also a good starting point

236
00:15:22,392 --> 00:15:25,966
for your journey if you're looking for solutions. If you're looking for

237
00:15:26,008 --> 00:15:29,590
more information, I wrote a blog post

238
00:15:29,660 --> 00:15:33,522
about this topic with a bit of a greater detail and some examples.

239
00:15:33,666 --> 00:15:37,410
I also have an interesting episode on the open observability

240
00:15:37,490 --> 00:15:41,654
Talks podcast talking about this topic and obviously

241
00:15:41,772 --> 00:15:45,666
the open source forums

242
00:15:45,698 --> 00:15:49,494
themselves, whether Prometheus or Openmetrics. And since

243
00:15:49,532 --> 00:15:53,466
these things change so quickly, these obviously the ongoing

244
00:15:53,498 --> 00:15:57,546
discussions on the CNCF track channels and the gitter and the mailing

245
00:15:57,578 --> 00:16:01,450
list are more than welcome to make sure that you're up to speed.

246
00:16:01,530 --> 00:16:04,846
And of course, feel free to reach out to me with

247
00:16:04,868 --> 00:16:08,862
any question or comment. I'd be more than happy to follow

248
00:16:08,916 --> 00:16:12,094
up. I'm Dotan Horovits and thank you very much

249
00:16:12,132 --> 00:16:12,490
for listening.

