1
00:00:22,250 --> 00:00:25,650
Good morning, good afternoon, good evening,

2
00:00:25,730 --> 00:00:29,270
wherever you are at our virtual world.

3
00:00:29,420 --> 00:00:32,678
My name is Ron Dagdag. I'm a

4
00:00:32,684 --> 00:00:36,054
lead software engineer at Spacey. Today I will

5
00:00:36,092 --> 00:00:39,734
be talking about leverage the power of machine learning with

6
00:00:39,772 --> 00:00:43,010
Onnx. For all you Pokemon

7
00:00:43,090 --> 00:00:46,326
fans out there, I will not be talking about the

8
00:00:46,348 --> 00:00:50,094
Pokemon onyx, nor I will be talking about the

9
00:00:50,252 --> 00:00:53,406
mineral onyx. I will be talking about

10
00:00:53,508 --> 00:00:57,882
Onyx on NX open neural network

11
00:00:57,946 --> 00:01:01,070
exchange. All right, let's go back to basics.

12
00:01:01,410 --> 00:01:04,430
What is programming? Programming,

13
00:01:04,510 --> 00:01:07,198
traditionally you have an input.

14
00:01:07,374 --> 00:01:10,740
You write an algorithm, you combine them together,

15
00:01:11,430 --> 00:01:14,386
run it, and it will spit out answers for you.

16
00:01:14,488 --> 00:01:18,002
In machine learning world, you have an input,

17
00:01:18,146 --> 00:01:22,118
you have examples of what the answers would be,

18
00:01:22,284 --> 00:01:25,814
and the computer's goal is to

19
00:01:25,852 --> 00:01:29,622
provide an algorithm for you. So as a primer,

20
00:01:29,686 --> 00:01:32,860
you have your programming, traditional programming at the right,

21
00:01:33,230 --> 00:01:37,834
machine learning on the left still

22
00:01:37,872 --> 00:01:41,066
have your input, answers, and your algorithm machine

23
00:01:41,098 --> 00:01:44,240
learning world, we call

24
00:01:44,690 --> 00:01:48,480
the input and the answers as your training data.

25
00:01:49,090 --> 00:01:52,770
You have to use a training framework in order

26
00:01:52,840 --> 00:01:56,194
to get a machine learning model.

27
00:01:56,392 --> 00:01:59,794
And based from that model you would

28
00:01:59,992 --> 00:02:03,330
substitute or use that into your

29
00:02:03,400 --> 00:02:07,350
application, and that's what we call inferencing.

30
00:02:08,010 --> 00:02:12,454
And you would use a runtime to

31
00:02:12,492 --> 00:02:15,734
be able to process your input and your model, and it would give

32
00:02:15,772 --> 00:02:19,354
you the answers. And now that you have more

33
00:02:19,392 --> 00:02:22,774
answers, it could be a good feedback loop to improve

34
00:02:22,822 --> 00:02:24,940
your training data.

35
00:02:25,630 --> 00:02:30,414
So typically data scientists would

36
00:02:30,452 --> 00:02:34,062
program, or would create a program

37
00:02:34,196 --> 00:02:37,950
in Pytorch. They would run it locally on their machine

38
00:02:38,930 --> 00:02:42,240
using the cpu. And then of course,

39
00:02:42,770 --> 00:02:47,194
if you are a JavaScript developer and you've

40
00:02:47,242 --> 00:02:50,874
seen all these different Javascript frameworks

41
00:02:50,922 --> 00:02:54,966
and all these different ways, and how you

42
00:02:55,028 --> 00:02:58,534
can create applications or

43
00:02:58,572 --> 00:03:03,074
web applications the same way as in machine

44
00:03:03,122 --> 00:03:06,594
learning world. There's all these different machine

45
00:03:06,642 --> 00:03:10,134
learning frameworks, training frameworks that you can use,

46
00:03:10,332 --> 00:03:14,166
and the ecosystem is growing.

47
00:03:14,278 --> 00:03:18,022
And of course we're not just limited in deploying it locally

48
00:03:18,166 --> 00:03:22,014
on our devices, not just on

49
00:03:22,052 --> 00:03:26,286
our laptops. Also, you have to sometimes use a phone or

50
00:03:26,468 --> 00:03:30,382
deploy it in the cloud. Sometimes you want better

51
00:03:30,436 --> 00:03:34,420
performance. You run it through a GPU or

52
00:03:34,950 --> 00:03:39,906
FPGA or ASIC, or you

53
00:03:39,928 --> 00:03:43,154
can also run it, you might wanted to also run it on

54
00:03:43,272 --> 00:03:46,050
a microcontroller.

55
00:03:47,930 --> 00:03:51,126
And that's when Onnx comes into

56
00:03:51,148 --> 00:03:54,934
the picture. Onyx is the bridge between how

57
00:03:54,972 --> 00:03:58,490
you get trained and where to deploy.

58
00:03:58,990 --> 00:04:02,554
Onnx is short for open

59
00:04:02,672 --> 00:04:06,534
neural network exchange. It is an open format

60
00:04:06,582 --> 00:04:10,134
for machine learning models. Notice that it's not just limited

61
00:04:10,182 --> 00:04:13,530
to neural networks, it's also capable

62
00:04:13,610 --> 00:04:17,550
of your traditional machine learning models too. It is on

63
00:04:17,620 --> 00:04:21,066
GitHub, GitHub.com onyx

64
00:04:21,258 --> 00:04:25,374
and best place to learn more about Onnx about

65
00:04:25,412 --> 00:04:29,106
Onnx is through Onyx AI. And when you go

66
00:04:29,128 --> 00:04:32,626
to this website, you'll notice that every time I would go

67
00:04:32,648 --> 00:04:36,086
in there's new partners coming in and be

68
00:04:36,108 --> 00:04:39,030
able to improve that ecosystem.

69
00:04:39,770 --> 00:04:42,982
We just started between partnership between Microsoft and

70
00:04:43,036 --> 00:04:46,054
Facebook and I've noticed that more and

71
00:04:46,092 --> 00:04:50,650
more there's partners

72
00:04:51,550 --> 00:04:55,690
using this application on GitHub

73
00:04:56,190 --> 00:05:00,250
about 10.9 what, 11,000 GitHub stars

74
00:05:00,670 --> 00:05:04,414
pull request about almost 2000 pull request about

75
00:05:04,452 --> 00:05:08,474
200 contributors about 2000 GitHub

76
00:05:08,522 --> 00:05:14,674
forks and there's also model zoo. Onnx is

77
00:05:14,712 --> 00:05:18,578
available out there. It is a graduate project of

78
00:05:18,664 --> 00:05:21,810
Linux Foundation AI.

79
00:05:22,630 --> 00:05:26,280
And so it's becoming more,

80
00:05:28,810 --> 00:05:33,602
there's a lot of traction going on for these Onyx

81
00:05:33,746 --> 00:05:36,710
application. When would you use Onnx?

82
00:05:38,910 --> 00:05:42,234
Is when you have something

83
00:05:42,272 --> 00:05:46,058
that's trained in Python and you want to deploy it to a

84
00:05:46,144 --> 00:05:49,526
C sharp application, or maybe you want to incorporate

85
00:05:49,558 --> 00:05:54,286
it to your Java application or JavaScript application when

86
00:05:54,308 --> 00:05:58,746
you have high inferency latency

87
00:05:58,938 --> 00:06:02,598
that you want for production use. So, meaning if it's

88
00:06:02,634 --> 00:06:06,274
too slow to run it or if you want

89
00:06:06,472 --> 00:06:09,822
performance so that you can use it for production.

90
00:06:09,886 --> 00:06:14,500
Because let's say if you have it in some training

91
00:06:15,930 --> 00:06:19,826
platform or training framework and it's

92
00:06:19,858 --> 00:06:23,110
not good enough and you want to improve,

93
00:06:25,370 --> 00:06:28,646
if you have high inferency, that would be a good use case for

94
00:06:28,668 --> 00:06:31,786
it. If you want to deploy it to an IoT device or

95
00:06:31,808 --> 00:06:35,258
an edge device, it might make sense to convert it

96
00:06:35,264 --> 00:06:39,302
to Onyx and be able to deploy it to those devices

97
00:06:39,446 --> 00:06:42,720
when it's trained on a different

98
00:06:43,490 --> 00:06:46,862
OS. And if you want to run that model into a different

99
00:06:46,916 --> 00:06:50,654
OS or different hardware, that is a good use case

100
00:06:50,692 --> 00:06:55,054
for it. When you want to combine

101
00:06:55,102 --> 00:06:59,090
the models, let's say you have a team of data scientists.

102
00:07:02,390 --> 00:07:06,386
Some of your models were created on, let's say Pytorch and some

103
00:07:06,408 --> 00:07:09,718
of the models were created in keras. And you want to create a

104
00:07:09,724 --> 00:07:13,846
pipeline and you want to combine these models that

105
00:07:13,868 --> 00:07:17,240
is trained from different frameworks. That is another way.

106
00:07:17,630 --> 00:07:21,354
Another one is through training, takes too

107
00:07:21,392 --> 00:07:25,414
long. And that's when you start talking about transformer

108
00:07:25,462 --> 00:07:29,290
models. Let's say if you want to train it locally

109
00:07:30,030 --> 00:07:33,694
at the edge, that is one way that you can use

110
00:07:33,732 --> 00:07:37,166
Onyx too. All right, so we

111
00:07:37,188 --> 00:07:40,846
did talk about what is Onyx. One thing I

112
00:07:40,868 --> 00:07:44,814
want to point out, one good way of to describe

113
00:07:44,862 --> 00:07:48,066
Onnx, it's kind of like PDF, right?

114
00:07:48,248 --> 00:07:52,500
You create your Word document in

115
00:07:53,190 --> 00:07:57,810
Microsoft Word. Or you create your documents

116
00:07:58,410 --> 00:08:02,866
in Microsoft Word or some word processing

117
00:08:03,058 --> 00:08:06,258
application, you convert it to PDF.

118
00:08:06,354 --> 00:08:11,500
Now you can display it on different types of devices using

119
00:08:12,190 --> 00:08:15,210
acrobat or PDF viewer.

120
00:08:15,790 --> 00:08:18,954
And then we did talk about when

121
00:08:18,992 --> 00:08:22,654
to use Onnx. And then we'll talk

122
00:08:22,692 --> 00:08:25,866
more about how to create the Onyx models

123
00:08:25,978 --> 00:08:29,470
and how to deploy onnx models,

124
00:08:30,050 --> 00:08:33,438
how to create Onyx models, step one. And then we'll

125
00:08:33,454 --> 00:08:37,106
talk more about step two. Step one. Let's focus on that. Have you

126
00:08:37,128 --> 00:08:40,498
ever baked a cake? And of course there's a

127
00:08:40,504 --> 00:08:44,450
lot of different ingredients, different procedures.

128
00:08:45,370 --> 00:08:49,240
Of course bakers specializes on this.

129
00:08:49,610 --> 00:08:54,150
My analogy in this is that bakers

130
00:08:55,050 --> 00:08:57,270
or your data scientist,

131
00:08:57,870 --> 00:09:01,802
your team, they're the ones who make the

132
00:09:01,856 --> 00:09:05,754
secret recipe for your business. They try

133
00:09:05,872 --> 00:09:10,442
different tweaks and different ingredients and different procedures

134
00:09:10,506 --> 00:09:14,634
and how to create these AI

135
00:09:14,682 --> 00:09:18,190
models, which is going to be your secret recipe.

136
00:09:18,850 --> 00:09:23,178
So how do you create these Onyx models? One way is to export

137
00:09:23,274 --> 00:09:26,370
or using Onyx model zoo.

138
00:09:28,390 --> 00:09:32,290
Onyx model zoo. There's existing

139
00:09:32,870 --> 00:09:36,598
models out there that you can just download off the Internet and start using

140
00:09:36,684 --> 00:09:40,742
incorporating to your application. You can use Azure custom

141
00:09:40,796 --> 00:09:44,614
vision or some service that exports to

142
00:09:44,652 --> 00:09:49,274
Onyx. You can convert an existing model and

143
00:09:49,312 --> 00:09:52,854
also you can train models in azure machine

144
00:09:52,902 --> 00:09:56,566
learning or some automated machine learning. So Onnx

145
00:09:56,598 --> 00:10:00,800
model zoo allows you to be able to just,

146
00:10:01,490 --> 00:10:04,958
someone already pre converted all these different

147
00:10:05,124 --> 00:10:09,022
popular AI models out there or machine learning

148
00:10:09,076 --> 00:10:13,246
models out there. If you're interested in Restnet, it's already converted

149
00:10:13,278 --> 00:10:16,660
to Onyx for you and you can just download it.

150
00:10:17,350 --> 00:10:20,930
These are some examples of the different sizes

151
00:10:21,750 --> 00:10:24,930
of that model once it's converted to onnx.

152
00:10:25,770 --> 00:10:28,680
So it's not just limited in image, there's also sound.

153
00:10:29,130 --> 00:10:32,454
There's different models out there

154
00:10:32,492 --> 00:10:36,310
you can just download. Another one is through custom

155
00:10:36,380 --> 00:10:41,334
vision, which allows you to do low code vision

156
00:10:41,382 --> 00:10:44,950
service where you would upload some photos,

157
00:10:45,110 --> 00:10:49,260
you tag them, start tagging them, you train

158
00:10:50,370 --> 00:10:54,110
to create a machine learning model for you

159
00:10:54,180 --> 00:10:57,150
and then you can export it to Onyx.

160
00:10:57,890 --> 00:11:01,920
Another way is to convert the model

161
00:11:02,390 --> 00:11:06,030
from the existing training frameworks.

162
00:11:06,190 --> 00:11:10,194
So let's say you

163
00:11:10,232 --> 00:11:14,910
have it in Pytorch or keras or Tensorflow or Scikitlearn.

164
00:11:14,990 --> 00:11:18,614
There's a way you can convert it to an Onyx model. Of course there's three

165
00:11:18,652 --> 00:11:21,590
steps loaded in existing model into memory.

166
00:11:22,410 --> 00:11:25,240
Convert to an Onyx and save that Onyx model.

167
00:11:28,270 --> 00:11:32,314
Here's an example of how you would use and

168
00:11:32,352 --> 00:11:37,690
convert from Pytorch to onnx.

169
00:11:38,350 --> 00:11:42,320
So you would load that model

170
00:11:43,010 --> 00:11:47,034
and provide some sample input and use this torch onnx

171
00:11:47,082 --> 00:11:50,510
to export it. Another way is know

172
00:11:50,580 --> 00:11:53,662
if you have it in keras. Same steps.

173
00:11:53,726 --> 00:11:57,042
You load the Keras model, convert the Keras model into

174
00:11:57,096 --> 00:12:00,994
Onyx, and then save it as

175
00:12:01,032 --> 00:12:05,586
a protoblock. And there

176
00:12:05,608 --> 00:12:09,206
is onyxml tools that

177
00:12:09,228 --> 00:12:13,302
you can do to pip install. You can also

178
00:12:13,356 --> 00:12:16,840
convert the Tensorflow model using command line.

179
00:12:17,210 --> 00:12:21,370
So where you specify, of course, your input, where's your

180
00:12:21,520 --> 00:12:24,954
saved model and then your output. A lot of good

181
00:12:24,992 --> 00:12:28,586
examples how you would do this on

182
00:12:28,608 --> 00:12:32,922
GitHub. This one is through ScikitLearn.

183
00:12:32,986 --> 00:12:36,590
Notice that there's an SKL to onnx

184
00:12:37,010 --> 00:12:42,222
where you can convert scikitlearn into

185
00:12:42,276 --> 00:12:45,934
an Onyx application or to onnx

186
00:12:46,062 --> 00:12:49,106
format. I keep

187
00:12:49,128 --> 00:12:52,180
on talking about Onyx. Let me go back real quick.

188
00:12:53,190 --> 00:12:57,926
There's this tool called nettron app

189
00:12:58,108 --> 00:13:02,280
that visualizes this Onyx model for you.

190
00:13:02,730 --> 00:13:06,434
And it also helps software

191
00:13:06,482 --> 00:13:10,534
engineers to kind of know what's

192
00:13:10,582 --> 00:13:14,426
the input and output of that existing model without going

193
00:13:14,448 --> 00:13:18,374
back to the data scientist, going back to the original

194
00:13:18,422 --> 00:13:21,914
code where it was trained from to know how to use can Onyx

195
00:13:21,962 --> 00:13:25,374
model. It visualizes the inputs and then

196
00:13:25,412 --> 00:13:29,258
be able to kind of visualize what the graph of operations

197
00:13:29,354 --> 00:13:34,478
would look like. If you go to Netron

198
00:13:34,574 --> 00:13:37,442
app, open an Onyx file there,

199
00:13:37,576 --> 00:13:41,700
and you can visualize it. All right.

200
00:13:42,950 --> 00:13:47,550
You can also use onnx as an intermediary format

201
00:13:47,710 --> 00:13:51,206
intermediate. Let's say if you

202
00:13:51,228 --> 00:13:55,314
have a Pytorch model and you want to convert it to Tensorflow, you can convert

203
00:13:55,362 --> 00:13:59,014
from Pytorch to Onyx and Onyx to Tensorflow.

204
00:13:59,062 --> 00:14:01,930
That is one way. Also,

205
00:14:02,000 --> 00:14:05,386
there's Onyx to core ML that you can

206
00:14:05,408 --> 00:14:08,906
use. There's ways also you

207
00:14:08,928 --> 00:14:12,846
can fine tune, can onyx model create

208
00:14:13,028 --> 00:14:16,590
and do transfer learning on an existing onnx model.

209
00:14:16,660 --> 00:14:20,606
If you're interested, of course

210
00:14:20,788 --> 00:14:24,242
you can train models in the cloud.

211
00:14:24,376 --> 00:14:26,210
You have a GPU clusters.

212
00:14:27,270 --> 00:14:30,914
But the important part here for me, and I wanted to talk more,

213
00:14:31,032 --> 00:14:34,210
this is your typical end to end machine learning process,

214
00:14:34,360 --> 00:14:37,800
where you have your experiments and you're building your

215
00:14:38,170 --> 00:14:41,880
base from your different iDe, or you create

216
00:14:43,130 --> 00:14:47,080
your training application. Once you train it,

217
00:14:47,950 --> 00:14:51,660
you would have a machine learning model.

218
00:14:52,670 --> 00:14:56,394
You register it somewhere in the cloud and

219
00:14:56,432 --> 00:15:00,206
manage these models. You can have these versionings and

220
00:15:00,228 --> 00:15:04,158
then based from that, kind of like when you have a

221
00:15:04,164 --> 00:15:07,886
docker image, you have kind of like Docker hub where you

222
00:15:07,908 --> 00:15:10,190
can store all these images.

223
00:15:10,930 --> 00:15:14,574
Also as your machine learning,

224
00:15:14,692 --> 00:15:18,146
as a way you can manage your models, you can upload these

225
00:15:18,248 --> 00:15:21,934
models there and be able to version

226
00:15:21,982 --> 00:15:26,358
them and also build a pipeline to create

227
00:15:26,524 --> 00:15:30,870
and to download these and incorporate it and create the image.

228
00:15:32,570 --> 00:15:35,750
So we did talk about step one,

229
00:15:35,820 --> 00:15:39,180
creating. Once we have an Onnx model,

230
00:15:39,950 --> 00:15:41,500
start deploying them.

231
00:15:42,430 --> 00:15:45,994
Okay, so we did talk about,

232
00:15:46,112 --> 00:15:49,754
as your data scientist, building kind of like a

233
00:15:49,792 --> 00:15:53,450
chef or a baker building your secret recipe.

234
00:15:53,610 --> 00:15:57,310
Now, let me ask you one thing. What is the difference between

235
00:15:57,380 --> 00:16:00,110
a baker and starting a bakery?

236
00:16:01,170 --> 00:16:04,900
Main difference is they all have different skill set.

237
00:16:05,990 --> 00:16:09,666
In order to create a successful business

238
00:16:09,768 --> 00:16:12,562
or successful bakery, you need both.

239
00:16:12,696 --> 00:16:16,086
Need the baker and also you

240
00:16:16,108 --> 00:16:18,790
need someone that actually manages the bakery.

241
00:16:19,850 --> 00:16:23,542
Software engineers are

242
00:16:23,596 --> 00:16:27,126
great at looking into how to

243
00:16:27,148 --> 00:16:30,518
start a bakery. They know where to put the cash here,

244
00:16:30,604 --> 00:16:34,314
how to collect money, right. How to create

245
00:16:34,512 --> 00:16:37,594
these pipelines and how you would display or

246
00:16:37,632 --> 00:16:41,566
use the application and be able to create

247
00:16:41,748 --> 00:16:44,400
those different areas of the business system,

248
00:16:45,650 --> 00:16:49,454
how you would use the

249
00:16:49,572 --> 00:16:53,354
machine learning model or how to create the whole application itself.

250
00:16:53,492 --> 00:16:57,154
What is the customer experience in all these different

251
00:16:57,352 --> 00:17:02,386
things? So it

252
00:17:02,408 --> 00:17:06,322
is important, whenever we create these machine learning

253
00:17:06,376 --> 00:17:09,686
models, it is important where we're going to

254
00:17:09,708 --> 00:17:12,520
deploy them. Some things to think about, right?

255
00:17:14,410 --> 00:17:17,786
You deploy it on a VM, you might want to deploy it on

256
00:17:17,808 --> 00:17:21,750
a Windows device or a Linux device or a Mac,

257
00:17:21,910 --> 00:17:25,660
you can deploy it on it. Edge devices or

258
00:17:26,030 --> 00:17:30,162
phones, different ways. How you would deploy

259
00:17:30,326 --> 00:17:34,606
and create these AI models and

260
00:17:34,628 --> 00:17:36,960
use these AI models. Of course,

261
00:17:38,130 --> 00:17:41,882
every time we think about deployment,

262
00:17:41,946 --> 00:17:45,378
think about where we can deploy this. We're going to deploy this to the cloud

263
00:17:45,464 --> 00:17:48,994
or at the edge. Edge meaning how close it is to

264
00:17:49,032 --> 00:17:51,170
your customers or your users.

265
00:17:52,790 --> 00:17:56,214
The analogy in that is McDonald's and

266
00:17:56,252 --> 00:17:59,906
subway. What's the difference in how they make the bread?

267
00:18:00,018 --> 00:18:03,826
Right. McDonald's most likely it's

268
00:18:03,858 --> 00:18:08,286
in not a warehouse, but it's outsourced.

269
00:18:08,418 --> 00:18:12,460
It's not at the edge, meaning it's not

270
00:18:13,230 --> 00:18:16,394
at the store, just compared to a

271
00:18:16,432 --> 00:18:20,506
subway, where you

272
00:18:20,528 --> 00:18:23,946
bake the bread at the store. So it's

273
00:18:23,978 --> 00:18:27,406
a different experience. Right. So what I'm trying to

274
00:18:27,428 --> 00:18:31,040
get at is whenever we talk about deployment, where we're going to run

275
00:18:33,410 --> 00:18:37,474
these AI models, where do we want to run them?

276
00:18:37,592 --> 00:18:41,742
Do we send the data to the cloud and then we run the inferencing

277
00:18:41,886 --> 00:18:45,254
at the cloud and then return the results to us,

278
00:18:45,292 --> 00:18:48,646
or at the edge, meaning closer to the user. Maybe it's on

279
00:18:48,668 --> 00:18:51,842
the phone, or maybe it's on the camera

280
00:18:51,906 --> 00:18:55,490
itself, or in a gateway

281
00:18:55,570 --> 00:18:58,806
closer to the user. So those

282
00:18:58,828 --> 00:19:02,218
are things we have to consider when we

283
00:19:02,384 --> 00:19:05,766
deploy these machine learning models,

284
00:19:05,878 --> 00:19:08,700
especially in the Onnx model. Of course,

285
00:19:10,030 --> 00:19:13,358
you can also deploy them in the cloud, how you would deploy them,

286
00:19:13,444 --> 00:19:17,520
since you already have registered in

287
00:19:17,970 --> 00:19:21,566
your machine learning model or your Onyx models in the cloud. As you

288
00:19:21,588 --> 00:19:24,318
build your image, you create your pipeline.

289
00:19:24,494 --> 00:19:29,698
That is one way where you

290
00:19:29,704 --> 00:19:33,010
can deploy it through a service, an app service.

291
00:19:33,080 --> 00:19:36,422
You can deploy it and run it in a

292
00:19:36,556 --> 00:19:40,360
docker container or in Kubernetes service.

293
00:19:41,850 --> 00:19:44,390
Speaking of docker images,

294
00:19:45,770 --> 00:19:49,180
there are Onyx Docker images that you can start using.

295
00:19:50,110 --> 00:19:54,186
There's an Onyx base that has minimal dependency that you

296
00:19:54,208 --> 00:19:57,740
can use it. If you want to run

297
00:19:58,190 --> 00:20:02,794
use onyx into your application. There's Onyx ecosystem

298
00:20:02,922 --> 00:20:06,686
that allows you to be able to convert without

299
00:20:06,868 --> 00:20:10,446
an installer, right? So let's say

300
00:20:10,468 --> 00:20:13,700
if you just want to convert an existing Onyx model,

301
00:20:14,470 --> 00:20:17,794
an existing application or an

302
00:20:17,832 --> 00:20:21,758
existing machine learning model, let's say it was written in Pytorch.

303
00:20:21,854 --> 00:20:25,846
You don't want to download all

304
00:20:25,868 --> 00:20:29,222
the converters locally in your machine. You can just

305
00:20:29,356 --> 00:20:33,750
use these docker

306
00:20:34,730 --> 00:20:38,006
images. So whenever we

307
00:20:38,028 --> 00:20:41,606
talk about edge, what is the edge?

308
00:20:41,718 --> 00:20:45,386
Remember the definition is how close it is to your customers

309
00:20:45,488 --> 00:20:48,954
or to your users. But of course, every time we

310
00:20:48,992 --> 00:20:52,334
think about the edge, we'll talk about

311
00:20:52,372 --> 00:20:56,206
deployment. When we deploy it to the cloud,

312
00:20:56,308 --> 00:21:00,666
most likely it's just you're deploying to the data centers. Maybe it's thousands

313
00:21:00,698 --> 00:21:04,626
of devices. If we talk about we're going to deploy it

314
00:21:04,648 --> 00:21:09,394
in 5g infrastructure where

315
00:21:09,432 --> 00:21:13,922
we deploy it to the fog, which is maybe

316
00:21:13,976 --> 00:21:17,798
just millions of devices, millions of

317
00:21:17,884 --> 00:21:21,750
models where you're going to deploy these. And of course, when you talk about edge

318
00:21:22,330 --> 00:21:25,894
might be billions of devices depending on the

319
00:21:25,932 --> 00:21:30,394
need, because each

320
00:21:30,432 --> 00:21:34,730
device may have those different deployment structure.

321
00:21:35,710 --> 00:21:39,322
So why would you want to deploy your machine learning

322
00:21:39,376 --> 00:21:42,686
model on the edge or run it on the

323
00:21:42,708 --> 00:21:45,562
edge? One is low latency.

324
00:21:45,706 --> 00:21:48,814
Think about, let's say you're collecting videos, right?

325
00:21:48,852 --> 00:21:52,394
You're doing inferencing based from

326
00:21:52,452 --> 00:21:57,470
video or sound. You want it faster,

327
00:21:57,630 --> 00:22:01,874
so it makes sense to run it locally on

328
00:22:01,912 --> 00:22:05,154
that device itself. So it's load in.

329
00:22:05,192 --> 00:22:08,854
See, think about it. If you have to ship that to the cloud,

330
00:22:08,972 --> 00:22:11,670
you have to ship each images, each frame.

331
00:22:12,330 --> 00:22:16,418
That might cost you money and of course produce scalability.

332
00:22:16,594 --> 00:22:20,250
So it might make sense to run it at the edge

333
00:22:21,070 --> 00:22:24,810
to provide scalability. Another one is flexibility.

334
00:22:25,230 --> 00:22:28,774
So it might make sense to run it locally

335
00:22:28,822 --> 00:22:32,190
so you don't have the need for Internet connection.

336
00:22:32,690 --> 00:22:36,830
Also rules, privacy rules,

337
00:22:37,170 --> 00:22:40,800
want to send any

338
00:22:42,690 --> 00:22:48,000
personally identify Pii information or

339
00:22:48,530 --> 00:22:52,318
might make sense to local laws that

340
00:22:52,404 --> 00:22:57,762
it's limited to certain geographical

341
00:22:57,826 --> 00:23:01,046
areas. So it might make sense to. It gives

342
00:23:01,068 --> 00:23:04,550
you that flexibility where you want to deploy

343
00:23:04,970 --> 00:23:08,330
and where you want to run this inferencing.

344
00:23:09,470 --> 00:23:13,146
There is Onyx runtime where you can run it's a

345
00:23:13,168 --> 00:23:16,602
high performance inference engine for

346
00:23:16,656 --> 00:23:20,174
your onnx models. It is actually open

347
00:23:20,212 --> 00:23:24,266
sourced by Microsoft under MIT license. So it's

348
00:23:24,298 --> 00:23:27,802
not just limited to neural networks.

349
00:23:27,946 --> 00:23:32,062
Also for traditional machine learning spec it has

350
00:23:32,116 --> 00:23:35,394
extensible architecture that allows to

351
00:23:35,432 --> 00:23:38,050
have different hardware accelerators.

352
00:23:38,790 --> 00:23:42,130
It's part of Windows ten as

353
00:23:42,280 --> 00:23:46,094
Winml. And if you want to learn more about Onyx Runtime,

354
00:23:46,222 --> 00:23:50,760
there's Onnx runtime AI website.

355
00:23:52,090 --> 00:23:56,706
The good thing about this is there's

356
00:23:56,738 --> 00:23:59,882
this part where I think it's pretty neat or

357
00:24:00,016 --> 00:24:04,006
let's say if you want to use different platforms,

358
00:24:04,038 --> 00:24:07,706
let's say I'm going to create a Linux application and I

359
00:24:07,728 --> 00:24:11,534
want to create a C sharp using C

360
00:24:11,572 --> 00:24:14,734
sharp API and this architecture X

361
00:24:14,772 --> 00:24:18,014
86. If you want to run ARM 64, you can

362
00:24:18,052 --> 00:24:21,310
select them and then you have these different architecture,

363
00:24:21,730 --> 00:24:25,374
different hardware accelerators. So if you want to use the gpu,

364
00:24:25,502 --> 00:24:29,106
select CUDA, or you can just use default cpu and it will

365
00:24:29,128 --> 00:24:32,738
give you instructions how you can incorporate it to

366
00:24:32,824 --> 00:24:33,860
your application.

367
00:24:36,150 --> 00:24:39,894
Notice that there's different hardware accelerators. So like

368
00:24:39,932 --> 00:24:43,186
for example if you wanted to run Openvino, you have to convert

369
00:24:43,218 --> 00:24:46,354
it. You don't have to convert let's say a Pytorch

370
00:24:46,402 --> 00:24:50,598
model to something that's compatible with Openvino.

371
00:24:50,694 --> 00:24:54,426
You can go Pytorch to Onyx and

372
00:24:54,448 --> 00:24:59,126
then use Onyx runtime with the Openvino

373
00:24:59,318 --> 00:25:00,910
hardware accelerator.

374
00:25:02,610 --> 00:25:06,394
Like I said, onnx runtime ships with Windows AI

375
00:25:06,442 --> 00:25:09,790
platform. So if you're

376
00:25:10,530 --> 00:25:13,470
as part of the Winml API,

377
00:25:13,810 --> 00:25:17,742
which is a practical, a simple model based API

378
00:25:17,806 --> 00:25:22,142
for inferencing in Windows. So let's say if you have can existing

379
00:25:22,286 --> 00:25:26,094
forms application and you want to add machine

380
00:25:26,142 --> 00:25:29,778
learning model, or you want to add

381
00:25:29,864 --> 00:25:33,258
machine learning to a windforms

382
00:25:33,294 --> 00:25:36,760
application, this allows you to be able to do that.

383
00:25:37,870 --> 00:25:41,210
There's also direct ML API,

384
00:25:41,790 --> 00:25:45,642
so that if you're creating a game, there is a way

385
00:25:45,696 --> 00:25:49,018
to be able to use direct ML that runs on

386
00:25:49,024 --> 00:25:52,682
top of DirectX twelve which has a real

387
00:25:52,736 --> 00:25:57,470
time high control machine learning operator API.

388
00:25:58,050 --> 00:26:01,806
And of course you have these robust driver models that

389
00:26:01,828 --> 00:26:05,050
it automatically knows if you have a gpu,

390
00:26:05,130 --> 00:26:08,878
a VPU or XPU fully defined,

391
00:26:08,974 --> 00:26:12,466
but it automatically switches. If it can

392
00:26:12,488 --> 00:26:15,606
run a cpu it would use it. If it can run in

393
00:26:15,628 --> 00:26:19,526
any one of these, then you'll be able to use that. That's how

394
00:26:19,548 --> 00:26:22,662
it's able to access those

395
00:26:22,716 --> 00:26:26,630
drivers. There is also Onnx JS

396
00:26:27,130 --> 00:26:30,986
which is a JavaScript library to run Onnx models in the

397
00:26:31,008 --> 00:26:35,446
browser or even in node it's using WebGL

398
00:26:35,558 --> 00:26:39,462
and webassembly and it could automatically

399
00:26:39,526 --> 00:26:42,718
use CPU or GPU. So think about

400
00:26:42,884 --> 00:26:45,790
let's say you have it in your browser.

401
00:26:46,130 --> 00:26:50,142
What it had to do is it would download the

402
00:26:50,276 --> 00:26:54,290
Onnx model to the browser

403
00:26:54,630 --> 00:26:58,386
locally and then use onyxjs to be

404
00:26:58,408 --> 00:27:02,210
able to use inferencing.

405
00:27:03,270 --> 00:27:07,414
So instead of sending it to cloud, the Onnx model

406
00:27:07,612 --> 00:27:11,574
is actually locally on the Chrome browser or

407
00:27:11,612 --> 00:27:16,422
on that browser itself and doing

408
00:27:16,476 --> 00:27:20,230
inferencing that way. It is compatible with Chrome

409
00:27:20,310 --> 00:27:23,546
Edge, Firefox, Opera. If you want

410
00:27:23,568 --> 00:27:27,740
an electron app, you can also integrate it with your node application.

411
00:27:28,110 --> 00:27:32,110
It's not just desktop, also mobile,

412
00:27:33,010 --> 00:27:36,480
Chrome Edge Firefox you can use too.

413
00:27:37,250 --> 00:27:40,880
All right, I'll do a little bit of demo. If you're interested in

414
00:27:41,570 --> 00:27:44,946
getting what I'm using to demo, here is

415
00:27:44,968 --> 00:27:47,620
the link and I will show that later. Again,

416
00:27:49,110 --> 00:27:52,500
let me pull this application

417
00:27:54,470 --> 00:27:57,640
for you back there.

418
00:27:59,050 --> 00:28:04,662
Okay. So if you go to that link, it will get you this

419
00:28:04,716 --> 00:28:07,560
application or this website.

420
00:28:08,590 --> 00:28:12,682
If you want to try out our demo today,

421
00:28:12,736 --> 00:28:15,660
you click this out to try it out.

422
00:28:16,910 --> 00:28:21,200
And what it'll do is it would pull up the

423
00:28:22,210 --> 00:28:28,766
docker file and create an instance of

424
00:28:28,788 --> 00:28:32,110
a Jupyter notebook using binder.

425
00:28:32,790 --> 00:28:35,620
This is what it looks like.

426
00:28:37,990 --> 00:28:41,426
So now that the kernel is ready, this is a c sharp application.

427
00:28:41,528 --> 00:28:46,086
So what I have here is running Jupyter notebook using.

428
00:28:46,188 --> 00:28:49,974
Net interactive so I can have a c sharp application.

429
00:28:50,172 --> 00:28:53,654
And what I want to demo here today is I wanted

430
00:28:53,692 --> 00:28:58,026
to convert a

431
00:28:58,048 --> 00:29:01,738
model trained in ML. Net into

432
00:29:01,824 --> 00:29:05,738
Onyx. So this is how I would get

433
00:29:05,824 --> 00:29:09,542
some nuget packages and download

434
00:29:09,606 --> 00:29:12,666
them. While it's downloading,

435
00:29:12,698 --> 00:29:16,240
let me kind of talk a little bit about the code.

436
00:29:16,930 --> 00:29:21,374
So this one right here is system

437
00:29:21,492 --> 00:29:24,506
IO. I'm using system IO,

438
00:29:24,618 --> 00:29:28,294
Microsoft data analysis Xplot

439
00:29:28,362 --> 00:29:32,606
plotly. And this one right here allows

440
00:29:32,638 --> 00:29:36,194
me to be able to format it properly, to display it

441
00:29:36,232 --> 00:29:39,890
properly on this Jupiter notebook.

442
00:29:39,970 --> 00:29:43,190
So it's just a library. Okay,

443
00:29:43,260 --> 00:29:46,854
let's wait until that one's done. So this

444
00:29:46,892 --> 00:29:51,974
one right here, I have a CSV file, salary CSV.

445
00:29:52,102 --> 00:29:55,706
Let me try to open that for you. So this is what

446
00:29:55,728 --> 00:29:59,062
it looks like. I have two columns on this csv file,

447
00:29:59,126 --> 00:30:02,286
years of experience and salary. I want the

448
00:30:02,308 --> 00:30:06,158
simplest example. I mean, this is not the best example if

449
00:30:06,164 --> 00:30:09,440
you're going to create a machine learning application. But I want to

450
00:30:09,970 --> 00:30:13,474
one input and one output. Input is your

451
00:30:13,512 --> 00:30:16,814
years experience. Output is salary.

452
00:30:16,942 --> 00:30:20,814
So we want to create a machine

453
00:30:20,862 --> 00:30:24,546
learning model that when you

454
00:30:24,568 --> 00:30:27,878
create years of experience, your input is years of experience.

455
00:30:28,044 --> 00:30:31,894
It would kind of guess how much is the salary based from that experience.

456
00:30:32,012 --> 00:30:34,280
It's just a contrived example.

457
00:30:35,450 --> 00:30:38,726
Okay, let's go back here.

458
00:30:38,908 --> 00:30:42,806
Now that that one is done, it was able to download all my nuget

459
00:30:42,838 --> 00:30:46,380
packages. Now I'm using to run this system.

460
00:30:46,830 --> 00:30:50,206
I'm going to run this to your application right here so that

461
00:30:50,228 --> 00:30:54,270
I can load the csv file using data frame.

462
00:30:54,850 --> 00:30:58,862
And based on that, this is what my data looks like,

463
00:30:58,916 --> 00:31:02,254
right? Notice that as

464
00:31:02,292 --> 00:31:05,790
the years increases the salary.

465
00:31:06,710 --> 00:31:10,802
So it's just simple example and

466
00:31:10,856 --> 00:31:14,306
looking at this description, then it gives me

467
00:31:14,408 --> 00:31:17,480
what's the min, the max. Right now I only have

468
00:31:18,410 --> 00:31:21,718
30 items on my list. And so at the

469
00:31:21,724 --> 00:31:25,286
end of the day, what this one is trying to do using ML net,

470
00:31:25,468 --> 00:31:28,586
I want to create a pipeline to be

471
00:31:28,608 --> 00:31:31,706
able to train a model.

472
00:31:31,808 --> 00:31:34,938
Whereas in order to train, there's two things you have to

473
00:31:34,944 --> 00:31:39,370
do. You have to use ML net and once you have that context,

474
00:31:39,970 --> 00:31:43,710
you create the pipeline and then you do fit

475
00:31:43,860 --> 00:31:46,350
and transform. There's always that pair.

476
00:31:47,170 --> 00:31:49,520
So once you have a transformer model,

477
00:31:50,930 --> 00:31:54,626
it'll create that model for

478
00:31:54,648 --> 00:31:58,066
you. So what I want

479
00:31:58,088 --> 00:32:01,442
to do now is now that I have that

480
00:32:01,496 --> 00:32:05,254
model, I want to convert it to

481
00:32:05,292 --> 00:32:09,174
Onyx. So I

482
00:32:09,212 --> 00:32:12,786
use context model convert

483
00:32:12,818 --> 00:32:17,030
to onyx. I pass in the stream

484
00:32:17,790 --> 00:32:21,162
and my data and

485
00:32:21,216 --> 00:32:26,666
what it'll do is it would create these

486
00:32:26,848 --> 00:32:29,340
model onyx for me.

487
00:32:30,110 --> 00:32:31,740
See where that one is?

488
00:32:33,150 --> 00:32:36,000
Model onyx. Put it in here.

489
00:32:40,610 --> 00:32:42,960
So let's try to open it again.

490
00:32:43,650 --> 00:32:47,438
There you go. See, I noticed Onyx

491
00:32:47,534 --> 00:32:50,260
model was generated for me.

492
00:32:50,950 --> 00:32:53,780
Think I can open that Onyx model?

493
00:32:55,130 --> 00:32:59,240
Okay, so now that I have an Onyx model,

494
00:33:00,410 --> 00:33:03,720
let me try to verify it and see how I can run.

495
00:33:04,730 --> 00:33:08,554
I'm going to open another project. This time I

496
00:33:08,592 --> 00:33:12,650
have this Onyx inference Python notebook.

497
00:33:13,950 --> 00:33:18,220
This is not a c sharp application,

498
00:33:18,910 --> 00:33:22,110
so I want to change my kernel.

499
00:33:23,570 --> 00:33:27,550
Let's change this kernel into a python.

500
00:33:29,250 --> 00:33:33,330
So this time I want to use Onyx runtime in Python

501
00:33:34,230 --> 00:33:38,110
to do the inferencing on that model onyx

502
00:33:38,270 --> 00:33:42,494
file. So I do pip install onyx

503
00:33:42,542 --> 00:33:46,530
runtime. And what I'll do is it would download all the necessary

504
00:33:46,610 --> 00:33:49,926
requirements to install to get

505
00:33:50,028 --> 00:33:53,654
Onyx runtime library. Of course,

506
00:33:53,692 --> 00:33:57,474
here I'm just importing them and I create

507
00:33:57,532 --> 00:33:59,050
this inference session.

508
00:34:00,270 --> 00:34:04,134
So notice that this model Onyx,

509
00:34:04,182 --> 00:34:07,914
if you go to netron app, it would display something

510
00:34:07,952 --> 00:34:11,274
like this where you can view the

511
00:34:11,312 --> 00:34:14,794
contents of your onnx model. And notice

512
00:34:14,842 --> 00:34:18,570
that it gives me the input and then the output.

513
00:34:18,730 --> 00:34:21,934
This input gives me the years of experience and

514
00:34:21,972 --> 00:34:25,650
salary. The output is like this.

515
00:34:25,800 --> 00:34:29,074
So whatever we're interested in is input. In this

516
00:34:29,112 --> 00:34:32,158
case, it's only years of experience salary.

517
00:34:32,254 --> 00:34:36,162
That's the one. We're trying to guess this point when you're doing inferencing.

518
00:34:36,306 --> 00:34:40,246
So it would be ignored, but it would use all

519
00:34:40,268 --> 00:34:43,638
your inputs. So even if you place a

520
00:34:43,644 --> 00:34:47,550
number here, it would be ignored, it won't

521
00:34:47,570 --> 00:34:51,510
be used. Notice how that one is not connected.

522
00:34:51,590 --> 00:34:56,038
So it's only using years experience. He's using feature vectorizer

523
00:34:56,134 --> 00:35:00,410
to be able to create this linear regressor

524
00:35:01,490 --> 00:35:04,702
to get the output. Of course,

525
00:35:04,756 --> 00:35:07,822
here, all these are not going to be used anyway.

526
00:35:07,876 --> 00:35:11,950
It's just going to be stub. What we're interested is the output

527
00:35:12,710 --> 00:35:14,900
right here. Okay,

528
00:35:15,750 --> 00:35:19,842
so this one, what I wanted

529
00:35:19,896 --> 00:35:23,774
to do is to get the name, the shape

530
00:35:23,822 --> 00:35:27,080
and the type of years experience,

531
00:35:28,810 --> 00:35:32,342
input, years, shape and type. This one is

532
00:35:32,396 --> 00:35:35,910
for another one for the salary.

533
00:35:36,490 --> 00:35:40,346
Years salary shape and type

534
00:35:40,528 --> 00:35:43,994
kind of gives me the descriptor and of course the

535
00:35:44,032 --> 00:35:47,020
output gives me the shape and type.

536
00:35:48,750 --> 00:35:51,920
In this case, how did I get four?

537
00:35:52,770 --> 00:35:55,150
It's the fourth of the output.

538
00:35:56,130 --> 00:35:59,214
So 01234.

539
00:35:59,332 --> 00:36:01,630
Right. So that's the fourth output.

540
00:36:03,190 --> 00:36:06,834
Let's run this one too. So now that I have

541
00:36:06,872 --> 00:36:10,900
that I can pass in that data,

542
00:36:11,670 --> 00:36:15,070
in this case pass in input experience,

543
00:36:15,240 --> 00:36:19,142
input salary, and it specify the years because

544
00:36:19,196 --> 00:36:22,680
I know the type and I need to

545
00:36:23,290 --> 00:36:26,550
place them into these array.

546
00:36:28,750 --> 00:36:31,610
So I got the years salary.

547
00:36:32,430 --> 00:36:36,106
Notice how I put zero because it's going to be ignored anyway. So let's say

548
00:36:36,128 --> 00:36:40,380
I change this to ten and

549
00:36:41,490 --> 00:36:44,730
the output would be identified

550
00:36:44,810 --> 00:36:48,046
here. And notice that if

551
00:36:48,068 --> 00:36:53,700
I have ten, that would be the value of

552
00:36:54,870 --> 00:36:58,114
the result. And now I can grab that

553
00:36:58,152 --> 00:37:02,370
one and that would be my output.

554
00:37:03,190 --> 00:37:08,102
So if I change it again to say three

555
00:37:08,156 --> 00:37:11,494
years, three and a half years, see what

556
00:37:11,532 --> 00:37:15,510
happens, and then I'll have a different output.

557
00:37:16,250 --> 00:37:18,700
Okay, so what happened so far?

558
00:37:19,470 --> 00:37:23,210
What I did was to train

559
00:37:23,280 --> 00:37:27,594
a model, export it to

560
00:37:27,632 --> 00:37:32,894
an Onyx file using ML net in

561
00:37:32,932 --> 00:37:36,720
c sharp. And based from that Onyx model,

562
00:37:37,570 --> 00:37:41,520
I use Onyx runtime to

563
00:37:41,890 --> 00:37:46,740
use it in my python application and

564
00:37:47,430 --> 00:37:50,900
do inferencing that way.

565
00:37:53,510 --> 00:37:57,080
This is how it feels like after learning all these things.

566
00:37:57,450 --> 00:38:01,080
Now it kind of connects all the different,

567
00:38:02,570 --> 00:38:06,562
how I can just easily

568
00:38:06,626 --> 00:38:10,666
use an existing Onnx is

569
00:38:10,688 --> 00:38:14,058
a way for us to be able lead software engineer to

570
00:38:14,064 --> 00:38:17,386
be able to talk to data scientists and

571
00:38:17,408 --> 00:38:21,222
also data scientists to talk lead lead software

572
00:38:21,366 --> 00:38:25,062
engineer that we can use these secret

573
00:38:25,126 --> 00:38:28,814
recipes, right, use these machine learning models and

574
00:38:28,852 --> 00:38:32,320
integrate it to our application. At the end of the day,

575
00:38:34,470 --> 00:38:37,700
all our best effort and

576
00:38:38,630 --> 00:38:40,740
all our programs. Actually,

577
00:38:43,270 --> 00:38:46,742
as long as we can integrate it to our application,

578
00:38:46,876 --> 00:38:51,080
not just existing application or any greenfield application,

579
00:38:52,170 --> 00:38:55,510
we can start incorporating these machine learning models

580
00:38:57,470 --> 00:39:00,966
through Onnx. So we did talk about as a recap,

581
00:39:01,158 --> 00:39:04,780
what is onyx? It's an open standard.

582
00:39:05,310 --> 00:39:08,602
Use the right tool for the right job and how

583
00:39:08,656 --> 00:39:12,446
you can efficiently run it on a target platform.

584
00:39:12,548 --> 00:39:16,046
It separates out how you train it and how you

585
00:39:16,068 --> 00:39:20,126
would run and do inferencing on that

586
00:39:20,308 --> 00:39:23,602
model. How you would create Onnx model.

587
00:39:23,656 --> 00:39:27,122
I did show you how to download it from the Onnx model

588
00:39:27,176 --> 00:39:31,300
zoo. You can create it and convert using

589
00:39:31,910 --> 00:39:33,730
some of the Onyx convert.

590
00:39:37,370 --> 00:39:41,254
There's different ways how you can create can onnx model. You can

591
00:39:41,292 --> 00:39:44,870
also deploy Onnx model.

592
00:39:45,020 --> 00:39:49,270
You can deploy it through Windows. NET JavaScript

593
00:39:49,430 --> 00:39:53,642
using Onyx. JS did

594
00:39:53,696 --> 00:39:59,500
a demo how you would use Onyx Runtime in Python to

595
00:40:00,110 --> 00:40:03,840
it in high performance. All right,

596
00:40:05,250 --> 00:40:08,090
if you want to learn more about me, my name is Ron Dagdag.

597
00:40:08,170 --> 00:40:11,978
I'm a lead software engineer at Spacey. I'm a 50 year Microsoft

598
00:40:12,154 --> 00:40:15,326
MVP. The best way to contact me is

599
00:40:15,348 --> 00:40:19,370
through LinkedIn or Twitter at Ron Dagdag

600
00:40:19,530 --> 00:40:23,738
I appreciate you geeking out with me about Onnx,

601
00:40:23,914 --> 00:40:28,870
Onnx Runtime, about Jupyter notebooks,

602
00:40:29,210 --> 00:40:33,510
about bakeries, bakers and breads.

603
00:40:33,850 --> 00:40:36,740
Thank you very much. Have a good day.

