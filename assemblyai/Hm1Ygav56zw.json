{"language_code": "en_us", "audio_url": "https://cdn.assemblyai.com/upload/fc88ca9e-e227-4e25-a4f8-0dff37776676", "punctuate": true, "format_text": true, "dual_channel": null, "webhook_url": null, "webhook_auth_header_name": null, "webhook_auth_header_value": null, "audio_start_from": null, "audio_end_at": null, "word_boost": ["asurion", "automated", "be", "by", "can", "checking", "clients", "code", "conditions", "conf fourty two", "devops engineer", "from", "improved", "ips", "kaye alvarado", "limits", "loadtesting", "pipelines", "realworld", "scenarios", "servers", "subjecting", "traffic", "type", "using"], "boost_param": "high", "filter_profanity": false, "redact_pii": false, "redact_pii_audio": false, "redact_pii_policies": null, "redact_pii_sub": null, "speaker_labels": false, "speakers_expected": null, "content_safety": false, "content_safety_confidence": null, "iab_categories": false, "custom_spelling": null, "disfluencies": false, "sentiment_analysis": false, "auto_chapters": false, "entity_detection": false, "summarization": true, "summary_model": "informative", "summary_type": "bullets_verbose", "auto_highlights": true, "language_detection": false, "speech_threshold": null, "id": "35db7135-aa4a-40db-8ada-ee7378b2e869", "status": "completed", "error": null, "text": "Hi everyone, today I'll be sharing about the topic loadtesting for multiple clients with a pipeline. We recently migrated our platform to a more modern architecture and has to ensure that the sizing of the new architecture maintains the same high availability of the previous one. To do this, we used load test as code, where we simulated real production traffic in an isolated nonproduction environment. Loadtesting in its definition is a type of performance test checking the upper limits of your system by subjecting it to extreme load conditions. Here we see a single client making a request to the server. Using load testing tools, we can build load test codes and simulate multiple user requests sent based on a specific load and timeline that we specify. There are several load testing tools in the market. In choosing a tool, we consider the following in making a decision on the tool that we will use. Most of the tools here, like Jmeter, locust and Gatling have the same features, so the final decision can also be a matter of preference, like if your developers have experience using a particular tool or are more used to the programming language used by a certain tool. So for us it was Katlin. And again, I won't be showing some data on how to select one, but would like to give an overview of how architects or engineers usually make this decision based on DevOps considerations. Now let's get to coding and walking through the components of the load test code in Gatling. Here we see the configuration file of Gatling. This is named Gatling Conf. Inside the conf directory of the code, a few mandatory lines that we need to update are the simulations and resources directory. We need to instruct Gatling about our code structure and where we are expecting it to load the code to run the tests. Resources are a set of values that you feed to your simulation tests. You can set up a scenarios and use values in any resource file for load tests where you are simulating different parameter values for a single scenario. You can also add a resource file for sample data or anything that you will be using in the simulation code. Next are simulations. Simulations contain your load tests. It's commonly coded in Scala, but it's now available with Java. If you are more familiar with this language in creating simulation code, let's add this code structure to a scala file. Diving deeper to the simulation code, we can add variables that we can use throughout the simulation. An example here is how we can assign credentials with value that we feed with a map. This map can refer back to the resource section of our code here. We can also create an HTTP configuration and store it in a variable. We can also add common headers like accept and content type. Next, we need a scenario which is the actual test. We can feed the credentials, execute an HTTP call, add headers, and also state the expected result from the API call. Finally, we set up the scenario and inject users based on common injection options where could be either open or closed. If we have several scenarios, we can add them in the same file delimited by a comma. We run the simulation by a shell script that contains instructions to run the Java program. This will open up a selection for the number of simulations we have in our directory and allow selection for which simulation to run. During the run, the screen will refresh every few seconds showing the current load test until it completes. The load test will produce an HTML report that shows various metrics like number of success versus failed requests, individual metrics like response times of each request, execution, success and failures. It will also show the increase, decrease, or constancy of the users sent by the load test tool and its equivalent responses per second. There's a few more graphs in the HTML report produced by Gatling that may be helpful in what you're trying to find out. With these data, you can easily confirm whether something needs to be tweaked in your infrastructure based on how it responds. Now, as a platform, we need to be able to simulate load testing from a lot more clients, even if one client is sending traffic with a heavy load. In the real world scenario, we expect the load to come from a distributed number of clients with different ips. So the simple solution is putting this in a pipelines. We simply put our code in GitHub and trigger the pipelines in parallel to spin up multiple GitHub runners, each with their own unique server ips, and each one sends traffic to our platform, thus simulating the real world scenario more accurately. And that's it. That's the solution. So before we mark this as done, I'll quickly walk you through some other integration decisions we made when putting together the Gatling and GitHub actions architecture. First is how to programmatically select a simulation. This one's fairly easy, as there's a config file used by Gatling where we can force simulation selection by merely updating it in the config file. This way we are not prompted with what file to run. It is automatically selected once we trigger the run command. The simple trick ips replacing this in the file in one of the GitHub action steps. Now we also want to make this reusable across environments in our platform. We distinguish this with the domain of the API call. Again the solution to this IPS programmatically update the environment in the simulation code using a simple said command. Problems three and four are pretty much similar to the previous ones. We also do this programmatically by means of an input to the GitHub actions workflow and updating the values in the Gatling load test code. The GitHub actions code of running Gatling is pretty straightforward. We install the dependency we need, which is Java, then run the load test using the shell provided script. In terms of output. Since the reports are produced in the server, in this case the GitHub runner, we can use a GitHub action to simply create an artifact to expose this after the workflow runs. We can add other things here, like putting it in a remote server, or sending the file in an email or a slack message and that's it. We ran the code, get a report, and everything is automated. Some important things to note the main purpose of load testing is to test the performance of our platform or even an application. We want to design load tests such that it can test the upper limits or close to the breaking point of a system. We can try to adjust the load accordingly during runs, or determine a base scenario based on production data. We can also design load tests to continuously run in a schedule. We can send heavier traffic on a particular time of day and lower traffic on off hours. And as all systems go, this is a personal preference, but going serverless means reducing the headache that comes with maintaining your own servers. Use cloud as much as possible and choose the serverless option when you can. If you want to learn more about this, I also have a blog created where I further explained how to develop load test codes. Even if you don't code at all. You can find this link here, published at developers at Asurion Devcommunity and that's it. A solution doesn't have to be complicated. If you know different tools used in DevOps, maybe combining them can be all ready. Be a solution to a problem that you're wanting to solve. Just keep things simple. I hope you enjoyed the presentation.", "words": [], "utterances": null, "confidence": 0.947229916918431, "audio_duration": 560.0, "webhook_status_code": null, "webhook_auth": false, "summary": "- Using load testing tools, we can build load test codes and simulate multiple user requests sent based on a specific load and timeline. In choosing a tool, we consider the following in making a decision on the tool that we will use.\n- Gatling is commonly coded in Scala, but it's now available with Java. Simulations contain your load tests. The load test will produce an HTML report that shows various metrics. With these data, you can easily confirm whether something needs to be tweaked in your infrastructure.\n- Gatling can be used to simulate load testing from a lot more clients. The GitHub actions code of running Gatling is pretty straightforward. A solution doesn't have to be complicated. If you know different tools used in DevOps, maybe combining them can be all ready.", "auto_highlights_result": {"status": "success", "results": [{"count": 2, "rank": 0.11, "text": "load test codes", "timestamps": [{"start": 82876, "end": 84130}, {"start": 534134, "end": 535468}]}, {"count": 9, "rank": 0.1, "text": "load test", "timestamps": [{"start": 58020, "end": 58622}, {"start": 82876, "end": 83510}, {"start": 137948, "end": 138442}, {"start": 285590, "end": 286112}, {"start": 289398, "end": 289844}, {"start": 309614, "end": 310152}, {"start": 440816, "end": 441428}, {"start": 451434, "end": 451892}, {"start": 534134, "end": 534720}]}, {"count": 2, "rank": 0.09, "text": "load testing tools", "timestamps": [{"start": 80940, "end": 82082}, {"start": 93990, "end": 95078}]}, {"count": 1, "rank": 0.08, "text": "several load testing tools", "timestamps": [{"start": 93408, "end": 95078}]}, {"count": 4, "rank": 0.08, "text": "load testing", "timestamps": [{"start": 80940, "end": 81586}, {"start": 93990, "end": 94774}, {"start": 333772, "end": 334524}, {"start": 483958, "end": 484684}]}, {"count": 4, "rank": 0.08, "text": "simulation code", "timestamps": [{"start": 182508, "end": 183720}, {"start": 197558, "end": 198538}, {"start": 204388, "end": 205470}, {"start": 425714, "end": 426572}]}, {"count": 1, "rank": 0.07, "text": "extreme load conditions", "timestamps": [{"start": 73848, "end": 75610}]}, {"count": 1, "rank": 0.07, "text": "Gatling load test code", "timestamps": [{"start": 440394, "end": 442150}]}, {"count": 1, "rank": 0.07, "text": "performance test", "timestamps": [{"start": 68632, "end": 69858}]}, {"count": 1, "rank": 0.06, "text": "load test tool", "timestamps": [{"start": 309614, "end": 310810}]}, {"count": 1, "rank": 0.06, "text": "simulation selection", "timestamps": [{"start": 391568, "end": 393040}]}, {"count": 1, "rank": 0.06, "text": "current load test", "timestamps": [{"start": 285238, "end": 286112}]}, {"count": 1, "rank": 0.06, "text": "multiple GitHub runners", "timestamps": [{"start": 355838, "end": 357652}]}, {"count": 1, "rank": 0.06, "text": "multiple user requests", "timestamps": [{"start": 85266, "end": 87202}]}, {"count": 1, "rank": 0.06, "text": "different tools", "timestamps": [{"start": 548150, "end": 548924}]}]}, "content_safety_labels": null, "iab_categories_result": null, "chapters": null, "sentiment_analysis_results": null, "entities": null}