1
00:00:23,850 --> 00:00:27,314
Hi Kim fan here. Welcome to ML

2
00:00:27,362 --> 00:00:30,770
enhanced event streaming apps with Python microservices

3
00:00:30,930 --> 00:00:33,510
because I couldn't fit any more words in that title.

4
00:00:34,330 --> 00:00:37,910
I'm a principal developer advocate for

5
00:00:37,980 --> 00:00:41,634
all the major streaming technologies. Flink, Kafka,

6
00:00:41,682 --> 00:00:45,400
Pulsar, Nifi. When you use a lot of these together,

7
00:00:46,010 --> 00:00:48,722
I like to give them a catchy little name either. Flip,

8
00:00:48,786 --> 00:00:52,510
flippin flank, all kinds of cool stuff. I've been doing

9
00:00:52,580 --> 00:00:55,934
lots of different streaming stuff for a number of

10
00:00:55,972 --> 00:00:59,770
years, trying to grow the open source streaming

11
00:00:59,850 --> 00:01:04,274
and doing that for a lot of different companies. Applications all over

12
00:01:04,392 --> 00:01:08,466
in the community put out a newsletter every week

13
00:01:08,648 --> 00:01:12,034
called the Flipstack Weekly. Covers all the cool tech out

14
00:01:12,072 --> 00:01:15,700
there. Scan it, check it out. It is

15
00:01:16,170 --> 00:01:19,714
easy to check out. If you don't want to subscribe, just look at GitHub.

16
00:01:19,762 --> 00:01:23,350
I put all the previous 71 episodes there.

17
00:01:23,500 --> 00:01:26,806
Easy to read, get through quickly.

18
00:01:26,988 --> 00:01:30,614
Great stuff. So when you're building

19
00:01:30,812 --> 00:01:34,986
these type of real time applications, you need a team for

20
00:01:35,008 --> 00:01:38,954
your stream. And now that team is a bunch of open source tools like

21
00:01:38,992 --> 00:01:42,090
Nifi, Flank, Pulsar, Kafka, Spark,

22
00:01:42,250 --> 00:01:45,390
Python, of course, Trino,

23
00:01:45,730 --> 00:01:50,080
and also other people you work with. So I

24
00:01:50,450 --> 00:01:54,226
thought I would pull in one of the top experts in

25
00:01:54,248 --> 00:01:57,618
Apache Pulsar to cover that section. So make it

26
00:01:57,624 --> 00:02:01,518
a little more understandable. I'm going to bring in my friend David,

27
00:02:01,614 --> 00:02:04,946
who literally wrote the book on Pulsar to

28
00:02:04,968 --> 00:02:08,598
give us a little background on that. Figured might as well go to

29
00:02:08,604 --> 00:02:12,790
the source and not have me go through that secondary.

30
00:02:13,530 --> 00:02:16,866
And while that's loading, we'll get I'm a committer

31
00:02:16,898 --> 00:02:20,134
on the Apache Pulsar project and also author of the book

32
00:02:20,172 --> 00:02:24,326
Pulsar in action by Manning Press. I formerly worked at Splunk

33
00:02:24,358 --> 00:02:27,974
as a principal software engineer with the day to day operations

34
00:02:28,022 --> 00:02:31,722
of the Pulsar as a service platform within that organization

35
00:02:31,866 --> 00:02:35,578
and served in several director and leadership roles

36
00:02:35,674 --> 00:02:38,750
at startups including Streamlio and Hortonworks.

37
00:02:40,050 --> 00:02:43,646
So I want to talk to you today about Apache Pulsar and why it

38
00:02:43,668 --> 00:02:47,554
has a growing and vibrant community. As you can see from these statistics here,

39
00:02:47,752 --> 00:02:51,618
we have a large number of contributors. We're up over 600 contributors now

40
00:02:51,784 --> 00:02:55,266
to the Pulsar project, 7000 plus Slack members in

41
00:02:55,288 --> 00:02:58,646
our open source Apache Slack channel, where our day to day Q and

42
00:02:58,668 --> 00:03:02,200
A questions are answered vibrantly throughout the community themselves,

43
00:03:02,570 --> 00:03:06,258
10,000 plus individual commits and back into the project and growing.

44
00:03:06,354 --> 00:03:10,150
And over 1000 organizations worldwide are using Apache Pulsar

45
00:03:10,230 --> 00:03:14,282
to solve their event streaming and messaging use cases. So why

46
00:03:14,336 --> 00:03:17,930
is Apache Pulsar growing in such popularity and gaining traction

47
00:03:18,350 --> 00:03:22,506
out there. It really comes down to the feature set that Apache Pulsar offers

48
00:03:22,538 --> 00:03:26,350
and some of its architectural differences that make it stand out from the other

49
00:03:26,500 --> 00:03:29,520
messaging solutions in the market today.

50
00:03:30,290 --> 00:03:34,530
Chief among them is the ability to scale elastically

51
00:03:35,030 --> 00:03:38,402
both horizontally and vertically to meet

52
00:03:38,456 --> 00:03:41,534
your peak demand, and then scale back down to minimize

53
00:03:41,582 --> 00:03:45,294
your infrastructure spend after your peak demand workload

54
00:03:45,342 --> 00:03:48,566
has been processed. And this is only capable due to

55
00:03:48,588 --> 00:03:52,262
the fact that we've architected Apache Pulsar in a cloud

56
00:03:52,316 --> 00:03:56,390
native way from day one. And what I mean by that is that we've completely

57
00:03:56,460 --> 00:04:00,346
decoupled the storage and compute layers in such a way that they

58
00:04:00,368 --> 00:04:02,700
can scale up independently from one another,

59
00:04:03,630 --> 00:04:06,170
so that if you have a burst of demand,

60
00:04:06,510 --> 00:04:10,138
brokers can be scaled up to serve the incoming request and

61
00:04:10,144 --> 00:04:13,614
then they can be scaled back down and the data retains there

62
00:04:13,652 --> 00:04:17,658
in the storage layer. This allows for seamless and instant partitioning

63
00:04:17,754 --> 00:04:21,918
and rebalancing of partition topics within pulsar without

64
00:04:22,084 --> 00:04:25,774
stop the world event. That's common in Apache Pulsar, where the

65
00:04:25,812 --> 00:04:29,614
entire topic is down until the data can be repartitioned and a new partition

66
00:04:29,742 --> 00:04:33,106
added as data is shuffled across the different brokers. That is

67
00:04:33,128 --> 00:04:36,438
not the case for pulsar, but it's more than just that. If it was just

68
00:04:36,444 --> 00:04:40,434
that, that wouldn't be really interesting. It's also built in georeplication

69
00:04:40,562 --> 00:04:43,794
and geographic redundancy and continuous availability

70
00:04:43,922 --> 00:04:47,858
features built into the Apache Pulsar framework that are second to none.

71
00:04:48,034 --> 00:04:51,338
That was a primary use case for Apache Pulsar back in 2012 when

72
00:04:51,344 --> 00:04:55,542
it was developed internally inside of Yahoo was for full mesh georeplication

73
00:04:55,606 --> 00:04:59,446
across eight data eight data centers that are geographically distributed

74
00:04:59,478 --> 00:05:02,966
across the globe. This capability is second to none

75
00:05:02,998 --> 00:05:06,414
in the messaging system today and allows you to replicate data very

76
00:05:06,452 --> 00:05:09,966
easily from point to point and is built in natively to the system,

77
00:05:10,148 --> 00:05:14,126
and it goes beyond just the ability to copy the data from

78
00:05:14,148 --> 00:05:17,534
one cluster to another. We also have what we call replicated

79
00:05:17,582 --> 00:05:20,994
subscriptions, so you keep track of where you are in that processing model as

80
00:05:21,032 --> 00:05:24,546
well as you transition clients over. And last but not least,

81
00:05:24,648 --> 00:05:28,346
we have failure aware clients built into the Apache Pulsar library

82
00:05:28,478 --> 00:05:31,954
where you can specify this is my preferred active primary cluster,

83
00:05:32,082 --> 00:05:35,586
and if you lose connectivity to that cluster, you can immediately

84
00:05:35,618 --> 00:05:39,410
switch those clients over to what you configure to be a secondary standby cluster

85
00:05:39,490 --> 00:05:43,462
so they can continuously operate without any need to doing

86
00:05:43,516 --> 00:05:46,822
the DNS entry shuffling or things like that. The clients

87
00:05:46,886 --> 00:05:50,214
themselves take care of it and then they switch back when they detect that primaries

88
00:05:50,262 --> 00:05:54,046
come back online. Last but not least is we support a

89
00:05:54,068 --> 00:05:57,246
flexible subscription model. So Hatchie Pulsar is

90
00:05:57,268 --> 00:06:00,362
defined as the cloud native messaging and event streaming

91
00:06:00,506 --> 00:06:03,914
platform, meaning they can support both your traditional messaging

92
00:06:03,962 --> 00:06:07,314
use cases like pub sub work

93
00:06:07,352 --> 00:06:10,626
queue sort of semantics with these different subscription models. And we also support the

94
00:06:10,648 --> 00:06:14,446
more modern use cases of event streaming, where processing

95
00:06:14,478 --> 00:06:17,822
of the data in order and retaining it in order is critical.

96
00:06:17,966 --> 00:06:21,506
Both of those are available with Apache Pulsar, and it's built in natively

97
00:06:21,538 --> 00:06:24,886
through what we call subscription models where you specify, this is how I want to

98
00:06:24,908 --> 00:06:28,826
subscribe to the data, and both those messaging and event streaming semantics are

99
00:06:28,848 --> 00:06:32,486
supported. Another key feature of Apache

100
00:06:32,518 --> 00:06:36,666
Pulsar is that we integrate the schema registry natively into the platform itself.

101
00:06:36,848 --> 00:06:40,106
Unlike other messaging systems which acted as

102
00:06:40,128 --> 00:06:43,546
an add on after the fact, you have to pay additional

103
00:06:43,578 --> 00:06:47,850
monies to get these capabilities. That's not the case with Apache Pulsar.

104
00:06:48,010 --> 00:06:51,786
Just like other messaging systems, we support schema

105
00:06:51,818 --> 00:06:55,090
aware producers and consumers so that when they connect,

106
00:06:55,240 --> 00:06:58,994
they can verify that the schema they're publishing with conforms to the

107
00:06:59,032 --> 00:07:03,022
expected schema of the downstream consumer, so that there's a data integrity

108
00:07:03,086 --> 00:07:07,006
between the two. Also support schema

109
00:07:07,038 --> 00:07:10,290
enforcement and auto updating. So if you publish by default

110
00:07:10,370 --> 00:07:13,698
a schema of the producer, let's say you connect a producer with a schema

111
00:07:13,714 --> 00:07:17,538
that adds a field and you're looking for a backwards compatible schema

112
00:07:17,714 --> 00:07:21,066
strategy, then that will be enforced because the consumers aren't aware of that,

113
00:07:21,168 --> 00:07:25,114
weren't aware of that field to begin with. They can continue processing using the old

114
00:07:25,152 --> 00:07:29,114
Schema version two while you're publishing messages with schema version three,

115
00:07:29,312 --> 00:07:32,602
and it interacts seamlessly and this is all built in out of the box

116
00:07:32,656 --> 00:07:36,560
and it's stored internally inside of Pulsar and you get these capabilities for free.

117
00:07:36,930 --> 00:07:40,702
Another really key feature that I'm high on among so many,

118
00:07:40,756 --> 00:07:43,598
that's what I want to call out, is what we call Kafka on Pulsar.

119
00:07:43,774 --> 00:07:48,302
Now, Pulsar is the first and only messaging system that supports

120
00:07:48,446 --> 00:07:51,566
multiple messaging protocols natively.

121
00:07:51,678 --> 00:07:54,738
And what I mean by that is other messaging systems that have

122
00:07:54,744 --> 00:07:58,642
been developed thus far usually have a low level wire

123
00:07:58,706 --> 00:08:01,910
transfer protocol that's unique to them. For example,

124
00:08:01,980 --> 00:08:05,206
RabbitMQ uses an AMQP protocol spec that

125
00:08:05,228 --> 00:08:08,646
sends commands back and forth to publish and retrieve data.

126
00:08:08,748 --> 00:08:12,646
Kafka similarly has a similar protocol for publishing and subscribing

127
00:08:12,678 --> 00:08:16,326
data. Pulsar has what's called a protocol handler framework shown

128
00:08:16,358 --> 00:08:20,394
here in this yellow box that would automatically take the commands from one

129
00:08:20,432 --> 00:08:24,634
protocol, in this case Kafka, and translate those automatically

130
00:08:24,682 --> 00:08:28,046
into the commands that Pulsar needs to handle in order to

131
00:08:28,148 --> 00:08:31,786
store data and retrieve data. Whatever the case may be. This allows

132
00:08:31,818 --> 00:08:35,818
you to natively take your existing Kafka applications using

133
00:08:35,844 --> 00:08:38,722
a Kafka library that you already tested and vetted out,

134
00:08:38,856 --> 00:08:42,850
and switch it to. Point to Apache Pulsar and you can

135
00:08:42,920 --> 00:08:46,846
operate seamlessly. This is a great use case. So if you've invested a significant

136
00:08:46,878 --> 00:08:50,134
amount of time into one of these messaging technologies, or maybe many of them,

137
00:08:50,252 --> 00:08:53,574
you can simply change a few

138
00:08:53,612 --> 00:08:57,186
configurations and reuse all that code and not have to rip it out and replace

139
00:08:57,218 --> 00:09:00,566
it because you want to use a new messaging system. So the barrier to adoption

140
00:09:00,598 --> 00:09:04,074
is much lower with this, as we mentioned

141
00:09:04,112 --> 00:09:08,470
before, the key to having a really good, strong streaming foundation

142
00:09:08,550 --> 00:09:12,574
is a teamwork, and in

143
00:09:12,612 --> 00:09:16,394
patchy pulsar that's reflected as a strong and thriving

144
00:09:16,442 --> 00:09:19,934
ecosystem across different components, as shown here. So as I mentioned

145
00:09:19,972 --> 00:09:23,486
before, the protocol handlers in addition to Kafka we spoke about

146
00:09:23,508 --> 00:09:27,366
in the previous slide, we have one for MQTT, we have one for AMQP

147
00:09:27,418 --> 00:09:30,994
as well. So you can speak all these different messaging protocols, a single system to

148
00:09:31,032 --> 00:09:34,606
speak them all. Multiple client libraries this is just a sampling

149
00:09:34,638 --> 00:09:37,346
of some of the ones that are supported. As you can see, all the more

150
00:09:37,368 --> 00:09:41,186
popular ones are supported. Java Python for those of you using Python,

151
00:09:41,218 --> 00:09:44,774
it's a very big deal. Python is a first order

152
00:09:44,892 --> 00:09:49,074
client library and has unique features that we will talk about connectors

153
00:09:49,122 --> 00:09:51,990
and sync so you can move your data into and out of these other systems.

154
00:09:52,150 --> 00:09:55,638
Process data inside using different stream processing engines

155
00:09:55,654 --> 00:09:59,146
shown there down below like flink, do some analytics on it and then push it

156
00:09:59,168 --> 00:10:02,666
to your delta lake. That's all very easy to do natively

157
00:10:02,698 --> 00:10:06,154
with these connectors. You also have pulsar

158
00:10:06,202 --> 00:10:09,566
functions, which is a lightweight stream processing framework. I'll talk about in the

159
00:10:09,588 --> 00:10:13,566
next slide that supports Python as

160
00:10:13,588 --> 00:10:16,974
a development language for one of these particular features, and we'll get into

161
00:10:17,012 --> 00:10:20,366
that in a little bit and how you can leverage those for machine learning applications

162
00:10:20,398 --> 00:10:24,146
out at the edge. And last but not least, tiered storage offload. So as I

163
00:10:24,168 --> 00:10:27,282
mentioned before, Pulsar is architected in such a way that your

164
00:10:27,336 --> 00:10:31,254
storage and compute layers are separated. You can take full advantage of that by

165
00:10:31,292 --> 00:10:35,094
moving older data, retaining it beyond the traditional seven days

166
00:10:35,132 --> 00:10:38,966
you do in something like a Kafka, 30 days, 90 days, however long you want,

167
00:10:39,068 --> 00:10:42,294
by moving it to more cost effective storage. Cloud storage this blob,

168
00:10:42,342 --> 00:10:45,626
this object store cloud providers like S three or

169
00:10:45,648 --> 00:10:49,606
Google Cloud storage, you can have it internally. If you have an existing hdfs

170
00:10:49,798 --> 00:10:54,026
cluster sitting around, you have nat network attached storage sitting around.

171
00:10:54,128 --> 00:10:57,422
You can offload that data and use this lower cost storage to put the data

172
00:10:57,476 --> 00:11:01,466
there. And Pulsar can read it natively. It doesn't really need to reconstitute

173
00:11:01,498 --> 00:11:05,034
it or spin up some clusters to put it back in first. Like other messaging

174
00:11:05,082 --> 00:11:07,938
systems, you can offload it to s three, but step one is to create a

175
00:11:07,944 --> 00:11:12,194
new broker and load it first. You don't have to do that since

176
00:11:12,232 --> 00:11:15,726
we're just reading it from a pointer, whether it's on our local storage,

177
00:11:15,758 --> 00:11:17,926
in our bookie layer, what we call bookkeeper, or if it's on s three,

178
00:11:17,948 --> 00:11:21,922
it doesn't matter, it's transparent to the end user and all these connectors

179
00:11:21,986 --> 00:11:25,906
and documentation and the source code and the download is available at hub

180
00:11:26,018 --> 00:11:29,194
streamnative IO here shown on the left. So if you want to learn more about

181
00:11:29,232 --> 00:11:32,634
them, get some more information, download them, try them out, it's all

182
00:11:32,672 --> 00:11:34,060
available for you to use.

183
00:11:36,430 --> 00:11:40,586
I mentioned pulsar functions in the previous slide and it

184
00:11:40,608 --> 00:11:44,026
has a very simplistic programming model. So for those of you familiar with AWS,

185
00:11:44,058 --> 00:11:47,934
lambdas or maybe Azure functions, it's very simple.

186
00:11:47,972 --> 00:11:51,310
You give a single, you provide a user provided function,

187
00:11:51,380 --> 00:11:54,030
a single method implemented, and this,

188
00:11:54,100 --> 00:11:57,378
that individual piece of code gets executed every time a

189
00:11:57,384 --> 00:12:01,406
new event arrives on one or more input topics that you configure

190
00:12:01,438 --> 00:12:04,914
the function to listen to. So you can say whatever these

191
00:12:04,952 --> 00:12:07,558
topics come in. When an event comes in, I want to execute this logic and

192
00:12:07,564 --> 00:12:11,186
boom, it happens. And again, there's three languages supported, Java,

193
00:12:11,218 --> 00:12:14,886
Python and go. And more importantly, even though

194
00:12:14,908 --> 00:12:18,534
they're simple single method pieces of code, so they're very

195
00:12:18,572 --> 00:12:22,202
easy to write, very easy to learn. You can still leverage the full power

196
00:12:22,256 --> 00:12:26,074
of any third party libraries you want to use inside this code itself. So we

197
00:12:26,112 --> 00:12:30,262
support the importing of these different third party machine learning execution

198
00:12:30,326 --> 00:12:33,354
models, for example. So you can do more complex stream processing,

199
00:12:33,482 --> 00:12:36,842
machine learning on these individual streams,

200
00:12:36,906 --> 00:12:40,478
data streams as they come in very easily on that. So it's a

201
00:12:40,484 --> 00:12:44,834
very nice framework for doing that. Here's an example of

202
00:12:44,952 --> 00:12:48,466
a pulsar function written in Python. As you can see

203
00:12:48,488 --> 00:12:51,970
here we have a couple of different import statements. We brought in a sentiment

204
00:12:52,550 --> 00:12:55,140
analysis tool in this regard,

205
00:12:56,310 --> 00:12:59,586
and as you can see we've imported the function definition

206
00:12:59,778 --> 00:13:03,842
and have a class that implements it here chat. The method

207
00:13:03,906 --> 00:13:07,318
there, called process, is the code that actually gets executed every time a

208
00:13:07,324 --> 00:13:10,994
new method arrives on one of the input topics, and the input,

209
00:13:11,042 --> 00:13:14,886
as you can expect, is that second parameter there called input, and you can manipulate

210
00:13:14,918 --> 00:13:18,154
that data as you see fit in this particular code example here,

211
00:13:18,272 --> 00:13:21,946
we parse it out as JSON. We know it's JSON. We pull

212
00:13:21,968 --> 00:13:24,858
out some individual fields that we know are there, we run some analysis on it,

213
00:13:24,864 --> 00:13:28,238
we determine if the sentiment is positive or negative, and then as a result of

214
00:13:28,244 --> 00:13:31,486
that we can publish those results to an output topic. And that's where

215
00:13:31,508 --> 00:13:34,602
this return statement comes in. So when you run the return statement,

216
00:13:34,746 --> 00:13:38,234
that's where the outputting of the data to a downstream topic has occurred.

217
00:13:38,282 --> 00:13:41,614
You don't have to do a return every time, but if you do, that constitutes

218
00:13:41,662 --> 00:13:44,786
the publishing of a new message at that point in time. So it's a really

219
00:13:44,808 --> 00:13:48,566
great feature. Again, this is for Python, and Tim will talk more about

220
00:13:48,588 --> 00:13:52,086
it in the presentation about how we're going to leverage these and

221
00:13:52,108 --> 00:13:55,382
Python library to do some cool machine learning

222
00:13:55,436 --> 00:13:57,910
models and machine learning processing with Python.

223
00:13:58,670 --> 00:14:02,794
So thanks David. Let's get to the next slide here.

224
00:14:02,992 --> 00:14:06,806
Always fun. Let me get out of full screen mode

225
00:14:06,838 --> 00:14:09,994
here, and we'll get to some of the other

226
00:14:10,032 --> 00:14:13,706
libraries here. So at a minimum you'll have your Python

227
00:14:13,738 --> 00:14:16,970
three environment on whatever kind of machine it is. Mac, windows,

228
00:14:17,130 --> 00:14:20,794
raspberry, PI, ubuntu, whatever it is, you do a pip

229
00:14:20,842 --> 00:14:24,138
install. 211 is the latest.

230
00:14:24,314 --> 00:14:27,630
Now if you're in an exotic platform, weird hardware,

231
00:14:27,790 --> 00:14:31,106
you might have to do a build. You can get the

232
00:14:31,128 --> 00:14:34,594
source code, do a c plus plus build, and then from there build

233
00:14:34,632 --> 00:14:38,434
the python one. Pretty straightforward, but for most platforms

234
00:14:38,482 --> 00:14:41,654
you won't need to do that, including Mac, M,

235
00:14:41,692 --> 00:14:45,638
one, silicon or intel doesn't matter. The code itself,

236
00:14:45,804 --> 00:14:49,190
really simple. You import pulsar,

237
00:14:49,350 --> 00:14:53,114
create a client, connect to your cluster on your port,

238
00:14:53,312 --> 00:14:56,602
then create a producer for that particular

239
00:14:56,736 --> 00:15:00,262
topic. Now the format for that is persistent.

240
00:15:00,406 --> 00:15:04,478
Most topics persist the data for as long as it needs to be.

241
00:15:04,564 --> 00:15:07,918
You can make it non persistent, not very common, but if you

242
00:15:07,924 --> 00:15:11,658
don't care about the messages, you just want fast throughput. Maybe you've

243
00:15:11,674 --> 00:15:15,298
got something that's always on, just as on some kind

244
00:15:15,304 --> 00:15:18,610
of IoT device. Maybe you don't need to persist it,

245
00:15:18,760 --> 00:15:21,714
send it, encode it, close the client, done.

246
00:15:21,832 --> 00:15:26,154
That's the simplest way to send messages with Python.

247
00:15:26,302 --> 00:15:30,518
That's probably not your production use case. You probably have to log in.

248
00:15:30,604 --> 00:15:34,646
So say I wanted to log into a secure cluster running

249
00:15:34,748 --> 00:15:38,278
in the cloud. I could use SSL,

250
00:15:38,374 --> 00:15:42,250
pick my port there again, connect to that topic.

251
00:15:42,830 --> 00:15:46,582
If I'm going to use Oauth, put out the issuer URL

252
00:15:46,646 --> 00:15:50,342
for that OAuth provider, point to my private key,

253
00:15:50,496 --> 00:15:53,934
set up what my audience is for that cluster when

254
00:15:53,972 --> 00:15:56,974
that data comes in. Same idea here,

255
00:15:57,012 --> 00:16:00,366
but I'm also importing authentication. It's going to

256
00:16:00,388 --> 00:16:03,886
authenticate against that. Got a number of examples on how to

257
00:16:03,908 --> 00:16:07,774
do that. If you're trying that out for yourself, you can get a free pulsar

258
00:16:07,822 --> 00:16:11,522
cluster at stream native to try this. It's a great way to

259
00:16:11,656 --> 00:16:15,758
learn it without having to install stuff or set up your own docker.

260
00:16:15,934 --> 00:16:19,750
There's also a cluster available if you do the free training

261
00:16:19,820 --> 00:16:23,314
at stream native. Now, if I want to use one of those schemas

262
00:16:23,362 --> 00:16:24,920
that I talked about before,

263
00:16:26,090 --> 00:16:29,230
Avro is one type, I'll import schema,

264
00:16:29,330 --> 00:16:32,666
I'll create a schema. It's a very simple class.

265
00:16:32,768 --> 00:16:36,982
Give it a name record which comes from the pulsar schema,

266
00:16:37,126 --> 00:16:41,022
and put all my field names and what type they are. You'd also

267
00:16:41,076 --> 00:16:44,080
specify if they're nullable or not.

268
00:16:44,530 --> 00:16:48,046
Connect to the client again and you could do oauth if you want.

269
00:16:48,228 --> 00:16:52,586
And then for thermal, I'll create the schema

270
00:16:52,698 --> 00:16:55,746
based on this over here and send that.

271
00:16:55,768 --> 00:16:59,890
When I create my producer, I'll specify the schema that way,

272
00:17:00,040 --> 00:17:03,586
put in a couple of extra parameters so people know who I

273
00:17:03,608 --> 00:17:07,086
am, and then create my record by setting

274
00:17:07,118 --> 00:17:10,546
a couple of fields and then send that here. I'm also setting

275
00:17:10,578 --> 00:17:14,178
a key. It's good so I can uniquely identify my records,

276
00:17:14,274 --> 00:17:17,506
and then bang, it's ready to go. If you've

277
00:17:17,538 --> 00:17:20,874
never sent a schema to that topic before, it'll create the

278
00:17:20,912 --> 00:17:24,886
first version of it. If this is an update, it'll add a new version.

279
00:17:24,998 --> 00:17:28,810
If you haven't created that topic before, if you have permissions, it'll create

280
00:17:28,880 --> 00:17:32,842
that topic under that tenant and namespace

281
00:17:32,906 --> 00:17:36,446
specified there, and set up that schema for you.

282
00:17:36,548 --> 00:17:40,542
It's really easy, especially for development. Once you're in production, you probably

283
00:17:40,596 --> 00:17:44,794
want someone to control who creates those schemas

284
00:17:44,842 --> 00:17:48,530
and topics and when and who has permissions. All that fun

285
00:17:48,600 --> 00:17:52,514
security stuff that people love out there. Now, if I want to do the same

286
00:17:52,552 --> 00:17:56,370
thing with Jason, it is not drastically different.

287
00:17:56,520 --> 00:17:59,970
I recommend you keep these topics separate,

288
00:18:00,050 --> 00:18:03,666
because I can have millions of topics, no reason to get in trouble

289
00:18:03,698 --> 00:18:07,074
and trying to put two different types of schema on the same topic.

290
00:18:07,202 --> 00:18:10,586
Create a new topic for everything you're doing. So if we want to do

291
00:18:10,608 --> 00:18:14,522
JSON schema almost identical, we just use

292
00:18:14,656 --> 00:18:19,046
JSON again. I could use that same class, put that in a separate file,

293
00:18:19,158 --> 00:18:22,522
and create one topic that has the JSON schema. One has Avro,

294
00:18:22,586 --> 00:18:26,462
one has protobuff. Whatever. Your downstream system

295
00:18:26,516 --> 00:18:30,026
is happier. Know Nifi really likes the JSON

296
00:18:30,058 --> 00:18:33,566
or Avro one. Flink might be better with know you

297
00:18:33,588 --> 00:18:37,026
might have a tool that works better with JSOn. Find that out,

298
00:18:37,128 --> 00:18:40,466
send that data that way. Very easy to do that.

299
00:18:40,568 --> 00:18:44,226
If you want to get data back, you subscribe to it, give it

300
00:18:44,248 --> 00:18:47,666
a subscription name. There's different types of subscriptions,

301
00:18:47,858 --> 00:18:51,286
it'll default to different types. And I could just go

302
00:18:51,308 --> 00:18:55,190
in a loop here, receive it, display it, and very important

303
00:18:55,260 --> 00:18:58,578
here acknowledge once I acknowledge a message for

304
00:18:58,604 --> 00:19:02,326
my subscription, mine for this topic, under that tenant

305
00:19:02,358 --> 00:19:05,786
and namespace, that message has been received and

306
00:19:05,808 --> 00:19:09,914
I acknowledged I received it. That means if there's no new messages out

307
00:19:09,952 --> 00:19:13,226
there, that data could be expired away if you want,

308
00:19:13,328 --> 00:19:15,978
or sent to long term storage,

309
00:19:16,154 --> 00:19:19,486
depending on your purpose out there. As long as no one else has a

310
00:19:19,508 --> 00:19:23,102
subscription that has unacknowledged messages. This is

311
00:19:23,156 --> 00:19:26,786
nice. I can acknowledge messages one at a time. It's not

312
00:19:26,808 --> 00:19:30,206
a build like in Kafka. This comes out very handy.

313
00:19:30,238 --> 00:19:33,326
If maybe you want to skip a message because it looks a little odd,

314
00:19:33,438 --> 00:19:36,386
maybe have someone else look at those later.

315
00:19:36,568 --> 00:19:39,826
Lots of reasons why you might want to not acknowledge messages,

316
00:19:39,938 --> 00:19:43,814
and you can unacknowledge them, and that way they'll live as long

317
00:19:43,852 --> 00:19:47,350
as your system allows them to live, which is configurable.

318
00:19:47,690 --> 00:19:52,134
Now, we talked about those other protocols, very important with ML.

319
00:19:52,262 --> 00:19:55,722
You may be grabbing stuff off devices, you may want to send things

320
00:19:55,776 --> 00:19:58,890
to a device. Maybe it only supports MQTT,

321
00:19:59,230 --> 00:20:03,214
use that protocol. And I could still point to pulsar, use those

322
00:20:03,252 --> 00:20:06,494
native python libraries, doesn't matter, point to those

323
00:20:06,532 --> 00:20:10,558
same topics. It is a very good way to have

324
00:20:10,724 --> 00:20:14,782
different protocols as part of your ML stream.

325
00:20:14,926 --> 00:20:18,526
Very common for IoT sensor use cases.

326
00:20:18,638 --> 00:20:21,902
This does come up a lot. This is a good protocol

327
00:20:21,966 --> 00:20:22,740
for that.

328
00:20:24,870 --> 00:20:27,570
It is perfect if you're sending a lot of data.

329
00:20:27,720 --> 00:20:30,854
It's not my favorite because it can be a little

330
00:20:30,892 --> 00:20:34,418
lossy. So be warned. If you're sending

331
00:20:34,434 --> 00:20:37,686
data with MQTT, keep an eye on it. But it is a nice way to

332
00:20:37,708 --> 00:20:41,206
do that. Now, another great thing, it's very easy to do

333
00:20:41,308 --> 00:20:45,254
websockets in Python, so we could use the websocket client

334
00:20:45,382 --> 00:20:49,366
to connect to pulsar as well. And we point to our topic.

335
00:20:49,478 --> 00:20:53,454
This will go over, rest, encode the data and send it.

336
00:20:53,572 --> 00:20:56,654
Very easy to do. We'll get a response back saying that it was

337
00:20:56,692 --> 00:21:00,110
received. Great way to get your data into

338
00:21:00,180 --> 00:21:04,002
pulsar. Also a great way to communicate with say a

339
00:21:04,056 --> 00:21:07,982
front end client which could be pure javascript.

340
00:21:08,126 --> 00:21:11,746
Nothing fancy there and we've got some examples later.

341
00:21:11,848 --> 00:21:16,290
It's a good way to communicate between apps written and say node js and Python.

342
00:21:16,790 --> 00:21:20,278
That can happen a lot in different ways that you want

343
00:21:20,284 --> 00:21:23,654
to display machine learning applications. Something to think about.

344
00:21:23,772 --> 00:21:27,486
I can get events and switch them over to Websocket style

345
00:21:27,538 --> 00:21:31,398
communications without having to add other servers,

346
00:21:31,574 --> 00:21:34,966
fancy libraries, or any kind of weirdness straight

347
00:21:34,998 --> 00:21:38,870
out of the box. Standard websockets and base 64 encoding.

348
00:21:38,950 --> 00:21:43,386
Boom. We can have as much security as you want as well. And including encryption.

349
00:21:43,498 --> 00:21:47,518
Of course those things get a little more complex now

350
00:21:47,684 --> 00:21:51,054
Kafka is huge and has a lot of great use

351
00:21:51,092 --> 00:21:54,550
cases. If I have an existing Kafka

352
00:21:54,570 --> 00:21:58,242
app in python, could use that same library port,

353
00:21:58,296 --> 00:22:01,870
those things get my data, use the standard Kafka producer.

354
00:22:01,950 --> 00:22:05,154
But I'm going to point now to pulsar which will act as

355
00:22:05,192 --> 00:22:08,706
those kafka bootstrap ones. Very straightforward,

356
00:22:08,818 --> 00:22:11,830
nothing to worry about there. And then do a flush.

357
00:22:12,570 --> 00:22:15,686
So real easy to do Kafka and Pulsar and

358
00:22:15,708 --> 00:22:19,242
I link to some resources there, show you how you do it again, the same

359
00:22:19,296 --> 00:22:22,090
code could just point to a Kafka cluster as well,

360
00:22:22,240 --> 00:22:25,706
so no reason to write any custom code.

361
00:22:25,888 --> 00:22:30,166
Point it to whatever Kafka compliant broker

362
00:22:30,198 --> 00:22:32,300
that you have out there. Really nice.

363
00:22:34,270 --> 00:22:38,222
Now I want to show a couple simple DevOps here

364
00:22:38,356 --> 00:22:42,286
just to show you that this is a modern system. Now we

365
00:22:42,308 --> 00:22:45,300
could use a lot of different ways to deploy things.

366
00:22:45,990 --> 00:22:49,774
One way is command line and that could be interactive.

367
00:22:49,822 --> 00:22:53,266
It could be command at a time. Obviously this could

368
00:22:53,288 --> 00:22:57,042
be automated a ton of different ways. Whatever automation

369
00:22:57,106 --> 00:23:00,822
DevOps tool you like, this can also be done through rest

370
00:23:00,876 --> 00:23:05,362
endpoints or through different cloud uis.

371
00:23:05,506 --> 00:23:09,194
There's a really nice one in the open source for

372
00:23:09,232 --> 00:23:12,778
doing pulsar administration. You could look at that one.

373
00:23:12,944 --> 00:23:17,066
This is as hard as it is to deploy a function and

374
00:23:17,088 --> 00:23:20,342
do a create set auto acknowledge.

375
00:23:20,406 --> 00:23:23,902
So when event comes into a function we acknowledge it right away.

376
00:23:24,036 --> 00:23:27,486
Point to my python script, give it the

377
00:23:27,508 --> 00:23:30,846
class name that I'm using in that application. Point to as

378
00:23:30,868 --> 00:23:34,386
many topics as I want to come in here for this one. It's just one.

379
00:23:34,488 --> 00:23:38,322
I could change that on the fly later. Also through

380
00:23:38,376 --> 00:23:41,198
DevOps add more easily.

381
00:23:41,374 --> 00:23:45,134
I had a log topic, so anywhere I use the context

382
00:23:45,182 --> 00:23:48,606
to log it will go to a topic. So you don't have to search through

383
00:23:48,648 --> 00:23:52,038
logs or write a third party logging system.

384
00:23:52,204 --> 00:23:55,862
These will be just data in a topic which I could have a function read

385
00:23:55,916 --> 00:23:59,138
or just any app, doesn't matter, including a Kafka

386
00:23:59,154 --> 00:24:03,100
app. Give it a name so I know what my functions is

387
00:24:03,630 --> 00:24:06,874
and put it an output now inside the

388
00:24:06,912 --> 00:24:10,538
code. I could send it to any other topic if I wanted to,

389
00:24:10,624 --> 00:24:14,206
including create topics on the fly. Source code for this one

390
00:24:14,228 --> 00:24:17,902
is down the bottom. This one's a cool little ML app. What happens

391
00:24:17,956 --> 00:24:22,334
is I connect a front end web page and

392
00:24:22,372 --> 00:24:26,270
it sends to a kafka topic which you can see here is chat.

393
00:24:26,430 --> 00:24:30,462
That triggers an event into this sentiment.

394
00:24:30,526 --> 00:24:34,034
Python app which reads that looks at

395
00:24:34,072 --> 00:24:37,506
your text there and applies the sentiment,

396
00:24:37,618 --> 00:24:41,554
puts the results of that and some other fields in an output

397
00:24:41,602 --> 00:24:44,774
topic which it also displays in

398
00:24:44,812 --> 00:24:48,706
that front end. Again, a great way to communicate between Python

399
00:24:48,898 --> 00:24:52,474
and JavaScript, even if that's in a phone app

400
00:24:52,512 --> 00:24:56,490
or a web app, or in a notebook, wherever it happens to be.

401
00:24:56,560 --> 00:25:00,566
Great way to communicate between systems. Do that asynchronously

402
00:25:00,678 --> 00:25:03,934
and apply ML in different pieces depending on

403
00:25:03,972 --> 00:25:07,854
where it makes sense. I don't know what that one

404
00:25:07,892 --> 00:25:11,614
was. We're going to walk through some code here. We saw this

405
00:25:11,652 --> 00:25:14,974
before. If I'm going to set up a schema, give it a name.

406
00:25:15,092 --> 00:25:19,458
Record comes from the schema library. That's important.

407
00:25:19,624 --> 00:25:22,900
We set our different types here. These are the names we're using.

408
00:25:23,350 --> 00:25:27,206
Very straightforward, but that is my schema. I don't have to know how

409
00:25:27,228 --> 00:25:30,562
to build an Avro schema or how to build JSon or protobuff.

410
00:25:30,626 --> 00:25:33,510
It'll be build from this, which is very nice.

411
00:25:33,580 --> 00:25:37,762
You just write some Python code, create connection

412
00:25:37,826 --> 00:25:41,386
here to that server. We'll build a JSON schema off

413
00:25:41,408 --> 00:25:45,242
of that, give it a name. You could put some other fields in here for

414
00:25:45,296 --> 00:25:48,794
tracking that doesn't affect the data here

415
00:25:48,832 --> 00:25:52,730
I like to create a unique key, or relatively unique

416
00:25:52,810 --> 00:25:56,618
doing a build in standard Python library

417
00:25:56,794 --> 00:26:00,430
format. The time put that together as a big key

418
00:26:00,500 --> 00:26:03,826
that's reasonably unique. Create a new

419
00:26:03,848 --> 00:26:07,022
instance of my stock, set some values.

420
00:26:07,086 --> 00:26:10,254
These are getting pulled off of a rest endpoint.

421
00:26:10,302 --> 00:26:13,730
I'm reading format these in the right format,

422
00:26:14,710 --> 00:26:18,226
clean them up, add my uuId. I'm also sending

423
00:26:18,258 --> 00:26:21,558
that as the partition key. If I got my stock data,

424
00:26:21,644 --> 00:26:25,586
send it, and then this could be sent to another processor

425
00:26:25,618 --> 00:26:29,514
to read this. And then the end result of this can be

426
00:26:29,552 --> 00:26:33,190
displayed in a front end. Because we could read websockets,

427
00:26:33,270 --> 00:26:37,146
you could also read rest. Reading websockets from

428
00:26:37,328 --> 00:26:40,634
Javascript is really easy. And I apply that with

429
00:26:40,672 --> 00:26:44,142
a table library. Boom. You could get

430
00:26:44,196 --> 00:26:49,146
live data. So as events come in, the websocket

431
00:26:49,178 --> 00:26:52,574
is subscribed and it'll just get more and more data and it'll come into this

432
00:26:52,612 --> 00:26:55,546
table and you could sort it. All that kind of fun stuff.

433
00:26:55,588 --> 00:26:59,522
Search it. Nice way to do that. When you're setting up these web

434
00:26:59,576 --> 00:27:03,026
pages, it could not be easier. You point to

435
00:27:03,048 --> 00:27:07,094
a style sheet out there. You could either download it locally or point to

436
00:27:07,132 --> 00:27:11,010
the CDN. I'm using jquery

437
00:27:11,090 --> 00:27:14,502
to do my formatting. Pretty straightforward. The data

438
00:27:14,556 --> 00:27:17,750
table library I'm using is also using jquery.

439
00:27:18,650 --> 00:27:21,882
Pretty simple here, and I point to it down the bottom.

440
00:27:22,016 --> 00:27:25,754
This also has my function that's doing some processing of some

441
00:27:25,792 --> 00:27:29,386
data that's coming from one of our friends in

442
00:27:29,408 --> 00:27:32,654
the streaming library. That's Apache nifi. Gets that

443
00:27:32,692 --> 00:27:36,014
data into the funnel for me. Very easy to

444
00:27:36,052 --> 00:27:39,614
format my table. Set up a head and a foot with

445
00:27:39,652 --> 00:27:43,602
some ths in the field names. Make sure I give it an id.

446
00:27:43,736 --> 00:27:47,250
I need that later so I can get access to that data

447
00:27:47,320 --> 00:27:51,710
table. Connect to Pulsar over that websocket.

448
00:27:51,870 --> 00:27:55,698
Set up a subscription name. I also say I'm

449
00:27:55,714 --> 00:28:00,162
a consumer for the persistent tenant

450
00:28:00,226 --> 00:28:03,954
namespace and topic here, and I give it the subscription

451
00:28:04,002 --> 00:28:07,574
name of TC reader and say it shares. If I want

452
00:28:07,692 --> 00:28:10,780
multiple people reading these at once, I could do that.

453
00:28:11,310 --> 00:28:15,638
Connect open my websocket. If there's an error, print out the errors

454
00:28:15,814 --> 00:28:19,980
here. When a message comes in, I parse that JSON data.

455
00:28:20,430 --> 00:28:23,742
If everything looks good, I'm going to

456
00:28:23,796 --> 00:28:27,610
take the payload that comes from that request,

457
00:28:27,770 --> 00:28:30,878
parse that as JSON, and get all these fields out of it.

458
00:28:30,964 --> 00:28:34,242
That's how we sent our message. Add a row to the screen,

459
00:28:34,376 --> 00:28:37,954
boom, it's displayed. Article shows you some details on

460
00:28:37,992 --> 00:28:41,794
that. Very cool way to display your data. Now to

461
00:28:41,832 --> 00:28:45,250
get data in, apply some machine learning,

462
00:28:45,400 --> 00:28:49,646
get data between different systems, maybe move it from pulsar

463
00:28:49,838 --> 00:28:53,298
to data store or say Apache iceberg.

464
00:28:53,474 --> 00:28:56,486
Data flows are a great way to do that. We ingest the data, move it

465
00:28:56,508 --> 00:28:59,994
around, routing all of it. Visual, very easy to do.

466
00:29:00,112 --> 00:29:03,526
We guarantee the data is delivered. We have buffering

467
00:29:03,558 --> 00:29:07,226
and backpress is supported. You could prioritize the

468
00:29:07,248 --> 00:29:10,806
queuing of data. You pick how much latency or throughput

469
00:29:10,838 --> 00:29:14,094
you need. We get data provenance lineage on everything.

470
00:29:14,212 --> 00:29:17,994
So I could track everything that happened. Someone tells me the data didn't

471
00:29:18,042 --> 00:29:21,850
show up downstream. I could look in the data provenance,

472
00:29:21,930 --> 00:29:25,342
which can also be programmatically looked at very easily.

473
00:29:25,486 --> 00:29:29,010
Data can be pushed or pulled. Hundreds of different ways to process

474
00:29:29,080 --> 00:29:32,578
your data, lots of different sources with version control,

475
00:29:32,664 --> 00:29:36,054
clustering, extensibility expands out to

476
00:29:36,092 --> 00:29:38,920
support millions of events a second.

477
00:29:39,290 --> 00:29:42,454
What's nice with Nifi is I could also move binary data,

478
00:29:42,572 --> 00:29:45,734
unstructured data, image data, PDF word

479
00:29:45,772 --> 00:29:49,510
documents, as well as data we traditionally think of

480
00:29:49,660 --> 00:29:53,386
as tabular data, table data, stuff that

481
00:29:53,408 --> 00:29:56,634
you might want to be working with usually. But this is a nice

482
00:29:56,672 --> 00:30:00,486
way to move some of that data you might need for machine learning and deep

483
00:30:00,518 --> 00:30:04,358
learning. Because I can move images, I could pull stuff off web cameras,

484
00:30:04,454 --> 00:30:08,778
you could pull stuff off street cameras very easily. Do some enrichment.

485
00:30:08,954 --> 00:30:12,470
I'll do it all visually, do some simple event processing,

486
00:30:12,650 --> 00:30:16,386
get that data into some central messaging system, lots of

487
00:30:16,408 --> 00:30:18,894
protocols supported Kafka, Pulsar,

488
00:30:18,942 --> 00:30:22,622
MQTT, Rabbit, all those sort of things. TCP,

489
00:30:22,686 --> 00:30:25,410
IP logs, all those sort of things.

490
00:30:25,480 --> 00:30:28,946
Simple architecture, but expands out to as many nodes

491
00:30:28,978 --> 00:30:32,898
as I need and could do all that with kubernetes

492
00:30:33,074 --> 00:30:37,046
or get some commercial hosting out there very easily in all

493
00:30:37,068 --> 00:30:40,746
the big clouds with no headaches. I have

494
00:30:40,768 --> 00:30:44,506
been Tim Spann, got all my contact information here.

495
00:30:44,608 --> 00:30:48,234
I do meetups like once a month. Definitely would like to

496
00:30:48,272 --> 00:30:51,662
see people. We do Python, we do

497
00:30:51,716 --> 00:30:54,874
nifi, Spark, Pulsar, Kafka,

498
00:30:55,002 --> 00:30:58,890
Iceberg, all the cool tech here. This is Nifi.

499
00:30:59,050 --> 00:31:02,814
It is a great way to integrate with Python and there's some

500
00:31:02,852 --> 00:31:06,386
new features coming out soon which will be out

501
00:31:06,408 --> 00:31:09,906
in the next release that lets you integrate Python and

502
00:31:09,928 --> 00:31:13,438
extend Nifi using Python apps.

503
00:31:13,534 --> 00:31:17,686
You just drop it in a directory and it'll automatically show up in this list.

504
00:31:17,868 --> 00:31:21,670
Or you could just use something like this thing

505
00:31:21,740 --> 00:31:24,978
here and this will execute your Python script

506
00:31:25,074 --> 00:31:28,930
or other languages. But face it, most people use Python.

507
00:31:29,010 --> 00:31:32,378
It's probably the best language for integrating anything

508
00:31:32,544 --> 00:31:35,482
and doing DevOps. But I just wanted to show you that.

509
00:31:35,536 --> 00:31:38,922
Give you an idea what you could do with something like

510
00:31:39,056 --> 00:31:43,102
Nifi and all the different things you can connect to

511
00:31:43,236 --> 00:31:46,270
kafka, pulsar,

512
00:31:48,930 --> 00:31:50,670
know Amazon,

513
00:31:51,890 --> 00:31:55,362
azure, google, lots of cool

514
00:31:55,416 --> 00:31:59,186
stuff there. I have example codes out there

515
00:31:59,368 --> 00:32:02,734
in GitHub. So if you go to my GitHub t span,

516
00:32:02,782 --> 00:32:06,582
hw, I've got a ton of stuff out there. One of them is

517
00:32:06,636 --> 00:32:09,686
for reading weather sensor data.

518
00:32:09,868 --> 00:32:12,998
Again, using pulsar part is easy.

519
00:32:13,164 --> 00:32:16,674
Connect into various libraries, create my schema

520
00:32:16,722 --> 00:32:20,234
from the fields, connect to all kinds of stuff there,

521
00:32:20,352 --> 00:32:24,714
build up my connection and my producer get

522
00:32:24,752 --> 00:32:28,762
my sensor data, get some other data like temperatures and

523
00:32:28,816 --> 00:32:32,478
other stuff on that device, format them all, send a

524
00:32:32,484 --> 00:32:36,062
record, print them very easily. Could also do

525
00:32:36,116 --> 00:32:39,390
things like do SQl against these

526
00:32:39,460 --> 00:32:42,834
live events with something like flink. Pretty easy

527
00:32:42,872 --> 00:32:46,606
to set up. We've got examples with the full docker

528
00:32:46,638 --> 00:32:49,762
here. Download those, get going.

529
00:32:49,896 --> 00:32:53,790
I've got other ones for doing different things like grabbing

530
00:32:53,950 --> 00:32:56,770
live thermal images,

531
00:32:57,190 --> 00:33:01,110
sending them up to imager, sending them slack discord email,

532
00:33:01,260 --> 00:33:04,406
and sending all the metadata to Kafka or something that

533
00:33:04,428 --> 00:33:08,294
looks like Kafka. And we could process with Spark or Flink or whatever.

534
00:33:08,412 --> 00:33:12,122
You could also send other data into Kafka. Pretty easy.

535
00:33:12,256 --> 00:33:15,594
Got the breakdown of the hardware here. You want to do that on your own.

536
00:33:15,632 --> 00:33:18,090
You could find yourself a raspberry PI somewhere.

537
00:33:19,310 --> 00:33:23,642
Example apps tend to be Nifi produces into Pulsar.

538
00:33:23,786 --> 00:33:26,910
And then maybe I'll do SQL with Flink. Maybe I'll be

539
00:33:26,980 --> 00:33:30,894
producing data with Python. Lots of different options

540
00:33:31,012 --> 00:33:33,918
here. Lots of examples out there.

541
00:33:34,084 --> 00:33:37,120
Thanks for joining me. If you have any questions,

542
00:33:37,650 --> 00:33:41,614
drop them in the system there or check me

543
00:33:41,652 --> 00:33:45,574
online. You can get me on Twitter, GitHub, any of my

544
00:33:45,612 --> 00:33:49,542
blogs out there. If you search Tim span and anything streaming you will find

545
00:33:49,596 --> 00:33:50,580
me. Thanks a lot.

