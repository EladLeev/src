1
00:00:00,410 --> 00:00:06,126
Jamaica make up real

2
00:00:06,148 --> 00:00:09,530
time feedback into the behavior of your distributed systems

3
00:00:09,610 --> 00:00:13,614
and observing changes exceptions. Errors in real

4
00:00:13,652 --> 00:00:17,914
time allows you to not only experiment with confidence, but respond

5
00:00:18,042 --> 00:00:20,480
instantly to get things working again.

6
00:00:24,610 --> 00:01:06,526
Those maintaining

7
00:01:06,558 --> 00:01:10,466
reliable systems today's focus will be on incident management and

8
00:01:10,488 --> 00:01:13,870
more specifically, how can customer impact be minimized

9
00:01:13,950 --> 00:01:17,118
by incidents management and postmortems? Before we

10
00:01:17,144 --> 00:01:20,626
start, I would like to introduce myself. I'm Ayelet Sachto.

11
00:01:20,738 --> 00:01:24,562
I'm a strategic cloud engineer and I'm also co leading PSO

12
00:01:24,626 --> 00:01:28,258
efforts in EME and currently I'm an SRE in GKE

13
00:01:28,354 --> 00:01:31,834
SRE London. Internet management is not new to me as

14
00:01:31,872 --> 00:01:35,546
I'm living and bleeding production in large scale for

15
00:01:35,648 --> 00:01:39,014
almost two decades now, most of them covering production

16
00:01:39,062 --> 00:01:42,314
on call. And today I'll share a few things that

17
00:01:42,352 --> 00:01:45,774
can make the life of an SRE or an on call a bit

18
00:01:45,812 --> 00:01:49,082
easier. I know it would have made mine

19
00:01:49,146 --> 00:01:52,350
much more so. What is an incidents?

20
00:01:52,790 --> 00:01:55,954
Incidents are issues that are escalated as they are

21
00:01:55,992 --> 00:02:00,510
too big for you to handle on your own, which required immediate

22
00:02:00,670 --> 00:02:03,426
and organized response. And remember,

23
00:02:03,528 --> 00:02:07,446
not all pages become incidents. Incidents mean

24
00:02:07,548 --> 00:02:11,062
loss in revenue, customers data and more,

25
00:02:11,196 --> 00:02:14,614
which all comes down to impact on our customers and our

26
00:02:14,652 --> 00:02:18,234
business. We want to avoid having too many or too

27
00:02:18,272 --> 00:02:21,338
severe as we want to keep our customers happy,

28
00:02:21,504 --> 00:02:25,098
otherwise they will leave us before we drill down,

29
00:02:25,264 --> 00:02:29,594
let's recap on our terminology. A service level indicator can

30
00:02:29,632 --> 00:02:33,422
SLI tells you at any moment in time how

31
00:02:33,476 --> 00:02:37,070
well is your service doing? Is it doing acceptably or not?

32
00:02:37,220 --> 00:02:41,114
A reliability target for an SLI called an SLO

33
00:02:41,242 --> 00:02:45,220
service level objective. Aggregate that over time.

34
00:02:45,990 --> 00:02:49,874
It says over this window of time, this is my target and

35
00:02:49,912 --> 00:02:53,506
how well I'm doing against it. Most of you are

36
00:02:53,528 --> 00:02:57,346
probably familiar with SLA. Service level agreement

37
00:02:57,538 --> 00:03:01,302
defines what you are willing to do. For example, provide a money

38
00:03:01,356 --> 00:03:04,386
refund if you are failing to meet your objective.

39
00:03:04,578 --> 00:03:08,554
To achieve that, we need our slos, our target, to be

40
00:03:08,592 --> 00:03:11,130
more restrictive than our slas.

41
00:03:11,550 --> 00:03:15,014
Our scope to examine and measure our user

42
00:03:15,062 --> 00:03:18,762
happiness is user journey. Your users are using

43
00:03:18,816 --> 00:03:22,718
your service to achieve a set of goals and the most important

44
00:03:22,804 --> 00:03:25,626
one called critical user journey.

45
00:03:25,818 --> 00:03:29,054
We know that failures will happen, but how

46
00:03:29,092 --> 00:03:33,154
can we make them hurt less? How can we reduce the

47
00:03:33,192 --> 00:03:36,994
impact? To answer this question, let's look

48
00:03:37,032 --> 00:03:41,518
at the production lifecycle, the time that we are not reliable

49
00:03:41,614 --> 00:03:43,860
and our users are not happy.

50
00:03:44,630 --> 00:03:48,486
That time includes those time to detect, time to

51
00:03:48,508 --> 00:03:52,466
repair, a time between failures. So to reduce

52
00:03:52,498 --> 00:03:55,990
the impact, we can tackle each one of those parts by

53
00:03:56,060 --> 00:04:00,022
reducing the time to detect, reducing the time to resolve

54
00:04:00,086 --> 00:04:03,914
or mitigate the incident and reducing the frequency of

55
00:04:03,952 --> 00:04:07,402
incidents, I. E increase the time between

56
00:04:07,456 --> 00:04:10,060
failures. So how can we do that?

57
00:04:10,590 --> 00:04:14,110
We do that with a combination of technology and human

58
00:04:14,180 --> 00:04:18,030
aspects like processes and enablement. At Google

59
00:04:18,100 --> 00:04:21,646
we found that once a human is involved, the outage will

60
00:04:21,668 --> 00:04:25,002
be at least 20 to 30 minutes, so automation

61
00:04:25,066 --> 00:04:28,270
and self healing system are a good strategy. In general,

62
00:04:28,420 --> 00:04:31,906
it both help to reduce the time to detect and the

63
00:04:31,928 --> 00:04:35,086
time to resolve. Let's zoom on each one of those parts

64
00:04:35,118 --> 00:04:39,046
separately. Those time to detect, also called TTD, is the

65
00:04:39,068 --> 00:04:42,422
amount of time from when an outage occurred to some human being

66
00:04:42,476 --> 00:04:46,418
notified or alerted that an issue is occurring.

67
00:04:46,594 --> 00:04:49,910
As part of our SLO drafting our reliability targets,

68
00:04:49,990 --> 00:04:53,674
we also want to do risk analysis and identify what

69
00:04:53,712 --> 00:04:57,878
we should focus on in order to identify and minimize

70
00:04:57,974 --> 00:05:02,000
the TDD. A few additional things we can do are

71
00:05:02,370 --> 00:05:06,250
aligning your slis, your indicators for customer happiness,

72
00:05:06,410 --> 00:05:10,574
as closely as possible to the user expectations which

73
00:05:10,612 --> 00:05:13,300
can be real people or other services.

74
00:05:13,910 --> 00:05:17,630
In addition, our alerts need to be aligned with our slos,

75
00:05:17,710 --> 00:05:21,790
our targets. We also want to review those periodically

76
00:05:21,870 --> 00:05:25,446
to make sure that they are still representing our customers

77
00:05:25,548 --> 00:05:29,762
happiness. The second thing is having quality alerts

78
00:05:29,906 --> 00:05:33,186
measured using different measurement strategies.

79
00:05:33,298 --> 00:05:36,982
It's important to choose what works best for getting

80
00:05:37,036 --> 00:05:40,726
the data. It can be from streams, logs or batch

81
00:05:40,758 --> 00:05:44,426
processing. In that regard, it's also important to find the

82
00:05:44,448 --> 00:05:48,570
right balance between alerting too quickly that can cause

83
00:05:48,640 --> 00:05:52,410
noise and alert fatigue versus alerting too slow

84
00:05:52,490 --> 00:05:54,640
as it may affect our customers.

85
00:05:55,570 --> 00:05:59,770
Note that noisy alert is one of the most repeating complaints

86
00:05:59,850 --> 00:06:03,890
we heard from operation teams, traditional DevOps and SRE.

87
00:06:04,230 --> 00:06:08,290
Another repeating issue is having alerts that are not actionable.

88
00:06:08,790 --> 00:06:12,002
We want to avoid alert fatigue and we also

89
00:06:12,056 --> 00:06:15,826
want pages that need immediate actions to achieve

90
00:06:15,858 --> 00:06:19,906
that. We want that only the right responders

91
00:06:20,018 --> 00:06:23,270
will get the alerts, only those specific team

92
00:06:23,340 --> 00:06:26,946
and owners. One of the most followed

93
00:06:26,978 --> 00:06:30,394
question is if we only page on

94
00:06:30,432 --> 00:06:33,626
things that required immediate action, what do

95
00:06:33,648 --> 00:06:36,794
we do with the rest of the issues? Remember,

96
00:06:36,912 --> 00:06:40,300
we have different tools and platform for a reason.

97
00:06:40,770 --> 00:06:44,826
Maybe the right platform is a ticketing system, a dashboard.

98
00:06:44,938 --> 00:06:49,210
Maybe we need only the metric for troubleshooting and debugging

99
00:06:49,290 --> 00:06:53,130
in a pool mode. The second part is TTR,

100
00:06:53,210 --> 00:06:57,246
time to repair. It begins from when someone was alerted

101
00:06:57,278 --> 00:07:01,202
to the issue to when it was mitigated. The key word

102
00:07:01,256 --> 00:07:04,514
here is mitigated. This doesn't mean the time

103
00:07:04,552 --> 00:07:07,366
it took you to submit code to fix the problem.

104
00:07:07,548 --> 00:07:11,494
It's the time it took those responder to mitigate those

105
00:07:11,532 --> 00:07:15,158
customer impact, for example by shifting traffic to another

106
00:07:15,244 --> 00:07:18,674
region. Reducing the time to repair is mostly

107
00:07:18,722 --> 00:07:22,838
about the human side that I mentioned. We want to train the responders

108
00:07:23,014 --> 00:07:26,678
having clear procedures and playbook and of course reduce

109
00:07:26,694 --> 00:07:30,038
the stress around on call. So let's expand on

110
00:07:30,064 --> 00:07:33,502
each. Unprepared on callers lead to

111
00:07:33,556 --> 00:07:36,974
longer repair times. So you want to have on call

112
00:07:37,012 --> 00:07:40,334
training like disaster recovery testing per on

113
00:07:40,372 --> 00:07:43,746
call or shadowing or running the wheel of

114
00:07:43,768 --> 00:07:47,234
misfortune exercise. Remember that on call can

115
00:07:47,272 --> 00:07:51,374
be stressful, so having clear incident management processes

116
00:07:51,502 --> 00:07:55,486
can reduce that stress as it's clear any ambiguity

117
00:07:55,598 --> 00:07:59,142
and clarify the actions needed. And for that

118
00:07:59,196 --> 00:08:02,390
purpose, let's introduce briefly how we can manage

119
00:08:02,460 --> 00:08:05,718
an incident incident management at Google

120
00:08:05,804 --> 00:08:09,258
Protocol iMog is a flexible framework based

121
00:08:09,344 --> 00:08:13,130
on the incident command system ICs used

122
00:08:13,200 --> 00:08:15,210
by firefighters and medics.

123
00:08:15,870 --> 00:08:19,322
It's a structure with clear roles, tasks and

124
00:08:19,376 --> 00:08:22,574
communication channels. It established a standard,

125
00:08:22,692 --> 00:08:26,030
consistent way to handle emergencies and

126
00:08:26,100 --> 00:08:29,502
organize an effective response. By using such

127
00:08:29,556 --> 00:08:33,146
protocols, we reduce those ambiguity, make it

128
00:08:33,188 --> 00:08:36,754
clear that it's a team effort and we reduce the

129
00:08:36,792 --> 00:08:40,578
time to repair. A few other things that you can do

130
00:08:40,664 --> 00:08:44,398
is to prioritize and set time for documentations.

131
00:08:44,574 --> 00:08:48,370
Create playbooks and policies that capture procedures

132
00:08:48,450 --> 00:08:51,718
and escalation path playbooks don't have to

133
00:08:51,724 --> 00:08:55,618
be robust at first. We want to start simple and iterate,

134
00:08:55,794 --> 00:08:59,146
which provide a clear starting point. A good

135
00:08:59,168 --> 00:09:03,066
rule of thumb that we advise our customers and you might

136
00:09:03,088 --> 00:09:06,810
be familiar with is see it, fix it and

137
00:09:06,880 --> 00:09:10,454
letting new team joiners to update those

138
00:09:10,512 --> 00:09:14,494
as part of their onboarding process. Remember, if the

139
00:09:14,532 --> 00:09:18,014
responders are exhausted, that will affect their

140
00:09:18,052 --> 00:09:21,626
ability to resolve the issue. We need to make sure that shifts

141
00:09:21,658 --> 00:09:25,442
are bonds and if not, use data to understand why

142
00:09:25,496 --> 00:09:29,106
and reduce toil. We also want to have as much quality

143
00:09:29,208 --> 00:09:33,058
data as possible. We especially want

144
00:09:33,144 --> 00:09:36,774
to measure things as closely to the customer experience as

145
00:09:36,812 --> 00:09:40,594
possible as it will help us troubleshoot and debug

146
00:09:40,642 --> 00:09:44,870
the problem. For that, we need to collect the application

147
00:09:45,020 --> 00:09:48,970
and business metric to have dashboard and visualization

148
00:09:49,310 --> 00:09:53,370
focused on customer experience and critical user journeys.

149
00:09:53,790 --> 00:09:57,814
That means dashboards that are aimed for specific audience

150
00:09:57,942 --> 00:10:01,962
with specific goal in mind. A manager view of slos

151
00:10:02,106 --> 00:10:05,838
will be very different than a dashboard that need to be used

152
00:10:05,924 --> 00:10:09,342
for troubleshooting. Can incident the third part

153
00:10:09,396 --> 00:10:13,654
is time between failures, which begins from the end of one outage

154
00:10:13,722 --> 00:10:18,094
to the beginning of the next. Other than architectural refactoring

155
00:10:18,142 --> 00:10:21,806
and addressing the failure points that come out of the risk

156
00:10:21,838 --> 00:10:25,358
analysis and process improvement, what else can we do?

157
00:10:25,544 --> 00:10:29,570
We would want to avoid global changes and also adopt

158
00:10:29,650 --> 00:10:33,714
advanced deployment strategies considering progressive

159
00:10:33,762 --> 00:10:37,506
and canary rollout over the course of hours days or

160
00:10:37,548 --> 00:10:40,746
weeks. This will allow you to reduce the risk and

161
00:10:40,768 --> 00:10:44,790
to identify the issue before all your users are affected.

162
00:10:44,950 --> 00:10:48,854
Those can be integrated into continuous integration and delivery

163
00:10:48,902 --> 00:10:53,370
pipeline, having automated testing and gradual rollout

164
00:10:53,530 --> 00:10:57,754
and automatic rollbacks CI CD save engineering

165
00:10:57,802 --> 00:11:01,214
time and reduce customer impact. It allows you

166
00:11:01,252 --> 00:11:05,362
to deploy with confidence. Another not surprising point

167
00:11:05,416 --> 00:11:09,854
is having robust architectures, having redundancy,

168
00:11:09,982 --> 00:11:13,774
no single point of failures, and implementing graceful

169
00:11:13,822 --> 00:11:17,282
degradation methods. We should also adopt

170
00:11:17,346 --> 00:11:21,094
dev practices that foster a culture of quality,

171
00:11:21,292 --> 00:11:24,822
create an integrated process of code review and

172
00:11:24,876 --> 00:11:28,694
robust testing. Remember, it's all about resilience.

173
00:11:28,822 --> 00:11:32,854
So in addition to training our responders and running disaster

174
00:11:32,902 --> 00:11:37,718
recovery exercises, we also want to practice chaos engineering,

175
00:11:37,894 --> 00:11:41,930
finding issues before they fund us by introducing

176
00:11:42,010 --> 00:11:46,510
fault injection and automated disaster recovery testing.

177
00:11:47,250 --> 00:11:50,494
Lastly, we want to learn from those failures and make

178
00:11:50,532 --> 00:11:54,210
tomorrow better than today. For that, our tool

179
00:11:54,280 --> 00:11:58,050
is postmortems. Postmortem are a recorded

180
00:11:58,200 --> 00:12:02,580
way of an incident and those should capture the actions needed.

181
00:12:02,950 --> 00:12:06,642
In Google, we found that establishing a culture of blameless

182
00:12:06,706 --> 00:12:10,790
postmortems result in more reliable system and

183
00:12:10,860 --> 00:12:14,242
is critical to creating and maintaining a successful

184
00:12:14,306 --> 00:12:18,154
SRE organization. For that, it's important to assume good

185
00:12:18,192 --> 00:12:22,438
intention and focus on fixing the systems

186
00:12:22,534 --> 00:12:27,334
that allow the incident to happen and not the people implementing

187
00:12:27,382 --> 00:12:31,610
postmortems. Start with educating the team about blameless

188
00:12:31,690 --> 00:12:35,694
postmortem, running postmortem exercises and crafting those

189
00:12:35,732 --> 00:12:39,294
policy so that we will learn from incident and

190
00:12:39,332 --> 00:12:42,862
will effectively plan work to prevent it from happening again

191
00:12:42,916 --> 00:12:46,514
in those future. We touch on many things we can do

192
00:12:46,552 --> 00:12:49,854
in order to reduce the impact, both from the technical

193
00:12:49,982 --> 00:12:53,554
and human aspects. But how do we know

194
00:12:53,672 --> 00:12:56,946
what we want to focus on? We want to be

195
00:12:56,968 --> 00:13:01,334
data driven in our decisions and we want to prioritize what

196
00:13:01,372 --> 00:13:04,886
is the most important things for us. That data can

197
00:13:04,908 --> 00:13:08,570
be a result of the risk analysis process and the measurement I mentioned

198
00:13:08,640 --> 00:13:12,374
before, but we also want to rely on data collected

199
00:13:12,422 --> 00:13:15,562
from postmortems. Once we have a critical mass

200
00:13:15,616 --> 00:13:19,594
of postmortems, we can identify patterns. It's important to

201
00:13:19,632 --> 00:13:23,194
let the postmortems be our guide, as our investment

202
00:13:23,242 --> 00:13:26,686
in failures can lead us to success and

203
00:13:26,708 --> 00:13:30,858
with our customers, we encourage them to create a shared repository

204
00:13:31,034 --> 00:13:34,270
and share them broadly across internal teams.

205
00:13:34,430 --> 00:13:37,726
We have a lot of public resources that developed

206
00:13:37,758 --> 00:13:40,866
by different teams in Google so you can learn more

207
00:13:40,968 --> 00:13:44,334
about Internet management and reducing customer impact.

208
00:13:44,462 --> 00:13:48,214
We of course have the books. We have a coursera course, the parts

209
00:13:48,252 --> 00:13:52,386
of slos that was developed by CRE team, blog parts

210
00:13:52,418 --> 00:13:56,086
and talks, and webinars developed by the Devrel team.

211
00:13:56,268 --> 00:14:00,842
I've curated for you a few resources to get started and

212
00:14:00,896 --> 00:14:04,650
in the final link you can find resources a

213
00:14:04,720 --> 00:14:07,834
publicly available breakdown by level and

214
00:14:07,872 --> 00:14:10,754
resource type, including the cheat sheet.

215
00:14:10,902 --> 00:14:14,910
Finally, there is a wonderful gift you can give any presenter

216
00:14:15,330 --> 00:14:19,214
the gift of feedback and

217
00:14:19,412 --> 00:14:22,526
as we are virtual and because we are

218
00:14:22,548 --> 00:14:26,906
all about data, I will kindly ask you to go to Bita,

219
00:14:26,938 --> 00:14:30,814
yell at feedback and provide your take.

220
00:14:31,012 --> 00:14:34,542
I was Ayelet Sachto and you are welcome to connect with me

221
00:14:34,596 --> 00:14:38,614
on Twitter or LinkedIn. I will be sharing soon a new white

222
00:14:38,652 --> 00:14:42,550
paper on incidents management. Thank you for listening

223
00:14:42,890 --> 00:14:45,540
and enjoy the rest of Con 42.

