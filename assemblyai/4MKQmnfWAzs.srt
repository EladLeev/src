1
00:00:20,730 --> 00:00:24,206
In this talk I will give you some guidelines to develop efficient Rust code for

2
00:00:24,228 --> 00:00:27,958
numerical application. Whether you are starting from scratch or improving on

3
00:00:27,964 --> 00:00:32,114
top of an existing application, I hope this talk will give you an overview

4
00:00:32,242 --> 00:00:35,282
of the general workflow for developing performance applications.

5
00:00:35,346 --> 00:00:38,522
Harnessing the power of was so a little bit about

6
00:00:38,576 --> 00:00:42,138
me well, I'm a scientific minded software engineer with a

7
00:00:42,144 --> 00:00:45,562
background in scientific simulations. Currently I develop

8
00:00:45,616 --> 00:00:49,466
autonomous trading systems in Russ and Python. During my

9
00:00:49,488 --> 00:00:52,810
career I have been implementing numerical applications in C.

10
00:00:52,960 --> 00:00:56,238
Haskell, Fortran, Python and Russ. In this talk,

11
00:00:56,324 --> 00:00:59,182
I don't intend to compare one programming language to another. Instead,

12
00:00:59,316 --> 00:01:02,926
I would like to give you my distilled experience. Specifically, what I have learned about

13
00:01:03,028 --> 00:01:07,118
numerical implementations in was. Now, numerical implementations

14
00:01:07,214 --> 00:01:10,878
are usually associated with applications that require a lot of speed.

15
00:01:10,974 --> 00:01:13,870
So in fields like science,

16
00:01:13,950 --> 00:01:17,618
engineering, finance, math, we need to perform a large number of

17
00:01:17,704 --> 00:01:21,510
calculations and simulations. We want to understand how proteins work.

18
00:01:21,580 --> 00:01:25,238
We want to know as soon as possible whether the seasonal rain discharge is going

19
00:01:25,244 --> 00:01:28,614
to put us underwater. We want to know whether to buy or sell.

20
00:01:28,652 --> 00:01:31,482
Now, we don't want to buy or sell in a couple of months.

21
00:01:31,616 --> 00:01:35,306
So we of course have several constraints over the calculation that we

22
00:01:35,328 --> 00:01:38,726
can perform because we have limited human and computing resources.

23
00:01:38,918 --> 00:01:42,206
Our goal is then to run as fast as possible,

24
00:01:42,308 --> 00:01:45,230
squeezing all the performance that we can from our hardware.

25
00:01:45,650 --> 00:01:49,246
Now wires for numerical applications as I

26
00:01:49,268 --> 00:01:52,814
mentioned before, we are looking for running as

27
00:01:52,852 --> 00:01:56,574
fast as we can, and Rust is blatantly fast. Together with its ownership

28
00:01:56,622 --> 00:01:59,886
model, it makes a superb language for numerical applications.

29
00:01:59,998 --> 00:02:03,726
But what I think is Rust's killer feature is the tooling.

30
00:02:03,838 --> 00:02:07,574
So having cargo taking care of compiling and linking is just amazing.

31
00:02:07,692 --> 00:02:11,094
If you ever develop numerical applications with other

32
00:02:11,132 --> 00:02:14,470
programming languages like Fortune or C Plus plus, you probably have to deal

33
00:02:14,540 --> 00:02:17,766
with cmake. And if you compare Cmake and

34
00:02:17,868 --> 00:02:21,338
cargo, well, you realize that cargo is light years ahead.

35
00:02:21,504 --> 00:02:25,654
So I really recommend you. Cargo for the medical applications was

36
00:02:25,702 --> 00:02:29,386
in general all the tooling system. Another great reason to

37
00:02:29,408 --> 00:02:32,814
use was is increasing ecosystem, having a

38
00:02:32,852 --> 00:02:35,854
strong open source community providing the right tool for the job.

39
00:02:35,972 --> 00:02:39,966
There is a way to build better software. Now we

40
00:02:39,988 --> 00:02:43,514
all know that, well, real work is complex and writing

41
00:02:43,562 --> 00:02:46,322
numerical applications to deal with the real world is difficult.

42
00:02:46,456 --> 00:02:49,794
So I would like to share with you my philosophical mindset to deal

43
00:02:49,832 --> 00:02:53,614
with this complexity. I firmly believe that intelligence

44
00:02:53,662 --> 00:02:57,026
is just a robust methodology to recursively improve upon

45
00:02:57,058 --> 00:02:58,630
my stupidity.

46
00:03:00,330 --> 00:03:03,942
I think that it doesn't matter how much we prepare for a given

47
00:03:03,996 --> 00:03:07,286
project, whether we know what are the

48
00:03:07,308 --> 00:03:10,986
right algorithms, and we carefully design the architecture. There are always going

49
00:03:11,008 --> 00:03:14,982
to be issues that we are going to discover Apostri.

50
00:03:15,126 --> 00:03:18,666
So we can be either eternally frustrated about it or we

51
00:03:18,688 --> 00:03:22,106
can try to recursively improve of our mistake. So I think that have

52
00:03:22,128 --> 00:03:26,000
a very open minded it's going to help us to keep the peace of mind

53
00:03:26,850 --> 00:03:30,586
and build great software. So when this prelude let us dive into numerical

54
00:03:30,618 --> 00:03:34,510
applications, so the general

55
00:03:34,580 --> 00:03:37,906
algorithm could be like follow. So we

56
00:03:37,928 --> 00:03:41,262
start from a correct but probably a slow version of a numerical workflow.

57
00:03:41,326 --> 00:03:45,566
This initial version could be handed to us by the research team who's

58
00:03:45,598 --> 00:03:49,634
developing it in Python. Then we implement an MVP in Russ.

59
00:03:49,762 --> 00:03:54,022
We decide whether this MVP is fast enough for our use

60
00:03:54,076 --> 00:03:57,366
case. Then if it's not, we would like

61
00:03:57,388 --> 00:04:00,854
to optimize. But before optimizing we would like

62
00:04:00,892 --> 00:04:04,358
to profile and benchmark, and I would like to highlight those two things and I'm

63
00:04:04,374 --> 00:04:07,786
going to discuss it in the next slides. So what is so

64
00:04:07,808 --> 00:04:11,590
important to do? So let's start with the MVP.

65
00:04:11,750 --> 00:04:15,098
So I like to see the development of the MVP

66
00:04:15,274 --> 00:04:19,246
as going through baby steps. So first we

67
00:04:19,268 --> 00:04:22,318
would like to use Clippy. So if you don't know what clippy is, it's a

68
00:04:22,324 --> 00:04:25,474
linter for us and it's using to suggest you to change the

69
00:04:25,512 --> 00:04:29,140
code using more performance patterns. If we don't have

70
00:04:29,670 --> 00:04:33,170
the best code. So it will basically

71
00:04:33,240 --> 00:04:37,174
say, okay, please replace this function by another one or

72
00:04:37,292 --> 00:04:40,534
change the code in a specific way. So I totally recommend

73
00:04:40,652 --> 00:04:44,134
you to use clipping then do not fight

74
00:04:44,172 --> 00:04:47,666
the border checker. We humans are terrible

75
00:04:47,778 --> 00:04:51,034
at knowing which part of the code is taking most of the time.

76
00:04:51,152 --> 00:04:54,666
So before you start drawing reference left

77
00:04:54,688 --> 00:04:58,250
and right, or trying to remove clones

78
00:04:58,590 --> 00:05:02,654
from the code, just focus on writing correct and easy

79
00:05:02,692 --> 00:05:06,206
to read code. Then we can work into

80
00:05:06,308 --> 00:05:09,886
optimizing the code. Finally, I totally recommend you

81
00:05:09,908 --> 00:05:13,694
to use a battle test library, especially if it's for performance critical

82
00:05:13,742 --> 00:05:18,546
operations. I would like to come

83
00:05:18,568 --> 00:05:22,370
back to the third party library subject in a bit now.

84
00:05:22,440 --> 00:05:26,478
So we have the MVP and it's time to decide whether it's

85
00:05:26,494 --> 00:05:30,566
fast enough. Well, it is up to your applications to know

86
00:05:30,588 --> 00:05:33,830
whether it's fast enough or not. So maybe it is

87
00:05:33,980 --> 00:05:36,902
or it is not, but you can throw hardware to it.

88
00:05:36,956 --> 00:05:40,842
Great, maybe it's okay, but you would like to squeeze the most out of it.

89
00:05:40,896 --> 00:05:44,646
So great. So you would like to do some optimizations,

90
00:05:44,758 --> 00:05:47,910
but before doing some optimizations, we need to profile,

91
00:05:48,070 --> 00:05:50,982
profile, then benchmark, then optimize.

92
00:05:51,126 --> 00:05:53,710
So what we would like to profile.

93
00:05:54,050 --> 00:05:57,694
So as we mentioned before, we humans are

94
00:05:57,732 --> 00:06:00,798
terrible at the using game of what in

95
00:06:00,804 --> 00:06:04,914
the code are we spending most of the time. So unless that you work

96
00:06:04,952 --> 00:06:08,910
professionally developing compilers, I just advise you to measure.

97
00:06:09,070 --> 00:06:12,286
So for measuring we can use tools

98
00:06:12,398 --> 00:06:15,682
like perf. So perf will help us to figure

99
00:06:15,736 --> 00:06:19,446
out where in the code we spend most of the time. Perf is

100
00:06:19,468 --> 00:06:23,314
a performance analysis tool for Linux that allows to collect and analyze various performance

101
00:06:23,362 --> 00:06:26,994
related data, as we will see next, we can use perf

102
00:06:27,042 --> 00:06:31,382
output using, we can visualize perf output using flame graphs.

103
00:06:31,526 --> 00:06:34,010
So let us see, what are flame graphs?

104
00:06:34,350 --> 00:06:38,202
So flame graphs are used to visualize where

105
00:06:38,256 --> 00:06:41,002
time is spent in our program. How do they work?

106
00:06:41,136 --> 00:06:44,906
Well, every time that our program is interacted by the os, the location

107
00:06:44,938 --> 00:06:48,794
of the function and its parents is recorded. This is called stack

108
00:06:48,842 --> 00:06:52,110
sampling. These samples are then processed in such a way

109
00:06:52,180 --> 00:06:55,566
that common functions are added together. The metrics are

110
00:06:55,588 --> 00:06:59,058
then collected in a graph showing the color stacks where each level of

111
00:06:59,064 --> 00:07:02,606
the stack is represented by horizontal bar. So the width

112
00:07:02,638 --> 00:07:06,130
of the bar correspond to the amount of time spent in that function.

113
00:07:06,280 --> 00:07:09,854
And the bars are arranged vertically to create a flame light shape.

114
00:07:09,902 --> 00:07:13,446
So what it means is like the main of our program is going to

115
00:07:13,468 --> 00:07:16,198
be in the bottom, so the libraries that we call are going to be in

116
00:07:16,204 --> 00:07:19,562
the middle and our final functions are going to be on top.

117
00:07:19,696 --> 00:07:24,026
So it's very important to realize, like the

118
00:07:24,048 --> 00:07:27,066
x axis doesn't indicate time.

119
00:07:27,248 --> 00:07:31,146
What we really care is about the width of the bars. So the width

120
00:07:31,178 --> 00:07:34,622
of the bars indicates where most of the time is

121
00:07:34,676 --> 00:07:38,558
spent by the cpu doing that particular call.

122
00:07:38,724 --> 00:07:42,126
So perf flame graphs are going

123
00:07:42,148 --> 00:07:45,986
to give us an indication of what are the functions, what are the libraries that

124
00:07:46,008 --> 00:07:47,620
are consuming most of the time.

125
00:07:48,630 --> 00:07:51,986
Now, once the profiling is done,

126
00:07:52,168 --> 00:07:55,474
we would like to do some benchmarking. And the reason

127
00:07:55,512 --> 00:07:59,058
for doing the benchmarking is like, okay, we need to know

128
00:07:59,224 --> 00:08:02,438
what is with confidence, to know with competence what is going to be the

129
00:08:02,444 --> 00:08:06,086
real impact of changing the code. So we don't want to do random changes and

130
00:08:06,108 --> 00:08:10,186
hope for the best. We would like to be able to measure what

131
00:08:10,208 --> 00:08:13,306
is the impact of changing certain part of the code. And the

132
00:08:13,328 --> 00:08:17,542
perfect tool for that is criterion. So criterion

133
00:08:17,686 --> 00:08:21,510
is a benchmarking library that provides a statistical, that provides

134
00:08:21,590 --> 00:08:25,322
statistical confidence on the size of the performance improvements or regressions.

135
00:08:25,466 --> 00:08:29,454
So what it does, it allows us to see

136
00:08:29,492 --> 00:08:32,666
if we change something, how much it affects the general performance

137
00:08:32,698 --> 00:08:35,874
of the code. So first I would like to show you how

138
00:08:35,912 --> 00:08:39,586
it works, and then I would like to show you some metrics of

139
00:08:39,608 --> 00:08:43,090
the output. So in this toy example,

140
00:08:43,240 --> 00:08:47,166
I'm going to use criterion, so we can use here criterion

141
00:08:47,278 --> 00:08:50,774
cmake. Then I'm going to use a library called

142
00:08:50,812 --> 00:08:54,662
mullah. It implements a function

143
00:08:54,716 --> 00:08:58,354
called convolve that is using to perform one dimensional convolution.

144
00:08:58,402 --> 00:09:01,658
If you don't know what is one dimensional convolution, do not worry about it.

145
00:09:01,744 --> 00:09:05,370
So the idea is like we are going to ask criterion

146
00:09:05,790 --> 00:09:10,894
to run this one deconvolution using

147
00:09:11,092 --> 00:09:14,606
arrays of 100 length. So we are going

148
00:09:14,628 --> 00:09:17,886
to generate these arrays. The elements of these arrays are

149
00:09:17,908 --> 00:09:21,630
random numbers taken from a gaussian distribution.

150
00:09:22,930 --> 00:09:26,158
Criterion is going to run this function many times. First it's

151
00:09:26,174 --> 00:09:29,634
going to warm up, and then it's going to run this function many times.

152
00:09:29,752 --> 00:09:33,220
And then we set out a baseline to know

153
00:09:33,670 --> 00:09:37,914
if we may make some changes. What is the actual performance using the baseline?

154
00:09:38,062 --> 00:09:41,286
So now that we have an idea how to use criterion, let us talk

155
00:09:41,308 --> 00:09:44,966
a little bit about optimizations. So I

156
00:09:44,988 --> 00:09:48,694
think that the three most important applications that we can perform in a code is

157
00:09:48,732 --> 00:09:52,102
first you make sure that you pick the right algorithm.

158
00:09:52,246 --> 00:09:55,578
Once you know that, then you make sure that you pick the

159
00:09:55,584 --> 00:09:59,066
right algorithm. And finally you ask someone else to check for you if

160
00:09:59,088 --> 00:10:02,266
you have picked the right algorithm. So what it means is

161
00:10:02,288 --> 00:10:05,626
like, well, you need to do your math homework. If you don't

162
00:10:05,658 --> 00:10:10,106
feel confident about your math, you need to chase your favorite mathematician

163
00:10:10,218 --> 00:10:13,834
to give you some help about it. Once you are pretty sure you have chosen

164
00:10:13,882 --> 00:10:17,278
the right algorithm, then you can write other applications.

165
00:10:17,294 --> 00:10:21,378
Like let us try to preallocate the vector. So there are functions like from back,

166
00:10:21,464 --> 00:10:25,082
like preallocate a vector with certain capacity.

167
00:10:25,246 --> 00:10:28,658
Or you can use a non cryptographic hash algorithm for the hash

168
00:10:28,674 --> 00:10:32,150
map. So hashmap the default behavior.

169
00:10:32,570 --> 00:10:36,822
It use a hashing algorithm that is for

170
00:10:36,876 --> 00:10:40,806
cryptographic applications. What it means is, takes some time to compute the hashes

171
00:10:40,838 --> 00:10:45,034
of the keys, because numerical applications are usually far away

172
00:10:45,152 --> 00:10:48,540
from user inputs. We want to run

173
00:10:49,630 --> 00:10:53,370
the hashing algorithm as fast as we can. So I totally recommend you

174
00:10:53,440 --> 00:10:57,182
to use a non cryptographic algorithm. So I will point

175
00:10:57,236 --> 00:11:00,954
to one of those algorithms, not the algorithms,

176
00:11:01,002 --> 00:11:03,520
but the actual library in a bit.

177
00:11:04,150 --> 00:11:08,020
If you want to know more, I recommend you to have a look at

178
00:11:08,390 --> 00:11:12,130
the perf book of Ras. Now let's go back to

179
00:11:12,200 --> 00:11:15,090
benchmarking plus optimizations.

180
00:11:15,510 --> 00:11:19,362
So we are trying to optimize our combo

181
00:11:19,426 --> 00:11:23,174
1D function. So what we do first is that we run

182
00:11:23,212 --> 00:11:27,106
the criterion without changing any code. So what it's

183
00:11:27,138 --> 00:11:30,522
going to do is going to try this function many

184
00:11:30,576 --> 00:11:34,678
times. It's going to warm up and then it's going to set a baseline.

185
00:11:34,854 --> 00:11:38,106
Then we do some code, some modification in the

186
00:11:38,128 --> 00:11:42,190
code, and then we run it again. We see

187
00:11:42,340 --> 00:11:46,074
that this new modification generates a meager

188
00:11:46,122 --> 00:11:49,566
2.5% of improvement. Then we say,

189
00:11:49,588 --> 00:11:52,974
okay, this seems to be going okay. I would say then

190
00:11:53,092 --> 00:11:56,738
let us try something different. Then you try get another change in

191
00:11:56,744 --> 00:12:00,958
the code. But unfortunately there are no optimizations

192
00:12:01,054 --> 00:12:04,258
gained. I would like to tell you something that

193
00:12:04,264 --> 00:12:07,966
is really important about benchmarking. So, benchmarking assumes

194
00:12:07,998 --> 00:12:11,494
that you are running in an isolated machine. So what it means

195
00:12:11,532 --> 00:12:14,998
is like if you try to run a benchmarking, your local machine where you

196
00:12:15,004 --> 00:12:18,754
are developing, and then you just have

197
00:12:18,892 --> 00:12:22,700
browser Spotify, a lot of application running. Then if you

198
00:12:23,070 --> 00:12:26,858
just stop Spotify, for instance, you will see an increase in performance of

199
00:12:26,864 --> 00:12:30,586
15%. So actually you need to run

200
00:12:30,608 --> 00:12:34,430
in a machine that is not doing anything else. Otherwise you're going to have spurious

201
00:12:34,770 --> 00:12:37,482
performance gain. So when you are benchmarking,

202
00:12:37,546 --> 00:12:40,622
just have another machine, a VM, another local

203
00:12:40,676 --> 00:12:45,586
machine, somewhere that you can access and only run the

204
00:12:45,608 --> 00:12:47,730
calculations that you are trying to benchmark.

205
00:12:48,550 --> 00:12:52,274
Okay, so now we know about benchmarking, we know

206
00:12:52,312 --> 00:12:55,730
how to optimize, we know what we are targeting.

207
00:12:56,170 --> 00:12:59,560
So it has arrived the moment to test.

208
00:13:00,010 --> 00:13:03,954
I think that there are three complementary strategies

209
00:13:04,002 --> 00:13:08,486
that we can use for testing. So let us review them. So first,

210
00:13:08,668 --> 00:13:12,458
you can ask your large language model if every large language model to

211
00:13:12,464 --> 00:13:16,406
generate unit tests for you. I'm not going to discuss whether AI

212
00:13:16,438 --> 00:13:20,234
is going to overtake the word. I'm just simply saying that large

213
00:13:20,272 --> 00:13:24,570
language models are fantastic for unit testing. So use them for generating

214
00:13:24,650 --> 00:13:28,094
tests for you. The other thing that you can do is just look

215
00:13:28,132 --> 00:13:32,046
for edge cases. By edge cases I mean you

216
00:13:32,068 --> 00:13:35,762
can try to look for inputs for your model, for model,

217
00:13:35,816 --> 00:13:39,198
or for your workflow that can generate

218
00:13:39,294 --> 00:13:42,466
numerical instabilities. Other thing that you

219
00:13:42,488 --> 00:13:45,406
can try, it is a property testing framework,

220
00:13:45,598 --> 00:13:49,334
something like protest. I think protest deserves its own

221
00:13:49,372 --> 00:13:53,078
explanation. So let me give you a little example of how it works.

222
00:13:53,164 --> 00:13:57,186
So, protest is a property testing framework. It's inspired

223
00:13:57,218 --> 00:14:01,020
by hypothesis from Python. What it does,

224
00:14:01,710 --> 00:14:05,386
it allows you to test that certain properties hold for

225
00:14:05,408 --> 00:14:09,030
your code, for arbitrary input. And if something fails,

226
00:14:09,110 --> 00:14:12,622
it will try to create a minimal test case that is going to tell you

227
00:14:12,756 --> 00:14:16,270
in what specific input your property is not hold

228
00:14:16,340 --> 00:14:20,238
and why it fails. So in this example,

229
00:14:20,324 --> 00:14:23,886
I have two functions that we

230
00:14:23,908 --> 00:14:27,374
are trying to find. The maximum of a slice of

231
00:14:27,492 --> 00:14:30,270
loads and the minimum of a slice of loads.

232
00:14:30,630 --> 00:14:33,730
Then using protest, we are going to set up

233
00:14:33,800 --> 00:14:37,154
a test that is going to generate random

234
00:14:37,202 --> 00:14:40,534
vectors when dimension from zero to

235
00:14:40,572 --> 00:14:44,758
1000 using floating points, and then

236
00:14:44,924 --> 00:14:48,458
using the

237
00:14:48,544 --> 00:14:52,140
protest macro, we are going to check that

238
00:14:52,510 --> 00:14:55,834
the property holds. So what is the property?

239
00:14:55,952 --> 00:14:59,802
Well, we want to know whether the mean value is

240
00:14:59,856 --> 00:15:03,598
smaller than the rest of the slice, and we want to know

241
00:15:03,684 --> 00:15:07,006
whether the max value is larger than the

242
00:15:07,028 --> 00:15:10,494
rest of the values in the slice. So prop test

243
00:15:10,532 --> 00:15:14,834
is basically going to help you to set up your

244
00:15:14,872 --> 00:15:18,338
tests in such a way that you can generate a bunch of

245
00:15:18,424 --> 00:15:22,766
random inputs and test it. The property of your interest holds.

246
00:15:22,958 --> 00:15:26,422
Great. So now we have covered testing and

247
00:15:26,476 --> 00:15:30,086
we have covered what is the general workflow for developing a

248
00:15:30,108 --> 00:15:33,960
numerical and algorithm. So to implement then,

249
00:15:34,890 --> 00:15:38,390
now let us talk about some other aspects

250
00:15:38,470 --> 00:15:41,878
that we need to know about numerical applications.

251
00:15:42,054 --> 00:15:45,530
So now one important thing is flaunting points.

252
00:15:45,680 --> 00:15:49,626
So in computing a round of error or random error is

253
00:15:49,648 --> 00:15:52,986
the difference between the result provided by a given algorithm. So if

254
00:15:53,008 --> 00:15:56,010
we take one algorithm and we use arithmetic,

255
00:15:56,170 --> 00:16:00,240
and the difference between that and using

256
00:16:01,170 --> 00:16:04,926
the same algorithm with the same input, using finite

257
00:16:04,958 --> 00:16:08,658
precision rounded arithmetic. So as we know, floating points,

258
00:16:08,744 --> 00:16:12,398
they cannot represent all real numbers accurately,

259
00:16:12,494 --> 00:16:16,150
so they are always running errors. So whether

260
00:16:16,220 --> 00:16:20,066
you are implementing something from scratch or taking an offshore

261
00:16:20,178 --> 00:16:23,686
library, you need to realize that there

262
00:16:23,708 --> 00:16:26,966
could be potential issues when plotting points and numbers. So there

263
00:16:26,988 --> 00:16:31,078
is a subfield of mathematics called numerical analysis

264
00:16:31,254 --> 00:16:35,286
that deals when designing methods that get approximate but accurate numerical

265
00:16:35,318 --> 00:16:38,966
solutions. So if you are into numerical

266
00:16:38,998 --> 00:16:42,954
applications, it is really important that you have a look into

267
00:16:42,992 --> 00:16:46,206
what numerical analysis says about the algorithms that you are going to use for your

268
00:16:46,228 --> 00:16:49,582
applications. Another important thing to mention is like

269
00:16:49,636 --> 00:16:52,846
if you are doing operation in finance, there is

270
00:16:52,868 --> 00:16:55,934
a library called rust decimal

271
00:16:55,982 --> 00:16:59,582
that is going to help you to perform calculations

272
00:16:59,646 --> 00:17:02,526
for finance without running errors.

273
00:17:02,718 --> 00:17:05,538
Now, once we have covered that,

274
00:17:05,704 --> 00:17:08,150
let's move to the third party libraries.

275
00:17:08,650 --> 00:17:12,466
Usually when we need to decide

276
00:17:12,498 --> 00:17:15,894
whether we want to implement sorting or we want to bring a third

277
00:17:15,932 --> 00:17:19,210
party library, we need to ask us several questions.

278
00:17:19,360 --> 00:17:23,066
Well, how important is this algorithm that

279
00:17:23,088 --> 00:17:26,860
we are trying to implement or to bring from another library to us?

280
00:17:27,390 --> 00:17:31,182
How confident are we about using, able to implement this?

281
00:17:31,316 --> 00:17:35,358
Are we willing to maintain this library? Or if there is something

282
00:17:35,524 --> 00:17:38,000
else outside, well,

283
00:17:38,370 --> 00:17:42,282
around in the open source community that already implement this algorithm,

284
00:17:42,346 --> 00:17:45,698
it is in a good state. These are the questions that we need

285
00:17:45,704 --> 00:17:49,650
to ask ourselves before deciding. I think

286
00:17:49,800 --> 00:17:53,410
that the general rule of thumbs is

287
00:17:53,480 --> 00:17:57,610
as follow. I think that for other than trivial algorithms,

288
00:17:57,790 --> 00:18:01,286
other than three lines algorithm, we should use

289
00:18:01,308 --> 00:18:04,498
a third party library, even though it's writing in a non

290
00:18:04,514 --> 00:18:08,406
so shiny language like c or c plus plus. And the reason for that is

291
00:18:08,428 --> 00:18:11,574
as follows. So if you want to know how an algorithm works,

292
00:18:11,612 --> 00:18:14,874
I think that the best way to do it is just try to implement it

293
00:18:14,912 --> 00:18:18,506
yourself. And this is a fantastic way of learning about how algorithms work,

294
00:18:18,528 --> 00:18:22,462
and I do it all the time. But there is a big difference between trying

295
00:18:22,516 --> 00:18:26,282
to learn something and trying to come out with a faster,

296
00:18:26,426 --> 00:18:30,158
better, more performant and more robust general application.

297
00:18:30,324 --> 00:18:34,266
That is usually what we found in open source code. So in

298
00:18:34,308 --> 00:18:37,874
open source libraries, what's going to happen is there have been

299
00:18:37,912 --> 00:18:41,140
many cycles where different approaches have been tried.

300
00:18:41,670 --> 00:18:45,138
It have gone through many errors, it have gone through a

301
00:18:45,144 --> 00:18:48,626
lot of applications. And I think that it would be really naive

302
00:18:48,738 --> 00:18:52,134
to try to think that for really like a really

303
00:18:52,172 --> 00:18:55,960
popular algorithms, we just go there and we come up with something much better.

304
00:18:56,570 --> 00:19:00,370
Well, if you are implementing an algorithm from scratch and you are a master

305
00:19:00,450 --> 00:19:03,770
in that part of mathematics, and you are the only one who knows that,

306
00:19:03,840 --> 00:19:07,354
yes, by all means, just go ahead and bring your algorithm to the community.

307
00:19:07,472 --> 00:19:10,554
But if we are talking about really well known algorithms, I think it is much

308
00:19:10,592 --> 00:19:13,770
better to join forces with another

309
00:19:13,840 --> 00:19:17,214
community, even though it's not the rust community, and try to maintain the

310
00:19:17,252 --> 00:19:20,080
most efficient algorithm that is available.

311
00:19:21,170 --> 00:19:24,340
Finally, I would like to speak about

312
00:19:24,950 --> 00:19:28,066
two things. One is some applications that are

313
00:19:28,088 --> 00:19:32,082
already available for was, and other is like the interface between

314
00:19:32,136 --> 00:19:36,014
was and python. First, let us talk about some libraries

315
00:19:36,062 --> 00:19:39,506
that are using every day, and that I think they are fantastic for numerical

316
00:19:39,538 --> 00:19:43,382
applications. So the first of them is not a single library, but a family

317
00:19:43,436 --> 00:19:46,934
of libraries that are called was ndrrate. So they are used for

318
00:19:46,972 --> 00:19:50,674
array manipulation, statistics, linear algebra, for array. They are

319
00:19:50,732 --> 00:19:54,038
fantastic well maintained library. Another well known

320
00:19:54,054 --> 00:19:58,278
library is Ryon. That is basically if you have sequential

321
00:19:58,454 --> 00:20:02,206
calculation and you want to run in parallel, you can use Ryon to help you

322
00:20:02,228 --> 00:20:06,270
to do that. Polars is a light infos data frame library.

323
00:20:06,850 --> 00:20:10,414
Rascash provides you with

324
00:20:10,452 --> 00:20:14,062
a non cryptographic hash algorithm, what I mentioned before. So like

325
00:20:14,116 --> 00:20:18,046
for caching of all numerical application, we are going to use a lot of hash

326
00:20:18,078 --> 00:20:21,614
maps, but we don't want a cryptographic hash

327
00:20:21,742 --> 00:20:25,186
algorithm because it's really slow. And also there are other two

328
00:20:25,208 --> 00:20:28,494
libraries approach. So grass and the array

329
00:20:28,542 --> 00:20:31,606
offers a feature when approached. So to help you

330
00:20:31,628 --> 00:20:36,002
to compare floating points. So when you are doing calculations

331
00:20:36,066 --> 00:20:39,206
using arrays, it's really important to be able to know whether two arrays

332
00:20:39,238 --> 00:20:42,810
are the same or not. Finally, there is this other library called Order

333
00:20:42,880 --> 00:20:46,634
float that is going to help you to

334
00:20:46,672 --> 00:20:49,690
compare totally order floats.

335
00:20:50,750 --> 00:20:53,934
With that I think that I have covered some of the perks that I consider

336
00:20:54,052 --> 00:20:57,322
in Amus Health for the medical applications.

337
00:20:57,466 --> 00:21:00,654
Finally, I would like to have a

338
00:21:00,692 --> 00:21:04,480
word about Russ and Python. So,

339
00:21:06,050 --> 00:21:08,922
engineers, scientists, quants,

340
00:21:09,066 --> 00:21:12,510
many professionals from the medical fields are proficient in Python,

341
00:21:12,590 --> 00:21:16,270
and they are not super proficient in rust. So what happened is, like,

342
00:21:16,440 --> 00:21:20,950
most of the numerical algorithms are going to be implemented in Python.

343
00:21:21,530 --> 00:21:24,806
And, well, as we probably know, Python is

344
00:21:24,828 --> 00:21:28,566
not the most efficient language there. So for

345
00:21:28,588 --> 00:21:32,646
me, it is natural. Like, we would like to bring

346
00:21:32,748 --> 00:21:36,486
grass to the Python community. So we would like to integrate grass

347
00:21:36,518 --> 00:21:40,218
with Python. So if you don't know, there is a

348
00:21:40,224 --> 00:21:43,360
library called matrix that is the perfect tool for this situation.

349
00:21:44,290 --> 00:21:48,362
Materin is just a was tool that facilitates the creation of python models

350
00:21:48,426 --> 00:21:52,174
that wrap ras code. So they make really easy to

351
00:21:52,212 --> 00:21:55,554
call your was code from inside Python without the need to

352
00:21:55,592 --> 00:21:59,586
do some integration using, say, python. And I think that by

353
00:21:59,608 --> 00:22:02,562
exposing our APIs, our libraries to the Python community,

354
00:22:02,696 --> 00:22:06,530
we're just not only giving the Python users

355
00:22:06,610 --> 00:22:10,354
well, we are going to accelerate

356
00:22:10,402 --> 00:22:14,694
their calculations, but also we are going to expose them to how

357
00:22:14,732 --> 00:22:18,342
was works. So I think this is a win, a win situation for

358
00:22:18,396 --> 00:22:21,846
us and python communities. And with that, I would like to thank you for your

359
00:22:21,868 --> 00:22:25,798
time. This is what I had to say today. If you have any questions,

360
00:22:25,884 --> 00:22:29,606
please, you can also always drop me

361
00:22:29,628 --> 00:22:33,626
a message reach, or you can send me a message through LinkedIn.

362
00:22:33,738 --> 00:22:34,780
Thank you very much for your time.

