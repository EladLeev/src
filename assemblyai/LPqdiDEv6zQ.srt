1
00:00:20,970 --> 00:00:24,046
Greetings everyone. My name is Jayanta along with

2
00:00:24,068 --> 00:00:28,146
Sapna, both of us we'll be talking about Apiz today so let's

3
00:00:28,178 --> 00:00:31,734
briefly understand what this talk is about. From this talk what all things

4
00:00:31,772 --> 00:00:35,026
we can expect. EpIs is a scientific computing

5
00:00:35,058 --> 00:00:39,078
case study. We'll be speaking about building epidemic

6
00:00:39,094 --> 00:00:42,842
simulator using a rust language. We'll be speaking about our

7
00:00:42,896 --> 00:00:46,634
journey, about how did we went ahead and built

8
00:00:46,672 --> 00:00:50,006
a simulator which is optimized for performance and

9
00:00:50,048 --> 00:00:53,774
it can support large scale. Also we'll be sharing some

10
00:00:53,812 --> 00:00:57,934
of our understanding and our insights how

11
00:00:57,972 --> 00:01:01,578
just as a language helped us building the specific simulator.

12
00:01:01,754 --> 00:01:05,554
What is out of scope for these specific talk is we will not be

13
00:01:05,672 --> 00:01:10,114
speaking much more detailed about our Covid-19 epidemic model.

14
00:01:10,312 --> 00:01:14,382
It deserves its own talk. And this talk is purely going to be technical

15
00:01:14,446 --> 00:01:18,006
in nature. But even being said that

16
00:01:18,108 --> 00:01:21,938
we need to cover some of the basic understanding

17
00:01:22,034 --> 00:01:25,634
so as to understand the problem statement. And initially

18
00:01:25,682 --> 00:01:29,302
in the agenda we'll specifically talk about APZ in the beginning

19
00:01:29,446 --> 00:01:33,366
these we can talk about memory optimization, performance optimization

20
00:01:33,478 --> 00:01:36,890
results and the insights which we gathered from this

21
00:01:36,960 --> 00:01:40,060
specific tool. That would be towards the end.

22
00:01:40,830 --> 00:01:44,618
So let's briefly understand what is an episode.

23
00:01:44,794 --> 00:01:48,106
As it has been described, it is an agent based large scale

24
00:01:48,138 --> 00:01:51,518
open source epidemic simulator. So let's try and understand

25
00:01:51,604 --> 00:01:54,942
each and every single term of this. So when we say it's an agent

26
00:01:54,996 --> 00:01:58,642
based simulator, what it means is we understand that

27
00:01:58,696 --> 00:02:02,594
spread of a disease is a complex phenomena. All of us have witnessed in

28
00:02:02,632 --> 00:02:05,970
past two years how Covid-19 is spreading. It is difficult

29
00:02:06,040 --> 00:02:09,990
to understand as there are numerous factors which has impact on these spread.

30
00:02:10,570 --> 00:02:13,974
So what an agent based model does is agent based model is

31
00:02:14,012 --> 00:02:17,574
much more bottom up approach is we start by

32
00:02:17,612 --> 00:02:21,558
creating every individual in the society or every individual

33
00:02:21,644 --> 00:02:25,194
in the system which these individual areas

34
00:02:25,232 --> 00:02:28,618
of interest for us. So we create every individual person,

35
00:02:28,704 --> 00:02:32,734
then we create their homes, then we create their workspaces and

36
00:02:32,772 --> 00:02:36,522
every individual, we assign certain schedule to these individuals.

37
00:02:36,666 --> 00:02:40,554
So we create a virtual life virtual society where these individuals

38
00:02:40,602 --> 00:02:44,174
are performing their day to day routine. They are interconnecting,

39
00:02:44,222 --> 00:02:47,874
exchanging some of the information they areas, meeting each

40
00:02:47,912 --> 00:02:51,854
other in physical world. And all of these interactions

41
00:02:51,982 --> 00:02:55,638
can potentially cause a spread of disease. So that is the

42
00:02:55,724 --> 00:02:59,542
basic fundamental about agent based simulation. So on the right

43
00:02:59,596 --> 00:03:03,218
we can see a grid

44
00:03:03,314 --> 00:03:07,266
which is depicting a minimalist model which we are using for ephesus.

45
00:03:07,378 --> 00:03:11,034
So every blue dot in this grid is

46
00:03:11,072 --> 00:03:14,922
an agents. So this is one of the smaller scale model

47
00:03:15,056 --> 00:03:17,926
which might be for like few hundred individuals.

48
00:03:18,038 --> 00:03:21,262
So all of these individuals, they stay in certain home

49
00:03:21,316 --> 00:03:24,558
area where they interact with their family members

50
00:03:24,724 --> 00:03:28,026
while going to office. Some of these individuals

51
00:03:28,058 --> 00:03:31,854
will take up, will take space and transport areas that they

52
00:03:31,892 --> 00:03:35,626
will be accompanied by other individuals who may be stranger.

53
00:03:35,818 --> 00:03:39,758
And just by sitting next to each other, there's a chance that a disease

54
00:03:39,854 --> 00:03:43,294
can be spread through. So that is why we have a transport section.

55
00:03:43,342 --> 00:03:46,658
Similarly, in work area people will come to work,

56
00:03:46,824 --> 00:03:50,994
they will interact with their colleagues. And again such interactions

57
00:03:51,042 --> 00:03:55,126
might lead to spread of disease. And once these individuals come back home,

58
00:03:55,308 --> 00:03:58,394
it might happen that someone might get infected at work.

59
00:03:58,512 --> 00:04:02,154
And again the person is in connection with the

60
00:04:02,192 --> 00:04:05,594
family members and might spread diseases at home as well.

61
00:04:05,712 --> 00:04:09,846
So it's a minimalist model which is depicting spread

62
00:04:09,878 --> 00:04:13,070
of diseases. And in this case specifically Covid-19.

63
00:04:14,530 --> 00:04:17,902
So at the bottom it sounds

64
00:04:17,956 --> 00:04:21,806
pretty complex, right? But if you want to take the most simple

65
00:04:21,908 --> 00:04:25,370
view of episode, I can easily

66
00:04:25,450 --> 00:04:28,802
explain it as two for loops. So the outer loop is

67
00:04:28,936 --> 00:04:32,462
a clock tick. So there is a vector clock which is simulating

68
00:04:32,526 --> 00:04:36,306
us in this context of a virtual society. So what

69
00:04:36,328 --> 00:04:39,918
we are doing is at ivz clock tick which is at iVzrl.

70
00:04:40,014 --> 00:04:43,094
We are iterating over every agent and asking them

71
00:04:43,132 --> 00:04:47,078
to perform their corresponding routine. And once this

72
00:04:47,164 --> 00:04:50,898
routine is performed, at the end of each action we

73
00:04:50,924 --> 00:04:54,634
are checking if there is a possibility of spread of a disease. And once

74
00:04:54,672 --> 00:04:58,486
we iterate through all the agents then towards the end we do stockkeeping.

75
00:04:58,518 --> 00:05:02,078
In terms of we see how many folks got infected in

76
00:05:02,084 --> 00:05:05,754
the last hour. Also if government wants to intervene,

77
00:05:05,802 --> 00:05:09,326
if there areas certain three conditions which ask government to

78
00:05:09,348 --> 00:05:13,890
put a lockdown which ask government to

79
00:05:13,960 --> 00:05:17,614
ask people to wear masks or to start a vaccination.

80
00:05:17,662 --> 00:05:20,994
Such intervention can as well be modeled. So that is

81
00:05:21,032 --> 00:05:23,410
most simplified view of apologist.

82
00:05:24,230 --> 00:05:28,214
So this is our journey. We started busy, simple. Initially we

83
00:05:28,252 --> 00:05:31,798
started with thousand population. Every individual

84
00:05:31,884 --> 00:05:35,858
in these skis was same as other person. These slowly

85
00:05:35,874 --> 00:05:40,066
we moved to 10,000 individuals. And we started bringing heterogeneity.

86
00:05:40,178 --> 00:05:43,370
Heterogeneity in terms of these individuals

87
00:05:44,110 --> 00:05:47,690
go to work. Not everyone in the society is working.

88
00:05:47,760 --> 00:05:51,114
There might be younger kids or elders who are staying at home.

89
00:05:51,232 --> 00:05:54,942
So based on age there's stratification. Then there are certain

90
00:05:54,996 --> 00:05:58,494
individuals who are essential workers as in these individuals will still

91
00:05:58,532 --> 00:06:02,014
work if these is a lockdown imposed. So we

92
00:06:02,052 --> 00:06:05,630
started slowly modeling, capturing the complexity from these domain

93
00:06:05,710 --> 00:06:09,474
and making our model rich in terms of domain. The way in next

94
00:06:09,512 --> 00:06:13,522
target after finishing ten k was to model Pune city

95
00:06:13,576 --> 00:06:17,782
which is a city in India from where both of us are. And since

96
00:06:17,836 --> 00:06:21,154
the scale was 3.2 million individuals.

97
00:06:21,282 --> 00:06:24,982
Scale and performance became the issue pretty quickly for us.

98
00:06:25,116 --> 00:06:28,614
Then once we modeled Pune, we went ahead

99
00:06:28,652 --> 00:06:32,058
and started modeling Mumbai. So Mumbai is one of

100
00:06:32,064 --> 00:06:35,274
the metro cities in India, which has population, which is 12 million.

101
00:06:35,392 --> 00:06:38,362
So roughly four times of Pune. Also,

102
00:06:38,416 --> 00:06:42,080
one of the challenges which we faced in the Mumbai context is

103
00:06:42,450 --> 00:06:45,786
modeling commute. Because many folks in Mumbai,

104
00:06:45,898 --> 00:06:49,134
they travel a large distance from one place to

105
00:06:49,172 --> 00:06:52,158
reach their workplaces and then come back home.

106
00:06:52,324 --> 00:06:55,886
And next goal, which is work in progress, is we want to reach a scale

107
00:06:55,918 --> 00:06:59,922
which is 100 million. With 100 million, we can model

108
00:07:00,056 --> 00:07:03,682
our state, which is Maharashtra, which is one of the largest state

109
00:07:03,736 --> 00:07:07,730
in India, and which as will help us to model multiple cities

110
00:07:07,810 --> 00:07:10,774
at the same time. So,

111
00:07:10,812 --> 00:07:14,658
having said that, let's briefly understand what are technical complexities

112
00:07:14,754 --> 00:07:18,530
in this context of episode. Epirust is compute intensive.

113
00:07:18,610 --> 00:07:22,246
So what it means is if we talk about a simulation

114
00:07:22,278 --> 00:07:26,026
which runs for 45 days, the outer loop, which we saw

115
00:07:26,048 --> 00:07:30,054
for time, it runs for 1080 ticks, because each tick

116
00:07:30,102 --> 00:07:33,562
is one r 45 into 24, which will come down to 10

117
00:07:33,616 --> 00:07:37,358
80 ticks. So if we started with a population of thousand

118
00:07:37,444 --> 00:07:40,506
individuals, so for such a smaller population,

119
00:07:40,698 --> 00:07:44,406
there would be 7 million behaviors, which needs to be executed

120
00:07:44,458 --> 00:07:47,746
in the context of this 45 days. So if

121
00:07:47,768 --> 00:07:51,886
we increase population to 1 million, the number from 7 million rises

122
00:07:51,918 --> 00:07:55,186
to 7 billion, and moving one step ahead from

123
00:07:55,208 --> 00:07:58,326
1 million to 100 million, the number of behaviors will move

124
00:07:58,348 --> 00:08:01,320
on to 700 million. Also,

125
00:08:01,930 --> 00:08:05,842
when we talk about scale, if we carefully

126
00:08:05,906 --> 00:08:09,474
look at this metrics on the right, the metrics is parse

127
00:08:09,602 --> 00:08:13,478
that not all, most of the places are empty in this context,

128
00:08:13,654 --> 00:08:17,238
which means we will as well need to take care of memory footprint

129
00:08:17,334 --> 00:08:21,166
when we are building such a model in memory. And then

130
00:08:21,188 --> 00:08:24,494
there are domain complexities. We need to understand what are

131
00:08:24,532 --> 00:08:28,206
different things, which goes with Covid-19 specifically because

132
00:08:28,308 --> 00:08:31,994
epirust is built specific initially for Covid-19.

133
00:08:32,122 --> 00:08:35,138
So we need to understand the way disease works. We need to understand

134
00:08:35,224 --> 00:08:38,914
government interventions and how these interventions have impact on

135
00:08:39,032 --> 00:08:42,606
individuals lives. And then adding

136
00:08:42,638 --> 00:08:45,886
on to this, there is one more difficulty.

137
00:08:46,078 --> 00:08:49,462
So the inner for loop, which we saw earlier, which is per

138
00:08:49,516 --> 00:08:52,742
agent, it executes agent one after other.

139
00:08:52,876 --> 00:08:56,134
But when we speak about agent interactions in real

140
00:08:56,172 --> 00:08:59,766
life, all of us move at the same time and we do not move one

141
00:08:59,788 --> 00:09:03,734
after other. So such a loop causes

142
00:09:03,782 --> 00:09:07,862
part dependency. To solve that, we are using 2d buffering algorithm

143
00:09:07,926 --> 00:09:11,390
to take care of to remove the part dependency from the picture.

144
00:09:12,530 --> 00:09:15,982
So how did we start? As I said earlier, we started

145
00:09:16,036 --> 00:09:19,786
with very simple mechanism. We started with two for loops.

146
00:09:19,818 --> 00:09:22,986
The code was pretty material. The grid

147
00:09:23,018 --> 00:09:25,998
which we saw was implemented as 2d metrics.

148
00:09:26,174 --> 00:09:29,746
Population for Pune City is 3.2 million, which means

149
00:09:29,928 --> 00:09:33,806
if we look at number of seals, this 2d metrics,

150
00:09:33,838 --> 00:09:37,094
number of elements, this 2d metrics used to hold that was

151
00:09:37,132 --> 00:09:41,190
close to 32 million cells. Which means the memory consumption

152
00:09:41,530 --> 00:09:45,446
went ahead from. And it was approximately somewhere between

153
00:09:45,548 --> 00:09:48,040
five to Tengb, which was pretty big.

154
00:09:48,890 --> 00:09:52,166
Keeping our goals in mind, we wanted to model not just for Pune

155
00:09:52,198 --> 00:09:55,386
City but as well as for Mumbai and want to

156
00:09:55,408 --> 00:09:58,380
go ahead and reach scale with 100 million.

157
00:09:59,070 --> 00:10:02,586
So we faced a challenge for modeling the

158
00:10:02,608 --> 00:10:06,014
specific grid. One solution which we came up with is if we can

159
00:10:06,052 --> 00:10:09,418
represent this 2d as a grid, as a hash

160
00:10:09,434 --> 00:10:12,894
map. So we created a structure where the key for

161
00:10:12,932 --> 00:10:16,450
hash map is point and the value becomes object of

162
00:10:16,520 --> 00:10:20,302
citizen struct. So point is nothing but xy location

163
00:10:20,366 --> 00:10:23,634
on the plane, on the grid. And the citizen is that

164
00:10:23,672 --> 00:10:27,506
individual agent who is occupying this specific sale.

165
00:10:27,698 --> 00:10:31,160
So essentially what happened is number of agents

166
00:10:31,530 --> 00:10:34,998
became numbers of entries in the specific hash map. Since it

167
00:10:35,004 --> 00:10:38,614
is a hash map, Zival became much more easier. Now instead of order

168
00:10:38,652 --> 00:10:41,578
of n square, now it is just plain order of one.

169
00:10:41,744 --> 00:10:45,414
Oppositions memory which was earlier in the extent

170
00:10:45,462 --> 00:10:49,290
of five to tengB, right away came to few hundred mb.

171
00:10:49,790 --> 00:10:53,214
We went one step ahead and then we started looking at what

172
00:10:53,252 --> 00:10:56,554
can be efficient hashing algorithm which can give us better mileage.

173
00:10:56,682 --> 00:11:00,938
So we experimented with hash brown, we experimented with FX hash,

174
00:11:01,034 --> 00:11:04,526
we experimented with FNV hash. FNV hash because it

175
00:11:04,548 --> 00:11:08,180
is non cryptographic and these hash is not exposed outside.

176
00:11:08,550 --> 00:11:13,134
So FNV hash gave us the best possible outcome

177
00:11:13,262 --> 00:11:15,490
and we went ahead with FNV hash.

178
00:11:16,650 --> 00:11:20,662
Having said that, I'll hand it over to Sapna to talk about

179
00:11:20,796 --> 00:11:23,110
how did we optimize for throughput.

180
00:11:25,370 --> 00:11:28,774
Thanks Taranta. Now we will be seeing how do

181
00:11:28,812 --> 00:11:32,874
we optimize for the throughput performance. So what

182
00:11:32,912 --> 00:11:36,566
we tried doing is first representing

183
00:11:36,598 --> 00:11:40,278
the parallel incrementation form. As you saw, we are using 2d

184
00:11:40,304 --> 00:11:44,254
buffers. One is the read buffer. From there, when our

185
00:11:44,292 --> 00:11:47,470
internal for group starts running, the agent will

186
00:11:47,540 --> 00:11:51,566
see its neighboring agent position from the read buffer and

187
00:11:51,668 --> 00:11:54,994
update its position according to its training routine into

188
00:11:55,032 --> 00:11:59,054
the write buffer. And each updation

189
00:11:59,102 --> 00:12:02,558
of each agent is independent of the other agent status.

190
00:12:02,654 --> 00:12:06,390
So we can easily use the data parallelization here.

191
00:12:06,540 --> 00:12:12,066
So one of the method was the map duries where we actually individually

192
00:12:12,178 --> 00:12:15,414
update the agent status and later on

193
00:12:15,452 --> 00:12:18,934
serially store it into the temporary

194
00:12:18,982 --> 00:12:22,950
sum data structure and later on serially update the status

195
00:12:23,110 --> 00:12:26,902
to the right buffer. That is just to avoid the collision

196
00:12:26,966 --> 00:12:30,970
into the map because we are using buffer as a map.

197
00:12:31,490 --> 00:12:35,566
In this case the parallelization wasn't completely one

198
00:12:35,588 --> 00:12:38,910
of the step, we were doing it parallel and another was a serial.

199
00:12:39,570 --> 00:12:42,106
It did give us some performance improvement.

200
00:12:42,298 --> 00:12:46,126
Another way was doing it using the parallel iterators where we use the rayon

201
00:12:46,158 --> 00:12:49,602
library. And what we are doing is using the

202
00:12:49,656 --> 00:12:52,958
just concurrent data structure dash map,

203
00:12:53,054 --> 00:12:56,606
which allows us to automatically

204
00:12:56,638 --> 00:13:00,326
do both the stages to check whether the particular

205
00:13:00,428 --> 00:13:03,766
entry in the dash map is empty or

206
00:13:03,788 --> 00:13:07,362
not. And if it is empty then you update the agent

207
00:13:07,436 --> 00:13:11,242
status there. And as you can see this

208
00:13:11,296 --> 00:13:15,420
graph, you can see how we are doing against the

209
00:13:16,750 --> 00:13:20,710
5 million population using the Mapreduce or parallel method.

210
00:13:20,790 --> 00:13:24,634
If you see the first where we have throughput 0.5 for the serial

211
00:13:24,682 --> 00:13:28,158
implementation, that increases to the double with

212
00:13:28,164 --> 00:13:31,214
the Mapreduce and almost nearby to it

213
00:13:31,252 --> 00:13:34,846
using parallel. But if we go ahead then the

214
00:13:34,868 --> 00:13:38,418
parallel and map reduce performs same.

215
00:13:38,504 --> 00:13:42,260
If we go with the higher number of scores, that is 6400.

216
00:13:46,790 --> 00:13:50,614
That's about the scaling up where we tried to scale up the population and

217
00:13:50,652 --> 00:13:53,766
did the parallel implementation. But another use

218
00:13:53,788 --> 00:13:57,158
case we had of the scaling out. As you

219
00:13:57,164 --> 00:14:00,950
see we use the 64 threads and those many cpu cores.

220
00:14:01,110 --> 00:14:04,554
If we want to scale it further, we always won't have those

221
00:14:04,592 --> 00:14:08,806
many resources available. And from the domains perspective,

222
00:14:08,998 --> 00:14:12,442
sometimes these disease specifications

223
00:14:12,506 --> 00:14:15,978
or the geography specifications can be different for the smaller unit

224
00:14:15,994 --> 00:14:19,550
of area. One city might have it differently than another

225
00:14:19,620 --> 00:14:23,730
city. That's what we did is we

226
00:14:23,800 --> 00:14:27,570
started those smaller area into the different engines. Each engine

227
00:14:27,640 --> 00:14:31,522
will have its own specifications, it will run its

228
00:14:31,576 --> 00:14:35,394
simulation and that way you

229
00:14:35,432 --> 00:14:38,726
can run it to the distributed mode which allows us

230
00:14:38,748 --> 00:14:42,690
to use the compute opensource efficiently

231
00:14:42,770 --> 00:14:46,578
and solves our domain problem as well. Another thing is these engines

232
00:14:46,594 --> 00:14:50,534
are not completely isolated from each other. Agents will

233
00:14:50,572 --> 00:14:53,994
be traveling from one city to another city or one world to another world.

234
00:14:54,112 --> 00:14:57,754
In that case that we are achieving using the kafka where

235
00:14:57,872 --> 00:15:01,082
a data of such commuters is sended over the Kafka and other

236
00:15:01,136 --> 00:15:04,702
engines will read it over from Kafka. And this has to be happened

237
00:15:04,756 --> 00:15:08,142
at the hour. If at our 24th somebody

238
00:15:08,196 --> 00:15:12,186
is traveling, it should be reaching at 25th hour or 24th

239
00:15:12,218 --> 00:15:15,958
hour depending on the condition. And that for that purpose we have the orchestrator

240
00:15:15,994 --> 00:15:19,426
which do nothing but the synchronization of the Indians to the

241
00:15:19,448 --> 00:15:23,710
particular points. And this is our distributed structure.

242
00:15:23,870 --> 00:15:27,186
And we will see how does it help us in optimizing

243
00:15:27,218 --> 00:15:31,046
throughput further. So as you see the first graph where

244
00:15:31,068 --> 00:15:34,790
we can see the number for the Mumbai and Pune against

245
00:15:34,860 --> 00:15:38,078
the serial, parallel and distributed setup.

246
00:15:38,274 --> 00:15:41,706
And you can see for

247
00:15:41,728 --> 00:15:45,574
the Mumbai or Pune it actually goes with the serial

248
00:15:45,622 --> 00:15:49,306
implementation. When we move to parallel the

249
00:15:49,328 --> 00:15:52,846
throughput has increased to almost twice. And again

250
00:15:52,868 --> 00:15:56,430
if we go to the distributed mode again it's as twice as

251
00:15:56,500 --> 00:15:59,230
the parallel implementation.

252
00:15:59,810 --> 00:16:03,178
Now that is one way of sharing out. Another way is

253
00:16:03,204 --> 00:16:07,102
like as we already saw, the Mumbai has 12 million population

254
00:16:07,246 --> 00:16:10,894
and how did we divide it into the distributed mode

255
00:16:10,942 --> 00:16:15,138
is the worldwide distribution where each engine was having

256
00:16:15,224 --> 00:16:18,726
population around half a million and there

257
00:16:18,748 --> 00:16:22,310
were 24 words. But if we can go little

258
00:16:22,380 --> 00:16:25,878
further and reduce the population per engine and increase the

259
00:16:25,884 --> 00:16:29,810
number of engines that what we tried with the 100k population,

260
00:16:29,890 --> 00:16:33,674
each engine, 100 engines which again sum up to the 10

261
00:16:33,712 --> 00:16:37,642
million which is near to the 12 million. Our throughput actually

262
00:16:37,776 --> 00:16:41,370
is the six times better as compared to the Mumbai throughput.

263
00:16:41,730 --> 00:16:45,280
And that's where these distributed helps us a lot.

264
00:16:47,810 --> 00:16:52,074
Now we saw we did changed

265
00:16:52,122 --> 00:16:55,746
our engines from 24 to 100, but it is

266
00:16:55,768 --> 00:16:59,314
not possible to manually start up the hundred of engines and

267
00:16:59,352 --> 00:17:02,194
then start up Kafka and do all of this stuff.

268
00:17:02,312 --> 00:17:06,078
We need some sort of scale optimization

269
00:17:06,254 --> 00:17:10,146
for here and some infrastructure which

270
00:17:10,248 --> 00:17:13,846
can be used at this scale. So for that we added the cloud.

271
00:17:13,948 --> 00:17:17,174
We migrated to the cloud and added cloud support to

272
00:17:17,212 --> 00:17:21,834
our application. First we containerized our aprest and

273
00:17:21,872 --> 00:17:25,638
then we used Kubernetes for the container management at the scale.

274
00:17:25,734 --> 00:17:30,890
Basically we are using the Kubernetes jobs which runs

275
00:17:31,250 --> 00:17:34,766
Kubernetes jobs for the engine and orchestrator and Kafka is

276
00:17:34,788 --> 00:17:38,570
again deployed on these cloud. We are using Helmchart

277
00:17:38,650 --> 00:17:42,254
to package the application. So anybody can just according to

278
00:17:42,292 --> 00:17:46,046
their use case change these config

279
00:17:46,078 --> 00:17:49,422
values in the helm chart, which way they want to run serial,

280
00:17:49,486 --> 00:17:52,670
parallel or distributed and how many engines, et cetera.

281
00:17:52,830 --> 00:17:56,558
And after the configuration with single command they will be able to

282
00:17:56,664 --> 00:18:00,146
start our application on the cloud and get these output

283
00:18:00,178 --> 00:18:04,182
data again. If we are having application

284
00:18:04,316 --> 00:18:08,106
running at a scale, there are so many issues you need to debug by the

285
00:18:08,128 --> 00:18:11,450
developing purpose to improve the performance.

286
00:18:11,950 --> 00:18:15,434
Most of the metrics are needed for that. We again

287
00:18:15,632 --> 00:18:19,126
take advantage of some open source tools like Elkstack,

288
00:18:19,158 --> 00:18:22,682
Prometheus and Grafana for the logging and monitoring purpose

289
00:18:22,826 --> 00:18:26,602
which helped us for debugging, logging and getting some automated

290
00:18:26,666 --> 00:18:30,446
alerts. Now we

291
00:18:30,468 --> 00:18:34,618
will move to the next part, that is Rust features. So as

292
00:18:34,644 --> 00:18:38,354
you know, our basic language is Rust and all this code, whatever we

293
00:18:38,472 --> 00:18:42,002
saw till now has been written in Rust and

294
00:18:42,136 --> 00:18:45,714
now we will see how Rust helped us to

295
00:18:45,752 --> 00:18:49,526
achieve our goals. So as you know, already. Rust is closer to

296
00:18:49,548 --> 00:18:53,382
the metal and it gives the performance pretty

297
00:18:53,436 --> 00:18:56,230
much similar to the C language.

298
00:18:56,810 --> 00:19:00,690
So that helped us a lot. No runtime and no garbage collection

299
00:19:00,770 --> 00:19:03,734
which gives us the performance.

300
00:19:03,862 --> 00:19:07,354
But the performance is similar to the C. But C

301
00:19:07,392 --> 00:19:11,834
and C Plus plus has many memory issues that Rust

302
00:19:11,882 --> 00:19:15,614
is taking care of already, and that's why it served us

303
00:19:15,652 --> 00:19:19,034
quite a lot of development time. With the fearless

304
00:19:19,082 --> 00:19:22,962
concurrency, we could easily go with the parallel implementation and

305
00:19:23,016 --> 00:19:26,500
most of the errors were coming at the compilation time.

306
00:19:27,830 --> 00:19:31,614
Whatever the issues we got larger on, there is nothing related

307
00:19:31,662 --> 00:19:35,286
to memory or something related to the

308
00:19:35,308 --> 00:19:38,582
language specific. Apart from that already

309
00:19:38,636 --> 00:19:42,626
we saw the performance and memory management helps us in productivity.

310
00:19:42,738 --> 00:19:45,970
Overall ecosystem also help us the documentation,

311
00:19:46,130 --> 00:19:49,446
the way it's written, the various grades for

312
00:19:49,468 --> 00:19:53,014
the different use cases, and the cargo as the build tool

313
00:19:53,132 --> 00:19:56,598
with its simplicity. All of us helps in

314
00:19:56,604 --> 00:20:00,574
these productivity. Most of the features areas doing

315
00:20:00,692 --> 00:20:04,346
converting into the parallel, using the rayon library,

316
00:20:04,458 --> 00:20:07,774
multi threading using Tokyo, etc. That all of us

317
00:20:07,812 --> 00:20:08,720
help us.

318
00:20:13,530 --> 00:20:17,030
This is the team which worked on the epirust

319
00:20:20,650 --> 00:20:22,020
and that's it.

