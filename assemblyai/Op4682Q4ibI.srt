1
00:00:25,570 --> 00:00:29,542
You. Hi, welcome to Comp 42 Cloud

2
00:00:29,596 --> 00:00:33,238
native. We're going to talk about service mesh to service meshes. Do you

3
00:00:33,244 --> 00:00:36,994
need a service mesh? How do you get started? We'll see demos of both Linkerd

4
00:00:37,042 --> 00:00:40,646
and istio in this process. So let's jump in. Here's the part where

5
00:00:40,668 --> 00:00:44,006
I tell you, I am definitely going to post the slides on my

6
00:00:44,028 --> 00:00:47,254
site tonight. I've been in enough talks that have done

7
00:00:47,292 --> 00:00:51,146
similar things. The slides are online, right? Right now let's

8
00:00:51,178 --> 00:00:55,200
head to robrich.org. We'll click on presentations here at the top

9
00:00:57,330 --> 00:01:01,040
and we can see service meshes. To service mesh is right there at the

10
00:01:01,570 --> 00:01:05,138
we're while we're here on robrich.org, let's click on about me and see some

11
00:01:05,144 --> 00:01:06,850
of the things that I've done recently.

12
00:01:08,630 --> 00:01:13,010
Both Docker and Microsoft have given me some awards and

13
00:01:13,160 --> 00:01:17,090
AZ give camp is really fun. AZ givecamp brings volunteer developers

14
00:01:17,170 --> 00:01:21,074
together with charities to build free software. We start building software

15
00:01:21,122 --> 00:01:24,566
Friday after work. Sunday afternoon we deliver completed software to

16
00:01:24,588 --> 00:01:28,198
the charities. Sleep is optional, caffeine provided. If you're in

17
00:01:28,204 --> 00:01:31,626
Phoenix, come join us to the next AZ give camp. Or if you'd like a

18
00:01:31,648 --> 00:01:35,082
give camp closer to you, hit me up here at the event or

19
00:01:35,136 --> 00:01:38,314
on any of the socials here and let's get a give camp

20
00:01:38,352 --> 00:01:42,442
in your neighborhood too. Some of the other things that I've done, I was

21
00:01:42,576 --> 00:01:46,142
awarded a tip of the captain's hat award by Docker last year.

22
00:01:46,196 --> 00:01:49,822
That was a lot of fun and one of the things I'm particularly proud of,

23
00:01:49,876 --> 00:01:53,386
I replied to a net Rocks podcast episode. They read my comments

24
00:01:53,418 --> 00:01:57,086
on the air and they sent me a mug. So there's my claim

25
00:01:57,118 --> 00:01:59,490
to fame, my coveted net rocks mug.

26
00:01:59,910 --> 00:02:03,250
So let's dig into service mesh to service meshes.

27
00:02:03,590 --> 00:02:07,390
We talked about this guy. So let's imagine

28
00:02:07,470 --> 00:02:11,046
that we're learning how to drive. Do you remember when

29
00:02:11,068 --> 00:02:14,166
you learned how to drive? Do you remember the fun it was to be able

30
00:02:14,188 --> 00:02:18,278
to hit that open road? You know, the excitement of going beyond

31
00:02:18,374 --> 00:02:21,862
just the current neighborhood into the next town?

32
00:02:21,926 --> 00:02:25,386
Maybe across the country perhaps? You know how

33
00:02:25,408 --> 00:02:29,318
fun it was to drive. Well, let's imagine a small town

34
00:02:29,414 --> 00:02:32,574
and yeah, we can drive anywhere. We can drive as fast as

35
00:02:32,612 --> 00:02:35,680
we want to. We can really enjoy the road.

36
00:02:36,370 --> 00:02:40,078
Well, over time the town starts to grow up,

37
00:02:40,164 --> 00:02:44,434
the traffic gets a little bit more congested, and now

38
00:02:44,472 --> 00:02:48,098
we've got traffic. So how do

39
00:02:48,104 --> 00:02:52,018
we solve traffic here in our small town? Well, I know

40
00:02:52,104 --> 00:02:55,554
it's those people coming into town that shouldn't be here so

41
00:02:55,672 --> 00:02:59,746
let's set up a traffic cop right at the edge of town. Anyone going faster

42
00:02:59,778 --> 00:03:04,274
or slower than we want to will penalize them and enforce conformity

43
00:03:04,322 --> 00:03:07,754
across our town. Now the traffic is flowing. Yeah,

44
00:03:07,792 --> 00:03:11,878
we enforced conformity, but we didn't really optimize

45
00:03:11,974 --> 00:03:15,130
travel. We optimized mediocrity.

46
00:03:17,070 --> 00:03:20,266
Really what we want to do is something like this. We want the cars to

47
00:03:20,288 --> 00:03:23,766
be able to communicate with each other, prioritize the traffic.

48
00:03:23,878 --> 00:03:27,502
Those cars that want to go fast or that need urgent access,

49
00:03:27,636 --> 00:03:30,926
they can go in one lane, and other cars that might go slower can go

50
00:03:30,948 --> 00:03:34,206
in other lanes. And we can coordinate this traffic to ensure

51
00:03:34,238 --> 00:03:37,346
that everyone reaches their destination with as much fun

52
00:03:37,448 --> 00:03:41,220
and expediency as is comfortable for their system.

53
00:03:41,910 --> 00:03:45,246
Yeah, if we could prioritize the traffic and communicate

54
00:03:45,278 --> 00:03:48,046
together, we wouldn't have to aim for mediocrity.

55
00:03:48,158 --> 00:03:52,200
We could excel at defining the system.

56
00:03:52,810 --> 00:03:56,406
Yeah, we'll take a similar analogy as we start to

57
00:03:56,428 --> 00:03:59,634
look at service mesh. Do we just want to aim for a conformity,

58
00:03:59,762 --> 00:04:03,106
or do we want to do something? Excellent. So we'll

59
00:04:03,138 --> 00:04:06,090
take a look at what is a service mesh? Why would I use it?

60
00:04:06,160 --> 00:04:09,482
How do I get started? What are the benefits of it? We'll see a demo

61
00:04:09,536 --> 00:04:12,030
of both istio and linkerd in this process.

62
00:04:12,180 --> 00:04:15,070
And finally, we'll talk about best practices.

63
00:04:15,730 --> 00:04:19,354
First up, a service mesh. A service mesh manages traffic

64
00:04:19,402 --> 00:04:22,318
between services in a graceful and scalable way.

65
00:04:22,484 --> 00:04:25,890
Or, said differently, a service mesh is the answer to the question,

66
00:04:26,040 --> 00:04:29,714
how do I observe, control, and secure the

67
00:04:29,752 --> 00:04:33,026
communication between my microservices? Now, if you have the

68
00:04:33,048 --> 00:04:36,910
need to observe, control, or secure traffic between your microservices,

69
00:04:36,990 --> 00:04:40,966
service mesh may be a great solution. Maybe if you just have one,

70
00:04:41,068 --> 00:04:44,406
it might be a little bit overkill. Let's dive into each

71
00:04:44,428 --> 00:04:48,214
of those. Observe well, we want to be able to watch the

72
00:04:48,252 --> 00:04:51,402
traffic flowing between our containers in

73
00:04:51,456 --> 00:04:55,050
our Kubernetes cluster and get a feel for how they behave.

74
00:04:55,470 --> 00:04:58,886
Are we getting microservices calling into places that they shouldn't?

75
00:04:58,998 --> 00:05:01,840
Are we getting road traffic coming through our system?

76
00:05:02,290 --> 00:05:05,866
Are services online? Are they behaving as expected?

77
00:05:05,978 --> 00:05:09,486
These are all things that we can observe as we get a service mesh in

78
00:05:09,508 --> 00:05:12,494
place. Next, let's upgrade to control.

79
00:05:12,692 --> 00:05:16,026
Let's create policies within our cluster that

80
00:05:16,068 --> 00:05:19,886
says this service can speak to this service, this service can accept traffic

81
00:05:19,918 --> 00:05:23,534
from this service. But all this other traffic that we really don't

82
00:05:23,582 --> 00:05:27,566
understand, we're just going to shut it down. We don't want rogue services

83
00:05:27,688 --> 00:05:30,854
calling into our project just because they happen to start

84
00:05:30,892 --> 00:05:34,594
up a pod there. Now, we do need to work carefully with developers

85
00:05:34,642 --> 00:05:37,858
to ensure the applications work as designed,

86
00:05:37,954 --> 00:05:41,514
but we can also stop rogue applications that happen to pop up

87
00:05:41,552 --> 00:05:44,666
within our cluster. They just can't get

88
00:05:44,688 --> 00:05:47,898
to our services. We've walled off the services to

89
00:05:47,984 --> 00:05:51,470
match the needs of those particular applications.

90
00:05:51,970 --> 00:05:55,406
Next, we can secure. Now, the beauty of securing our

91
00:05:55,428 --> 00:05:59,166
applications is by default, within Kubernetes, all of our

92
00:05:59,188 --> 00:06:03,510
services communicate over HTTP unencrypted.

93
00:06:03,690 --> 00:06:08,594
Now, maybe they're doing GRPC or other

94
00:06:08,632 --> 00:06:12,590
forms of communication rest graphql

95
00:06:12,670 --> 00:06:16,426
but at the end of the day they're doing HTTP and they're doing it unencrypted.

96
00:06:16,558 --> 00:06:20,082
Well, if we have the need to encrypt traffic within our cluster,

97
00:06:20,226 --> 00:06:24,230
we can use service mesh's mutual tls to be able to create

98
00:06:24,300 --> 00:06:28,454
encrypted tunnels where services communicating between each other can

99
00:06:28,492 --> 00:06:31,946
hit each other through secure tunnels without needing to

100
00:06:31,968 --> 00:06:35,846
change our application. Now, back in the day when we had monoliths,

101
00:06:35,878 --> 00:06:39,830
it was really easy. We deployed all of the pieces of our application holistically

102
00:06:39,910 --> 00:06:43,854
together. As containers came about, we were able to split our

103
00:06:43,892 --> 00:06:47,502
application into lots of different services. Now, we love this

104
00:06:47,556 --> 00:06:51,402
because now we can deploy little pieces, scale them independently,

105
00:06:51,546 --> 00:06:54,814
replace them independently, maybe even develop them independently.

106
00:06:54,942 --> 00:06:58,706
We can build and deploy and scale our services much easier than

107
00:06:58,728 --> 00:07:02,594
we could in a monolithic system. But now

108
00:07:02,632 --> 00:07:07,106
our application's internal pieces have IP addresses.

109
00:07:07,298 --> 00:07:11,046
And so right now, microservices own their own

110
00:07:11,068 --> 00:07:14,866
data, and we've contained that mechanism.

111
00:07:14,978 --> 00:07:18,330
The user interface is able to call the microservices that they need to,

112
00:07:18,400 --> 00:07:22,374
and everything is fine. As we talk about traffic

113
00:07:22,422 --> 00:07:26,182
within our cluster, we'll talk about both north south traffic

114
00:07:26,246 --> 00:07:29,686
and east west traffic. North south

115
00:07:29,718 --> 00:07:32,634
traffic is traffic flowing into or out of our cluster.

116
00:07:32,762 --> 00:07:37,274
By comparison, east west traffic is traffic flowing between our microservices

117
00:07:37,402 --> 00:07:40,526
inside of our cluster. And the beauty here is that a

118
00:07:40,548 --> 00:07:44,526
service mesh can secure both. Well, what came before us,

119
00:07:44,628 --> 00:07:47,614
well, back in the day, we had an API gateway. We could think of this

120
00:07:47,652 --> 00:07:51,202
as like a fence around our cluster. Now that's great.

121
00:07:51,256 --> 00:07:53,746
We had a traffic cop at the edge of town, and we were making sure

122
00:07:53,768 --> 00:07:57,942
that anyone that came into town was behaving as expected. But what about the people

123
00:07:57,996 --> 00:08:01,670
who are already in town? What about the traffic already

124
00:08:01,740 --> 00:08:05,314
in our cluster? We can see that the API gateway

125
00:08:05,362 --> 00:08:08,670
has no visibility into microservices calling each other's

126
00:08:08,690 --> 00:08:12,266
data stores, or microservices calling other microservices that

127
00:08:12,288 --> 00:08:15,382
it shouldn't. The API gateway is merely a fence

128
00:08:15,446 --> 00:08:18,470
around our system. Now, it's a great fence.

129
00:08:18,550 --> 00:08:22,480
We can use it for monitoring inbound traffic. We can use it for

130
00:08:23,490 --> 00:08:26,938
counting usage and billing back to those systems

131
00:08:26,954 --> 00:08:30,382
that need it. But it can only see

132
00:08:30,436 --> 00:08:34,162
traffic at the boundary of our cluster. It can't see traffic within

133
00:08:34,216 --> 00:08:38,174
our cluster. It can see north south traffic, it can't see east west

134
00:08:38,222 --> 00:08:40,580
traffic. So now what?

135
00:08:41,030 --> 00:08:44,420
Well, let's take a look at how service mesh works.

136
00:08:44,950 --> 00:08:48,678
Now, what's really cool is if service a needs to call service b without

137
00:08:48,764 --> 00:08:52,278
a service mesh, it just calls it. But if service a needs to

138
00:08:52,284 --> 00:08:55,622
call service b within a service mesh, it works like this.

139
00:08:55,756 --> 00:08:59,410
We start out with service a INSIde of its own pod,

140
00:08:59,490 --> 00:09:02,986
reaching out to this proxy. Now, this proxy was deployed as part

141
00:09:03,008 --> 00:09:06,826
of this pod to ensure that service a can communicate securely with all the

142
00:09:06,848 --> 00:09:10,374
things. This proxy reaches out to the service mesh

143
00:09:10,422 --> 00:09:14,074
control plane, different from the cluster control plane, and the service mesh

144
00:09:14,122 --> 00:09:17,534
control plane can validate that traffic. Am I allowed to talk to

145
00:09:17,572 --> 00:09:20,910
service B? In this case, the service mesh says yes.

146
00:09:21,060 --> 00:09:24,942
Now, this proxy connects to service B's proxy, and service

147
00:09:24,996 --> 00:09:28,306
B's proxy again reaches out to the service mesh. Am I allowed to

148
00:09:28,328 --> 00:09:31,522
accept traffic from service A? In this case, the service mesh says

149
00:09:31,576 --> 00:09:35,074
yes, and the traffic is forwarded on to service B. Service B

150
00:09:35,112 --> 00:09:39,398
replies, and across that proxy, the response goes to Service A.

151
00:09:39,564 --> 00:09:43,634
Now, the beauty here is inside the pod, all the traffic

152
00:09:43,682 --> 00:09:47,750
can communicate between the service and its proxy just across

153
00:09:47,820 --> 00:09:51,718
localhost. But anytime it leaves that pod boundary,

154
00:09:51,814 --> 00:09:55,546
it's going to run through this proxy connection. And the

155
00:09:55,568 --> 00:09:59,706
beauty here is that we can secure this connection with mutual tls. So this

156
00:09:59,728 --> 00:10:03,806
side has a certificate, that side has a certificate. It's bound to

157
00:10:03,828 --> 00:10:07,914
the trust chain within the service mesh. And now we have a great communication

158
00:10:07,962 --> 00:10:11,818
pattern that is secure anytime traffic leaves the pod.

159
00:10:11,994 --> 00:10:15,762
And we did all that without needing to modify service A or

160
00:10:15,816 --> 00:10:19,298
service B. Service A to the proxy proxy, to the

161
00:10:19,304 --> 00:10:22,926
service mesh, the service mesh says yes, let's create a mutual tls

162
00:10:22,958 --> 00:10:27,130
tunnel. Service B's proxy reach out to the mesh, service B's proxy

163
00:10:27,230 --> 00:10:31,126
forwards it off to service B. And all this happens transparently to

164
00:10:31,148 --> 00:10:35,334
the two services who just communicate with whatever the service mesh needs

165
00:10:35,372 --> 00:10:39,094
to tell them about. Now, that's great. We could also replace

166
00:10:39,142 --> 00:10:43,226
this with ingress, or replace this with egress so

167
00:10:43,248 --> 00:10:46,922
that traffic going into or out of our cluster is also

168
00:10:47,056 --> 00:10:50,906
mutually tls and validated with the service mesh.

169
00:10:51,098 --> 00:10:54,782
So service meshes we can observe, control and

170
00:10:54,836 --> 00:10:58,234
secure the traffic going through our cluster. Because we're

171
00:10:58,282 --> 00:11:01,818
proxying all the traffic through these envoy proxies.

172
00:11:01,994 --> 00:11:05,554
Now that's great. Now that we can visualize, now that

173
00:11:05,592 --> 00:11:09,090
all the traffic is flowing between these proxies, we can observe it,

174
00:11:09,160 --> 00:11:12,802
we can visualize it, we can understand its system,

175
00:11:12,936 --> 00:11:16,466
we can also control it. No service a is not allowed to

176
00:11:16,488 --> 00:11:20,278
connect to Service B, or rogue service X is not allowed to connect to

177
00:11:20,284 --> 00:11:24,082
service B. And then finally we can secure it with mutual tls,

178
00:11:24,226 --> 00:11:27,526
mutual tls through a trust chain to the

179
00:11:27,628 --> 00:11:31,654
service mesh that may also then have a trust chain into our PKI

180
00:11:31,702 --> 00:11:34,742
system. Now it's more than just a proxy.

181
00:11:34,806 --> 00:11:38,394
Let's take a look at the other features that the service mesh might give us.

182
00:11:38,592 --> 00:11:42,106
Because all the traffic is flowing through this proxy, we can

183
00:11:42,128 --> 00:11:45,806
start to build a network topology. Now what's interesting here, this is

184
00:11:45,828 --> 00:11:49,594
not the way the architect designed the system, but what we've observed

185
00:11:49,642 --> 00:11:53,338
from actual traffic flowing through the system. We can build these graphs

186
00:11:53,354 --> 00:11:56,654
that will have really impactful, meaningful details.

187
00:11:56,782 --> 00:12:00,306
Let's compare it to the architect's version and see if maybe we

188
00:12:00,328 --> 00:12:04,180
didn't deploy all the pieces, or maybe we accidentally turned off a service

189
00:12:04,790 --> 00:12:08,198
with a service flag. Next we can take a

190
00:12:08,204 --> 00:12:11,426
look at service health. Now the beauty here with monitoring

191
00:12:11,458 --> 00:12:15,254
service health is that we can capture 500 or high

192
00:12:15,292 --> 00:12:19,762
latency things and start to report that back to the controllers.

193
00:12:19,906 --> 00:12:23,354
Now here we can take a look at the traffic flowing through our

194
00:12:23,392 --> 00:12:26,874
cluster. We can compare it to known good things. We can

195
00:12:26,912 --> 00:12:30,714
understand when our cluster is starting to misbehave. This is perfect.

196
00:12:30,912 --> 00:12:34,206
And we can also log let's log all the traffic between all

197
00:12:34,228 --> 00:12:38,074
the services. Let's log the HTTP status code, the results,

198
00:12:38,202 --> 00:12:41,374
log the call chains between the services.

199
00:12:41,492 --> 00:12:45,078
We have a really great mechanism of being able to capture

200
00:12:45,194 --> 00:12:48,210
the network traffic going between these systems.

201
00:12:48,710 --> 00:12:51,986
Let's level up again and take a look at additional features that

202
00:12:52,008 --> 00:12:55,730
a service mesh can bring us. We can do a b testing

203
00:12:56,230 --> 00:13:00,338
now because we're routing through this envoy proxy. We could redirect

204
00:13:00,434 --> 00:13:03,634
the service mesh, could redirect it to two different versions.

205
00:13:03,762 --> 00:13:07,062
Let's create a version a and a version b and see how they perform,

206
00:13:07,196 --> 00:13:10,120
and then lean into the one that performs best.

207
00:13:10,570 --> 00:13:14,346
We can also create a beta channel. Let's create a new

208
00:13:14,448 --> 00:13:17,962
version of our software that maybe we don't have as much confidence in,

209
00:13:18,016 --> 00:13:21,754
or maybe has advanced features that we want to get early feedback on

210
00:13:21,872 --> 00:13:26,010
and enroll certain users in that beta channel or canary release.

211
00:13:26,170 --> 00:13:29,406
Once we validate that the system works as expected, now we

212
00:13:29,428 --> 00:13:32,894
can roll it out to the rest of the users as well. Some users may

213
00:13:32,932 --> 00:13:36,274
really enjoy being part of that early feedback cycle and get

214
00:13:36,312 --> 00:13:38,580
access to features as soon as they're available,

215
00:13:39,430 --> 00:13:42,738
and we can create circuit breakers. If a service

216
00:13:42,824 --> 00:13:46,766
becomes overloaded, it's really easy for us to accidentally topple

217
00:13:46,878 --> 00:13:50,578
over that service. Well, all of the clients noticing that they didn't

218
00:13:50,594 --> 00:13:54,306
get a response and presuming that it's just intermittent network

219
00:13:54,338 --> 00:13:57,400
traffic might say, well, let me just retry it.

220
00:13:57,770 --> 00:14:01,594
As soon as the service comes back online, it gets overwhelmed with all

221
00:14:01,632 --> 00:14:05,206
of the requests coming in from all of those services that are retrying

222
00:14:05,238 --> 00:14:07,580
and promptly falls over again.

223
00:14:08,430 --> 00:14:11,674
So we can put in a circuit breaker that says, hey, things,

224
00:14:11,712 --> 00:14:15,262
service is not doing well, I'm just going to fail all these requests right now

225
00:14:15,316 --> 00:14:19,006
and let the service start back up gently, reach a

226
00:14:19,028 --> 00:14:23,306
healthy state. Now we'll send in a little traffic, and unlike the circuit breakers

227
00:14:23,338 --> 00:14:27,522
in our house, the machinery can automatically turn this back on once

228
00:14:27,576 --> 00:14:31,694
the service is healthy. These are features that we get out of a service mesh

229
00:14:31,742 --> 00:14:35,380
because we're proxying all the traffic between all of our services

230
00:14:35,850 --> 00:14:39,302
within the Kubernetes cluster. We also get some

231
00:14:39,356 --> 00:14:43,480
really great dashboards that allow us to visualize the traffic and

232
00:14:44,250 --> 00:14:46,600
understand the health of our system.

233
00:14:47,450 --> 00:14:51,578
Yeah, we started out with a service where everything

234
00:14:51,664 --> 00:14:55,766
was calling everything, and we really don't like that mechanism. We grabbed

235
00:14:55,798 --> 00:14:59,414
a service mesh to be able to control, observe and secure

236
00:14:59,462 --> 00:15:03,306
the traffic within our cluster to ensure that our microservices

237
00:15:03,418 --> 00:15:07,306
are calling the appropriate endpoints and rogue microservices aren't

238
00:15:07,338 --> 00:15:11,038
able to exfiltrate data from our system. Let's take a

239
00:15:11,044 --> 00:15:13,994
look at some service meshes. Now, as we look at service meshes,

240
00:15:14,042 --> 00:15:17,074
we'll compare quite a few examples of this. Now,

241
00:15:17,112 --> 00:15:20,430
service meshes are getting built really fast right now, and their features

242
00:15:20,510 --> 00:15:23,806
are evolving quickly. So we're not going to compare feature sets,

243
00:15:23,838 --> 00:15:27,858
but rather methodologies of these systems. We'll look in particular

244
00:15:27,944 --> 00:15:31,718
at Istio and Linkerd, but there's many more service meshes that you

245
00:15:31,724 --> 00:15:35,126
may choose from. For the longest time, Linkerd was

246
00:15:35,148 --> 00:15:37,970
the only one in CNCF, and so it became quite popular.

247
00:15:38,050 --> 00:15:41,542
Istio was amazingly popular, but had some governance

248
00:15:41,606 --> 00:15:44,986
restrictions that are now no longer the case. So do

249
00:15:45,008 --> 00:15:48,666
you need Istio or Linkerd? Those are good places to start, and as

250
00:15:48,688 --> 00:15:52,160
you search for those, you may find others that best match your needs.

251
00:15:52,690 --> 00:15:56,378
First up, Linkerd. Now, Linkerd's methodology

252
00:15:56,474 --> 00:15:59,710
is very simple install and really

253
00:15:59,780 --> 00:16:03,214
easy to use. They focus on having everything that

254
00:16:03,252 --> 00:16:05,520
you need to get started in the box.

255
00:16:06,130 --> 00:16:09,666
Now, that's great. You can get started really easily, but it does mean

256
00:16:09,688 --> 00:16:13,422
that if you want to stray beyond their initial set of features,

257
00:16:13,486 --> 00:16:17,330
that you'll probably need to look to third parties to be able to augment Linkerd.

258
00:16:17,490 --> 00:16:21,074
Linkerd is great at contributing back to the rust

259
00:16:21,122 --> 00:16:24,294
community, so a lot of the rust networking stack was actually

260
00:16:24,332 --> 00:16:26,870
built to facilitate Linkerd.

261
00:16:27,370 --> 00:16:30,654
Next up, istio. Now, istio's methodology

262
00:16:30,722 --> 00:16:34,442
is very different. It tries to include the best of open

263
00:16:34,496 --> 00:16:37,946
source projects to ensure that you have all of the features that you

264
00:16:37,968 --> 00:16:41,562
need. Then you can turn on and off features based on

265
00:16:41,616 --> 00:16:44,766
profiles or based on just turning features on and off, and then

266
00:16:44,788 --> 00:16:48,686
you can tune Istio to be exactly the thing that you need.

267
00:16:48,868 --> 00:16:52,238
Now we'll dig in deep with Istio's virtual services to be

268
00:16:52,244 --> 00:16:56,014
able to see how we might choose to host some traffic in one service

269
00:16:56,132 --> 00:16:59,986
and some traffic in another service. An A B test and this is

270
00:17:00,008 --> 00:17:03,410
a feature of all service meshes, but we'll get to see it here in istio.

271
00:17:03,910 --> 00:17:07,526
So let's take a look at these. First up,

272
00:17:07,628 --> 00:17:11,574
let's fire up Linkerd. Oh, let's not

273
00:17:11,612 --> 00:17:15,826
fire up that one, let's fire up this one. Let's use Linkerd.

274
00:17:16,018 --> 00:17:19,894
And what Linkerd focuses on is a really elegant and smooth

275
00:17:19,942 --> 00:17:23,174
install experience. So let's head off to the Linkerd

276
00:17:23,222 --> 00:17:27,382
docs and take a look at getting started. Well, I start off by downloading

277
00:17:27,446 --> 00:17:31,354
the Linkerd Cli and then I can

278
00:17:31,392 --> 00:17:34,526
do a Linkerd check pre says I know

279
00:17:34,548 --> 00:17:38,462
that Linkerd isn't installed yet, but let's just validate it that it's there.

280
00:17:38,596 --> 00:17:43,506
Then I'll install the Crds, then I'll install Linkerd and

281
00:17:43,528 --> 00:17:46,610
then I can run Linkerd check. I've already done these just to

282
00:17:46,680 --> 00:17:50,434
speed up this presentation. Next we could take a look at

283
00:17:50,552 --> 00:17:54,046
the dashboards. So we'll

284
00:17:54,078 --> 00:17:57,270
say Linkerd viz install and we'll install that.

285
00:17:57,340 --> 00:18:00,694
I've already done it, but let's do it again just in case.

286
00:18:00,892 --> 00:18:04,614
Let's get Linkerd installed and then next up we can

287
00:18:04,652 --> 00:18:07,240
check to see if Linkerd is running.

288
00:18:07,770 --> 00:18:10,954
So Linkerd check. And what I like about this is that

289
00:18:10,992 --> 00:18:14,826
not only will it validate that Linkerd is running, but it'll also wait for

290
00:18:14,848 --> 00:18:18,186
it if it isn't. So let's double check that the

291
00:18:18,208 --> 00:18:21,662
viz extension is in place and once we get the green

292
00:18:21,716 --> 00:18:24,560
light now we know that Linkerd is ready to go.

293
00:18:25,090 --> 00:18:28,970
Now Linkerd will augment namespaces

294
00:18:29,050 --> 00:18:32,634
to be able to show which namespaces should have that sidecar

295
00:18:32,682 --> 00:18:36,514
applied. So we can see the labels here in our default namespace says

296
00:18:36,552 --> 00:18:40,574
Linkerd inject enabled.

297
00:18:40,702 --> 00:18:44,434
And so now anything that we start in the default namespace will get that

298
00:18:44,472 --> 00:18:48,534
sidecar applied. Well, let's take a look at the

299
00:18:48,732 --> 00:18:50,706
dashboard inside Linkerd.

300
00:18:50,818 --> 00:18:53,830
Linkerd dashboard.

301
00:18:54,650 --> 00:18:58,118
And now we've started the built in dashboard for Linkerd,

302
00:18:58,214 --> 00:19:02,058
we can take a look at the various namespaces in our system. Take a

303
00:19:02,064 --> 00:19:06,214
look at the automatic discovery

304
00:19:06,262 --> 00:19:09,894
of the service integration because we've got them injected

305
00:19:09,942 --> 00:19:13,430
through Linkerd. Yep, Linkerd is running for Linkerd.

306
00:19:13,590 --> 00:19:17,054
And then we can take a look at all the deployments and the

307
00:19:17,092 --> 00:19:20,446
health of those services. Picking a particular service.

308
00:19:20,548 --> 00:19:23,806
We can take a look at the details of that service. Well, it looks like

309
00:19:23,828 --> 00:19:27,886
we're up 100% of the time now, and we have the references of what calls

310
00:19:27,918 --> 00:19:31,826
what on the way past. That's really elegant. Now if we

311
00:19:31,848 --> 00:19:34,594
don't want to view it through an API, we can definitely do it a different

312
00:19:34,632 --> 00:19:38,302
way as well. Linkerd stat.

313
00:19:38,446 --> 00:19:41,766
And I'll take a look at the Linkerd namespace and take

314
00:19:41,788 --> 00:19:45,350
a look at deployments. Here's that same output from the command line.

315
00:19:45,420 --> 00:19:49,058
And I could also grab it from Prometheus syncs and pipe

316
00:19:49,074 --> 00:19:51,900
it off to Grafana or splunk or another system.

317
00:19:52,350 --> 00:19:55,770
So that was great to be able to take a look at Linkerd. The install

318
00:19:55,840 --> 00:20:00,134
experience is super fast and allows us to get going really easily.

319
00:20:00,262 --> 00:20:03,466
It does kind of have a bare bone system. They want everything in the

320
00:20:03,488 --> 00:20:06,494
box, so we may need to reach out to others if we want to go

321
00:20:06,532 --> 00:20:09,710
farther. Next, let's take a look at istio.

322
00:20:10,050 --> 00:20:14,050
Now with istio we have a similar setup of getting started.

323
00:20:14,200 --> 00:20:17,858
We can start by downloading the

324
00:20:17,944 --> 00:20:21,346
istio CLI and then once we've got

325
00:20:21,368 --> 00:20:25,038
that in our path, we'll install istio picking the profile that

326
00:20:25,064 --> 00:20:28,934
we want. In this case we'll use demo, which turns everything on.

327
00:20:29,132 --> 00:20:33,506
Next we can enable that namespace

328
00:20:33,538 --> 00:20:37,506
injection. So let's take a look at the namespace

329
00:20:37,618 --> 00:20:41,194
and we can see that we've got istio set

330
00:20:41,232 --> 00:20:44,490
up to be able to automatically inject the

331
00:20:44,560 --> 00:20:48,406
sidecar into each of the pods launched in this namespace.

332
00:20:48,598 --> 00:20:52,366
Next we can launch a sample application. Now this sample application is

333
00:20:52,388 --> 00:20:55,994
a really good way to look at istio and Istio's virtual

334
00:20:56,042 --> 00:20:59,582
routing. So we have an ingress that

335
00:20:59,636 --> 00:21:03,690
might route to a product page. Our product page shows some details and

336
00:21:03,700 --> 00:21:07,060
then also gets reviews. We have three different review services.

337
00:21:07,430 --> 00:21:10,878
Now, we can think of this as developing the various reviews,

338
00:21:10,974 --> 00:21:14,414
and we'll look at the automatic upgrades

339
00:21:14,462 --> 00:21:18,118
through those processes. Now, you probably wouldn't run all three at the same time,

340
00:21:18,204 --> 00:21:21,426
but we're going to do that for this demo. And version

341
00:21:21,458 --> 00:21:25,126
two and version three show stars reaching into another service.

342
00:21:25,308 --> 00:21:28,726
Each of these gray boxes is an envoy proxy that allows us

343
00:21:28,748 --> 00:21:31,900
to be able to virtually route traffic as we need to.

344
00:21:32,270 --> 00:21:36,042
So here's our bookstore app, and right now we're going equally between

345
00:21:36,096 --> 00:21:39,638
the three systems. So you'll see, sometimes I have no stars.

346
00:21:39,734 --> 00:21:44,126
Sometimes I have stars in black color. Sometimes I have stars in red color.

347
00:21:44,308 --> 00:21:47,146
This is great to be able to show the various versions.

348
00:21:47,258 --> 00:21:51,102
Version one has no stars. Version two has stars in black

349
00:21:51,156 --> 00:21:54,530
color, and version three has stars in red color.

350
00:21:54,680 --> 00:21:58,366
How did we get that? Well, here's that service that allows

351
00:21:58,398 --> 00:22:02,274
us to be able to look at all three. It has

352
00:22:02,312 --> 00:22:06,222
this virtual service that routes traffic evenly

353
00:22:06,286 --> 00:22:10,134
between them. Well, almost. So now that we've got

354
00:22:10,172 --> 00:22:13,654
traffic flowing evenly between them, let's take a look at an

355
00:22:13,692 --> 00:22:17,202
upgrade cycle of how we might use Istio to be able to route traffic

356
00:22:17,266 --> 00:22:21,142
without downtime, taking advantage of av channels canary

357
00:22:21,206 --> 00:22:25,338
deploys. Let's start by sending it all to version one.

358
00:22:25,504 --> 00:22:30,202
So let's cubectl apply f

359
00:22:30,336 --> 00:22:33,680
virtual service reviews. We'll do v one.

360
00:22:34,530 --> 00:22:38,286
And now all of our traffic will go to version one.

361
00:22:38,468 --> 00:22:42,094
We'll see that we now have no stars no matter how many times we

362
00:22:42,132 --> 00:22:45,714
refresh it. Excellent. Now we

363
00:22:45,752 --> 00:22:49,154
want to start routing traffic to version two, but we only

364
00:22:49,192 --> 00:22:52,078
want to grab, say, 20% of the traffic.

365
00:22:52,174 --> 00:22:55,702
Let's make sure that version two works as expected. Okay,

366
00:22:55,836 --> 00:22:59,014
so let's go grab this one and we'll apply this

367
00:22:59,052 --> 00:23:03,110
rule. Cubectl apply f

368
00:23:03,260 --> 00:23:06,962
that one. Now, 80% of the time we'll get no stars.

369
00:23:07,026 --> 00:23:10,540
And 20% of the time we'll get stars in black color.

370
00:23:11,150 --> 00:23:14,646
Yeah, it looks like that was working as expected.

371
00:23:14,758 --> 00:23:18,186
My system is behaving. So let's flip over to

372
00:23:18,208 --> 00:23:20,460
go completely to version two.

373
00:23:20,910 --> 00:23:24,222
Okay, here's version two. And now

374
00:23:24,276 --> 00:23:27,966
with version two, we have 100% of the traffic going to

375
00:23:27,988 --> 00:23:31,498
version two. We were able to migrate without downtime,

376
00:23:31,594 --> 00:23:34,938
giving some users access to the early features. Well,

377
00:23:34,964 --> 00:23:38,078
let's take that a little further and let's create a canary release.

378
00:23:38,254 --> 00:23:41,458
Well, here we want to say if the user is

379
00:23:41,544 --> 00:23:45,210
json, then we'll give them version three. Otherwise we'll

380
00:23:45,230 --> 00:23:49,042
give them the original version, version two. Okay, so let's

381
00:23:49,106 --> 00:23:52,438
Cubectl apply f

382
00:23:52,604 --> 00:23:57,110
virtual service reviews

383
00:23:57,790 --> 00:24:01,530
json up typos.

384
00:24:02,350 --> 00:24:05,658
Let's try that again. There we go.

385
00:24:05,824 --> 00:24:08,874
Oh, cubectl apply

386
00:24:09,072 --> 00:24:12,080
f. That one.

387
00:24:13,090 --> 00:24:16,560
Nice. Now that we've got that one in place,

388
00:24:17,330 --> 00:24:21,006
let's refresh our app and we'll see that most of

389
00:24:21,028 --> 00:24:24,858
the time, while unauthenticated,

390
00:24:24,954 --> 00:24:28,034
we get version two. Stars in black color. Well, let's sign

391
00:24:28,072 --> 00:24:31,442
in to the canary release. I'll log in as

392
00:24:31,496 --> 00:24:35,220
Jason. And now we can see that we get version three.

393
00:24:36,310 --> 00:24:39,750
Jason is really excited for these new features. It looks like it's working

394
00:24:39,820 --> 00:24:43,186
well. And if we log back out, we'll see that we get back to version

395
00:24:43,218 --> 00:24:46,566
two consistently. Our regular users are not impacted by

396
00:24:46,588 --> 00:24:50,374
this test. Now that we've gotten version

397
00:24:50,422 --> 00:24:54,458
three ready to go, let's flip over exclusively to version three.

398
00:24:54,624 --> 00:24:58,506
Cubectl apply f

399
00:24:58,608 --> 00:25:02,400
virtual service reviews v three.

400
00:25:03,090 --> 00:25:06,670
And now that we're exclusively in version three, we always

401
00:25:06,740 --> 00:25:10,894
have the stars in red color. Now, we were able to upgrade through

402
00:25:10,932 --> 00:25:14,530
these versions with no downtime. That's excellent.

403
00:25:14,950 --> 00:25:18,242
Let's flip back to the one that is 33

404
00:25:18,296 --> 00:25:21,700
and 33 for our next demo.

405
00:25:22,390 --> 00:25:26,150
And so now we can see that we have all three of the stars.

406
00:25:26,810 --> 00:25:29,320
Version one, version three.

407
00:25:30,250 --> 00:25:34,290
Version two. Now let's take a look at the istio

408
00:25:34,370 --> 00:25:35,510
dashboards.

409
00:25:36,170 --> 00:25:41,690
Istio dashboard Prometheus.

410
00:25:42,430 --> 00:25:45,962
The Prometheus dashboard is great at being able to look

411
00:25:46,016 --> 00:25:49,740
deep into the istio system. So let's look for

412
00:25:50,130 --> 00:25:53,854
istio requests total,

413
00:25:54,052 --> 00:25:57,386
and we can see those Prometheus metrics flowing

414
00:25:57,418 --> 00:26:00,782
in. Now, that may not be the best way to visualize it. So instead

415
00:26:00,836 --> 00:26:04,402
of visualizing it through Prometheus, let's visualize it

416
00:26:04,456 --> 00:26:06,866
through Grafana. Now,

417
00:26:06,888 --> 00:26:11,070
Grafana is an industry standard dashboard. And with istio,

418
00:26:11,150 --> 00:26:14,274
you get some grafana dashboards. So let's take a look

419
00:26:14,312 --> 00:26:18,386
at the istio control plane dashboard. We can see all kinds

420
00:26:18,418 --> 00:26:21,814
of interesting metrics associated with our cluster and

421
00:26:21,852 --> 00:26:25,510
the various traffic within it. That looks pretty neat.

422
00:26:26,010 --> 00:26:30,026
Let's dig into the next dashboard that comes built in with istio that

423
00:26:30,048 --> 00:26:33,654
we might choose to enable. I'm going to use Jaeger.

424
00:26:33,782 --> 00:26:37,206
Now, Yeager is really great for open telemetry. It allows

425
00:26:37,238 --> 00:26:40,410
us to be able to grab traces between our system.

426
00:26:40,560 --> 00:26:44,666
So let's take a look at this one. We'll take a look at traces,

427
00:26:44,778 --> 00:26:49,022
and we can see the various calls to this system.

428
00:26:49,156 --> 00:26:52,302
Ooh, this one looks interesting. Let's pop open this one.

429
00:26:52,436 --> 00:26:56,334
We can see the request came into the istio ingress gateway.

430
00:26:56,462 --> 00:27:00,478
It was forwarded off to the product page microservice.

431
00:27:00,574 --> 00:27:04,926
The product page microservice called the details page and it ran

432
00:27:04,958 --> 00:27:09,062
for this long. It also called the product reviews service.

433
00:27:09,196 --> 00:27:12,278
Now we can see the details service didn't run very long,

434
00:27:12,364 --> 00:27:15,830
but the reviews service ran a little bit longer and

435
00:27:15,900 --> 00:27:19,366
the product page did a whole lot of processing after that. So if

436
00:27:19,388 --> 00:27:22,778
we were to optimize this system, working on the

437
00:27:22,784 --> 00:27:26,074
details page is probably not going to optimize our use

438
00:27:26,112 --> 00:27:29,738
case. Now, it's great to be able to then dig into each of those things

439
00:27:29,824 --> 00:27:33,482
and understand those distributed traces so that we have context

440
00:27:33,546 --> 00:27:36,990
across our system. Next dashboard that we look at.

441
00:27:37,060 --> 00:27:39,854
Let's take a look at Kiali. Now,

442
00:27:39,892 --> 00:27:43,694
Kiali is great for visualizing who calls what. We'll log

443
00:27:43,732 --> 00:27:47,122
into Kiali. We'll take a look at the graphs and we'll change

444
00:27:47,176 --> 00:27:50,766
this from 1 minute to 30 minutes to take a look at the calls

445
00:27:50,798 --> 00:27:54,418
through our system. Now what's beautiful here is that we get

446
00:27:54,504 --> 00:27:58,162
a network diagram of our system. We called the product

447
00:27:58,216 --> 00:28:01,382
page and it called the details page V one. We also

448
00:28:01,436 --> 00:28:04,998
called the product page that called the review system. And over the

449
00:28:05,004 --> 00:28:08,082
course of our experience, we ended up with all three versions

450
00:28:08,146 --> 00:28:11,420
getting called. We saw two and three called the rating service.

451
00:28:11,870 --> 00:28:15,322
Now what's interesting here is this is what's actually happening within

452
00:28:15,376 --> 00:28:18,886
our system. That's great, but what if we notice

453
00:28:18,918 --> 00:28:22,422
that v two isn't calling the rating system? Did we have a feature

454
00:28:22,486 --> 00:28:26,174
flag that disabled the system and we forgot to turn it back on?

455
00:28:26,292 --> 00:28:29,690
We can get a feel for how our system is actually behaving.

456
00:28:29,770 --> 00:28:32,974
Compare that to what the architect expected and make

457
00:28:33,012 --> 00:28:36,402
some different choices. Oh, it looks like we haven't used v one in a while

458
00:28:36,456 --> 00:28:39,970
and so that one started going gray. That's excellent.

459
00:28:40,310 --> 00:28:44,206
So we were able to look at both Istio and Linkerd.

460
00:28:44,318 --> 00:28:47,854
Istio was great at showing all of the different details,

461
00:28:47,902 --> 00:28:51,526
having features that we could turn on and off to get deep into our system.

462
00:28:51,628 --> 00:28:55,330
It includes the best of open source projects. By comparison,

463
00:28:55,410 --> 00:28:58,642
Linkerd is super easy to get started with and includes

464
00:28:58,706 --> 00:29:02,378
pretty much everything in the box that we need to start. But if we want

465
00:29:02,384 --> 00:29:05,610
to go farther, we need to reach outside of Linkerd.

466
00:29:06,030 --> 00:29:09,738
Now that was great. We got to see both systems, compare and contrast them.

467
00:29:09,824 --> 00:29:12,766
If one of those is a great fit for you, that's great. If you want

468
00:29:12,788 --> 00:29:16,654
to look at other things, perhaps searching for these two

469
00:29:16,772 --> 00:29:20,160
will help you find the one that exactly matches your needs.

470
00:29:20,610 --> 00:29:24,402
Now we got to see, as we were looking at service meshes that

471
00:29:24,456 --> 00:29:28,478
when we first start crawling, we get monitoring,

472
00:29:28,574 --> 00:29:31,906
logging, service, health. These are all features that we get as

473
00:29:31,928 --> 00:29:35,234
we proxy through our service mesh, upgrading from

474
00:29:35,272 --> 00:29:39,078
crawl to walk, we can see that we got intelligent routing. We were

475
00:29:39,084 --> 00:29:42,482
able to create a b tests, we were able to create canary

476
00:29:42,546 --> 00:29:46,342
releases, we were able to virtually route between versions while

477
00:29:46,396 --> 00:29:49,750
both of them were working simultaneously within our cluster.

478
00:29:49,910 --> 00:29:53,898
And when we upgrade from walk to run, we get a live

479
00:29:53,984 --> 00:29:57,626
network topology diagram that shows us exactly what's happening in

480
00:29:57,648 --> 00:30:01,210
our cluster. Distributed traces, live network diagrams.

481
00:30:01,290 --> 00:30:04,606
We get great monitoring and diagnostics from our

482
00:30:04,628 --> 00:30:08,670
system, because we're proxying between each of those microservices.

483
00:30:11,330 --> 00:30:14,730
Now, a service mesh is not without its costs. On the

484
00:30:14,740 --> 00:30:18,066
left is a typical architecture diagram for kubernetes. We can

485
00:30:18,088 --> 00:30:21,746
see the control plane and the worker nodes, and then we also have a

486
00:30:21,768 --> 00:30:24,866
control plane and envoy proxies with

487
00:30:24,888 --> 00:30:28,918
a service mesh. Now that means that we're running

488
00:30:29,004 --> 00:30:32,918
more containers. Now, granted, envoy proxy is a

489
00:30:32,924 --> 00:30:36,822
lot leaner than a Java Tomcat app, so maybe

490
00:30:36,876 --> 00:30:40,362
we're not running twice the workload, but we're probably running

491
00:30:40,416 --> 00:30:44,106
twice the containers, maybe one and a half the workload, or one and a

492
00:30:44,128 --> 00:30:47,898
third the workload. We will run more stuff,

493
00:30:48,064 --> 00:30:51,706
and that does mean additional hosting costs. So how

494
00:30:51,728 --> 00:30:55,214
do we know when a service mesh is right? Is it worth the investment to

495
00:30:55,252 --> 00:30:58,560
have that level of observability, control and security?

496
00:30:59,330 --> 00:31:02,074
The benefits of a service mesh? We get to observe,

497
00:31:02,122 --> 00:31:05,682
control and secure the system. And if we have these needs, a service

498
00:31:05,736 --> 00:31:09,134
mesh is a really elegant tool. We can watch the traffic

499
00:31:09,182 --> 00:31:12,334
flowing between our cluster. We can create network policies

500
00:31:12,382 --> 00:31:15,874
that route it to beta channels, or that just

501
00:31:15,992 --> 00:31:19,286
discard it if it's not coming in the best way. And we

502
00:31:19,308 --> 00:31:22,678
get mutual tls between all of our services, ensuring that the

503
00:31:22,684 --> 00:31:25,922
services are not attacked by rogue containers

504
00:31:25,986 --> 00:31:29,658
running in our cluster. So when should we use this?

505
00:31:29,824 --> 00:31:33,114
Well, a service mesh is really great if we have

506
00:31:33,152 --> 00:31:36,134
a mix of trusted and untrusted workloads.

507
00:31:36,262 --> 00:31:40,150
So for example, maybe we have very highly sensitive workloads,

508
00:31:40,230 --> 00:31:43,646
PKI or PCI workloads, and we need to ensure that they

509
00:31:43,668 --> 00:31:47,582
are completely separate. We'll build a virtual cage for those services,

510
00:31:47,716 --> 00:31:51,086
so that only those pieces that need to can communicate with those

511
00:31:51,108 --> 00:31:53,962
services running untrusted workloads.

512
00:31:54,026 --> 00:31:57,810
Maybe we have a multi tenant system, or we're running

513
00:31:57,880 --> 00:32:01,682
things on behalf of others and we're not quite sure what they are. We definitely

514
00:32:01,736 --> 00:32:04,786
need to be able to segregate those out so they don't impact the

515
00:32:04,808 --> 00:32:08,770
majority of our workloads. Maybe I'm running a multitenant workload

516
00:32:08,850 --> 00:32:12,514
and I need to be able to segregate different lanes for different environments.

517
00:32:12,642 --> 00:32:16,054
And so now I can create mechanisms where each tenant can

518
00:32:16,092 --> 00:32:19,482
get their own bounded mechanism and

519
00:32:19,536 --> 00:32:23,382
not interfere with other clients running elsewhere in the cluster.

520
00:32:23,526 --> 00:32:27,142
Now, by default, Kubernetes has namespaces, but namespaces

521
00:32:27,206 --> 00:32:31,042
are an organizational boundary, not a security boundary.

522
00:32:31,206 --> 00:32:34,270
By comparison, when I add a service mesh,

523
00:32:35,410 --> 00:32:39,166
I'm able to create those hard boundaries between services to

524
00:32:39,188 --> 00:32:43,054
ensure that only those things that need to are able to reach it.

525
00:32:43,252 --> 00:32:47,266
If I need security in depth, if I need HTTPs within

526
00:32:47,368 --> 00:32:51,230
my cluster, not just to the front door of my cluster,

527
00:32:51,310 --> 00:32:54,638
then a service mesh can be a great way to get mutual tls.

528
00:32:54,814 --> 00:32:58,626
If all I need is mutual tls, I might find a lighter weight solution.

529
00:32:58,738 --> 00:33:02,582
But if I need mutual tls together with observability and control,

530
00:33:02,716 --> 00:33:06,134
then perhaps service mesh is great. If I need

531
00:33:06,172 --> 00:33:09,798
additional features like AB routing or a beta channel,

532
00:33:09,884 --> 00:33:13,914
a service mesh can be a great opportunity to get that. Now there are

533
00:33:13,952 --> 00:33:17,706
other ways to be able to get multiple versions running at the

534
00:33:17,728 --> 00:33:21,178
same time and virtually route between them. But if that's one of

535
00:33:21,184 --> 00:33:24,958
my needs together with other needs in things list, then service meshes might

536
00:33:24,964 --> 00:33:28,666
be a great fit. This has been a lot of fun getting to introduce

537
00:33:28,698 --> 00:33:31,342
to you servicemesh and show you when it makes sense,

538
00:33:31,396 --> 00:33:35,214
and maybe when it doesn't make sense. If you're watching this on

539
00:33:35,252 --> 00:33:39,546
demand, find me on Twitter at rob underscore rich or on MacedonRobrich

540
00:33:39,578 --> 00:33:43,306
at hashaderm IO. Or find all the other socials

541
00:33:43,338 --> 00:33:46,902
on robrich.org and you can download this presentation right

542
00:33:46,956 --> 00:33:50,886
now from robrich.org. Click on presentations if

543
00:33:50,908 --> 00:33:54,358
you're watching this live. I'll see you in a minute at that spot where the

544
00:33:54,364 --> 00:33:56,760
conference is designated for live q and a.

545
00:33:57,130 --> 00:34:01,330
Thanks for joining us for service meshes service meshes to service mesh

546
00:34:01,410 --> 00:34:04,580
here at comp 40 two's cube native thanks for coming.

