1
00:00:00,250 --> 00:00:03,946
Welcome everyone to this session on Chaos Engineering for SQL

2
00:00:03,978 --> 00:00:07,450
Server. A little bit about myself first. My name is Andrew Pruski.

3
00:00:07,530 --> 00:00:11,418
I am a SQL server DBA and Microsoft data platform MVP.

4
00:00:11,514 --> 00:00:15,694
Originally from Swansea, but I've been living in Dublin for the last just

5
00:00:15,732 --> 00:00:19,534
over six years now. My twitter handle at dbA from the cold and

6
00:00:19,572 --> 00:00:23,022
my email address dbA from thecald@gmail.com are on the slide there.

7
00:00:23,076 --> 00:00:26,630
So if you have any questions after today, please feel free to reach out.

8
00:00:26,700 --> 00:00:30,006
I'm always willing to talk about this stuff. My blogs are as well.

9
00:00:30,028 --> 00:00:33,542
Dbafthecol.com posting an article next week about

10
00:00:33,596 --> 00:00:36,966
SQL Server and Chaos Engineering. And then finally at the bottom there is my

11
00:00:36,988 --> 00:00:40,258
GitHub account. All the slides and the code for the demos

12
00:00:40,274 --> 00:00:43,626
I'll be running are available there and I'll post an exact link at the

13
00:00:43,648 --> 00:00:46,970
end of the session. So onto the session.

14
00:00:47,710 --> 00:00:51,114
The aim of this session is to talk about how we can apply chaos

15
00:00:51,162 --> 00:00:55,134
engineering principles and practices to SQL Server and the

16
00:00:55,172 --> 00:00:58,346
systems around it. I'm a database administrator

17
00:00:58,378 --> 00:01:02,958
by trade. Have we got any dbas in? I can't see no

18
00:01:03,044 --> 00:01:06,498
good, so I can say pretty much what I want. I'm a

19
00:01:06,504 --> 00:01:09,842
database administrator by trade and we're taught to think about

20
00:01:09,896 --> 00:01:13,710
failures. For example, one of the core aspects of being a DBA

21
00:01:13,790 --> 00:01:17,538
is backups. We backup our databases and we

22
00:01:17,544 --> 00:01:21,334
don't just back up our databases and forget about them. We verify those backups and

23
00:01:21,372 --> 00:01:24,614
we practice restores because we need to be able to

24
00:01:24,652 --> 00:01:28,038
restore our databases if they encounter a failure in production

25
00:01:28,134 --> 00:01:31,974
to our company's RPO recovery point objective

26
00:01:32,102 --> 00:01:35,946
and within our company's RTO. And it's because of practices like

27
00:01:35,968 --> 00:01:39,558
this. I think that being a DBA and chaos engineering

28
00:01:39,654 --> 00:01:43,674
really do go hand in hand. I think all dbas should be embracing

29
00:01:43,722 --> 00:01:47,546
chaos engineering principles and performing chaos engineering

30
00:01:47,578 --> 00:01:51,022
experiments. So here's what we're going to cover. We're going to talk about

31
00:01:51,156 --> 00:01:54,814
identifying potential failures and weaknesses in SQL

32
00:01:54,862 --> 00:01:58,402
Server and the systems around it that we can

33
00:01:58,456 --> 00:02:01,278
test with chaos engineering experiments.

34
00:02:01,454 --> 00:02:04,962
Then we're going to actually run a chaos engineering experiment up in a lab

35
00:02:05,016 --> 00:02:08,020
that I've got in Azure. If the Wi Fi holds out on me,

36
00:02:08,390 --> 00:02:11,766
fingers crossed. And then finally to round off the session, just have a bit

37
00:02:11,788 --> 00:02:15,762
of fun. We're going to talk about SQL Server on kubernetes,

38
00:02:15,906 --> 00:02:18,838
and we've got a little tool that I found online that we can use to

39
00:02:18,844 --> 00:02:22,486
test SQL running on kubernetes. It's a really exciting time to be a SQL

40
00:02:22,518 --> 00:02:25,706
Server DBA at the moment, over the last few years we've had all

41
00:02:25,728 --> 00:02:29,686
these brand new platforms. We've had SQL servers on Linux, SQL Server in Docker

42
00:02:29,718 --> 00:02:33,082
containers, and SQL Server on kubernetes.

43
00:02:33,226 --> 00:02:36,490
So we'll have a little test about testing SQL Server's

44
00:02:36,570 --> 00:02:39,902
HA capabilities on kubernetes to round the session off.

45
00:02:39,956 --> 00:02:43,406
But first things first, let's talk about how we

46
00:02:43,428 --> 00:02:46,862
can identify weaknesses for SQL Server and its systems

47
00:02:46,926 --> 00:02:50,594
that we want to test. So the best way to start with this is

48
00:02:50,632 --> 00:02:53,666
to do a past incident analysis, or as

49
00:02:53,688 --> 00:02:57,170
Ross would say, past surprise analysis.

50
00:02:58,070 --> 00:03:01,526
So the reason we do this is because we want to find a

51
00:03:01,548 --> 00:03:04,886
potential weakness that has happened in the past or is likely to happen. There's no

52
00:03:04,908 --> 00:03:08,294
point in running a chaos engineering experiment against something that's never going to happen

53
00:03:08,332 --> 00:03:12,934
in your very unlikely to happen in your environment. We want to get some actionable

54
00:03:12,982 --> 00:03:15,846
results from our chaos engineering experiments.

55
00:03:15,878 --> 00:03:19,222
So look at the past incidents that have happened in your environment,

56
00:03:19,366 --> 00:03:22,846
what's happened and what technologies and strategies were put in place to

57
00:03:22,868 --> 00:03:26,538
mitigate that issue. So say we had a standalone SQL

58
00:03:26,554 --> 00:03:29,886
server instance in production. That instance went down

59
00:03:29,988 --> 00:03:32,782
and some sort of ha strategy was put in place,

60
00:03:32,916 --> 00:03:36,834
whatever that may be. Maybe it's mirroring, maybe it's clustering, maybe it's always

61
00:03:36,872 --> 00:03:40,350
on availability group. Or if you're mad, we can use replication.

62
00:03:40,510 --> 00:03:44,066
So maybe we want to run a chaos engineering experiment against that Ha

63
00:03:44,168 --> 00:03:47,810
strategy to further build confidence in our systems

64
00:03:47,890 --> 00:03:51,398
so that they will react as we expect them to

65
00:03:51,564 --> 00:03:54,882
when, not if, they encounter a failure,

66
00:03:55,026 --> 00:03:58,826
because we want to learn from the previous failures and that will guide us to

67
00:03:58,848 --> 00:04:02,406
our future experiments so we can do a past surprise

68
00:04:02,438 --> 00:04:06,314
analysis. Another really good method is to do what's called a

69
00:04:06,432 --> 00:04:09,834
likelihood impact map. So we think

70
00:04:09,872 --> 00:04:13,306
about all the failures that could happen in our systems and

71
00:04:13,328 --> 00:04:16,666
we rank them on how likely they are to their impact.

72
00:04:16,698 --> 00:04:20,522
So all the way down the bottom there in the green, least likely, least impact,

73
00:04:20,666 --> 00:04:24,526
all the way up to the top there with most likely, oh holy moly,

74
00:04:24,638 --> 00:04:28,178
impact brand. When people do this,

75
00:04:28,264 --> 00:04:32,114
they do tend to be eitherly overly pessimistic or

76
00:04:32,152 --> 00:04:35,742
overly optimistic about the failures in their systems. I'll let you decide

77
00:04:35,806 --> 00:04:37,460
which way I fall on that one.

78
00:04:38,630 --> 00:04:42,006
But if that does happen, you'll see that all your failures, you clump down

79
00:04:42,028 --> 00:04:45,078
the bottom or at the top. If that happens, drill in and really,

80
00:04:45,164 --> 00:04:48,578
really think about the failures and perform the analysis again. So you

81
00:04:48,604 --> 00:04:52,422
get a nice spread across the graph with hopefully

82
00:04:52,486 --> 00:04:56,310
three or four up in the red there that are potential candidates

83
00:04:56,390 --> 00:05:00,220
for you to run your chaos engineering experiments against.

84
00:05:00,610 --> 00:05:04,234
So let's think about potential

85
00:05:04,282 --> 00:05:07,486
failures for SQL server. I've already mentioned one high

86
00:05:07,508 --> 00:05:10,794
availability we built. Say we had a standalone

87
00:05:10,842 --> 00:05:14,526
SQL instance in production that went down. So a

88
00:05:14,548 --> 00:05:18,706
two node always on availability group cluster has built. So we had a primary and

89
00:05:18,728 --> 00:05:22,382
a secondary. If the primary fails, we've configured it automatic

90
00:05:22,446 --> 00:05:26,462
failover to the secondary. The secondary will stay up and our databases

91
00:05:26,526 --> 00:05:30,162
will remain online. Now we can perform tests

92
00:05:30,226 --> 00:05:33,926
in SQL server to fail over that availability group and fail it back with

93
00:05:33,948 --> 00:05:38,330
SQL. We can say alter availability group failover, hit execute,

94
00:05:38,670 --> 00:05:42,086
watch it fail over. But that's not how SQL

95
00:05:42,118 --> 00:05:45,914
servers is going to fail out in production. What about

96
00:05:46,112 --> 00:05:50,066
just turning off the primary node, nuking that primary

97
00:05:50,118 --> 00:05:53,454
node, seeing what happens? Isn't that a more realistic test of

98
00:05:53,492 --> 00:05:57,280
how SQL will fail out in production? Yes, absolutely.

99
00:05:58,610 --> 00:06:01,920
So that is one test we could possibly run.

100
00:06:02,850 --> 00:06:06,260
So let's think about some more failures. Let's think about

101
00:06:06,870 --> 00:06:10,894
monitoring systems monitoring is important. Monitoring things like pagerduty

102
00:06:10,942 --> 00:06:15,118
gets alerted, you get paged. If that goes down, you're in trouble. So let's

103
00:06:15,134 --> 00:06:19,390
think about a runaway query in the database. Runaway query

104
00:06:19,470 --> 00:06:22,898
fills up the transaction log of a SQL server database. If that happens in SQL

105
00:06:22,914 --> 00:06:25,618
Server, no more writes can hit that database.

106
00:06:25,794 --> 00:06:29,058
Now there's no point in really writing a chaos engineering experiment

107
00:06:29,074 --> 00:06:31,814
to test that. We know what's going to happen.

108
00:06:32,012 --> 00:06:35,338
But what about your monitoring? When did you get alerted there

109
00:06:35,344 --> 00:06:38,406
was an issue? Did you get an alert at the end saying, oh, the transaction

110
00:06:38,438 --> 00:06:42,394
logs full, you need to do something? Or did you get preemptive alerts saying,

111
00:06:42,512 --> 00:06:45,818
hey, there's a lot of growth happening here, you need to investigate

112
00:06:45,914 --> 00:06:49,306
before an issue occurs. So maybe we could write a chaos engineering

113
00:06:49,338 --> 00:06:52,686
experiment to test that. Thinking of another one.

114
00:06:52,708 --> 00:06:56,210
Let's go. Backups already mentioned backups.

115
00:06:56,870 --> 00:06:59,854
We test our backups, right? We back them up to the same server the databases

116
00:06:59,902 --> 00:07:03,010
are on? No, that server goes offline,

117
00:07:03,910 --> 00:07:07,270
we've lost not only our databases, we've lost the ability to recover them.

118
00:07:07,340 --> 00:07:10,726
So we put them off site somewhere on

119
00:07:10,748 --> 00:07:13,958
an external server. We just back them up there and

120
00:07:13,964 --> 00:07:17,486
leave them, right? No, we regularly run restore

121
00:07:17,538 --> 00:07:21,594
tests against those backs. We verify them and then we regularly restore those

122
00:07:21,632 --> 00:07:25,878
databases because issues with restores

123
00:07:25,974 --> 00:07:29,578
do happen. A few jobs ago, I was working as

124
00:07:29,584 --> 00:07:33,226
a SQL server DBA and I was using a third party tool to backup

125
00:07:33,258 --> 00:07:36,478
my databases. The reason for this was we had kind of

126
00:07:36,484 --> 00:07:40,462
large databases and I wanted

127
00:07:40,516 --> 00:07:43,678
to have native backup compression. We were

128
00:07:43,684 --> 00:07:47,226
working with SQL Server 2005. This is how long ago it was SQL Server

129
00:07:47,258 --> 00:07:49,938
didn't have native backup compression. So this third party tool did it. I'm not going

130
00:07:49,944 --> 00:07:51,570
to mention it because I don't want to get sued,

131
00:07:52,790 --> 00:07:56,970
but it gave us the compression. It was great. It worked everywhere. I has implemented

132
00:07:56,990 --> 00:08:00,710
it absolutely everywhere. Compressing my backups. Happy as Larry.

133
00:08:01,610 --> 00:08:05,398
One day I needed to restore a three terabyte. We had an issue in

134
00:08:05,404 --> 00:08:08,790
production. I needed to restore a three terabyte database.

135
00:08:08,950 --> 00:08:12,410
Not a problem, I said, I have the backups, they've all been verified.

136
00:08:12,990 --> 00:08:16,262
Went to the software, clicked the backup file,

137
00:08:16,406 --> 00:08:20,414
clicked restore, and I also clicked a

138
00:08:20,452 --> 00:08:24,190
function that this third party tool had called instant Restore

139
00:08:24,690 --> 00:08:28,378
because who wouldn't want to instantly restore a three terabyte

140
00:08:28,394 --> 00:08:32,502
database? Awesome. So I clicked instant restore,

141
00:08:32,586 --> 00:08:37,234
I hit execute and the software went to 99%

142
00:08:37,432 --> 00:08:42,130
and there it stayed for 5 hours and then promptly failed.

143
00:08:42,790 --> 00:08:46,946
I hadn't been regularly testing my restores using that feature.

144
00:08:47,058 --> 00:08:51,170
I should have regularly been restoring databases to a test server

145
00:08:51,250 --> 00:08:54,742
using those settings so that I know that that process will

146
00:08:54,796 --> 00:08:58,134
work as I expect it to when I really

147
00:08:58,252 --> 00:09:01,546
needed it. And guess what? I do now. I mean, there's loads of tools we

148
00:09:01,568 --> 00:09:04,598
have out there these days. Has everyone here heard of DBA tools?

149
00:09:04,614 --> 00:09:07,302
Powershell module? Yay.

150
00:09:07,366 --> 00:09:10,950
Nay. Yep, we got a few in there. So it's a Powershell module

151
00:09:11,030 --> 00:09:14,506
specifically designed for database administration and you can do database features with

152
00:09:14,528 --> 00:09:17,806
it. So I have it running on a separate server and it picks databases at

153
00:09:17,828 --> 00:09:21,210
random, grabs their backups, restores them for me to another server,

154
00:09:21,290 --> 00:09:24,574
and then flags any issues, if there are any. So that when I

155
00:09:24,612 --> 00:09:28,238
come to needed to be able to restore a production database,

156
00:09:28,414 --> 00:09:31,540
I know that process will work.

157
00:09:32,790 --> 00:09:36,514
Okay, more failures. Let's go for an obvious

158
00:09:36,552 --> 00:09:39,670
one. Let's go. User error. Who here has ever

159
00:09:39,740 --> 00:09:43,186
run can update statement against a production table without a where clause? So you've

160
00:09:43,218 --> 00:09:46,200
updated all the rows instead of just the ones you wanted.

161
00:09:47,370 --> 00:09:50,758
Be honest. Happened to

162
00:09:50,764 --> 00:09:53,046
a friend of mine in a previous job that I did. He ran an update

163
00:09:53,078 --> 00:09:56,554
statement against a production table, the outcome of which, for a period of about half

164
00:09:56,592 --> 00:09:59,802
an hour, a 15% discount was applied to every single

165
00:09:59,856 --> 00:10:03,546
transaction that company did. Lovely.

166
00:10:03,738 --> 00:10:07,646
So this is a test of maybe our deployment pipelines or our

167
00:10:07,668 --> 00:10:11,466
practices. Should you have been able to run an ad hoc update query

168
00:10:11,578 --> 00:10:15,182
against those production servers? Do your admins really need admin

169
00:10:15,246 --> 00:10:18,180
access? All the time? Really?

170
00:10:18,870 --> 00:10:22,706
So maybe this is a way we can test a deployment pipeline. Maybe this

171
00:10:22,728 --> 00:10:26,446
is a test of our security policies. Does he need admin

172
00:10:26,478 --> 00:10:29,698
access all the time? The outcome of this actually was. We restricted our ad accounts

173
00:10:29,714 --> 00:10:33,446
to read only access to the databases. And then when we needed to

174
00:10:33,468 --> 00:10:37,462
do stuff like that, we logged in with SQL authenticate accounts. And then we

175
00:10:37,596 --> 00:10:41,042
installed another tool called SMS boost to management Studio,

176
00:10:41,106 --> 00:10:44,866
a tool for managing SQL server databases. And that would scan any script

177
00:10:44,898 --> 00:10:48,546
that you're about to run on certain servers and say, hey, this update

178
00:10:48,578 --> 00:10:51,598
statement doesn't have a where clause. Are you sure you want to run this?

179
00:10:51,684 --> 00:10:54,926
So maybe there's extra steps we could put in there. Maybe there's something around that

180
00:10:55,028 --> 00:10:57,898
we could test to try and prevent human error. You're never going to get rid

181
00:10:57,914 --> 00:11:01,070
of it completely. One of the things with SMS boost is you got just really

182
00:11:01,140 --> 00:11:04,180
used to just clicking. Ok. So are you ready to run this? Yes. Click.

183
00:11:04,870 --> 00:11:07,780
But maybe there's something we could test there.

184
00:11:08,390 --> 00:11:11,922
Okay, let's do one more. Let's go nuclear. Let's think.

185
00:11:11,976 --> 00:11:16,386
A production data center outage.

186
00:11:16,578 --> 00:11:20,342
You're running all your production systems in a private data center.

187
00:11:20,476 --> 00:11:24,198
That data center has an outage. Okay.

188
00:11:24,284 --> 00:11:27,570
Likelihood, not very likely. Impact,

189
00:11:27,730 --> 00:11:31,482
pretty bad. So this would be a test of your

190
00:11:31,536 --> 00:11:35,354
Dr. Solution. You need to have a Dr. Solution in place

191
00:11:35,472 --> 00:11:39,594
and you need to be regularly testing that Dr. Plan. You do not want

192
00:11:39,632 --> 00:11:42,814
to be enacting your Dr. Strategy for the first time when

193
00:11:42,852 --> 00:11:47,002
you have an issue in production. Now I've said fairly unlikely,

194
00:11:47,066 --> 00:11:50,462
and it is, but it does happen. Happened to can ex

195
00:11:50,516 --> 00:11:54,318
colleague of mine not two months ago. Their production data center

196
00:11:54,484 --> 00:11:58,434
had a power outage. They didn't have a Dr. Plan. They were out for 4

197
00:11:58,472 --> 00:12:02,546
hours. Think of the damage that'll do to a business. You need

198
00:12:02,568 --> 00:12:06,660
to have a Dr. Strategy in place. Brand. You need to be regularly testing it.

199
00:12:07,590 --> 00:12:11,366
Okay. So we do the likelihood impact map. We think about

200
00:12:11,388 --> 00:12:14,914
all these failures because we want to identify things like which failure has the highest

201
00:12:14,962 --> 00:12:18,214
likelihood, which failure has the highest impact, and what will we

202
00:12:18,252 --> 00:12:22,194
gain from testing these failures? Remember, we want to get some actionable results from running

203
00:12:22,252 --> 00:12:25,786
these chaos engineering experiments. Okay. It's great. If you run an experiment and

204
00:12:25,808 --> 00:12:29,526
everything passes and it works exactly as we expect it to, we've built confidence

205
00:12:29,558 --> 00:12:33,098
in the system. But I'm pretty sure I can guarantee you

206
00:12:33,184 --> 00:12:36,766
you will find issues out there in the wild. And the

207
00:12:36,788 --> 00:12:40,154
last point there is, is there anything else that can be tested?

208
00:12:40,282 --> 00:12:44,238
Now this is sort of. Is there anything else that you haven't thought of?

209
00:12:44,404 --> 00:12:46,958
That's a bit of a weird question because it's Andrew, is there anything you haven't

210
00:12:46,974 --> 00:12:50,866
thought of? I don't know. I haven't thought of it. What this means is go

211
00:12:50,888 --> 00:12:53,826
and talk to people outside of your team.

212
00:12:53,928 --> 00:12:57,470
Go and talk to, say, sysadmins, your network admins, your database developers.

213
00:12:57,550 --> 00:13:01,286
Talk to your end users because I guarantee they

214
00:13:01,308 --> 00:13:04,934
will come up with perceived weaknesses or failures that they've seen that you

215
00:13:04,972 --> 00:13:08,914
haven't thought of. So maybe there's something there that you can test to further build

216
00:13:08,972 --> 00:13:12,470
confidence in all of your systems that they will react

217
00:13:12,630 --> 00:13:16,730
as you expect them to when they encounter a failure.

218
00:13:17,390 --> 00:13:21,094
So let's go ahead and let's have a look at running an experiment.

219
00:13:21,142 --> 00:13:24,446
So what test are we going to run? So I've

220
00:13:24,468 --> 00:13:28,394
already mentioned it. Let's have a look at what happens if the primary node

221
00:13:28,442 --> 00:13:31,870
in an availability group cluster fails.

222
00:13:32,210 --> 00:13:35,566
So we're going to have a two node cluster. That primary

223
00:13:35,598 --> 00:13:38,834
node is going to go down. What are

224
00:13:38,872 --> 00:13:42,126
we expecting to happen? So let's define

225
00:13:42,158 --> 00:13:45,506
the experiment. The hypothesis, the listener of the availability group

226
00:13:45,528 --> 00:13:48,846
should remain online. The listener is the endpoint. This is what all the applications

227
00:13:48,878 --> 00:13:52,278
and users connect to so that they can access the databases. So if the

228
00:13:52,284 --> 00:13:55,986
primary node goes down, we want the listener, the availability group, to fail

229
00:13:56,018 --> 00:14:00,026
over to the secondary node, brand that listener to remain online so that our

230
00:14:00,048 --> 00:14:03,446
databases are accessible. Method.

231
00:14:03,558 --> 00:14:07,162
Now I've said we can run TSQL statements to availability groups over,

232
00:14:07,216 --> 00:14:10,662
but very sanitized way of testing.

233
00:14:10,726 --> 00:14:14,174
So how about stopping the database engine service on

234
00:14:14,212 --> 00:14:18,190
that primary node? So we're simulating that primary node going down

235
00:14:18,260 --> 00:14:21,520
and we'll see what happens with that availability group.

236
00:14:21,970 --> 00:14:25,386
Of course, because we're running this test, we need a rollback

237
00:14:25,418 --> 00:14:28,606
strategy. Now if this was me, even if I was doing it in a development

238
00:14:28,638 --> 00:14:32,558
environment, in my work, I would have a set load of scripts

239
00:14:32,654 --> 00:14:35,698
that would perform a point in time restore of all the databases there just in

240
00:14:35,704 --> 00:14:39,618
case something horribly went wrong. But as this is just a demo environment,

241
00:14:39,794 --> 00:14:43,042
all I'm going to do is restart the primary database

242
00:14:43,106 --> 00:14:47,206
engine service on the primary node. So let's go

243
00:14:47,228 --> 00:14:49,930
ahead and let's run a chaos engineering experiment.

244
00:14:51,550 --> 00:14:55,142
Okay, here we have a two node availability group cluster.

245
00:14:55,206 --> 00:14:59,594
So we have AP SQlago one which is our primary and

246
00:14:59,632 --> 00:15:03,430
AP SQL two which is the secondary.

247
00:15:03,590 --> 00:15:07,486
So let's use SQL to failover from the primary to the secondary so

248
00:15:07,508 --> 00:15:11,210
we can execute this. And this is where we tell SQL to perform the failover

249
00:15:11,290 --> 00:15:15,010
of our availability group from one to two.

250
00:15:15,080 --> 00:15:17,860
And there we are, that's completed. So if we do a refresh here,

251
00:15:19,030 --> 00:15:23,890
we can see one is now the secondary and

252
00:15:23,960 --> 00:15:27,366
two is our primary. So if I connect back to

253
00:15:27,388 --> 00:15:31,222
the one, I can fail it back just to verify that failover works

254
00:15:31,276 --> 00:15:35,046
both ways. And now we're saying fail back

255
00:15:35,068 --> 00:15:38,726
from two to one. So we're verifying that the

256
00:15:38,748 --> 00:15:41,994
AG can fail from one node to the other and then back again.

257
00:15:42,032 --> 00:15:44,570
And there we go, that's completed. So if I do a refresh,

258
00:15:47,630 --> 00:15:51,050
one is back to our primary and

259
00:15:51,120 --> 00:15:54,810
two is our secondary. But this isn't how an availability

260
00:15:54,890 --> 00:15:58,238
group will fail, but in the wild. So let's run a

261
00:15:58,244 --> 00:16:01,946
chaos engineering experiment to simulate solutions in production

262
00:16:02,058 --> 00:16:06,046
that an availability group would experience, say the primary

263
00:16:06,078 --> 00:16:09,586
node failing. And then we'll see if the availability group can

264
00:16:09,608 --> 00:16:12,754
still fail over. So if we jump into visual studio code,

265
00:16:12,952 --> 00:16:16,514
I have this scripture scrub to the top. First thing

266
00:16:16,552 --> 00:16:19,990
it does is just grab the listener name running some SQL,

267
00:16:20,730 --> 00:16:24,150
and then we have a load of write hosts here just to make it basically

268
00:16:24,220 --> 00:16:27,666
look pretty, to be honest. And then we run a testnet

269
00:16:27,698 --> 00:16:30,838
connection against the listener to make sure the listener is online.

270
00:16:30,924 --> 00:16:34,154
And then we put that into a pest test to verify whether or

271
00:16:34,192 --> 00:16:37,866
not it's online. If testnet connection comes back with

272
00:16:37,888 --> 00:16:41,082
a value of true, the pester test will pass. Anything else?

273
00:16:41,136 --> 00:16:44,666
The pester test will fail. And this is our steady state hypothesis.

274
00:16:44,778 --> 00:16:47,902
We run this to make sure the listener is online before

275
00:16:47,956 --> 00:16:50,750
we actually run our chaos engineering experiment.

276
00:16:51,330 --> 00:16:54,798
So if steady state hypothesis

277
00:16:54,894 --> 00:16:58,002
passes, we then stop the

278
00:16:58,056 --> 00:17:01,678
database engine service on the primary. So we're simulating

279
00:17:01,774 --> 00:17:05,682
the primary failing and then we test

280
00:17:05,736 --> 00:17:09,478
the listener again. The listener is online. It means it's failed over to the

281
00:17:09,484 --> 00:17:12,934
secondary node brand, the availability group is online,

282
00:17:13,052 --> 00:17:16,822
and SQL has reacted to the primary node shutting down

283
00:17:16,956 --> 00:17:20,934
as we expect it to. So let's run our chaos

284
00:17:20,982 --> 00:17:25,946
engineering experiment. Let's make sure I'm in the right location and

285
00:17:25,968 --> 00:17:29,446
let's run. So first thing we're

286
00:17:29,478 --> 00:17:33,260
doing is our steady state hypothesis is that listener online.

287
00:17:36,640 --> 00:17:40,416
Listener is online. So now we can shut down SQL on

288
00:17:40,438 --> 00:17:43,650
the primary node. So wait for that to go down.

289
00:17:50,040 --> 00:17:53,684
And now we're running our test again to see if that listener is

290
00:17:53,722 --> 00:17:57,204
online. And boom, there we are.

291
00:17:57,402 --> 00:18:01,284
Looks like our chaos engineering experiment

292
00:18:01,332 --> 00:18:05,112
has passed. Let's have a look in SQL itself

293
00:18:05,246 --> 00:18:06,840
and let's do a refresh.

294
00:18:12,210 --> 00:18:15,930
And one is now the secondary brand. Two is the primary.

295
00:18:16,010 --> 00:18:19,170
So the same has running the SQL by shutting down the node, sorry,

296
00:18:19,240 --> 00:18:22,846
by shutting down SQL on the primary node, the availability group has failed

297
00:18:22,878 --> 00:18:26,100
over to the secondary node as we expected it to.

298
00:18:26,550 --> 00:18:29,720
So let's run the test again. Let's fail back from two to one,

299
00:18:33,850 --> 00:18:37,222
verifying that the availability group can fail both ways. So same

300
00:18:37,276 --> 00:18:41,242
again. Starting off is that listener online

301
00:18:41,376 --> 00:18:45,066
on two. It's online. So now we can shut down the

302
00:18:45,088 --> 00:18:49,178
database engine service on that node. And let's see if

303
00:18:49,264 --> 00:18:52,826
the availability group fails over. So database

304
00:18:52,858 --> 00:18:57,086
engine service going down and

305
00:18:57,108 --> 00:19:00,590
we rerun our testnet connection against the listener.

306
00:19:05,300 --> 00:19:08,436
So this has taken a little bit longer than before. This might be

307
00:19:08,458 --> 00:19:10,870
something gone wrong. We'll have to give it a little bit to see.

308
00:19:17,540 --> 00:19:19,600
And we go, it looks like it's failed.

309
00:19:21,940 --> 00:19:25,136
Yes. Okay, so the listener is no longer online. There we

310
00:19:25,158 --> 00:19:28,550
go. And let's catch engineering expected from rolling back.

311
00:19:29,960 --> 00:19:33,860
So let's have a look in SQL.

312
00:19:34,440 --> 00:19:35,990
So refresh here.

313
00:19:38,600 --> 00:19:40,250
Let's do a refresh here.

314
00:19:41,740 --> 00:19:45,160
Okay, so two is still the primary brand,

315
00:19:45,230 --> 00:19:49,044
one is still the secondary. So our Chaos

316
00:19:49,092 --> 00:19:52,840
engineering experiment has failed. We were unable to fail that availability

317
00:19:52,920 --> 00:19:57,292
group back from two to one. Now the reason for this

318
00:19:57,426 --> 00:20:01,176
is settings in the clustered

319
00:20:01,208 --> 00:20:04,540
role. If I come down here to properties, I click failover.

320
00:20:05,600 --> 00:20:08,976
We have these settings here. And what these settings are saying is

321
00:20:08,998 --> 00:20:12,992
the maximum failures in the specified period can be one and the period is hours.

322
00:20:13,126 --> 00:20:16,756
Now what this is saying is if there is more than one failure of that

323
00:20:16,778 --> 00:20:20,740
clustered role, the availability group in 6 hours, if another

324
00:20:20,810 --> 00:20:24,240
failover occurs, the clustered role, the availability

325
00:20:24,320 --> 00:20:27,030
group will be left in the failed state.

326
00:20:27,480 --> 00:20:31,476
So I can use SQL, I want to fail the availability group backwards and forwards.

327
00:20:31,588 --> 00:20:34,964
But if the availability group experiences a primary node

328
00:20:35,012 --> 00:20:38,472
failure like we've simulated with our chaos engineering experiment more than once

329
00:20:38,526 --> 00:20:42,376
in 6 hours with these settings, that failover

330
00:20:42,488 --> 00:20:45,848
won't work. And guess what the default settings

331
00:20:45,864 --> 00:20:48,510
are for a clustered role. Theyll are these ones here.

332
00:20:49,040 --> 00:20:52,636
So by performing this chaos engineering experiment, we've identified

333
00:20:52,748 --> 00:20:56,192
a setting in our availability group that

334
00:20:56,246 --> 00:21:00,208
could possibly cause SQL to react in a

335
00:21:00,214 --> 00:21:03,600
way we don't expect it to. So we could come in,

336
00:21:03,670 --> 00:21:09,924
make changes here. I'm just going to pick say ten brand,

337
00:21:09,962 --> 00:21:18,770
let's rerun our experimental.

338
00:21:19,590 --> 00:21:22,040
So let's check that the listener is online again.

339
00:21:22,970 --> 00:21:27,126
It's online. And now let's shut down that database engine service on

340
00:21:27,228 --> 00:21:31,026
two. Brand, let's see if the setting changes we've made will

341
00:21:31,068 --> 00:21:34,966
allow that availability group to fail over. So waiting for the database

342
00:21:34,998 --> 00:21:38,140
engine service to go down and we're rerunning our test.

343
00:21:44,840 --> 00:21:46,790
Boom, listener is online.

344
00:21:51,020 --> 00:21:54,680
Come into SQL, let's do a refresh.

345
00:21:56,780 --> 00:22:01,640
SQL is now the prior, sorry, one is now the primary brand,

346
00:22:01,710 --> 00:22:05,436
two is the secondary. So by making that configuration change, we are now able to

347
00:22:05,458 --> 00:22:09,180
see SQL failing over back from two. When we shut down the database engine

348
00:22:09,250 --> 00:22:11,070
service on two.

349
00:22:12,960 --> 00:22:16,476
Okay, little bit of a contrived experiment, but those are the default settings in that

350
00:22:16,498 --> 00:22:19,888
clustered role of one failure every 6 hours. And I have to

351
00:22:19,894 --> 00:22:22,976
admit, when I was building this test, this demo,

352
00:22:23,158 --> 00:22:26,320
I was sort of sitting there thinking, I can't really remember

353
00:22:26,390 --> 00:22:30,236
the last time I checked that setting for my production SQL

354
00:22:30,268 --> 00:22:33,652
instances. And I went and checked and guess what, a couple of them

355
00:22:33,706 --> 00:22:36,836
had those default settings I need to go in and change. So even

356
00:22:36,858 --> 00:22:40,020
just by performing a chaos engineering experiment against this SQL instance

357
00:22:40,100 --> 00:22:43,656
in a lab, I got some actionable results that I could take

358
00:22:43,678 --> 00:22:47,640
away and put into my production environment.

359
00:22:52,400 --> 00:22:56,972
Okay, so that

360
00:22:57,026 --> 00:23:00,984
is running a chaos engineering experiment against a SQL

361
00:23:01,032 --> 00:23:04,348
always on availability group. Now to round the session off, I want

362
00:23:04,354 --> 00:23:08,064
to talk about SQL Server running on kubernetes. Anyone here

363
00:23:08,102 --> 00:23:11,184
got experience with kubernetes? Oh, loads of you. All right,

364
00:23:11,222 --> 00:23:14,784
cool. I love Kubernetes. I think it's really cool. I really

365
00:23:14,822 --> 00:23:18,148
think it's really, really cool that we can run SQL server on it because I

366
00:23:18,154 --> 00:23:22,310
like to take advantage of Kubernetes. Kubernetes is

367
00:23:22,760 --> 00:23:27,952
built in high availability features.

368
00:23:28,016 --> 00:23:31,876
So I have my desired state. I define my deployment in

369
00:23:31,898 --> 00:23:35,364
say a config file or a YAML file, and then I deploy it into kubernetes

370
00:23:35,412 --> 00:23:39,336
and it becomes my running state. And Kubernetes is constantly checking between the two

371
00:23:39,438 --> 00:23:42,330
to see if there are any issues. If there are any issues,

372
00:23:42,700 --> 00:23:46,028
Kubernetes will fix them for me. So if my instance of SQL running in my

373
00:23:46,034 --> 00:23:50,536
pod has an issue, Kubernetes will spin me up a new one automatically.

374
00:23:50,728 --> 00:23:54,444
And I was demoing this to my boss and we'll run through that.

375
00:23:54,482 --> 00:23:58,008
Now. He was saying, that's great, Andrew, but this is a brand new platform

376
00:23:58,114 --> 00:24:01,776
SQL server. So doing it in the command line,

377
00:24:01,878 --> 00:24:05,008
that's okay. But can you jazz up your presentation a little bit? Can you make

378
00:24:05,014 --> 00:24:08,416
it a little bit more wow factor? I went, well, you want

379
00:24:08,438 --> 00:24:11,316
me to use a GUI? And he went, yeah, yeah, a nice gui would be

380
00:24:11,338 --> 00:24:14,116
good. And I went, great, I can use a gui. And wouldn't it be great

381
00:24:14,138 --> 00:24:17,140
if that GuI was Space Invaders?

382
00:24:19,240 --> 00:24:22,736
This is a project has mentioned earlier. It's on GitHub.

383
00:24:22,768 --> 00:24:26,056
It's called Kube Invaders. And guess what? Your pods are the invaders and you play

384
00:24:26,078 --> 00:24:29,656
the spaceship and you can nuke your pods and you can test

385
00:24:29,758 --> 00:24:33,496
your pods high availability by killing them playing

386
00:24:33,528 --> 00:24:37,004
Space Invaders. So let's run through the

387
00:24:37,042 --> 00:24:40,508
demo. So I'm first on my laptop here and then

388
00:24:40,594 --> 00:24:43,916
we'll run up into if I knew what I was

389
00:24:43,938 --> 00:24:47,452
doing. So I have a one node cluster running

390
00:24:47,506 --> 00:24:51,216
on my laptop here. It's called Micro K eight. So it's actually really good.

391
00:24:51,238 --> 00:24:53,984
If you're running on Windows you have the ability to run Docker desktop and that

392
00:24:54,022 --> 00:24:57,588
has a Kubernetes feature. You just have a tick box and you've got

393
00:24:57,594 --> 00:25:01,056
it up and running. So let's run one pod with SQl

394
00:25:01,088 --> 00:25:04,276
server and let's expose that on its default port. So there

395
00:25:04,298 --> 00:25:05,030
we are.

396
00:25:08,680 --> 00:25:12,570
There's my pod, there's my service up and running. Lovely stuff.

397
00:25:13,420 --> 00:25:17,172
So I can connect to it. So say this is my steady state hypothesis.

398
00:25:17,236 --> 00:25:20,948
Is SQL up and running connecting with Ms SQl

399
00:25:20,964 --> 00:25:24,008
Cli. Just a little command line tool. And there we are. We have Microsoft SQl

400
00:25:24,024 --> 00:25:25,950
server 2019 rc one.

401
00:25:28,960 --> 00:25:32,716
So there's my pod. We can see IP address of ten 161

402
00:25:32,738 --> 00:25:36,000
at 57 age about 28 seconds.

403
00:25:37,380 --> 00:25:41,344
Let's nuke that pod. There we go. So what's happened here

404
00:25:41,382 --> 00:25:45,276
is the running state no longer matches the desired state. Kubernetes should automatically

405
00:25:45,308 --> 00:25:49,172
say, oh hang on, something's not right here brand it should fix it for me.

406
00:25:49,306 --> 00:25:52,804
So what we should see when this comes back is that

407
00:25:52,922 --> 00:25:56,484
I now have a new

408
00:25:56,522 --> 00:26:00,244
pod running for 16 seconds with a new IP address and

409
00:26:00,282 --> 00:26:04,208
I can further test because

410
00:26:04,234 --> 00:26:07,656
I still have my service and I can run my query again and that

411
00:26:07,678 --> 00:26:10,804
should using the same IP address go in and I should be able to query

412
00:26:10,852 --> 00:26:14,348
that and get my version back. So that's a very,

413
00:26:14,434 --> 00:26:18,044
very basic chaos engineering experiment against SQL server running

414
00:26:18,082 --> 00:26:23,100
on Kubernetes. Making use of kubernetes ha systems.

415
00:26:24,980 --> 00:26:29,296
Okay so that's great. Let's clean up and

416
00:26:29,318 --> 00:26:33,504
I'm going to switch to a cluster running up in Azure and

417
00:26:33,542 --> 00:26:37,270
I'm going to deploy ten pods to it.

418
00:26:37,880 --> 00:26:44,096
Let's see how that gets on. Okay I've

419
00:26:44,128 --> 00:26:48,230
got a few pods coming. There we are running

420
00:26:49,180 --> 00:26:55,770
and now I can play my game. Oh hang on. Let's grab that.

421
00:27:00,140 --> 00:27:03,556
Here we go. And okay let's

422
00:27:03,588 --> 00:27:07,308
get rid of that one. Let's get rid of that one. And we

423
00:27:07,314 --> 00:27:10,684
can see the pods on the side there been killed and

424
00:27:10,722 --> 00:27:14,108
hopefully Kubernetes will realize that the running state

425
00:27:14,194 --> 00:27:17,516
no longer matches the desired state and it will fix them for me.

426
00:27:17,538 --> 00:27:20,380
It should sync up and I should get a whole bunch of brand new pods

427
00:27:20,460 --> 00:27:22,370
coming into the system there.

428
00:27:24,260 --> 00:27:26,928
Another cool thing about this is I can stick it on automatic mode and I

429
00:27:26,934 --> 00:27:29,728
can let it play for me so I can let that go off and I

430
00:27:29,734 --> 00:27:32,436
can stand here and I could talk about it and I can decide when it

431
00:27:32,458 --> 00:27:34,804
wants to start killing pods and if any of them are going to sink back

432
00:27:34,842 --> 00:27:39,536
in one of the really nice things. But this is that it's

433
00:27:39,568 --> 00:27:43,690
shooting little mini Kubernetes logos, which I think is a really

434
00:27:44,140 --> 00:27:45,210
nice touch.

435
00:27:47,180 --> 00:27:49,864
So we can let that play there. Some of them are coming back in.

436
00:27:49,902 --> 00:27:53,336
There we go. See, Kubernetes realized that the

437
00:27:53,358 --> 00:27:56,556
running state no longer matched the desired state and fixed that for me,

438
00:27:56,578 --> 00:28:00,136
brought all those pods back in brand. It's going to let that play and it'll

439
00:28:00,168 --> 00:28:01,950
just keep killing them as I'm talking.

440
00:28:03,520 --> 00:28:07,416
So let's do a cleanup there and jump

441
00:28:07,448 --> 00:28:11,216
back into the slides. So round off. Just got

442
00:28:11,238 --> 00:28:14,764
a couple of resources there. The top one there is the link to my GitHub

443
00:28:14,812 --> 00:28:18,640
account with the slides and the code for all the demos I just ran.

444
00:28:18,720 --> 00:28:22,384
And the second link there is to the Kube

445
00:28:22,432 --> 00:28:26,230
Invaders project on GitHub as well. Thank you very much.

446
00:28:31,260 --> 00:28:33,770
Are there any questions? Yes,

447
00:28:35,900 --> 00:28:39,092
any sort of persistent storage like relational

448
00:28:39,156 --> 00:28:42,308
databases within Kubernetes and Docker?

449
00:28:42,484 --> 00:28:46,300
Are you talking about that as just a playground?

450
00:28:46,800 --> 00:28:50,284
No. At the moment we are in the middle of running a

451
00:28:50,322 --> 00:28:53,816
proof of concept. So the question is, is SQL server

452
00:28:53,848 --> 00:28:57,136
running on Kubernetes and Docker just a playground? Are we looking at something

453
00:28:57,158 --> 00:29:00,844
more serious? We are looking at something more serious. We've built our own Kubernetes

454
00:29:00,892 --> 00:29:04,690
cluster running of a pure storage array with

455
00:29:05,060 --> 00:29:08,492
obviously provisioning persistent volumes for the databases,

456
00:29:08,556 --> 00:29:11,396
the system databases, transaction logs and then backing them off site.

457
00:29:11,498 --> 00:29:15,376
And then we are properly nuking it. I've managed

458
00:29:15,408 --> 00:29:17,510
to corrupt quite a few databases so far,

459
00:29:19,080 --> 00:29:23,316
but that again will figure into how quickly we can restore brand.

460
00:29:23,498 --> 00:29:27,380
It's a lot of learning processes here because maybe we got the configuration wrong.

461
00:29:27,530 --> 00:29:30,708
Pure storage eraser, so lightning fast, so that's not going to be an issue I

462
00:29:30,714 --> 00:29:34,184
reckon. I think it's more us nuking pods whilst there's heavy transactions

463
00:29:34,232 --> 00:29:36,984
going on, which will happen with if you own SQL server in a vm,

464
00:29:37,032 --> 00:29:37,290
to be honest.

