1
00:00:34,610 --> 00:00:38,614
Hello, I am Federica Chufon. I'm a specialist solutions architect for

2
00:00:38,652 --> 00:00:41,414
container technologies at Amazon Web Services.

3
00:00:41,612 --> 00:00:44,802
Today I'm going to talk about how the Kubernetes

4
00:00:44,866 --> 00:00:48,774
ecosystem has evolved to answer to more complex application

5
00:00:48,892 --> 00:00:51,710
patterns from a networking standpoint.

6
00:00:52,530 --> 00:00:55,680
So let's dive right into it.

7
00:00:56,210 --> 00:00:59,530
Let's start with the easiest pattern,

8
00:00:59,610 --> 00:01:02,926
which is the monoith how do we handle

9
00:01:02,958 --> 00:01:06,690
internal calls and client traffic within the monolith?

10
00:01:07,110 --> 00:01:10,446
Well, applications are on the same physical

11
00:01:10,558 --> 00:01:14,162
and virtual machine into the monolith. Therefore we

12
00:01:14,216 --> 00:01:17,442
use loopback interfaces to grant

13
00:01:17,506 --> 00:01:21,010
communication. And how do we handle

14
00:01:21,090 --> 00:01:24,642
client traffic? Well, if you want to be fancies

15
00:01:24,706 --> 00:01:28,410
you could use load balancer or a proxy.

16
00:01:28,990 --> 00:01:32,362
But this pattern, the monolithic one, is not

17
00:01:32,416 --> 00:01:36,454
used anymore because of a number of reasons, as reducing

18
00:01:36,502 --> 00:01:38,678
dependencies between applications,

19
00:01:38,854 --> 00:01:42,106
enabling developer flexibility,

20
00:01:42,298 --> 00:01:46,000
reducing blast radius and decreased time to market.

21
00:01:46,530 --> 00:01:49,806
So we broke down, or a

22
00:01:49,828 --> 00:01:53,842
pattern that is very common is to break down these monoes into

23
00:01:53,896 --> 00:01:56,740
microservices. In particular,

24
00:01:57,270 --> 00:02:00,578
you can containerize those and manage them

25
00:02:00,744 --> 00:02:03,170
with, for example, kubernetes.

26
00:02:03,510 --> 00:02:07,130
So in the Kubernetes context, how do we handle

27
00:02:07,310 --> 00:02:11,510
service communication? Well, when you containerize

28
00:02:14,090 --> 00:02:17,518
your application and put this into a Kubernetes cluster,

29
00:02:17,634 --> 00:02:20,982
your application will talk thanks to the container

30
00:02:21,046 --> 00:02:25,046
network interface. Some examples include

31
00:02:25,158 --> 00:02:27,770
Amazon VPC, CNI,

32
00:02:28,430 --> 00:02:31,870
Cilium and Calico CNIs.

33
00:02:33,170 --> 00:02:37,322
Well, client traffic can also be handled by load balancers.

34
00:02:37,386 --> 00:02:40,634
But when we are talking about application traffic,

35
00:02:40,762 --> 00:02:44,820
Kubernetes ecosystem needs

36
00:02:45,350 --> 00:02:49,214
something more and this is actually the ingress

37
00:02:49,342 --> 00:02:53,598
resource that needs deployed into your cluster

38
00:02:53,774 --> 00:02:57,766
and an application which is the ingress controller which

39
00:02:57,868 --> 00:03:01,686
picks up the creation, update or

40
00:03:01,708 --> 00:03:05,762
deletion of ingress. Kubernetes objects and automatically

41
00:03:05,826 --> 00:03:08,810
creates updates or deletes load balancers.

42
00:03:10,990 --> 00:03:14,618
Well, of course, breaking down your application

43
00:03:14,704 --> 00:03:17,420
into microservices is only the first step.

44
00:03:18,430 --> 00:03:22,446
The pattern that we see with our medium large scale customers

45
00:03:22,628 --> 00:03:26,110
is to further break down applications into different

46
00:03:26,180 --> 00:03:29,534
clusters or different areas of the cloud

47
00:03:29,652 --> 00:03:33,266
to gain flexibility and for separation of

48
00:03:33,288 --> 00:03:35,620
duties and security.

49
00:03:36,550 --> 00:03:40,814
Well, so let's recap this a little bit. You have multiple

50
00:03:40,862 --> 00:03:44,938
microservices which are spread across different clusters,

51
00:03:45,054 --> 00:03:48,646
different areas of the cloud, and those are written in

52
00:03:48,668 --> 00:03:51,846
different programming languages usually. So in

53
00:03:51,868 --> 00:03:55,734
this context, how do we handle service to service communication and

54
00:03:55,772 --> 00:03:58,986
client traffic? Well, we need to take

55
00:03:59,088 --> 00:04:02,250
into account some challenges.

56
00:04:02,750 --> 00:04:06,954
So first of all, how do

57
00:04:06,992 --> 00:04:10,578
we manage network connectivity and traffic routing,

58
00:04:10,774 --> 00:04:14,622
and how do we obtain visibility into service

59
00:04:14,676 --> 00:04:18,366
to service communication? Let's say, how do we manage the

60
00:04:18,388 --> 00:04:22,706
observability of my systems and how

61
00:04:22,728 --> 00:04:26,270
do I ensure trust by automatic

62
00:04:26,350 --> 00:04:30,254
security and compliance in my systems

63
00:04:30,302 --> 00:04:33,854
and applications also tied

64
00:04:33,902 --> 00:04:38,082
to the modernization of our application patterns

65
00:04:38,226 --> 00:04:41,894
and architectures. We need also to modernize our

66
00:04:42,092 --> 00:04:45,766
organization and really create a

67
00:04:45,788 --> 00:04:49,690
culture of innovation by organizing well

68
00:04:49,760 --> 00:04:53,366
our teams into small DevOp teams.

69
00:04:53,558 --> 00:04:57,194
So those objectives needs to be

70
00:04:57,232 --> 00:05:01,134
taken into account or were taken into account

71
00:05:01,332 --> 00:05:04,878
from the Kubernetes ecosystem when deploying more

72
00:05:04,964 --> 00:05:08,766
advanced offerings and answers to

73
00:05:08,868 --> 00:05:13,506
application networking in Kubernetes let

74
00:05:13,528 --> 00:05:17,154
me introduce a couple of concepts from Amazon Web

75
00:05:17,192 --> 00:05:21,454
services. The first one is Amazon eks elastic

76
00:05:21,502 --> 00:05:25,470
Kubernetes service. This is a managed Kubernetes cluster

77
00:05:25,550 --> 00:05:29,094
from AWS in which we manage the control plane of

78
00:05:29,132 --> 00:05:32,946
a Kubernetes cluster. In the slide you can also see Amazon

79
00:05:32,978 --> 00:05:36,470
VPC, Amazon virtual private cloud. It's basically

80
00:05:36,540 --> 00:05:40,358
a chunk of the AWS Amazon network

81
00:05:40,534 --> 00:05:44,380
that we give to you and that you manage completely.

82
00:05:44,910 --> 00:05:49,450
Resources like Amazon eks live in an Amazon VPC.

83
00:05:50,370 --> 00:05:53,662
So again, how do we set up and manage the

84
00:05:53,716 --> 00:05:57,386
ingress and service to service communications

85
00:05:57,578 --> 00:06:00,720
while guaranteeing agility, control,

86
00:06:01,090 --> 00:06:03,060
visibility and security?

87
00:06:03,830 --> 00:06:07,806
Well, you could use again load balancers,

88
00:06:07,838 --> 00:06:11,170
but planning for scale, you don't want really

89
00:06:11,240 --> 00:06:14,900
to provision and manage a load balancer per service.

90
00:06:15,290 --> 00:06:19,426
There are techniques with ingress controllers

91
00:06:19,538 --> 00:06:23,190
that enable you to well share

92
00:06:23,260 --> 00:06:26,418
load balancer across multiple applications,

93
00:06:26,514 --> 00:06:30,106
multiple backends, but still when your application

94
00:06:30,208 --> 00:06:33,446
scales massively or when you have stricter

95
00:06:33,478 --> 00:06:36,330
requirements, you may need to adapt.

96
00:06:38,510 --> 00:06:42,682
Let's recap. We have seen how networking

97
00:06:42,746 --> 00:06:45,898
is handled in monolithic applications,

98
00:06:46,074 --> 00:06:49,914
how Kubernetes answers to containerization

99
00:06:50,042 --> 00:06:53,870
with CNIs and ingress controllers. So let's

100
00:06:53,950 --> 00:06:57,758
talk about now how microservices

101
00:06:57,854 --> 00:07:01,934
high handed by the Kubernetes ecosystem

102
00:07:02,062 --> 00:07:07,030
and how the networking offering has

103
00:07:07,180 --> 00:07:11,074
evolved to answer to the problems of having a distributed

104
00:07:11,122 --> 00:07:14,120
system. Well,

105
00:07:14,970 --> 00:07:17,602
Kubernetes project said,

106
00:07:17,676 --> 00:07:21,690
okay, proxies are really useful

107
00:07:22,430 --> 00:07:26,134
to manage networking in general. Why don't

108
00:07:26,182 --> 00:07:29,500
we use them within a Kubernetes cluster as well?

109
00:07:30,370 --> 00:07:33,822
In fact, there is a pattern which is called

110
00:07:33,956 --> 00:07:38,240
to put this proxy as sidecar within

111
00:07:38,610 --> 00:07:41,994
the same pod, sidecar to my container

112
00:07:42,042 --> 00:07:45,460
that is next to the container which handles the application

113
00:07:45,910 --> 00:07:49,346
and put these in each pod that

114
00:07:49,368 --> 00:07:53,262
I have in my cluster. This pattern

115
00:07:53,406 --> 00:07:56,630
is known as cycle proxy pattern

116
00:07:57,370 --> 00:08:00,806
and it is basically a way to decouple the

117
00:08:00,828 --> 00:08:03,510
networking and the application layer.

118
00:08:04,010 --> 00:08:07,882
All the traffic that needs to go into

119
00:08:07,936 --> 00:08:12,300
the application pod or container or

120
00:08:12,670 --> 00:08:16,874
generates from the application container passes through the

121
00:08:16,912 --> 00:08:20,294
proxy. This enabled me

122
00:08:20,432 --> 00:08:23,886
some nebulas to have,

123
00:08:24,068 --> 00:08:27,758
let's say, more flexibility between

124
00:08:27,844 --> 00:08:31,214
our applications teams. Let's say that

125
00:08:31,252 --> 00:08:34,754
we have a team that manages a front end

126
00:08:34,872 --> 00:08:39,134
and that needs to talk with a team that manages

127
00:08:39,182 --> 00:08:43,022
a back end, when the back end team releases a new version,

128
00:08:43,166 --> 00:08:46,758
we need to actually change and point

129
00:08:46,844 --> 00:08:50,374
our application code in the front end and point to

130
00:08:50,412 --> 00:08:54,214
the new backend. Well, this is

131
00:08:54,252 --> 00:08:58,002
not needed anymore. If we are using a proxy

132
00:08:58,146 --> 00:09:02,074
AWS sidecar, because we can actually use

133
00:09:02,112 --> 00:09:05,420
the proxy to point to the new application.

134
00:09:06,190 --> 00:09:09,850
So inconsistencies are minimized.

135
00:09:10,610 --> 00:09:14,190
We are separating, let's say business

136
00:09:14,260 --> 00:09:18,590
logic from operation and also install

137
00:09:18,740 --> 00:09:22,586
operation of installs, upgrades, et cetera are a little bit

138
00:09:22,628 --> 00:09:27,026
more easier. And well,

139
00:09:27,208 --> 00:09:31,140
again, one of our objectives was

140
00:09:32,230 --> 00:09:35,082
to make our application observable.

141
00:09:35,246 --> 00:09:38,658
Now we have a unified layer,

142
00:09:38,754 --> 00:09:42,662
one of the proxy that manage our service

143
00:09:42,716 --> 00:09:46,646
to service communication and gives logs which

144
00:09:46,668 --> 00:09:50,426
are in the same programming language and in the

145
00:09:50,448 --> 00:09:53,882
same way because it's actually the same proxy next to

146
00:09:54,016 --> 00:09:57,526
each container of our application takes out a layer

147
00:09:57,558 --> 00:10:01,054
of observability that is very important

148
00:10:01,172 --> 00:10:05,054
when we are managing sidecar or application at

149
00:10:05,092 --> 00:10:08,778
scale. But again we are talking about scale.

150
00:10:08,874 --> 00:10:11,966
How do we manage all of this sidecar? Well,

151
00:10:12,068 --> 00:10:15,426
much like in a Kubernetes cluster, we manage

152
00:10:15,528 --> 00:10:18,562
our applications with a control

153
00:10:18,616 --> 00:10:22,114
plane and well think

154
00:10:22,152 --> 00:10:25,726
about having a control plane also for our sidecar

155
00:10:25,758 --> 00:10:29,014
proxies and the control plane plus the

156
00:10:29,052 --> 00:10:31,670
data plane which are the sitecar proxy themselves,

157
00:10:31,820 --> 00:10:35,558
it is a service mesh. On the left side

158
00:10:35,644 --> 00:10:39,894
of the slide you can see some examples of service meshes

159
00:10:39,942 --> 00:10:42,140
which are commonly used by our customers.

160
00:10:44,590 --> 00:10:48,380
Let's dive a little bit deeper into use cases. Well,

161
00:10:48,990 --> 00:10:52,926
obviously traffic routing is something that we want to manage with

162
00:10:52,948 --> 00:10:57,226
a service mesh, for example routing

163
00:10:57,258 --> 00:11:00,702
from a version one to a version two. But also

164
00:11:00,756 --> 00:11:04,980
there are some advanced features for routing like for example

165
00:11:05,350 --> 00:11:09,214
prefix query parameters or specifying

166
00:11:09,262 --> 00:11:10,610
HTTP headers.

167
00:11:13,750 --> 00:11:17,382
Let's say that we want to protect also our application from

168
00:11:17,516 --> 00:11:21,138
large spikes in traffic and we want to assure

169
00:11:21,234 --> 00:11:24,434
a good level of services. Well, the service mesh

170
00:11:24,482 --> 00:11:28,878
enables me to implement automated retries, secret breaking

171
00:11:28,994 --> 00:11:32,250
and rate limiting into our applications.

172
00:11:33,470 --> 00:11:37,926
Security is also one of our objective and you can enforce

173
00:11:38,038 --> 00:11:42,298
that your application are able or not to talk either within

174
00:11:42,384 --> 00:11:46,050
themselves or among theirselves

175
00:11:46,230 --> 00:11:49,070
or to third party APIs.

176
00:11:50,130 --> 00:11:53,834
You can also enforce that the communication are encrypted and

177
00:11:53,892 --> 00:11:57,634
so reinforce Tls, for example at

178
00:11:57,672 --> 00:12:01,106
the reverse proxy level, not at application

179
00:12:01,208 --> 00:12:04,514
level. This can also integrate with third

180
00:12:04,552 --> 00:12:08,230
party services to generate and renew certificates.

181
00:12:10,010 --> 00:12:13,670
Not only Tls, but we can also

182
00:12:13,820 --> 00:12:18,102
enforce service to service authentication with

183
00:12:18,156 --> 00:12:19,770
mutual Tls.

184
00:12:22,750 --> 00:12:26,234
Another use case is observability. We talked

185
00:12:26,272 --> 00:12:30,042
about it a little bit, but basically the

186
00:12:30,096 --> 00:12:33,914
layer of observability that I gain is that I can

187
00:12:33,952 --> 00:12:38,250
also implement tracing within the proxy to understand upstream

188
00:12:38,330 --> 00:12:42,702
and downstream dependencies and also I can understand where

189
00:12:42,756 --> 00:12:46,194
the bottlenecks are and

190
00:12:46,312 --> 00:12:49,874
identify patterns and understand service

191
00:12:49,992 --> 00:12:51,010
performance.

192
00:12:52,870 --> 00:12:57,018
Another common use case is multicluster.

193
00:12:57,214 --> 00:13:01,010
So connecting together different clusters

194
00:13:01,090 --> 00:13:05,074
which are placed maybe also in different vpcs

195
00:13:05,122 --> 00:13:09,114
or different accounts and the

196
00:13:09,152 --> 00:13:12,710
communication can be granted by implemented

197
00:13:12,790 --> 00:13:18,602
in the correct way. Obviously a service mesh one

198
00:13:18,656 --> 00:13:22,110
common proxy that is used within

199
00:13:22,260 --> 00:13:26,074
meshes is envoy, which is basically highly

200
00:13:26,122 --> 00:13:29,930
performant cloud native proxies. It's very efficient,

201
00:13:30,010 --> 00:13:33,566
it has a really small footprint and

202
00:13:33,748 --> 00:13:37,294
it handles well client side

203
00:13:37,412 --> 00:13:41,250
load balancing, retries, timeouts, rate limiting

204
00:13:41,590 --> 00:13:45,150
enables you to have observability layer seven traffic

205
00:13:45,230 --> 00:13:49,010
and supports different for example HTTP

206
00:13:49,170 --> 00:13:53,190
HP, two CRP and TCP protocols

207
00:13:53,770 --> 00:13:57,570
and also it has rich

208
00:13:57,650 --> 00:14:01,130
and robust APIs for configuration.

209
00:14:02,510 --> 00:14:05,894
One service mesh that uses ongoing proxies

210
00:14:06,022 --> 00:14:09,846
is istio. You can install ISIO into your cluster

211
00:14:09,958 --> 00:14:13,710
using istioctl or istio operator,

212
00:14:14,290 --> 00:14:17,806
create and manage the components like for example

213
00:14:17,908 --> 00:14:21,626
mesh virtual service, virtual gateways, et cetera,

214
00:14:21,738 --> 00:14:24,610
through istioctl and Kubernetes APIs.

215
00:14:25,190 --> 00:14:29,090
There are different supported platforms on

216
00:14:29,160 --> 00:14:32,900
Amazon Web services. You can configure and run

217
00:14:33,350 --> 00:14:35,970
your mesh in Amazon EKS,

218
00:14:36,490 --> 00:14:39,938
Amazon EKS Fargate self, also Kubernetes

219
00:14:40,034 --> 00:14:43,714
on Amazon EC two and also it integrates

220
00:14:43,762 --> 00:14:47,830
with Kali. Kali is a console for istio service mesh

221
00:14:48,170 --> 00:14:51,286
and you can use it configure, visualize,

222
00:14:51,398 --> 00:14:54,250
validate and troubleshoot your mesh.

223
00:14:55,150 --> 00:14:58,842
Let's see how it is istio deploying to our

224
00:14:58,896 --> 00:15:01,982
cluster. We have an is cluster. We want service

225
00:15:02,036 --> 00:15:05,854
a and service b to be able to communicate. So when you

226
00:15:05,892 --> 00:15:10,190
deploy Istio, we are actually deploying the data plane which are

227
00:15:10,340 --> 00:15:14,286
again the proxies throughout the service

228
00:15:14,468 --> 00:15:17,630
flows and the control

229
00:15:17,700 --> 00:15:21,026
plane. The control plane is well a set of

230
00:15:21,048 --> 00:15:25,086
pods which is ECP and it's not managed

231
00:15:25,118 --> 00:15:28,978
by AWS. It lives into the clusters as

232
00:15:29,064 --> 00:15:33,350
again AWS a resource pod. So service

233
00:15:33,420 --> 00:15:37,158
meshes is really the answer to a lot of our

234
00:15:37,324 --> 00:15:40,140
challenges, but well,

235
00:15:40,750 --> 00:15:44,330
still some remain. We have

236
00:15:44,400 --> 00:15:48,618
a lot of now sidecar proxies that we have

237
00:15:48,704 --> 00:15:52,346
to deploy and maintain at a scale. This can be

238
00:15:52,448 --> 00:15:56,534
challenging. It only integrates

239
00:15:56,582 --> 00:16:00,206
with container based workloads. Let's say we are

240
00:16:00,228 --> 00:16:04,134
using say a serverless service like AWS lambda

241
00:16:04,282 --> 00:16:08,002
or Amazon EC two. This is not

242
00:16:08,056 --> 00:16:12,290
really supported. And also before even thinking

243
00:16:12,360 --> 00:16:17,426
about deploying a mesh, we need to have intervpc

244
00:16:17,458 --> 00:16:21,558
networking that we need to actually

245
00:16:21,644 --> 00:16:25,138
implement to grant connectivity across vpcs,

246
00:16:25,154 --> 00:16:28,840
for example, and to enforce security.

247
00:16:30,030 --> 00:16:33,914
So how can we answer

248
00:16:34,112 --> 00:16:38,470
to these challenges? Well, let me introduce

249
00:16:38,630 --> 00:16:42,074
the Kubernetes gateway API. The Kubernetes

250
00:16:42,122 --> 00:16:45,646
Gateway API is a sig network project

251
00:16:45,828 --> 00:16:49,760
that took the lesser learned from the ingress project

252
00:16:50,210 --> 00:16:53,626
and implemented a new way of doing

253
00:16:53,748 --> 00:16:57,554
networking into Kubernetes. We have different

254
00:16:57,672 --> 00:17:02,302
objects that belong to the Kubernetes gateway API.

255
00:17:02,446 --> 00:17:06,582
The first of it is gateway class. The gateway class

256
00:17:06,636 --> 00:17:10,466
enables me to formalize basically the type of load

257
00:17:10,498 --> 00:17:14,486
balancing implementation that I can use. Let's say

258
00:17:14,668 --> 00:17:18,070
if it were ingress, for example, it would be a load balancer.

259
00:17:18,750 --> 00:17:22,342
The gateway actually specifies

260
00:17:22,406 --> 00:17:25,958
a point where the traffic can be translated

261
00:17:26,134 --> 00:17:29,734
into the Kubernetes services. So how do I translate traffic

262
00:17:29,782 --> 00:17:33,066
that is coming from outside the class, inside the

263
00:17:33,088 --> 00:17:37,430
class and where this is HTTP

264
00:17:37,510 --> 00:17:41,322
routes. Those are rules for mapping the request

265
00:17:41,386 --> 00:17:45,022
from a gateway to Kubernetes

266
00:17:45,166 --> 00:17:49,554
services, and the services are

267
00:17:49,592 --> 00:17:53,650
basically the targets of our HTTP routes.

268
00:17:54,550 --> 00:17:59,334
A nice feature to the Kubernetes Gateway API is

269
00:17:59,372 --> 00:18:02,902
that it is role oriented and

270
00:18:02,956 --> 00:18:07,138
we have finally a mapping between the Kubernetes

271
00:18:07,234 --> 00:18:11,174
objects that we have in our cluster and a role

272
00:18:11,222 --> 00:18:15,050
within our organization. There are different

273
00:18:15,120 --> 00:18:18,758
implementations, like for example for the ingress,

274
00:18:18,854 --> 00:18:21,446
we have different ingress controllers,

275
00:18:21,638 --> 00:18:25,154
so we have different controllers for the Kubernetes

276
00:18:25,222 --> 00:18:29,754
gateway API. You cannot recognize different icons

277
00:18:29,802 --> 00:18:33,722
that we have already seen in our presentation, like for example istio

278
00:18:33,866 --> 00:18:37,634
njs haproxy and a new one.

279
00:18:37,832 --> 00:18:41,438
This is the Amazon VPC lattice icon.

280
00:18:41,534 --> 00:18:45,826
Amazon VPC lattice is a networking service that we have

281
00:18:46,008 --> 00:18:49,830
in preview and announced in our

282
00:18:49,900 --> 00:18:52,438
previous Rainband Rainband 2022.

283
00:18:52,444 --> 00:18:55,880
And basically this

284
00:18:56,410 --> 00:19:00,230
implements a layer seven fabric

285
00:19:00,830 --> 00:19:04,198
layer within the application VPC

286
00:19:04,294 --> 00:19:08,394
fabric. So to say we

287
00:19:08,432 --> 00:19:12,506
can enable service to service communication without sidecar

288
00:19:12,538 --> 00:19:16,606
proxy, which are required, and much like

289
00:19:16,708 --> 00:19:20,766
a service mesh, but without the

290
00:19:20,788 --> 00:19:24,370
need of deploying sidecars. It works

291
00:19:24,440 --> 00:19:28,430
across all the compute options, EC two KIAS

292
00:19:28,510 --> 00:19:33,054
cs lambda, and enables

293
00:19:33,102 --> 00:19:37,282
you to deploy complex

294
00:19:37,426 --> 00:19:41,400
security, let's say architectures, because actually

295
00:19:41,850 --> 00:19:45,446
it's really easy to grant or implement the

296
00:19:45,468 --> 00:19:49,010
networking also across vpcs and accounts where before you needed,

297
00:19:49,100 --> 00:19:52,886
for example to deploy transit gateway pin connections,

298
00:19:52,998 --> 00:19:54,010
et cetera.

299
00:19:56,190 --> 00:19:59,542
Let me give you an overview of Amazon VPC lattice

300
00:19:59,606 --> 00:20:02,746
components. We have a service network

301
00:20:02,938 --> 00:20:06,734
which is basically a logical boundary that you can

302
00:20:06,772 --> 00:20:10,058
define across vpcs and accounts,

303
00:20:10,234 --> 00:20:14,230
and it is used to apply common access and observability

304
00:20:14,330 --> 00:20:17,634
policies. Then we have service.

305
00:20:17,752 --> 00:20:21,906
The service is a unit of applications and it extends across all

306
00:20:21,928 --> 00:20:24,430
the compute instances, containers,

307
00:20:24,510 --> 00:20:27,510
serverless, et cetera.

308
00:20:28,410 --> 00:20:32,130
We have then the service directory which is a centralized

309
00:20:32,210 --> 00:20:35,286
register for the services that are

310
00:20:35,308 --> 00:20:38,550
registered into Amazon VPC lattice.

311
00:20:39,230 --> 00:20:42,474
And then we have security policies which

312
00:20:42,512 --> 00:20:46,166
are identity access management, declarative policies

313
00:20:46,278 --> 00:20:50,490
to configure access, observability, traffic management

314
00:20:51,170 --> 00:20:55,546
and those can be applied at a service gateway

315
00:20:55,658 --> 00:20:58,640
and or application or network level.

316
00:21:02,530 --> 00:21:06,094
Amazon eks supports Amazon EPC lattice

317
00:21:06,142 --> 00:21:10,274
and we actually have built a controller that when we

318
00:21:10,312 --> 00:21:13,540
create Kubernetes gateway API object

319
00:21:14,470 --> 00:21:17,926
automatically, the controller works and creates updates and

320
00:21:17,948 --> 00:21:21,030
deletes lattice resources.

321
00:21:24,330 --> 00:21:27,720
Let's see a little bit deeper how the controller works.

322
00:21:28,170 --> 00:21:31,306
So we have for example an

323
00:21:31,328 --> 00:21:34,486
object like the gateway which is deployed

324
00:21:34,598 --> 00:21:37,866
and the gateway automatically will

325
00:21:38,048 --> 00:21:41,910
once applied, the controller will automatically

326
00:21:41,990 --> 00:21:45,562
create a service network and associate

327
00:21:45,626 --> 00:21:49,562
the vpcs where the EKs clusters

328
00:21:49,706 --> 00:21:53,310
live into the service network

329
00:21:54,210 --> 00:21:58,174
that is. Now we have network

330
00:21:58,302 --> 00:22:02,322
connectivity between the two

331
00:22:02,376 --> 00:22:05,170
vpcs that you can see in slide,

332
00:22:06,790 --> 00:22:10,246
much like as if we had configured for example a

333
00:22:10,268 --> 00:22:13,606
transit gateway and attached the two vpcs to

334
00:22:13,628 --> 00:22:17,174
it. Another thing that

335
00:22:17,212 --> 00:22:22,374
you need is that, well now we have connectivity

336
00:22:22,502 --> 00:22:26,506
networking that is set up, but we have

337
00:22:26,608 --> 00:22:30,650
not defined how the traffic flow. To do so we need

338
00:22:30,720 --> 00:22:34,394
an HTTP route resource. HTTP route

339
00:22:34,442 --> 00:22:38,880
will automatically once applied create

340
00:22:40,130 --> 00:22:43,498
services which are again the targets.

341
00:22:43,594 --> 00:22:47,726
Well the targets the unit of applications that are defined

342
00:22:47,838 --> 00:22:50,370
for Amazon VPC lattice.

343
00:22:50,790 --> 00:22:53,794
In this case we have defined two services.

344
00:22:53,992 --> 00:22:57,506
One it is local to our cluster and one which

345
00:22:57,528 --> 00:23:01,160
is another service which we actually

346
00:23:01,530 --> 00:23:05,026
imported, exported and imported from one cluster

347
00:23:05,058 --> 00:23:09,400
to another. That is to speak, let me

348
00:23:09,770 --> 00:23:13,574
grab pen. This HTTP root

349
00:23:13,622 --> 00:23:17,130
will be applied, for example in the first cluster as a resource,

350
00:23:18,030 --> 00:23:22,278
and we will have exported and imported

351
00:23:22,454 --> 00:23:26,174
this service into this cluster so

352
00:23:26,212 --> 00:23:29,914
that it is visible to be picked up from the HTTP

353
00:23:29,962 --> 00:23:30,590
route.

354
00:23:34,770 --> 00:23:38,094
Then there are other things from the HTTP

355
00:23:38,142 --> 00:23:42,206
route we specify the gateway

356
00:23:42,318 --> 00:23:46,242
which is tied to. So basically we are associating our services

357
00:23:46,376 --> 00:23:50,120
to the service network. And again

358
00:23:50,890 --> 00:23:55,110
we also specify how the traffic it is redirected

359
00:23:55,450 --> 00:23:58,790
from the gateway to our

360
00:23:58,860 --> 00:24:03,674
targets, which in this case are these two services from

361
00:24:03,872 --> 00:24:07,530
one from the first cluster and one from the other cluster.

362
00:24:08,430 --> 00:24:12,074
So you can see how now we have

363
00:24:12,112 --> 00:24:15,600
grant in a really easy way, easy set up

364
00:24:16,290 --> 00:24:20,110
cross cluster communication without even

365
00:24:20,180 --> 00:24:23,786
needing to set up underlying, let's say networking

366
00:24:23,818 --> 00:24:26,926
companies like Transit Gateway VPC, period, and then

367
00:24:26,948 --> 00:24:30,414
deploying a mesh and deploying and maintaining a

368
00:24:30,452 --> 00:24:32,910
set of sidecar proxies.

369
00:24:34,130 --> 00:24:37,782
Thank you. I hope this session has been useful for you

370
00:24:37,876 --> 00:24:41,846
understand a little bit better the ecosystem that we have within

371
00:24:42,028 --> 00:24:46,022
Kubernetes and in particular within the answer or

372
00:24:46,076 --> 00:24:49,970
integration from Amazon Web services to the Kubernetes

373
00:24:50,050 --> 00:24:52,770
ecosystem, in particular the Kubernetes gateway API.

