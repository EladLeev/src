1
00:00:41,330 --> 00:00:44,726
Hi, my name is Phil Wilkins. Thank you for joining me today

2
00:00:44,828 --> 00:00:48,134
on this session about Fluentd and how we can use it to

3
00:00:48,172 --> 00:00:51,646
make our logs so much more productive. Let me

4
00:00:51,668 --> 00:00:54,560
start by giving you a brief introduction to myself.

5
00:00:55,010 --> 00:00:58,362
So in five a family man, a blogger,

6
00:00:58,426 --> 00:01:02,538
an author of several books, I am both a technology evangelist

7
00:01:02,634 --> 00:01:06,370
for our team and a technical architect, which means

8
00:01:06,440 --> 00:01:09,426
not only do I have to talk about these technology, I have to actually go

9
00:01:09,448 --> 00:01:12,962
and make it work. So a lot of this is hard

10
00:01:13,016 --> 00:01:16,594
won, proven stories. So I happen

11
00:01:16,632 --> 00:01:20,374
to work for Capgemini in the UK and our

12
00:01:20,412 --> 00:01:24,374
team has won quite a few awards over the last 510 years,

13
00:01:24,572 --> 00:01:28,234
which we're as a team very proud of and continue to work

14
00:01:28,272 --> 00:01:31,366
to innovate and deliver excellence.

15
00:01:31,478 --> 00:01:34,806
As a team. We work with Oracle Technologies. So that's

16
00:01:34,838 --> 00:01:38,298
Oracle Cloud, that's Oracle middleware, as well as

17
00:01:38,384 --> 00:01:41,714
a lot of open source. And a lot of people perhaps don't realize

18
00:01:41,782 --> 00:01:45,950
Oracle have really embraced open source in the last few years.

19
00:01:46,100 --> 00:01:49,978
Again, there's a lot out there that's based on open source technologies

20
00:01:50,154 --> 00:01:53,726
as well as licensed product. And of course if you want to know lots

21
00:01:53,758 --> 00:01:56,994
more, that my website's got logs and lots of information,

22
00:01:57,112 --> 00:02:01,038
including material around this session. If I'm not writing,

23
00:02:01,134 --> 00:02:04,590
I'm also helping host a meetup based in London,

24
00:02:04,670 --> 00:02:08,898
which at the moment is being done virtually. But hopefully

25
00:02:09,074 --> 00:02:12,294
someday in the not so far away future we will

26
00:02:12,332 --> 00:02:16,054
be back to getting together physically and of course talking about

27
00:02:16,092 --> 00:02:19,526
fluentd that I have to obviously drop a little plugin. I'm in

28
00:02:19,548 --> 00:02:23,034
the process of finalizing these book called logging in action. You can get

29
00:02:23,072 --> 00:02:26,566
early access to it now and a lot of examples

30
00:02:26,598 --> 00:02:30,494
and content that I'm going to talk about today comes from the book.

31
00:02:30,612 --> 00:02:33,994
So hopefully you will find this useful and perhaps

32
00:02:34,042 --> 00:02:37,518
go and buy a copy. I mentioned Capgemini, it's a

33
00:02:37,604 --> 00:02:41,226
substantial organizations and thriving despite

34
00:02:41,258 --> 00:02:45,166
the difficult times that we're in today. But I would encourage you

35
00:02:45,188 --> 00:02:48,014
to go have a look and if you're thinking about jumping a ship,

36
00:02:48,062 --> 00:02:51,794
then we are looking for good people. So I always try and start my

37
00:02:51,832 --> 00:02:56,114
sessions with a little bit of context and logging and monitoring

38
00:02:56,242 --> 00:03:00,706
our interest in animals because we all come from it from slightly different perspectives

39
00:03:00,898 --> 00:03:04,802
as applications development, as package integration

40
00:03:04,946 --> 00:03:07,858
perspective, from an infrastructure perspective.

41
00:03:07,954 --> 00:03:11,418
And what we look at logging and think about does actually

42
00:03:11,504 --> 00:03:14,522
vary quite a bit. So I think the key messages here

43
00:03:14,576 --> 00:03:18,470
is about it's information over time. So time is an important asset.

44
00:03:18,550 --> 00:03:22,314
It's about what is changing and what is happening and helping us

45
00:03:22,352 --> 00:03:25,966
understand what's evolving now that sounds a little bit obvious, but in the

46
00:03:25,988 --> 00:03:29,710
different perspectives, that information and what it

47
00:03:29,780 --> 00:03:33,426
is differs quite significantly. So let me

48
00:03:33,528 --> 00:03:38,318
illustrate this in a little bit more practical. From an infrastructure perspective,

49
00:03:38,414 --> 00:03:42,254
you're more likely to be worrying about things like the CPU

50
00:03:42,302 --> 00:03:46,130
and your memory resources, disk storage allocations

51
00:03:46,290 --> 00:03:49,634
and details like that. These as an apps

52
00:03:49,682 --> 00:03:52,806
person or DevOps person, you're probably going to

53
00:03:52,828 --> 00:03:57,282
be looking more at the textual logs which characterize

54
00:03:57,346 --> 00:04:00,886
what's going on, rather than numerical statistical figures.

55
00:04:00,918 --> 00:04:04,742
You have 80% CPU usage. You're going to be seeing SNMP

56
00:04:04,806 --> 00:04:08,394
teams where file write errors occur and things like

57
00:04:08,432 --> 00:04:11,870
that. You're going to have your applications logging. If you're a Java person,

58
00:04:11,940 --> 00:04:15,342
you've probably been using something like sl for j or log

59
00:04:15,396 --> 00:04:18,986
for j or one of that kind of family of logging

60
00:04:19,018 --> 00:04:23,274
frameworks. But of course if you're then dealing

61
00:04:23,322 --> 00:04:27,774
with end to end stories, you've got to think about the tracing dimension

62
00:04:27,822 --> 00:04:31,746
of it, of what happens at each point in the solution, not just in my

63
00:04:31,768 --> 00:04:35,254
little bit of code. So you can track what's going on, where is

64
00:04:35,292 --> 00:04:39,350
something getting held up or falling over. And we can look at

65
00:04:39,420 --> 00:04:42,998
the different aspects and these trace information

66
00:04:43,084 --> 00:04:46,278
vis the log information, vis the metrics based

67
00:04:46,364 --> 00:04:50,214
information against the different layers. So a hosting

68
00:04:50,342 --> 00:04:53,914
layer is heavily influenced by metrics low on the

69
00:04:53,952 --> 00:04:57,546
log characteristics, on the textual content. As you move

70
00:04:57,568 --> 00:05:01,178
into virtualization and containerization you're into a

71
00:05:01,184 --> 00:05:05,098
bit more of the logs because you're going to have your VMware

72
00:05:05,194 --> 00:05:08,746
or Kubernetes environments writing a bit about what it's

73
00:05:08,778 --> 00:05:13,210
doing and so on. But you've still got a strong percentage statistical

74
00:05:13,290 --> 00:05:16,786
analysis going on of resource utilization and things like

75
00:05:16,808 --> 00:05:20,354
that. When you get into applications. We are now really starting to

76
00:05:20,392 --> 00:05:23,890
look at a lot more of these. What's the application doing,

77
00:05:23,960 --> 00:05:27,458
where is it executing some numerical data still? Because we're

78
00:05:27,474 --> 00:05:31,686
still collecting metrics, numbers of users signed on, things like that.

79
00:05:31,788 --> 00:05:35,590
And then of course if you're a business person, you're going to be more interested

80
00:05:35,660 --> 00:05:39,126
in business application, it's still monitoring, you're still interested in

81
00:05:39,148 --> 00:05:42,490
what's going on, but it more becomes who's doing

82
00:05:42,560 --> 00:05:45,786
what, when have they done it and how many transactions are

83
00:05:45,808 --> 00:05:49,162
allowing in the environment at this time and things like that.

84
00:05:49,296 --> 00:05:53,054
But theyre all logs, they're all numerical data. Theyre will need

85
00:05:53,092 --> 00:05:56,762
to be monitoring and either sucked into a graphical

86
00:05:56,826 --> 00:06:00,062
representation of percentages used and things like that,

87
00:06:00,116 --> 00:06:03,482
or into textual search engine to start analyzing.

88
00:06:03,546 --> 00:06:08,238
And of course then around these side you've got security considerations

89
00:06:08,334 --> 00:06:11,586
and your capacity even in today's world of

90
00:06:11,608 --> 00:06:14,814
cloud, you need to be looking after your capacity. You got elastic

91
00:06:14,862 --> 00:06:18,546
scaling, that's fantastic. But is somebody doing something that

92
00:06:18,568 --> 00:06:22,486
means that your scaling is just running away and you're going to be bring on

93
00:06:22,508 --> 00:06:25,634
a lot more money than perhaps you expected. So capacity

94
00:06:25,682 --> 00:06:28,806
monitoring is still something to be considered. So let's look

95
00:06:28,828 --> 00:06:32,554
at these applications of the logs. I've touched upon some of this, but obviously

96
00:06:32,672 --> 00:06:35,910
one of our key issues is looking for unexpected errors

97
00:06:35,990 --> 00:06:39,946
or warnings of an error scenario occurring. If we're getting

98
00:06:40,048 --> 00:06:43,722
our file system getting used up to very high percentages,

99
00:06:43,866 --> 00:06:47,514
then we are likely to get faults start occurring because files

100
00:06:47,562 --> 00:06:51,374
can't be written. So it's worth thinking about that, because one of the things that

101
00:06:51,492 --> 00:06:56,174
we'll see is we tend to look at logs in retrospective,

102
00:06:56,302 --> 00:06:59,950
rather than looking at them as opportunities

103
00:07:00,030 --> 00:07:03,438
to be precursors and therefore become preemptive.

104
00:07:03,534 --> 00:07:07,026
Rather than waiting till a problem occurs, we can use the logs to

105
00:07:07,048 --> 00:07:10,626
warn us or give us indications that a problem is going to occur.

106
00:07:10,658 --> 00:07:14,374
If we don't intervene, we can look at performance issues. I've talked about

107
00:07:14,412 --> 00:07:17,786
cpu and dis storage, but what about slow queries in

108
00:07:17,808 --> 00:07:21,750
databases? There are plenty of databases

109
00:07:21,910 --> 00:07:25,574
technologies out there, but most of them give you access or insight

110
00:07:25,622 --> 00:07:29,190
into what's underperforming or performing badly.

111
00:07:29,350 --> 00:07:32,986
Some dedicated tools exist to do that, but perhaps you

112
00:07:33,008 --> 00:07:36,894
want to look at that in the context of how your application is

113
00:07:36,932 --> 00:07:39,886
performing as well. What is cause and what is effect?

114
00:07:40,068 --> 00:07:44,066
Obviously we've got, in this day and age, the world of security to

115
00:07:44,088 --> 00:07:47,422
worry about as well through SiEm tooling.

116
00:07:47,486 --> 00:07:50,930
And that's quite interesting, because quite often you'll see a completely

117
00:07:51,000 --> 00:07:55,454
different set of tools for analyzing that information compared to what DevOps

118
00:07:55,502 --> 00:07:59,046
teams or app developers might be using. And of course, as we

119
00:07:59,068 --> 00:08:02,390
get into bigger and bigger systems, we need to bring the picture together

120
00:08:02,460 --> 00:08:06,246
from lots of different components distributed across our network to

121
00:08:06,268 --> 00:08:09,766
be able to understand what's going on end to end. Otherwise it's

122
00:08:09,798 --> 00:08:13,402
very easy for one team to blame another and say, well, our system

123
00:08:13,456 --> 00:08:16,854
looks like it's performing and reporting okay, problems somewhere

124
00:08:16,902 --> 00:08:20,426
else. Not me, Gov. The bottom line is, whatever we

125
00:08:20,448 --> 00:08:23,542
do with logging, we actually are monitoring.

126
00:08:23,606 --> 00:08:26,846
It's all got to support the business and it's all got to

127
00:08:26,868 --> 00:08:30,270
come back to what is going on and how does it impact our business.

128
00:08:30,340 --> 00:08:34,114
If you're monitoring something that's not related to the business at

129
00:08:34,152 --> 00:08:37,842
all, ultimately what value is it going to bring? Could be interesting.

130
00:08:37,976 --> 00:08:42,146
It might be a potential problem, but ultimately in

131
00:08:42,168 --> 00:08:45,742
most organizations everything comes back to increasing

132
00:08:45,806 --> 00:08:49,282
or improving the business value, and we need to overcome

133
00:08:49,346 --> 00:08:52,726
that. There's very few organizations that are going to

134
00:08:52,748 --> 00:08:56,230
let you do some blue sky thinking in the monitoring space.

135
00:08:56,380 --> 00:09:00,282
These diagram is trying to help us understand

136
00:09:00,416 --> 00:09:04,202
or visualize the way monitoring has evolved and just how

137
00:09:04,256 --> 00:09:07,802
complicated the problem has become. Years ago we used to be

138
00:09:07,856 --> 00:09:10,678
on single applications, single threads,

139
00:09:10,774 --> 00:09:13,582
single machine applications. Great,

140
00:09:13,636 --> 00:09:17,694
fantastic, easy to monitor because it was just one location that

141
00:09:17,732 --> 00:09:21,386
was producing it. And the chances are you would be able to spot

142
00:09:21,418 --> 00:09:25,234
the cause of problems quite quickly. As we've evolved over

143
00:09:25,272 --> 00:09:29,182
time, we've started to see concurrency and these asynchronous

144
00:09:29,246 --> 00:09:33,294
behaviors. So we start parking activities temporarily

145
00:09:33,342 --> 00:09:37,266
whilst we wait for something to happen and then pick it up, which is what

146
00:09:37,288 --> 00:09:40,534
we see. We node, we see multithreading in a more

147
00:09:40,572 --> 00:09:44,214
traditional concurrency and we get thread blocks and things

148
00:09:44,252 --> 00:09:47,574
like that. And these of course in the last five years

149
00:09:47,612 --> 00:09:51,770
or so we've seen the likes of containerization really

150
00:09:51,840 --> 00:09:55,690
take that to a whole new level and that's increased distribution of

151
00:09:55,760 --> 00:09:59,146
solutions as well. The latest generations of that sort of

152
00:09:59,168 --> 00:10:02,526
thinking in serverless functions means that the

153
00:10:02,548 --> 00:10:05,870
whole problem gains another scale of complexity, because now

154
00:10:05,940 --> 00:10:08,970
each part of your application is completely transient.

155
00:10:09,050 --> 00:10:11,978
It fires up, does its job and then disappears.

156
00:10:12,074 --> 00:10:14,702
So how are you going to go and see what was bring on?

157
00:10:14,836 --> 00:10:18,322
Because the container that was running or the platform that was running,

158
00:10:18,376 --> 00:10:22,226
your solution will have gone by the time you probably get to

159
00:10:22,248 --> 00:10:25,620
the logs. A real challenge there to deal with.

160
00:10:26,230 --> 00:10:29,998
Okay, so I've set out the landscape somewhat, and I want to

161
00:10:30,024 --> 00:10:33,606
talk a little bit about getting to fluentd now and talk about what it is

162
00:10:33,628 --> 00:10:36,998
actually capable of and set the scene for that. Most people have

163
00:10:37,084 --> 00:10:40,586
heard of fluentd because of its involvement in CNCF. If you're

164
00:10:40,608 --> 00:10:44,442
working with containerization, you've probably heard about

165
00:10:44,576 --> 00:10:48,042
fluentd as an option for log driving in

166
00:10:48,096 --> 00:10:51,606
Docker and its relationship with kubernetes

167
00:10:51,718 --> 00:10:54,910
particularly. But not so many people understand quite

168
00:10:54,980 --> 00:10:58,910
what it's capable of. So let me drill in and just show you

169
00:10:59,060 --> 00:11:02,442
what it is. So it is a highly pluggable framework.

170
00:11:02,506 --> 00:11:06,318
In fact, everything is pretty much seen as a plugin,

171
00:11:06,414 --> 00:11:10,334
or described as a plugin within the fluentd ecosystem.

172
00:11:10,462 --> 00:11:14,222
Now, within fluentd, it starts with a collection of standard plugins

173
00:11:14,286 --> 00:11:18,006
being present, but everything operates in the same model,

174
00:11:18,108 --> 00:11:21,826
which makes it very extensible. And one of Fluentd's

175
00:11:21,858 --> 00:11:26,134
benefits is the amount of work that's been done not just by the

176
00:11:26,252 --> 00:11:29,766
core open source team and the committees that work hard on

177
00:11:29,788 --> 00:11:33,142
fluentd, but also organizations beyond

178
00:11:33,206 --> 00:11:36,554
that. As a result, we can take in log events in

179
00:11:36,592 --> 00:11:40,290
many means different ways. It's not just a case of just polling

180
00:11:40,470 --> 00:11:43,646
file systems looking for logs or maybe talking to

181
00:11:43,668 --> 00:11:46,814
an SMP framework. It can talk to many,

182
00:11:46,852 --> 00:11:51,194
many different sources. It's very flexible. It is largely

183
00:11:51,242 --> 00:11:55,202
platform agnostic. It will run on Windows equally as comfortable as

184
00:11:55,256 --> 00:11:58,974
it will run on a Linux environment or a Mac on outputs.

185
00:11:59,102 --> 00:12:02,942
It is equally as powerful, perhaps even more powerful

186
00:12:03,006 --> 00:12:06,110
in terms of plugins, because one of the beauties of it is if you

187
00:12:06,120 --> 00:12:09,478
can get all these different types of information sourced in,

188
00:12:09,564 --> 00:12:12,742
everybody's going to come to you to get that information into

189
00:12:12,796 --> 00:12:16,722
their product. You've got custom plugins for Splunk,

190
00:12:16,786 --> 00:12:20,178
for Prometheus, for Grafana, kafka,

191
00:12:20,274 --> 00:12:23,206
jabber tools, slack emails,

192
00:12:23,318 --> 00:12:26,154
and the list goes on and on and on. So it's very,

193
00:12:26,192 --> 00:12:29,642
very flexible. And this is part of the core story around

194
00:12:29,776 --> 00:12:33,462
fluentd. It talks about described itself as a log unification

195
00:12:33,526 --> 00:12:36,286
layer, which means that we can bring all the logs together,

196
00:12:36,388 --> 00:12:39,518
unify it and then feed it to where it needs to be.

197
00:12:39,604 --> 00:12:42,618
That can be more than one system, as we'll

198
00:12:42,634 --> 00:12:46,810
see in a minute. Of course formatters are necessary. Different sources

199
00:12:46,890 --> 00:12:50,142
will describe their logs in different ways and structures,

200
00:12:50,206 --> 00:12:53,822
whether that's a CSV or JSON or XML

201
00:12:53,886 --> 00:12:57,086
or some custom bespoke format.

202
00:12:57,198 --> 00:13:00,886
But theyre are many, many formatters that can cope with that and therefore we

203
00:13:00,908 --> 00:13:04,166
can translate the log events into a consistent form

204
00:13:04,268 --> 00:13:07,138
to be consumed into the different systems.

205
00:13:07,234 --> 00:13:10,922
We can filter things, some log events are just

206
00:13:10,976 --> 00:13:15,062
not relevant beyond perhaps keeping a local copy.

207
00:13:15,126 --> 00:13:18,586
Just show that everything is running smoothly and you

208
00:13:18,608 --> 00:13:22,294
might want to just be told about things that are abnormal.

209
00:13:22,342 --> 00:13:25,934
So we can filter out the log events and say, well that's interesting that

210
00:13:25,972 --> 00:13:29,742
we can just bim because that's just the application confirming that everything

211
00:13:29,796 --> 00:13:33,394
is okay, we compose it. This is important not

212
00:13:33,432 --> 00:13:37,282
only just to help do major transformation of data

213
00:13:37,336 --> 00:13:41,038
structures, but importantly to translate logs

214
00:13:41,134 --> 00:13:44,834
as data to logs to something meaningful. We need to take what could

215
00:13:44,872 --> 00:13:48,306
be a huge great string of key value pairs and translate

216
00:13:48,338 --> 00:13:52,050
it to something meaningful that can be acted upon or queried

217
00:13:52,130 --> 00:13:55,762
a lot more easily. And that requires parsing buffers and caching.

218
00:13:55,826 --> 00:13:59,206
So in the day of distributed solutions we need

219
00:13:59,228 --> 00:14:03,574
to pass these things around. And you do get transient errors in networks

220
00:14:03,702 --> 00:14:07,322
from time to time. Therefore we need to buffer and cache things up

221
00:14:07,376 --> 00:14:10,070
to stop losing the information.

222
00:14:10,240 --> 00:14:13,694
Whilst those connections restore themselves, you can use

223
00:14:13,732 --> 00:14:17,306
your buffer and cache bring obviously to optimize processes

224
00:14:17,338 --> 00:14:21,134
of distributing the logs as well. So we get

225
00:14:21,172 --> 00:14:24,786
efficient use of the events and of course,

226
00:14:24,888 --> 00:14:28,526
storage. So if you're not sending it to an active storage

227
00:14:28,558 --> 00:14:32,174
solution, we might just want to write it to a file for legal

228
00:14:32,222 --> 00:14:36,206
archival issues. If we're creating a record of what's

229
00:14:36,238 --> 00:14:40,006
happened in these system, just to show all is well and that

230
00:14:40,028 --> 00:14:43,126
there have been no security breaches, and you might need

231
00:14:43,148 --> 00:14:45,814
to retain that for a long time. We just want to write that to a

232
00:14:45,852 --> 00:14:49,910
storage system that is very efficient and only

233
00:14:49,980 --> 00:14:53,978
pull that out of storage and consume it into an analysis tool

234
00:14:54,064 --> 00:14:58,278
if or when there is a real problem. And then, as you may have guessed,

235
00:14:58,374 --> 00:15:01,950
there is a clear and strong framework for building

236
00:15:02,020 --> 00:15:05,018
your own custom elements so you can build your own plugins.

237
00:15:05,114 --> 00:15:09,066
It's all based around Ruby, which means it's pretty portable

238
00:15:09,178 --> 00:15:13,054
and very flexible, and most people can get their heads around

239
00:15:13,092 --> 00:15:16,474
it. It's not like it's a very weird bespoke language.

240
00:15:16,602 --> 00:15:20,302
If you're an average developer, you'll soon get to grips with it, because the framework

241
00:15:20,366 --> 00:15:23,746
makes it very easy to understand what to look at and what you

242
00:15:23,768 --> 00:15:27,966
need to produce to get a plugin up and running and get it deployed.

243
00:15:28,078 --> 00:15:31,302
So fluentd looks at logs in the form

244
00:15:31,356 --> 00:15:35,650
of three parts, a tag, which is just effectively a label,

245
00:15:35,730 --> 00:15:38,882
so we can associate it with something that's just a shortcut,

246
00:15:38,946 --> 00:15:42,106
largely timestamp, which is critical, because as we said,

247
00:15:42,208 --> 00:15:45,642
all events are in context of time, and then the record,

248
00:15:45,776 --> 00:15:49,334
which can be just about anything you like. But the more meaning

249
00:15:49,382 --> 00:15:53,102
that's in there, the better it is, because it means that we can

250
00:15:53,156 --> 00:15:56,000
process the record in more detail.

251
00:15:56,370 --> 00:15:59,358
As I say, to make the most of fluent D,

252
00:15:59,444 --> 00:16:02,894
we need, or logs in generally. Actually, we need to go

253
00:16:02,932 --> 00:16:06,386
from capturing, just capturing bits of strings or data

254
00:16:06,488 --> 00:16:10,306
to actually translating it, to being very meaningful. And you've got

255
00:16:10,328 --> 00:16:14,178
a kind of lifecycle of events here where we start at

256
00:16:14,264 --> 00:16:18,198
the source, these capture, just get the information in to process.

257
00:16:18,364 --> 00:16:21,606
We then need to structure that information to work out

258
00:16:21,628 --> 00:16:25,154
how to handle it and process it with a bit more meaning.

259
00:16:25,282 --> 00:16:28,822
Then we're into the world of potentially aggregating

260
00:16:28,886 --> 00:16:32,550
and analyzing it. So looking for repeating events,

261
00:16:32,630 --> 00:16:36,106
things like that, and we're starting to move in to

262
00:16:36,128 --> 00:16:39,514
the world where other tools are better suited, and we

263
00:16:39,552 --> 00:16:43,642
use fluent D to get that data there so that we can maximize

264
00:16:43,706 --> 00:16:47,098
the other tools. So yeah, when you get into these analytical

265
00:16:47,194 --> 00:16:50,574
processes, unification tools is not the best thing.

266
00:16:50,612 --> 00:16:54,354
You want a data analytics engine that's looking at data that

267
00:16:54,392 --> 00:16:57,918
you've made structured and meaningful, and fluentd

268
00:16:58,014 --> 00:17:02,126
is passing it all to the right places to then do the analysis

269
00:17:02,238 --> 00:17:06,102
of course, once you've done login analysis, we need to do

270
00:17:06,156 --> 00:17:09,574
some sort of visualization to make it easier to consume these

271
00:17:09,612 --> 00:17:13,654
information, pull out what's important, not present every little detail.

272
00:17:13,772 --> 00:17:17,174
And of course, ideally when things are indicated as

273
00:17:17,212 --> 00:17:20,666
not healthy, we're generating alerts that could be a

274
00:17:20,688 --> 00:17:24,346
Jira ticket to give details of an error that's been thrown in the

275
00:17:24,368 --> 00:17:27,770
application. That could be a service desk support thing.

276
00:17:27,920 --> 00:17:31,786
That could be a slack message to someone looking after the system at

277
00:17:31,808 --> 00:17:35,610
that moment in time to say the file system on server XYZ

278
00:17:35,690 --> 00:17:39,102
is about to hit 100% use. Go fix it now

279
00:17:39,156 --> 00:17:42,410
mate. If you've heard of fluentd, you've probably

280
00:17:42,500 --> 00:17:46,094
encountered the two common stacks which are described

281
00:17:46,142 --> 00:17:50,382
as the elk stack or elasticsearch, log stash

282
00:17:50,446 --> 00:17:53,602
and kibana, which is this. And as you can see,

283
00:17:53,656 --> 00:17:56,806
I've taken that lifecycle and drawn it into

284
00:17:56,908 --> 00:18:00,866
these core, these the log aggregation, the unification,

285
00:18:01,058 --> 00:18:04,530
the basic transformation to make it meaningful,

286
00:18:04,690 --> 00:18:07,938
which is log stash primarily with a

287
00:18:07,964 --> 00:18:11,194
baby brother of beats, which is ideal for

288
00:18:11,232 --> 00:18:15,142
very, very small footprint deployments. You've got elasticsearch,

289
00:18:15,206 --> 00:18:18,858
which is your analytics and search engine, and then Kibana gives you

290
00:18:18,864 --> 00:18:22,750
the visualization. But there is an alternate stack that's coming

291
00:18:22,820 --> 00:18:26,394
up getting more and more mention and that's the EFK,

292
00:18:26,522 --> 00:18:30,090
which is essentially the same thing, except we now replace log stash

293
00:18:30,170 --> 00:18:34,490
with fluentd. Now the reason for doing or considering

294
00:18:34,570 --> 00:18:38,482
doing that replacement and moving from milk to fc is the fact

295
00:18:38,536 --> 00:18:43,150
that logs stash, whilst it's a good tool, has a bigger footprint.

296
00:18:43,230 --> 00:18:46,566
It also has the disadvantage, the fact it is a

297
00:18:46,588 --> 00:18:50,566
product aligned to a vendor in these form of elastic. Nothing wrong with

298
00:18:50,588 --> 00:18:54,422
that in itself, except elastic perhaps don't make

299
00:18:54,476 --> 00:18:58,302
it or encourage the development of plugins,

300
00:18:58,466 --> 00:19:02,474
which means that the number of systems that logstash can reach

301
00:19:02,592 --> 00:19:05,990
is smaller than fluentd by quite a margin.

302
00:19:06,150 --> 00:19:09,930
So that's one of the key benefits of considering moving over

303
00:19:10,080 --> 00:19:13,754
to the fluentd approach. There is also the fact that

304
00:19:13,872 --> 00:19:17,134
if you're working with Docker, there are pre built

305
00:19:17,252 --> 00:19:21,354
drivers to take docker logs and just pump them to fluentd.

306
00:19:21,482 --> 00:19:24,922
And again, there's a lot of work being made available to package

307
00:19:24,986 --> 00:19:28,910
up and make fluentd available to kubernetes

308
00:19:28,990 --> 00:19:32,434
based environments out of the box or

309
00:19:32,472 --> 00:19:35,750
very close to being out of the box, because you do need to obviously apply

310
00:19:35,820 --> 00:19:39,042
a few configuration items like where your servers

311
00:19:39,106 --> 00:19:42,982
are. So theyre the differences there. There's nothing

312
00:19:43,036 --> 00:19:46,386
wrong with elk, but there are more opportunities,

313
00:19:46,498 --> 00:19:50,306
let's put it that way, with the fluentd based stack

314
00:19:50,418 --> 00:19:54,406
and they work equally well. They just need slightly different configs

315
00:19:54,438 --> 00:19:57,482
at the bottom. So I'm going to do a little demo in a moment.

316
00:19:57,536 --> 00:20:00,666
I'm going to run it for you, but let's talk about what the

317
00:20:00,688 --> 00:20:04,126
demo does just to give you some real world events of the art of the

318
00:20:04,148 --> 00:20:08,094
possible. I'm bring to start with a setup where I've got,

319
00:20:08,212 --> 00:20:12,202
let's say a couple of applications running in a Kubernetes

320
00:20:12,266 --> 00:20:15,714
environment. They could be running in a vm or running on

321
00:20:15,752 --> 00:20:19,966
bare metal. It really doesn't matter particularly fluentd

322
00:20:20,078 --> 00:20:23,502
does not care. It works in legacy environments

323
00:20:23,566 --> 00:20:28,030
as equally well as the most modern containerization setups.

324
00:20:28,190 --> 00:20:31,946
It just happens that those people on the containerisation

325
00:20:31,998 --> 00:20:35,554
end of that scale perhaps are a bit more aware of fluentd.

326
00:20:35,602 --> 00:20:38,786
And what we got in our demo is we've got what's

327
00:20:38,818 --> 00:20:42,474
called a pipeline, which is going to take one log file been

328
00:20:42,512 --> 00:20:45,766
generated or simulated. I don't actually have real applications.

329
00:20:45,798 --> 00:20:49,382
I'm going to run what I've built as a log simulator,

330
00:20:49,526 --> 00:20:53,662
which is a utility that can generate log events or play

331
00:20:53,716 --> 00:20:57,294
back old log files and restamp them so it looks

332
00:20:57,332 --> 00:21:00,958
like it's can active live log. And we'll see the log

333
00:21:01,124 --> 00:21:04,462
files being filled out in real

334
00:21:04,516 --> 00:21:07,746
time, rather than it being one great big lump that can

335
00:21:07,768 --> 00:21:11,282
be passed instantly. And in that file I'm going to do

336
00:21:11,336 --> 00:21:14,674
a little bit manipulation, because in one file, as you will

337
00:21:14,712 --> 00:21:18,770
see in a minute, it describes as payload using one attribute,

338
00:21:18,850 --> 00:21:22,034
and in the other application file it describes

339
00:21:22,082 --> 00:21:25,974
it slightly differently. And what I want to do downstream is

340
00:21:26,012 --> 00:21:29,062
to process the log events in the same way.

341
00:21:29,116 --> 00:21:32,150
And it also just pumps out the log events to a file.

342
00:21:32,230 --> 00:21:36,390
So we got an audit trail of how log events have been tweaked.

343
00:21:36,550 --> 00:21:40,106
And that could be one node or it could be many. Okay,

344
00:21:40,208 --> 00:21:44,074
so that's the aggregation and we can scale that out

345
00:21:44,192 --> 00:21:48,202
and we just get more and more fluentd nodes doing that preprocessing.

346
00:21:48,266 --> 00:21:52,046
So I'm going to do with it, I'm going to do the fun, funky thing,

347
00:21:52,068 --> 00:21:55,474
because what I often encounter is a lot of people think about

348
00:21:55,512 --> 00:21:58,770
logs as things to search after an event,

349
00:21:58,840 --> 00:22:02,366
but we should be and can be preemptive. If you've

350
00:22:02,398 --> 00:22:06,066
got a log event that indicates something is going to become a

351
00:22:06,088 --> 00:22:09,746
problem. And this is a great way of dealing with legacy applications.

352
00:22:09,858 --> 00:22:13,590
Over time you'll understand what the log messages mean. And sometimes

353
00:22:13,660 --> 00:22:17,320
you get these little log events that look quite

354
00:22:18,010 --> 00:22:21,866
innocent but actually indicate a serious issue. And one

355
00:22:21,888 --> 00:22:24,714
of the things we can do with fluent D is actually process it and go

356
00:22:24,752 --> 00:22:28,186
actually, whilst that logs innocent, it's a precursor to

357
00:22:28,208 --> 00:22:32,026
a very very significant issue and therefore you can act upon it

358
00:22:32,128 --> 00:22:35,498
before your system collapses because something is broken.

359
00:22:35,594 --> 00:22:38,426
In the demo, we're going to have just one node running that and one node

360
00:22:38,458 --> 00:22:41,354
running on the source, both on the same machine.

361
00:22:41,482 --> 00:22:45,146
One of the things that I mentioned fluent and fluent bit its baby

362
00:22:45,178 --> 00:22:48,706
brother, are tiny footprints. So you can run all of this in

363
00:22:48,728 --> 00:22:52,306
these most simplistic of machines if you

364
00:22:52,328 --> 00:22:55,378
want. And what we're going to be doing on this second node is again

365
00:22:55,464 --> 00:22:58,950
filtering it out, going to write all the logs to a standard

366
00:22:59,020 --> 00:23:02,674
out, which could be a more practical use case of warboard.

367
00:23:02,722 --> 00:23:05,974
And I'm going to send to slack anything that has a

368
00:23:06,012 --> 00:23:09,526
particular context in the log event. So you can see

369
00:23:09,628 --> 00:23:13,466
it pushing a message to me very, very quickly. And of course therefore we

370
00:23:13,488 --> 00:23:17,114
need to run this across a network of some sort. I have talked about

371
00:23:17,152 --> 00:23:21,354
and mentioned these existence of elasticsearch and Prometheus

372
00:23:21,402 --> 00:23:25,310
and Sim. We could equally also route these events to

373
00:23:25,380 --> 00:23:29,434
an analytics engine for looking for anomalous

374
00:23:29,482 --> 00:23:33,226
behaviors and for doing other mining of log events

375
00:23:33,258 --> 00:23:36,722
as well. And we could separate that out that we can do both.

376
00:23:36,856 --> 00:23:40,594
It's not of one or the other. We're not tying our notifications and

377
00:23:40,632 --> 00:23:44,386
alerts to the analysis these which if

378
00:23:44,408 --> 00:23:48,162
you used splunk or tools like that, they will alert

379
00:23:48,226 --> 00:23:51,638
off the analysis these once they've ingested the

380
00:23:51,644 --> 00:23:54,546
log event and theyre run their queries.

381
00:23:54,658 --> 00:23:57,686
But for today I'm not going to do that. I'm just going to concentrate on

382
00:23:57,708 --> 00:24:01,002
this little piece just because it's quite funky and it gives you something

383
00:24:01,056 --> 00:24:04,890
fairly visual to see. So let me show you the setup.

384
00:24:05,470 --> 00:24:08,810
This is the configuration file for these first

385
00:24:08,880 --> 00:24:12,078
server node one which is generating the source. As you

386
00:24:12,084 --> 00:24:15,818
can see it's a combination of XML style teams

387
00:24:15,914 --> 00:24:19,498
and name value pairs. It's a bit strange

388
00:24:19,594 --> 00:24:22,654
to start with, but the moment you get your head round it,

389
00:24:22,772 --> 00:24:26,114
it's incredibly easy to work with. Here are the two

390
00:24:26,152 --> 00:24:30,414
sources. You can see two different files coming from two different solutions,

391
00:24:30,542 --> 00:24:34,194
and I've set it up so that we can track them. So if

392
00:24:34,232 --> 00:24:37,770
my fluentd node was to be stopped temporarily,

393
00:24:37,870 --> 00:24:41,634
or it gets reconfigured or moved around the network,

394
00:24:41,762 --> 00:24:44,886
it will know where to pick up and carry on.

395
00:24:44,988 --> 00:24:48,726
It can deal with all sorts of things like log rotation and stuff like that.

396
00:24:48,828 --> 00:24:52,826
And it's going to convert the payload. Let's move on. And the

397
00:24:52,848 --> 00:24:56,906
next bit is filter, which is going to actually

398
00:24:57,008 --> 00:25:00,406
apply a record change. Now all this is doing is because they're

399
00:25:00,438 --> 00:25:03,774
coming in with slightly different structures. This is going to

400
00:25:03,812 --> 00:25:07,210
adjust the log event so it has the same structure.

401
00:25:07,290 --> 00:25:10,682
In one message. I'm bringing the payload, the core payload

402
00:25:10,746 --> 00:25:14,686
in with a tag called event and the other one is message and

403
00:25:14,708 --> 00:25:18,434
I want to make it consistent and I'm going to actually just flag that

404
00:25:18,472 --> 00:25:22,674
it's transformed and then I'm going to do the routing and

405
00:25:22,712 --> 00:25:26,690
sending of the events. As you can see, I've got these file store

406
00:25:26,760 --> 00:25:30,102
there and I've got a little bit of buffering going on, just so the I

407
00:25:30,156 --> 00:25:33,734
O is a bit more efficient. And then on the right

408
00:25:33,772 --> 00:25:37,618
hand side you can see I'm sending to another node on my

409
00:25:37,724 --> 00:25:41,478
machine every 5 seconds the log events

410
00:25:41,654 --> 00:25:45,194
for the second node to process, which gives

411
00:25:45,232 --> 00:25:48,678
us this configuration. And you can see now I'm

412
00:25:48,694 --> 00:25:53,150
saying, okay, I'm going to accept from anywhere this is a pretty promiscuous

413
00:25:53,570 --> 00:25:56,590
ingestion of log events that have been transmitted.

414
00:25:56,930 --> 00:26:00,206
I'm then going to filter it out and I'm looking for anything that has got

415
00:26:00,228 --> 00:26:04,046
a reference to computer, so it can be

416
00:26:04,228 --> 00:26:08,100
a lowercase or an uppercase c for computer or anything like that

417
00:26:08,470 --> 00:26:12,258
out of my log events. And then if it's got that value,

418
00:26:12,424 --> 00:26:15,734
it will go to my slack channel. And just

419
00:26:15,772 --> 00:26:19,826
to show you, here's this open source utility I've built for helping

420
00:26:19,858 --> 00:26:22,866
to simulate application logging.

421
00:26:22,978 --> 00:26:26,786
It means that we can configure and test our fluent

422
00:26:26,818 --> 00:26:30,634
d pipelines without needing to run these real life

423
00:26:30,672 --> 00:26:33,834
systems. And if we want to see what happens when an

424
00:26:33,872 --> 00:26:37,546
error occurs, we can ply log file with errors in

425
00:26:37,648 --> 00:26:41,198
into the environment as if it was live and

426
00:26:41,284 --> 00:26:44,830
see or confirm that the right alerts are being generated

427
00:26:45,170 --> 00:26:48,798
and whether our analytics engines is going

428
00:26:48,804 --> 00:26:51,642
to be able to pick up these details.

429
00:26:51,786 --> 00:26:55,106
And as you can see, importantly here is how I

430
00:26:55,128 --> 00:26:58,386
am structuring the payload. And you can

431
00:26:58,408 --> 00:27:01,774
see I'm saying it's going to be an event. And that's the payload

432
00:27:01,902 --> 00:27:06,002
log payload. And over here I've called it message

433
00:27:06,136 --> 00:27:10,338
and I've put it in a slightly different sequence in the file,

434
00:27:10,434 --> 00:27:14,258
but it will find the message piece and do the analysis

435
00:27:14,354 --> 00:27:17,766
to determine whether it talks about computer. And this is what

436
00:27:17,788 --> 00:27:21,142
we're going to see in a moment. So let me drop out of the presentation

437
00:27:21,206 --> 00:27:25,002
deck and we'll see this happening for real.

438
00:27:25,136 --> 00:27:28,970
I am bring to just run a little startup script

439
00:27:30,670 --> 00:27:33,994
which is going to fire up a number of shell

440
00:27:34,042 --> 00:27:37,262
windows which are going to give me the two log

441
00:27:37,316 --> 00:27:41,386
generators which you can see here. One is going to be fairly

442
00:27:41,418 --> 00:27:45,230
quiet. One is going to be very chatty because I've got the different settings.

443
00:27:45,310 --> 00:27:48,258
Then I've got two fluent D nodes. There's server one,

444
00:27:48,344 --> 00:27:49,860
here's server two.

445
00:27:51,510 --> 00:27:55,854
And you can see it's just reported the console,

446
00:27:55,902 --> 00:27:59,654
the configuration file it's picked up and what it's up

447
00:27:59,692 --> 00:28:03,314
to. And you can see over here on fluentd node

448
00:28:03,362 --> 00:28:07,126
two it's actually receiving the log events and

449
00:28:07,148 --> 00:28:10,570
theyre all coming through with message in the title.

450
00:28:10,990 --> 00:28:15,450
All right. And up here you can see that's very quiet.

451
00:28:15,950 --> 00:28:19,162
This one is being verbose and it's showing me the log

452
00:28:19,216 --> 00:28:22,990
event source file it's processing and

453
00:28:23,060 --> 00:28:26,622
how many times it's looping through that data set to create

454
00:28:26,676 --> 00:28:30,320
log events. Just to understand,

455
00:28:31,330 --> 00:28:35,342
rather than playing real log files or application logs and generating

456
00:28:35,406 --> 00:28:39,202
them, I've set my tool up to run from

457
00:28:39,256 --> 00:28:42,882
a mock data set which is just basically

458
00:28:42,936 --> 00:28:46,422
full of jokes. And what we've got over here,

459
00:28:46,476 --> 00:28:49,990
this is my slack desktop client

460
00:28:50,730 --> 00:28:54,678
and you can see here that

461
00:28:54,844 --> 00:28:58,674
periodically it will scroll as new messages

462
00:28:58,722 --> 00:29:02,134
come in. And it's just the messages

463
00:29:02,182 --> 00:29:05,434
that have got a reference to computer. So you can see, there you go.

464
00:29:05,552 --> 00:29:09,290
Node two says oldest computer can be traced back to Adam and Eve.

465
00:29:09,630 --> 00:29:12,240
And you can see every one of these.

466
00:29:16,290 --> 00:29:21,070
Whilst these are just junk jokes

467
00:29:21,410 --> 00:29:25,282
being pushed through, you could imagine that this is a genuine log event.

468
00:29:25,416 --> 00:29:29,602
And whilst I'm just showing what the text is, you could very easily say okay,

469
00:29:29,736 --> 00:29:32,946
when I identify this event, I'm going to

470
00:29:32,968 --> 00:29:36,054
put a more meaningful message in. You could

471
00:29:36,092 --> 00:29:39,254
potentially put hyperlink into your

472
00:29:39,292 --> 00:29:42,390
ops processes to how do I resolve this?

473
00:29:42,540 --> 00:29:46,258
And things like that. So that's

474
00:29:46,434 --> 00:29:50,460
what it's doing. And if I come into here,

475
00:29:52,190 --> 00:29:55,100
what we're going to see, if I just stop that,

476
00:29:55,870 --> 00:29:59,050
if I go into the demo folder,

477
00:30:01,250 --> 00:30:05,022
we can see there is log

478
00:30:05,076 --> 00:30:08,746
file being generated and there's

479
00:30:08,778 --> 00:30:12,358
a folder for it, for it. So one of the logs

480
00:30:12,474 --> 00:30:15,746
we've got is log rotating. So if

481
00:30:15,768 --> 00:30:17,090
I go to label,

482
00:30:21,510 --> 00:30:24,900
label pipeline and we look in here,

483
00:30:25,930 --> 00:30:28,120
I haven't left it running long enough,

484
00:30:33,050 --> 00:30:36,822
but you'll see given

485
00:30:36,876 --> 00:30:39,190
enough time it will do log rotation.

486
00:30:40,510 --> 00:30:44,534
And you can see the two position files that I mentioned. So it's tracking

487
00:30:44,582 --> 00:30:48,266
what's going on. So if I restart stopped and restarted, they would

488
00:30:48,288 --> 00:30:51,260
carry on consuming from where they left off.

489
00:30:52,110 --> 00:30:55,946
Okay, I can see it's just on a log rotate there for label

490
00:30:55,978 --> 00:30:59,200
pipeline file output. And there's one and there's two.

491
00:30:59,810 --> 00:31:03,134
So there we go. Let me take you back

492
00:31:03,172 --> 00:31:06,814
to the deck and we'll wrap up. So one of the things

493
00:31:06,852 --> 00:31:10,238
that's, yep, that's fine, that's a little bit Mickey Mouse, but you'll get a

494
00:31:10,244 --> 00:31:13,790
sense of the art of the possible there. And certainly in terms of making

495
00:31:13,860 --> 00:31:17,446
things proactive, why filtering out log events is certainly

496
00:31:17,548 --> 00:31:20,742
a possibility, but in the real world we've got to deal with things like

497
00:31:20,796 --> 00:31:24,514
scaling. I mentioned the complexities of highly distributed

498
00:31:24,562 --> 00:31:28,054
solutions and the fact that we want to bring logs together from

499
00:31:28,092 --> 00:31:31,754
logs of different sources to be able to analyze it properly end

500
00:31:31,792 --> 00:31:35,654
to end. Are we seeing messages, know API

501
00:31:35,702 --> 00:31:39,434
calls coming in that hit our API gateway, but never seem to move

502
00:31:39,472 --> 00:31:43,374
beyond that? Things like that. So as I say, lots of

503
00:31:43,492 --> 00:31:47,374
challenges. There is a pretty modern piece of thinking around

504
00:31:47,492 --> 00:31:50,942
what's known as open tracing, which is the idea that we can start

505
00:31:50,996 --> 00:31:54,638
to create a means to trace events through the different

506
00:31:54,724 --> 00:31:57,874
applications. We got to deal with the possibility that in

507
00:31:57,912 --> 00:32:01,086
a lot of organizations, different teams with different jobs

508
00:32:01,118 --> 00:32:04,466
will want to use different tools. And in that kind

509
00:32:04,488 --> 00:32:08,150
of situation, you either have one tool pulling all the logs together

510
00:32:08,220 --> 00:32:12,322
and sending them across to splunk or whatever the dbas

511
00:32:12,386 --> 00:32:16,290
are using, or the infra guys if they're using nadias,

512
00:32:16,370 --> 00:32:20,522
or you end up with your database server having three different

513
00:32:20,576 --> 00:32:24,022
agents, which is a bit overkill. And of course the network

514
00:32:24,086 --> 00:32:28,042
setup is that much more complicated as well. Whereas if you've got a

515
00:32:28,096 --> 00:32:31,694
log unification engine like fluentd or log stash for that

516
00:32:31,732 --> 00:32:35,166
matter, picking all this information up, sending it over

517
00:32:35,268 --> 00:32:38,810
to these right places, or sending it to a central distribution

518
00:32:38,890 --> 00:32:42,266
point, then your network is a lot more controlled

519
00:32:42,378 --> 00:32:45,618
and your security guys are going to be a lot happier. I mentioned

520
00:32:45,704 --> 00:32:49,650
briefly these idea that doing this sort of thing, we can these address

521
00:32:49,720 --> 00:32:53,310
some of our operational monitoring challenges around legacy applications

522
00:32:53,390 --> 00:32:57,490
where people don't want you anywhere near it. So you can just

523
00:32:57,640 --> 00:33:01,226
start interpreting the logs and saying, well, actually this innocent logs

524
00:33:01,278 --> 00:33:04,502
log message is significant, or when it reports this, it's actually

525
00:33:04,556 --> 00:33:07,746
the root cause is that those more delicate systems,

526
00:33:07,778 --> 00:33:11,466
you can do more preventative action rather than saying it's in

527
00:33:11,488 --> 00:33:14,938
kubernetes, it will recover and sort it out for us.

528
00:33:15,024 --> 00:33:18,282
So here's a scaled deployment where you can see,

529
00:33:18,336 --> 00:33:22,186
I've got five servers across the top. Now we're collecting different types

530
00:33:22,218 --> 00:33:25,466
of logs, different information from these servers, and they're

531
00:33:25,498 --> 00:33:29,050
sending it to a concentration point or two concentration

532
00:33:29,130 --> 00:33:32,362
points, which are then processing those logs

533
00:33:32,506 --> 00:33:35,934
and doing what's necessary. The ops alerting,

534
00:33:35,982 --> 00:33:40,274
potentially the very minimum, bringing it all together in a common platform

535
00:33:40,392 --> 00:33:43,854
for doing the analytics, you can see the dash lines

536
00:33:43,902 --> 00:33:47,702
where we could put failover and resilience into the setup as well.

537
00:33:47,756 --> 00:33:51,494
So if concentrator node in the form of one of these

538
00:33:51,612 --> 00:33:54,610
mid tier servers is enabling,

539
00:33:54,690 --> 00:33:57,926
then the sources of the logs will look for

540
00:33:57,948 --> 00:34:01,274
the next node that you've identified and use that.

541
00:34:01,312 --> 00:34:04,662
And that can then start passing the logs through so you're not losing

542
00:34:04,726 --> 00:34:08,854
stuff. The important thing is that's classic deployment. There's no virtualization

543
00:34:08,982 --> 00:34:12,350
here theyre is no containerization going on.

544
00:34:12,420 --> 00:34:15,866
But I could make this containerized or virtualized.

545
00:34:16,058 --> 00:34:19,534
And if you looked at this from the perspective of

546
00:34:19,572 --> 00:34:23,054
a containerization model, on the left I've

547
00:34:23,102 --> 00:34:26,866
got the apps now just spitting out to system out or

548
00:34:26,888 --> 00:34:30,498
a file that's set up appropriately in

549
00:34:30,664 --> 00:34:34,658
the container environment. And then we have a

550
00:34:34,744 --> 00:34:38,198
demon set, as kubernetes refers to it,

551
00:34:38,364 --> 00:34:41,814
running fluentd, picking up the logs from the different

552
00:34:41,852 --> 00:34:45,298
bits of kubernetes, plus these log capture,

553
00:34:45,394 --> 00:34:48,706
that is going to standard out and system out and assuage

554
00:34:48,738 --> 00:34:52,346
standard error that's happening in the applications in their pods and

555
00:34:52,368 --> 00:34:56,058
sorting that out and getting those routed to the right places. Or you

556
00:34:56,064 --> 00:35:00,262
can go to the next level and start colocating

557
00:35:00,406 --> 00:35:04,286
either fluentd or fluent bit into a pod, which is

558
00:35:04,308 --> 00:35:08,142
what you've got on the right. And we could adapt the

559
00:35:08,196 --> 00:35:11,662
other pods to be more like that. Or we could even go

560
00:35:11,716 --> 00:35:14,846
to using containerization patterns like

561
00:35:15,028 --> 00:35:18,314
sidecars to deploy our fluent

562
00:35:18,362 --> 00:35:21,566
bit or fluentd nodes. That gives us enormous amounts

563
00:35:21,598 --> 00:35:25,186
of scaling. The other thing to keep in mind is that a

564
00:35:25,208 --> 00:35:29,046
lot of people like Prometheus and Grafana for visualizing and

565
00:35:29,068 --> 00:35:32,390
reporting on their activities and what's going on.

566
00:35:32,460 --> 00:35:36,166
Nothing to stop you from doing that. Fluentd becomes a

567
00:35:36,268 --> 00:35:40,438
contributor to the whole process. This is the architecture

568
00:35:40,534 --> 00:35:44,442
for a Prometheus setup. And you can see on the left hand

569
00:35:44,496 --> 00:35:48,762
side we've got fluentd feeding details into

570
00:35:48,896 --> 00:35:52,570
the core server and taking, we've consolidated

571
00:35:52,730 --> 00:35:56,414
data and Prometheus monitoring data out on

572
00:35:56,452 --> 00:36:00,174
an export on the left hand side. And of course if Prometheus is

573
00:36:00,212 --> 00:36:03,322
filtering out or doing correlation analysis,

574
00:36:03,386 --> 00:36:07,278
it could generate new log alerts and you could then use fluentd

575
00:36:07,374 --> 00:36:10,814
to route those to all the different systems. So in addition

576
00:36:10,862 --> 00:36:14,414
to the raw data, you could be taking the Prometheus alerts

577
00:36:14,462 --> 00:36:18,546
and combining that with your log analytics

578
00:36:18,658 --> 00:36:21,926
platform, for example. So not only do you end

579
00:36:21,948 --> 00:36:26,262
up with a history of what happened, you can put into your

580
00:36:26,316 --> 00:36:29,954
core analytics log analytics when it was recognized

581
00:36:30,002 --> 00:36:33,882
that there was a problem and alerted. So you can then show when something

582
00:36:33,936 --> 00:36:37,130
was addressed or should have been addressed in the process.

583
00:36:37,280 --> 00:36:40,586
Yeah, that could even be. Whilst Prometheus is done in,

584
00:36:40,608 --> 00:36:44,490
fluentd is actually kicking off an active process to remediate.

585
00:36:44,570 --> 00:36:48,490
Now this is a platform set up that has been shown logically

586
00:36:48,570 --> 00:36:52,334
that we've established in the past. It's a real use

587
00:36:52,372 --> 00:36:55,506
case. And across the bottom I've put the lifecycle that

588
00:36:55,528 --> 00:36:58,286
I introduced earlier on into the diagram.

589
00:36:58,398 --> 00:37:02,014
And on the left hand side you can see the different types of sources

590
00:37:02,062 --> 00:37:06,382
that we're handling. So we have got kubernetes clusters

591
00:37:06,446 --> 00:37:10,134
involved and we are running a demon set. So we're using fluent D rather

592
00:37:10,172 --> 00:37:13,926
than fluent bit because it's a little bit more usable for

593
00:37:13,948 --> 00:37:17,394
that kind of use case. But we've also got dedicated virtual

594
00:37:17,442 --> 00:37:20,950
machines. We are running this particular use case

595
00:37:21,020 --> 00:37:24,794
in a multi cloud setup. So we are doing cloud watch,

596
00:37:24,912 --> 00:37:28,522
which is giving us specific information from that cloud

597
00:37:28,576 --> 00:37:31,914
vendor. When we're pulling that and combining

598
00:37:31,962 --> 00:37:36,254
that with other sources of information such

599
00:37:36,292 --> 00:37:40,206
as logs for J. So we've taken a legacy app that's been migrated to

600
00:37:40,228 --> 00:37:43,806
the cloud and rather than trying to configure it all so that that

601
00:37:43,828 --> 00:37:47,266
goes into cloud watch, we've just said, okay, we'll leave it

602
00:37:47,288 --> 00:37:51,074
as is untouched and we'll put a fluent bit node in play

603
00:37:51,192 --> 00:37:54,258
and there's a bit of open tracing going on.

604
00:37:54,344 --> 00:37:58,006
So we've got the Jaeger collectors and Jaeger analytics to help with

605
00:37:58,028 --> 00:38:01,410
the tracing, showing these performance of open tracing

606
00:38:01,490 --> 00:38:04,806
information. And it's complementary. It's not instead

607
00:38:04,908 --> 00:38:08,746
of, and we could easily put that into fluentd as well to

608
00:38:08,768 --> 00:38:12,774
do further processing on those trace

609
00:38:12,822 --> 00:38:16,326
steps. And of course we want to analyze it. So we've

610
00:38:16,358 --> 00:38:20,394
got the elasticsearch and Kibana for visualization and we've

611
00:38:20,442 --> 00:38:24,238
started to put a few small alerts into

612
00:38:24,324 --> 00:38:27,774
a slack channel and a secondary email that

613
00:38:27,812 --> 00:38:31,098
could easily be pager duty dealing

614
00:38:31,114 --> 00:38:34,366
with the fact that you might be running around these clock and who's on

615
00:38:34,388 --> 00:38:37,406
call overnight rather than just slacking. Everybody.

616
00:38:37,588 --> 00:38:40,762
And that's me done. Thank you very much. Thank you for listening.

617
00:38:40,826 --> 00:38:44,142
I hope that was really helpful. If you want to know more

618
00:38:44,196 --> 00:38:47,622
then please visit my website. There's plenty more

619
00:38:47,676 --> 00:38:51,510
information there. The slides will be available and

620
00:38:51,660 --> 00:38:55,318
I can be contacted through the site if you're interested

621
00:38:55,404 --> 00:38:57,940
in allowing more. Otherwise, thank you very much.

