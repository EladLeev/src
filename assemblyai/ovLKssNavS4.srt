1
00:00:22,010 --> 00:00:25,734
Today we will talk about deploying ML solutions with low latency in python in.

2
00:00:25,932 --> 00:00:28,600
You can find me at my LinkedIn address mentioned here.

3
00:00:29,130 --> 00:00:32,806
So, as the world moves forward with these research in improving accuracies of

4
00:00:32,828 --> 00:00:36,306
deep learning algorithms, we face an imminent problem of deploying

5
00:00:36,338 --> 00:00:39,414
those algorithms. So, as many of my fellow

6
00:00:39,462 --> 00:00:43,574
researchers might know, everyone here present is involved

7
00:00:43,622 --> 00:00:47,610
in, directly or indirectly, in improving the algorithms that we currently

8
00:00:47,680 --> 00:00:51,434
use today. However, these developers that are

9
00:00:51,472 --> 00:00:55,274
using those algorithms often face the problem of making those algorithms

10
00:00:55,322 --> 00:00:58,846
real time, even after doing a bunch of stuff that we'll be

11
00:00:58,868 --> 00:01:02,474
discussing today. So what do I mean by low

12
00:01:02,532 --> 00:01:06,354
latency? So, it is a term used

13
00:01:06,392 --> 00:01:09,166
to describe the performance of can ML pipeline,

14
00:01:09,358 --> 00:01:13,666
and it's basically the time taken to process data

15
00:01:13,848 --> 00:01:17,526
by the ML algorithms. It's like a

16
00:01:17,548 --> 00:01:20,774
time taken to process a single image or a single video,

17
00:01:20,892 --> 00:01:24,600
and highthroughput is the time taken to process these entire data,

18
00:01:24,970 --> 00:01:28,326
and latency is inversely proportional to throughput. And our

19
00:01:28,348 --> 00:01:31,574
focus in this talk will be to convert

20
00:01:31,622 --> 00:01:35,034
or deploy our ML pipelines in low latency. So as you can see

21
00:01:35,072 --> 00:01:38,906
in this graph, entire ML pipeline is it starts with the

22
00:01:38,928 --> 00:01:42,658
input data and it goes through several steps before we can get the output.

23
00:01:42,774 --> 00:01:46,446
And these. The bottleneck we face most of the

24
00:01:46,468 --> 00:01:50,094
time is the model inference. And there are several methods by which we

25
00:01:50,132 --> 00:01:54,186
can improve the latency of the algorithms. First, one is weight quantization,

26
00:01:54,378 --> 00:01:57,746
wherein you can deploy the model

27
00:01:57,848 --> 00:02:01,422
in multiple quantization methods using multiple quantization

28
00:02:01,486 --> 00:02:05,026
methods such as float 16 int eight. There are

29
00:02:05,048 --> 00:02:08,462
two types of quantizations. One is post training quantization,

30
00:02:08,526 --> 00:02:11,846
and one is quantization aware training. I cannot go in detail in

31
00:02:11,868 --> 00:02:15,878
each of these methods as the time won't permit me to do so. But basically

32
00:02:15,964 --> 00:02:19,838
you can use any of those to check which method

33
00:02:19,874 --> 00:02:23,814
is giving you better accuracies. And quantization often involves

34
00:02:23,862 --> 00:02:27,190
you to deal with accuracies and speed versus speed.

35
00:02:27,270 --> 00:02:30,294
So in case you go with int eight, you will get much better speed.

36
00:02:30,342 --> 00:02:33,666
However, there will be a drop in accuracy, and I generally

37
00:02:33,718 --> 00:02:37,386
find that float 16 is the best method to go because it preserves

38
00:02:37,418 --> 00:02:41,054
my accuracy and it also gives me like 1.5

39
00:02:41,092 --> 00:02:45,220
or two times of what float 32 gives me. The second method that

40
00:02:45,670 --> 00:02:49,086
is often used is model pruning. So the basic concept

41
00:02:49,118 --> 00:02:52,494
is that you prune certain layers and connections of the models

42
00:02:52,542 --> 00:02:56,114
based on several experiments. You run your

43
00:02:56,152 --> 00:02:59,554
model with multiple and like a bunch of images

44
00:02:59,602 --> 00:03:03,298
to see which layers can be omitted

45
00:03:03,394 --> 00:03:06,934
and can be skipped so that these parameters won't be

46
00:03:06,972 --> 00:03:10,614
calculated by this way. Sometimes you can remove like

47
00:03:10,652 --> 00:03:14,134
99% of your connections and you can still preserve your accuracy,

48
00:03:14,182 --> 00:03:17,654
but that is like the best case solution. Another method that is quite popular

49
00:03:17,702 --> 00:03:21,606
today is knowledge distillation, and it is the concept wherein you are transferring

50
00:03:21,638 --> 00:03:25,226
the knowledge of a bigger model into a smaller model. So suppose

51
00:03:25,258 --> 00:03:29,422
you have a Stanford car data set and you have trained a Resnet 151 and

52
00:03:29,476 --> 00:03:33,262
you get a testing classification accuracy of 95%.

53
00:03:33,396 --> 00:03:36,642
Now when you train a REsnet 18 on the similar data on the same data

54
00:03:36,696 --> 00:03:40,542
set, you might not find that it is giving you a 95% accuracy.

55
00:03:40,606 --> 00:03:44,274
You often see that it is limited to 80% to 90% because

56
00:03:44,312 --> 00:03:47,574
of the shallow depth of it. So with using

57
00:03:47,612 --> 00:03:51,382
knowledge distillation, what you can do is you can transfer the knowledge learned by

58
00:03:51,436 --> 00:03:55,078
Resnet 151 and

59
00:03:55,164 --> 00:03:58,946
use Resnet 18 as if the like use resonate

60
00:03:58,978 --> 00:04:02,534
18 with 95% of accuracies. And the fourth method

61
00:04:02,582 --> 00:04:06,518
that is my topic of the talk today, that is framework based deployment.

62
00:04:06,694 --> 00:04:10,790
So we will be seeing two frameworks, Tensorrt and deep

63
00:04:10,870 --> 00:04:14,986
steam, and we'll be seeing how they help us in deploying these algorithms.

64
00:04:15,178 --> 00:04:18,880
So what is Tensorrt? Tensorrt is

65
00:04:19,410 --> 00:04:23,098
SDK for high performance deep learning inference. It is provided

66
00:04:23,114 --> 00:04:27,362
by Nvidia and they have written the entire tensorrt in C

67
00:04:27,496 --> 00:04:31,246
with Python bindings available. It includes a deep learning inference

68
00:04:31,278 --> 00:04:35,246
optimizer and runtime. So the optimizer's job is to optimize

69
00:04:35,278 --> 00:04:39,234
your model, like when you are converting it to tensorrt, it will optimize

70
00:04:39,282 --> 00:04:43,666
the entire model and it will convert the layers using advanced

71
00:04:43,698 --> 00:04:47,474
CUDA methods in C. And the runtime is responsible

72
00:04:47,522 --> 00:04:51,866
for actually running your tensorrt engines. So Tensorrt often

73
00:04:52,048 --> 00:04:55,974
delivers low latency and high throughput for several deep learning applications.

74
00:04:56,102 --> 00:04:59,514
I have tried Tensorrt in the industry and it works great.

75
00:04:59,712 --> 00:05:03,674
It supports both Python and C. And nowadays

76
00:05:03,722 --> 00:05:07,150
Tensorrt supports conversion from multiple frameworks such as

77
00:05:07,220 --> 00:05:09,802
Tensorflow, Pytorch, Mixnet, Theano,

78
00:05:09,946 --> 00:05:13,054
Onnx, etc. For reference, I have

79
00:05:13,172 --> 00:05:17,038
linked the tensorrt's official documentation and developer page

80
00:05:17,124 --> 00:05:20,770
below. So how does Tensorrt do the

81
00:05:20,840 --> 00:05:24,686
entire thing? So it is responsible for optimizing

82
00:05:24,718 --> 00:05:28,486
your model, and it does so using the method shown here.

83
00:05:28,588 --> 00:05:32,802
What it does is it does layer in tensorfusion,

84
00:05:32,946 --> 00:05:36,454
it does kernel auto tuning, it does precision calibration, it does

85
00:05:36,492 --> 00:05:39,538
dynamic tensor memory, it uses dynamic tensor memory,

86
00:05:39,634 --> 00:05:43,134
and it is also possible to use multi steam execution with Tensorrt.

87
00:05:43,202 --> 00:05:47,126
That is, you won't have to worry about like you can actually use batch processing

88
00:05:47,238 --> 00:05:50,942
for this using Tensorrt. And we'll be talking about more of these

89
00:05:50,996 --> 00:05:54,938
methods in further slides. So let's

90
00:05:55,034 --> 00:05:58,574
talk about weight and activation precision calibration. So,

91
00:05:58,612 --> 00:06:01,886
to quantize full precision information into intate while

92
00:06:01,908 --> 00:06:05,474
minimizing accuracies loss, Tensorrt must perform a process called

93
00:06:05,512 --> 00:06:08,958
calibration to determine how best to represent the weights

94
00:06:08,974 --> 00:06:12,402
and activations as eight bit integers. These calibration step

95
00:06:12,456 --> 00:06:15,854
requires you to provide tensorrt with a representative sample of the input

96
00:06:15,902 --> 00:06:19,574
training data, and no additional fine tuning or restraining of the model is

97
00:06:19,612 --> 00:06:23,014
necessary. Also, you don't need to have access to the entire training data

98
00:06:23,052 --> 00:06:26,854
set, you just give it a sample. Calibration is a

99
00:06:26,892 --> 00:06:30,698
completely automated and parameter free method for converting your model from

100
00:06:30,784 --> 00:06:34,438
f float 32 to NTA eight. What is kernel autotuning?

101
00:06:34,534 --> 00:06:37,574
So, during the optimization phase of Tensorrt,

102
00:06:37,702 --> 00:06:41,814
it also chooses from hundreds of specialized kernels that are created by default,

103
00:06:41,942 --> 00:06:45,354
and many of them are hand tuned and optimized for a range of parameters

104
00:06:45,402 --> 00:06:48,794
and target platforms. So as an example, there are several different algorithms

105
00:06:48,842 --> 00:06:52,926
to do convolutions. Tensorrt will pick the implementation from

106
00:06:52,948 --> 00:06:56,670
a library of kernels that delivers the best performance for the target gpu,

107
00:06:56,750 --> 00:07:00,862
input data size, filter size, tensorrt layout, batch size and other parameters.

108
00:07:01,006 --> 00:07:04,898
This ensures that the developed model is highperformance tuned for the specific

109
00:07:04,984 --> 00:07:08,686
development platform as well as for the specific neural network being deploying.

110
00:07:08,798 --> 00:07:12,386
So also, TensorRt is supposed to convert

111
00:07:12,418 --> 00:07:15,926
your models on a particular development platform. So you

112
00:07:15,948 --> 00:07:20,322
cannot run a TensorRt engine converted on suppose

113
00:07:20,466 --> 00:07:24,726
NVDiA's 1050 Ti and use it on a 2060 Ti.

114
00:07:24,838 --> 00:07:28,122
You have to do it on that particular GPU to be used on that particular

115
00:07:28,176 --> 00:07:31,398
GPU. So what is dynamic tensor memory?

116
00:07:31,574 --> 00:07:35,126
Tensorrt also reduces memory footprint and improves

117
00:07:35,238 --> 00:07:39,194
memory you reuse by designating memory for each tensor

118
00:07:39,242 --> 00:07:43,034
only for the duration of its use, avoiding memory allocation overhead

119
00:07:43,082 --> 00:07:46,146
for fast and efficient execution. So what

120
00:07:46,168 --> 00:07:49,906
is multi stream execution? So, as I mentioned before, it is basically the ability of

121
00:07:49,928 --> 00:07:53,230
tensorrt to process multiple input streams in parallel,

122
00:07:53,310 --> 00:07:56,734
and it does so beautifully.

123
00:07:56,862 --> 00:08:00,222
And what is layer and tensorfusion?

124
00:08:00,366 --> 00:08:04,358
So, tensorrt calibrates, while in

125
00:08:04,364 --> 00:08:07,462
the optimization step it calibrates your entire model,

126
00:08:07,516 --> 00:08:10,966
it sees the entire model, and it basically fuses

127
00:08:10,998 --> 00:08:14,362
several layers or tensors together, so that you

128
00:08:14,416 --> 00:08:17,754
reduce the parameter calculation and you reduce these

129
00:08:17,792 --> 00:08:21,434
multiple times. The data has to be passed from one layer to another. It is

130
00:08:21,472 --> 00:08:25,258
a single block. So suppose you have a convolution layer,

131
00:08:25,434 --> 00:08:28,686
activation function layer, and a fully connected layer in a

132
00:08:28,708 --> 00:08:32,126
network. What tensorrt it will do is it will combine all those three into

133
00:08:32,148 --> 00:08:35,346
a single module. So basically it reduces the

134
00:08:35,368 --> 00:08:38,546
time to traverse the data, and it

135
00:08:38,568 --> 00:08:42,606
also reduces some of the overhead that is incurred

136
00:08:42,638 --> 00:08:45,942
by each layer calls. So here

137
00:08:45,996 --> 00:08:49,830
you can see the difference between the optimizer network and

138
00:08:49,980 --> 00:08:54,146
the tensor RT optimized network. This is an example of Google's

139
00:08:54,178 --> 00:08:58,758
Leannet architecture, which won the imagenet competition in 2014.

140
00:08:58,854 --> 00:09:02,906
Sorry, it's called Google Net. So as

141
00:09:03,088 --> 00:09:06,730
shown here, after layer intensive fusion, what happens

142
00:09:06,800 --> 00:09:10,534
is you can see on the left side you have multiple layers,

143
00:09:10,582 --> 00:09:14,010
whereas on the right side you have quite a few amount, number of layers.

144
00:09:15,090 --> 00:09:18,650
A deep learning framework, what it does is it does multiple function calls

145
00:09:18,730 --> 00:09:22,206
for calling each layer, and as each layer is

146
00:09:22,228 --> 00:09:25,882
on the GPU, it translates to multiple CudA kernel launches.

147
00:09:26,026 --> 00:09:29,454
The kernel computation is often very fast relative to the kernel

148
00:09:29,502 --> 00:09:32,558
launch overhead and the cost of reading and writing the tensor

149
00:09:32,574 --> 00:09:37,086
data for each layer. This results in the memory bandwidth bottleneck and underutilization

150
00:09:37,118 --> 00:09:41,282
of the available GPU resources. And Tensorrt addresses this by vertically

151
00:09:41,346 --> 00:09:44,214
fusing kernels to perform the sequential operations. Together,

152
00:09:44,332 --> 00:09:47,782
these layer fusion reduces kernel launches and avoids writing into

153
00:09:47,836 --> 00:09:51,338
and reading from memory between layers. So in the

154
00:09:51,344 --> 00:09:55,146
figure shown, the convolution bias and relu layers of various sizes can

155
00:09:55,168 --> 00:09:58,250
be combined into a single layer called as CBR.

156
00:09:58,670 --> 00:10:01,766
And a simple analogy is making three separate

157
00:10:01,798 --> 00:10:05,278
trips to the supermarket to buy three items versus buying all the

158
00:10:05,284 --> 00:10:09,066
three in a single trip. And Tensorrt also recognizes

159
00:10:09,098 --> 00:10:12,254
layers that share the same input data and filter size, but have different

160
00:10:12,292 --> 00:10:15,666
weights. Instead of three separate kernels, tensorrt fuses them

161
00:10:15,688 --> 00:10:19,326
horizontally into a single wider kernel, as shown

162
00:10:19,358 --> 00:10:23,060
as one into one CBR. And tensorrt also eliminates the

163
00:10:23,430 --> 00:10:26,834
concatenation layer by preallocating output buffers and

164
00:10:26,872 --> 00:10:30,694
writing into them into a styled fashion. And that reduces a lot of,

165
00:10:30,732 --> 00:10:34,066
the lot of the overhead. So I actually performed

166
00:10:34,098 --> 00:10:37,954
several calculations and I ran several networks

167
00:10:38,002 --> 00:10:42,038
as shown. These are three networks that I tested with using tensorrt

168
00:10:42,134 --> 00:10:45,638
on my 1050 Ti GPU. Sorry, not 1050 Ti

169
00:10:45,814 --> 00:10:49,690
1650 Ti. You see the number of layers before

170
00:10:49,760 --> 00:10:52,058
fusion and number of layers after fusion,

171
00:10:52,234 --> 00:10:55,840
and those are quite diminished and those are quite less.

172
00:10:56,210 --> 00:10:59,546
That is definitely a lot of less calls.

173
00:10:59,738 --> 00:11:03,134
So how does tensorrt workers like, how do you make it work?

174
00:11:03,252 --> 00:11:06,386
So suppose we are using a pytorch based model.

175
00:11:06,568 --> 00:11:09,666
What you simply have to do is connect the pytos based

176
00:11:09,688 --> 00:11:13,646
model into onnx and import the onnx into tensorrt.

177
00:11:13,758 --> 00:11:17,726
You don't have to select anything else. Tensorrt will automatically generate these applications

178
00:11:17,758 --> 00:11:21,458
and generate an engine. You can then use that engine to perform inference

179
00:11:21,474 --> 00:11:25,046
on the GPU. This is these similar process even for

180
00:11:25,068 --> 00:11:28,118
a tensorflow based model or an Mxnate based model.

181
00:11:28,284 --> 00:11:32,394
Another way by which you can convert to tensorrt is by using

182
00:11:32,432 --> 00:11:35,770
the network definition API provided with C and Python.

183
00:11:36,190 --> 00:11:39,770
It does give you a benefit like you

184
00:11:39,840 --> 00:11:43,806
do get better accuracy and a better speed. Those are

185
00:11:43,828 --> 00:11:47,440
marginally better in some cases and exceptionally better in some others.

186
00:11:47,810 --> 00:11:50,974
But you can try out both methods and the easiest one is to

187
00:11:51,012 --> 00:11:55,006
directly use on Nx parser that is provided with Tensorrt. These are some

188
00:11:55,028 --> 00:11:58,974
of the metrics that I ran with another gpu

189
00:11:59,022 --> 00:12:02,306
that is on JTX 1080 and I used only

190
00:12:02,408 --> 00:12:06,034
for one batch size and I used the Retina phase Resnet 50 based

191
00:12:06,072 --> 00:12:09,278
model as well as the mobile net zero point 25.

192
00:12:09,384 --> 00:12:13,174
So here you can see with FP 32 and an input shape of 640

193
00:12:13,212 --> 00:12:17,366
by 480 I got an FPS of 81

194
00:12:17,548 --> 00:12:21,818
and with int eight based model I got can FPS of 190.

195
00:12:21,984 --> 00:12:25,686
So these are better than real time and you can basically use multistream

196
00:12:25,718 --> 00:12:29,306
now with these models. And believe me, Retina phase won't do

197
00:12:29,328 --> 00:12:32,746
this default by default on Pytorch or tensorflow. And the

198
00:12:32,768 --> 00:12:37,050
Retina phase mobile net based model, even with a float 32 quantized

199
00:12:37,130 --> 00:12:40,926
state, it gave me an FPS of 400. And if you

200
00:12:40,948 --> 00:12:44,398
look at the object detection model Yolo V five, so you can

201
00:12:44,404 --> 00:12:47,746
see the FPS metrics on the right and they are super awesome. And you

202
00:12:47,768 --> 00:12:51,634
can basically even use Yolo V five large on a GTX 1080 for

203
00:12:51,672 --> 00:12:54,610
doing real time processing.

204
00:12:55,190 --> 00:12:58,294
What are the best practices to deploy the model?

205
00:12:58,492 --> 00:13:02,054
Basically use multiple quantization methods. Try those out. Do not

206
00:13:02,092 --> 00:13:05,910
discard intake as easily as possible. Do not build an engine

207
00:13:05,980 --> 00:13:09,746
for each inference as that is an overhead. Save the model, serialize the model

208
00:13:09,788 --> 00:13:13,434
on the disk and then reuse it for your inference. Do try out

209
00:13:13,472 --> 00:13:17,222
different workspace sizes because that would reduce your memory

210
00:13:17,366 --> 00:13:21,190
things to keep in mind while using tensorrt. So engines

211
00:13:21,350 --> 00:13:25,194
generated are specific to the machine. Installation takes time without Docker.

212
00:13:25,322 --> 00:13:28,666
I often go with Docker whenever I'm installing Tensorrt because that's

213
00:13:28,698 --> 00:13:32,058
quite easier. And there are multiple APIs for conversion.

214
00:13:32,154 --> 00:13:36,174
That is Onnx passer, UFf passer network definition API

215
00:13:36,222 --> 00:13:39,730
provided in C network definition API provided in Python.

216
00:13:40,150 --> 00:13:43,806
So that was all that I wanted to talk about of Tensorrt.

217
00:13:43,918 --> 00:13:47,126
It is a very broad topic and I would recommend you guys to go check

218
00:13:47,148 --> 00:13:51,000
it out. Moving forward, we will be talking about again,

219
00:13:51,450 --> 00:13:54,994
it involves Tensorrt, but it is a pipeline

220
00:13:55,042 --> 00:13:59,218
provided by Nvidia specifically for deep learning solutions

221
00:13:59,314 --> 00:14:03,334
and deploying ML solutions. So it is a multi scaled framework,

222
00:14:03,462 --> 00:14:06,300
multiplatform, scalable framework with TLS security.

223
00:14:06,750 --> 00:14:10,234
It can deploy on edge as well as on any cloud. It supports both

224
00:14:10,272 --> 00:14:13,686
Python and C, and it uses GStreamer.

225
00:14:13,718 --> 00:14:17,498
But Nvidia has custom developed the G Streamer objects

226
00:14:17,514 --> 00:14:21,166
for the GPU, so that you often get low overhead in

227
00:14:21,188 --> 00:14:25,126
the pre processing and post processing steps. So in Tensorrt

228
00:14:25,178 --> 00:14:28,846
we saw that our target was the model inference phase,

229
00:14:28,878 --> 00:14:32,926
which was a bottleneck. But after converting an engine to model inference,

230
00:14:33,038 --> 00:14:36,806
even if we want to get more speed, we can use Deepstream, as it

231
00:14:36,828 --> 00:14:39,926
will optimizer the entire pipeline. So the applications and

232
00:14:39,948 --> 00:14:43,526
services of Deepstream are as shown below. You can use Python or

233
00:14:43,548 --> 00:14:47,074
C Plus Plus. Deepstream SDK provides hardware

234
00:14:47,122 --> 00:14:50,842
accelerated plugins, bi directional IoT messaging, Otmo model,

235
00:14:50,896 --> 00:14:54,278
update, reference application, and helm charts.

236
00:14:54,454 --> 00:14:57,926
Below that there is a CUDA layer which is used to deploy

237
00:14:57,958 --> 00:15:01,926
your models, and you can use any of the Nvidia computing platforms

238
00:15:01,958 --> 00:15:05,146
as shown here. So what is the process of a Deepstream pipeline?

239
00:15:05,258 --> 00:15:08,718
So the first process is capturing your stream. It could be a

240
00:15:08,724 --> 00:15:12,062
raw Deepstream RTSP stream, HTTP stream, or a video

241
00:15:12,116 --> 00:15:15,770
recorded sent to a disk.

242
00:15:15,930 --> 00:15:19,566
It is generally read using cpu, it's not read using GPU

243
00:15:19,598 --> 00:15:22,686
right now. After that you have to often decode

244
00:15:22,718 --> 00:15:26,626
the steam because it can be in multiple formats, and that decoding is actually done

245
00:15:26,648 --> 00:15:30,086
on the GPU, so it's quite faster than on CPU, as you can

246
00:15:30,108 --> 00:15:33,746
imagine. After that, Deepstream does image processing.

247
00:15:33,778 --> 00:15:37,682
That is, in case you want any preprocessing steps such as scaling,

248
00:15:37,746 --> 00:15:40,938
dwarping, cropping, et cetera. And all these steps are done on the

249
00:15:40,944 --> 00:15:44,806
GPU. And with Deepstream you get automatic batching,

250
00:15:44,838 --> 00:15:48,406
so you don't have to worry about batching together and then sending

251
00:15:48,438 --> 00:15:51,990
it renders, like rewriting your pipeline to send it to the model.

252
00:15:52,160 --> 00:15:55,838
Deepstream does this job on its own, while this job is done on

253
00:15:55,844 --> 00:15:59,486
the cpu. After that you have several classifiers or

254
00:15:59,508 --> 00:16:03,194
detect layers or segmentation layers that are on Tensorrt

255
00:16:03,242 --> 00:16:06,674
or on the Triton inferencing server. Deepstream also

256
00:16:06,712 --> 00:16:10,254
provides you with an option of by default, tracking.

257
00:16:10,382 --> 00:16:14,702
It is done on both GPU and CPU. You can use that tracker,

258
00:16:14,766 --> 00:16:18,414
and it's quite easy as it's already custom built in Deepstream.

259
00:16:18,542 --> 00:16:22,434
After that you can do two things. Either you can visualize your results

260
00:16:22,562 --> 00:16:26,354
on an on screen display and that conversion is done on the GPU,

261
00:16:26,482 --> 00:16:30,630
or else you can store it to the cloud or send it to a disk.

262
00:16:30,710 --> 00:16:34,406
And that can be done on an HDMI

263
00:16:34,438 --> 00:16:38,314
cable, SATA cable, or using the Nvenc plugin. So these

264
00:16:38,352 --> 00:16:41,466
are some of the models that Nvidia provides. These have

265
00:16:41,488 --> 00:16:45,306
custom built all these for deep steam. So people, the use cases

266
00:16:45,338 --> 00:16:49,006
are pretty specific with the model names and you

267
00:16:49,028 --> 00:16:52,414
can see that you get an FPS of 1100

268
00:16:52,452 --> 00:16:56,354
on a t four inference server and you get a real time FPS on

269
00:16:56,472 --> 00:17:00,386
Jetson. Xavier this is quite awesome and

270
00:17:00,488 --> 00:17:04,270
you can basically use a these detection on Jetson Nano with a 95 FPS.

271
00:17:04,430 --> 00:17:08,114
And if you practically use model pruning and several

272
00:17:08,162 --> 00:17:11,702
other steps that we discussed before, you can get even better

273
00:17:11,756 --> 00:17:14,470
FPS on the models.

274
00:17:14,970 --> 00:17:18,626
These are several other resources on Tensorrt and these are several

275
00:17:18,658 --> 00:17:22,418
resources on the Deepstream. The slide deck is provided

276
00:17:22,514 --> 00:17:25,926
to you and I hope you guys will check this out using these two

277
00:17:25,948 --> 00:17:29,462
resources. Thank you and I'm glad to be

278
00:17:29,516 --> 00:17:33,534
present here. Here the code for retina these tensorrt conversion is

279
00:17:33,732 --> 00:17:37,466
shown on the GitHub link. Please go and see how you can convert

280
00:17:37,498 --> 00:17:40,586
your retina these based model to Tensorrt and deploy

281
00:17:40,618 --> 00:17:42,460
it on any machine. Thank you.

