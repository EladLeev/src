1
00:00:27,170 --> 00:00:28,790
Let's talk data ops.

2
00:00:30,330 --> 00:00:33,666
DevOps revolutionized software engineering by adopting

3
00:00:33,698 --> 00:00:36,818
a lot of agile limb practices post recuperation.

4
00:00:36,994 --> 00:00:41,190
And we know the same needs to happen in data engineering. And today

5
00:00:41,340 --> 00:00:46,262
that's what I'd like to talk about. What we can learn from

6
00:00:46,316 --> 00:00:50,046
DevOps and how we can adapt and adopt it for data and make

7
00:00:50,108 --> 00:00:53,454
data ops. Now, data is in our

8
00:00:53,492 --> 00:00:56,954
everyday lives, everything around us, music, movies,

9
00:00:57,002 --> 00:01:00,442
healthcare, shopping, travel, school, university,

10
00:01:00,586 --> 00:01:04,642
everything relies on it. And there is no inspiration in saying that

11
00:01:04,776 --> 00:01:08,514
every company needs to be a data company. And those

12
00:01:08,552 --> 00:01:11,598
that are not, they are not very successful,

13
00:01:11,694 --> 00:01:15,246
not for long. And it's not a secret

14
00:01:15,278 --> 00:01:19,334
that efficiency of using data is still pretty bad. Nowhere we are

15
00:01:19,452 --> 00:01:23,154
nearly as efficient in creating data project as we are creating software

16
00:01:23,202 --> 00:01:26,662
products. Then way too many examples of failed data

17
00:01:26,716 --> 00:01:29,846
project Gartner multiple or

18
00:01:29,868 --> 00:01:33,502
other similar statistic comes out where aas easily

19
00:01:33,586 --> 00:01:37,018
70, 60, 80% of data in

20
00:01:37,024 --> 00:01:38,970
AI projects fail to reach production.

21
00:01:39,550 --> 00:01:44,234
That's pretty bad. Data Ops

22
00:01:44,282 --> 00:01:48,026
promises to fix that. Data Ops promise to ensure

23
00:01:48,058 --> 00:01:51,360
the most efficient creation of business value from data.

24
00:01:52,130 --> 00:01:56,850
That's about data. Most probably the only thing that people who

25
00:01:57,000 --> 00:02:00,740
study data ops can agree with. It's go.

26
00:02:01,190 --> 00:02:04,594
But there are so many different variety of ways of how to achieve that,

27
00:02:04,632 --> 00:02:06,200
what to do, what does it mean?

28
00:02:07,290 --> 00:02:11,382
And there's still no some

29
00:02:11,436 --> 00:02:15,334
converging opinion about what the solution should be. There are some common

30
00:02:15,372 --> 00:02:18,774
themes and one

31
00:02:18,812 --> 00:02:22,186
of them is that we can learn from the success to

32
00:02:22,208 --> 00:02:25,946
cause in DevOps and try to adapt those for

33
00:02:25,968 --> 00:02:29,482
data, because this is not the same. And we will show how we do that

34
00:02:29,536 --> 00:02:32,826
today. If we're going to talk about data Ops,

35
00:02:32,858 --> 00:02:36,830
we should mention DevOps. It does have very similar goal.

36
00:02:38,050 --> 00:02:41,822
Its simplest, the goal is to ensure most efficient software development from an idea

37
00:02:41,876 --> 00:02:45,474
to a reality to a software product.

38
00:02:45,672 --> 00:02:49,010
Again, how it's done is varied, though there are

39
00:02:49,080 --> 00:02:51,790
now much better established best practices.

40
00:02:51,950 --> 00:02:55,714
Still, depending on who you ask, who have completely different

41
00:02:55,752 --> 00:02:59,554
idea about data DevOps. Really, there are

42
00:02:59,592 --> 00:03:03,106
a lot of best practices in DevOps community though, and we can award

43
00:03:03,138 --> 00:03:06,898
and apply and adopt and most importantly adapt

44
00:03:07,074 --> 00:03:10,026
them from data community. Before that,

45
00:03:10,048 --> 00:03:13,990
let's look at the problem from the different perspectives, different stakeholders involved,

46
00:03:14,150 --> 00:03:17,770
and we'll group them in two categories

47
00:03:18,270 --> 00:03:21,146
for simplifications purposes.

48
00:03:21,258 --> 00:03:24,654
On one hand, we will introduce the

49
00:03:24,692 --> 00:03:27,920
first hero of our story, the infrastructure operations team.

50
00:03:28,850 --> 00:03:31,200
What I'm talking about, I'm talking people who,

51
00:03:31,890 --> 00:03:35,550
when we talk about infrastructure sets, are the people who understand how to provision,

52
00:03:35,710 --> 00:03:39,006
maybe containers, virtual machines, how to set up firewalls

53
00:03:39,038 --> 00:03:42,562
and network for provision, spark Kafka cluster. They understand

54
00:03:42,616 --> 00:03:46,178
performance implication of that infrastructure. Maybe it's

55
00:03:46,194 --> 00:03:49,654
better to use big files with Kafka and small

56
00:03:49,692 --> 00:03:51,400
files with HFS for example.

57
00:03:52,970 --> 00:03:56,406
And operations people are those that say the best operations and

58
00:03:56,428 --> 00:04:00,170
default practices, how to build continuous integrations, continuous development,

59
00:04:00,670 --> 00:04:03,050
how to ensure code is version traceable.

60
00:04:03,710 --> 00:04:07,786
And their goal is ultimately to make sure everything

61
00:04:07,888 --> 00:04:11,626
works as written. They need to optimize reliability

62
00:04:11,658 --> 00:04:15,386
and availability. The other hero

63
00:04:15,418 --> 00:04:18,782
of our story would be those data practitioners, the people

64
00:04:18,836 --> 00:04:22,734
who really create the products end products. That could

65
00:04:22,772 --> 00:04:26,286
be for data. Those could be data engineers, data scientists,

66
00:04:26,318 --> 00:04:30,386
data analysts, analytics engineers, mo engineers. There are a lot of

67
00:04:30,408 --> 00:04:33,966
titles. They do have the domain a business knowledge

68
00:04:34,158 --> 00:04:38,914
and responsible for certain analysis requests from different stakeholders,

69
00:04:39,042 --> 00:04:42,118
marketing executives, so that

70
00:04:42,204 --> 00:04:46,440
either company can make correct quick decisions or can create compelling products.

71
00:04:46,810 --> 00:04:50,134
They do tend to have more domain knowledge, understands how to build data,

72
00:04:50,172 --> 00:04:53,306
project how to join data, set different tables together, how to

73
00:04:53,328 --> 00:04:57,302
report numbers and create predictive models to make recommendation systems.

74
00:04:57,446 --> 00:05:01,770
And their focus is on optimizing agility.

75
00:05:03,730 --> 00:05:06,986
Nowadays businesses need to move so very high speed

76
00:05:07,018 --> 00:05:10,362
and if the data does not catch up then business will be forced

77
00:05:10,426 --> 00:05:13,602
not to use data and probably

78
00:05:13,656 --> 00:05:17,566
fail. In some ways pair goes fairly

79
00:05:17,598 --> 00:05:21,380
conflicting as tend to be between products

80
00:05:21,750 --> 00:05:25,406
and operations teams because their periodics

81
00:05:25,438 --> 00:05:28,658
are conflicting. And this is fairly similar to what we observed

82
00:05:28,674 --> 00:05:31,814
in development operations before DevOps 2030

83
00:05:31,852 --> 00:05:35,206
years ago. Here the data person wants

84
00:05:35,228 --> 00:05:38,710
to optimize cuddly value of data for operations.

85
00:05:39,770 --> 00:05:44,922
Why would like to optimize availability of that data and

86
00:05:45,056 --> 00:05:48,806
how we solve that? There is no clear separation often clarity between the team's

87
00:05:48,838 --> 00:05:53,626
responsibility. Where operations team had to debug data

88
00:05:53,808 --> 00:05:57,326
engineering work and data engineer engineers would need

89
00:05:57,348 --> 00:06:00,800
to provision infrastructure. Well,

90
00:06:01,410 --> 00:06:05,506
let's see how we can adopting and adapt them. And one

91
00:06:05,528 --> 00:06:08,706
of the particular lessons that we need to learn from DevOps is we need to

92
00:06:08,728 --> 00:06:11,906
start treating data as a product and

93
00:06:11,928 --> 00:06:15,970
not just aas a side effect from those source

94
00:06:16,130 --> 00:06:19,622
to the end, be it report

95
00:06:19,756 --> 00:06:23,746
or another product. And Westar

96
00:06:23,778 --> 00:06:27,950
data kit is a framework aimed

97
00:06:27,970 --> 00:06:33,226
to help both the data teams. Information teams sort of make

98
00:06:33,248 --> 00:06:37,174
sure that everything knows what needs to be done and everything is responsible

99
00:06:37,222 --> 00:06:40,570
for their own. It enable

100
00:06:40,650 --> 00:06:44,474
easy contribution to create new data projects and separate

101
00:06:44,522 --> 00:06:48,266
ownership. It does this by introducing

102
00:06:48,298 --> 00:06:51,258
two high level concepts.

103
00:06:51,434 --> 00:06:54,706
One is automating and starting the data journey in

104
00:06:54,728 --> 00:06:58,994
the types of starting the DevOps or data upcycle by

105
00:06:59,032 --> 00:07:02,734
talking about automating and abstracting the data journey which is primarily

106
00:07:02,782 --> 00:07:06,146
responsive of restarted data kit SDK which is

107
00:07:06,168 --> 00:07:09,338
a library for automation of data extraction transformations evolving

108
00:07:09,374 --> 00:07:13,362
of data and a very versatile plugin framework

109
00:07:13,426 --> 00:07:16,502
which allows users to extend this

110
00:07:16,556 --> 00:07:20,442
according to their specific requirements so that implementer people who know best,

111
00:07:20,496 --> 00:07:24,982
for example, that you cannot try to send big messages into Kafka,

112
00:07:25,126 --> 00:07:29,210
they can create very easy plugins that would automatically challenges the data

113
00:07:29,280 --> 00:07:31,280
before it even is being sent.

114
00:07:31,890 --> 00:07:35,470
And the control service

115
00:07:35,620 --> 00:07:39,054
which automatically abstracts the DevOps cycle would allow users to create,

116
00:07:39,092 --> 00:07:42,786
deploy, manage in

117
00:07:42,808 --> 00:07:46,290
automated way those data jobs in a runtime environment

118
00:07:46,950 --> 00:07:50,750
and allow automatic versioning deployment.

119
00:07:50,910 --> 00:07:54,322
And at the same time allow DevOps people

120
00:07:54,376 --> 00:07:59,494
in the company who best know how to build KCD to

121
00:07:59,532 --> 00:08:02,950
extend using their own knowledge

122
00:08:03,370 --> 00:08:07,094
and using their own best practices, they want to apply for their own

123
00:08:07,132 --> 00:08:10,454
organization. Well, let's look at an

124
00:08:10,492 --> 00:08:13,786
example. We're talking about

125
00:08:13,888 --> 00:08:17,946
automate abstracting the DevOps journey and

126
00:08:17,968 --> 00:08:21,566
the data journey. Here we

127
00:08:21,588 --> 00:08:24,814
can see how one infrared structure team

128
00:08:24,852 --> 00:08:28,462
can, for example, intercept through plugins every

129
00:08:28,516 --> 00:08:32,206
single SQL query being sent to a database before it

130
00:08:32,228 --> 00:08:36,434
even finishes the

131
00:08:36,472 --> 00:08:40,402
job, before even including when the job is being run locally during

132
00:08:40,456 --> 00:08:44,418
debug in development and apply

133
00:08:44,504 --> 00:08:47,682
some kind of optimization and layering.

134
00:08:47,826 --> 00:08:51,110
In this concrete example, in the picture we are saying

135
00:08:51,180 --> 00:08:54,486
there's a plugin who collects lineage information to

136
00:08:54,508 --> 00:08:59,734
enable easier troubleshooting and inspecting

137
00:08:59,782 --> 00:09:03,562
of jobs so that one can see where the data comes

138
00:09:03,616 --> 00:09:07,210
from. But this is just an example really.

139
00:09:07,280 --> 00:09:11,562
The sky is those limit the improvements team

140
00:09:11,616 --> 00:09:15,242
can do any kind of plugins and they can be applied across all jobs

141
00:09:15,386 --> 00:09:18,190
AAS. The teams can also create their own plugins.

142
00:09:19,250 --> 00:09:22,670
And then let's look at the DevOps cycle.

143
00:09:23,490 --> 00:09:27,226
Can we do something to automate how development process leads

144
00:09:27,258 --> 00:09:31,154
the DevOps cycle part? What versatile data

145
00:09:31,192 --> 00:09:35,394
kit? What the control service particularly does is it flatten

146
00:09:35,442 --> 00:09:39,906
it? Let's flatten it. And it's

147
00:09:39,938 --> 00:09:43,318
important to provide sales service environment to data engineers to create end to end

148
00:09:43,324 --> 00:09:47,390
data pipelines. This sales service environment

149
00:09:47,490 --> 00:09:51,114
automate large part of default cycle. So as far as data

150
00:09:51,152 --> 00:09:55,260
engineers or data team is concerned, they maybe click just one click button

151
00:09:56,670 --> 00:10:01,120
deploy or CLI command deploy and

152
00:10:01,650 --> 00:10:04,590
building, testing, releasing, deploying can happen automatically.

153
00:10:05,250 --> 00:10:08,458
At the same time, we need to enable some either decentralized

154
00:10:08,554 --> 00:10:12,934
data team or we go with our Persona

155
00:10:13,002 --> 00:10:17,502
some operations of infra team to ensure

156
00:10:17,646 --> 00:10:20,962
that there is a consistency correctness of

157
00:10:21,016 --> 00:10:24,386
those data jobs and all the

158
00:10:24,408 --> 00:10:28,514
compliance quality company policies that are

159
00:10:28,552 --> 00:10:31,906
in place. And since the people they have best knowledge how to implement

160
00:10:31,938 --> 00:10:35,826
these kinds of policies, especially DevOps best practices correctly, or the DevOps precious

161
00:10:35,858 --> 00:10:39,334
people, there's a way for them to

162
00:10:39,372 --> 00:10:42,950
ensure this across all jobs. Quick example

163
00:10:43,100 --> 00:10:46,840
again, the DevOps plugins are actually

164
00:10:48,570 --> 00:10:51,806
more of an build the Docker images that

165
00:10:51,828 --> 00:10:55,886
can be extended and one for

166
00:10:55,908 --> 00:11:00,906
example can extend the build and test those by implementing extending

167
00:11:01,018 --> 00:11:04,334
the default job builder image. And let's say they add some

168
00:11:04,372 --> 00:11:07,300
central assistant test to ensure quality.

169
00:11:09,110 --> 00:11:12,580
Or we want to make sure that

170
00:11:13,270 --> 00:11:16,854
no job can execute arbitrary files so we remove all

171
00:11:16,892 --> 00:11:20,246
execution privileges and that's very

172
00:11:20,268 --> 00:11:23,894
easy. It's pretty simple. Docker image which

173
00:11:23,932 --> 00:11:27,880
can be configured when installing the verstaile data kit control service.

174
00:11:29,050 --> 00:11:32,258
That's our intro into verstaile

175
00:11:32,274 --> 00:11:37,080
data kit. If you want to learn more and

176
00:11:37,530 --> 00:11:41,594
talk about us about these problems and try solving

177
00:11:41,642 --> 00:11:45,582
together with us, contact us at any of the

178
00:11:45,716 --> 00:11:49,402
channels. Those easiest one is through GitHub VMware

179
00:11:49,466 --> 00:11:51,900
style that you contact. Thank you.

