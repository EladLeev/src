1
00:00:00,250 --> 00:00:01,630
Are you an SRE?

2
00:00:03,570 --> 00:00:07,358
A developer? A quality

3
00:00:07,444 --> 00:00:11,162
engineer who wants to tackle the challenge of improving reliability

4
00:00:11,226 --> 00:00:14,970
in your DevOps? You can enable your DevOps for reliability

5
00:00:15,050 --> 00:00:19,114
with chaos native. Create your free account at Chaos

6
00:00:19,162 --> 00:01:17,254
native. Litmus Cloud hi

7
00:01:17,292 --> 00:01:20,786
everyone, welcome to this talk on engineering reliable

8
00:01:20,818 --> 00:01:24,326
mobile applications. My name is Pranjal. I'm speaking to

9
00:01:24,348 --> 00:01:27,426
you from San Francisco, California, and I'm

10
00:01:27,458 --> 00:01:31,130
an SRE program manager at Google. So I've been in Google

11
00:01:31,200 --> 00:01:35,094
for about five years and I've worked on Android

12
00:01:35,142 --> 00:01:38,810
SRE client infrastructure, SRE, Firebase SRE, and also

13
00:01:38,880 --> 00:01:42,734
Gmail, spam and abuse SRE. In my previous years

14
00:01:42,932 --> 00:01:45,870
I've co authored two external publications,

15
00:01:46,290 --> 00:01:49,566
blameless post mortem culture. I've given many talks on

16
00:01:49,588 --> 00:01:52,874
the topic and this talk is going to be about an article

17
00:01:52,922 --> 00:01:56,846
I co authored called Engineering Reliable mobile applications.

18
00:01:57,038 --> 00:02:00,514
Before joining Google, I was sort of

19
00:02:00,552 --> 00:02:04,242
a jack of all trades sort of a role at a company

20
00:02:04,296 --> 00:02:08,054
called Bright Idea, where I was fortunate to

21
00:02:08,252 --> 00:02:11,846
juggle a lot of different roles and that got me

22
00:02:11,868 --> 00:02:15,382
a lot of experience into different arenas. Besides work,

23
00:02:15,436 --> 00:02:18,794
I also love traveling, especially solo, and I also love

24
00:02:18,832 --> 00:02:22,410
exploring cuisines and different sort of alcoholic beverages.

25
00:02:22,830 --> 00:02:26,694
So here's our agenda for today. I'll briefly cover the basics

26
00:02:26,742 --> 00:02:30,430
of traditional SRE and compare and contrast it

27
00:02:30,500 --> 00:02:34,474
with what it means to be SRE for mobile. I'll go deeper

28
00:02:34,522 --> 00:02:37,726
and talk about several distinct challenges such as

29
00:02:37,828 --> 00:02:41,550
scale, monitoring, control and challenges management.

30
00:02:41,970 --> 00:02:45,966
Then I'll go into high level strategies for developing resilient native mobile

31
00:02:45,998 --> 00:02:49,438
apps. This will be followed by three case studies,

32
00:02:49,614 --> 00:02:53,106
actual outages that happened with our apps, and then what we

33
00:02:53,128 --> 00:02:56,254
learned. In the end, I'll summarize some key takeaways,

34
00:02:56,382 --> 00:02:59,922
some things to keep in mind to evaluate the reliability

35
00:03:00,066 --> 00:03:02,870
of your current and future mobile applications.

36
00:03:03,450 --> 00:03:06,214
Now, I assume most of you already know this,

37
00:03:06,332 --> 00:03:10,022
but to set some context, if you are new to the industry,

38
00:03:10,086 --> 00:03:14,026
if you don't know what SRE is, SRE think of

39
00:03:14,048 --> 00:03:18,102
it as a specialized job function that focuses on reliability

40
00:03:18,246 --> 00:03:21,686
and maintainability of large systems. It's also

41
00:03:21,728 --> 00:03:25,962
a mindset. The approach is that of constructive pessimism.

42
00:03:26,106 --> 00:03:29,674
So where you hope for the best, but you plan for the worst,

43
00:03:29,802 --> 00:03:33,282
because hope is not a strategy in terms of people.

44
00:03:33,416 --> 00:03:37,086
While SRE is a distinct job role, they partner closely

45
00:03:37,118 --> 00:03:40,686
and collaboratively with our partner product dev teams throughout

46
00:03:40,718 --> 00:03:44,610
the product lifecycle. So now when I talk about availability,

47
00:03:45,290 --> 00:03:48,950
it's commonly assumed that we're talking about server side availability.

48
00:03:49,530 --> 00:03:54,018
Most slis have also traditionally been bound by server side availability.

49
00:03:54,194 --> 00:03:57,510
So, for example, two to three, nine of uptime

50
00:03:58,510 --> 00:04:01,882
but the problem with relying on server side

51
00:04:01,936 --> 00:04:05,594
availability as an indicator of service health is

52
00:04:05,632 --> 00:04:09,462
that the users perceive the reliability of a service through the devices

53
00:04:09,526 --> 00:04:13,130
in their hands. We have seen a number of production incidents

54
00:04:13,210 --> 00:04:16,798
in which server side instrumentation taken by itself

55
00:04:16,964 --> 00:04:20,714
would have shown no trouble. But a view inclusive of client

56
00:04:20,762 --> 00:04:24,500
side reflected user problems. So, for example,

57
00:04:24,870 --> 00:04:28,462
your services stack is successfully returning what it thinks

58
00:04:28,526 --> 00:04:31,714
are perfectly valid responses. But users of your

59
00:04:31,752 --> 00:04:35,286
app see blank screens, or you're in a new city and

60
00:04:35,308 --> 00:04:38,454
you open the maps app and the app just

61
00:04:38,492 --> 00:04:43,122
keeps crashing. Or maybe your app receives an update.

62
00:04:43,266 --> 00:04:46,680
And although nothing is visibly changed in the app itself,

63
00:04:47,390 --> 00:04:51,034
you experience significantly worse battery life than

64
00:04:51,072 --> 00:04:54,666
before. So these are all issues that cannot be

65
00:04:54,688 --> 00:04:58,438
detected by just monitoring the servers and data centers.

66
00:04:58,614 --> 00:05:02,734
Now we can compare a mobile app to a distributed system that

67
00:05:02,772 --> 00:05:06,286
has billions of machines. So this scale is

68
00:05:06,308 --> 00:05:09,966
just one of the unique challenges of mobile. Things we

69
00:05:09,988 --> 00:05:13,090
take for granted in the server world today have before

70
00:05:13,160 --> 00:05:17,342
very complicated to accomplish for mobile, if not impossible for native

71
00:05:17,406 --> 00:05:21,250
mobile apps. And so here sre just some of the few challenges

72
00:05:23,430 --> 00:05:27,234
in terms of scale. So think of this. There are billions

73
00:05:27,282 --> 00:05:30,930
of devices with thousands of device models,

74
00:05:31,090 --> 00:05:34,390
hundreds of apps running on them, and each app

75
00:05:34,460 --> 00:05:38,074
has its own multiple versions. It becomes more

76
00:05:38,112 --> 00:05:40,870
difficult to accurately attribute,

77
00:05:40,950 --> 00:05:44,342
degrading ux to unreliable network connections,

78
00:05:44,486 --> 00:05:47,450
service reliability or external factors.

79
00:05:48,270 --> 00:05:52,190
All of this combined can make for a very complex ecosystem.

80
00:05:52,770 --> 00:05:56,474
Now, challenge number two is monitoring. We need to tolerate

81
00:05:56,522 --> 00:06:00,266
potential inconsistency in the mobile world because we're relying

82
00:06:00,298 --> 00:06:03,280
on a piece of hardware that's beyond our control.

83
00:06:03,970 --> 00:06:07,634
There's very little we can do when an app is in a state in

84
00:06:07,672 --> 00:06:11,218
which it cannot send information back to you. So in

85
00:06:11,224 --> 00:06:15,334
this diverse and complex ecosystem, the task of monitoring every

86
00:06:15,372 --> 00:06:19,062
single metric has many possible dimensions with

87
00:06:19,116 --> 00:06:22,674
many possible values. So it's infeasible to monitor

88
00:06:22,722 --> 00:06:26,134
every combination independently. And then we must also

89
00:06:26,172 --> 00:06:29,734
consider the effects of logging and monitoring on the end user,

90
00:06:29,862 --> 00:06:32,950
given that they pay the price of resource usage.

91
00:06:33,110 --> 00:06:36,794
So let's say battery and network, for example. Now on

92
00:06:36,832 --> 00:06:40,810
serverside we can change binaries and update configs on demand.

93
00:06:41,150 --> 00:06:44,510
In the mobile world, this power lies with the users.

94
00:06:45,090 --> 00:06:48,894
In the case of native apps, after an update is available to

95
00:06:48,932 --> 00:06:53,002
users, we cannot force them to download a binary or config.

96
00:06:53,146 --> 00:06:56,766
Users may sometimes consider upgrades as indication of poor

97
00:06:56,798 --> 00:07:01,150
software quality and assume that all upgrades are just bug fixes.

98
00:07:01,310 --> 00:07:05,650
It's also important to remember that upgrades can have a very tangible cost.

99
00:07:05,800 --> 00:07:09,518
So, for example, network metered network usage

100
00:07:09,614 --> 00:07:13,826
to the end user, ondevice storage might be constrained

101
00:07:13,938 --> 00:07:18,310
and data connection may be either sparse or completely nonexistent.

102
00:07:19,130 --> 00:07:22,726
Now, if there's a bad change, one's immediate response

103
00:07:22,758 --> 00:07:26,790
is to roll it back. We can quickly roll back serverside

104
00:07:26,950 --> 00:07:29,986
and we know that we will no longer be on the bad version

105
00:07:30,038 --> 00:07:33,854
after the rollback is complete. On the other hand,

106
00:07:34,052 --> 00:07:37,866
it's quite difficult, if not impossible, to roll

107
00:07:37,898 --> 00:07:41,086
back a binary for a native mobile app for Android and

108
00:07:41,108 --> 00:07:45,266
iOS. Instead, the current standard is to roll forward

109
00:07:45,448 --> 00:07:49,490
and hope that affected users will upgrade to the newest version.

110
00:07:49,910 --> 00:07:53,314
Now, considering the lack scale and lack of

111
00:07:53,352 --> 00:07:56,866
control in the mobile environment, managing changes in

112
00:07:56,888 --> 00:08:00,482
a safe and reliable manner is one of the most critical

113
00:08:00,546 --> 00:08:03,240
pieces of managing a reliable mobile app.

114
00:08:04,010 --> 00:08:07,622
So in this next section, I'll talk about some concepts that are not

115
00:08:07,676 --> 00:08:11,254
foreign to us as sres, but I'll put them in the context

116
00:08:11,302 --> 00:08:15,020
of mobile and then discuss some strategies to tackle them.

117
00:08:15,790 --> 00:08:20,090
Now availability is one of the most important measures of reliability,

118
00:08:20,670 --> 00:08:24,526
so think of a time when this happened to you. You tapped an

119
00:08:24,548 --> 00:08:28,442
app icon, the app was about to load and then it immediately

120
00:08:28,506 --> 00:08:32,318
vanished. Or a message displayed saying your application

121
00:08:32,404 --> 00:08:36,466
has stopped. Or you tapped a button, waited for

122
00:08:36,488 --> 00:08:40,740
something to load, and eventually abandoned it by clicking the back button.

123
00:08:41,190 --> 00:08:45,502
Now, these are all examples of your app being effectively unavailable

124
00:08:45,566 --> 00:08:48,706
to you. You the user interacted

125
00:08:48,738 --> 00:08:52,290
with the app and it did not perform the way you expected.

126
00:08:52,450 --> 00:08:56,038
So one way to think about mobile app reliability is its

127
00:08:56,124 --> 00:08:59,814
ability to be available servicing interactions

128
00:08:59,942 --> 00:09:03,190
consistently well relative to your expectations

129
00:09:03,270 --> 00:09:07,526
or the user's expectations. Now users are constantly

130
00:09:07,558 --> 00:09:11,214
interacting with their apps and to understand how available these

131
00:09:11,252 --> 00:09:15,178
apps are, we need on device client side telemetry

132
00:09:15,274 --> 00:09:18,574
to measure and gain visibility, privacy and

133
00:09:18,612 --> 00:09:22,014
security SRE topmost concerns, so we need

134
00:09:22,052 --> 00:09:25,550
to keep those in mind while developing a monitoring strategy.

135
00:09:25,710 --> 00:09:29,406
Now, one example of gathering availability data is gathering

136
00:09:29,438 --> 00:09:32,430
crash reports, so when an app is crashing,

137
00:09:32,510 --> 00:09:36,370
it's a clear sign of unavailability. A user's

138
00:09:36,450 --> 00:09:39,666
experience might be interrupted by a crash dialogue.

139
00:09:39,778 --> 00:09:43,206
The application may close unexpectedly and you

140
00:09:43,228 --> 00:09:46,150
may be prompted to notify via a bug.

141
00:09:46,730 --> 00:09:50,538
Crashes can occur for a number of reasons, but what's important

142
00:09:50,624 --> 00:09:54,214
is that we log them and we monitor them. Now, who doesn't

143
00:09:54,262 --> 00:09:58,246
love real time metrics? The faster one is able to alert

144
00:09:58,278 --> 00:10:02,506
and recognize a problem, the faster one can begin investigating.

145
00:10:02,698 --> 00:10:06,366
Real time metrics also help us sre the results of our fixes and

146
00:10:06,388 --> 00:10:10,026
get feedback on product changes. Most incidents

147
00:10:10,058 --> 00:10:13,506
have a minimum resolution time that is driven more

148
00:10:13,608 --> 00:10:17,726
by humans organizing themselves around problems and then determining

149
00:10:17,758 --> 00:10:21,838
how to fix them. For mobile, this resolution

150
00:10:21,934 --> 00:10:25,538
is also affected by how fast we can push

151
00:10:25,634 --> 00:10:29,154
the fix to services. So most mobile

152
00:10:29,202 --> 00:10:33,122
experimentation and config at Google is polling oriented

153
00:10:33,266 --> 00:10:37,014
with devices updating themselves during opportune times of

154
00:10:37,052 --> 00:10:39,610
battery and bandwidth conservation.

155
00:10:40,190 --> 00:10:43,754
This means that after submitting a fix, it still

156
00:10:43,792 --> 00:10:47,594
may take several hours before client side metrics can

157
00:10:47,792 --> 00:10:51,914
be expected to normalize. Now telemetry for widely

158
00:10:51,962 --> 00:10:55,534
deployed apps is constantly arriving, so despite this

159
00:10:55,572 --> 00:10:58,974
latency, some devices may have picked up the fix

160
00:10:59,092 --> 00:11:02,906
and metrics from them will arrive shortly. One approach

161
00:11:02,938 --> 00:11:06,546
to design metrics derived from services telemetry is to

162
00:11:06,568 --> 00:11:09,874
include config state as a dimension. You can then

163
00:11:09,912 --> 00:11:13,346
constrain your view of the error metrics to only the

164
00:11:13,368 --> 00:11:17,042
devices that are using your fix. This becomes easier

165
00:11:17,106 --> 00:11:20,994
if client changes are being rolled out through launch experiments,

166
00:11:21,122 --> 00:11:24,742
and you can look at the experiment population and experiment id

167
00:11:24,876 --> 00:11:28,566
to monitor the effect of your changes. When developing an

168
00:11:28,588 --> 00:11:31,926
app, one does have the luxury of opening a

169
00:11:31,948 --> 00:11:35,546
bug console and then looking at fine grained lock statements to

170
00:11:35,568 --> 00:11:39,366
inspect app execution and state. However, when it's

171
00:11:39,398 --> 00:11:43,790
deployed to an end user's device, we have visibility into

172
00:11:43,860 --> 00:11:47,934
only what we've chosen to measure and then transport back to us.

173
00:11:48,132 --> 00:11:51,566
So measuring counts of attempts, errors, states or

174
00:11:51,588 --> 00:11:55,506
timers in the code, particularly around key entry or exit points

175
00:11:55,608 --> 00:11:59,486
or business logic, provides indications of the app's usage

176
00:11:59,598 --> 00:12:02,834
and correct functioning. Another idea could be to

177
00:12:02,872 --> 00:12:06,790
run UI test probes to monitor critical user journeys.

178
00:12:07,210 --> 00:12:10,514
This may entail launching the current version of a binary

179
00:12:10,562 --> 00:12:13,666
on a simulated device or emulated device,

180
00:12:13,858 --> 00:12:17,702
inputting actions as a user would, and then asserting that certain

181
00:12:17,756 --> 00:12:22,182
properties hold true throughout the test. Now, what enables

182
00:12:22,246 --> 00:12:24,860
your phone to be mobile in the first place?

183
00:12:25,470 --> 00:12:29,146
So it's the battery, the resource, which is one

184
00:12:29,168 --> 00:12:32,186
of the most valuable resource in a mobile device.

185
00:12:32,378 --> 00:12:35,946
Then there are other precious shared resources like cpu,

186
00:12:36,058 --> 00:12:39,454
memory, storage, network, and no

187
00:12:39,492 --> 00:12:43,262
application wants to misuse any of these resources and get negative

188
00:12:43,326 --> 00:12:46,850
reviews. Efficiency is

189
00:12:46,920 --> 00:12:50,674
particularly important on lower end devices and

190
00:12:50,712 --> 00:12:53,940
in markets where Internet or data usage is high cost.

191
00:12:54,630 --> 00:12:59,058
The platform as a whole suffers when shared resources are misused,

192
00:12:59,154 --> 00:13:02,630
and increasingly the platform needs to place restrictions

193
00:13:03,050 --> 00:13:07,206
to ensure common standards. So one such example is

194
00:13:07,228 --> 00:13:10,746
a program called Android Vitals, where within the

195
00:13:10,768 --> 00:13:14,460
company that helps attribute problems in the app itself.

196
00:13:15,150 --> 00:13:19,062
So our apps have sometimes been blocked from releasing for battery regressions

197
00:13:19,126 --> 00:13:22,702
as low as 0.1% because of negative effect

198
00:13:22,756 --> 00:13:26,490
on user happiness. Any expected regressions

199
00:13:26,570 --> 00:13:30,414
need to be released only after careful considerations of

200
00:13:30,452 --> 00:13:34,480
the trade offs, let's say, or the benefits a feature would provide.

201
00:13:35,430 --> 00:13:39,314
Now, change management is really important, so I'm going to spend a little bit

202
00:13:39,352 --> 00:13:42,914
longer on this Slis because there sre just lots to talk about.

203
00:13:43,112 --> 00:13:46,562
So many of these practices while releasing client

204
00:13:46,626 --> 00:13:49,670
apps are based on general SRE best practices.

205
00:13:50,650 --> 00:13:54,054
Change management is extremely important in

206
00:13:54,092 --> 00:13:58,034
mobile ecosystem because a new single faulty

207
00:13:58,082 --> 00:14:01,734
version can erode users trust and they may decide

208
00:14:01,782 --> 00:14:04,346
to never install or upgrade your app again.

209
00:14:04,528 --> 00:14:08,762
Rollbacks in the mobile world still sort of near impossible and

210
00:14:08,816 --> 00:14:11,546
problems found in product are irrecoverable,

211
00:14:11,658 --> 00:14:15,534
which makes them or which makes proper change management very

212
00:14:15,572 --> 00:14:18,846
important. So the first strategy here is a

213
00:14:18,868 --> 00:14:22,526
stage rollout. This simply means gradually increasing the

214
00:14:22,548 --> 00:14:25,300
number of users the app is being released to.

215
00:14:25,910 --> 00:14:29,746
This also allows you to gather feedback on production changes

216
00:14:29,928 --> 00:14:33,540
gradually instead of hurting all of your users at once.

217
00:14:34,070 --> 00:14:37,746
Now one thing to keep in mind here is internal testing, and testing

218
00:14:37,778 --> 00:14:41,170
on developer devices does not provide a representative

219
00:14:41,250 --> 00:14:44,280
sample of the device diversity that's out there.

220
00:14:44,810 --> 00:14:48,934
If you have the option to release to a selected few beta users,

221
00:14:49,062 --> 00:14:52,780
it will help widen the pool of devices and users also.

222
00:14:53,230 --> 00:14:57,370
Second way to do it is experimentation. Now one issue

223
00:14:57,440 --> 00:15:00,962
with stage rollouts is the bias towards newest

224
00:15:01,046 --> 00:15:05,130
and most upgraded devices. So metrics such as latency

225
00:15:05,210 --> 00:15:08,794
can look great on newer devices, but then the trend becomes

226
00:15:08,842 --> 00:15:12,734
worse over time and then you're just wondering what happened.

227
00:15:12,932 --> 00:15:16,126
Folks with better networks may also upgrade

228
00:15:16,158 --> 00:15:19,874
earlier, which is another example of this bias. Now these

229
00:15:19,912 --> 00:15:23,950
factors make it difficult to accurately measure the effects of a new release.

230
00:15:24,110 --> 00:15:27,654
It's not a simple before and after comparison and may

231
00:15:27,692 --> 00:15:30,710
require some manual intervention and interpretation.

232
00:15:31,450 --> 00:15:35,270
A better way to release metrics or release changes, sorry,

233
00:15:35,420 --> 00:15:39,226
is to release via experiments and conduct ab analysis as

234
00:15:39,248 --> 00:15:42,346
shown in the figure. Whenever possible,

235
00:15:42,528 --> 00:15:46,214
randomize the control and treatment population to ensure

236
00:15:46,262 --> 00:15:50,410
the same group of people are not repeatedly upgrading the wraps.

237
00:15:51,570 --> 00:15:54,670
Another way to do this is feature flag.

238
00:15:55,250 --> 00:15:58,640
Feature flags is another option to manage production changes.

239
00:15:59,170 --> 00:16:02,866
General best practice is to release a

240
00:16:02,888 --> 00:16:06,782
new binary with new code and then turn feature flag

241
00:16:06,846 --> 00:16:11,006
off by default. This gives more control over determining

242
00:16:11,038 --> 00:16:14,322
release radius and timelines. Rolling back

243
00:16:14,376 --> 00:16:17,686
can be as simple as ramping down the feature flag instead of

244
00:16:17,708 --> 00:16:21,366
building an entire binary and then releasing it again. Now this

245
00:16:21,388 --> 00:16:24,774
is with the assumption that rolling back your

246
00:16:24,812 --> 00:16:28,374
flag is not breaking your app. One quick word

247
00:16:28,412 --> 00:16:31,786
of caution in terms of accuracy of metrics is to

248
00:16:31,808 --> 00:16:35,478
take into account the effect of upgrade side effects and noise

249
00:16:35,574 --> 00:16:39,514
between the control and experiment population. The simple

250
00:16:39,712 --> 00:16:43,614
act of upgrading an app itself can introduce noise as

251
00:16:43,652 --> 00:16:46,734
the app restarts. One way to address this

252
00:16:46,772 --> 00:16:50,366
is to release a placebo binary to a control group, doesn't do

253
00:16:50,388 --> 00:16:53,386
anything but will also upgrade or restart,

254
00:16:53,498 --> 00:16:57,826
thereby giving you a more apples to apples comparison of metrics this

255
00:16:57,848 --> 00:17:01,422
is, however, not ideal and has to be done with a lot of caution

256
00:17:01,566 --> 00:17:05,054
to avoid negative effects such as battery consumption or

257
00:17:05,112 --> 00:17:09,014
network usage. Now, even after doing all

258
00:17:09,052 --> 00:17:12,438
of the above and putting all of this governance in place,

259
00:17:12,604 --> 00:17:16,710
the power to update their apps still lies with the user.

260
00:17:17,130 --> 00:17:20,934
This means that as new versions keep releasing, older versions

261
00:17:20,982 --> 00:17:24,874
will continue to coexist. Now, there can be many reasons that

262
00:17:24,912 --> 00:17:28,246
people don't upgrade their apps. Some users disable auto

263
00:17:28,278 --> 00:17:32,490
updates, sometimes there's just not enough storage, et cetera.

264
00:17:32,650 --> 00:17:36,458
On the other hand, privacy and security fixes are strong motivators

265
00:17:36,474 --> 00:17:39,902
for upgrades. Sometimes users are shown a dialog box

266
00:17:39,956 --> 00:17:44,014
and forced to update. Now, as client apps grow and

267
00:17:44,052 --> 00:17:46,958
more and more features and functionalities are added,

268
00:17:47,134 --> 00:17:50,142
some things may not be doable in older versions.

269
00:17:50,286 --> 00:17:53,794
So, for example, if you add a new critical metric to your

270
00:17:53,832 --> 00:17:57,462
application in version 20, that metric will only

271
00:17:57,516 --> 00:18:00,360
be available in version 20 and up.

272
00:18:00,970 --> 00:18:04,038
It may not be sustainable for SRE to support

273
00:18:04,124 --> 00:18:07,686
all possible versions of a client, especially when only

274
00:18:07,708 --> 00:18:11,530
a small fraction of the user population is on the older versions.

275
00:18:12,190 --> 00:18:15,754
So possible tradeoff is developer teams can continue to

276
00:18:15,792 --> 00:18:19,366
support older versions and make business or product decisions

277
00:18:19,398 --> 00:18:21,600
around supporting timeline in general.

278
00:18:23,410 --> 00:18:26,720
Now, we've focused a lot on the client side so far.

279
00:18:27,170 --> 00:18:30,846
This slide is a quick reminder that making client changes to an

280
00:18:30,868 --> 00:18:34,180
app can have server side consequences as well.

281
00:18:34,950 --> 00:18:38,994
Now your servers have, let's say, are already ready

282
00:18:39,112 --> 00:18:43,170
and are scaled to accommodate daily requests. But problem

283
00:18:43,240 --> 00:18:46,470
will arise when unintended consequences occur.

284
00:18:47,050 --> 00:18:50,502
Some examples could be time synchronization requests or

285
00:18:50,556 --> 00:18:54,470
a large global event causing a number of requests to your serverside.

286
00:18:54,890 --> 00:18:58,166
Sometimes bugs in new releases can suddenly increase the

287
00:18:58,188 --> 00:19:00,886
number of requests and overload your servers.

288
00:19:01,078 --> 00:19:04,950
We'll see an example of one such outage in the coming section,

289
00:19:05,030 --> 00:19:07,210
which brings me to case studies.

290
00:19:08,030 --> 00:19:12,026
Now in this section, we address topics covered in previous sections

291
00:19:12,138 --> 00:19:15,918
through several concrete examples of client side issues that

292
00:19:16,004 --> 00:19:19,326
have happened in real life and that our teams have

293
00:19:19,348 --> 00:19:22,826
encountered. The slides also note practical

294
00:19:22,938 --> 00:19:26,546
takeaways for each case study, which you can then apply to your

295
00:19:26,568 --> 00:19:30,194
own apps. This first case study is

296
00:19:30,232 --> 00:19:33,118
from 2016, when AGSA,

297
00:19:33,214 --> 00:19:36,578
which means the Android Google Search app, tried to

298
00:19:36,584 --> 00:19:39,682
etc the config of a new doodle from the server,

299
00:19:39,826 --> 00:19:43,400
but the doodle did not have its color field set.

300
00:19:43,930 --> 00:19:47,794
The code expected the color field to be set and this in turn

301
00:19:47,852 --> 00:19:51,706
resulted in the app crashing. Now the

302
00:19:51,728 --> 00:19:55,354
font is pretty small, but the graph shows crash rate along

303
00:19:55,392 --> 00:19:58,906
a time services. The reason the graph looks like that

304
00:19:59,008 --> 00:20:02,574
is because Doodle launches are happening at midnight for every

305
00:20:02,612 --> 00:20:06,830
time zone, resulting in crash rate spikes at regular intervals.

306
00:20:07,330 --> 00:20:11,070
So the higher the traffic in the region, the higher was the spike.

307
00:20:11,650 --> 00:20:15,458
This graph actually made it easy for the team to correlate issues to

308
00:20:15,464 --> 00:20:18,820
a live Google versus an infrastructure change.

309
00:20:19,590 --> 00:20:23,534
Now, in terms of fixes, it took a series of steps to implement

310
00:20:23,582 --> 00:20:25,890
an all round concrete solution.

311
00:20:26,490 --> 00:20:30,054
So in order to fix the outage first, the config was

312
00:20:30,092 --> 00:20:34,054
fixed and released immediately, but the error did not go down right

313
00:20:34,092 --> 00:20:37,698
away. This was because to avoid causing

314
00:20:37,794 --> 00:20:41,258
avoid calling server backend, the client code

315
00:20:41,344 --> 00:20:44,810
cached the doodle config for a set period of time before

316
00:20:44,880 --> 00:20:48,458
calling the server again for a new config. For this particular

317
00:20:48,544 --> 00:20:52,882
incident, this meant that the bad config was still on services

318
00:20:52,966 --> 00:20:56,462
until the cache expired. Then a second

319
00:20:56,516 --> 00:21:00,506
client side fix was also submitted to prevent the app from crashing

320
00:21:00,538 --> 00:21:04,174
in this situation. But unfortunately, a similar outage

321
00:21:04,222 --> 00:21:07,938
happened again after a few months, and this time only

322
00:21:08,024 --> 00:21:11,534
older versions were without. Older versions were affected,

323
00:21:11,582 --> 00:21:15,394
which did not have the fix I just described. After this

324
00:21:15,432 --> 00:21:19,026
outage reoccurrence, a third solution in the form of server

325
00:21:19,058 --> 00:21:22,742
side guardrails were also put in place to prevent bad

326
00:21:22,796 --> 00:21:25,320
config from being released in the first place.

327
00:21:25,690 --> 00:21:29,194
So quick key takeaways here were client side

328
00:21:29,232 --> 00:21:33,018
fixes don't fix everything. In particular, there are always

329
00:21:33,104 --> 00:21:36,650
older versions out there in the wild that never get updated.

330
00:21:36,990 --> 00:21:40,682
So whenever possible, it's also important to add a services side

331
00:21:40,736 --> 00:21:44,190
fix to increase coverage and prevent a repeat incident.

332
00:21:44,770 --> 00:21:48,286
And also good to know the app's dependencies. It can

333
00:21:48,308 --> 00:21:51,994
help to understand what changes external to the app may be causing

334
00:21:52,042 --> 00:21:56,218
failures. It's also good to have clear documentation

335
00:21:56,314 --> 00:21:59,454
on your client's dependencies like server backends and

336
00:21:59,492 --> 00:22:00,990
config mechanisms.

337
00:22:02,530 --> 00:22:06,434
So here's another case study again from AGSA.

338
00:22:06,562 --> 00:22:09,830
My team on call for AGSA that happened

339
00:22:09,900 --> 00:22:14,322
in 2017. It was one of our largest outages,

340
00:22:14,466 --> 00:22:17,838
but a near miss of a truly disastrous outage,

341
00:22:17,954 --> 00:22:20,650
as it didn't affect the latest production version.

342
00:22:21,070 --> 00:22:24,842
Now, this outage was triggered by a simple four character change

343
00:22:24,896 --> 00:22:28,070
to a legacy config that was unfortunately

344
00:22:28,150 --> 00:22:31,930
not canaryed. It was just tested on a local device,

345
00:22:32,010 --> 00:22:35,866
run through automated testing, committed to production, and then rolled

346
00:22:35,898 --> 00:22:39,678
out. So two unexpected issues occurred due to

347
00:22:39,684 --> 00:22:42,862
a build error. The change was picked up by an old app

348
00:22:42,916 --> 00:22:46,722
version that could not support it, or rather multiple older app

349
00:22:46,776 --> 00:22:50,754
versions that could not support it. And secondly, the change was

350
00:22:50,792 --> 00:22:54,450
rolled out to a wider population than originally intended.

351
00:22:54,790 --> 00:22:58,274
Now, when the change was picked up by older versions, they started failing

352
00:22:58,322 --> 00:23:01,750
on startups and once they failed, they would continue

353
00:23:01,820 --> 00:23:05,574
to fail by reading the cached config. Even after

354
00:23:05,612 --> 00:23:09,142
the config was fixed and rolled out, the app would keep crashing

355
00:23:09,206 --> 00:23:12,666
before it could etc it so this graph that you see on

356
00:23:12,688 --> 00:23:16,646
your screen shows daily active users of the AGSA

357
00:23:16,678 --> 00:23:19,834
app on just the affected version range over

358
00:23:19,872 --> 00:23:24,270
a two week period leading up to and then after the outage.

359
00:23:24,610 --> 00:23:28,122
After scrambling for days. The only recovery

360
00:23:28,186 --> 00:23:31,834
was manual user action where we had to request

361
00:23:31,882 --> 00:23:36,138
our users to upgrade via sending them a notification, which prolonged

362
00:23:36,154 --> 00:23:40,100
the outage and then also resulted in really bad user experience.

363
00:23:40,790 --> 00:23:43,458
So lots of key takeaways on this one.

364
00:23:43,624 --> 00:23:47,414
Anything that causes users to not upgrade is similar to

365
00:23:47,452 --> 00:23:51,314
accumulating tech debt. Older apps are highly

366
00:23:51,362 --> 00:23:54,962
susceptible to issues, so the less successful app updates

367
00:23:55,026 --> 00:23:58,454
are, the larger the probability that a bigger population

368
00:23:58,502 --> 00:24:02,220
will get impacted by a backward incompatible change.

369
00:24:03,150 --> 00:24:07,606
Then always validate before committing, that is, caching a new config

370
00:24:07,798 --> 00:24:10,966
configs can be downloaded and successfully parsed,

371
00:24:11,078 --> 00:24:14,702
but an app should interpret and exercise a new version before

372
00:24:14,756 --> 00:24:18,622
it becomes the active one. Cached config, especially when

373
00:24:18,676 --> 00:24:21,934
RedX startup, can make recovery difficult if it's not

374
00:24:21,972 --> 00:24:25,326
handled correctly. If an app caches a config,

375
00:24:25,438 --> 00:24:29,918
it must be capable of expiring, refreshing or even discarding

376
00:24:29,934 --> 00:24:33,646
the cache without needing the user to step in and don't

377
00:24:33,678 --> 00:24:37,554
rely on recovery outside the app. Sending notifications,

378
00:24:37,602 --> 00:24:41,302
which we did in this case, has limited utility and

379
00:24:41,436 --> 00:24:45,330
instead regularly refreshing configs and having a crash

380
00:24:45,490 --> 00:24:49,306
recovery mechanism is better, but at the

381
00:24:49,328 --> 00:24:53,622
same time, similar to backups, case recovery mechanism

382
00:24:53,686 --> 00:24:56,170
is valid only if it's tested.

383
00:24:56,750 --> 00:25:00,538
So when apps exercise crash recovery, it's still a

384
00:25:00,544 --> 00:25:04,400
warning sign. Crash recovery can conceal real problems.

385
00:25:05,170 --> 00:25:08,634
If your app would have failed if it was not for the crash

386
00:25:08,682 --> 00:25:12,666
recovery, you're again in an unsafe condition because the crash recovery

387
00:25:12,698 --> 00:25:15,630
thing is the only thing that's preventing failure.

388
00:25:15,790 --> 00:25:19,378
So it's a good idea to monitor your case recovery rates also

389
00:25:19,544 --> 00:25:23,394
and treat high rates of recoveries as problems in their own right,

390
00:25:23,512 --> 00:25:26,150
requiring investigation and correlation.

391
00:25:26,490 --> 00:25:30,162
So this last case study demonstrates how apps

392
00:25:30,226 --> 00:25:33,730
sometimes do things in response to inorganic phenomena,

393
00:25:33,890 --> 00:25:37,222
and capacity planning can go for a toss if

394
00:25:37,276 --> 00:25:39,980
proper throttling mechanisms are not in place.

395
00:25:40,430 --> 00:25:44,326
So this graph is a picture of the rate at which certain mobile apps

396
00:25:44,358 --> 00:25:49,078
were registering to receive messages through firebase cloud messaging.

397
00:25:49,254 --> 00:25:52,574
This registration is usually done in the background, that is,

398
00:25:52,612 --> 00:25:56,506
if users install apps for the first time when tokens

399
00:25:56,538 --> 00:26:00,234
need to be refreshed. This is usually a gentle curve

400
00:26:00,282 --> 00:26:03,958
which organically follows the world's population as they wake

401
00:26:03,994 --> 00:26:07,502
up. So what we noticed one day were two spikes

402
00:26:07,566 --> 00:26:11,074
followed by Plateau. The first plateau went back

403
00:26:11,112 --> 00:26:14,642
to the normal trend, but the second spike in Plateau that

404
00:26:14,696 --> 00:26:18,440
looked like teeth of a comb represented the app

405
00:26:18,810 --> 00:26:22,280
repeatedly exhausting its quota and then trying again.

406
00:26:23,050 --> 00:26:27,282
The root cause was identified as a rollout of a new version

407
00:26:27,346 --> 00:26:30,922
of Google Play services, and the new version kept making

408
00:26:30,976 --> 00:26:34,666
FCM registration calls. So the teeth of the

409
00:26:34,688 --> 00:26:38,566
comb that you see on the graph were from the app repeatedly exhausting

410
00:26:38,598 --> 00:26:41,500
its quota and then repeatedly being topped off.

411
00:26:41,870 --> 00:26:44,906
FCM was working fine for other apps, thankfully,

412
00:26:45,018 --> 00:26:48,846
because of the poor app throttling mechanism in place

413
00:26:49,028 --> 00:26:52,746
and service availability wasn't at risk. But this is an

414
00:26:52,788 --> 00:26:56,914
example of the consequences of scale mobile our

415
00:26:56,952 --> 00:27:00,382
devices have limited computing power, but there are billions

416
00:27:00,446 --> 00:27:04,034
of them, and when they accidentally do things together,

417
00:27:04,152 --> 00:27:07,986
it can go downhill really fast. So the key takeaway

418
00:27:08,018 --> 00:27:11,538
here was avoid upgrade proportional load

419
00:27:11,714 --> 00:27:15,362
in terms of release management and uptake rates for installation.

420
00:27:15,506 --> 00:27:19,434
We created a new principle. Mobile apps must not make

421
00:27:19,472 --> 00:27:23,222
service calls during upgrade time, and releases must prove,

422
00:27:23,286 --> 00:27:27,590
via release regression metrics, that they haven't introduced new service calls

423
00:27:27,670 --> 00:27:31,646
purely as a result of the upgrade. If new service calls are

424
00:27:31,668 --> 00:27:35,194
being deliberately introduced, they must be enabled via

425
00:27:35,242 --> 00:27:38,846
an independent ramp up of an experiment and not a

426
00:27:38,868 --> 00:27:42,410
binary review. In terms of scale,

427
00:27:42,570 --> 00:27:46,274
a developer writing and testing their code for

428
00:27:46,312 --> 00:27:49,220
them, a one time service call feels like nothing.

429
00:27:49,830 --> 00:27:53,074
However, two things can change that. So when the app

430
00:27:53,112 --> 00:27:57,094
has a very large user base, and when the service is

431
00:27:57,132 --> 00:28:00,520
scaled only for steady state demand of your app.

432
00:28:01,050 --> 00:28:04,582
If a service exists primarily to support a specific app,

433
00:28:04,716 --> 00:28:08,874
over time, its capacity management optimizes for a footprint that

434
00:28:08,912 --> 00:28:11,660
is close to the app's steady state needs.

435
00:28:12,190 --> 00:28:16,026
So avoiding upgrade proportional load is valuable at large as

436
00:28:16,048 --> 00:28:20,134
well as smaller scales. So this brings us

437
00:28:20,192 --> 00:28:23,466
to the last slide of the stack. To quickly summarize

438
00:28:23,498 --> 00:28:27,374
this talk, a modern product stack is only reliable and

439
00:28:27,412 --> 00:28:30,682
supportable if it's engineered for reliable

440
00:28:30,746 --> 00:28:34,446
operation, all the way from server backends to the app's

441
00:28:34,478 --> 00:28:38,402
user interface. Mobile environments are very different from

442
00:28:38,456 --> 00:28:42,238
server environments and also browser based clients presenting

443
00:28:42,254 --> 00:28:46,870
a unique set of behaviors, failure modes, and management challenges.

444
00:28:47,290 --> 00:28:50,742
Engineering reliability into our mobile apps is as

445
00:28:50,796 --> 00:28:53,430
crucial as building reliable serverside.

446
00:28:54,330 --> 00:28:58,146
Users perceive the reliability of our products based

447
00:28:58,188 --> 00:29:01,914
on the totality of the system, and their mobile device is

448
00:29:01,952 --> 00:29:04,780
their most tangible experience of the service.

449
00:29:05,470 --> 00:29:09,546
So I have five key takeaways, more than my usual three,

450
00:29:09,648 --> 00:29:13,662
but I'll speak through all five of them. So number one is

451
00:29:13,796 --> 00:29:17,546
design mobile apps to be resilient to unexpected inputs,

452
00:29:17,658 --> 00:29:20,874
to recover from management errors, and to roll

453
00:29:20,922 --> 00:29:23,860
out changes in a controlled, metric driven way.

454
00:29:24,790 --> 00:29:28,174
Second thing is monitor the app in production by measuring

455
00:29:28,222 --> 00:29:31,774
critical user interactions and other key health metrics,

456
00:29:31,902 --> 00:29:35,122
for example, responsiveness, data freshness, and, of course,

457
00:29:35,176 --> 00:29:38,914
crashes. Design your success criteria to relate

458
00:29:38,962 --> 00:29:42,646
directly to the expectations of your users as they move through your

459
00:29:42,668 --> 00:29:46,150
app's critical journeys. Then, number three,

460
00:29:46,300 --> 00:29:50,346
release changes carefully by a feature flag so that they

461
00:29:50,368 --> 00:29:54,230
can be evaluated using experiments and rollback independently

462
00:29:54,310 --> 00:29:57,834
of binary releases. Number four,

463
00:29:58,032 --> 00:30:01,786
understand and prepare for the app's impact on servers,

464
00:30:01,978 --> 00:30:04,910
including preventing known bad behaviors.

465
00:30:05,330 --> 00:30:09,114
Establish development and release practices that avoid

466
00:30:09,162 --> 00:30:12,240
problematic feedback patterns between apps and services.

467
00:30:12,690 --> 00:30:15,802
And then lastly, it's also ideal

468
00:30:15,866 --> 00:30:19,914
to proactively create playbooks and incident management processes

469
00:30:20,042 --> 00:30:23,434
in anticipation of a client side outage and the impact

470
00:30:23,482 --> 00:30:25,150
it might have on your servers.

471
00:30:26,570 --> 00:30:27,060
Thank you.

