1
00:00:20,570 --> 00:00:23,882
Hello everyone, this is Ley from Les Dbting.

2
00:00:24,026 --> 00:00:27,426
Today it's an honor to present our journey of writing

3
00:00:27,458 --> 00:00:29,990
a new vectors database in Rust,

4
00:00:30,730 --> 00:00:34,562
introducing LesDB. It is a fresh

5
00:00:34,626 --> 00:00:38,026
addition to the world of open source and in process

6
00:00:38,128 --> 00:00:41,722
databases. Our journey to this

7
00:00:41,776 --> 00:00:45,834
innovation database began in early 2023.

8
00:00:46,032 --> 00:00:49,466
You can think about LensDB as a SQL lite with

9
00:00:49,488 --> 00:00:52,430
DubDB, but for vector databases.

10
00:00:53,010 --> 00:00:56,842
Thanks to the OSS implementations and modern day storage

11
00:00:56,906 --> 00:01:00,874
technologies, LensDB offers blazing

12
00:01:00,922 --> 00:01:03,586
fast vector search,

13
00:01:03,768 --> 00:01:07,010
SQL search and full text search capabilities.

14
00:01:07,670 --> 00:01:11,566
Our users do not only store vectors

15
00:01:11,598 --> 00:01:15,906
in lensDB, we can store multimodal

16
00:01:15,938 --> 00:01:19,782
data and machine learning data directly with

17
00:01:19,836 --> 00:01:23,334
their embeddings in a single store and

18
00:01:23,372 --> 00:01:26,600
our user loves transDB because of it.

19
00:01:27,050 --> 00:01:31,170
In contrast to a lot of other vector database,

20
00:01:31,330 --> 00:01:34,554
we designed a vector index to be cloud native from

21
00:01:34,592 --> 00:01:38,278
the very beginning. As a result, we can deliver

22
00:01:38,374 --> 00:01:42,518
great latency. Even all of your data and index are

23
00:01:42,544 --> 00:01:46,654
on cloud storage like s three. Let me

24
00:01:46,692 --> 00:01:50,286
tell you how Nest DB starts. From the

25
00:01:50,308 --> 00:01:54,382
end of last year, we started to build this new columnar format

26
00:01:54,446 --> 00:01:57,922
called Lens for AI Data Lake in C Plus

27
00:01:57,976 --> 00:02:01,614
plus. However, it was such a painful

28
00:02:01,662 --> 00:02:05,350
experience for our small team to fight with the C Plus five building

29
00:02:05,420 --> 00:02:09,254
system while trying to maintain the velocity to

30
00:02:09,292 --> 00:02:13,046
deliver values to customers. Here we do

31
00:02:13,068 --> 00:02:16,594
it again. Around the new year, our team decided

32
00:02:16,642 --> 00:02:20,486
to try Rust after we have so many rewrite

33
00:02:20,518 --> 00:02:24,010
my projects in rust blocks. It was a huge,

34
00:02:24,080 --> 00:02:27,690
successful task of rewriting. Not only

35
00:02:27,760 --> 00:02:31,146
we reimplant mass column format

36
00:02:31,258 --> 00:02:35,118
and build vectors search capabilities on top of it,

37
00:02:35,284 --> 00:02:38,622
it just used less than one month of

38
00:02:38,676 --> 00:02:42,750
two of our engineering to reimplant that from scratch.

39
00:02:43,190 --> 00:02:46,962
We were surprised that the performance was actually better than

40
00:02:47,016 --> 00:02:49,810
our original C implementations.

41
00:02:50,230 --> 00:02:53,938
Community experience is a huge boost of work

42
00:02:54,104 --> 00:02:57,414
and more importantly, our developers are

43
00:02:57,452 --> 00:03:00,120
very happy writing the clean code.

44
00:03:00,570 --> 00:03:04,162
Our engineering team does not have pre ras

45
00:03:04,306 --> 00:03:08,486
experience, but we love ros. Even though this

46
00:03:08,508 --> 00:03:12,598
is the first line of ROS we are writing. First and foremost,

47
00:03:12,694 --> 00:03:16,662
the biggest reason we want to try ras was because how painful

48
00:03:16,726 --> 00:03:19,850
the cmake is. Cargo is so well

49
00:03:19,920 --> 00:03:24,254
designed that with a massive community around it,

50
00:03:24,372 --> 00:03:28,058
there are a lot of high quality libraries that we can use immediately

51
00:03:28,154 --> 00:03:31,774
without too much hassle like instemake and

52
00:03:31,812 --> 00:03:35,166
not to mention those common traces of the

53
00:03:35,188 --> 00:03:38,846
Russ languages itself. For example, the meaningful

54
00:03:38,878 --> 00:03:42,974
error messages and opinionated software

55
00:03:43,102 --> 00:03:46,498
engineering practice like models, building tests

56
00:03:46,514 --> 00:03:50,182
and benchmark save us a lot of time to discuss what

57
00:03:50,236 --> 00:03:53,430
code standard we should put in place to

58
00:03:53,500 --> 00:03:56,870
provide guarantee for the code qualities

59
00:03:57,610 --> 00:04:01,346
vulnerable. Since we like to support multi

60
00:04:01,378 --> 00:04:04,446
languages sdks for LesDB,

61
00:04:04,578 --> 00:04:08,154
we do need one of the native languages that can

62
00:04:08,192 --> 00:04:10,490
be easily embedded into other languages.

63
00:04:11,010 --> 00:04:14,814
Lastly, because running vector database has a

64
00:04:14,852 --> 00:04:19,310
huge amount of mass computations, especially vector computations,

65
00:04:19,810 --> 00:04:22,946
be able to easily access the latest syndicate code

66
00:04:23,048 --> 00:04:26,580
from standard library has been a huge win for us.

67
00:04:27,270 --> 00:04:29,970
So what is a vector database?

68
00:04:30,630 --> 00:04:34,078
A vector database is different to traditional database

69
00:04:34,174 --> 00:04:37,314
because the search those vector database

70
00:04:37,362 --> 00:04:41,122
try to surface, try to find relevant

71
00:04:41,186 --> 00:04:45,142
data in a high dimensional vector space. Usually our

72
00:04:45,196 --> 00:04:48,562
user data ranges from a few hundred dimension

73
00:04:48,626 --> 00:04:52,170
to one or 1002 dimensions. There's a difference

74
00:04:52,240 --> 00:04:55,910
from traditional database where each of the columnar

75
00:04:55,990 --> 00:04:58,860
there presents just one dimension of data.

76
00:04:59,390 --> 00:05:03,742
Vector database start to be more recognizable recently because

77
00:05:03,876 --> 00:05:08,138
people start to build AI or ARm applications

78
00:05:08,234 --> 00:05:11,802
where those models use huge vector internally

79
00:05:11,866 --> 00:05:15,426
to present their outputs. Vector database in

80
00:05:15,448 --> 00:05:19,854
fact became their machine learning database

81
00:05:19,902 --> 00:05:23,826
of knowledge. For example, a typical application

82
00:05:23,928 --> 00:05:27,222
useless is to store the embeddings from machine learning

83
00:05:27,276 --> 00:05:30,818
models. And here we take the open Eclipse

84
00:05:30,834 --> 00:05:34,950
model as an example where it does

85
00:05:35,020 --> 00:05:39,866
the text to image embedding mapping our

86
00:05:39,968 --> 00:05:43,642
customer will firstly build a data pipeline to

87
00:05:43,696 --> 00:05:46,566
generate embeddings for all the images,

88
00:05:46,758 --> 00:05:50,118
store them and index them in LensDB.

89
00:05:50,294 --> 00:05:53,902
Then they will build a nature language search application

90
00:05:54,036 --> 00:05:59,502
on top of LensDB to search those images through

91
00:05:59,556 --> 00:06:01,870
the English language.

92
00:06:02,450 --> 00:06:06,206
For example. Within this example you

93
00:06:06,228 --> 00:06:10,474
can easily find me top ten of hammer

94
00:06:10,522 --> 00:06:14,394
movies where you have Einstein chatting

95
00:06:14,442 --> 00:06:17,974
with it and the cliff model will give

96
00:06:18,012 --> 00:06:22,166
you vector and find top ten

97
00:06:22,348 --> 00:06:25,446
most relevant results and return back to the

98
00:06:25,468 --> 00:06:27,350
users within subsequent.

99
00:06:27,870 --> 00:06:31,654
However, building a fast vector

100
00:06:31,702 --> 00:06:34,890
database has its own challenges.

101
00:06:35,310 --> 00:06:38,230
Due to the curse of dimensionalities,

102
00:06:38,390 --> 00:06:42,142
it is not practical to have a perfect index that

103
00:06:42,196 --> 00:06:45,498
is both fast and 100% accurate.

104
00:06:45,594 --> 00:06:49,406
You have to pick from one. That's why a

105
00:06:49,428 --> 00:06:54,518
lot of vector index is called approachmate

106
00:06:54,714 --> 00:06:58,450
nearest neighbors and because

107
00:06:58,520 --> 00:07:02,446
we store those index on s three and a lot of random

108
00:07:02,478 --> 00:07:06,850
I o need to be done to reranking the last results.

109
00:07:07,010 --> 00:07:10,838
It is more difficult to support

110
00:07:10,924 --> 00:07:14,470
the cloud native index vector index

111
00:07:14,970 --> 00:07:18,042
our typical data set in DB is usually

112
00:07:18,096 --> 00:07:22,038
range from 700 to 1500 dimensions

113
00:07:22,214 --> 00:07:25,914
with half million to 1 billion factors in

114
00:07:25,952 --> 00:07:29,878
it. The data type in those database

115
00:07:30,054 --> 00:07:34,270
usually flows d two for the vectors and additional

116
00:07:34,610 --> 00:07:37,070
metadata for other filters.

117
00:07:37,810 --> 00:07:41,662
So building a vector index in

118
00:07:41,716 --> 00:07:45,166
rust the first one we built is called AVFPQ

119
00:07:45,358 --> 00:07:49,010
which firstly dividing the vector space into

120
00:07:49,080 --> 00:07:52,494
small partitions. It's called noise

121
00:07:52,542 --> 00:07:56,150
cells and within each cell we use

122
00:07:56,220 --> 00:08:00,614
product quantization to compress the vectors. So to make

123
00:08:00,652 --> 00:08:04,582
the storage smaller for benefit later

124
00:08:04,636 --> 00:08:08,678
I oss that way to answer

125
00:08:08,764 --> 00:08:12,314
one query we just need to scan small fractions of

126
00:08:12,352 --> 00:08:16,262
partitions that are close to the query vector and avoid

127
00:08:16,326 --> 00:08:19,546
a full data set scan. As a result,

128
00:08:19,728 --> 00:08:23,534
we can speed up the retrieval as well to

129
00:08:23,572 --> 00:08:27,802
achieve high indexing performance and low query latency.

130
00:08:27,946 --> 00:08:31,114
We have pushed a lot of performance gain

131
00:08:31,242 --> 00:08:35,646
out of modern cpus. For example, we wrote

132
00:08:35,838 --> 00:08:40,158
yet another gaming implementations with arrow format

133
00:08:40,254 --> 00:08:43,982
and manually tuned Cindy for distance

134
00:08:44,046 --> 00:08:48,242
computations. The code is specially manually tuned

135
00:08:48,306 --> 00:08:52,440
for l one l two cache friendly and we do

136
00:08:53,530 --> 00:08:56,998
manual loop rolling to achieve better

137
00:08:57,084 --> 00:09:00,600
cpu bandwidth. Some other

138
00:09:01,850 --> 00:09:05,594
optimization has been applied into the gaming as well.

139
00:09:05,792 --> 00:09:09,002
By the end of the day, our SIMD code

140
00:09:09,056 --> 00:09:13,030
is actually faster than numpy arrow with a SIMD implementation

141
00:09:13,190 --> 00:09:16,582
and also AI auto recognitions

142
00:09:16,646 --> 00:09:20,686
and other benchmarks. It was mentioned

143
00:09:20,868 --> 00:09:24,522
that all of our code is using stable

144
00:09:24,586 --> 00:09:28,150
rust instead of unstable rust,

145
00:09:28,330 --> 00:09:32,018
so everyone can take that today during the process

146
00:09:32,104 --> 00:09:35,534
of influence caming and

147
00:09:35,592 --> 00:09:38,998
distance functions, rust help us

148
00:09:39,084 --> 00:09:42,886
to work on these low level details while still maintaining the readability of

149
00:09:42,908 --> 00:09:46,470
the code base. We love rust, especially if you

150
00:09:46,540 --> 00:09:50,282
came from the c or C background. You would know

151
00:09:50,336 --> 00:09:54,266
what a nightmare it is to write multi architecture code

152
00:09:54,448 --> 00:09:58,410
with all the defined and defaults.

153
00:09:58,990 --> 00:10:02,250
Rust configured macros help us super

154
00:10:02,320 --> 00:10:05,934
easy to fine tune the small details of a code layout to support

155
00:10:06,052 --> 00:10:10,122
both XCT eight and arm

156
00:10:10,186 --> 00:10:14,426
architecture like hypo M one m two cargo

157
00:10:14,458 --> 00:10:17,554
benchmarks makes a team keep benchmarking as

158
00:10:17,592 --> 00:10:21,618
part of development culture, which is super

159
00:10:21,704 --> 00:10:25,090
important for such a database engineering teams.

160
00:10:25,450 --> 00:10:29,094
We use flame graph extensively because it is so much

161
00:10:29,132 --> 00:10:33,138
easier for us to identify the hot spots

162
00:10:33,234 --> 00:10:37,174
in the code, while the cargo flame graph is

163
00:10:37,212 --> 00:10:41,030
much easier than the original flame graph which

164
00:10:41,180 --> 00:10:44,794
involves multi pieces of code on

165
00:10:44,832 --> 00:10:48,620
Linux. In the end we use

166
00:10:48,990 --> 00:10:52,954
goldbolt. The compiler Explorer a lot

167
00:10:53,152 --> 00:10:57,054
help us to check and verify the generator subject code to make

168
00:10:57,092 --> 00:11:00,190
sure we have correctly used the right instruction set

169
00:11:00,260 --> 00:11:03,570
without stalling the cpu pipelines.

170
00:11:04,230 --> 00:11:07,650
The only missing piece in ROS we miss the most

171
00:11:07,720 --> 00:11:12,382
would be the generic specialization supporting

172
00:11:12,446 --> 00:11:16,360
C Plus plus, which allowed us to support

173
00:11:17,290 --> 00:11:21,030
optimization for multiple different data types

174
00:11:21,930 --> 00:11:25,350
still with a fallback default implementation,

175
00:11:25,930 --> 00:11:29,670
so it is not only hard to do cpu optimization

176
00:11:29,750 --> 00:11:33,660
in a vector database. The I O is also very tricky too.

177
00:11:34,590 --> 00:11:37,558
Because the disk space is linear,

178
00:11:37,734 --> 00:11:41,046
we cannot present multiple dimension

179
00:11:41,238 --> 00:11:44,474
distance statically and efficiently mapping

180
00:11:44,522 --> 00:11:48,046
to this linear space. As a result,

181
00:11:48,228 --> 00:11:52,254
vector index usually involve a lot of scan. It's more like

182
00:11:52,292 --> 00:11:55,854
an olap kind of workload. Many of

183
00:11:55,892 --> 00:11:59,534
our l optimization is on how to reduce the scan

184
00:11:59,582 --> 00:12:03,534
size as well as reduce the later random

185
00:12:03,582 --> 00:12:07,334
IO which used to re rank the results caused by

186
00:12:07,452 --> 00:12:09,670
pq distortion.

187
00:12:10,170 --> 00:12:14,470
Each of IVF PQ is stored on one file.

188
00:12:15,370 --> 00:12:19,830
IVF century block is stored in the beginning of the file

189
00:12:19,990 --> 00:12:24,250
and it presents the sentry of each of the partition

190
00:12:25,550 --> 00:12:26,970
in this index.

191
00:12:31,550 --> 00:12:36,178
We just open this file once and cache the IVF century.

192
00:12:36,374 --> 00:12:40,314
A search vector comes in and use those IVF

193
00:12:40,362 --> 00:12:44,046
centuries to identify which partition to read and scan

194
00:12:44,078 --> 00:12:48,286
them accordingly. Within each of such partition

195
00:12:48,398 --> 00:12:52,222
we use product quantitization codebook

196
00:12:52,286 --> 00:12:56,286
and PQ code to regenerate the original

197
00:12:56,318 --> 00:12:59,798
vectors and do a margin sort for

198
00:12:59,964 --> 00:13:03,414
the final result. In the

199
00:13:03,452 --> 00:13:07,234
end of this retrieval, pipelines are using the rest of the metadata

200
00:13:07,282 --> 00:13:11,094
from deroid ids to do the

201
00:13:11,212 --> 00:13:14,860
hybrid filters with vector search.

202
00:13:15,470 --> 00:13:19,242
Because rust has so much better library support to read

203
00:13:19,296 --> 00:13:23,146
from cloud storage, it makes our implementation

204
00:13:23,258 --> 00:13:26,960
easier and fast and we can build on top of

205
00:13:27,490 --> 00:13:31,070
great community so we can deliver

206
00:13:31,490 --> 00:13:33,070
this thing faster.

207
00:13:34,470 --> 00:13:38,126
So how about other search capabilities?

208
00:13:38,238 --> 00:13:41,646
In lens we do support SQL

209
00:13:41,678 --> 00:13:45,382
and footech search as well. Because NestDB is built

210
00:13:45,436 --> 00:13:48,614
on the lens column format which is

211
00:13:48,652 --> 00:13:52,790
the fastest growing column format out there. It's faster.

212
00:13:54,250 --> 00:13:57,734
The main capability of a lens have a

213
00:13:57,772 --> 00:14:01,382
similar scan performance like Parquet. However it

214
00:14:01,436 --> 00:14:05,446
deliver 2000 times fast point curry than Parker.

215
00:14:05,638 --> 00:14:09,626
We build that on top of data fusion and SQL password so

216
00:14:09,648 --> 00:14:12,838
it support SQL engine internally.

217
00:14:13,014 --> 00:14:17,034
Also we use tentative rust

218
00:14:17,082 --> 00:14:20,938
package with customization to support cloud storage

219
00:14:21,034 --> 00:14:23,700
so we can support footext search as well.

220
00:14:24,550 --> 00:14:28,226
Underneath of this architecture we use a lot of

221
00:14:28,408 --> 00:14:31,794
asynch like Tokyo features and

222
00:14:31,832 --> 00:14:35,314
object store which allow us to read the

223
00:14:35,352 --> 00:14:39,000
object store s three gcs fast without

224
00:14:39,770 --> 00:14:42,230
large amount of cpu cycles.

225
00:14:45,290 --> 00:14:48,562
So by here hopefully

226
00:14:48,626 --> 00:14:52,554
you enjoy the tech details about lens and you would love to

227
00:14:52,592 --> 00:14:55,834
use it. Let me show how can we use

228
00:14:55,872 --> 00:14:59,066
it. LensDB is an in process

229
00:14:59,168 --> 00:15:02,238
database. That means there's no extra

230
00:15:02,324 --> 00:15:05,806
servers, no kubernetes for you to manage and

231
00:15:05,828 --> 00:15:08,430
if there's disk based index,

232
00:15:08,770 --> 00:15:12,202
that means no huge serverless. You need to load

233
00:15:12,266 --> 00:15:15,890
everything in memory before you can serve your first request.

234
00:15:16,230 --> 00:15:20,094
We do support native Python and typescript

235
00:15:20,222 --> 00:15:23,982
native sdks through the grid pile

236
00:15:24,046 --> 00:15:27,922
three and neon package to bring the FFI

237
00:15:27,986 --> 00:15:31,590
into other languages. Of course

238
00:15:31,740 --> 00:15:35,522
we do have the Rust SDK

239
00:15:35,586 --> 00:15:38,790
as well. You can just install it through cargo.

240
00:15:40,410 --> 00:15:44,170
Since LensDB is an in process DB, it can be used

241
00:15:44,240 --> 00:15:48,314
as simple as sqlite. With DarkdB you just import

242
00:15:48,512 --> 00:15:52,730
lens and connect that to a URL

243
00:15:52,890 --> 00:15:56,682
and you can use the database immediately

244
00:15:56,746 --> 00:16:00,622
in your script or in your server. The interface is

245
00:16:00,676 --> 00:16:04,830
designed to follow the data frame style

246
00:16:04,910 --> 00:16:11,970
of APIs. It's very similar to Pandas or SparkdB.

247
00:16:12,630 --> 00:16:16,482
Realistically we only have three language

248
00:16:16,546 --> 00:16:21,030
candidates can be used to build a multi language inprocess database.

249
00:16:21,450 --> 00:16:24,690
For example c send ros.

250
00:16:24,850 --> 00:16:29,660
Hopefully all the presentation I have did in

251
00:16:30,030 --> 00:16:33,862
this presentation can demonstrate the choice.

252
00:16:33,926 --> 00:16:35,020
It's very obvious.

253
00:16:37,070 --> 00:16:41,310
Lastly, we will launch NestDB cloud

254
00:16:41,380 --> 00:16:44,640
SaaS in a week or so.

255
00:16:45,090 --> 00:16:48,682
By just changing the connection URL

256
00:16:48,746 --> 00:16:54,302
to database, we can connect

257
00:16:54,356 --> 00:16:56,800
to the LensDB SaaS with the same experience.

258
00:16:57,250 --> 00:17:01,386
And our SaaS is the pay per query task models, so it's

259
00:17:01,418 --> 00:17:05,222
only paid by usage and fully managed. And the rest rest

260
00:17:05,276 --> 00:17:09,480
of the user experience is the same as our open source project.

261
00:17:10,090 --> 00:17:13,494
Hope you will enjoy it and try it

262
00:17:13,532 --> 00:17:17,126
out. Thank you very much.

263
00:17:17,228 --> 00:17:21,062
We are looking for your feedback and if you have time

264
00:17:21,116 --> 00:17:24,294
please check out our GitHub repo and give

265
00:17:24,332 --> 00:17:27,060
us a star. Thank you very much.

