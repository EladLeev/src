1
00:02:14,840 --> 00:02:17,860
Smoke detectors in large scale production systems.

2
00:02:18,680 --> 00:02:22,196
Before we begin, let's go back in time 15 years ago to look

3
00:02:22,218 --> 00:02:26,324
at the life of sres. Back then. Well, not sres and

4
00:02:26,362 --> 00:02:29,444
not DevOps. We were called sysadmins back then.

5
00:02:29,562 --> 00:02:33,472
Traffic was low, services were enough and weekends held promise.

6
00:02:33,616 --> 00:02:37,384
There were tools like Nagios and Zavix which had against to monitor various

7
00:02:37,432 --> 00:02:41,372
resource usages and emitted zero if all was okay,

8
00:02:41,506 --> 00:02:44,840
one in case of a warnings and two in case of an error.

9
00:02:45,000 --> 00:02:48,572
Static thresholds were good enough for these situations. When stuff

10
00:02:48,626 --> 00:02:52,568
broke, it broke in a fairly predictable manner. Not to say that

11
00:02:52,594 --> 00:02:56,268
when things broke at the time, they were easy to fix. Who wants to configure

12
00:02:56,284 --> 00:02:59,584
a raid array in today's world? Well, definitely not me.

13
00:02:59,702 --> 00:03:02,844
The domain was fairly known and the problems repeated

14
00:03:02,892 --> 00:03:06,480
enough for us to know what broke and how they broke.

15
00:03:06,640 --> 00:03:09,844
Obviously there are quite a few war stories, but the good part

16
00:03:09,882 --> 00:03:13,968
is that there were no auto scaling applications, no Kubernetes clusters,

17
00:03:14,064 --> 00:03:17,556
and more importantly no yaml to write. Now let's

18
00:03:17,588 --> 00:03:20,696
take a look at today and what the future look like.

19
00:03:20,878 --> 00:03:24,484
Applications no longer reside in a single data center in someone's

20
00:03:24,532 --> 00:03:29,144
basement. They are spread across the world. We have auto scaled applications.

21
00:03:29,272 --> 00:03:33,544
There are cloud native workloads spanning across multiple containers and orchestrators,

22
00:03:33,672 --> 00:03:37,644
primarily kubernetes clusters. There are huge number crunching data

23
00:03:37,682 --> 00:03:41,104
lakes. We have functions as a service, lambdas and

24
00:03:41,142 --> 00:03:45,024
whatnot. It can't be denied that the scope has increased and

25
00:03:45,062 --> 00:03:48,512
the domain has exploded. Newer problems and

26
00:03:48,566 --> 00:03:52,588
different variants of the older problems keep on occurring, breaking the systems

27
00:03:52,604 --> 00:03:56,180
in ways that prove very difficult to diagnose and debug.

28
00:03:56,520 --> 00:04:00,432
Along this transition to the new world order, we realize that static

29
00:04:00,496 --> 00:04:03,956
thresholds don't cut it anymore. If they are enough,

30
00:04:04,058 --> 00:04:07,368
there would be definitive answers to these questions. What is a

31
00:04:07,374 --> 00:04:10,852
good number of five xx errors? Depends on the throughput.

32
00:04:10,996 --> 00:04:14,696
If I'm getting a few 505 xx errors in

33
00:04:14,718 --> 00:04:18,660
a few minutes, then it's completely acceptable if my traffic

34
00:04:18,820 --> 00:04:22,428
is at the scale of millions. However, if my traffic itself is

35
00:04:22,434 --> 00:04:25,736
in hundreds or even thousands, it is a cause for concern

36
00:04:25,848 --> 00:04:30,168
and may even cause the slos to fail. How many container restarts

37
00:04:30,184 --> 00:04:33,884
for kubernetes is too many? Depends on the desired containers.

38
00:04:34,012 --> 00:04:37,296
If the restarts are fairly small in numbers compared to the

39
00:04:37,318 --> 00:04:40,704
number of desired containers, it does not affect the availability of my

40
00:04:40,742 --> 00:04:44,324
systems and are of no concern to me. Coming to the last

41
00:04:44,362 --> 00:04:47,684
question, how slow is slow enough? Again,

42
00:04:47,802 --> 00:04:50,996
it depends on the workload. For an ecommerce site,

43
00:04:51,098 --> 00:04:54,630
1 second is too slow and might lead to frustrated customers.

44
00:04:55,000 --> 00:04:58,664
However, in the case of asynchronous workloads, 1 second might

45
00:04:58,702 --> 00:05:02,376
be fine if the workload involves crunching data, or might even be

46
00:05:02,398 --> 00:05:05,828
considered blazing fast with a video editing pipeline,

47
00:05:06,004 --> 00:05:09,416
we can safely assume that context is king

48
00:05:09,608 --> 00:05:12,988
with increasingly complex systems. Context pertaining to the

49
00:05:12,994 --> 00:05:16,396
nature of the systems is paramount. Owing to the

50
00:05:16,418 --> 00:05:20,204
sheer complexity and variety of components involved

51
00:05:20,252 --> 00:05:24,364
in modern distributed systems, it has become harder to isolate faults

52
00:05:24,412 --> 00:05:28,316
and mitigate them accordingly. There are no sitting ducks anymore.

53
00:05:28,428 --> 00:05:31,128
The ducks are armed and dangerous.

54
00:05:31,324 --> 00:05:34,596
However, one might argue that since services have

55
00:05:34,618 --> 00:05:37,796
to abide by slas, aren't slos enough?

56
00:05:37,978 --> 00:05:41,988
Not really. Slos are lagging indicators of system health.

57
00:05:42,154 --> 00:05:46,136
They indicate system performance in the past and not the

58
00:05:46,158 --> 00:05:49,960
future. As a consequence of that, we should be more concerned about

59
00:05:50,030 --> 00:05:53,464
leading indicators of systems performance. For example,

60
00:05:53,662 --> 00:05:57,204
my car alerting washed in the rains outside is a lagging indicators

61
00:05:57,332 --> 00:06:00,264
as it can only happen after it has rained.

62
00:06:00,392 --> 00:06:04,392
However, cloudy weather and lightning are leading indicators

63
00:06:04,456 --> 00:06:07,756
that it may rain. It might not always rain, but there is

64
00:06:07,778 --> 00:06:11,432
a pretty good chance that it might slos are primarily of two kinds,

65
00:06:11,496 --> 00:06:15,312
request based and window slos. Request based slos perform

66
00:06:15,366 --> 00:06:18,768
some aggregation along the lines of good requests versus the

67
00:06:18,774 --> 00:06:23,112
total number of requests. For example, while computing the availability SLO

68
00:06:23,196 --> 00:06:26,596
for a compliance duration, we would simply count the total number of

69
00:06:26,618 --> 00:06:29,588
requests and the total failed requests. However,

70
00:06:29,674 --> 00:06:33,348
consider this scenario during a holiday sale on a shopping site,

71
00:06:33,434 --> 00:06:37,252
the system services 99% of requests successfully during

72
00:06:37,306 --> 00:06:41,112
the day. However, during a 30 minutes window in the day after

73
00:06:41,166 --> 00:06:44,612
the PCR, all requests failed and consequently

74
00:06:44,676 --> 00:06:48,008
most of the customers got upset and did not core back to the site.

75
00:06:48,094 --> 00:06:51,384
A request based SLO of 99% would look undefeated.

76
00:06:51,432 --> 00:06:54,604
In this case. However, the result it can be said

77
00:06:54,722 --> 00:06:58,568
request based slos give us no indicators of consistent performance

78
00:06:58,664 --> 00:07:01,884
throughout the SLO window. They only serve as an indicator

79
00:07:01,932 --> 00:07:05,212
of overall performance. To circumvent the issues above,

80
00:07:05,276 --> 00:07:09,004
we must use window slos. A windowed slo is on the lines

81
00:07:09,052 --> 00:07:12,720
of a certain percentage of time intervals should

82
00:07:12,790 --> 00:07:16,324
satisfy a certain criteria. For example, in the past 24

83
00:07:16,362 --> 00:07:20,196
hours, at least 99% of 1 minute intervals should have a

84
00:07:20,218 --> 00:07:23,652
success ratio of 95% or above. Now, the question

85
00:07:23,706 --> 00:07:27,016
that arises is what kind of slo should be set on what

86
00:07:27,038 --> 00:07:30,788
kind of service? Consider the service that caters to payments.

87
00:07:30,884 --> 00:07:33,988
The service only cares about successful payments.

88
00:07:34,084 --> 00:07:38,264
A request based slos would be ideal in this scenario, a sample objective

89
00:07:38,312 --> 00:07:42,156
could be over the last seven days, 99.9% of the

90
00:07:42,178 --> 00:07:45,756
total payments should be successful. However, on the

91
00:07:45,778 --> 00:07:49,724
other hand, a streaming service would care mostly about

92
00:07:49,842 --> 00:07:53,344
the users being uninterrupted over the weekend so that they can

93
00:07:53,382 --> 00:07:56,668
continue binge watching their shows. In such a scenario,

94
00:07:56,764 --> 00:08:00,224
window based slos are ideal. A sample objective could be

95
00:08:00,262 --> 00:08:04,064
that over the last seven days, 99% of the time, the server should

96
00:08:04,102 --> 00:08:07,480
have served reasonably successful intervals of 15 minutes seats.

97
00:08:07,580 --> 00:08:11,556
An interval is appropriate if 95% of the users did not receive another.

98
00:08:11,738 --> 00:08:15,716
Hence, it is imperative that we need to have some kind of dynamic alerting

99
00:08:15,748 --> 00:08:19,588
in place that gives us information about leading indicators.

100
00:08:19,684 --> 00:08:23,256
Does that mean we need to employ machine learning? Not really.

101
00:08:23,438 --> 00:08:27,632
Machine learning models are cost intensive and will require extensive

102
00:08:27,716 --> 00:08:31,724
training using metrics observed in the past. Since each model needs

103
00:08:31,762 --> 00:08:36,232
to be trained using the metric, it will observe as the number of components

104
00:08:36,376 --> 00:08:40,376
and their corresponding metrics increase, the costs pertaining

105
00:08:40,408 --> 00:08:44,272
to training. Deploying and maintaining the machine learning models goes

106
00:08:44,326 --> 00:08:47,904
through the roof. How do we go about dynamic alerting then?

107
00:08:48,022 --> 00:08:50,748
Well, we have high school math to the rescue.

108
00:08:50,924 --> 00:08:53,760
Armed with high school math, basic statistics,

109
00:08:53,840 --> 00:08:57,824
and other avengers, we at last nine built a smoke detector to monitor

110
00:08:57,872 --> 00:09:01,572
increasingly sophisticated distributed systems. Now the question

111
00:09:01,626 --> 00:09:05,288
that arises is what does a smoke detector really do?

112
00:09:05,454 --> 00:09:09,348
Well, it raises an alarm when an anomaly is detected.

113
00:09:09,524 --> 00:09:13,192
What exactly is an anomaly then? An anomaly is

114
00:09:13,246 --> 00:09:16,932
any deviation in normal system behavior. It could be owing

115
00:09:16,996 --> 00:09:20,408
to any of the following characteristic changes in the metrics

116
00:09:20,504 --> 00:09:23,820
rate at which a particular metric is increasing or decreasing

117
00:09:24,160 --> 00:09:27,196
the amplitude or spikes, which basically means the

118
00:09:27,218 --> 00:09:30,716
absolute value of a metric and also the time of the day at

119
00:09:30,738 --> 00:09:34,640
which the particular value was observed. Let's start with rate.

120
00:09:34,790 --> 00:09:38,080
Rate is basically a measure of how fast a metric value changes.

121
00:09:38,230 --> 00:09:41,920
If something is spiking up fast enough and then goes back to normal,

122
00:09:42,280 --> 00:09:45,748
is it worth alerting? Maybe or maybe not.

123
00:09:45,914 --> 00:09:50,244
A simple way to go about computing rate changes in a metric could

124
00:09:50,282 --> 00:09:54,116
be using standard deviation across a sliding window. If there is

125
00:09:54,138 --> 00:09:57,448
an increase in the standard deviations being observed, that means

126
00:09:57,534 --> 00:10:01,288
there are sudden changes in the metrics that weren't happening before.

127
00:10:01,454 --> 00:10:05,288
However, when exactly do we alert based on rate changes?

128
00:10:05,454 --> 00:10:09,304
If the percentage of four xx errors observed in a system suddenly spikes

129
00:10:09,352 --> 00:10:12,856
up, it is worth alerting and so is the queue depth

130
00:10:12,888 --> 00:10:16,636
on an RDS database. However, if the

131
00:10:16,658 --> 00:10:20,648
cpu or memory usage suddenly increases from 20% to

132
00:10:20,674 --> 00:10:24,224
50%, it should not really be alerted upon it as

133
00:10:24,262 --> 00:10:27,996
it's not really a cause for concern. However, if it continues

134
00:10:28,028 --> 00:10:31,264
to increase after 50% and stays up,

135
00:10:31,382 --> 00:10:35,184
it could be a cause of concern as it could indicate shortage of resources

136
00:10:35,232 --> 00:10:38,804
which could degrade the system performance. Now consider the case

137
00:10:38,842 --> 00:10:42,464
of a services which experiences very sparse traffic. The resource

138
00:10:42,512 --> 00:10:45,736
usage would be negligible most of the time. However,

139
00:10:45,838 --> 00:10:50,276
whenever there is any traffic, resource usage would increase suddenly

140
00:10:50,468 --> 00:10:53,908
and a rate alert would be sent out. Hence,

141
00:10:54,004 --> 00:10:58,044
it is imperative that we include historical context as well while

142
00:10:58,082 --> 00:11:02,152
checking anomalous behavior. As the name suggests, spike detection

143
00:11:02,216 --> 00:11:05,436
only detects spikes or unusually high values which are not

144
00:11:05,458 --> 00:11:08,796
usually observed. Continuing the same example,

145
00:11:08,978 --> 00:11:12,468
whenever the service has traffic, the resource usage increases.

146
00:11:12,584 --> 00:11:15,744
Therefore, an alert should be sent out only when the increased value

147
00:11:15,782 --> 00:11:19,436
is unusually high with reference to the past data to minimize alert

148
00:11:19,468 --> 00:11:23,472
fatigue. A simple way to go about this would be to use percentile based

149
00:11:23,526 --> 00:11:27,236
cutoffs to detectors anomalies. Let's say we set

150
00:11:27,258 --> 00:11:30,644
the 95th percentile as the upper bound as and when new

151
00:11:30,682 --> 00:11:34,512
values are observed, the values corresponding to the 95th percentile adjust

152
00:11:34,576 --> 00:11:38,248
and the bounds get adjusted. However, consider the case of a

153
00:11:38,254 --> 00:11:42,250
database which has a backup job scheduled to run at 04:00 a.m. Every morning.

154
00:11:42,620 --> 00:11:46,584
Whenever the job runs, the network I op creates unusually high

155
00:11:46,622 --> 00:11:49,660
values for a minute and then goes back to normal.

156
00:11:50,000 --> 00:11:53,548
In such a case, both the rate based algorithm and the

157
00:11:53,554 --> 00:11:56,952
spike detection algorithm is likely to send out a false alert.

158
00:11:57,096 --> 00:12:00,380
Therefore, it can be argued that the time at which the value

159
00:12:00,450 --> 00:12:04,448
of the metric was observed is just as important as the value itself.

160
00:12:04,614 --> 00:12:08,316
This brings us to seasonality. Seasonality can be loosely

161
00:12:08,348 --> 00:12:12,256
defined as a reckoning pattern in the metric depending on the time of the

162
00:12:12,278 --> 00:12:16,148
day, the day of the week, or the month of the year. To include the

163
00:12:16,154 --> 00:12:19,844
context of time while computing the bounds, we only include the

164
00:12:19,882 --> 00:12:24,052
data which was observed in a similar time frame in the past. For example,

165
00:12:24,186 --> 00:12:27,976
while checking if a value observed at 06:30 p.m. In the evening is an

166
00:12:27,998 --> 00:12:31,752
anomaly, I can look at values observed from 06:00 p.m. To 07:00

167
00:12:31,806 --> 00:12:35,160
p.m. In the past week. This warnings us to the next question.

168
00:12:35,310 --> 00:12:38,904
What exactly should be the size of the time frame under consideration?

169
00:12:39,032 --> 00:12:43,052
It depends on how predictable the traffic pattern is.

170
00:12:43,186 --> 00:12:46,856
For example, in case of a streaming service which is broadcasting

171
00:12:46,888 --> 00:12:50,156
a sports tournament, the time window could be as small as

172
00:12:50,178 --> 00:12:53,936
an hours, since the time at which the event starts and ends are

173
00:12:53,958 --> 00:12:57,792
mostly known. However, in case of an ecommerce site,

174
00:12:57,926 --> 00:13:01,728
traffic in the weekend or holidays might not strictly adhere to

175
00:13:01,734 --> 00:13:05,072
the same pattern that was observed in the past. Therefore,

176
00:13:05,136 --> 00:13:08,788
a broader time window spanning over 3 hours or even 5 hours

177
00:13:08,874 --> 00:13:12,656
should be used. In this case, a combination of the previously

178
00:13:12,688 --> 00:13:16,108
discussed characteristics can also be used to check if an alert

179
00:13:16,144 --> 00:13:19,416
is to be sent out. Depending on the use case, more and more

180
00:13:19,438 --> 00:13:23,096
such characteristics can be included to add more context upon which

181
00:13:23,198 --> 00:13:26,676
anomalies can be detected and accordingly alerts

182
00:13:26,708 --> 00:13:30,476
can be sent out. On top of this, the number of

183
00:13:30,498 --> 00:13:34,236
characteristics that are being flagged as anomalies can also help us

184
00:13:34,258 --> 00:13:37,676
gauge the severity of the situation. Now. Till now,

185
00:13:37,778 --> 00:13:41,176
all of these detection mechanisms hinged on the fact that

186
00:13:41,218 --> 00:13:45,004
metrics are coming into hours observability setup and are being monitored

187
00:13:45,052 --> 00:13:48,832
continuously. However, if the metrics themselves were to stop

188
00:13:48,886 --> 00:13:51,900
coming in, all of these methods would be rendered useless.

189
00:13:52,060 --> 00:13:55,680
What do we do then? Detecting loss of metrics

190
00:13:55,760 --> 00:13:58,944
is just as important as detecting anomalies in the metrics.

191
00:13:59,072 --> 00:14:02,596
Depending on the nature of the service, it becomes difficult to gauge if

192
00:14:02,618 --> 00:14:06,032
metrics have stopped choosing in or is the traffic genuinely missing.

193
00:14:06,176 --> 00:14:09,828
It could also be the case that the pipeline which feeds metrics into hours setup

194
00:14:09,924 --> 00:14:13,016
has gone down. Consider a high traffic service on

195
00:14:13,038 --> 00:14:16,516
an ecommerce site. It is very unlikely that all of the traffic

196
00:14:16,548 --> 00:14:20,332
stopped coming in suddenly. Hence any gap in metrics should

197
00:14:20,386 --> 00:14:23,580
definitely send out an alert. On the other hand,

198
00:14:23,730 --> 00:14:27,244
if I have a lambda function which gets triggered every hour

199
00:14:27,282 --> 00:14:30,764
or so, I'd be observing metrics only once every hour.

200
00:14:30,882 --> 00:14:33,916
How do I know if the lambda has not triggered or if the metric

201
00:14:33,948 --> 00:14:37,136
pipeline has broken down? A simple way would be to

202
00:14:37,158 --> 00:14:40,572
track the time duration observed between consecutive metric values

203
00:14:40,636 --> 00:14:44,432
and setting percentile based cutoffs. If the duration

204
00:14:44,496 --> 00:14:47,524
is unusually high, an alert should be sent out.

205
00:14:47,642 --> 00:14:51,316
So if I have not observed metrics for the lambda function

206
00:14:51,418 --> 00:14:54,836
in the last 75 minutes, I can brush it off.

207
00:14:55,018 --> 00:14:58,280
However, if no metrics have core for the past 2 hours,

208
00:14:58,350 --> 00:15:01,812
it is definitely a cause of concern. Before we conclude,

209
00:15:01,876 --> 00:15:05,192
let's recap. We looked at the old way of doing things.

210
00:15:05,326 --> 00:15:08,584
We looked at the need of doing things in a new way and we looked

211
00:15:08,622 --> 00:15:12,156
at different ways of alerting which were based on rate, amplitude and even

212
00:15:12,178 --> 00:15:15,276
the time of the day. It is to be noted that there is no

213
00:15:15,298 --> 00:15:18,136
silver bullet, no one size fits all approach.

214
00:15:18,248 --> 00:15:22,284
Even in this day and age, I find myself guilty of using static threshold based

215
00:15:22,322 --> 00:15:26,376
alerts to keep track of basic things like system uptime. While uptime

216
00:15:26,408 --> 00:15:29,976
may not be the same as availability, having this algorithmic toolbox

217
00:15:30,008 --> 00:15:33,844
at hours disposal gives has great power to detect issues faster and

218
00:15:33,882 --> 00:15:37,444
resolve them. Owing to their domain agnostic nature, it makes

219
00:15:37,482 --> 00:15:41,312
them flexible enough to address a huge variety of distributed systems.

220
00:15:41,456 --> 00:15:44,516
I hope this session was useful and shed some light on various ways

221
00:15:44,538 --> 00:15:47,984
to go about dynamic alerting. Feel free to reach out to me on the discord

222
00:15:48,032 --> 00:15:48,960
server or on LinkedIn.

