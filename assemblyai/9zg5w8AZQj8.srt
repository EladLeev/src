1
00:02:09,190 --> 00:02:12,598
Welcome to there's no place like production. I'm Paige

2
00:02:12,614 --> 00:02:15,834
Cruz from Chronosphere. This talk was

3
00:02:15,872 --> 00:02:19,814
inspired by all of the discourse that swirls around a little phrase

4
00:02:19,942 --> 00:02:23,462
I test in production. It's one of those phrases

5
00:02:23,526 --> 00:02:26,874
that people seem to want to take the wrong way and

6
00:02:26,912 --> 00:02:30,522
get into Internet fights about. So I want to tell

7
00:02:30,576 --> 00:02:34,202
you the story of how I learned that there's no place like

8
00:02:34,256 --> 00:02:37,522
production and that a I do test

9
00:02:37,576 --> 00:02:41,300
in prod, but there's a few things you should know first.

10
00:02:42,630 --> 00:02:46,018
I started my tech career at a monitoring company.

11
00:02:46,184 --> 00:02:49,554
And when you work for a monitoring company, no matter where

12
00:02:49,592 --> 00:02:53,842
else you go after that, you are considered the monitoring expert,

13
00:02:53,986 --> 00:02:56,470
and that is a big mantle to carry.

14
00:02:56,890 --> 00:03:00,818
The second thing you should know is that I was hired into this organization

15
00:03:00,914 --> 00:03:04,410
as a senior site reliability engineer,

16
00:03:04,910 --> 00:03:08,522
and something that had been low key stressing me out since

17
00:03:08,576 --> 00:03:12,106
being an SRE and holding the pager for like five or

18
00:03:12,128 --> 00:03:15,306
six years, was that I hadn't yet caused

19
00:03:15,338 --> 00:03:19,034
an incident. All of my friends who were seasoned had tales

20
00:03:19,082 --> 00:03:23,530
of taking down production, accidentally deleting clusters or databases.

21
00:03:23,690 --> 00:03:28,030
And I was just sitting there waiting for my moment. And the last thing

22
00:03:28,180 --> 00:03:31,826
is that during this time, I had a lot of stressors going on.

23
00:03:31,928 --> 00:03:35,982
I was stressed about the pandemic. I was stressed about a Kubernetes

24
00:03:36,046 --> 00:03:39,362
migration that I was having to kind of close out,

25
00:03:39,496 --> 00:03:42,754
which would be fast, followed by a CI CD migration,

26
00:03:42,882 --> 00:03:46,198
which are just two really big projects back to back.

27
00:03:46,364 --> 00:03:49,926
I was stressed about losing like 80% of my department to

28
00:03:49,948 --> 00:03:53,482
attrition, and finally just normal personal life

29
00:03:53,536 --> 00:03:57,782
stressors. So all of this is going on as the backdrop

30
00:03:57,846 --> 00:04:02,022
to this story. Where did this all happen? A sprawling

31
00:04:02,086 --> 00:04:06,042
cannabis platform that was 13 years old by the time I joined.

32
00:04:06,186 --> 00:04:10,094
We connected consumers who were looking to buy cannabis with

33
00:04:10,132 --> 00:04:14,094
dispensaries who were looking to sell cannabis. And as a part of this platform,

34
00:04:14,212 --> 00:04:17,810
we offered many different features, such as an online

35
00:04:17,880 --> 00:04:21,106
marketplace with live inventory, updating a

36
00:04:21,128 --> 00:04:23,886
mobile app for delivery and messaging,

37
00:04:24,078 --> 00:04:27,394
lots of ad purchasing and promotion, as well as

38
00:04:27,432 --> 00:04:30,834
point of sale register systems, really bread and butter

39
00:04:30,882 --> 00:04:34,354
technologies that these dispensaries and consumers depended

40
00:04:34,402 --> 00:04:37,682
on. Our story starts with a little component

41
00:04:37,746 --> 00:04:41,466
called traffic. One day, I was investigating some

42
00:04:41,488 --> 00:04:45,546
issue into production, and I noticed something shocking about our

43
00:04:45,568 --> 00:04:49,606
traffic logs. All of the logs were unstructured,

44
00:04:49,718 --> 00:04:54,322
therefore unindexed, and not queryable or visualizable.

45
00:04:54,486 --> 00:04:58,046
Basically, to me, useless. I couldn't do

46
00:04:58,068 --> 00:05:02,542
anything with these blobs of text. What I wanted were

47
00:05:02,596 --> 00:05:05,966
beautiful, structured, indexed logs that

48
00:05:05,988 --> 00:05:09,018
would let me group and filter by request, path,

49
00:05:09,114 --> 00:05:12,674
remote ip referer, router, name, you name

50
00:05:12,712 --> 00:05:16,462
it. I wanted all of the flexibility that came with structured

51
00:05:16,526 --> 00:05:20,254
login. So what all was involved in making this

52
00:05:20,312 --> 00:05:23,842
change? To sum up, traffic is essentially

53
00:05:23,986 --> 00:05:27,814
an error traffic controller sort of routing requests from the outside

54
00:05:27,932 --> 00:05:31,330
to the inside, or from one part of your system to another.

55
00:05:31,500 --> 00:05:35,034
And it was one of two proxies doing this

56
00:05:35,072 --> 00:05:38,746
type of work. So here is an approximation of my

57
00:05:38,768 --> 00:05:42,954
mental model pre incident of how our system worked.

58
00:05:43,152 --> 00:05:46,858
A client would make requests like hey, is there any turpey

59
00:05:46,874 --> 00:05:50,394
slurpee flower near me? That would hit our CDN,

60
00:05:50,522 --> 00:05:53,742
which would bop over to our load balancer, pass that

61
00:05:53,796 --> 00:05:57,842
information to traffic, which would then give it to a Kubernetes service

62
00:05:57,976 --> 00:06:01,058
and ultimately land in a container in a

63
00:06:01,064 --> 00:06:04,590
pod. All along the way, at every hop we were emitting

64
00:06:04,670 --> 00:06:08,490
telemetry like metrics, logs and traces that were shipped

65
00:06:08,590 --> 00:06:11,862
to one observability vendor and one on

66
00:06:11,916 --> 00:06:16,870
prem monitoring stack. So this

67
00:06:16,940 --> 00:06:20,150
is the steps that I followed to make my change happen.

68
00:06:20,300 --> 00:06:23,734
I needed to update configuration that was stored

69
00:06:23,782 --> 00:06:27,434
in a helm chart. We did not use helm, but we did use

70
00:06:27,472 --> 00:06:31,286
Helm's templating. We would take those helm charts,

71
00:06:31,398 --> 00:06:35,002
render them into raw Kubernetes Yaml

72
00:06:35,066 --> 00:06:38,890
manifest, pass that to an argo CD

73
00:06:39,050 --> 00:06:42,906
app of apps. So first layer of Argo

74
00:06:42,938 --> 00:06:46,740
CD applications, which would then itself point to

75
00:06:47,190 --> 00:06:50,100
an individual argo application,

76
00:06:50,470 --> 00:06:54,450
which would then get synced by Argo and rolled out to our clusters.

77
00:06:54,950 --> 00:06:58,558
But hey, this was a one line configuration

78
00:06:58,654 --> 00:07:02,310
change. I'd been making these types of changes for months.

79
00:07:02,460 --> 00:07:05,874
What could go wrong? Let's find out and embark

80
00:07:05,922 --> 00:07:09,526
on deploying this change, shall we? I started

81
00:07:09,628 --> 00:07:13,058
with a foolproof plan. Whenever I'm a little bit nervous about

82
00:07:13,084 --> 00:07:17,370
a change, I really like to break down what my plan is

83
00:07:17,440 --> 00:07:20,858
for getting from where I am to production. And in this case,

84
00:07:20,944 --> 00:07:24,394
I started with extensive local testing. I wanted

85
00:07:24,432 --> 00:07:28,526
to make sure that our deployment process was up to date, that these changes would

86
00:07:28,548 --> 00:07:32,526
get picked up. I then wanted to announce these changes to the

87
00:07:32,548 --> 00:07:35,774
developers and my other teammates when they would hit

88
00:07:35,812 --> 00:07:39,134
each acceptance and staging. I planned,

89
00:07:39,182 --> 00:07:42,002
of course, to just let it bake, give it time,

90
00:07:42,136 --> 00:07:45,746
and finally, I would schedule and find a quiet time to

91
00:07:45,768 --> 00:07:49,426
try it out tomorrow. In production, I made sure

92
00:07:49,448 --> 00:07:53,414
to request a PR review from the most tenured person on my team with

93
00:07:53,452 --> 00:07:57,574
the most exposure to the system and how it had gotten built up over time.

94
00:07:57,692 --> 00:08:01,414
This helped me feel a lot more confident that I wasn't going to make some

95
00:08:01,452 --> 00:08:04,746
radical change by accident. So after it

96
00:08:04,768 --> 00:08:08,202
passes review and all of the PR checks, it was off to our first

97
00:08:08,256 --> 00:08:12,086
environment, this change landed first in acceptance,

98
00:08:12,198 --> 00:08:14,926
which was sort of a free for all. It was the first place all these

99
00:08:14,948 --> 00:08:17,760
changes would land to see if they would play nicely together.

100
00:08:18,290 --> 00:08:22,138
Unfortunately, this environmentsthere was a little bit undermonitored

101
00:08:22,234 --> 00:08:25,600
and relied on what we call the scream test,

102
00:08:25,910 --> 00:08:29,422
where unless somebody complains or actively

103
00:08:29,486 --> 00:08:32,660
screams that your change has caused an issue,

104
00:08:33,270 --> 00:08:36,740
you consider things good to go and operationally fine.

105
00:08:37,510 --> 00:08:40,718
So after letting it bake in acceptance for a little bit,

106
00:08:40,824 --> 00:08:43,910
I decided I was brave enough to push this to staging.

107
00:08:44,330 --> 00:08:48,006
So I deployed a staging, decided to let it bake for

108
00:08:48,028 --> 00:08:51,240
a little bit longer overnight. One day later,

109
00:08:51,610 --> 00:08:55,126
and it was time to take this change to production.

110
00:08:55,318 --> 00:08:59,242
And at this point, I had a little bit of what we now know is

111
00:08:59,296 --> 00:09:02,602
false confidence that it had caused through two

112
00:09:02,656 --> 00:09:06,302
environments, it had passed through a human PR review and

113
00:09:06,356 --> 00:09:09,774
automated PR checks, and again, was a one

114
00:09:09,812 --> 00:09:13,086
line config change. So at this point, I was

115
00:09:13,108 --> 00:09:17,134
feeling so confident that as soon as the deploy status turned green and said

116
00:09:17,172 --> 00:09:20,466
it was successful, went back to the circus act that

117
00:09:20,488 --> 00:09:24,274
was juggling all of those migrations without a manager and

118
00:09:24,312 --> 00:09:27,890
just trying to get my day job done. Which brings us

119
00:09:27,960 --> 00:09:32,130
to the incident, because several

120
00:09:32,210 --> 00:09:35,890
minutes later, I noticed the incident channel in slack,

121
00:09:35,970 --> 00:09:39,574
lighting up. And from a quick skim, it didn't seem like

122
00:09:39,612 --> 00:09:43,098
this was just a small, isolated issue.

123
00:09:43,184 --> 00:09:46,726
In fact, we had teammates from across the organization,

124
00:09:46,838 --> 00:09:50,346
different components and layers of the stack reporting in

125
00:09:50,448 --> 00:09:54,080
that they'd been alerted, and things were wrong.

126
00:09:54,530 --> 00:09:57,310
So impact was broad and swift.

127
00:09:57,890 --> 00:10:01,680
I kind of sat in the back, panicked, thinking to myself,

128
00:10:02,530 --> 00:10:04,640
was it my change?

129
00:10:06,950 --> 00:10:11,026
No. How? There's no way it was my change.

130
00:10:11,208 --> 00:10:15,534
We had all those environments, it would have come up before then. We've tested

131
00:10:15,582 --> 00:10:19,282
this. It's not my change. And having

132
00:10:19,336 --> 00:10:23,560
convinced myself of that, I muted the incident channel because

133
00:10:24,010 --> 00:10:27,686
I was not primary on call. It was not my responsibility to go

134
00:10:27,708 --> 00:10:31,478
in and investigate this. And I had a bit of a habit of

135
00:10:31,564 --> 00:10:35,210
trying to get involved in all the incidents that I could because I just

136
00:10:35,360 --> 00:10:38,874
find the way that systems break fascinating. And with my

137
00:10:38,912 --> 00:10:43,334
knowledge of monitoring and observability, can sometimes help surface telemetry

138
00:10:43,382 --> 00:10:47,086
or queries that speed along the investigation. But I

139
00:10:47,108 --> 00:10:50,640
was on my best behavior, and I muted the channel and went back to work

140
00:10:51,090 --> 00:10:55,402
until I got a slack dm from my primary

141
00:10:55,466 --> 00:10:59,074
on call, who, incidentally, had reviewed the pr that said,

142
00:10:59,112 --> 00:11:02,658
hey, I think it was your traffic change, and I'm going

143
00:11:02,664 --> 00:11:07,086
to need you to roll that back. And again, my brain just exploded

144
00:11:07,118 --> 00:11:10,214
with questions. How? Why?

145
00:11:10,412 --> 00:11:13,750
What is going wrong? What is different about

146
00:11:13,820 --> 00:11:17,126
production than every other environment where this change went

147
00:11:17,148 --> 00:11:20,454
out. Fine, but this was an

148
00:11:20,492 --> 00:11:24,170
incident. I didn't need to necessarily know or believe

149
00:11:24,240 --> 00:11:27,178
100% that my change was causing the issues.

150
00:11:27,344 --> 00:11:31,146
What needed to happen was very quick mitigation of

151
00:11:31,168 --> 00:11:35,120
the impact that was causing our customers and their customers

152
00:11:35,730 --> 00:11:39,294
to not be able to use our products. And so even

153
00:11:39,332 --> 00:11:42,494
though I was unsure about the

154
00:11:42,532 --> 00:11:46,366
fact that it was my change, it immediately went to that

155
00:11:46,388 --> 00:11:50,770
revert button and I rolled back my changes after

156
00:11:50,840 --> 00:11:53,902
that because now I was a part of the incident response.

157
00:11:53,966 --> 00:11:57,406
I hopped on the video call and just said, hey, I think it's

158
00:11:57,438 --> 00:12:00,958
possible that it was my traffic change. I have no idea why,

159
00:12:01,064 --> 00:12:04,434
but I've gone ahead and rolled back. Let's continue to monitor

160
00:12:04,482 --> 00:12:08,120
and see if there's a change. And very quickly,

161
00:12:08,890 --> 00:12:12,758
all of the graphs were trending in the right direction. Requests were flowing

162
00:12:12,774 --> 00:12:16,074
through our system just like normal, and peace had been

163
00:12:16,112 --> 00:12:19,482
restored. Interestingly, during and after

164
00:12:19,536 --> 00:12:23,462
this incident, I received multiple dms from engineers

165
00:12:23,606 --> 00:12:27,230
commending me on being brave enough to own

166
00:12:27,380 --> 00:12:31,146
being a part of the problem and kind of broadcast

167
00:12:31,258 --> 00:12:34,734
that I was rolling things back and it was probably my

168
00:12:34,772 --> 00:12:38,514
change and really just kind of owning my

169
00:12:38,552 --> 00:12:42,146
part of the problem. And that got me thinking that

170
00:12:42,168 --> 00:12:45,234
we perhaps had some cultural issues with on call

171
00:12:45,272 --> 00:12:48,562
and production operations. So I filed that one away

172
00:12:48,616 --> 00:12:52,086
for the future. And even later that day when

173
00:12:52,108 --> 00:12:55,686
I was telling a friend about I finally had the day that

174
00:12:55,708 --> 00:12:59,458
I took prod down, they replied, hashtag hug ups.

175
00:12:59,554 --> 00:13:03,546
Oh my God. That must have been so stressful that you were the reason

176
00:13:03,648 --> 00:13:07,066
that things broke. But I

177
00:13:07,088 --> 00:13:10,886
didn't really see it the same way. I actually didn't

178
00:13:10,918 --> 00:13:14,678
blame myself at all. I think I took all of the

179
00:13:14,704 --> 00:13:19,360
precautions that I could have. I was very intentional and

180
00:13:19,970 --> 00:13:23,594
did everything in my power to make sure that this change would be safe

181
00:13:23,642 --> 00:13:27,214
before rolling it out to production. And I didn't see what

182
00:13:27,252 --> 00:13:30,642
would be difference between me making this change or someone

183
00:13:30,696 --> 00:13:33,778
else trying to make that change. I think we would have ended up with the

184
00:13:33,784 --> 00:13:37,474
same result. So I didn't blame myself at

185
00:13:37,512 --> 00:13:41,106
all. And I credit this to the learning from incidents

186
00:13:41,138 --> 00:13:45,126
community and just general seasoned SRE fo folks who've seen it

187
00:13:45,148 --> 00:13:48,694
all. So, thank you. And I

188
00:13:48,732 --> 00:13:52,154
go to bed. I wake up the next day, I could

189
00:13:52,192 --> 00:13:55,514
sense that all eyes were on me and

190
00:13:55,552 --> 00:13:59,238
this incident. Engineers from all up and down the stack

191
00:13:59,334 --> 00:14:02,730
were also asking what happened, how,

192
00:14:02,880 --> 00:14:06,506
why? And I realized this was

193
00:14:06,528 --> 00:14:10,046
no incident. No. This was actually a

194
00:14:10,068 --> 00:14:13,326
gift that I was giving to the organization. This was

195
00:14:13,348 --> 00:14:17,102
an opportunity to learn across the more

196
00:14:17,156 --> 00:14:21,074
about how our system works and we were going to cherish this

197
00:14:21,112 --> 00:14:24,786
gift. I became the Regina George of incidents and said, get in

198
00:14:24,808 --> 00:14:28,500
betch, we are learning. And I wanted to bring everyone.

199
00:14:28,870 --> 00:14:33,322
I was really determined to capitalize on this organization

200
00:14:33,406 --> 00:14:37,160
wide attention onto how our infrastructure works.

201
00:14:37,610 --> 00:14:41,062
I had a little bit of work to do because I myself was still

202
00:14:41,116 --> 00:14:44,714
mystified as to how and why this could have happened,

203
00:14:44,832 --> 00:14:48,540
and it was time to start gathering information for the incident review.

204
00:14:49,070 --> 00:14:52,966
It boiled down essentially into a hard mode

205
00:14:52,998 --> 00:14:56,942
version of spot the differences between these two pictures after

206
00:14:56,996 --> 00:15:00,442
bopping around a few of our observability tools, I realized

207
00:15:00,506 --> 00:15:04,750
the quickest way to figure out what went wrong

208
00:15:04,900 --> 00:15:08,810
was to render the helm charts into the raw Kubernetes

209
00:15:08,890 --> 00:15:13,286
manifest yamls and diff those facepalmingly

210
00:15:13,338 --> 00:15:17,010
simple. But in the heat of the moment was not something that

211
00:15:17,080 --> 00:15:21,382
immediately sprung to mind, not when everyone's saying that

212
00:15:21,436 --> 00:15:24,742
everything is down. And let's remember my mental model of the system.

213
00:15:24,876 --> 00:15:28,534
CDN load balancer traffic service pod I

214
00:15:28,572 --> 00:15:32,454
really didn't understand why changing from unstructured logs to structured

215
00:15:32,502 --> 00:15:35,818
logs would affect the path that a request takes.

216
00:15:35,984 --> 00:15:39,114
And it turns out when I played spot the difference,

217
00:15:39,232 --> 00:15:42,906
there was a key component missing from only the

218
00:15:42,928 --> 00:15:47,086
production environmentsthere. How was it that I missed an entire

219
00:15:47,188 --> 00:15:50,766
component getting deleted from my pr change?

220
00:15:50,948 --> 00:15:54,490
Well, we were all in on Gitops, using repositories

221
00:15:54,570 --> 00:15:58,174
as a source of truth and leveraging Argocd's reconciliation

222
00:15:58,222 --> 00:16:02,290
loop to apply changes from the repository into our clusters.

223
00:16:03,110 --> 00:16:06,654
We used an Argo CD app of apps

224
00:16:06,782 --> 00:16:10,514
to bootstrap clusters. How did Argo get Kubernetes manifest

225
00:16:10,562 --> 00:16:13,670
to even deploy helm? In our case,

226
00:16:13,740 --> 00:16:17,222
we would take service secret deployment with whatever

227
00:16:17,276 --> 00:16:21,286
values YAML was the base, plus whatever values YAML was

228
00:16:21,308 --> 00:16:25,194
for that environment. Interpolate and bungee those all together and

229
00:16:25,232 --> 00:16:28,534
spit out the raw Kubernetes Yaml, which we passed

230
00:16:28,582 --> 00:16:29,930
over to Argo.

231
00:16:31,550 --> 00:16:35,150
So what was missing? What I discovered, was that

232
00:16:35,220 --> 00:16:38,266
there was a second container in the pod

233
00:16:38,378 --> 00:16:41,614
where the traffic container was. It was a security

234
00:16:41,732 --> 00:16:44,858
sidecar that acted essentially as a gatekeeper,

235
00:16:44,954 --> 00:16:48,430
vetting and letting requests into our systems.

236
00:16:49,010 --> 00:16:52,434
So difffing the Kubernetes manifest got me

237
00:16:52,472 --> 00:16:56,210
to what happened, but not really

238
00:16:56,280 --> 00:16:59,458
to how. And lo and behold,

239
00:16:59,554 --> 00:17:03,506
I noticed that where I entered my one line to do JSON formatted

240
00:17:03,538 --> 00:17:07,382
logs in a specific values yaml in

241
00:17:07,436 --> 00:17:11,198
the layers of hierarchy, I had unknowingly

242
00:17:11,314 --> 00:17:15,174
overwritten the block for that second container

243
00:17:15,222 --> 00:17:19,270
definition in the pod, aka that security sidecar.

244
00:17:19,430 --> 00:17:23,642
So I accidentally deleted it

245
00:17:23,776 --> 00:17:27,518
and it caused a lot of havoc, but I had no warnings along the

246
00:17:27,524 --> 00:17:30,750
way that this very critical component had disappeared.

247
00:17:31,170 --> 00:17:34,858
Let's talk about learnings, because during the course of this investigation,

248
00:17:34,954 --> 00:17:38,322
I personally learned a lot about how my mental model

249
00:17:38,376 --> 00:17:41,714
of the system didn't reflect reality, what was

250
00:17:41,752 --> 00:17:44,334
actually happening when we merged prs,

251
00:17:44,462 --> 00:17:48,100
and what to look out for next time I made a config change.

252
00:17:48,710 --> 00:17:51,506
But I also had a lot of curious engineers,

253
00:17:51,618 --> 00:17:54,806
managers and leaders wondering what happened,

254
00:17:54,908 --> 00:17:58,294
because anytime there's a sitewide outage, you get a lot of

255
00:17:58,332 --> 00:18:02,042
attention. So here's how I shared my learnings. The first place for

256
00:18:02,096 --> 00:18:05,226
sharing learnings, of course, is the incident retro, and I

257
00:18:05,248 --> 00:18:09,062
made sure that my document had a clear timeline of events,

258
00:18:09,206 --> 00:18:12,650
talked about in detail each step of

259
00:18:12,720 --> 00:18:16,202
translating from helm template to Kubernetes,

260
00:18:16,266 --> 00:18:19,774
manifest to argo application to app

261
00:18:19,812 --> 00:18:22,878
of apps, the whole process from start to end.

262
00:18:22,964 --> 00:18:26,386
Because I wanted anybody, not just the people on

263
00:18:26,408 --> 00:18:30,558
the SRE or the infrastructure team, I wanted anybody in the organization

264
00:18:30,654 --> 00:18:34,260
to be able to understand how this happened. After that,

265
00:18:34,870 --> 00:18:38,798
I took this little incident retro document

266
00:18:38,894 --> 00:18:43,282
along with a life of a request diagram, first to chapter backend,

267
00:18:43,346 --> 00:18:46,754
which was a community group for sharing learnings and announcements

268
00:18:46,802 --> 00:18:50,166
across all the backend engineers. And then I also took

269
00:18:50,188 --> 00:18:53,626
it to chapter front end to share all of this knowledge with our

270
00:18:53,648 --> 00:18:57,162
front end engineers. There was really no shame in my learning

271
00:18:57,216 --> 00:19:01,034
game. I was taking this presentation anywhere people would have

272
00:19:01,072 --> 00:19:04,990
me. And finally I took it to SRE study time,

273
00:19:05,060 --> 00:19:09,294
which was the dedicated space for learning, for my team to

274
00:19:09,332 --> 00:19:12,686
really dig into the details. The thing that

275
00:19:12,708 --> 00:19:16,626
came in the most handy, though, was a metaphor, because I was

276
00:19:16,648 --> 00:19:20,942
talking to leaders. I was talking to maybe engineers

277
00:19:21,006 --> 00:19:24,626
who hadn't been exposed to Kubernetes. So I needed a

278
00:19:24,648 --> 00:19:28,100
really handy metaphor to explain the impact. I said

279
00:19:28,710 --> 00:19:32,806
my pr essentially resulted in me taking out the front door of

280
00:19:32,828 --> 00:19:35,830
our house and replacing it with a brick wall.

281
00:19:35,980 --> 00:19:39,574
Nobody could go inside, but the people who were already inside could talk

282
00:19:39,612 --> 00:19:43,302
to each other. This was a really simple analogy that answered

283
00:19:43,366 --> 00:19:46,854
the immediate question of what happened and what was the impact,

284
00:19:46,982 --> 00:19:50,902
which allowed the attendees to focus on the deeper details

285
00:19:50,966 --> 00:19:54,586
of how and why. Something I kept in

286
00:19:54,608 --> 00:19:57,934
mind when explaining this to teams outside of my own was

287
00:19:57,972 --> 00:20:01,534
our information and knowledge silos. Because we were

288
00:20:01,572 --> 00:20:05,150
a ruby node and elixir shop, we weren't a ghost shop.

289
00:20:05,300 --> 00:20:08,466
We didn't have everybody trained up on kubernetes. If you

290
00:20:08,488 --> 00:20:11,918
saw from the first few slides, we actually had just migrated

291
00:20:11,934 --> 00:20:15,166
to Kubernetes. So a lot of the infrastructure was mysterious

292
00:20:15,198 --> 00:20:18,402
to our developers. So I made sure,

293
00:20:18,536 --> 00:20:21,942
specifically for chapter front end and back end to call out

294
00:20:21,996 --> 00:20:25,590
the idiosyncrasies of go templating and its errors

295
00:20:26,090 --> 00:20:29,170
what the order of precedence is for values.

296
00:20:29,250 --> 00:20:33,094
Yaml file in helm and reviewed the app of apps

297
00:20:33,142 --> 00:20:37,274
pattern with Argo and explained what actually happened when one of them

298
00:20:37,312 --> 00:20:41,242
merged a pr this went a really long way

299
00:20:41,376 --> 00:20:45,366
for building a shared foundation of understanding.

300
00:20:45,558 --> 00:20:49,166
The most popular part of my presentations, by and far, was the

301
00:20:49,188 --> 00:20:52,554
life of a request diagram. It broke

302
00:20:52,602 --> 00:20:56,334
down the end to end that a request would take from

303
00:20:56,372 --> 00:20:59,940
client all the way down to application running in a

304
00:21:00,310 --> 00:21:03,714
and this was the first time that some of these engineers had even

305
00:21:03,752 --> 00:21:07,426
seen this fuller picture of what was going on in

306
00:21:07,448 --> 00:21:10,774
our system. So it felt really good to be able to share this

307
00:21:10,812 --> 00:21:14,390
knowledge. Ultimately, I kind of reflected on the central question

308
00:21:14,460 --> 00:21:18,550
I had been asking myself. What was different

309
00:21:18,700 --> 00:21:22,438
between the production environment, where my change blew everything

310
00:21:22,524 --> 00:21:25,850
up, and staging or acceptance or local,

311
00:21:25,920 --> 00:21:29,226
where my changes seemed to test just fine? It felt like I

312
00:21:29,248 --> 00:21:32,826
was playing spot the difference in extreme mode, which I've recreated for

313
00:21:32,848 --> 00:21:36,286
you here. Tell me which one of these is a

314
00:21:36,308 --> 00:21:39,758
crow and which one is a raven unless you're a

315
00:21:39,764 --> 00:21:43,230
birder, it's pretty hard to say. Sometimes working

316
00:21:43,300 --> 00:21:46,514
in these complex systems can feel like playing a

317
00:21:46,552 --> 00:21:50,034
very risky game of Django. I kept thinking

318
00:21:50,152 --> 00:21:53,806
that on their own, Yaml go templates,

319
00:21:53,918 --> 00:21:57,598
kubernetes, application definitions, and even the argo

320
00:21:57,614 --> 00:22:00,850
app of apps pattern aren't terribly complex

321
00:22:00,930 --> 00:22:03,480
or confusing concepts to understand,

322
00:22:04,010 --> 00:22:07,766
but it's the way that they're all puzzle pieced together into a

323
00:22:07,788 --> 00:22:11,454
system, or stacked and pointed to each other and layered

324
00:22:11,522 --> 00:22:15,098
that made this change in this complex system really difficult

325
00:22:15,184 --> 00:22:18,122
to diagnose. To sum it up,

326
00:22:18,176 --> 00:22:21,434
I learned the hard way that there literally is

327
00:22:21,472 --> 00:22:24,794
no place like production. And it's not that I

328
00:22:24,832 --> 00:22:28,170
test in prod, but we all test in prod.

329
00:22:28,510 --> 00:22:32,038
Thanks so much for listening. I'm Paige Cruz with chronosphere.

330
00:22:32,134 --> 00:22:36,222
Catch up with me at Pagerduty on Mastodon LinkedIn.

331
00:22:36,406 --> 00:22:40,462
Email me would love to chat about the time you took production

332
00:22:40,526 --> 00:22:41,042
down.

