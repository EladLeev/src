1
00:00:26,850 --> 00:00:30,374
Hey everyone, welcome to my talk about observability of microservices using

2
00:00:30,412 --> 00:00:33,974
open source tools. Hope you're having a good time at the conference so far

3
00:00:34,012 --> 00:00:37,654
and that you take away something valuable from my talk. So, a quick

4
00:00:37,692 --> 00:00:41,186
introduction to break the ice hi, I'm Shubham and I'm a developed

5
00:00:41,218 --> 00:00:44,706
relations engineer at the NUD. It's an advanced incident management and response

6
00:00:44,738 --> 00:00:48,534
orchestration platform. I'm an expert in making mistakes, learning from

7
00:00:48,572 --> 00:00:52,826
them, and advocating for best practices for setting up their DevOps,

8
00:00:52,858 --> 00:00:56,718
SRE and production engineering teams. So the burning question

9
00:00:56,804 --> 00:01:00,414
what is this talk about? So firstly, we'll have a quick brief on

10
00:01:00,452 --> 00:01:04,414
observability, something I'm sure the brilliant speakers before me must have done a good job

11
00:01:04,452 --> 00:01:07,794
covering, but we'll quickly glance on the same why

12
00:01:07,832 --> 00:01:10,926
we need observability from day one. Then we're going to be setting

13
00:01:10,958 --> 00:01:14,878
up an observability system using open source tools in record time.

14
00:01:14,984 --> 00:01:18,214
And finally, we'll talk about what comes after you have

15
00:01:18,252 --> 00:01:19,720
observability set up.

16
00:01:20,970 --> 00:01:23,730
So again, what is observability?

17
00:01:23,890 --> 00:01:27,862
It's the ability to measure the internal states of the system by just

18
00:01:27,916 --> 00:01:31,434
examining its outputs. So observability gives engineers a

19
00:01:31,472 --> 00:01:34,618
rather proactive approach to optimizing the system.

20
00:01:34,704 --> 00:01:38,582
It provides a connected, real time view of all the operational data in your software

21
00:01:38,646 --> 00:01:42,378
system, as well as the flexibility to ask the question on

22
00:01:42,384 --> 00:01:46,398
the fly about your applications and get the answers that you need.

23
00:01:46,564 --> 00:01:50,046
And that is the key part, having to know ahead of time what you

24
00:01:50,068 --> 00:01:54,030
wanted to ask. It's also about understanding the service

25
00:01:54,100 --> 00:01:57,278
level required to meet the customer needs, and then instrumenting

26
00:01:57,294 --> 00:02:00,626
the necessary component to ensure that the desired level of

27
00:02:00,648 --> 00:02:04,594
performance is achieved. It is not about collecting a colossal amount

28
00:02:04,632 --> 00:02:08,614
of data to just have all the time, but rather the

29
00:02:08,652 --> 00:02:12,646
right metrics to track and monitor the

30
00:02:12,668 --> 00:02:16,406
key performance indicators relevant to your software and adapting the

31
00:02:16,428 --> 00:02:18,970
system to meet the customer expectations.

32
00:02:20,190 --> 00:02:23,660
So why do I keep hearing about it all the time now?

33
00:02:24,590 --> 00:02:28,266
The word has been thrown around everywhere, and every fourth post on

34
00:02:28,288 --> 00:02:31,930
LinkedIn does have observability as the core concept.

35
00:02:32,350 --> 00:02:35,998
It's a fair question. We've been around for a considerably long time now, and the

36
00:02:36,004 --> 00:02:39,274
buzz around the world has begun to catch on only very recently,

37
00:02:39,322 --> 00:02:42,802
like about five to six years ago. Why now? And,

38
00:02:42,856 --> 00:02:46,494
well, simply. But software complexity is increasing at an exponential

39
00:02:46,542 --> 00:02:48,866
rate, and products are innovating in a crazy,

40
00:02:48,968 --> 00:02:52,082
unpredictable direction. On the infrastructure side,

41
00:02:52,136 --> 00:02:56,214
microservices, polyglot persistence and containers are enabling the

42
00:02:56,332 --> 00:03:00,274
decomposition of monoliths into agile, complex, ever evolving

43
00:03:00,322 --> 00:03:04,006
systems. Meanwhile, on the product side and platform

44
00:03:04,108 --> 00:03:07,994
side, creative solutions are empowering users to do new

45
00:03:08,032 --> 00:03:11,526
and exciting things. However, it creates challenges

46
00:03:11,718 --> 00:03:15,610
for developers to make stable and reliable component

47
00:03:16,270 --> 00:03:19,834
as recently as five years ago, I think most systems were much

48
00:03:19,872 --> 00:03:22,522
simpler. You'd have a classic lamp stack,

49
00:03:22,666 --> 00:03:26,378
one big database, an app tier, a web layer, a caching layer,

50
00:03:26,554 --> 00:03:30,810
some basic load balancing, and you could predict most failures.

51
00:03:30,970 --> 00:03:34,738
Craft a few dashboards that addressed nearly every

52
00:03:34,824 --> 00:03:38,242
performance RCA that you might need

53
00:03:38,296 --> 00:03:41,778
over the course of time, and your team did not spend a lot of

54
00:03:41,784 --> 00:03:44,900
time chasing known unknowns and

55
00:03:45,770 --> 00:03:49,526
it was relatively easy to find out what was

56
00:03:49,548 --> 00:03:53,526
really going on behind the scenes. But now, with a platform

57
00:03:53,628 --> 00:03:57,794
or a microservices architecture, or millions of unique

58
00:03:57,842 --> 00:04:01,610
users or applications connected, you have a long fat

59
00:04:02,350 --> 00:04:05,260
line of unique questions to answer all the time.

60
00:04:05,790 --> 00:04:09,340
There are many more potential combinations of things going wrong, and sometimes

61
00:04:09,710 --> 00:04:12,910
they connect in a manner that is hard to decipher.

62
00:04:13,970 --> 00:04:17,370
Now why do we need day one observability?

63
00:04:17,530 --> 00:04:21,966
Why are we solving a problem before it even exists, right when

64
00:04:22,068 --> 00:04:26,034
environments are as complex as they are today, simply monitoring for

65
00:04:26,072 --> 00:04:29,934
known problems doesn't understand the growing number of new issues that arise.

66
00:04:30,062 --> 00:04:33,890
So these new issues, the unknown unknowns that we talked about earlier,

67
00:04:34,710 --> 00:04:38,646
mean that an observable system in

68
00:04:38,668 --> 00:04:42,406
an observable system you know what is causing the problem and you dont

69
00:04:42,428 --> 00:04:45,206
have a standard starting point to find out.

70
00:04:45,388 --> 00:04:48,438
So without deep observability, it is natural to

71
00:04:48,444 --> 00:04:51,430
make assumptions about production system behavior,

72
00:04:51,590 --> 00:04:55,350
including what we think may be potential performance bottlenecks or failure scenarios.

73
00:04:55,430 --> 00:04:58,714
When failures do occur, we are often in the dark as

74
00:04:58,752 --> 00:05:02,874
to whether they have occurred and both the impact and potential fixes.

75
00:05:03,002 --> 00:05:06,538
This leads to wasted time and effort, like throughout the organization,

76
00:05:06,634 --> 00:05:10,478
jumping from one theory to another, one change to another maybe.

77
00:05:10,644 --> 00:05:14,500
And you really do not understand how

78
00:05:14,870 --> 00:05:18,306
any of this is going to impact the system if customers are

79
00:05:18,328 --> 00:05:22,382
impacted. Cost of the guesswork is exponentially

80
00:05:22,446 --> 00:05:26,082
high and it can escalate as quickly

81
00:05:26,216 --> 00:05:30,258
for any stage and in production. While Kubernetes

82
00:05:30,354 --> 00:05:33,606
can help recover from some failures, we've seen it be

83
00:05:33,628 --> 00:05:37,522
a lifesaver for a lot of organisations. But there are many scenarios where

84
00:05:37,596 --> 00:05:41,206
that can cause system to run suboptimally or fail continuously.

85
00:05:41,238 --> 00:05:45,478
Even and even when service availability is maintained,

86
00:05:45,654 --> 00:05:49,446
performance bottlenecks can result in premature auto scaling,

87
00:05:49,558 --> 00:05:53,354
resulting in the excessive use of costly cloud computing resources,

88
00:05:53,402 --> 00:05:57,006
to say the very least. Indeed, there are cases where the first

89
00:05:57,028 --> 00:06:00,462
signs of failure is an astronomically high cloud

90
00:06:00,516 --> 00:06:04,374
computing bill. So now, without any further stalling,

91
00:06:04,522 --> 00:06:08,514
let's move on to the meat of it building a complete open source solution for

92
00:06:08,552 --> 00:06:11,826
extracting and shipping traces, metrics and logs and

93
00:06:11,848 --> 00:06:15,406
correlation between them. So a few prerequisites

94
00:06:15,438 --> 00:06:19,430
to fulfill this demo. First up we have the kind tool.

95
00:06:19,580 --> 00:06:23,506
Kind is a tool for running local Kubernetes clusters using docker container

96
00:06:23,538 --> 00:06:27,202
nodes. Kind was primarily designed for testing Kubernetes

97
00:06:27,266 --> 00:06:31,702
itself. And yeah, if you have go 1.17

98
00:06:31,766 --> 00:06:35,306
plus and Docker installed, all you have to do is run this command to

99
00:06:35,328 --> 00:06:38,614
install and run kind locally. I'll be sharing

100
00:06:38,662 --> 00:06:42,366
all links, snippets and resources I mentioned in the talk in a gist and

101
00:06:42,388 --> 00:06:45,040
you can find the link for the same down below.

102
00:06:46,610 --> 00:06:50,122
Next up we have Kubectl, the Kubernetes

103
00:06:50,186 --> 00:06:53,870
command line tool which allows us to run commands against Kubernetes clusters.

104
00:06:54,450 --> 00:06:58,286
And finally we have helps helps helps

105
00:06:58,318 --> 00:07:01,634
us manage Kubernetes applications via hem charts which help

106
00:07:01,672 --> 00:07:04,914
us define, install and upgrade even

107
00:07:04,952 --> 00:07:08,626
the most complex case applications. So let's

108
00:07:08,658 --> 00:07:12,694
move on to the observability backend as there is currently no one

109
00:07:12,732 --> 00:07:16,498
database that can store all logs, traces and metrics.

110
00:07:16,594 --> 00:07:20,230
We will deploy three different databases and the visualization

111
00:07:20,310 --> 00:07:24,006
tool. So we have Grafana

112
00:07:24,038 --> 00:07:27,340
for dashboards and visualization of the data

113
00:07:27,710 --> 00:07:31,038
we have from Mythius. It records real time metrics in a

114
00:07:31,044 --> 00:07:34,426
time series database. It allows for high dimensionality

115
00:07:34,458 --> 00:07:37,630
with flexible queries and real time alerting.

116
00:07:39,010 --> 00:07:42,010
We'll use Loki which is a horizontally scalable,

117
00:07:42,090 --> 00:07:45,806
highly available and multi tenant log aggregation system inspired

118
00:07:45,838 --> 00:07:50,238
by Prometheus. And finally we'll use tempo,

119
00:07:50,334 --> 00:07:54,702
which is an easy to use and highly scalable

120
00:07:54,766 --> 00:07:58,818
distributed tracing backend. Tempo is cost efficient

121
00:07:58,914 --> 00:08:02,760
and requires only object storage to operate and

122
00:08:03,370 --> 00:08:07,094
naturally it offers very deep integrations with Grafana, Prometheus and

123
00:08:07,132 --> 00:08:11,430
Loki. So now we need the observability

124
00:08:11,510 --> 00:08:14,630
control plane which will automatically instrument

125
00:08:14,710 --> 00:08:18,582
our applications and for that we'll be using Odigos.

126
00:08:18,726 --> 00:08:22,326
So Audiogos is an open source observability control plane that helps

127
00:08:22,358 --> 00:08:25,690
us in two ways. Firstly, automatic instrumentation.

128
00:08:26,030 --> 00:08:29,850
Odigos automatically instruments your applications and produces

129
00:08:29,930 --> 00:08:33,054
distributed traces and metrics without any code changes.

130
00:08:33,252 --> 00:08:37,230
And collector management which is audibles automatically deploys

131
00:08:37,310 --> 00:08:41,266
and scales collectors according to application traffic. So we

132
00:08:41,368 --> 00:08:45,410
don't need to spend a lot of time deploying and configuring collectors.

133
00:08:46,890 --> 00:08:50,646
So now for this demonstration, our target application

134
00:08:50,748 --> 00:08:54,534
will be a microservices based application written in

135
00:08:54,652 --> 00:08:58,498
Java or Python. We'll be using a fork of the bank of

136
00:08:58,604 --> 00:09:02,362
Anthos example application which was made by Google, and you can find

137
00:09:02,416 --> 00:09:06,054
the link in the resource section. You can deploy

138
00:09:06,102 --> 00:09:10,246
the target application by simply running the following code

139
00:09:10,368 --> 00:09:12,000
and you should be good to go.

140
00:09:13,810 --> 00:09:16,602
To install the observability backend,

141
00:09:16,746 --> 00:09:20,442
we'll execute the following helm chart which deploys tempo, the traces

142
00:09:20,506 --> 00:09:23,754
database, Prometheus, the metrics database and Loki,

143
00:09:23,802 --> 00:09:27,634
the logs database, as well as a pre configured Grafana instance with

144
00:09:27,672 --> 00:09:31,554
those databases as the data sources. So once

145
00:09:31,592 --> 00:09:35,390
our test application is up and running, our observability databases

146
00:09:35,470 --> 00:09:39,638
have been set up and ready to receive data. We'll install

147
00:09:39,724 --> 00:09:43,410
add Audigos as the control plane to collect and transfer logs,

148
00:09:43,490 --> 00:09:46,706
metrics and traces from our applications to the obsolete

149
00:09:46,738 --> 00:09:50,626
database. So to install audios via the helm chart,

150
00:09:50,818 --> 00:09:54,026
you just need to execute the following commands and after all

151
00:09:54,048 --> 00:09:57,466
the odds in the audio system namespace are running, we need to

152
00:09:57,488 --> 00:10:00,438
open the audios UI by running the port forward command.

153
00:10:00,614 --> 00:10:05,018
Once that's done, all you need to do is navigate to localhost

154
00:10:05,194 --> 00:10:08,080
3000 to see your audio setup and ready to go.

155
00:10:09,010 --> 00:10:12,894
So now there are two ways to select which applications audiogo should

156
00:10:12,932 --> 00:10:16,610
instrument. The first one is opting out basically

157
00:10:16,680 --> 00:10:20,578
which will instrument everything, including every new application that will be deployed in

158
00:10:20,584 --> 00:10:24,306
the future. You can still exclude any application that you

159
00:10:24,328 --> 00:10:27,634
do not want to be instrumented, and the other way is opting in where

160
00:10:27,672 --> 00:10:31,170
you select only a certain few applications that you want to be instrumented.

161
00:10:31,250 --> 00:10:34,726
And for the purposes of this tutorial, we'll use the simple way,

162
00:10:34,748 --> 00:10:37,880
but which is opt out and instrumenting everything.

163
00:10:38,590 --> 00:10:41,942
So the next step is to tell audios how to reach the three databases

164
00:10:42,006 --> 00:10:45,722
that we just deployed. And to do that we'll just add

165
00:10:45,776 --> 00:10:49,542
the following three destinations

166
00:10:49,686 --> 00:10:53,266
inside audiogos, Loki, Prometheus and tempo.

167
00:10:53,318 --> 00:10:56,686
To do that, you just need to enter these URLs as mentioned over here.

168
00:10:56,788 --> 00:11:00,286
These are also present in the resources section, so feel free to use them

169
00:11:00,308 --> 00:11:01,120
from there.

170
00:11:06,360 --> 00:11:10,016
And all we need to do after this is wait a few seconds for audios

171
00:11:10,048 --> 00:11:13,904
to finish deploying the required collectors and instrument the target

172
00:11:13,952 --> 00:11:17,348
applications. And now finally,

173
00:11:17,514 --> 00:11:20,724
the last step is to explore observability

174
00:11:20,772 --> 00:11:24,516
data in Grafana. We can now see and correlate metrics

175
00:11:24,548 --> 00:11:28,676
to traces to logs in order to dive deeply into how our application behaves.

176
00:11:28,788 --> 00:11:32,044
And to do that, we'll need to port forward a Grafana instance by

177
00:11:32,082 --> 00:11:35,944
running the following command. Then we navigate to localhost,

178
00:11:36,072 --> 00:11:39,196
enter admin as the default username, and for the password, you need to enter

179
00:11:39,218 --> 00:11:42,664
the output of this following command. Please note that there's a percentage

180
00:11:42,712 --> 00:11:46,268
symbol at the end of the string. Make sure that you remove that while you're

181
00:11:46,284 --> 00:11:47,920
venturing it into the password.

182
00:11:48,980 --> 00:11:51,810
And now time to see the power of data.

183
00:11:52,180 --> 00:11:56,372
So just log on to your grafana instance and

184
00:11:56,426 --> 00:12:00,352
we'll start by viewing a service graph of a microservices

185
00:12:00,416 --> 00:12:03,910
application. To do that, just go to explore over here,

186
00:12:04,600 --> 00:12:07,050
make sure tempo is selected on top,

187
00:12:07,900 --> 00:12:11,592
choose the service graph and just run

188
00:12:11,646 --> 00:12:15,364
the query. And there you go, you have a basic

189
00:12:15,412 --> 00:12:19,370
node graph available to you with a single user service application.

190
00:12:20,460 --> 00:12:23,870
Now let's use some metrics, right? So we'll click on the user service

191
00:12:24,240 --> 00:12:27,836
and we'll choose request rate. And there

192
00:12:27,858 --> 00:12:31,212
you go. A graph for all of the metrics that we want is present

193
00:12:31,266 --> 00:12:35,052
to us. There are three kinds of metrics that Audigo supports.

194
00:12:35,196 --> 00:12:38,240
Firstly, it's metrics related to running of the application,

195
00:12:38,310 --> 00:12:41,760
that is number of HTTP requests, latency, DB concoction.

196
00:12:42,180 --> 00:12:45,616
Secondly, metrics related to the language runtime, that is threads,

197
00:12:45,648 --> 00:12:49,072
heaps, all of that and metrics related to the host environments,

198
00:12:49,136 --> 00:12:52,100
which is cpu, memory and disk usage.

199
00:12:52,840 --> 00:12:56,256
So now let's look at some traces. So this time

200
00:12:56,298 --> 00:13:00,840
we'll click on the user service and just select histogram.

201
00:13:02,780 --> 00:13:06,536
And in order to correlation metrics to traces we'll use a

202
00:13:06,558 --> 00:13:09,828
feature called exemplars. To show exemplars we'll

203
00:13:09,844 --> 00:13:13,132
need to click on options over here and just

204
00:13:13,186 --> 00:13:16,716
enable exemplars. These tiny diamonds will be

205
00:13:16,738 --> 00:13:20,124
now visible to you. You can just select any of these and click

206
00:13:20,162 --> 00:13:23,676
on query with tempo. And there you go. A trace like

207
00:13:23,698 --> 00:13:27,168
this should be present to you. You can see exactly how much time

208
00:13:27,254 --> 00:13:31,088
each part of the entire request took. And digging into one of

209
00:13:31,094 --> 00:13:34,248
the sections will show additional information such as database queries,

210
00:13:34,364 --> 00:13:38,356
HTTP headers and. Yeah, now if

211
00:13:38,378 --> 00:13:42,192
you want to drill down further, we'll go into logs,

212
00:13:42,336 --> 00:13:45,904
which is through Loki, and you can just simply

213
00:13:45,952 --> 00:13:49,684
query the relevant logs as we need it. So to that we'll just firstly

214
00:13:49,732 --> 00:13:53,496
choose our namespace and

215
00:13:53,598 --> 00:13:58,440
use this traces id as an identifier

216
00:14:00,060 --> 00:14:03,870
and just run this query. And there you go, you have

217
00:14:04,480 --> 00:14:08,264
all the data relevant to this particular trace and you can map

218
00:14:08,312 --> 00:14:11,784
what it did end to end. And that is your observability

219
00:14:11,832 --> 00:14:13,070
framework for you guys.

220
00:14:14,980 --> 00:14:18,524
So we've learned how easy it can be to extract and ship

221
00:14:18,572 --> 00:14:21,788
logs, traces and metrics using only open source solutions.

222
00:14:21,884 --> 00:14:25,456
And in addition we were also able to generate traces, metrics and logs from an

223
00:14:25,478 --> 00:14:29,424
application within minutes. We also have the ability to correlation

224
00:14:29,472 --> 00:14:32,516
between different signals. We correlated metrics to traces and

225
00:14:32,538 --> 00:14:36,196
traces to logs, and most importantly we have all the data we

226
00:14:36,218 --> 00:14:39,736
need to quickly detect and fix production issues on

227
00:14:39,758 --> 00:14:43,000
target applications. So great,

228
00:14:43,150 --> 00:14:46,168
we will now be able to detect and have all the data to diagnose and

229
00:14:46,174 --> 00:14:50,596
fix issues. But how do we approach incident management here for quicker resolution?

230
00:14:50,788 --> 00:14:54,204
So an appropriately observable system is definitely a must for quick

231
00:14:54,242 --> 00:14:57,708
resolution. But make sure that you don't lose the war against

232
00:14:57,794 --> 00:15:01,580
time during firefights due to human errors or miscommunication.

233
00:15:02,080 --> 00:15:05,576
Assess whether your organization could benefit from an incident response

234
00:15:05,608 --> 00:15:08,464
tooling like Zend to keep responders on the same page,

235
00:15:08,582 --> 00:15:12,172
harness context rich data from your observatory plane and enable

236
00:15:12,236 --> 00:15:15,490
your team to jump back from issues as fast as they can.

237
00:15:16,340 --> 00:15:19,744
And that's all my time. Thanks a lot for tuning in,

238
00:15:19,782 --> 00:15:22,876
and I hope you take something valuable away from this session.

239
00:15:23,068 --> 00:15:26,696
The resources link is right here. Again, feel free to reach out to me me

240
00:15:26,798 --> 00:15:30,376
on Twitter or LinkedIn in case you have any questions. And have

241
00:15:30,398 --> 00:15:31,110
a great day.

