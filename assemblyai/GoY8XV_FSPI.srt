1
00:00:23,050 --> 00:00:26,182
Hi, welcome to Con for Machine Learning

2
00:00:26,236 --> 00:00:29,378
2022 conference. I am Karan Singh,

3
00:00:29,474 --> 00:00:32,950
senior principal architect and developer evangelist at Rodhat.

4
00:00:33,530 --> 00:00:36,674
In this talk, I will explain how to deploy

5
00:00:36,722 --> 00:00:40,562
and use edge to core machine learning data pipelines

6
00:00:40,626 --> 00:00:44,120
on kubernetes for a smart city use case.

7
00:00:44,490 --> 00:00:47,734
We will also learn how to automate everything

8
00:00:47,852 --> 00:00:51,200
using ansible it. Let's get started.

9
00:00:51,890 --> 00:00:55,726
Let's start with the business use case. A business requirement. We need to

10
00:00:55,748 --> 00:00:59,102
build an app which or a solution. It's not a single app,

11
00:00:59,156 --> 00:01:03,054
it's a collection of apps. We need to build a tax stack that should help

12
00:01:03,092 --> 00:01:06,926
us reduce some congestion. And we need to charge

13
00:01:07,038 --> 00:01:10,834
vehicles some kind of fee as they drive into a city area.

14
00:01:10,952 --> 00:01:14,482
So we have chosen an area in London called as

15
00:01:14,616 --> 00:01:18,086
ultra low emission zone, in which there is

16
00:01:18,108 --> 00:01:22,242
a special charge that the vehicle needs to pay because the authorities

17
00:01:22,306 --> 00:01:25,782
wants to protect or because

18
00:01:25,836 --> 00:01:28,918
of environmental reasons they want not all vehicles should just

19
00:01:28,924 --> 00:01:32,374
enter into the city. So yeah, reduce some congestion, reduce some pollution,

20
00:01:32,502 --> 00:01:36,540
charge some dirty vehicle fees. If the vehicle does not

21
00:01:37,150 --> 00:01:40,446
meets emission standards, we want to apply more charges so

22
00:01:40,468 --> 00:01:44,234
that the owner will not better the city very frequently.

23
00:01:44,362 --> 00:01:47,630
And maybe a third use case would be, hey, I want to locate a wanted

24
00:01:47,700 --> 00:01:49,680
vehicle, right,

25
00:01:51,170 --> 00:01:54,802
which the officials can use the system

26
00:01:54,856 --> 00:01:58,142
for. So that's a business, very high level business requirement.

27
00:01:58,206 --> 00:02:01,314
And we're going to walk together on this and touch

28
00:02:01,352 --> 00:02:03,940
upon how we can capture all these things.

29
00:02:04,550 --> 00:02:08,338
So that's the techsu I was talking about.

30
00:02:08,424 --> 00:02:11,890
I'm going to show you lots of tech, lots of, lots of tools.

31
00:02:11,970 --> 00:02:14,390
Open source and red hat tools,

32
00:02:15,530 --> 00:02:19,218
lots of tool. And each of them have their specific reason why

33
00:02:19,244 --> 00:02:22,374
they exist on this solution. Right. We don't want to over engineer,

34
00:02:22,422 --> 00:02:25,542
but yeah, there is a special place for Kafka, for Ceph,

35
00:02:25,606 --> 00:02:29,450
for superset, for Grafana, right? For databases,

36
00:02:30,050 --> 00:02:33,918
obviously. Kubernetes is the heart, the beating heart for

37
00:02:34,004 --> 00:02:37,530
all the solution. Ansible is going to be the magic

38
00:02:37,610 --> 00:02:41,774
through which we're going to create some tech soup on

39
00:02:41,812 --> 00:02:45,514
this. So yes, before we go into

40
00:02:45,572 --> 00:02:49,330
the Morse presentation mode I'm using to go and show you,

41
00:02:49,480 --> 00:02:53,374
let's deployed, let's deploy this thing right now and let's use ansible

42
00:02:53,422 --> 00:02:56,966
to do it for us. And while Ansible will do the heavy lifting in the

43
00:02:56,988 --> 00:03:00,546
background, I'm going to take you over to a few more journey,

44
00:03:00,578 --> 00:03:04,226
like what I'm doing in this actual implementation.

45
00:03:04,338 --> 00:03:07,910
Meanwhile, the system will be on its toes.

46
00:03:08,490 --> 00:03:12,106
So what you guys are seeing here is I have my terminal opened up

47
00:03:12,128 --> 00:03:16,278
here, I've already logged in to my openshift Cli.

48
00:03:16,454 --> 00:03:19,638
On the other side we have Openshift console through which we're

49
00:03:19,654 --> 00:03:23,326
going to see how things operate. So step one, there is

50
00:03:23,348 --> 00:03:27,482
nothing on the system right now. It's a vanilla, not vanilla,

51
00:03:27,546 --> 00:03:31,002
but it's an empty openshift cluster.

52
00:03:31,066 --> 00:03:34,754
And that's the first workload, the smart city workload we want to

53
00:03:34,792 --> 00:03:38,190
deploy on this cluster. So starting with starting the basics,

54
00:03:38,270 --> 00:03:42,382
create a new project, right? New project OC,

55
00:03:42,446 --> 00:03:46,082
new project called a smart City. You want to run this?

56
00:03:46,136 --> 00:03:49,426
Okay, we are into the project and the CLI is smart.

57
00:03:49,538 --> 00:03:53,602
It have already added me to the project. Next I'm going to run an Ansible

58
00:03:53,666 --> 00:03:56,918
playbook, which I'm going to explain you in a

59
00:03:56,924 --> 00:03:59,914
few slides. So bear with me. This is going to do a lot of things

60
00:03:59,952 --> 00:04:03,514
in the background. This process will take close to ten

61
00:04:03,552 --> 00:04:07,466
to 15 minutes. Meanwhile, we'll go and explore how

62
00:04:07,488 --> 00:04:10,630
the overall solution looks like. So we're going to run ansible.

63
00:04:10,710 --> 00:04:13,486
So those of you who are familiar with Ansible, Ansible playbook is the command to

64
00:04:13,508 --> 00:04:17,166
do it. I'm going to run it from local because I have the

65
00:04:17,188 --> 00:04:21,566
connection established on my local machine. And then this

66
00:04:21,588 --> 00:04:24,722
is the main master playbook we're going to run against. So I'm going to hit

67
00:04:24,776 --> 00:04:28,194
this. And meanwhile this is running. Let me

68
00:04:28,232 --> 00:04:32,340
pull up my project, smart city.

69
00:04:32,790 --> 00:04:37,030
The project is here. And just go to my favorite

70
00:04:37,930 --> 00:04:41,846
section, workloads and parts. So we're going to see lots of

71
00:04:41,868 --> 00:04:45,910
stuff coming up, right. So coming back to my deck,

72
00:04:46,970 --> 00:04:50,650
back into presentation mode, system will take some time

73
00:04:50,720 --> 00:04:54,682
to set up. Let's understand, let's understand, meanwhile what

74
00:04:54,736 --> 00:04:58,726
is happening under the covers. So from the solution

75
00:04:58,838 --> 00:05:02,286
design point of view, okay, we have this city of

76
00:05:02,308 --> 00:05:05,994
London ultra low emission zone. We have all these cameras installed

77
00:05:06,042 --> 00:05:09,854
across various toll locations across the city. And the use

78
00:05:09,892 --> 00:05:13,170
case is that each toll location that we're calling

79
00:05:13,320 --> 00:05:17,330
as an edge, right. Each edge location will recognize

80
00:05:17,670 --> 00:05:21,742
the vehicle, the passing vehicle, and detect its license plate.

81
00:05:21,886 --> 00:05:25,230
And through machine learning models we're

82
00:05:25,730 --> 00:05:29,560
going to grab the string that the model detects in real time

83
00:05:30,010 --> 00:05:33,746
from all these edge locations and append some metadata

84
00:05:33,778 --> 00:05:37,542
to it. Like okay, what is the timestamp through which

85
00:05:37,676 --> 00:05:41,466
this image was captured? What is the geolat long? So that we can do all

86
00:05:41,488 --> 00:05:44,730
sorts of amazing stuff once we have a lot of data into the system.

87
00:05:44,880 --> 00:05:49,030
Okay, but the data is being generated at the edge

88
00:05:49,190 --> 00:05:52,814
and there are multiple edge locations here. How we can move

89
00:05:52,852 --> 00:05:56,270
the data to the central data center,

90
00:05:56,340 --> 00:05:59,854
to the central system where we can do

91
00:05:59,892 --> 00:06:03,834
lots of compute and analytics, do some fee calculation

92
00:06:03,882 --> 00:06:07,266
on the data that we have collected, apply some more business logic like,

93
00:06:07,288 --> 00:06:11,054
hey, apply some dirty vehicle, dirty charges to the vehicle

94
00:06:11,182 --> 00:06:15,186
and notify the officials in case if

95
00:06:15,208 --> 00:06:18,658
the system finds out, a lost vehicle.

96
00:06:18,754 --> 00:06:22,246
Right. So remember our use case that I was explaining before. So that's a very

97
00:06:22,268 --> 00:06:26,294
high level design. Part one, in part two.

98
00:06:26,412 --> 00:06:29,942
Great. We are live capturing data across

99
00:06:30,076 --> 00:06:33,414
from all the edge locations onto the central core location.

100
00:06:33,542 --> 00:06:36,954
We have done some business analytics in there. But wait

101
00:06:36,992 --> 00:06:39,914
a minute, we are capturing a lot of data. What should we do with this

102
00:06:39,952 --> 00:06:43,582
data? Well, we should retrain the model because obviously

103
00:06:43,716 --> 00:06:46,766
machine learning, the model has to be

104
00:06:46,788 --> 00:06:50,222
updated as we learn about the data, as we collect more data.

105
00:06:50,276 --> 00:06:54,206
Right. So we need to store vehicle images, license plate images

106
00:06:54,318 --> 00:06:57,458
and license plate strings into the system. Right.

107
00:06:57,544 --> 00:07:01,278
And then retrain the model. We have a model already deployed,

108
00:07:01,294 --> 00:07:04,894
the edge, retrain the model, improve its prediction accuracy,

109
00:07:04,942 --> 00:07:08,278
and then deploy the same model or the new version of the

110
00:07:08,284 --> 00:07:12,178
model across to multiple edge locations. So Openshift helps

111
00:07:12,194 --> 00:07:16,326
to do that. And tools on top of Openshift helps you to

112
00:07:16,348 --> 00:07:20,198
build a solution like this where you have multiple, few hundreds to thousands

113
00:07:20,214 --> 00:07:24,442
of edge locations. You just need a system that

114
00:07:24,496 --> 00:07:28,154
provides you the right tool to do this kind of stuff. So that's part two

115
00:07:28,192 --> 00:07:32,206
of the solution design. Right? Deploying the model at the edge. Let's come

116
00:07:32,228 --> 00:07:35,102
back and understand how these things work.

117
00:07:35,236 --> 00:07:38,942
Right. So this is more kind of a 1000 foot view of the system.

118
00:07:39,076 --> 00:07:42,474
We have multiple edge locations running Openshift

119
00:07:42,522 --> 00:07:46,554
environment. It could be fat edge where

120
00:07:46,612 --> 00:07:50,018
there could be a three node openshift cluster or maybe a very

121
00:07:50,104 --> 00:07:53,646
thin location where they just need a single node, a single node openshift.

122
00:07:53,678 --> 00:07:56,020
Right. That's really a thing right now.

123
00:07:57,110 --> 00:08:00,690
So as the video streams produce images, live images

124
00:08:00,770 --> 00:08:04,118
from the system, the images will go

125
00:08:04,124 --> 00:08:08,342
into a license plate recognition model. This model will then extract

126
00:08:08,406 --> 00:08:12,090
out the areas of the image where it detects

127
00:08:12,430 --> 00:08:15,690
a potential license plate, and then it will basically

128
00:08:15,760 --> 00:08:18,906
give this subset of the image to

129
00:08:18,928 --> 00:08:22,894
an OCR model, optical character recognition model, which will read the

130
00:08:22,932 --> 00:08:26,542
letters from the number plate. Right. It's pretty standard. And then

131
00:08:26,596 --> 00:08:30,350
once we have, once you have data onto the edge,

132
00:08:30,930 --> 00:08:34,282
what we'll use, we'll use Kafka. Kafka is the go to technology

133
00:08:34,436 --> 00:08:38,302
to handle this kind of workload. So we'll use Kafka, Kafka producer,

134
00:08:38,366 --> 00:08:41,646
and then Kafka producer will produce some messages

135
00:08:41,678 --> 00:08:45,474
onto the local Kafka cluster running on the edge. But now

136
00:08:45,512 --> 00:08:49,334
here's the magic. We have lots of edge locations and we need to move

137
00:08:49,372 --> 00:08:52,470
the data. We need to build this loosely coupled system

138
00:08:52,620 --> 00:08:55,478
that should handle the network disruptions, right?

139
00:08:55,564 --> 00:08:58,338
So Kafka does it pretty well for us.

140
00:08:58,524 --> 00:09:02,518
All the events, all the metadata will be stored on the local Kafka

141
00:09:02,534 --> 00:09:06,506
edge clusters. And then asynchronously it

142
00:09:06,528 --> 00:09:10,506
will move the data from edge to core using a technology or using

143
00:09:10,528 --> 00:09:13,886
a feature called as Kafka Mirror maker, which will move

144
00:09:13,908 --> 00:09:17,562
the data from one edge or maybe thousands of edge locations

145
00:09:17,626 --> 00:09:21,278
onto the core central data point, which is

146
00:09:21,364 --> 00:09:25,150
Kafka again. So we are capturing the data onto the central Kafka

147
00:09:25,230 --> 00:09:28,782
cluster. And once we have the data onto the central

148
00:09:28,846 --> 00:09:31,742
Kafka subsystem, fantastic.

149
00:09:31,806 --> 00:09:35,366
We can do lots of things with it. For example, we can build our own

150
00:09:35,388 --> 00:09:39,174
Kafka consumers, maybe a python go or Quarkus apps that

151
00:09:39,212 --> 00:09:42,950
can read and tap into the Kafka topic and

152
00:09:43,020 --> 00:09:46,914
build some business logics, do some wanted vehicle notifications

153
00:09:47,042 --> 00:09:50,154
or else for long term preserving the data

154
00:09:50,192 --> 00:09:53,114
of kafka. Because remember, this is all governmental data,

155
00:09:53,232 --> 00:09:56,310
right? We want to store data historically,

156
00:09:56,390 --> 00:09:59,802
correct. So we need some persistent layer that can

157
00:09:59,856 --> 00:10:03,466
move data that is coming to Kafka. Move into an object storage.

158
00:10:03,498 --> 00:10:07,194
Because object storage is autonomous

159
00:10:07,242 --> 00:10:10,746
storage, correct. You just store it. It's simple to use and it is cheap,

160
00:10:10,778 --> 00:10:14,638
it's low cost. So we'll use something called a secr which can help

161
00:10:14,804 --> 00:10:18,686
secure is by the way, a Kafka consumer which will tap into the Kafka topic,

162
00:10:18,798 --> 00:10:22,626
read the data from Kafka, store that onto object storage bucket. And here

163
00:10:22,648 --> 00:10:26,214
we are using Ceph object storage, which is the

164
00:10:26,252 --> 00:10:29,606
choice. And next, once we

165
00:10:29,628 --> 00:10:32,902
have data onto object storage, we can build,

166
00:10:32,956 --> 00:10:36,722
analyzing and reporting, real time analytics and reporting

167
00:10:36,786 --> 00:10:40,134
using tools provided by open data hub like Superset,

168
00:10:40,182 --> 00:10:42,774
Apache Superset, Open Grafana,

169
00:10:42,902 --> 00:10:46,362
Starbus. So Starbus is a very amazing technology. It can allow

170
00:10:46,416 --> 00:10:50,178
you to write and build distributed

171
00:10:50,214 --> 00:10:54,094
queries across heterogeneous data

172
00:10:54,132 --> 00:10:57,738
sources. Like you can write a SQL query that's very powerful,

173
00:10:57,834 --> 00:11:01,786
which can go and read data from a SQL database,

174
00:11:01,898 --> 00:11:05,586
and at the same time you can join it using data that

175
00:11:05,608 --> 00:11:09,106
is living on the object storage s three interfaces. So this is the

176
00:11:09,128 --> 00:11:13,074
power of superset that

177
00:11:13,112 --> 00:11:18,338
provides you to have this kind of environments,

178
00:11:18,434 --> 00:11:22,054
which is a very powerful engine. And then Superset is basically the

179
00:11:22,092 --> 00:11:26,614
dashboard and reporting part of it, which will show you

180
00:11:26,812 --> 00:11:30,282
in real time the reports. And Grafana. Obviously, to manage

181
00:11:30,336 --> 00:11:33,786
this system, we would need cool developers like you,

182
00:11:33,888 --> 00:11:38,054
as well as some operations people who want to keep an eye Eagle's eye

183
00:11:38,182 --> 00:11:41,518
onto their dashboard. And they need some real time dashboard and

184
00:11:41,524 --> 00:11:45,790
Grafana is like the best choice, at least in my opinion, because I love Grafana.

185
00:11:46,290 --> 00:11:51,006
So this is the overall solution that the

186
00:11:51,028 --> 00:11:53,938
ansible automation under the covers is doing for us,

187
00:11:54,024 --> 00:11:57,394
right? So what ansible is doing for us, let's understand.

188
00:11:57,512 --> 00:12:01,010
It's installing and setting up database for us.

189
00:12:01,080 --> 00:12:05,826
It's installing and setting up Kafka clusters because this

190
00:12:05,848 --> 00:12:09,178
is a demo right now. So Ansible is doing deploying an Kafka

191
00:12:09,214 --> 00:12:13,218
edge cluster as well as a Kafka core cluster on the same openshift environment

192
00:12:13,314 --> 00:12:16,578
that we have. And then ansible going to deploy multiple microservices.

193
00:12:16,674 --> 00:12:20,970
So one of them would be license plate recognition microservice, which includes

194
00:12:21,310 --> 00:12:24,646
using build config builds and then from git

195
00:12:24,678 --> 00:12:27,974
source to images and deploying that into deployment

196
00:12:28,022 --> 00:12:31,534
config and launching the pod and creating the service

197
00:12:31,732 --> 00:12:34,926
and exposing it through the route. So Ansible is actually doing a

198
00:12:34,948 --> 00:12:38,190
lot of work. Similarly, like LPR,

199
00:12:38,690 --> 00:12:41,706
Ansible is setting up microservice for events,

200
00:12:41,738 --> 00:12:44,926
microservices for image server, and microservice for

201
00:12:44,948 --> 00:12:48,850
load generator. Because I don't have a real camera hanging around and

202
00:12:48,920 --> 00:12:51,998
looking at the street. But yeah, so we have a load generator for the sake

203
00:12:52,014 --> 00:12:55,426
of this demo and then setting up secure, which will move the data into

204
00:12:55,448 --> 00:12:58,838
the system. So this is all microservice setup that Ansible will do for us.

205
00:12:58,924 --> 00:13:02,582
And then setting up open data hub which is the collection of

206
00:13:02,636 --> 00:13:06,630
tools which will help us do analytics in real time. And then finally

207
00:13:06,700 --> 00:13:10,282
the Grafana dashboard setup. So Ansible is really

208
00:13:10,336 --> 00:13:13,498
doing the heavy lifting here.

209
00:13:13,584 --> 00:13:16,778
While I'm speaking and talking to you right the moment the system is doing the

210
00:13:16,784 --> 00:13:20,586
job for me and for all the

211
00:13:20,608 --> 00:13:24,670
enthusiasts out there. What all modules I'm using to

212
00:13:24,740 --> 00:13:28,574
make this happen, it is just these eight modules I'm using to

213
00:13:28,612 --> 00:13:31,518
get my system up and running from zero to 100.

214
00:13:31,604 --> 00:13:35,250
So there's a community model called as KS, which basically

215
00:13:35,320 --> 00:13:39,534
uses Openshift client, Python client library client

216
00:13:39,582 --> 00:13:43,394
tool, I think library, which it will call up, and it

217
00:13:43,432 --> 00:13:46,926
has full access to all sorts of openshift objects.

218
00:13:46,958 --> 00:13:50,626
So ansible can just tap onto the KS module and boom,

219
00:13:50,738 --> 00:13:54,114
you can just do whatever you want, you can do with an OC client,

220
00:13:54,162 --> 00:13:57,554
right? Similarly, by the way, OC client does crud,

221
00:13:57,602 --> 00:14:01,414
it will do and change the objects while Ks info another module,

222
00:14:01,462 --> 00:14:04,986
it will go and read out basically all

223
00:14:05,008 --> 00:14:08,886
the get commands that you do, like OC get or kubectl get. Ks info

224
00:14:08,918 --> 00:14:12,314
will do the more or less the same stuff. We are using set facts

225
00:14:12,362 --> 00:14:16,298
because we have to manage the state and dynamically update the inventories

226
00:14:16,394 --> 00:14:19,454
and have some variables that we wanted to capture into

227
00:14:19,492 --> 00:14:23,086
the system. I'm running some commands because there

228
00:14:23,108 --> 00:14:26,050
are certain things which are still complex and I don't have a module for.

229
00:14:26,120 --> 00:14:29,918
So I'm like supercept, superset, data updations.

230
00:14:30,014 --> 00:14:34,222
I'm using some commands for it. I'm using copy modules to copy images

231
00:14:34,366 --> 00:14:37,978
from my data store to s three and then adding

232
00:14:38,014 --> 00:14:41,494
host to the inventory s three, sync to move the data and a

233
00:14:41,532 --> 00:14:44,642
pause just for sake of completion.

234
00:14:44,706 --> 00:14:47,894
Like I'm pausing it just so that system can get stable. So yeah,

235
00:14:47,932 --> 00:14:51,386
look at this. I mean, I'm just using the eight modules to do a

236
00:14:51,408 --> 00:14:55,146
lot of heavy lifting using ansible. And Ansible is a really powerful tool to

237
00:14:55,168 --> 00:14:57,500
do this kind of automation for you.

238
00:14:58,270 --> 00:15:02,602
Okay, let's now quickly check how does ansible, Ks module

239
00:15:02,666 --> 00:15:05,854
or ansible usage will look like you will write something like this.

240
00:15:05,892 --> 00:15:12,078
Okay, Ks module, the state is present and I

241
00:15:12,084 --> 00:15:15,698
need to apply multiple yaml files like, okay, create a secret, create a

242
00:15:15,704 --> 00:15:19,326
deployment config, create a service, create image stream,

243
00:15:19,358 --> 00:15:23,540
create build config. So ks module and ansible will make sure you are

244
00:15:23,910 --> 00:15:27,734
applying these yaml files like you do in your OC create or

245
00:15:27,772 --> 00:15:31,410
OC apply minus f command right and into the right namespace.

246
00:15:31,570 --> 00:15:35,640
So that's what Ks module will do. It's pretty simple

247
00:15:36,410 --> 00:15:39,938
to use and then KTas info is doing the other way.

248
00:15:40,044 --> 00:15:43,446
It's basically getting the data from the kube API.

249
00:15:43,558 --> 00:15:46,982
Okay, tell me the secret and the name of the secret is this. And register

250
00:15:47,046 --> 00:15:50,518
the value in a postgreSQl register variable.

251
00:15:50,614 --> 00:15:54,666
And then later on you set facts to pull out secrets

252
00:15:54,698 --> 00:15:58,638
like database username, passwords and name that you can later on use in your

253
00:15:58,644 --> 00:16:02,786
playbook. So this is how you will create these

254
00:16:02,808 --> 00:16:06,574
ansible playbooks for your lovely Pacman

255
00:16:06,622 --> 00:16:09,954
apps. Okay, and all

256
00:16:09,992 --> 00:16:13,250
of the code is available onto the git repository. It's open.

257
00:16:13,320 --> 00:16:17,062
Go and look it out. Check out. Happy to get some prs. If you guys

258
00:16:17,196 --> 00:16:20,646
interested, Edson will share some links to

259
00:16:20,668 --> 00:16:24,022
the link and credits to my friend Chris Bloom who is an awesome

260
00:16:24,076 --> 00:16:27,842
guy who helped us with lot of ansible automation

261
00:16:27,906 --> 00:16:31,206
around that you're going to see. Okay, let me go and get back

262
00:16:31,228 --> 00:16:34,746
to my system. Okay, so the system is still doing the job. You can

263
00:16:34,768 --> 00:16:38,186
see that ansible, I'm going to scroll up a little bit and show

264
00:16:38,208 --> 00:16:42,142
you. It's not very user friendly, but since

265
00:16:42,196 --> 00:16:45,898
I'm using this in a very narrow window.

266
00:16:45,994 --> 00:16:49,386
But yeah, look at this. Deploying databases deployed

267
00:16:49,418 --> 00:16:53,650
Kafka clusters and deployed Kafka drop APIs

268
00:16:54,230 --> 00:16:57,842
and then all sorts of stuff that I explained to you.

269
00:16:57,896 --> 00:17:01,426
So system is still doing the job. So bear with the system.

270
00:17:01,608 --> 00:17:02,740
Let me see.

271
00:17:06,390 --> 00:17:09,366
Yes, so I'm into all projects. Let me go to smart city.

272
00:17:09,468 --> 00:17:12,680
Project smart city is here.

273
00:17:13,050 --> 00:17:16,198
And yes, let me see if I'm getting any failures. No, there's no

274
00:17:16,204 --> 00:17:18,280
failure at the moment. So things are.

275
00:17:21,070 --> 00:17:24,710
So at this point, Ansible is importing some data source into superset.

276
00:17:24,790 --> 00:17:28,698
So at this point it has done lots of setup already.

277
00:17:28,864 --> 00:17:32,438
We are into the superset part of the equation.

278
00:17:32,614 --> 00:17:36,430
Meanwhile, while this is getting ready, I have another environment where

279
00:17:36,500 --> 00:17:39,280
I was pretty sure that this is going to take a little bit of time.

280
00:17:40,050 --> 00:17:43,674
So I'm going to switch my window. It's another environment

281
00:17:43,722 --> 00:17:46,570
which is completely set up. So I'm not cheating here, I guess,

282
00:17:46,660 --> 00:17:50,194
or you guys can let me know if I'm cheating. So just to save time

283
00:17:50,232 --> 00:17:53,826
here. So this is how the end result will

284
00:17:53,848 --> 00:17:57,878
look like you're going to deploy lots of containers, lots of pods, lots of services.

285
00:17:58,044 --> 00:18:01,446
And Ansible is basically automating and

286
00:18:01,628 --> 00:18:04,870
doing all the heavy lifting here. So this is my developer view.

287
00:18:04,940 --> 00:18:08,842
If you are a fan of administration view, go to the right project

288
00:18:08,976 --> 00:18:11,100
from here, smart city,

289
00:18:12,190 --> 00:18:15,546
and then you will see. Okay, great. I have lots of

290
00:18:15,568 --> 00:18:19,670
pods running into the system, I guess. Yeah, 36 pods.

291
00:18:19,830 --> 00:18:23,294
These are jobs. But yeah, 36 pods up and running at the moment right

292
00:18:23,332 --> 00:18:26,478
now, which is making happen this demo. So I'll open

293
00:18:26,644 --> 00:18:31,086
one of my route, which is the dashboard, Grafana dashboard for

294
00:18:31,268 --> 00:18:34,370
the bird's eye view of the city. So as I mentioned before,

295
00:18:34,440 --> 00:18:37,954
this is city of London. And all these black boxes here are

296
00:18:37,992 --> 00:18:41,586
camera, and we have this generator which

297
00:18:41,608 --> 00:18:45,422
is generating images or like simulating

298
00:18:45,486 --> 00:18:49,078
vehicles which are passing from the station in real time.

299
00:18:49,164 --> 00:18:52,482
We are capturing the count of the vehicle. We are detecting the vehicle,

300
00:18:52,546 --> 00:18:55,526
last known vehicle from the system.

301
00:18:55,628 --> 00:18:59,894
We are detecting its license plate, which is the optical character recognition,

302
00:18:59,942 --> 00:19:03,706
and the license plate plate recognition model of

303
00:19:03,728 --> 00:19:07,354
the vehicle and who is the owner. So by the way, these names are

304
00:19:07,392 --> 00:19:08,650
just made up names.

305
00:19:10,750 --> 00:19:14,654
And here's a nice graph which can tell you this is a rich data

306
00:19:14,692 --> 00:19:17,630
that we are collecting. Okay, please tell me, out of the city,

307
00:19:17,700 --> 00:19:20,542
what are the top stations we have in the city?

308
00:19:20,596 --> 00:19:23,934
Right? So station one a is very popular because

309
00:19:23,972 --> 00:19:27,314
there are so many vehicles passing from the system. So you know what, these are

310
00:19:27,352 --> 00:19:30,722
some business metrics and data that you can take up.

311
00:19:30,776 --> 00:19:34,514
I'm going to real quick show you the cpu chart because I like this,

312
00:19:34,552 --> 00:19:37,846
because it's a gif and it's pretty awesome to see

313
00:19:37,868 --> 00:19:41,586
how this looks like. Okay, as I mentioned before, we have edge Openshift.

314
00:19:41,618 --> 00:19:45,378
Edge openshift core. We have Kafka and inferencing

315
00:19:45,474 --> 00:19:49,394
happening on the edge. And data flows from the edge via mirror maker

316
00:19:49,442 --> 00:19:52,554
onto the score Kafka cluster, and via secure data,

317
00:19:52,592 --> 00:19:55,994
move to ceph self object storage. And then later we use Starburst and

318
00:19:56,032 --> 00:19:59,290
PostgreSQL and other tools to do

319
00:19:59,360 --> 00:20:02,814
some dashboarding around it, which looks like this.

320
00:20:03,012 --> 00:20:06,302
So this is the view for your managers and

321
00:20:06,436 --> 00:20:09,806
your key stakeholders. This is a reporting view that they will see.

322
00:20:09,908 --> 00:20:13,966
Okay, so far we have done, like 215,000 pounds

323
00:20:13,998 --> 00:20:17,566
of collection, toll fee collection, and 100,000 pounds

324
00:20:17,598 --> 00:20:22,242
of pollution fees. And 45,000

325
00:20:22,296 --> 00:20:25,522
vehicles have passed from the city. Right? Again, this is all

326
00:20:25,576 --> 00:20:29,190
generated data, similar data. However, just to give you an idea that

327
00:20:29,260 --> 00:20:33,234
what you can do with these tools once they all work in tandem,

328
00:20:33,362 --> 00:20:37,358
and then we have these nice panels in superset

329
00:20:37,474 --> 00:20:40,506
which you can use, adjust it according to your business case,

330
00:20:40,608 --> 00:20:44,634
and you can get very precise metrics about

331
00:20:44,672 --> 00:20:48,140
your business. Okay, station number

332
00:20:48,510 --> 00:20:52,318
5201 is getting 22% of the total traffic of the

333
00:20:52,324 --> 00:20:55,774
city, which means we need to do something here, right? Maybe add one more

334
00:20:55,812 --> 00:20:59,230
lane, maybe build a superpath here,

335
00:20:59,300 --> 00:21:02,974
right? And then again, vehicle, so not very

336
00:21:03,012 --> 00:21:06,274
interesting. But anyways, what type of vehicles are more

337
00:21:06,312 --> 00:21:09,874
popular in the city? Okay, Nissans are popular, or Audi R eight

338
00:21:09,912 --> 00:21:13,442
are popular? We can debate on that. And some

339
00:21:13,576 --> 00:21:17,362
table panels that. Okay, tell me, my top

340
00:21:17,416 --> 00:21:20,758
consumers, who needs to pay a lot of money to the

341
00:21:20,764 --> 00:21:23,862
government? Okay, so Suzanne King needs to pay, like,

342
00:21:23,916 --> 00:21:27,126
whatever, 15,000 pounds. Oh, why? That's a

343
00:21:27,148 --> 00:21:30,690
lot of money to the government. But anyways, this is a fake data,

344
00:21:30,780 --> 00:21:34,406
and you get an idea, right. This is the view for your managers

345
00:21:34,438 --> 00:21:37,926
that they can enjoy. And openshift and tools

346
00:21:37,958 --> 00:21:42,586
and projects running on openshift can make it happen. And all

347
00:21:42,608 --> 00:21:45,898
the nifty Kafka lovers here, I have seen that. Kafka. I love Kafka.

348
00:21:45,914 --> 00:21:50,026
Kafka is great. And so I'm using this kafdrop, which is a nice, very nifty

349
00:21:50,058 --> 00:21:53,680
utility. It's an open source project that you can tap into.

350
00:21:54,210 --> 00:21:57,550
You'll deploy it. You have the code in my repo,

351
00:21:57,710 --> 00:22:01,166
so, yes, it will help you to check out Kafka

352
00:22:01,198 --> 00:22:05,074
messages in real time. No need for Kafka cat. Instead, just plug into this.

353
00:22:05,192 --> 00:22:08,898
It's an open source project. When you open this LPR,

354
00:22:08,994 --> 00:22:12,166
and then you will quickly show me the messages. So this

355
00:22:12,188 --> 00:22:15,894
is real time messages coming onto the LPR. Topic at the

356
00:22:15,932 --> 00:22:19,706
core data center right through this URL. So when

357
00:22:19,728 --> 00:22:23,322
I expand this I'm getting events like okay, timestamp and event

358
00:22:23,376 --> 00:22:27,420
id and vehicle license plate and detection is successful and

359
00:22:28,430 --> 00:22:32,000
on which station we are doing the detection those kind of data.

360
00:22:32,770 --> 00:22:37,066
I think we definitely have another deployment

361
00:22:37,098 --> 00:22:40,094
of this. Let me see where we are on my deployment. Okay,

362
00:22:40,132 --> 00:22:44,066
wonderful. Fantastic. So this worked at least demo God is

363
00:22:44,088 --> 00:22:47,874
with me. So ansible is

364
00:22:47,912 --> 00:22:51,874
completed. It has took like 29,

365
00:22:52,072 --> 00:22:55,922
oh yeah, 15 minutes. It has taken 15 minutes to deploy the

366
00:22:55,976 --> 00:22:59,346
entire setup that we have shown. By the way, we have already gone

367
00:22:59,368 --> 00:23:02,438
through all these things that you want to show. The only thing I want to

368
00:23:02,444 --> 00:23:05,442
show you here is if I go to network and see routes.

369
00:23:05,586 --> 00:23:08,598
There are so many routes in here. Odh. OdH is a

370
00:23:08,604 --> 00:23:12,118
great piece of solution. I'm just using two components

371
00:23:12,134 --> 00:23:15,046
of ODF. That's why we don't have a lot of components. But check out ODF.

372
00:23:15,078 --> 00:23:18,906
ODF is pretty cool. And then I

373
00:23:18,928 --> 00:23:22,758
want to show you just last thing I've

374
00:23:22,774 --> 00:23:26,414
shown you core, right? I've shown you core. This is core Kafka cluster. I just

375
00:23:26,452 --> 00:23:29,402
want to show you my edge Kafka cluster as well. So these are the cluster

376
00:23:29,466 --> 00:23:33,418
deployed at the edge. The topic name is LPR license plate

377
00:23:33,434 --> 00:23:37,010
decode recognition. I'm going to view messages and I'll do

378
00:23:37,080 --> 00:23:40,818
quick view. Okay, so the point here is that

379
00:23:40,984 --> 00:23:45,034
we are capturing the message at the edge via mirror maker.

380
00:23:45,182 --> 00:23:48,646
Oh, let me go two steps back. We are doing inferencing at the

381
00:23:48,668 --> 00:23:52,146
edge, detecting the vehicle,

382
00:23:52,258 --> 00:23:55,954
capturing the license plates in real time, putting that strings

383
00:23:56,002 --> 00:23:59,722
onto Kafka topic from the edge, moving the data

384
00:23:59,776 --> 00:24:03,626
to the core. And then once the data is in the core we

385
00:24:03,648 --> 00:24:07,146
are doing analysis on top of the data. Analysis like this.

386
00:24:07,248 --> 00:24:11,726
Analysis like this. So this

387
00:24:11,748 --> 00:24:16,058
is what I have to show you guys. Coming back to my deck.

388
00:24:16,234 --> 00:24:19,534
This ansible openshift and

389
00:24:19,572 --> 00:24:22,800
lot of the tools makes this happen.

390
00:24:23,810 --> 00:24:26,960
Okay, so I think I am

391
00:24:27,570 --> 00:24:30,974
done with my prepared content and this brings us

392
00:24:31,012 --> 00:24:34,334
to the end of the presentation. Thank you so much for your time. I hope

393
00:24:34,372 --> 00:24:36,660
this is what's branding. Have a nice day.

