1
00:00:00,250 --> 00:00:01,630
Are you an SRE,

2
00:00:03,570 --> 00:00:04,830
a developer,

3
00:00:06,610 --> 00:00:10,474
a quality engineer who wants to tackle the challenge of improving

4
00:00:10,522 --> 00:00:14,254
reliability in your DevOps? You can enable your DevOps for

5
00:00:14,292 --> 00:00:17,614
reliability with chaos native. Create your

6
00:00:17,652 --> 00:01:17,030
free account at Chaos native Litmus Cloud hello

7
00:01:17,100 --> 00:01:21,522
and welcome to the conference site Reliability Engineering Conference presentation

8
00:01:21,586 --> 00:01:24,966
on system state clustering using EBPF data. What is our

9
00:01:24,988 --> 00:01:29,058
agenda for today? We will be looking at what is EBPF? What is clustering,

10
00:01:29,154 --> 00:01:32,598
how EBPF and clustering can work together, how EBPF

11
00:01:32,614 --> 00:01:36,346
and SRE work together, what are potential use cases? And we'll be

12
00:01:36,368 --> 00:01:39,846
wrapping up with some question and answers. What is EBPF?

13
00:01:40,038 --> 00:01:43,838
EBPF programs are event driven and are run when kernel or application passes a

14
00:01:43,844 --> 00:01:47,226
certain hook points. Now, what are hook points? Hook points include

15
00:01:47,258 --> 00:01:50,350
system calls, function entry exits, kernel trace points,

16
00:01:50,420 --> 00:01:54,666
network events, and several others. Once your execution passes

17
00:01:54,698 --> 00:01:57,906
that hook point, from there on you can run some of your

18
00:01:57,928 --> 00:02:01,602
own programs, nothing but the EBPF programs. And those EBPF programs can pass

19
00:02:01,656 --> 00:02:04,798
data back to the use space via some data structures called maps.

20
00:02:04,974 --> 00:02:09,170
Just to give you an example, let's say you want to know

21
00:02:09,240 --> 00:02:12,306
how many times a file is being opened. So that is nothing but a file

22
00:02:12,338 --> 00:02:15,974
open system call, right? So you can have an EVPF program defined that

23
00:02:16,012 --> 00:02:19,606
actually doors on to this particular system call, which will be

24
00:02:19,628 --> 00:02:23,174
triggered when your file is opened. And once that program is triggered,

25
00:02:23,302 --> 00:02:27,180
you actually get control into that, and then you can pass

26
00:02:27,630 --> 00:02:31,178
data in and out of the map, which will actually. In which you

27
00:02:31,184 --> 00:02:33,966
can maintain account of how many times a particular file is opened or not.

28
00:02:33,988 --> 00:02:38,270
So that's the basic fundamental of how EBPF works right

29
00:02:38,420 --> 00:02:41,466
now here, explaining the same thing which I was talking about earlier.

30
00:02:41,578 --> 00:02:45,098
So here we have two diagrams

31
00:02:45,114 --> 00:02:48,526
here. Like one is a storage hook EBPF, and the other one is a network

32
00:02:48,558 --> 00:02:52,142
hook, EBPF. So whenever I issue a syscall,

33
00:02:52,286 --> 00:02:55,822
which is going to do a write or read on a particular file,

34
00:02:55,966 --> 00:02:59,394
that syscall actually reads the file descriptor, and then via

35
00:02:59,442 --> 00:03:02,482
the block device, it actually reads and writes to the storage.

36
00:03:02,546 --> 00:03:05,734
Right. And that's where you can actually hook the EBPF program,

37
00:03:05,852 --> 00:03:10,774
and you can trigger that program to understand which

38
00:03:10,812 --> 00:03:13,866
file is opened or how many times a particular file is opened. On the right

39
00:03:13,888 --> 00:03:17,414
hand side, if you see the same kind of behavior

40
00:03:17,462 --> 00:03:20,714
you can get for, let's say you want some program

41
00:03:20,752 --> 00:03:23,722
to be triggered in case of network activity,

42
00:03:23,786 --> 00:03:27,760
like some sort of traffic you have received, or some sort of.

43
00:03:28,450 --> 00:03:31,726
Whenever you want to get information of receiving a

44
00:03:31,748 --> 00:03:34,978
request over a network, you can actually hook an EVPF program into that

45
00:03:35,064 --> 00:03:38,194
and then pass the data about that program back

46
00:03:38,232 --> 00:03:42,254
to the user space via map. So this is how essentially

47
00:03:42,302 --> 00:03:46,018
it works together, delving into it a little bit more.

48
00:03:46,104 --> 00:03:49,526
So how it happens is actually you write code. You write

49
00:03:49,548 --> 00:03:53,266
the EBPF code and then you compile it into a bytecode, and then that bytecode.

50
00:03:53,298 --> 00:03:56,870
Actually there is an EBPF verifier that actually verifies whether your

51
00:03:56,940 --> 00:04:00,294
code doesn't go out of the bounds that

52
00:04:00,332 --> 00:04:04,106
has been set of an EBPF program. So if

53
00:04:04,128 --> 00:04:07,594
it is going out, then the verifier will not execute the EBPF program, because basically

54
00:04:07,632 --> 00:04:11,310
your program is going to execute in the kernel, right? So we can't afford

55
00:04:12,130 --> 00:04:15,806
unsafe programs being executed there. So once your

56
00:04:15,828 --> 00:04:19,486
program satisfies all the verifier and jit bounds, it can be

57
00:04:19,508 --> 00:04:22,366
attached to many hook points, right?

58
00:04:22,468 --> 00:04:26,690
We talked about hook points. So one of those points could be like a class

59
00:04:26,760 --> 00:04:30,226
ingress or any sockets. So you can have socket filters, or you

60
00:04:30,248 --> 00:04:34,178
can have TC ingress or TC ingress. So you

61
00:04:34,184 --> 00:04:37,814
can attach your program in these kind of places, and then whenever you

62
00:04:37,852 --> 00:04:41,062
see any traffic coming or any particular network event

63
00:04:41,116 --> 00:04:44,742
happening, your program will get triggered. That is

64
00:04:44,796 --> 00:04:48,022
essentially how one of the software that I'm going to discuss later,

65
00:04:48,076 --> 00:04:51,466
cilium, that's how essentially it works. Right. Here we look

66
00:04:51,488 --> 00:04:55,594
at an example of an EBPF program, right? In this one.

67
00:04:55,712 --> 00:04:58,646
This is actually an example of how many times a kernel function is entered.

68
00:04:58,678 --> 00:05:02,502
Now our function here that we're trying to trace is exec v Cecv,

69
00:05:02,566 --> 00:05:05,582
right? So you see on top that there is a map, right?

70
00:05:05,636 --> 00:05:09,758
That's a k probe map, which basically says that how many times in

71
00:05:09,764 --> 00:05:13,694
this map you are going to maintain the number of times this particular function was

72
00:05:13,732 --> 00:05:16,622
entered, right? And that is the map.

73
00:05:16,766 --> 00:05:20,206
And in the program section in the functions,

74
00:05:20,238 --> 00:05:23,714
you see the k probe execv function. In that one, you actually

75
00:05:23,752 --> 00:05:27,150
have a map lookup element. So this is basically going to look up into the

76
00:05:27,160 --> 00:05:31,906
map and see how many times already it's there. And it's

77
00:05:31,938 --> 00:05:35,026
just using to update the map. It's going to fetch

78
00:05:35,058 --> 00:05:38,742
and aided one and using to update the map so this

79
00:05:38,796 --> 00:05:42,346
program can be attached. And then once the program is

80
00:05:42,368 --> 00:05:46,266
triggered, it will automatically implement or increment the

81
00:05:46,288 --> 00:05:49,786
number of entries, which basically means the number of times this particular function has

82
00:05:49,808 --> 00:05:53,886
been triggered. Good. So now we

83
00:05:53,908 --> 00:05:57,680
talked about EBPF, right? So EBPF helped us understand

84
00:05:58,530 --> 00:06:02,046
how at a certain execution points we can

85
00:06:02,068 --> 00:06:05,646
actually understand the way the

86
00:06:05,668 --> 00:06:08,866
program is getting triggered. Right. Now, how can that be used with

87
00:06:08,888 --> 00:06:12,286
machine learning? So we are going to talk about a simple use case of machine

88
00:06:12,318 --> 00:06:15,682
learning called clustering, right? So what is clustering? So, clustering is nothing,

89
00:06:15,736 --> 00:06:19,394
but if you have a lot of data and

90
00:06:19,432 --> 00:06:22,774
you want to make sense of what group each data belongs. So, for example,

91
00:06:22,812 --> 00:06:25,830
in this particular figure, right, you have lot of data points,

92
00:06:25,900 --> 00:06:29,974
and the color of the data points basically is telling us the

93
00:06:30,012 --> 00:06:33,642
type of data point. And if you want to actually maintain a fine line between

94
00:06:33,696 --> 00:06:37,190
all these three different types of clusters. So this lining,

95
00:06:37,270 --> 00:06:40,714
or the grouping up of different clusters, different data points

96
00:06:40,752 --> 00:06:44,654
into clusters, is called clustering, right? So it's an algorithm that can help cluster different

97
00:06:44,692 --> 00:06:48,414
data points into classes, right? And the data

98
00:06:48,452 --> 00:06:52,062
points that are similar will be closer to each other.

99
00:06:52,116 --> 00:06:55,550
Right. And when you represent them dimensionally,

100
00:06:55,970 --> 00:06:59,518
ideally it should be closer to each other. And the algorithm that helps

101
00:06:59,534 --> 00:07:03,214
us to understand or to do that, clustering, there are many algorithms

102
00:07:03,262 --> 00:07:06,994
can do that. So the methodology is clustering, and there are many

103
00:07:07,032 --> 00:07:10,566
algorithms to do that. Right? Now, here we

104
00:07:10,588 --> 00:07:14,502
look at an example. It's a very famous example. Whenever any person starts machine learning,

105
00:07:14,556 --> 00:07:17,926
the first thing I believe that I had done was

106
00:07:17,948 --> 00:07:21,434
the iris example to actually understand how clustering works,

107
00:07:21,472 --> 00:07:25,334
right? So iris is nothing but a data set of flowers,

108
00:07:25,382 --> 00:07:28,906
right? There is this particular flower which has the data that

109
00:07:28,928 --> 00:07:32,374
is provided is length, length of the sepal, width of the sepal height,

110
00:07:32,422 --> 00:07:35,722
and those kind of things. So, petal width, sepal length, petal length are provided.

111
00:07:35,866 --> 00:07:39,566
And these are three dimensions that are provided. And when you

112
00:07:39,588 --> 00:07:43,006
actually group them, you can actually see that

113
00:07:43,188 --> 00:07:46,594
the similar flowers or the similar flowers actually tend to light

114
00:07:46,632 --> 00:07:50,338
together. And that's what we are seeing. The green, yellow and the purple, they are

115
00:07:50,344 --> 00:07:53,538
actually similar data points and they are tending to light together. Right?

116
00:07:53,704 --> 00:07:58,286
Next, so we looked at what EBPF was and

117
00:07:58,328 --> 00:08:02,422
what clustering is, how both of them can work together.

118
00:08:02,476 --> 00:08:06,166
So what is the basic use case of? Why would we want them to

119
00:08:06,188 --> 00:08:09,830
work together? So EBPF can help us understand the basic characteristics of a system

120
00:08:09,900 --> 00:08:13,178
behaving right now. So how many network calls are coming,

121
00:08:13,264 --> 00:08:16,378
how many times this particular file is getting opened? Right? So these are data points.

122
00:08:16,464 --> 00:08:19,898
So these data points are generated by EBPF. Right? Now, once you have

123
00:08:19,904 --> 00:08:23,822
the data points, we can use clustering to actually cluster those data points

124
00:08:23,876 --> 00:08:28,686
and understand and basically create some

125
00:08:28,708 --> 00:08:31,966
sort of domain in which we say, okay, this behavior, this system behavior is

126
00:08:31,988 --> 00:08:35,786
good. This system behavior is not good. And that differentiation

127
00:08:35,818 --> 00:08:39,486
in domain can be done by our machine learning logic. Which is the clustering

128
00:08:39,518 --> 00:08:43,042
logic, right? So we have EVPF that generates the data points.

129
00:08:43,176 --> 00:08:46,382
Clustering can actually clustering the data points, right? So initially,

130
00:08:46,446 --> 00:08:49,926
to help us, so every machine learning problem is essentially a

131
00:08:49,948 --> 00:08:53,634
label problem, right? So initially

132
00:08:53,682 --> 00:08:56,674
we can label some data, right? We could actually label them positive,

133
00:08:56,722 --> 00:09:00,546
or we could label them negative. And then in a production

134
00:09:00,578 --> 00:09:04,954
scenario, we can use the model that is created to actually read

135
00:09:05,152 --> 00:09:08,298
whatever EBPF generates the data we can actually subject to the model. And then the

136
00:09:08,304 --> 00:09:12,414
model will tell us whether in the real time whether that is a good

137
00:09:12,612 --> 00:09:15,866
system behavior data point or bad system behavior data point. So that's

138
00:09:15,898 --> 00:09:19,120
how we can use the function together,

139
00:09:20,050 --> 00:09:23,178
to take an example. So we have something called express data path.

140
00:09:23,194 --> 00:09:26,930
In EBPF, express data path is actually the earliest.

141
00:09:28,230 --> 00:09:31,854
So whenever you get a network traffic incoming,

142
00:09:31,982 --> 00:09:35,138
there are a location of sockets to it, socket buffers are located to it,

143
00:09:35,144 --> 00:09:38,506
and then that socket buffer location happens in the kernel,

144
00:09:38,558 --> 00:09:42,214
and then the traffic moves up. But even before that,

145
00:09:42,252 --> 00:09:45,558
we have an express data path functionality in which even before the

146
00:09:45,564 --> 00:09:49,338
socket buffers are located, we can actually understand some characteristics of

147
00:09:49,344 --> 00:09:52,378
the incoming network request, right?

148
00:09:52,544 --> 00:09:56,486
And along with XTP, we can add our filter programs,

149
00:09:56,518 --> 00:10:00,474
right? So we can attach EBPF program to the XDP, and that

150
00:10:00,512 --> 00:10:04,106
EBPF program can actually help us understand the data about the received packets

151
00:10:04,138 --> 00:10:07,486
of the network, right? So there is a EBPF program that

152
00:10:07,508 --> 00:10:11,658
you have attached to an XDP filter, right? And once the network packets

153
00:10:11,834 --> 00:10:15,362
keep on coming, your program writes data

154
00:10:15,416 --> 00:10:19,298
about all these received packets into a map and

155
00:10:19,464 --> 00:10:23,358
user space. Program then reads the map and pushes,

156
00:10:23,454 --> 00:10:27,074
let us say it, to any data store,

157
00:10:27,192 --> 00:10:30,694
and then from that data store, your clustering algorithm can actually understand whether

158
00:10:30,732 --> 00:10:34,070
this incoming network request is normal or not. Right?

159
00:10:34,220 --> 00:10:37,698
If now there are a lot of requests coming in, right? It doesn't

160
00:10:37,714 --> 00:10:41,126
make sense to actually predict each and

161
00:10:41,148 --> 00:10:44,518
every data because there is an infinite number of data requests

162
00:10:44,534 --> 00:10:47,626
that are going to keep on coming. So you can apply some

163
00:10:47,648 --> 00:10:51,578
sort of a time series algorithm on top of all these to understand, okay,

164
00:10:51,744 --> 00:10:55,534
is it gradually building up into a series of not normal requests? And if

165
00:10:55,572 --> 00:10:58,814
so, that means there is something wrong that is happening. You could actually

166
00:10:58,852 --> 00:11:02,586
be facing a DDoS attack, or you could actually be facing a

167
00:11:02,628 --> 00:11:06,926
denial attack. So these kind of things can be estimated

168
00:11:07,118 --> 00:11:10,866
or predicted prior to that event already happening. So here we

169
00:11:10,888 --> 00:11:14,826
see how EBPF can actually help in understanding

170
00:11:14,958 --> 00:11:18,214
whether a system behaving is in the right

171
00:11:18,252 --> 00:11:20,440
domain or is in the wrong domain. Right?

172
00:11:21,850 --> 00:11:25,206
Next we'll talk about some

173
00:11:25,228 --> 00:11:28,490
of the existing open source software

174
00:11:28,830 --> 00:11:31,766
that actually use EBPF.

175
00:11:31,878 --> 00:11:35,834
And they are also be very

176
00:11:35,872 --> 00:11:39,382
good at. Very good software at providing

177
00:11:39,446 --> 00:11:43,226
full system view related to system reliability engineering or site reliable engineering,

178
00:11:43,258 --> 00:11:46,906
right? So you can actually install these software on your cluster,

179
00:11:47,018 --> 00:11:50,574
and you can actually have a single view of how

180
00:11:50,612 --> 00:11:54,526
your entire cluster is behaving and how do they

181
00:11:54,548 --> 00:11:58,110
do that. So we are going to discuss cilium and pixie.

182
00:11:58,270 --> 00:12:01,454
Both of these software actually use EBPF as their fundamental

183
00:12:01,502 --> 00:12:05,502
base to hook on to

184
00:12:05,576 --> 00:12:08,966
many system or kernel points. Like for

185
00:12:08,988 --> 00:12:12,390
example, network events or file events.

186
00:12:12,730 --> 00:12:15,750
They hook onto these system points and then they generate data.

187
00:12:15,900 --> 00:12:19,746
And then there is an API that actually captures all this data and then shows

188
00:12:19,868 --> 00:12:23,482
it as a UI to the user saying that, okay, this is what is happening

189
00:12:23,536 --> 00:12:27,526
in your cluster right now. And that is what essentially site reliability engineering.

190
00:12:27,638 --> 00:12:31,398
Site reliability depends on observability,

191
00:12:31,494 --> 00:12:35,226
right? And observability is enabled by software like cilium and

192
00:12:35,248 --> 00:12:38,938
Pixie. Not only observability, there are a lot of other things that are also enabled.

193
00:12:38,954 --> 00:12:43,546
For example, network traffic control. So cilium

194
00:12:43,578 --> 00:12:47,342
actually does a lot of load balancing. There is a lot of network policy management,

195
00:12:47,486 --> 00:12:51,374
bandwidth management and all that. And finally, you can also have operations

196
00:12:51,422 --> 00:12:54,654
and security metrics also available via the Hubble

197
00:12:54,782 --> 00:12:58,594
framework. So, Hubble framework is the one which actually

198
00:12:58,712 --> 00:13:01,958
reads all the data which is generated by cilium and provides it

199
00:13:02,044 --> 00:13:05,478
as a UI. So what does cilium do? Cilium actually provides the

200
00:13:05,484 --> 00:13:09,366
right level of information for troubleshooting application and connectivity issues. Right? Now,

201
00:13:09,548 --> 00:13:13,106
if you look at this diagram on the right hand side,

202
00:13:13,148 --> 00:13:15,894
right? So you see that there is a NIC card, right? There is a network

203
00:13:15,942 --> 00:13:19,158
card, the kernel, there is a EVPF program that is hooked onto

204
00:13:19,174 --> 00:13:22,800
this network card, right? And in user space,

205
00:13:23,570 --> 00:13:27,198
you have installed cilium, which is actually connected to

206
00:13:27,204 --> 00:13:30,718
the CNI. And there are pods that

207
00:13:30,724 --> 00:13:33,738
are running. So what happens is anytime your pod issues a traffic,

208
00:13:33,754 --> 00:13:37,742
or receives a traffic, cilium can track it, right? Cilium tracks it using EBPF doors

209
00:13:37,886 --> 00:13:41,598
and then sends it to data spaces, those databases, nothing but maps.

210
00:13:41,694 --> 00:13:45,326
And then those maps, those maps are like. Or ring

211
00:13:45,358 --> 00:13:49,334
buffers. Ring buffer is a map, right? So those ring buffers keep on getting

212
00:13:49,372 --> 00:13:52,454
populated, and then once they're getting populated, it automatically keeps on

213
00:13:52,492 --> 00:13:55,958
refreshing the data on your framework, on your hubble framework. So that

214
00:13:55,964 --> 00:13:59,980
is the UI. So essentially how it works is

215
00:14:00,430 --> 00:14:04,426
EBPF is doing the high volume work of

216
00:14:04,608 --> 00:14:07,946
understanding the requests and those requests. The data about the request is

217
00:14:07,968 --> 00:14:10,678
passing onto the map. Cilium reads those maps,

218
00:14:10,774 --> 00:14:15,458
provides it as a UI. So that's how this whole cilium

219
00:14:15,574 --> 00:14:19,226
can hook into the observability factor of SRE.

220
00:14:19,418 --> 00:14:23,280
The next one that we are going to discuss is Pixie. Now, pixie is similar

221
00:14:23,650 --> 00:14:27,338
to cilium. It's very, very similar to cilium. What it

222
00:14:27,364 --> 00:14:30,446
basically does is it adds EBPF probes to provide access to metrics,

223
00:14:30,478 --> 00:14:33,826
event stresses and logs. There are pixie scripts, right? So Pixie has

224
00:14:33,848 --> 00:14:37,826
scripts that you can run, so you can execute PX script commands and

225
00:14:37,848 --> 00:14:41,266
you can get data out, and you can actually get data out in JSON format,

226
00:14:41,298 --> 00:14:44,406
and it's very user friendly, right? Just like

227
00:14:44,428 --> 00:14:47,846
cilium, Pixie also has a CLA and a live UI. So if you

228
00:14:47,868 --> 00:14:51,174
look at this diagram, right? So pixie actually

229
00:14:51,212 --> 00:14:54,886
works on something called pems. Pems are nothing but pixie edge modules.

230
00:14:54,918 --> 00:14:57,946
They are nothing but daemons that run on all the nodes. So those demons are

231
00:14:57,968 --> 00:15:01,258
actually hooked on. So those daemon processes are nothing but EBPF modules that

232
00:15:01,264 --> 00:15:04,766
are hooked on to some certain system events. So anytime the system event gets

233
00:15:04,788 --> 00:15:08,446
triggered, your PEM or your edge module actually transfer

234
00:15:08,548 --> 00:15:11,886
data to the process that is running on the

235
00:15:11,908 --> 00:15:15,362
cluster, and that is actually going to collate everything and

236
00:15:15,416 --> 00:15:18,658
provide it as data on the

237
00:15:18,664 --> 00:15:22,370
CLI or UI. Or that is a place where the pixie API runs. And then

238
00:15:22,440 --> 00:15:25,874
pixie scripts can actually read off that API and then provide you

239
00:15:25,912 --> 00:15:29,794
good results. So if you look at both of these software, what essentially

240
00:15:29,842 --> 00:15:33,494
they do is at the lowest level of the system,

241
00:15:33,612 --> 00:15:36,838
at the lowest level of the system, which is nothing but the network card,

242
00:15:36,924 --> 00:15:39,240
or at the kernel level,

243
00:15:39,610 --> 00:15:43,862
they run EBPF programs. And those EBPF programs actually collect

244
00:15:43,926 --> 00:15:47,420
data, and then they collate that data and they send it,

245
00:15:48,350 --> 00:15:51,758
or they help in the observability part, just by collecting the

246
00:15:51,764 --> 00:15:55,422
data from the base, like at the earliest, that system

247
00:15:55,476 --> 00:15:58,958
can get a request. That's where it collects the data from.

248
00:15:59,044 --> 00:16:03,022
So these two softwares are very common.

249
00:16:03,156 --> 00:16:06,754
In fact, we are using many components of these software to actually help

250
00:16:06,792 --> 00:16:10,786
in the observability aspect. We have many verticals in

251
00:16:10,808 --> 00:16:14,430
our SRE system, and one of the verticals being observability.

252
00:16:14,510 --> 00:16:17,778
So we use both of these software and the fundamentals involved

253
00:16:17,794 --> 00:16:21,794
in these software very rigorously to understand the observability aspect

254
00:16:21,842 --> 00:16:24,834
of the site reliability engineering.

255
00:16:24,962 --> 00:16:28,054
So we looked at these two software. Right now,

256
00:16:28,092 --> 00:16:31,260
what are the potential use cases? So like I said,

257
00:16:31,870 --> 00:16:35,286
how we can understand the system behaving system behavior

258
00:16:35,318 --> 00:16:38,886
can be understood by the data that the EBPF doors

259
00:16:38,918 --> 00:16:45,242
are generating, right? So there is system performance degradation. Check whether

260
00:16:45,296 --> 00:16:49,018
your system, whether you are so for example, system performance can be measured

261
00:16:49,034 --> 00:16:51,966
by the number of database reads. Let's say there is a database read. Every time

262
00:16:51,988 --> 00:16:55,474
there is a database read, you expect some

263
00:16:55,592 --> 00:16:58,914
minimum level of database reads. If that is not happening, that means your system

264
00:16:58,952 --> 00:17:02,674
performance might be going down. Network traffic check

265
00:17:02,792 --> 00:17:06,402
how can you actually understand whether

266
00:17:06,456 --> 00:17:09,686
the optimum amount of network traffic is being handled or whether there is lot of

267
00:17:09,708 --> 00:17:13,014
network traffic which is malicious is coming into your system?

268
00:17:13,132 --> 00:17:16,242
So that also can be tracked by EBPF hooks,

269
00:17:16,306 --> 00:17:20,006
right? Preventive maintenance. Preventive maintenance is nothing. But let's say you have an API

270
00:17:20,038 --> 00:17:24,314
that is not behaving properly as it is. So now that API can,

271
00:17:24,432 --> 00:17:28,522
API aware modules of cilium can actually understand

272
00:17:28,576 --> 00:17:31,766
whether the API is behaving properly or not. How many get calls are issued,

273
00:17:31,798 --> 00:17:35,886
how many put calls are getting issued. So those kind of things can be

274
00:17:35,908 --> 00:17:39,806
understand by these software. And then if

275
00:17:39,828 --> 00:17:42,958
the API behavior is not consistent with what is expected, then we

276
00:17:42,964 --> 00:17:46,738
can actually run sort of preventive maintenance, as in we can actually upgrade the

277
00:17:46,744 --> 00:17:50,066
versions or upgrade the backend or something like that to actually

278
00:17:50,168 --> 00:17:53,838
help us cover the multiple aspects

279
00:17:53,854 --> 00:17:56,546
of SRE. So these are some of the use cases. Now we look at one

280
00:17:56,568 --> 00:18:00,246
use case in particular. So let's take a network traffic use

281
00:18:00,268 --> 00:18:03,766
case, right? So like I said earlier, so we have

282
00:18:03,788 --> 00:18:07,474
XDP. XDP is nothing but express data path, right? Express data path

283
00:18:07,522 --> 00:18:10,726
is the earliest in the

284
00:18:10,748 --> 00:18:14,074
system that actually an interception of a network packet can happen.

285
00:18:14,112 --> 00:18:17,670
So even before socket buffer, even before SKB, you have XDP.

286
00:18:17,750 --> 00:18:21,002
So XDP is the earliest. So basically if you are able to filter out

287
00:18:21,056 --> 00:18:24,430
something at XTP range itself, your system will not actually

288
00:18:24,500 --> 00:18:27,626
waste much resources in allocating socket

289
00:18:27,658 --> 00:18:31,086
buffers and doing all the processing up the kernel. So if you are able to

290
00:18:31,108 --> 00:18:34,554
filter it under, which is basically the entry

291
00:18:34,602 --> 00:18:37,614
point, which is XTP. If you're able to filter your traffic at that point itself,

292
00:18:37,732 --> 00:18:40,434
that means you are going to save a lot of load on your system later

293
00:18:40,552 --> 00:18:43,854
in allocating socket power. So how does it work? Right? So there is a receive

294
00:18:43,902 --> 00:18:47,766
queue, it receives incoming requests, right? But at the

295
00:18:47,788 --> 00:18:51,586
traffic class ingress, we have actually hooked a BPF

296
00:18:51,618 --> 00:18:54,440
program. Right? Now, our BPF program,

297
00:18:55,290 --> 00:18:59,234
let's say we have a BPF program which says that, okay, from a particular IP,

298
00:18:59,282 --> 00:19:02,282
if there is a packet, I need to drop it. Why? I say that because

299
00:19:02,336 --> 00:19:05,606
I believe that that IP is a malicious IP,

300
00:19:05,638 --> 00:19:08,934
right? And that IP is maintained in the map. So the BPF map actually maintains

301
00:19:08,982 --> 00:19:12,458
that IP, right? So you have a packet that

302
00:19:12,464 --> 00:19:16,666
is coming from a received queue your BPF program checks the receive

303
00:19:16,778 --> 00:19:19,934
frame, right? That's the XTP receive frame it,

304
00:19:19,972 --> 00:19:22,718
check it, look at the IP address, and if the IP address matches the one

305
00:19:22,724 --> 00:19:26,354
on the BPF map, then it actually does the XTP underscore drop, which basically nothing

306
00:19:26,392 --> 00:19:30,734
but drop it. On the other hand, if I have to redirect

307
00:19:30,782 --> 00:19:34,162
it or send it somewhere else,

308
00:19:34,296 --> 00:19:37,858
so I don't need the packet to go up the kernel and

309
00:19:37,864 --> 00:19:41,186
then come back, I can actually send the XTP underscore

310
00:19:41,218 --> 00:19:44,774
TX itself. And at that point it's when the packet can

311
00:19:44,892 --> 00:19:48,466
go out to some other interface. So that's how we can do XDP

312
00:19:48,498 --> 00:19:52,582
redirect or HTTP transmit these kind of things. And if

313
00:19:52,636 --> 00:19:56,166
everything is passed, if you feel that everything is green, then you issue an XDP

314
00:19:56,198 --> 00:19:59,222
understand pass, which basically says that, okay, now you can go up the network stack,

315
00:19:59,286 --> 00:20:01,870
you can get the buffers allocated, and all those cases.

316
00:20:03,650 --> 00:20:07,486
If you look at this, right, your single EPPF program has done

317
00:20:07,508 --> 00:20:11,614
a lot of, has saved you a lot of effort in

318
00:20:11,652 --> 00:20:16,306
this particular use case. Why? Because let us say you

319
00:20:16,328 --> 00:20:18,820
are maintaining a list of ips, right?

320
00:20:21,670 --> 00:20:24,734
Every program or every organization maintains a list of ips

321
00:20:24,782 --> 00:20:28,258
that are considered malicious, right? So you maintain a list of these ips, right?

322
00:20:28,344 --> 00:20:31,686
And you pass them onto a BPF map. All that you have to do is

323
00:20:31,708 --> 00:20:35,670
you actually update these maps, you actually update this map to just update

324
00:20:36,010 --> 00:20:40,294
the malicious IPS, and your EBPF program actually dynamically

325
00:20:40,422 --> 00:20:44,086
keeps on rejecting traffic if it's from those ips. So this is an excellent

326
00:20:44,118 --> 00:20:47,514
example of an interrupt traffic use case in which you

327
00:20:47,552 --> 00:20:50,826
actually use EBPF to control your

328
00:20:50,848 --> 00:20:54,346
resources, your resource allocation, and to

329
00:20:54,528 --> 00:20:58,222
stop any malicious actor from obtaining access

330
00:20:58,276 --> 00:21:02,062
to your system. So yeah, this is one of those use cases that

331
00:21:02,196 --> 00:21:06,414
we are actually trying to explore a lot more in

332
00:21:06,452 --> 00:21:10,334
almost all our systems, right? So that

333
00:21:10,372 --> 00:21:13,774
brings us to the end of this particular session. I hope

334
00:21:13,812 --> 00:21:18,486
it has informative, hope it triggered some sort of thinking as

335
00:21:18,508 --> 00:21:22,006
to how EPPF machine learning and SRE can work

336
00:21:22,028 --> 00:21:25,158
together. Now I'm open for questions. Thank you

337
00:21:25,164 --> 00:21:29,030
once again for listening, and all the best using

338
00:21:29,100 --> 00:21:30,740
EPPF. Thank you.

