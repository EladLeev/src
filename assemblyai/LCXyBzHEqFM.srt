1
00:00:25,490 --> 00:00:29,362
Hi, and welcome to this talk. I'm Francesco Tisiot, developer advocate at Ive.

2
00:00:29,426 --> 00:00:33,190
In this session we will check how you can build event driven applications using

3
00:00:33,260 --> 00:00:37,062
Apache, Kafka and Python. If you are here at conference 42,

4
00:00:37,116 --> 00:00:40,422
I believe you are somehow familiar with Python, but on the other

5
00:00:40,476 --> 00:00:43,766
side you may start wondering, what is Kafka? Why should

6
00:00:43,788 --> 00:00:47,160
I care about Kafka? Well, the reality is that

7
00:00:47,530 --> 00:00:51,546
if you are into Python, you are somehow either creating

8
00:00:51,578 --> 00:00:55,402
a new shiny app or you inherited an old rusty

9
00:00:55,466 --> 00:00:59,210
app that you have to support, maybe extend,

10
00:00:59,290 --> 00:01:03,554
maybe take to the new world, no matter if it's new

11
00:01:03,672 --> 00:01:07,438
or old, if it's shiny or rusty.

12
00:01:07,614 --> 00:01:11,874
I've never seen an application, a Python program working in complete

13
00:01:11,992 --> 00:01:15,970
isolation. You will have companies within

14
00:01:16,120 --> 00:01:19,926
Python that needs to talk with each other, or you

15
00:01:19,948 --> 00:01:23,574
are exposing this old application into the word. So you have this

16
00:01:23,612 --> 00:01:27,550
old application talking with another application. Let me tell you a secret.

17
00:01:27,650 --> 00:01:31,734
Kafka is a tool that makes this communication

18
00:01:31,862 --> 00:01:34,810
easy and reliable at scale.

19
00:01:35,150 --> 00:01:39,082
Why should we use Kafka? Well, we used to

20
00:01:39,136 --> 00:01:43,214
have kind of what I call the old way of building

21
00:01:43,412 --> 00:01:46,910
applications, which were an application that at a certain

22
00:01:46,980 --> 00:01:50,254
point had to write the data somewhere. Where did

23
00:01:50,292 --> 00:01:53,258
the application write to? Well, usually it was a database,

24
00:01:53,434 --> 00:01:56,254
but the application wasn't existing to the database.

25
00:01:56,302 --> 00:02:00,062
Every single record, it was taking records,

26
00:02:00,126 --> 00:02:03,742
packaging them up, few of them, and then pushing them to the database.

27
00:02:03,806 --> 00:02:07,878
Or at the same time when it was reading from the database, it was

28
00:02:08,044 --> 00:02:12,054
reading a set of a batch of records and

29
00:02:12,092 --> 00:02:15,666
then existing a little bit before rereading the following batch.

30
00:02:15,778 --> 00:02:19,210
This means that basically every time we were using such

31
00:02:19,360 --> 00:02:22,778
a way of communicating, we were adding a

32
00:02:22,784 --> 00:02:26,602
custom delay which was called batch time, between when

33
00:02:26,656 --> 00:02:30,186
the event was available in the application and when it was pushed on

34
00:02:30,208 --> 00:02:33,706
the database, or when the events was available in the database

35
00:02:33,738 --> 00:02:36,190
and when it was read from the application.

36
00:02:36,340 --> 00:02:39,566
Now we are living in a fast word and we

37
00:02:39,588 --> 00:02:43,486
cannot wait batch time. You can imagine in batch time going between

38
00:02:43,588 --> 00:02:46,814
like few seconds or milliseconds to minutes or

39
00:02:46,852 --> 00:02:49,986
hours, depending on the application and the use case. Now we live

40
00:02:50,008 --> 00:02:52,978
in a fast world and we don't want to wait the batch time.

41
00:02:53,064 --> 00:02:56,626
We want to build event driven application. What are those

42
00:02:56,808 --> 00:03:00,342
application that as soon as an event happens in the real

43
00:03:00,396 --> 00:03:04,118
life, they want to know about it, they want to start

44
00:03:04,204 --> 00:03:08,294
parsing it in order to strike the relevant information. And probably

45
00:03:08,412 --> 00:03:12,106
they want to push the output of their basing to

46
00:03:12,128 --> 00:03:15,766
another application, which will be more likely another event driven

47
00:03:15,798 --> 00:03:19,610
application that will create a changing of those application. And we want to do it

48
00:03:19,680 --> 00:03:23,550
immediately. But let's do a step back.

49
00:03:23,700 --> 00:03:27,646
Let's try to understand what is an event. We are

50
00:03:27,668 --> 00:03:31,486
all used to for example mobile phones and we are all used to

51
00:03:31,588 --> 00:03:35,154
notifications. Notification tell us that an

52
00:03:35,192 --> 00:03:38,418
event happened. We receive a message. We made a

53
00:03:38,424 --> 00:03:41,806
payment with our credit card and we received the notification.

54
00:03:41,998 --> 00:03:45,486
Someone else stole our credit card details

55
00:03:45,518 --> 00:03:48,150
and made a payment. We receive a word notification.

56
00:03:48,570 --> 00:03:51,954
As you might understand, we cannot wait batch

57
00:03:52,002 --> 00:03:55,222
time of 2 hours, ten minutes, five minutes.

58
00:03:55,356 --> 00:03:58,762
We want to know immediately about someone stolen our credit

59
00:03:58,816 --> 00:04:02,714
card and we persons, we react as

60
00:04:02,752 --> 00:04:06,326
an event driven application by immediately phoning

61
00:04:06,358 --> 00:04:10,166
up our bank to block the credit card. This is why events

62
00:04:10,198 --> 00:04:13,854
and event event event driven applications important. But even without going into

63
00:04:13,892 --> 00:04:17,262
the digital world, we are used to events happening in the real life

64
00:04:17,316 --> 00:04:20,926
since long time. Just imagine where your alarm beeps in

65
00:04:20,948 --> 00:04:25,086
the morning, you wake up and you act as an event. Event driven applications,

66
00:04:25,108 --> 00:04:28,674
you are not only receiving passively events, you are creating events. Just think,

67
00:04:28,712 --> 00:04:32,690
when you change the time of your alarm, that will change your future,

68
00:04:32,840 --> 00:04:36,382
your actions in the future. Well, going back to mobile

69
00:04:36,446 --> 00:04:40,694
phones, especially in this time of pandemic, we have all been used to

70
00:04:40,812 --> 00:04:43,830
for example order food from an app.

71
00:04:43,900 --> 00:04:47,266
Well from the time that you open the app, you select the restaurant,

72
00:04:47,298 --> 00:04:50,982
you select which pizzas you want, then you create can order. This will

73
00:04:51,036 --> 00:04:54,714
create a chain of events because the order will be taken from the app

74
00:04:54,752 --> 00:04:58,170
and sent to the restaurant which will act as an event driven application

75
00:04:58,240 --> 00:05:01,850
and create the pizzas for you. And once the pizza is ready, boom. Another event

76
00:05:01,920 --> 00:05:05,262
for probably the delivery people to come and pick it up

77
00:05:05,316 --> 00:05:08,622
and take it to your place. So why event

78
00:05:08,676 --> 00:05:11,886
event event driven applications important? Because as I said, we live

79
00:05:11,908 --> 00:05:15,782
in a fast word and the value of the information is strictly

80
00:05:15,866 --> 00:05:18,898
relate to the time that it takes to be delivered. If we go back to

81
00:05:18,904 --> 00:05:22,610
the credit card example, I cannot wait half an hour

82
00:05:22,680 --> 00:05:26,462
or 5 hours before knowing that my cart has been stolen.

83
00:05:26,526 --> 00:05:29,398
I want to know immediately. But even if we talk about food and you know

84
00:05:29,404 --> 00:05:32,902
I'm italian so I'm a passionate about food. For examples,

85
00:05:33,036 --> 00:05:36,294
if we are waiting for our pizzas at home and we want to know

86
00:05:36,332 --> 00:05:39,638
where the pizza is, where the delivery person is, the information

87
00:05:39,724 --> 00:05:43,354
about the position of the delivery person is useful only if from the

88
00:05:43,392 --> 00:05:46,826
time that is taken from the person mobile phone to the time that

89
00:05:46,848 --> 00:05:50,954
it lands on my map on my mobile phone, the delay is minimal as

90
00:05:50,992 --> 00:05:54,462
10 seconds. I couldn't care lets what the position was ten minutes

91
00:05:54,516 --> 00:05:57,630
ago. The information has value only if

92
00:05:57,700 --> 00:06:01,950
delivered on time. So we need to have a way

93
00:06:02,100 --> 00:06:05,314
to deliver this information, to create this sort of

94
00:06:05,352 --> 00:06:08,514
communication between components, real time, in real

95
00:06:08,552 --> 00:06:11,986
time and highly available and at scale. How can

96
00:06:12,008 --> 00:06:15,086
we do that? Well, we can use Apache kafka. What is Apache

97
00:06:15,118 --> 00:06:18,902
Kafka? Well, the idea of Apache Kafka is really,

98
00:06:18,956 --> 00:06:22,530
really simple. The basic idea is the idea of a log file.

99
00:06:22,610 --> 00:06:26,374
A log file where as soon as an event is

100
00:06:26,412 --> 00:06:29,990
created, we store them. We store it. So event number zero

101
00:06:30,060 --> 00:06:33,770
happens. We store it as a message in the log file. Event number one happens.

102
00:06:33,840 --> 00:06:37,338
We store it after event zero, two, three and four even more.

103
00:06:37,424 --> 00:06:40,874
Kafka has concept of a log file which is append only

104
00:06:40,912 --> 00:06:44,638
and immutable. This means that once we store event zero

105
00:06:44,724 --> 00:06:48,586
in the log, we cannot change it. It's not like a record in a database

106
00:06:48,618 --> 00:06:52,142
that we can go and update it. Once the event is there,

107
00:06:52,196 --> 00:06:55,714
it's there. If something changes the reality that is

108
00:06:55,752 --> 00:06:59,554
represented by the event zero, we will store it

109
00:06:59,592 --> 00:07:03,614
as a new event in our log. But of course we know that events

110
00:07:03,742 --> 00:07:07,862
take multiple shapes in different types. Just think about

111
00:07:07,996 --> 00:07:11,814
I could have events regarding pizza orders and I could have

112
00:07:11,852 --> 00:07:15,766
other events regarding delivery position. And Kafka allows me to

113
00:07:15,788 --> 00:07:19,718
store them in different locks logs which in kafka

114
00:07:19,734 --> 00:07:22,506
terms are called toppings. Even more,

115
00:07:22,608 --> 00:07:26,554
kafka is not meant to run

116
00:07:26,592 --> 00:07:30,234
in a huge single server. It's meant to be

117
00:07:30,272 --> 00:07:34,154
distributed. So this means that when you create a Kafka instance,

118
00:07:34,282 --> 00:07:37,818
most of the times you will create a cluster of nodes

119
00:07:37,914 --> 00:07:41,018
which are called, in kafka terms, brokers.

120
00:07:41,114 --> 00:07:44,622
And the log information will be stored

121
00:07:44,766 --> 00:07:48,274
across the brokers in your cluster not only one

122
00:07:48,312 --> 00:07:52,434
time, but multiple times. The number of times that each log

123
00:07:52,552 --> 00:07:56,654
will be stored in your cluster is defined by a parameter called replication

124
00:07:56,702 --> 00:08:00,322
factor. In our case we have three companies of the sharp edges

125
00:08:00,386 --> 00:08:03,846
log. So applications factor of three and two copies of

126
00:08:03,868 --> 00:08:06,870
the runs edges log. So replication factor of two.

127
00:08:07,020 --> 00:08:10,470
Why do we store multiple copies of each log?

128
00:08:10,540 --> 00:08:14,566
Well, because we know that computers are not entirely reliable

129
00:08:14,678 --> 00:08:18,106
so we could lose a node. But still, as you can see, we are

130
00:08:18,128 --> 00:08:21,690
not going to lose any data. So let's have a deep look at

131
00:08:21,760 --> 00:08:25,534
what happens with Kafka. So we have our three

132
00:08:25,572 --> 00:08:28,926
nodes and let's go to a very simple version of it where we

133
00:08:28,948 --> 00:08:32,486
have just one topic, our sharp edges topic with two copies.

134
00:08:32,538 --> 00:08:36,594
So replication factor of two. So now let's assume that

135
00:08:36,712 --> 00:08:39,826
we have a building node. What happens now?

136
00:08:39,928 --> 00:08:44,030
Well, Kafka will detect this node whats been failing

137
00:08:44,110 --> 00:08:47,506
and will check well, which are the logs available in my cluster. Well,

138
00:08:47,528 --> 00:08:51,394
there is the sharp edges log with only one copy, but a applications

139
00:08:51,442 --> 00:08:54,854
factor of two. So Kafka at that point will take care of

140
00:08:54,892 --> 00:08:58,806
creating a second copy in order to keep the number of copies equal to

141
00:08:58,828 --> 00:09:02,294
the applications factor. So once we have also the second copy,

142
00:09:02,342 --> 00:09:05,674
even if now we lose a second node, we still are not

143
00:09:05,712 --> 00:09:09,158
losing any information. So as of now we understood how Kafka

144
00:09:09,174 --> 00:09:12,430
works and what Kafka is. But Kafka is something

145
00:09:12,500 --> 00:09:16,654
to store events. What is an event for

146
00:09:16,692 --> 00:09:19,854
Kafka? Well, for all that matters to Kafka, an event

147
00:09:19,892 --> 00:09:23,534
is just a key value pair. A key value pair where

148
00:09:23,652 --> 00:09:27,010
you can put whatever you want in key and value.

149
00:09:27,080 --> 00:09:30,574
Kafka doesn't care. For Kafka it's just a series of bytes.

150
00:09:30,702 --> 00:09:34,466
So you could go from very simple use cases where you

151
00:09:34,488 --> 00:09:38,006
put key, the max temperature label and 35

152
00:09:38,108 --> 00:09:41,942
three as the value itself. Or you could go wild and you could

153
00:09:41,996 --> 00:09:46,214
add both in the key and the value JSON formats, explaining the

154
00:09:46,412 --> 00:09:49,814
restaurant receiving your pizza order and the phone line

155
00:09:49,852 --> 00:09:53,610
used to make the call, and in the value, the order id,

156
00:09:53,760 --> 00:09:57,466
the name of the person calling and the list of pizzas. Usually the

157
00:09:57,488 --> 00:10:01,050
payload. The message size is around one meg and you can use

158
00:10:01,120 --> 00:10:04,554
like in this case JSON formats, which is really cool because I

159
00:10:04,592 --> 00:10:07,966
can read through it, but it's on the other side a little bit heavy when

160
00:10:07,988 --> 00:10:11,406
you push this on wire, because for every field contains both the

161
00:10:11,428 --> 00:10:15,406
field name and the fill value. If you want to have more compacted

162
00:10:15,518 --> 00:10:19,454
representation of the same information, you could use formats like Avro

163
00:10:19,502 --> 00:10:23,022
or protopath, which detach the schema from the payload

164
00:10:23,086 --> 00:10:27,174
and use a schema registry in order to tell to Kafka how

165
00:10:27,212 --> 00:10:30,694
I compacted my message. So when I

166
00:10:30,732 --> 00:10:34,290
read the message I can ask the schema to Kafka

167
00:10:34,370 --> 00:10:37,778
and I can recreate the message properly.

168
00:10:37,874 --> 00:10:41,670
So these are just methods for all that matters to Kafka

169
00:10:41,750 --> 00:10:45,146
you are sending just a series of types. So how can

170
00:10:45,168 --> 00:10:48,154
you send that series of bytes to Kafka? Well,

171
00:10:48,272 --> 00:10:51,774
you are trying to write to Kafka and probably in this case we will

172
00:10:51,812 --> 00:10:55,450
have a Python application existing to Kafka which is called a producer,

173
00:10:55,530 --> 00:10:59,194
and just remembering the things that we said earlier,

174
00:10:59,322 --> 00:11:03,166
it writes to a topic or multiple topics. In order to write

175
00:11:03,188 --> 00:11:06,866
to Kafka, all the producer has to know is where to

176
00:11:06,888 --> 00:11:10,654
find Kafka, list of hostname and ports, how to authenticate,

177
00:11:10,782 --> 00:11:14,926
do I use Sl? Do I use SASL? Do you use other methods?

178
00:11:15,038 --> 00:11:18,482
And then since I have, for example, my information available as

179
00:11:18,536 --> 00:11:21,894
JSON objects, I need to encode that in order to be

180
00:11:21,932 --> 00:11:25,606
the row series of bytes that Kafka understands. So I need to know how

181
00:11:25,628 --> 00:11:29,046
to encode the information on the other side. Once I have my

182
00:11:29,068 --> 00:11:32,666
data in Kafka, I want to read, and if I want to read from

183
00:11:32,688 --> 00:11:35,994
a topic with my python application that is called

184
00:11:36,032 --> 00:11:39,354
a consumer, how the consumer works is that it will read

185
00:11:39,472 --> 00:11:43,446
the message number zero and then communicate back to Kafka.

186
00:11:43,478 --> 00:11:47,006
Hey, number zero done. Let's move the offset to number one. It will

187
00:11:47,028 --> 00:11:50,702
read number one and move the offset to number two. Read number two,

188
00:11:50,756 --> 00:11:54,546
move the offset to number three. Why moving the

189
00:11:54,568 --> 00:11:57,922
offset? Communicating back the offset is important. Well,

190
00:11:57,976 --> 00:12:01,134
because we know, again, computers are not entirely

191
00:12:01,182 --> 00:12:04,594
reliable, so the consumer could go down. So the next

192
00:12:04,632 --> 00:12:08,278
time that the consumer pops up, Kafka still knows until

193
00:12:08,364 --> 00:12:11,734
what point that particular consumer read in that

194
00:12:11,772 --> 00:12:15,654
particular log. So the next time the consumer will pop up, will probably send

195
00:12:15,692 --> 00:12:19,206
the item the message number three, because it was the

196
00:12:19,228 --> 00:12:23,062
first not being read by the consumer. In order to consume

197
00:12:23,126 --> 00:12:27,146
data from Kafka, the consumer has to know kind of the similar information as

198
00:12:27,168 --> 00:12:31,366
the producers where to find Kafka Osnam import how to authenticate

199
00:12:31,478 --> 00:12:35,006
before we were encoding. Now we need to understand how to decode and

200
00:12:35,028 --> 00:12:38,894
we also need to understand, we need to know which topic or which topics we

201
00:12:38,932 --> 00:12:42,446
want to read from Kafka. So now it was a

202
00:12:42,468 --> 00:12:45,760
lot of content. Let's look at the demo.

203
00:12:46,450 --> 00:12:49,954
What we will look at here is a series of notebooks that I built

204
00:12:49,992 --> 00:12:53,506
in order to make our life easier. The first notebook that

205
00:12:53,528 --> 00:12:57,262
I created is actually a notebook that allows me to create automatically

206
00:12:57,326 --> 00:13:00,742
all the resources that I need for the follow up within

207
00:13:00,796 --> 00:13:04,418
Ivan, you can run this and you will access to this series of notebooks

208
00:13:04,434 --> 00:13:08,050
later on. But as of now, let me show you that I pre created

209
00:13:08,130 --> 00:13:11,866
two instances, one of Kafka and one of postgres that we

210
00:13:11,888 --> 00:13:15,450
will use later on. With Ivan, you can create

211
00:13:15,520 --> 00:13:18,582
your open source data platform across many clouds.

212
00:13:18,646 --> 00:13:22,074
In this case, we created a Kafka. Well, instead of showing you something that

213
00:13:22,112 --> 00:13:25,254
is already created, let me create a new instance. As you can see,

214
00:13:25,312 --> 00:13:29,226
you can create not only Kafka, but a lot of other open source data platforms.

215
00:13:29,338 --> 00:13:32,526
And once you select which data platform you want to create, you can select which

216
00:13:32,548 --> 00:13:36,246
cloud producers and within the cloud provider, the cloud rigid,

217
00:13:36,298 --> 00:13:39,826
so you can customize this per units. At the bottom you can also select the

218
00:13:39,848 --> 00:13:43,138
plan driving the amount of resources and the associated cost,

219
00:13:43,224 --> 00:13:47,282
which is all inclusive. Finally, you can give a name to the instance and after

220
00:13:47,336 --> 00:13:50,566
a few minutes, the instance will be up and running for you to have

221
00:13:50,588 --> 00:13:53,974
a look. The goodies about Ivan is not only that you can create open

222
00:13:54,012 --> 00:13:56,870
demand, but if you have an instance like this,

223
00:13:57,020 --> 00:14:00,278
you can upgrade it if a new version

224
00:14:00,294 --> 00:14:03,786
of Kafka comes up or you can changing the plan to

225
00:14:03,808 --> 00:14:07,590
upgrade, upscale or downscale. Or you can migrate

226
00:14:07,670 --> 00:14:10,854
while the service is online, the whole platform to a different region

227
00:14:10,902 --> 00:14:14,198
within the same cloud, or to a completely new cloud provider. So now

228
00:14:14,224 --> 00:14:17,374
instead of talking about Ivan, let's talk about how to create

229
00:14:17,492 --> 00:14:20,842
and produce messages to Kafka. So let's start a producers.

230
00:14:20,906 --> 00:14:24,234
The first thing that we will do is to install Kafka Python,

231
00:14:24,282 --> 00:14:28,078
which is the default basic library that allow us to connect to Kafka.

232
00:14:28,174 --> 00:14:31,954
And then we will create a producer. We create a producer by

233
00:14:31,992 --> 00:14:35,538
saying where to find Kafka, list of host, name and port,

234
00:14:35,704 --> 00:14:38,970
how to connect using SSL and three SSL certificates,

235
00:14:39,070 --> 00:14:42,742
and how to encode the information, how to serialize them. So both

236
00:14:42,796 --> 00:14:46,086
for key and value we will take the JSON and move it

237
00:14:46,108 --> 00:14:48,838
to a series of byte encoded in Ashi.

238
00:14:48,934 --> 00:14:51,818
So let's create this. Okay,

239
00:14:51,984 --> 00:14:55,866
and now we are ready to send our first message. We will send

240
00:14:55,888 --> 00:14:59,338
a pizza order again, I'm italian so pizza is key.

241
00:14:59,504 --> 00:15:02,958
And from myself, an order for myself ordering a

242
00:15:02,964 --> 00:15:06,638
pizza margarita. Okay, now the order, the message

243
00:15:06,724 --> 00:15:09,742
is sent to Kafka. How can we be sure about that? Well,

244
00:15:09,796 --> 00:15:13,466
let's create a consumer now let's

245
00:15:13,498 --> 00:15:17,282
move the consumer on the right and let's close the list of

246
00:15:17,416 --> 00:15:20,786
notebooks. We create a consumer. All we

247
00:15:20,808 --> 00:15:24,258
have to say is apart from the group id that we will check later.

248
00:15:24,344 --> 00:15:27,560
I'm calling it client one, I can call it whatever I want

249
00:15:28,090 --> 00:15:31,266
the same properties in order to connect osname,

250
00:15:31,298 --> 00:15:34,066
import SSL with the three certificates.

251
00:15:34,258 --> 00:15:37,526
And how do I deserialize now the data from

252
00:15:37,548 --> 00:15:40,998
the row series of bytes to JSON with the two formula

253
00:15:41,014 --> 00:15:44,518
CIA. Okay, so let me create the consumer.

254
00:15:44,614 --> 00:15:48,058
Now I can check which topics are available in Kafka and

255
00:15:48,144 --> 00:15:52,394
I can check there are some internal topics together with a nice Francesco

256
00:15:52,442 --> 00:15:55,242
pizza topic that I just created for this purpose.

257
00:15:55,386 --> 00:15:59,150
I can subscribe to it and now I can start

258
00:15:59,220 --> 00:16:03,250
reading. We can immediately see two things when

259
00:16:03,320 --> 00:16:07,506
reading. The first one being whats the

260
00:16:07,608 --> 00:16:11,154
consumer thread never ends. This is because we

261
00:16:11,192 --> 00:16:14,850
want to be there, ready as soon as an order comes in

262
00:16:14,920 --> 00:16:18,502
Kafka, we want to be there, ready to read it. And there is no

263
00:16:18,556 --> 00:16:22,022
end time in streaming there is no end date.

264
00:16:22,156 --> 00:16:25,782
We will always be there, ready to consume the data.

265
00:16:25,916 --> 00:16:29,866
The second thing that we can notice is that even if we send the

266
00:16:29,888 --> 00:16:34,026
first pizza order from Francesco, we are not receiving it

267
00:16:34,128 --> 00:16:37,802
in here. Why is that? Well, because by

268
00:16:37,856 --> 00:16:41,494
default, when a consumer attaches to Kafka, it starts

269
00:16:41,542 --> 00:16:44,918
consuming. From the time that it attached to Kafka, it doesn't

270
00:16:44,934 --> 00:16:48,366
go back in history. This is the default behavior and we will see how to

271
00:16:48,388 --> 00:16:51,626
change this later on. But just bear in mind this is the default.

272
00:16:51,738 --> 00:16:55,054
In order to show you that the wall pipeline producers consumer

273
00:16:55,102 --> 00:16:58,718
works, I'm going to send another couple of events I'm

274
00:16:58,734 --> 00:17:02,814
using to send an order for Adele with pizza y the pineapple

275
00:17:02,862 --> 00:17:05,886
pizza and an order for mark with pizza with chocolate.

276
00:17:05,998 --> 00:17:09,622
So I'm italian and both choices, there are not

277
00:17:09,676 --> 00:17:13,634
what I would call right choices for pizza. However, I respect

278
00:17:13,682 --> 00:17:16,966
your right to order whatever you want. Just try not to do that in

279
00:17:16,988 --> 00:17:20,270
Italy. Okay, so let's produce the two orders

280
00:17:20,290 --> 00:17:23,734
and if everything works, we should see them appearing on the consumer

281
00:17:23,782 --> 00:17:27,238
side immediately. There we are. We see that both Adele

282
00:17:27,254 --> 00:17:30,602
and mark orders are working in the consumer side.

283
00:17:30,736 --> 00:17:34,094
Our pipeline is working. So now let's go back

284
00:17:34,132 --> 00:17:37,582
to a little bit more slides. Let's talk about the log size.

285
00:17:37,716 --> 00:17:41,326
We want to send messages to a log. We want to send

286
00:17:41,428 --> 00:17:45,314
huge messages, huge number of messages to a

287
00:17:45,352 --> 00:17:48,942
log. But I told you that the log is stored in a broker.

288
00:17:49,006 --> 00:17:53,406
Is this meaning that we cannot have more messages

289
00:17:53,598 --> 00:17:56,878
than the bigger disk on the bigger

290
00:17:56,974 --> 00:18:00,246
server in our cluster? Well, this is not going

291
00:18:00,268 --> 00:18:04,306
to work well, if we want to send massive amounts of events,

292
00:18:04,418 --> 00:18:07,894
we don't want to have the trade off between disk space and

293
00:18:07,932 --> 00:18:11,714
amount of data. We don't want to need to purchase

294
00:18:11,842 --> 00:18:15,670
huge disk in order to store the wall topic in one disk

295
00:18:15,750 --> 00:18:19,046
on the other side. We don't want to limit ourselves and the number of events

296
00:18:19,078 --> 00:18:22,746
that we want to send to a particular topic because of disk space. We are

297
00:18:22,768 --> 00:18:25,950
lucky because Kafka doesn't impose that trade off on us.

298
00:18:26,020 --> 00:18:28,570
With Kafka we have the concept of partitions.

299
00:18:28,650 --> 00:18:32,186
Partition is just a way of taking events of the same time belonging

300
00:18:32,218 --> 00:18:36,270
to the same topic and divide them into subtopics,

301
00:18:36,350 --> 00:18:40,034
sub logs. For example, if I have my pizza orders, I could

302
00:18:40,072 --> 00:18:43,826
partition them based on the restaurant receiving the order because I want

303
00:18:43,928 --> 00:18:47,634
all the records, for example, for Luigi restaurant being

304
00:18:47,672 --> 00:18:51,618
the blue being together. But I don't care really what happens between the orders

305
00:18:51,634 --> 00:18:54,966
of Luigi restaurant and the orders of Mario the yellow one or

306
00:18:54,988 --> 00:18:58,438
Francesco the red ones. Now, why partitions are

307
00:18:58,524 --> 00:19:02,314
good for this kind of disk space trade off because the partition is

308
00:19:02,352 --> 00:19:05,994
what is actually stored on a node. So this means that if we

309
00:19:06,032 --> 00:19:09,846
want to have a huge amount of events landing

310
00:19:09,878 --> 00:19:13,550
in a topic, we just need more partition to fit

311
00:19:13,620 --> 00:19:17,102
the wall topic into smaller disks. And again,

312
00:19:17,236 --> 00:19:21,342
since it's distributed, we will have them stored across our

313
00:19:21,396 --> 00:19:24,618
cluster in a number of copies.

314
00:19:24,714 --> 00:19:28,306
In this case, the number of copies is equal to every partition because it's a

315
00:19:28,328 --> 00:19:31,826
topic level. And even if we lose a code again,

316
00:19:31,928 --> 00:19:35,346
we will not lose any data from any of the partitions because it will be

317
00:19:35,368 --> 00:19:38,534
available in the other copies in the other brokers. We said

318
00:19:38,572 --> 00:19:42,486
initially that we push data to Kafka and then it

319
00:19:42,508 --> 00:19:46,034
will be stored in the log forever. Well, this is not entirely

320
00:19:46,082 --> 00:19:49,346
true, because we can set what are called topic

321
00:19:49,378 --> 00:19:52,746
retention policies, so we can say for how long we want

322
00:19:52,768 --> 00:19:56,426
to keep the data in Kafka. We could say that based on time. So we

323
00:19:56,448 --> 00:19:59,674
can say, well, I want to keep the data on Kafka for two

324
00:19:59,712 --> 00:20:02,986
weeks, six hour, 30 minutes, or forever.

325
00:20:03,098 --> 00:20:06,270
Or we can say that based on log size. Basically,

326
00:20:06,340 --> 00:20:10,478
I want to keep the events in kafka until the

327
00:20:10,564 --> 00:20:14,154
kafka log reaches 10gb and then delete the oldest

328
00:20:14,202 --> 00:20:17,666
chunk. I can also use both, and the first threshold that will

329
00:20:17,688 --> 00:20:21,346
be hit between time and size will dictate when I will delete the

330
00:20:21,368 --> 00:20:24,606
oldest set of records. So we understood that partitions

331
00:20:24,638 --> 00:20:28,214
are good. How do you select a partition? Well, usually it's done with the key

332
00:20:28,252 --> 00:20:31,686
component of the message. And what

333
00:20:31,708 --> 00:20:36,006
Kafka does by default is that it ashes the key

334
00:20:36,108 --> 00:20:39,798
and takes the result of the ash in order to select one partition,

335
00:20:39,894 --> 00:20:43,562
ensuring that messages having the same key always land

336
00:20:43,616 --> 00:20:47,222
in the same partition. Why this is useful?

337
00:20:47,366 --> 00:20:51,110
Well, let me show you what happens when you start using partition.

338
00:20:51,190 --> 00:20:54,606
It's useful for ordering. Let me show you this little example. I have

339
00:20:54,628 --> 00:20:58,154
my producer which produces data to a topic with two partitions,

340
00:20:58,202 --> 00:21:01,374
and then I have a consumer. Let's assume a very simple use case

341
00:21:01,412 --> 00:21:05,330
where I have only three events, blue one happening first,

342
00:21:05,480 --> 00:21:08,722
yellow one happening second, red one happening third. Now,

343
00:21:08,776 --> 00:21:12,014
when pushing this data into our topic, the blue

344
00:21:12,062 --> 00:21:15,534
event will land in partition zero, the yellow event will land in partition

345
00:21:15,582 --> 00:21:18,850
one, and the red event will be in partition zero again.

346
00:21:18,920 --> 00:21:21,958
Now, when reading data from the topic, it could happen,

347
00:21:22,044 --> 00:21:25,078
it will not always be the case, but it could happen that I will read

348
00:21:25,164 --> 00:21:28,246
events in this order. Blue one first, red 1

349
00:21:28,348 --> 00:21:31,926
second, yellow one third. So if you check, the global ordering

350
00:21:31,958 --> 00:21:36,038
is not correct. Why is that? Well, because when we start using partition,

351
00:21:36,134 --> 00:21:40,018
we have to give up on global ordering. Kafka ensures the correct ordering

352
00:21:40,134 --> 00:21:43,518
only per partition. So this means whats we have to start

353
00:21:43,604 --> 00:21:47,354
thinking about for which events,

354
00:21:47,482 --> 00:21:51,450
for which subset of events the related altering

355
00:21:51,530 --> 00:21:55,170
is necessary and for which not. If we go back to our pizza order

356
00:21:55,240 --> 00:21:59,186
example, it makes sense to keep all the orders of

357
00:21:59,208 --> 00:22:03,134
the same restaurant together because we want to know which person ordered

358
00:22:03,182 --> 00:22:07,046
before or after the other. But we don't really care if an

359
00:22:07,068 --> 00:22:10,966
order for Luigi's pizza was done before another order for

360
00:22:11,068 --> 00:22:14,546
Mario's pizza. So we understood that partitions

361
00:22:14,578 --> 00:22:17,954
are good because they ease the trade off between disk

362
00:22:18,002 --> 00:22:21,354
space and log size. But partitions are bad because we

363
00:22:21,392 --> 00:22:24,746
have to ive up on global ordering. But if you think

364
00:22:24,768 --> 00:22:28,342
about partitions, and if you think about a log with a single partition,

365
00:22:28,406 --> 00:22:31,562
it's just one unique thread appending one event

366
00:22:31,616 --> 00:22:35,774
after the other. And you can think that the throughput is done by the single

367
00:22:35,812 --> 00:22:38,826
thread doing the work. Now, if we have more partition,

368
00:22:38,938 --> 00:22:42,842
we have multiple independent threads that can append

369
00:22:42,906 --> 00:22:46,226
data one after the other. So you can still think, I know that there

370
00:22:46,248 --> 00:22:49,950
are other components, but the throughput of those three processes

371
00:22:50,030 --> 00:22:53,262
is roughly three times the throughput of the original

372
00:22:53,326 --> 00:22:57,102
process. So this means that we can have much more producer producing

373
00:22:57,166 --> 00:23:01,026
data to Kafka, and also we can have much more threats consuming

374
00:23:01,058 --> 00:23:04,726
data from Kafka. But still we want to consume all the events of

375
00:23:04,748 --> 00:23:08,482
a certain topic, but we don't want to consume the same event twice.

376
00:23:08,626 --> 00:23:12,934
How does Kafka handles that? Well, it does by assigning

377
00:23:12,982 --> 00:23:16,262
a non overlapping subset of partitions to the consumer.

378
00:23:16,326 --> 00:23:19,226
If these last few words didn't make a lot of sense for you, well,

379
00:23:19,248 --> 00:23:22,362
let's check. In this demo we have two consumers and three partitions.

380
00:23:22,426 --> 00:23:26,506
What Kafka will do is assign the top, the blue

381
00:23:26,538 --> 00:23:29,594
partition to consumer one, and the yellow and red partition

382
00:23:29,642 --> 00:23:33,298
to consumer two, ensuring that everything works

383
00:23:33,384 --> 00:23:36,850
as expected. Even more. Let me just focus

384
00:23:36,920 --> 00:23:40,050
on this one. If consumer one now

385
00:23:40,200 --> 00:23:44,302
dies, Kafka will understand that after a timeout and redirect

386
00:23:44,366 --> 00:23:47,426
the blue arrow from consumer one which died,

387
00:23:47,538 --> 00:23:51,638
to the consumer which is still available consumer two. So now

388
00:23:51,724 --> 00:23:55,622
let me show you this behavior in a demo. Let me show you

389
00:23:55,676 --> 00:23:59,994
partitioning in a demo. And this time let's create

390
00:24:00,112 --> 00:24:03,450
a new producers which is similar

391
00:24:03,520 --> 00:24:06,826
to the above, nothing different. Apart from now we

392
00:24:06,848 --> 00:24:09,986
are using Kafka admin client to connect on the admin

393
00:24:10,038 --> 00:24:14,490
side of Kafka and create a new topic

394
00:24:14,650 --> 00:24:17,774
with two partitions. So we will force the number

395
00:24:17,812 --> 00:24:21,406
of partitions here. Okay, now what we

396
00:24:21,428 --> 00:24:25,426
want on the other side is we have a producer with

397
00:24:25,448 --> 00:24:28,546
a topic of two partitions. Let's create two consumers that

398
00:24:28,568 --> 00:24:31,858
are working one against the other to consume all

399
00:24:31,944 --> 00:24:36,194
the messages from that topic. So let's move consumer

400
00:24:36,242 --> 00:24:39,846
one here and consumer two

401
00:24:39,948 --> 00:24:43,986
there. So I'm saying to Kafka

402
00:24:44,098 --> 00:24:47,714
that im existing those two consumers to the same topic.

403
00:24:47,842 --> 00:24:50,934
Since I have two partitions and two consumers,

404
00:24:50,982 --> 00:24:54,486
what Kafka should do is assign one consumer

405
00:24:54,518 --> 00:24:58,006
to the top partition and one consumer to the bottom partition.

406
00:24:58,118 --> 00:25:02,254
If I go now, let me check that the

407
00:25:02,292 --> 00:25:07,050
top one is started. Let me start also the bottom consumer.

408
00:25:07,210 --> 00:25:10,846
So all the two consumers are started. Now let me

409
00:25:10,868 --> 00:25:14,414
go back to the producer here and let me send

410
00:25:14,532 --> 00:25:18,194
a couple of messages. If you remember what I

411
00:25:18,232 --> 00:25:21,618
told you before, the partition is selected with the key.

412
00:25:21,704 --> 00:25:25,102
So Kafka does a hash of the key and selects the partition.

413
00:25:25,166 --> 00:25:28,582
What I'm doing here, I'm using two records with

414
00:25:28,636 --> 00:25:32,182
slightly different key ed one ed zero. So I'm expecting them

415
00:25:32,236 --> 00:25:35,894
to land into two different partitions. Let me try this

416
00:25:35,932 --> 00:25:39,414
out exactly. So I receive one record

417
00:25:39,532 --> 00:25:42,630
on the top consumer, one record at the bottom consumer.

418
00:25:42,710 --> 00:25:46,746
If you wonder what those two flex means, this means that the top consumer is

419
00:25:46,768 --> 00:25:50,486
reading from partition zero, the offset zero. So the first record

420
00:25:50,608 --> 00:25:54,174
of partition zero, the bottom consumer is reading from partition one,

421
00:25:54,212 --> 00:25:57,534
offset zero, first record of partition one. If now I

422
00:25:57,572 --> 00:26:01,134
send another couple of messages from the

423
00:26:01,172 --> 00:26:05,106
same producer reusing the same keys since what I

424
00:26:05,128 --> 00:26:08,530
told you before, that messages with the same key

425
00:26:08,600 --> 00:26:12,514
will end up in the same partition. Im expecting mark order to

426
00:26:12,552 --> 00:26:16,354
land in the same partition as Frank because they share the same

427
00:26:16,392 --> 00:26:19,554
key and the same for Jan and Adele. So let's

428
00:26:19,602 --> 00:26:23,090
check this out. Runs this and as expected,

429
00:26:23,170 --> 00:26:26,326
mark is landing in the same partition as Frank and can in the

430
00:26:26,348 --> 00:26:29,530
same partition as Adele. So offset 1

431
00:26:29,600 --> 00:26:33,082
second record or partition zero offset 1 second record

432
00:26:33,136 --> 00:26:36,650
of partition one. So let's check also the latest bit.

433
00:26:36,720 --> 00:26:39,210
What happens if a consumer now fails?

434
00:26:39,970 --> 00:26:43,818
Kafka will somehow understand that after a timeout

435
00:26:43,994 --> 00:26:47,838
and should redirect also partition zero to consumers one.

436
00:26:47,924 --> 00:26:52,298
So let's check now what happens if I send another two events

437
00:26:52,394 --> 00:26:56,846
to the topic? I'm expecting to read both of them at the bottom consumer.

438
00:26:56,958 --> 00:27:00,754
Let's check this out exactly. So now I'm reading both from

439
00:27:00,792 --> 00:27:04,174
partition zero and partition one with the consumer

440
00:27:04,222 --> 00:27:07,654
which was left alive. So all as of

441
00:27:07,692 --> 00:27:11,382
now working as expected. Let's go back to a little bit

442
00:27:11,436 --> 00:27:15,586
more slides. So as of now we saw a pretty linear

443
00:27:15,698 --> 00:27:19,046
way of defining data pipelines. We had one or more

444
00:27:19,068 --> 00:27:22,950
threads of producer Kafka and one or more threads of consumers

445
00:27:23,030 --> 00:27:26,554
that were fighting one against the other in order to read all

446
00:27:26,592 --> 00:27:30,058
the messages from a Kafka topic. For example, if we go back to

447
00:27:30,064 --> 00:27:33,386
the pizza case we had, those two consumers could be the

448
00:27:33,408 --> 00:27:36,810
two pizza makers that are fighting one against the other in order to consume

449
00:27:36,890 --> 00:27:39,966
all the orders from the pizza order topic. But they

450
00:27:39,988 --> 00:27:42,718
don't want to make the same pizza twice. So they don't want to read the

451
00:27:42,724 --> 00:27:46,114
same pizza order twice. However, when they read a message,

452
00:27:46,232 --> 00:27:50,114
the message is not deleted from Kafka. This makes it available

453
00:27:50,232 --> 00:27:53,538
for other applications to read. So for example, I could have my

454
00:27:53,624 --> 00:27:57,506
billing person that wants to receive a copy of every order in order

455
00:27:57,528 --> 00:28:00,886
to make the bill and it wants to read from the topic at its

456
00:28:00,908 --> 00:28:04,598
own pace. How can I manage that? Well, with Kafka it's really simple.

457
00:28:04,684 --> 00:28:08,742
It's the concept of consumer groups. So I have to define the two

458
00:28:08,796 --> 00:28:12,346
pizza makers are part of the same consumer group. And then I will create a

459
00:28:12,368 --> 00:28:15,878
new application called like billing person and we'll

460
00:28:15,974 --> 00:28:19,206
set that as part of a new consumer group and Kafka

461
00:28:19,238 --> 00:28:23,162
will understand that it's a new application and we'll start sending a copy of

462
00:28:23,216 --> 00:28:26,606
the topic data to this new application that will read at its own pace that

463
00:28:26,628 --> 00:28:29,982
has nothing to do with the two pizza makers. Let's check this

464
00:28:30,036 --> 00:28:33,666
out again. Let's go back to the notebook. And now

465
00:28:33,768 --> 00:28:37,826
what we will see is we

466
00:28:37,848 --> 00:28:41,282
will create a new consumer part of a new consumer group.

467
00:28:41,416 --> 00:28:44,722
So if we go back to the original consumer we had

468
00:28:44,776 --> 00:28:47,942
this group id that I told you before. We will check that later. Well,

469
00:28:47,996 --> 00:28:51,570
it's time now. The top consumer was called pizza makers.

470
00:28:51,650 --> 00:28:55,302
This is how you event driven applications being part of a consumers group

471
00:28:55,356 --> 00:28:58,438
on can Android. The new consumer is called bill in person.

472
00:28:58,524 --> 00:29:02,086
So this is for Kafka. It's a completely new application reading

473
00:29:02,118 --> 00:29:05,306
data from the topic. If you remember when

474
00:29:05,328 --> 00:29:09,142
we had this original consumer we managed to attach to the topic

475
00:29:09,206 --> 00:29:12,590
but read not from the beginning of the topic, from the time

476
00:29:12,660 --> 00:29:16,106
when we attach to the topic. So we were missing

477
00:29:16,138 --> 00:29:19,374
the order number one. Now with things new application

478
00:29:19,492 --> 00:29:23,166
we also say out offset reset equal to earliest. So we

479
00:29:23,188 --> 00:29:26,434
say we're attaching to a topic in Kafka and we want to read from

480
00:29:26,472 --> 00:29:30,578
the beginning. So when we now start this new application

481
00:29:30,744 --> 00:29:33,950
we should receive the two messages above

482
00:29:34,030 --> 00:29:37,986
plus also the first message, the original message of Francesco

483
00:29:38,018 --> 00:29:41,974
order. There we are. We are receiving all the three messages since

484
00:29:42,012 --> 00:29:46,066
we started reading from the beginning. Now if we go to the original

485
00:29:46,098 --> 00:29:49,706
producer and we now send a new event to

486
00:29:49,728 --> 00:29:53,754
the original topic, we should receive it in both because those are

487
00:29:53,792 --> 00:29:58,250
two different application. Let's try this out exactly.

488
00:29:58,320 --> 00:30:01,962
We will receive down order both in the top and the bottom application.

489
00:30:02,096 --> 00:30:05,946
Everyone adding the data because there are different application reusing

490
00:30:05,978 --> 00:30:09,742
the same topic. Now we understood also different

491
00:30:09,796 --> 00:30:13,546
consumer groups. Let's check a little bit more.

492
00:30:13,668 --> 00:30:17,602
What we saw so far was us writing some code

493
00:30:17,656 --> 00:30:21,842
in order to produce data or to consume data. However, it's very

494
00:30:21,896 --> 00:30:25,462
hard that Kafka will be your first tool, your first data

495
00:30:25,516 --> 00:30:29,842
tool in your company. You will probably want to integrate

496
00:30:29,906 --> 00:30:33,506
Kafka with an existing set of data tools,

497
00:30:33,618 --> 00:30:36,918
databases, data stores, any kind of data tool.

498
00:30:37,004 --> 00:30:40,546
And believe me, you don't want to write your own code for each of those

499
00:30:40,588 --> 00:30:43,706
connectors. There is something that solves this problem for you

500
00:30:43,728 --> 00:30:47,574
and it's called Kafka Connect. Kafka Connect is a pre built framework

501
00:30:47,622 --> 00:30:51,446
that allows you to take data from existing data sources

502
00:30:51,558 --> 00:30:54,926
and put them in a Kafka topic, or take data from a

503
00:30:54,948 --> 00:30:58,170
Kafka topic and push them to a set of data syncs.

504
00:30:58,250 --> 00:31:01,806
And all is driven by just a config file. And one

505
00:31:01,828 --> 00:31:05,406
or more threads of Kafka Connect allows you to

506
00:31:05,428 --> 00:31:09,406
event event driven applications. If we go back to one of the initial slides

507
00:31:09,438 --> 00:31:12,994
where we had our producer producing data to a database, well,

508
00:31:13,032 --> 00:31:16,978
we now want to include Kafka in the picture, but still we don't want

509
00:31:17,064 --> 00:31:21,074
to change the original setup, which is still working. How can we include

510
00:31:21,122 --> 00:31:24,454
Kafka in the picture? Well, with Kafka Connect and with a change

511
00:31:24,492 --> 00:31:28,406
data capture solution, we can monitor all changes happening in

512
00:31:28,428 --> 00:31:32,230
a set of tables in the database and propagate those changes as events,

513
00:31:32,310 --> 00:31:35,962
as messages in Kafka. Very very easy. But also

514
00:31:36,016 --> 00:31:38,934
we can use Kafka Connect in order to distribute events.

515
00:31:38,982 --> 00:31:42,446
So for example, if we have our application that already writes to

516
00:31:42,468 --> 00:31:46,026
Kafka and we have the data in Kafka in a topic,

517
00:31:46,138 --> 00:31:49,710
well, is our team needing the data in a

518
00:31:49,860 --> 00:31:53,918
database? Any JDBC database? With Kafka Connect we just can

519
00:31:54,004 --> 00:31:57,698
ship the data to a JDBC database. They want another copy to a

520
00:31:57,704 --> 00:32:01,486
postgres database. There we are. They want a third copy to Bigquery.

521
00:32:01,598 --> 00:32:04,914
Really easy. They want a fourth copy into s

522
00:32:04,952 --> 00:32:08,434
three for long term storage. Just another Kafka connect thread.

523
00:32:08,482 --> 00:32:11,814
And again, what you need to do is just define a

524
00:32:11,852 --> 00:32:15,702
config file from which topic you want to take and

525
00:32:15,836 --> 00:32:18,562
where you want to bring them. And with Ivan,

526
00:32:18,626 --> 00:32:22,650
Kafka Connect is also managed service. So you just have to figure out

527
00:32:22,720 --> 00:32:26,186
how to write the config file. So let's check also this as a

528
00:32:26,208 --> 00:32:29,938
demo. If we go back to our notebook

529
00:32:30,134 --> 00:32:33,738
we can now check the Kafka

530
00:32:33,754 --> 00:32:38,666
connect one. Let me close a little bit of my notebooks.

531
00:32:38,778 --> 00:32:42,606
What we are doing here, we are creating a new producer. What we will

532
00:32:42,628 --> 00:32:45,966
create now is a different topic and within each message

533
00:32:46,068 --> 00:32:49,666
we are going to send both the schema of the

534
00:32:49,688 --> 00:32:53,106
key and the value and the payload of the key and the value. Why do

535
00:32:53,128 --> 00:32:56,786
we do that? Well, because we want to make sure that our Kafka

536
00:32:56,818 --> 00:33:00,342
connect connector understand the structure of the record and

537
00:33:00,396 --> 00:33:04,418
populates a downstream application that in this case it's postgres table

538
00:33:04,594 --> 00:33:08,246
properly. So we define the schema of the key and

539
00:33:08,268 --> 00:33:11,802
the value. And now we pass, we create three

540
00:33:11,856 --> 00:33:16,166
records. We push three messages to Kafka containing both the schema

541
00:33:16,198 --> 00:33:19,286
and the value itself. With Frank ordering a pizza margarita,

542
00:33:19,318 --> 00:33:23,082
Dan ordering a pizza with fries and Jan ordering a pizza with mushrooms.

543
00:33:23,226 --> 00:33:26,314
Okay so we push the data into a topic.

544
00:33:26,442 --> 00:33:30,126
Now we want to take the data from this topic and push that

545
00:33:30,228 --> 00:33:33,674
to postgres. In order to do that I have everything scripted

546
00:33:33,722 --> 00:33:37,474
but I want to show you how you can do that with Ivan web

547
00:33:37,512 --> 00:33:40,786
UI. If I go to config I can

548
00:33:40,888 --> 00:33:44,354
check that there is a Kafka connect set up waiting for me.

549
00:33:44,392 --> 00:33:47,494
So this is the config file that you have to write in order to send

550
00:33:47,532 --> 00:33:51,720
data from Kafka to any other data target. So let's check this out.

551
00:33:52,250 --> 00:33:55,926
What do we have to show? We have to create send the

552
00:33:55,948 --> 00:33:59,338
data to a postgres database using this connection using a

553
00:33:59,344 --> 00:34:02,394
very secure new PG user new password. One, two,

554
00:34:02,432 --> 00:34:06,090
three. Very secure. And what do we want to send to this

555
00:34:06,160 --> 00:34:09,606
postgres database? A topic called Francesco

556
00:34:09,638 --> 00:34:13,674
pizza schema and we are going to call things connector sync

557
00:34:13,722 --> 00:34:17,150
Kafka postgres. Additionally we want to tell that the

558
00:34:17,220 --> 00:34:20,906
value is adjacent and we are using a JDBC sync

559
00:34:20,938 --> 00:34:24,274
connector. We are pushing data to a JDBC and please if

560
00:34:24,312 --> 00:34:27,646
the target table doesn't exist, autocreate.

561
00:34:27,758 --> 00:34:30,722
Okay let me show you on the database side.

562
00:34:30,776 --> 00:34:34,994
Whats have postgres database called 42 whats

563
00:34:35,032 --> 00:34:38,610
I'm connecting to. It has a default Db there whats schemas?

564
00:34:38,690 --> 00:34:42,162
The public schema and set of tables which is empty.

565
00:34:42,306 --> 00:34:45,526
Okay now let me take the config file that

566
00:34:45,548 --> 00:34:49,094
I've been talking to you about earlier on and let me go

567
00:34:49,132 --> 00:34:52,406
to Ivan Web Ui. Im going into my kafka

568
00:34:52,438 --> 00:34:56,486
service. There is a connector tab where I can create a new connector.

569
00:34:56,598 --> 00:35:01,030
I select a JDB sync and syncing the data to a JDBC database

570
00:35:01,110 --> 00:35:04,906
and I could fill all the information here. Or since I have my config file

571
00:35:04,938 --> 00:35:08,446
I can copy and paste into the configuration section and

572
00:35:08,468 --> 00:35:11,614
things parses the information and fills all the details as

573
00:35:11,652 --> 00:35:15,502
shown before im creating a connector with name sync Kafka postgres

574
00:35:15,566 --> 00:35:19,582
classes JDBC sync connector and the value is a JSON database

575
00:35:19,646 --> 00:35:23,438
and I'm sending the Francesco pizza schema topic.

576
00:35:23,614 --> 00:35:27,974
So now we can create a new connector and the connector is

577
00:35:28,092 --> 00:35:31,970
running. So this means that now the data should be available in postgres.

578
00:35:32,050 --> 00:35:36,274
Let me check that out. We can see that now my database

579
00:35:36,322 --> 00:35:39,834
has still tables. Let's refresh the list of tables and I

580
00:35:39,872 --> 00:35:43,574
see my Francesco pizza schema which has the same name as the topic.

581
00:35:43,622 --> 00:35:47,594
If I double click on it and

582
00:35:47,632 --> 00:35:51,326
I select the data I can see the three orders from

583
00:35:51,348 --> 00:35:54,586
Frank can and Jan being populated correctly.

584
00:35:54,698 --> 00:35:58,826
If I go back to the notebook and let's

585
00:35:58,858 --> 00:36:02,494
go back to our Kafka connect and send

586
00:36:02,612 --> 00:36:06,514
another order for Giuseppe order in Pizza y this

587
00:36:06,552 --> 00:36:10,526
is sent to Kafka. Let's go back to the database and let's

588
00:36:10,558 --> 00:36:14,526
try to refresh and also the order for Giuseppe ordering pizza

589
00:36:14,558 --> 00:36:17,682
y is there. So Kafka connect managed to create the table,

590
00:36:17,746 --> 00:36:20,966
populate the table, and keeps populating the table as soon as a

591
00:36:20,988 --> 00:36:24,150
new row arrives in the Kafka topic.

592
00:36:24,570 --> 00:36:27,990
So going back to last few slides, I believe

593
00:36:28,060 --> 00:36:31,226
in less than an hour we saw a lot of things.

594
00:36:31,328 --> 00:36:35,366
We saw how to produce and consume data, how to use partitions,

595
00:36:35,478 --> 00:36:39,206
how to define multiple applications via multiple consumer groups

596
00:36:39,238 --> 00:36:42,934
and what Kafka Connect is and how to make it work.

597
00:36:43,072 --> 00:36:46,574
Now if you have more questions or if you have any

598
00:36:46,612 --> 00:36:50,126
questions, I will give you some extra resources. First of

599
00:36:50,148 --> 00:36:53,834
all, my twitter handle. You can find me at ftziot. My messages

600
00:36:53,882 --> 00:36:56,878
are open so if you have any question regarding any of the content that I've

601
00:36:56,894 --> 00:37:00,386
been talking to you so far, just shoes out. Second thing, if you want a

602
00:37:00,408 --> 00:37:03,714
copy of the notebooks that I've been showing you, you can find

603
00:37:03,752 --> 00:37:07,326
them as an open source GitHub repository.

604
00:37:07,438 --> 00:37:10,998
If you want to try Kafka but you don't have a nice streaming data

605
00:37:11,084 --> 00:37:15,398
source that you can use to push data to Kafka. Well, I created a

606
00:37:15,484 --> 00:37:18,978
fake pizza producer which creates even more complex

607
00:37:19,074 --> 00:37:23,254
fake pizza orders than the one that I've been showing you. The last one is

608
00:37:23,372 --> 00:37:26,642
if you want to try kafka but you don't have Kafka.

609
00:37:26,706 --> 00:37:30,298
Well check out Ivan IO because we offer that as managed

610
00:37:30,314 --> 00:37:33,450
service. So you can just start your instance.

611
00:37:33,530 --> 00:37:36,942
We will take care of it and you will have only to take care about

612
00:37:36,996 --> 00:37:40,606
your development. I hope this session was useful for you to

613
00:37:40,628 --> 00:37:44,122
understand what Kafka is and how you can start using it with Python.

614
00:37:44,186 --> 00:37:48,046
If you have any questions, I will be always there available for

615
00:37:48,068 --> 00:37:51,726
you to help. Just reach out on LinkedIn or Twitter. Thank you very much and

616
00:37:51,748 --> 00:37:52,890
Ciao from Francesco.

