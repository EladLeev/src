1
00:02:01,060 --> 00:02:05,260
Hello. Thank you for joining me in this talk. My name is Pablo Chacin.

2
00:02:05,340 --> 00:02:08,444
I'm the Chaos engineering lead at Casey's Grafana Labs.

3
00:02:08,572 --> 00:02:12,164
Today, I will be talking about how organization can build confidence

4
00:02:12,212 --> 00:02:16,580
in their ability to withstand failures by shifting left chaos testing.

5
00:02:16,740 --> 00:02:19,956
I will be talking about why achieving reliability in modern

6
00:02:19,988 --> 00:02:23,576
application is hard and how chaos engineering emerged in response to

7
00:02:23,598 --> 00:02:26,984
this reality. To help organization build confidence in their

8
00:02:27,022 --> 00:02:30,296
ability to continue operating in the precedent of failures.

9
00:02:30,488 --> 00:02:34,364
I will also discuss some of the obstacles organization may face when

10
00:02:34,402 --> 00:02:37,836
trying to adopt chaos engineering. I will then introduce

11
00:02:37,868 --> 00:02:42,000
chaos testing as a foundation to facilitate adoption of chaos engineering.

12
00:02:42,340 --> 00:02:45,808
Finally, I will exemplify the principle of chaos testing with

13
00:02:45,814 --> 00:02:49,596
a case study and will demonstrate how kcs, an open source

14
00:02:49,628 --> 00:02:53,456
reliability testing tool, can be used for developing chaos tests.

15
00:02:53,568 --> 00:02:57,364
So let's start why achieving reliability in modern application is

16
00:02:57,402 --> 00:03:01,204
hard modern applications follow a microservices architecture that

17
00:03:01,242 --> 00:03:04,664
leverage crew native technologies. This de facto standard has

18
00:03:04,702 --> 00:03:08,676
many benefits, but it also increased the complexity of these applications.

19
00:03:08,868 --> 00:03:12,136
This complexity is often beyond the ability of engineers to

20
00:03:12,158 --> 00:03:16,104
fully predict how application will behave in production and

21
00:03:16,142 --> 00:03:20,072
how will react unexpected conditions like the failure of a dependency

22
00:03:20,216 --> 00:03:23,630
network, congestion, resort depletions, and others.

23
00:03:24,480 --> 00:03:28,044
Under these conditions, applications frequently fail in unpredictable

24
00:03:28,092 --> 00:03:31,696
ways. In many cases, these features are the

25
00:03:31,718 --> 00:03:35,948
consequence of misconfigured timeouts and inadequate fallback strategies

26
00:03:36,044 --> 00:03:39,300
that create retry, storm and cascading features.

27
00:03:40,120 --> 00:03:42,900
This can be considered effects in the applications.

28
00:03:43,240 --> 00:03:46,884
Unfortunately, traditional testing methodologies and tools do not help

29
00:03:46,922 --> 00:03:50,992
in finding them, mostly because they manifest in the interaction between

30
00:03:51,066 --> 00:03:54,440
services and are triggered by very specific conditions.

31
00:03:54,780 --> 00:03:57,928
Implementing tests that reproduce this condition is difficult

32
00:03:58,014 --> 00:04:01,796
and time consuming, and frequently the resulting tests

33
00:04:01,828 --> 00:04:06,140
are themselves unreliable because they produce unpredictable results.

34
00:04:06,560 --> 00:04:10,652
So how organization can build confidence in their ability to consume these

35
00:04:10,706 --> 00:04:14,872
failures? The most common way is by battle testing their applications,

36
00:04:14,936 --> 00:04:19,064
procedures and people. By going through incidents. By implementing

37
00:04:19,112 --> 00:04:22,876
war structure, post incident reviews, and adopting a blameless culture,

38
00:04:22,988 --> 00:04:26,256
organization can learn from incidents and improve their ability to

39
00:04:26,278 --> 00:04:30,308
handle them. But incidents don't make a good learning tool.

40
00:04:30,474 --> 00:04:34,020
They are unpredictable. They induce stress to the people

41
00:04:34,090 --> 00:04:38,036
involved. You cannot decide what or when to learn,

42
00:04:38,218 --> 00:04:41,830
not to mention their potential impact in the user and the business.

43
00:04:42,360 --> 00:04:45,912
So why not induce this incident on purpose? In this

44
00:04:45,966 --> 00:04:49,256
way, incident response team can be prepared in advance and

45
00:04:49,278 --> 00:04:52,616
tested procedures with less stress. This is a better way

46
00:04:52,638 --> 00:04:56,076
for learning, but there are still risks, mostly in the

47
00:04:56,098 --> 00:04:59,260
initial stages when the procedures are not well tested.

48
00:04:59,760 --> 00:05:03,292
Also, there is a limit on the incident an organization can try

49
00:05:03,346 --> 00:05:07,660
before affecting their service levels objectives. Another limitation

50
00:05:07,740 --> 00:05:11,088
is that they are preparing the organization for situation they have already

51
00:05:11,174 --> 00:05:14,784
experienced or can predict somehow. But as we

52
00:05:14,822 --> 00:05:18,144
discussed previously, modern systems sometimes fail in our

53
00:05:18,182 --> 00:05:22,064
ways. Therefore, we need a way to experiment with this system and

54
00:05:22,102 --> 00:05:25,904
learn more about how it fails. Chaos engineering is a discipline

55
00:05:25,952 --> 00:05:29,396
that emerged as a response for this need. It builds on the

56
00:05:29,418 --> 00:05:33,044
idea of experimenting on a system by injecting different type of faults

57
00:05:33,092 --> 00:05:36,676
to uncover systemic weakness, for instance,

58
00:05:36,788 --> 00:05:40,436
killing or overloading random compute instances, or disrupting

59
00:05:40,468 --> 00:05:44,264
the network traffic, and doing this on a continuous way,

60
00:05:44,382 --> 00:05:47,852
making faults the norm instead of deception with

61
00:05:47,906 --> 00:05:51,628
intention that developers get used to facing them and therefore consider

62
00:05:51,714 --> 00:05:55,420
recovery mechanisms in the design of their applications instead

63
00:05:55,490 --> 00:05:58,800
of introducing them later in response to incidents.

64
00:05:59,380 --> 00:06:03,104
This approach has been championed by companies such as Netflix with the

65
00:06:03,142 --> 00:06:06,396
iconic chaos monkey, but despite

66
00:06:06,428 --> 00:06:09,908
its promises, some obstacles still remain for chaos engineering to

67
00:06:09,914 --> 00:06:14,016
be adopted by most organizations. First, chaos engineering

68
00:06:14,048 --> 00:06:17,284
set a high adoption bar by focusing on experimenting in

69
00:06:17,322 --> 00:06:20,752
production, and we cannot argue against this principle.

70
00:06:20,896 --> 00:06:23,530
Nothing can substitute testing this real stuff.

71
00:06:23,900 --> 00:06:27,576
Unfortunately, many organizations are not prepared for this.

72
00:06:27,758 --> 00:06:31,684
They don't have battle tested procedures, and the teams may lack confidence

73
00:06:31,732 --> 00:06:34,940
in their ability to contain the effects of such experiments.

74
00:06:35,360 --> 00:06:38,476
Another significant issue is the unpredictability of

75
00:06:38,498 --> 00:06:42,632
the result of these experiments. Killing or overloading instances.

76
00:06:42,696 --> 00:06:46,804
Also, disrupting the network may affect multiple application components,

77
00:06:46,952 --> 00:06:50,476
introducing unexpected side effects and making the brass radius

78
00:06:50,508 --> 00:06:54,620
hard to predict. Moreover, modern infrastructure

79
00:06:54,700 --> 00:06:58,656
has many recovery mechanisms that may came into play and interact in

80
00:06:58,678 --> 00:07:01,988
complex ways. All these factors made the result of

81
00:07:01,994 --> 00:07:05,604
the experiment hard to predict and this is in part

82
00:07:05,642 --> 00:07:09,796
the idea. This is why it is called chaos engineering after all.

83
00:07:09,978 --> 00:07:13,192
But it is difficult to test recovery strategies for a

84
00:07:13,246 --> 00:07:17,000
specific situation if you cannot reproduce it consistently.

85
00:07:17,820 --> 00:07:21,668
Finally, adopting chaos engineering tools can also be challenging.

86
00:07:21,844 --> 00:07:25,272
Installing and using them sometimes requires a considerable knowledge

87
00:07:25,336 --> 00:07:29,016
on infrastructure and operations. They seem designed

88
00:07:29,048 --> 00:07:32,344
by and for SREs and DevOps, and it makes sense as chaos

89
00:07:32,392 --> 00:07:35,804
engineering has its roots in these communities. However,

90
00:07:35,922 --> 00:07:39,724
this complexity rise the adoption bar for most developers that

91
00:07:39,762 --> 00:07:42,556
cannot be self sufficient when using these tools.

92
00:07:42,748 --> 00:07:46,492
In summary, chaos engineering presupposes a level of technical

93
00:07:46,556 --> 00:07:50,336
proficiency and maturity that many teams and organizations do

94
00:07:50,358 --> 00:07:53,448
not have. So how more organizations

95
00:07:53,484 --> 00:07:57,060
can start building confidence in their ability to withstand failures.

96
00:07:57,480 --> 00:08:01,188
Is there an alternative to bonji jumping into chaos engineering in

97
00:08:01,194 --> 00:08:04,488
production? We propose shifted chaos testing to

98
00:08:04,494 --> 00:08:08,052
the left, incorporating chaos testing as part of the regular

99
00:08:08,116 --> 00:08:10,890
testing practices early in the development process,

100
00:08:11,260 --> 00:08:14,524
submitting the application to four that have been identified from

101
00:08:14,562 --> 00:08:17,916
incidents and validating if they can handle them in

102
00:08:17,938 --> 00:08:21,544
an acceptable way. Implementing and testing recovery

103
00:08:21,592 --> 00:08:24,956
mechanisms, if not. At the core of

104
00:08:24,978 --> 00:08:27,772
chaos testing, is for injection four.

105
00:08:27,826 --> 00:08:31,136
Injection is the software testing technique of introducing errors on

106
00:08:31,158 --> 00:08:34,496
a system to ensure it can withstand and recover from

107
00:08:34,518 --> 00:08:37,984
dossy conditions. This is not a novel idea.

108
00:08:38,102 --> 00:08:42,400
It has been used extensively in the development of safety critical systems.

109
00:08:42,560 --> 00:08:46,864
However, it has generally been used for testing how application handle

110
00:08:46,912 --> 00:08:50,630
isolated errors such as processing concrupted data.

111
00:08:51,180 --> 00:08:54,932
The challenge for modern application is to inject the complex error

112
00:08:54,996 --> 00:08:59,080
patterns they will experience in their interaction with other components.

113
00:08:59,980 --> 00:09:03,804
Fortunately, as explained in this quote from two former members

114
00:09:03,842 --> 00:09:07,580
of Netflix Chaos engineering team, from the distributed system

115
00:09:07,650 --> 00:09:11,176
perspective, almost all interesting availability experiments

116
00:09:11,208 --> 00:09:14,750
can be driven by affecting latency or the response type.

117
00:09:15,280 --> 00:09:18,844
Later in this presentation we will discuss how this can be achieved

118
00:09:18,892 --> 00:09:22,224
using cases. But at the beginning of this

119
00:09:22,262 --> 00:09:25,964
presentation, we said that the main challenge of the modern distributed

120
00:09:26,012 --> 00:09:30,260
applications was their unpredictable behavior on the turbulent conditions.

121
00:09:30,680 --> 00:09:34,340
Therefore, is it valid to ask what benefits can we expect

122
00:09:34,410 --> 00:09:37,700
from testing known faults in controlled development environments?

123
00:09:38,280 --> 00:09:42,124
Will this really contribute to improve the reliability of the applications

124
00:09:42,272 --> 00:09:45,640
or will it only create a sense of force confidence?

125
00:09:46,380 --> 00:09:49,636
According to a study of failure in real world distributed

126
00:09:49,668 --> 00:09:53,556
systems, 92% of the catastrophic system features

127
00:09:53,668 --> 00:09:57,320
were the result of incorrect handling on nonfatal errors,

128
00:09:57,820 --> 00:10:01,536
and in 58 of these cases, the resulting force

129
00:10:01,588 --> 00:10:05,470
could have been detected through simple testing or error handle code.

130
00:10:06,480 --> 00:10:09,664
And how hard is to improve this error handle code?

131
00:10:09,862 --> 00:10:13,820
According to the same study, in 35% of the cases

132
00:10:13,900 --> 00:10:17,548
the error handle code fall into one of three patterns.

133
00:10:17,724 --> 00:10:21,472
It overreacted, aborting the system under nonfalton

134
00:10:21,536 --> 00:10:25,056
errors was empty or only containing

135
00:10:25,088 --> 00:10:28,864
a lock printing statement. It contained

136
00:10:28,912 --> 00:10:32,520
expressions like fix me or to do in the comments.

137
00:10:34,700 --> 00:10:38,532
What this study comes to tell us is that there is a significant

138
00:10:38,596 --> 00:10:42,504
room for improvement in the reliability of comprehensive distributed application

139
00:10:42,702 --> 00:10:46,684
by just testing the error handle code and

140
00:10:46,722 --> 00:10:50,344
this is what chaos testing proposed. Incorporate the principle

141
00:10:50,392 --> 00:10:54,424
of chaos engineering early into the development process as an integral

142
00:10:54,472 --> 00:10:57,884
part of the testing practices. Shifted the emphasis

143
00:10:57,932 --> 00:11:01,724
from experimentation to verification for uncovering

144
00:11:01,772 --> 00:11:06,000
unknown fault, to ensuring proper handling of the known faults.

145
00:11:06,500 --> 00:11:10,292
By adopting chaos testing, teams can build confidence for moving

146
00:11:10,346 --> 00:11:14,292
forward to chaos experiments in productions and then using

147
00:11:14,346 --> 00:11:17,696
the insight obtained from these experiments and for incidents,

148
00:11:17,808 --> 00:11:21,620
improve their chaos test, creating a process of continuous

149
00:11:21,700 --> 00:11:25,540
reliability improvement in order to achieve its goal,

150
00:11:25,620 --> 00:11:28,920
chaos testing is sustained in four guiding principles.

151
00:11:29,500 --> 00:11:32,984
Incremental adoption organizations should be able to

152
00:11:33,022 --> 00:11:37,112
incorporate chaos testing into their existing teams and development processes

153
00:11:37,176 --> 00:11:40,716
in an incremental manner, starting with simple tests so they

154
00:11:40,738 --> 00:11:43,992
can understand better how their system handled faults

155
00:11:44,056 --> 00:11:47,260
and then building more sophisticated test cases.

156
00:11:48,000 --> 00:11:51,504
Applicationcentric testing developers should be able

157
00:11:51,542 --> 00:11:55,216
to reproduce in their tests the same fault pattern observed in their

158
00:11:55,238 --> 00:11:58,636
applications using familiar terms such as latency and

159
00:11:58,678 --> 00:12:02,740
error rates without having to understand the underlying infrastructure.

160
00:12:03,640 --> 00:12:07,696
Chaos testing as code switching between application testing tools

161
00:12:07,728 --> 00:12:12,376
and chaos testing tools will create production in the process and

162
00:12:12,398 --> 00:12:16,084
as we discussed before, it may reduce the autonomy of developers

163
00:12:16,132 --> 00:12:19,044
for creating chaos test. Therefore,

164
00:12:19,172 --> 00:12:22,376
developers should be able to implement chaos tests using the

165
00:12:22,398 --> 00:12:24,990
same automation tool they are familiar with.

166
00:12:26,480 --> 00:12:29,630
But adoption of chaos as code have other benefits.

167
00:12:30,240 --> 00:12:33,596
Developers can reuse log, pattern and user journeys from

168
00:12:33,618 --> 00:12:37,244
their existing tests. In this way, they can ensure

169
00:12:37,292 --> 00:12:40,944
they are testing how the application react to faults on the realistic use

170
00:12:40,982 --> 00:12:45,036
cases. Control chaos faults introduced

171
00:12:45,068 --> 00:12:48,396
by chaos tests should be reproducible and predictable to ensure

172
00:12:48,428 --> 00:12:52,770
the tests are reliable. You cannot be confident from flocky test

173
00:12:53,380 --> 00:12:57,056
test tests should also have a minimal blast radius. It should be

174
00:12:57,078 --> 00:13:01,028
possible to run them insure infrastructure, for example staging

175
00:13:01,084 --> 00:13:04,356
environment with little interference between teams

176
00:13:04,388 --> 00:13:08,424
and services. Let's put these principles into

177
00:13:08,462 --> 00:13:12,424
action using a fictional case study this case

178
00:13:12,462 --> 00:13:16,372
study used the sock shop. This is a demo application that implements

179
00:13:16,436 --> 00:13:20,284
an ecommerce site that allow users to broad a catalog of products

180
00:13:20,402 --> 00:13:24,028
and buy items from it. It follows a

181
00:13:24,034 --> 00:13:26,300
polyglot microservice architecture.

182
00:13:26,740 --> 00:13:30,704
Microservices communicate using HTTP requests and

183
00:13:30,742 --> 00:13:32,560
it is deployed in kubernetes.

184
00:13:33,860 --> 00:13:38,156
The front end service works both as a backend for the web interface

185
00:13:38,268 --> 00:13:41,430
and also exposes the APIs of other services

186
00:13:41,800 --> 00:13:45,732
working as a kind of API gateway. Let's now

187
00:13:45,786 --> 00:13:48,470
imagine an incident that affected the sock shop.

188
00:13:48,920 --> 00:13:52,932
In this incident, the catalog service database was overloaded by long

189
00:13:52,986 --> 00:13:56,488
running queries. This overload caused delays in

190
00:13:56,494 --> 00:14:00,648
the request up to 100 milliseconds over the normal response time

191
00:14:00,814 --> 00:14:05,100
and eventually made some requests, failed and returned an HTTP 500

192
00:14:05,170 --> 00:14:08,764
error. The catalog service team will investigate the

193
00:14:08,802 --> 00:14:10,830
incident to address the root cause.

194
00:14:11,680 --> 00:14:15,992
However, the front end team wonders how similar incident

195
00:14:16,056 --> 00:14:18,540
will affect the service and the end users.

196
00:14:19,940 --> 00:14:23,904
To investigate this, let's start with a load test for the front end service that

197
00:14:23,942 --> 00:14:27,516
will serve as a baseline. This test applies a load

198
00:14:27,548 --> 00:14:30,740
to the front end service requesting products from the catalog.

199
00:14:31,400 --> 00:14:34,870
The front end service will make requests to the catalog service.

200
00:14:35,880 --> 00:14:39,030
The front end service is the four the system under test.

201
00:14:39,480 --> 00:14:42,810
We will measure two metrics for the request to the front end service,

202
00:14:43,340 --> 00:14:46,644
the failure rate and the percentile 95 of the response

203
00:14:46,692 --> 00:14:50,468
time. We will send this metric to our grafana dashboard

204
00:14:50,484 --> 00:14:54,280
for visualization and we will implement this test using

205
00:14:54,350 --> 00:14:57,944
cases. Casey's is an open source reliability

206
00:14:57,992 --> 00:15:02,700
testing tool. In cases, tests are implemented using JavaScript.

207
00:15:03,280 --> 00:15:06,840
Cases cover different types of testing needs such as load testing,

208
00:15:06,920 --> 00:15:10,720
end to end testing, synthetic testing, and chaos testing.

209
00:15:11,300 --> 00:15:15,120
It can send tests resort to common backend social Prometheus.

210
00:15:15,940 --> 00:15:20,096
Its capabilities can be extended using a growing catalog of extension including

211
00:15:20,128 --> 00:15:23,376
kafka, NoSQL databases, kubernetes,

212
00:15:23,488 --> 00:15:27,204
SQL, and many others. Even when we

213
00:15:27,242 --> 00:15:30,036
are not going into too much detail in this example,

214
00:15:30,218 --> 00:15:33,476
there are some concepts that are useful for understanding the code we

215
00:15:33,498 --> 00:15:37,172
will discuss next in cases, user flows

216
00:15:37,236 --> 00:15:40,616
are implemented as JavaScript functions that make requests to

217
00:15:40,638 --> 00:15:44,256
the system under test, generally using a protocol

218
00:15:44,308 --> 00:15:47,724
such as HTTP if we are testing an API, or our

219
00:15:47,762 --> 00:15:51,260
simulated browser session if we are testing the user interface.

220
00:15:52,000 --> 00:15:55,580
The result of these requests are validated using checks.

221
00:15:56,160 --> 00:15:59,736
Scenarios describe a workload in term of a user flow

222
00:15:59,848 --> 00:16:03,504
and number of concurrent users. The rate at which the user may

223
00:16:03,542 --> 00:16:07,324
request and the duration of the load threshold

224
00:16:07,372 --> 00:16:11,040
are used for specifying SLO for metrics such as latency

225
00:16:11,120 --> 00:16:14,516
and error rates, let's make a work through the

226
00:16:14,538 --> 00:16:18,656
test code. Don't worry, we will just skim over the code highlighting

227
00:16:18,688 --> 00:16:21,872
the most relevant parts. At the end of the presentation,

228
00:16:21,936 --> 00:16:25,530
you will find additional resources that explain this code in detail.

229
00:16:26,860 --> 00:16:30,552
The test has two main parts, a function that makes

230
00:16:30,606 --> 00:16:34,456
the call to the front end service and check for errors, and in

231
00:16:34,478 --> 00:16:38,364
a scenario that describe how much load will be applied and for how long.

232
00:16:38,562 --> 00:16:42,680
Let's run this test and check the performance metrics.

233
00:16:42,840 --> 00:16:46,168
We can see the error rate with zero. That is, all requests were

234
00:16:46,194 --> 00:16:49,472
successful and the latency was around 50

235
00:16:49,526 --> 00:16:53,520
milliseconds. We will use this result as a baseline.

236
00:16:54,100 --> 00:16:57,090
Now let's add some chaos to this test.

237
00:16:57,780 --> 00:17:01,132
We will repeat the same load test, but this time

238
00:17:01,206 --> 00:17:04,832
while the load is applied to the front end service, we will inject

239
00:17:04,896 --> 00:17:08,676
fault in the request served by the catalog service, reproducing the

240
00:17:08,698 --> 00:17:11,920
pattern observed in the incident. More specifically,

241
00:17:12,000 --> 00:17:15,816
we will increase the latency and inject a certain amount of errors in

242
00:17:15,838 --> 00:17:19,624
the responses. Notice that the frontend service is still

243
00:17:19,662 --> 00:17:23,304
the system under test. For doing so, we will

244
00:17:23,342 --> 00:17:27,256
be using the KC disruptor extension. This eruptor

245
00:17:27,288 --> 00:17:31,500
is an extension that adds fork injection capabilities to kcs.

246
00:17:32,800 --> 00:17:36,830
We are not going into the technical details about how this extension works.

247
00:17:37,200 --> 00:17:41,004
For now, it is sufficient to say that it works by installing

248
00:17:41,052 --> 00:17:44,464
an agent into the target of the chaos test, for example

249
00:17:44,582 --> 00:17:48,412
a group of Kubernetes pods. These agents have the ability

250
00:17:48,476 --> 00:17:51,572
to inject different type of faults such as protocol level

251
00:17:51,626 --> 00:17:55,076
errors and this is done from the test code

252
00:17:55,178 --> 00:17:58,404
as we will see next without any external tool or

253
00:17:58,442 --> 00:18:02,072
setup. At the end of the presentation, you will find

254
00:18:02,126 --> 00:18:04,696
resources for exploring this extension in detail,

255
00:18:04,878 --> 00:18:06,840
including its architecture.

256
00:18:07,660 --> 00:18:11,464
Let's see how this works in the code, we add

257
00:18:11,502 --> 00:18:13,690
a function that inject faults in a service.

258
00:18:14,780 --> 00:18:18,076
This function defines a fault in term of a latency that

259
00:18:18,098 --> 00:18:21,868
will be added to each request and the rate of request that will return a

260
00:18:21,874 --> 00:18:25,464
given error. In this case, 10% of request

261
00:18:25,512 --> 00:18:26,930
will return a 500.

262
00:18:28,900 --> 00:18:33,276
Then it select the catalog service as a target for the four injections.

263
00:18:33,468 --> 00:18:37,068
This interrupts the disruptor to install the agents in the pods

264
00:18:37,084 --> 00:18:40,704
that backtick service. Finally it

265
00:18:40,742 --> 00:18:44,624
inject the four for a given period of time, in this case the total duration

266
00:18:44,672 --> 00:18:48,960
of the test. Then we add a scenario

267
00:18:49,040 --> 00:18:52,436
that invokes this function at a given point during the execution of

268
00:18:52,458 --> 00:18:56,056
the test. In this case, we will inject in the fault from

269
00:18:56,078 --> 00:18:59,770
the beginning of the test and this is all that we need.

270
00:19:00,780 --> 00:19:04,612
Let's run this test. We can see that the latency

271
00:19:04,676 --> 00:19:08,072
reflect the additional 100 milliseconds that we injected.

272
00:19:08,216 --> 00:19:12,268
We can also observe that now we have an error rate of almost 12%,

273
00:19:12,434 --> 00:19:16,540
a slightly over the 10% that we define in the fault description.

274
00:19:17,440 --> 00:19:20,944
It's important to remark that we are injecting the faults into

275
00:19:20,982 --> 00:19:24,656
the catalog service, but we are measuring the error rate at the

276
00:19:24,678 --> 00:19:28,064
front end service so we can see the front end

277
00:19:28,102 --> 00:19:31,424
service is not handling the errors in the request to the catalog

278
00:19:31,472 --> 00:19:34,644
service. Apparently there are no retry over

279
00:19:34,682 --> 00:19:38,324
fail request. I wouldn't be surprised if we find

280
00:19:38,362 --> 00:19:40,870
a two to comment in the error handle code.

281
00:19:42,220 --> 00:19:45,576
How this test help the front end team first

282
00:19:45,678 --> 00:19:49,690
by uncovering proper error handling logic as we just saw,

283
00:19:50,300 --> 00:19:53,816
and then enabling them to validate different solution onto the

284
00:19:53,838 --> 00:19:58,220
obtained unacceptable error rate. For example, introducing retries.

285
00:19:58,880 --> 00:20:03,016
They can also easily modify the test to reflect other situations

286
00:20:03,128 --> 00:20:06,496
like higher error rates in order to fine tune the

287
00:20:06,518 --> 00:20:10,000
solution and avoid issues such as retry and storms.

288
00:20:10,820 --> 00:20:15,120
This brief example shows the principle of scale station in action.

289
00:20:15,700 --> 00:20:19,076
A load of functional tests can be reused to test the

290
00:20:19,098 --> 00:20:22,624
system under turbulent conditions. These conditions

291
00:20:22,672 --> 00:20:26,176
are defined in terms that are familiar to developers. Latency and error

292
00:20:26,208 --> 00:20:29,396
rate. The test has a control effects on the

293
00:20:29,418 --> 00:20:32,612
target system. The test is repeatable

294
00:20:32,676 --> 00:20:36,564
and the results are predictable. Default injection

295
00:20:36,612 --> 00:20:38,650
is coordinated from the test code.

296
00:20:39,340 --> 00:20:42,916
Default injection does not add any operational complexity.

297
00:20:43,028 --> 00:20:46,952
There is no need to install any additional component or define additional

298
00:20:47,016 --> 00:20:50,840
pipeline for triggering default injection. To conclude,

299
00:20:50,920 --> 00:20:54,344
let me make some final remarks. We firmly

300
00:20:54,392 --> 00:20:57,916
believe that the ability to operate reliably shouldn't

301
00:20:57,948 --> 00:21:00,400
be a privilege of the technology elite.

302
00:21:01,060 --> 00:21:04,556
Chaos engineering can be democratized by promoting the adoption

303
00:21:04,588 --> 00:21:07,964
of chaos testing, but to be effective,

304
00:21:08,012 --> 00:21:12,180
chaos testing will be adapted to the existing practices of testing

305
00:21:12,760 --> 00:21:16,390
in grafana cases. We are committed to making this possible

306
00:21:17,000 --> 00:21:20,836
making chaos engineering practices accessible to a broad spectrum of

307
00:21:20,858 --> 00:21:24,344
organizations by building a solid foundation from

308
00:21:24,382 --> 00:21:27,640
which they can progress toward more reliable applications.

309
00:21:28,460 --> 00:21:31,864
Thank you very much for attending. I hope you have found this

310
00:21:31,902 --> 00:21:35,224
presentation useful. If you want to learn more about

311
00:21:35,262 --> 00:21:39,160
chaos testing using cases, you may find these resources useful.

312
00:21:39,580 --> 00:21:43,864
You will find an in depth walkthrough for the example we saw today and

313
00:21:43,902 --> 00:21:46,380
more technical details about the disruptor distinction.

