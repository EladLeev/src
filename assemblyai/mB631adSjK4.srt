1
00:00:34,690 --> 00:00:38,466
Hi, my name is Ismayer. I am cloud native developer

2
00:00:38,498 --> 00:00:41,394
at Wescale. In today's talk with Charles,

3
00:00:41,442 --> 00:00:44,710
we want you to introduce you with the infamous notion of

4
00:00:44,780 --> 00:00:49,106
data mesh coined by Zamag Delgani in 2019.

5
00:00:49,218 --> 00:00:52,618
In our famous article with shards,

6
00:00:52,634 --> 00:00:56,298
we believe that this world became more and more a buzzword,

7
00:00:56,394 --> 00:01:00,158
meaning that most people talk about it,

8
00:01:00,244 --> 00:01:04,350
but not really master it. As a software developer,

9
00:01:04,430 --> 00:01:07,954
we saw that this data mesh notion is

10
00:01:07,992 --> 00:01:12,238
deeply rooted into software designs considerations,

11
00:01:12,414 --> 00:01:15,974
and we want you to share with

12
00:01:16,012 --> 00:01:19,746
you this understanding so that data mesh

13
00:01:19,858 --> 00:01:23,350
is no more a mysterious notion for you and

14
00:01:23,420 --> 00:01:26,834
you are in capacity to efficiently

15
00:01:26,882 --> 00:01:30,650
implement it. But first of all,

16
00:01:30,800 --> 00:01:34,234
let's take a citation that is not from us,

17
00:01:34,272 --> 00:01:37,958
but that would guide us through this presentation.

18
00:01:38,134 --> 00:01:41,822
There is no sense in talking about the solution before we agree

19
00:01:41,876 --> 00:01:45,054
on the problem and no sense talking about

20
00:01:45,092 --> 00:01:48,334
the implementation steps before we agree on the

21
00:01:48,372 --> 00:01:51,934
solution. And this will serve as a guideline to

22
00:01:51,972 --> 00:01:55,906
this presentation. We will first show what

23
00:01:56,008 --> 00:01:59,586
is the problem that the data mesh is trying to

24
00:01:59,608 --> 00:02:03,826
solve. Then we would see

25
00:02:03,928 --> 00:02:06,962
the data mesh and for what reasons.

26
00:02:07,026 --> 00:02:10,518
It seems to be the solution to the problem we just

27
00:02:10,684 --> 00:02:15,334
introduced. Finally, we will share

28
00:02:15,372 --> 00:02:19,066
with you possible implementation, and I insist on

29
00:02:19,088 --> 00:02:22,570
the word possible of the data mesh.

30
00:02:23,310 --> 00:02:27,146
So, prologue when we talk

31
00:02:27,248 --> 00:02:30,858
about data, we talk in fact about a wide

32
00:02:30,954 --> 00:02:34,394
reality. We have many jobs, many notions

33
00:02:34,522 --> 00:02:38,174
to talk about, but we gather all these

34
00:02:38,212 --> 00:02:40,640
notion under the name of data.

35
00:02:41,330 --> 00:02:45,598
And this is very important for an enterprise

36
00:02:45,694 --> 00:02:48,910
because it's from the data that will fetch

37
00:02:48,990 --> 00:02:52,754
insights, that will be important to

38
00:02:52,792 --> 00:02:56,854
create new features. And the cycle begins with

39
00:02:56,892 --> 00:03:00,806
final users which will create what we

40
00:03:00,828 --> 00:03:04,454
call operational data serialized inside

41
00:03:04,572 --> 00:03:08,490
relational database, for instance, and which represents

42
00:03:08,910 --> 00:03:13,050
the business entities which are manipulated.

43
00:03:13,950 --> 00:03:16,826
And then from this operational world,

44
00:03:17,008 --> 00:03:20,714
we want to understand, to have a broad understanding

45
00:03:20,762 --> 00:03:23,870
of our business in order to maybe fix

46
00:03:23,940 --> 00:03:27,966
it, or more likely evolve it,

47
00:03:28,068 --> 00:03:32,046
enhance it, to answer new kind of needs

48
00:03:32,148 --> 00:03:35,918
from the final users. And this operation

49
00:03:36,014 --> 00:03:39,586
will consist in bridging this very

50
00:03:39,688 --> 00:03:44,158
operational data into an analytics

51
00:03:44,254 --> 00:03:47,350
world where we would mix

52
00:03:47,690 --> 00:03:51,400
the facts from the operational data with

53
00:03:51,770 --> 00:03:56,258
new dimensions coming from third party providers.

54
00:03:56,354 --> 00:03:59,900
In order to mix joins all those data

55
00:04:00,350 --> 00:04:03,734
and explicit them into graphs,

56
00:04:03,782 --> 00:04:07,514
for instance, that business owners and analysts will

57
00:04:07,552 --> 00:04:11,134
share with product owners which will be in

58
00:04:11,172 --> 00:04:15,246
position to later create new

59
00:04:15,348 --> 00:04:19,246
features to the final users that would be in

60
00:04:19,268 --> 00:04:23,226
use. And we see that we have this cycle and there

61
00:04:23,268 --> 00:04:27,282
is no secret in the sense that when we call the data the

62
00:04:27,336 --> 00:04:31,266
new 21st century oil, that is true because

63
00:04:31,368 --> 00:04:35,022
it's from the data that you would get

64
00:04:35,096 --> 00:04:39,302
some valuable insights to make evolve your

65
00:04:39,356 --> 00:04:43,254
applications and let's not forget about the

66
00:04:43,292 --> 00:04:46,550
data people who are the key

67
00:04:46,700 --> 00:04:51,290
for this cycle as we have here, data engineers,

68
00:04:51,790 --> 00:04:55,402
database administrators, data scientists and

69
00:04:55,456 --> 00:04:59,980
so on. And thanks to those people, we are going to create

70
00:05:00,350 --> 00:05:03,918
a current in order to make the

71
00:05:04,004 --> 00:05:07,578
dialogue between the operational and analytics

72
00:05:07,674 --> 00:05:08,880
world possible.

73
00:05:11,170 --> 00:05:15,410
We are talking about operational and analytics. What are the

74
00:05:15,480 --> 00:05:19,618
fundamental differences? In an operational world,

75
00:05:19,784 --> 00:05:23,842
we focus on the business entities and

76
00:05:23,896 --> 00:05:26,790
their relationship. Moreover,

77
00:05:27,290 --> 00:05:31,170
we require consistency

78
00:05:31,250 --> 00:05:34,470
over availability, maybe real time,

79
00:05:34,620 --> 00:05:38,426
and we usually take volumetries such as

80
00:05:38,528 --> 00:05:39,610
gigabytes.

81
00:05:40,750 --> 00:05:43,834
Conversely, in the analytics world,

82
00:05:43,952 --> 00:05:47,434
we want to have a broad understanding of the business. It's not

83
00:05:47,472 --> 00:05:50,686
about business entities, but it's about the business.

84
00:05:50,868 --> 00:05:54,506
And we will manipulate facts

85
00:05:54,698 --> 00:05:58,762
rather than business entities that we mix

86
00:05:58,906 --> 00:06:02,754
with dimensions in order to create

87
00:06:02,792 --> 00:06:06,978
this broad understanding and maybe have

88
00:06:07,064 --> 00:06:10,674
a better new understanding of the business to create

89
00:06:10,792 --> 00:06:12,130
new features.

90
00:06:14,970 --> 00:06:17,560
So what would be the problem?

91
00:06:19,210 --> 00:06:23,570
Because we have different needs between operational

92
00:06:23,650 --> 00:06:26,854
world and analytics world, we most

93
00:06:26,892 --> 00:06:30,874
likely want to bridge these different

94
00:06:31,072 --> 00:06:35,334
approaches in order to extract

95
00:06:35,382 --> 00:06:39,242
from the operational world the needed information to do

96
00:06:39,376 --> 00:06:43,838
analytics. And usually we

97
00:06:43,924 --> 00:06:47,258
pass through a dedicated pipeline

98
00:06:47,354 --> 00:06:50,682
called ETL pipeline for extract,

99
00:06:50,746 --> 00:06:55,250
transform, load that would make

100
00:06:55,320 --> 00:06:59,330
this transition between the operational world and analytics.

101
00:06:59,750 --> 00:07:04,334
So we first extract from the databases

102
00:07:04,462 --> 00:07:07,960
such as SQL for instance, the data,

103
00:07:08,970 --> 00:07:13,046
we transform it and then we load it into

104
00:07:13,228 --> 00:07:17,238
dedicated analysis database that we

105
00:07:17,404 --> 00:07:21,366
usually call data warehouses. So let's

106
00:07:21,398 --> 00:07:25,094
take a closer look to this operational and analytic

107
00:07:25,142 --> 00:07:28,794
bridge. We have this operational word represented by for

108
00:07:28,832 --> 00:07:32,526
instance a MySQL database, an analytics word represented by

109
00:07:32,548 --> 00:07:36,986
the data warehouse and in between the transformation pipeline.

110
00:07:37,178 --> 00:07:40,686
It is a logic called extract, transform, load ETL. Like I

111
00:07:40,708 --> 00:07:43,682
said, two problems with this approach. First,

112
00:07:43,736 --> 00:07:47,442
one is we try to put an entire

113
00:07:47,576 --> 00:07:50,946
business domain inside the very same

114
00:07:51,048 --> 00:07:54,562
data warehouse. So we have to think about

115
00:07:54,696 --> 00:07:59,346
a consistent way to put all these operational facts

116
00:07:59,458 --> 00:08:03,122
inside the analytics database,

117
00:08:03,266 --> 00:08:06,454
which is not a trivial issue

118
00:08:06,652 --> 00:08:10,314
as we need to keep this understanding to have

119
00:08:10,432 --> 00:08:14,090
the good insight from our business domain. The second

120
00:08:14,160 --> 00:08:18,390
problem is about coupling between this operational

121
00:08:18,470 --> 00:08:22,410
and analytics world. What happens if we decide

122
00:08:22,490 --> 00:08:26,366
to change the schema of a table here? We would

123
00:08:26,468 --> 00:08:29,774
break the pipeline because at some

124
00:08:29,812 --> 00:08:33,310
point we use those schema

125
00:08:33,470 --> 00:08:37,442
as a contract between the operational and this

126
00:08:37,496 --> 00:08:40,958
pipeline represented by the ETL pipeline.

127
00:08:41,054 --> 00:08:44,466
So from a database administration point of view we

128
00:08:44,488 --> 00:08:48,178
would say that wait a minute, I don't have to change

129
00:08:48,264 --> 00:08:51,766
this schema because I know that we have some hundreds of

130
00:08:51,788 --> 00:08:55,666
pipelines sourcing from this table, very same table.

131
00:08:55,858 --> 00:09:01,126
I don't think it's a valid reason. So from operational

132
00:09:01,238 --> 00:09:05,340
perspective, we don't mind about this analytics world

133
00:09:05,790 --> 00:09:09,050
and this transformation has to be

134
00:09:09,200 --> 00:09:12,574
kind of agnostic of the schema we

135
00:09:12,612 --> 00:09:16,062
have here. And this is why we introduce data

136
00:09:16,116 --> 00:09:19,758
lake technology in order to

137
00:09:19,924 --> 00:09:23,378
first extract and load inside a

138
00:09:23,384 --> 00:09:27,454
data lake to get the ownership back on the schema

139
00:09:27,582 --> 00:09:31,714
when we transform the data to

140
00:09:31,752 --> 00:09:34,826
load it inside the data warehouse.

141
00:09:34,958 --> 00:09:38,134
So here we are not worried about

142
00:09:38,172 --> 00:09:42,390
the fact that some schema may change because of database administration

143
00:09:42,810 --> 00:09:46,886
operations. We have the ownership back here

144
00:09:46,988 --> 00:09:50,780
as we extracted and cloud the raw data inside

145
00:09:51,230 --> 00:09:54,730
the data lake. But we still have this

146
00:09:54,880 --> 00:09:58,714
first problem about putting indistinctly all

147
00:09:58,752 --> 00:10:01,950
the data inside the data lake. That would become

148
00:10:02,100 --> 00:10:05,550
a data swamp from which it is hard to get

149
00:10:05,620 --> 00:10:09,294
sense of. So if we

150
00:10:09,332 --> 00:10:12,898
sum up, it's not just about the two problems

151
00:10:12,984 --> 00:10:16,754
I mentioned, it's also an

152
00:10:16,792 --> 00:10:20,660
organizational issue because the

153
00:10:21,190 --> 00:10:25,214
classical approach, as the Conway's law

154
00:10:25,272 --> 00:10:30,134
stated, it is to split our

155
00:10:30,172 --> 00:10:34,242
project or products by technical teams.

156
00:10:34,386 --> 00:10:38,758
We have the data engineering teams, we have the DBA

157
00:10:38,934 --> 00:10:42,458
team, and we have the data analyst data science team.

158
00:10:42,624 --> 00:10:46,166
All of these will communicate by Jira

159
00:10:46,198 --> 00:10:49,722
tickets. Usually, for instance, the data

160
00:10:49,776 --> 00:10:54,330
science team would ask new dimensions

161
00:10:54,490 --> 00:10:58,730
to the data engineering teams, which has no clue

162
00:10:58,810 --> 00:11:02,000
on what this means in term of business.

163
00:11:03,030 --> 00:11:06,466
All the translation here are only in

164
00:11:06,488 --> 00:11:10,482
terms of technical needs. And another

165
00:11:10,616 --> 00:11:14,050
problem is between the

166
00:11:14,200 --> 00:11:17,894
team's data science team and data engineering team. Usually we would

167
00:11:17,932 --> 00:11:21,282
have a bottleneck because this central

168
00:11:21,346 --> 00:11:25,462
team became the central point.

169
00:11:25,596 --> 00:11:28,970
When all those teams have an issue,

170
00:11:29,040 --> 00:11:32,954
have a need, and what would happen usually is

171
00:11:32,992 --> 00:11:36,570
that, okay, you don't give me the

172
00:11:36,720 --> 00:11:39,830
feature in time, I will do it myself.

173
00:11:40,000 --> 00:11:43,950
And we have shadow it appearing different

174
00:11:44,100 --> 00:11:48,394
source of truth, which will harm the broad understanding

175
00:11:48,522 --> 00:11:52,286
obviously of our business. So I

176
00:11:52,308 --> 00:11:55,358
would say that the problem is not really technical.

177
00:11:55,534 --> 00:11:59,086
All these technologies will scale.

178
00:11:59,198 --> 00:12:02,050
Problem is mainly organizational.

179
00:12:03,110 --> 00:12:06,738
It would be hard to maintain solvent of pipelines.

180
00:12:06,914 --> 00:12:10,514
It would be hard to maintain an efficient communication

181
00:12:10,642 --> 00:12:14,390
between all those teams being from the operational team

182
00:12:14,460 --> 00:12:18,166
to engineering team, but also from

183
00:12:18,188 --> 00:12:22,282
the data engineering team to the data analyst science team.

184
00:12:22,416 --> 00:12:25,674
We have an issue to solve. So what

185
00:12:25,712 --> 00:12:28,886
would be the solution? In her article,

186
00:12:28,998 --> 00:12:32,794
Zamag Delgani tells us, but solutions

187
00:12:32,842 --> 00:12:36,846
coming from the software design world. And what I didn't mention

188
00:12:36,948 --> 00:12:40,702
is that at the time, Zamac was an employee of

189
00:12:40,756 --> 00:12:44,622
thoughtwork, a software consultancy firm specialized

190
00:12:44,686 --> 00:12:48,398
in software design. And I think there is no coincidence

191
00:12:48,494 --> 00:12:51,970
that it was one of their employee

192
00:12:52,310 --> 00:12:56,070
who came with this notion of data mesh,

193
00:12:56,490 --> 00:13:00,198
because like Zamax said, we can find

194
00:13:00,284 --> 00:13:03,634
some insights inside the domain driven

195
00:13:03,682 --> 00:13:08,646
design approach. This approach is about discussion

196
00:13:08,838 --> 00:13:12,182
with a strategic phase and a tactical

197
00:13:12,246 --> 00:13:15,786
phase. In this first strategic phase, we are

198
00:13:15,808 --> 00:13:19,370
going to understand the business

199
00:13:19,440 --> 00:13:22,986
domain dividing it into subdomains

200
00:13:23,018 --> 00:13:26,202
and gathering them into bounded context,

201
00:13:26,266 --> 00:13:29,950
which appears as physical boundaries

202
00:13:30,370 --> 00:13:34,382
between concrete that should communicate

203
00:13:34,446 --> 00:13:38,290
with one another, but at the same time be

204
00:13:38,360 --> 00:13:41,790
autonomous in their growth.

205
00:13:41,950 --> 00:13:46,674
And this discussion should happens between a multidisciplinary

206
00:13:46,802 --> 00:13:50,982
team inside a multidisciplinary team made

207
00:13:51,036 --> 00:13:55,298
of business analysts, product owner, product manager

208
00:13:55,474 --> 00:13:59,514
and also developers. So that we

209
00:13:59,712 --> 00:14:02,954
begin to have an ubiquitous language that

210
00:14:02,992 --> 00:14:06,662
we would use to create the different user stories.

211
00:14:06,806 --> 00:14:10,334
And at that point when we have the

212
00:14:10,372 --> 00:14:13,418
different words verbs,

213
00:14:13,514 --> 00:14:17,210
relationship between our business entities,

214
00:14:17,370 --> 00:14:21,678
when we have this ubiquitous language, we can define this

215
00:14:21,764 --> 00:14:25,822
bounded context. We begin to implement

216
00:14:25,886 --> 00:14:29,682
it through a tactical phase. And this implementation would

217
00:14:29,736 --> 00:14:33,534
come with technical patterns such as exagonal

218
00:14:33,582 --> 00:14:37,350
architecture, securers, event sourcing

219
00:14:37,690 --> 00:14:40,934
and so on, in order to have an application that

220
00:14:40,972 --> 00:14:43,510
is testable, maintainable,

221
00:14:43,930 --> 00:14:47,942
evolvable, and so on. And the fact

222
00:14:47,996 --> 00:14:51,686
that we talked about just before

223
00:14:51,788 --> 00:14:55,690
about data swamp, meaning a data lake which is

224
00:14:55,840 --> 00:14:59,590
really hard to understand. We can make a parallel

225
00:14:59,670 --> 00:15:03,694
with software design where we have the same kind of notion called big

226
00:15:03,732 --> 00:15:07,226
ball of mud, and the fact to use domain

227
00:15:07,258 --> 00:15:10,606
driven design is a good approach to

228
00:15:10,708 --> 00:15:14,606
avoid at all cost. This notion of big

229
00:15:14,628 --> 00:15:18,306
ball of mud and domain driven design is a

230
00:15:18,328 --> 00:15:22,114
kind of cycle. It does not end when we have our

231
00:15:22,232 --> 00:15:25,826
ebikitus language or even the code to represent

232
00:15:25,858 --> 00:15:29,954
the different user stories. We would need new features.

233
00:15:30,082 --> 00:15:33,320
So we would create new,

234
00:15:33,690 --> 00:15:37,442
enrich our ambiguous language with

235
00:15:37,596 --> 00:15:41,626
new verbs, new nouns, and maybe create

236
00:15:41,808 --> 00:15:45,574
another kind of language to create a new bounded

237
00:15:45,622 --> 00:15:50,350
context. And the idea of

238
00:15:50,420 --> 00:15:54,206
Zamac was to apply this

239
00:15:54,388 --> 00:15:58,014
way of thinking into the data world,

240
00:15:58,132 --> 00:16:01,626
in particular the data analytics world. If we sum

241
00:16:01,658 --> 00:16:05,246
up the main goal of the

242
00:16:05,348 --> 00:16:09,026
DDD, domain driven design is to

243
00:16:09,208 --> 00:16:13,470
make emerge or different ubiquitous languages

244
00:16:13,630 --> 00:16:17,202
which will be protected by the bonded context

245
00:16:17,346 --> 00:16:21,810
and then implemented as domain model decomposed

246
00:16:21,890 --> 00:16:25,810
into subdomains and finally implemented

247
00:16:25,890 --> 00:16:29,754
using some software design pattern. You may

248
00:16:29,792 --> 00:16:33,402
know some of them, MVC for model view

249
00:16:33,456 --> 00:16:37,594
controller, exagonal, secure rest, event sourcing and

250
00:16:37,632 --> 00:16:41,754
so on. If we take business domain

251
00:16:41,802 --> 00:16:46,410
like marine, we may have some different subdomains.

252
00:16:46,570 --> 00:16:49,934
And the idea of these slides is to show you that the

253
00:16:49,972 --> 00:16:54,094
discussion between the product owners, the business analysts

254
00:16:54,222 --> 00:16:58,366
and developers may conclude in different stories.

255
00:16:58,558 --> 00:17:02,046
We may have different bonded contexts according

256
00:17:02,078 --> 00:17:05,654
to the kind of discussion we would have, and especially the

257
00:17:05,692 --> 00:17:09,414
kind of issue we want to tackle. So at some point we may

258
00:17:09,452 --> 00:17:13,254
have four different bonded contexts or three different.

259
00:17:13,372 --> 00:17:16,774
It depends on your business needs, obviously,

260
00:17:16,892 --> 00:17:20,090
because all of those bonded contexts are

261
00:17:20,160 --> 00:17:24,038
part of the same business domain. They are to communicate,

262
00:17:24,134 --> 00:17:27,958
they cannot live alone. So communication

263
00:17:28,054 --> 00:17:32,160
also have to be consistent in terms of

264
00:17:32,530 --> 00:17:36,046
models we use to communicate between all

265
00:17:36,068 --> 00:17:39,290
those bonded, between all bonded contexts.

266
00:17:39,370 --> 00:17:43,070
And we have some patterns to apply this consistency.

267
00:17:43,150 --> 00:17:47,186
For instance, an anticorruption layer that would

268
00:17:47,368 --> 00:17:51,086
give the consumer here represented

269
00:17:51,118 --> 00:17:54,660
by the context two, the guarantee that

270
00:17:54,970 --> 00:17:59,350
what we consume from the provider context one,

271
00:17:59,500 --> 00:18:02,598
will match the needs in term of

272
00:18:02,684 --> 00:18:06,022
type, in terms of fields that

273
00:18:06,076 --> 00:18:09,270
we have in the context two, conversely,

274
00:18:09,350 --> 00:18:12,874
the provider, the data provider, would also be

275
00:18:12,912 --> 00:18:16,490
able to apply a logical layer

276
00:18:16,830 --> 00:18:20,334
that would allow him, allow it to

277
00:18:20,372 --> 00:18:23,674
create its own published language,

278
00:18:23,802 --> 00:18:28,202
to not pollute, let's say, the inner

279
00:18:28,266 --> 00:18:32,014
ubiquitous language, and have this autonomy we

280
00:18:32,052 --> 00:18:34,830
want for each bonded context.

281
00:18:34,990 --> 00:18:38,782
In term of organization, the DDD requires

282
00:18:38,846 --> 00:18:42,702
you to have a unique team per bonded context.

283
00:18:42,766 --> 00:18:46,294
And it's very important that we have this unique team so

284
00:18:46,332 --> 00:18:50,294
that the ownership is clearly stated and

285
00:18:50,412 --> 00:18:53,554
this very specific team will be in charge,

286
00:18:53,602 --> 00:18:57,834
will be accountable for the quality of its

287
00:18:57,872 --> 00:19:01,382
bonded context first to create SLA,

288
00:19:01,446 --> 00:19:04,774
SLO, for instance, but also accountable

289
00:19:04,902 --> 00:19:08,234
to the global consistency, the global rules we have

290
00:19:08,272 --> 00:19:11,962
inside this business domain

291
00:19:12,106 --> 00:19:16,014
splitted into bonded contexts. We don't want all those

292
00:19:16,052 --> 00:19:19,630
bonded contexts communicate in their way.

293
00:19:19,780 --> 00:19:23,022
We want them to communicate as

294
00:19:23,076 --> 00:19:27,390
if they were part of the same business domain. So here, for instance,

295
00:19:27,550 --> 00:19:31,582
we have the team one, which is accountable for bonded context

296
00:19:31,646 --> 00:19:35,410
one and two. It is possible it is a one to many

297
00:19:35,560 --> 00:19:39,762
relationship, but a given bonded context cannot

298
00:19:39,826 --> 00:19:44,262
have two teams accountable. And that's why the

299
00:19:44,396 --> 00:19:48,562
second team here will not be in charge of the first context,

300
00:19:48,626 --> 00:19:52,246
because team one is already in charge. So here we

301
00:19:52,268 --> 00:19:55,686
are, datamesh. So what is, for God's sake,

302
00:19:55,718 --> 00:19:58,682
the relationship between DDE and data mesh? Well,

303
00:19:58,736 --> 00:20:02,622
it's kind of obvious, because instead of thinking this way,

304
00:20:02,676 --> 00:20:06,762
operational world, analytics world, and then the ETL pipeline

305
00:20:06,826 --> 00:20:10,014
bridge, we apply the same mechanism we just

306
00:20:10,052 --> 00:20:13,950
saw in DDD. We have a multidisciplinary

307
00:20:14,030 --> 00:20:17,586
team which will discuss about a

308
00:20:17,608 --> 00:20:22,340
business domain and start to subdividing it into

309
00:20:23,590 --> 00:20:26,662
bounded context. So in a way, the data

310
00:20:26,716 --> 00:20:30,934
mesh where we have data domain should

311
00:20:30,972 --> 00:20:34,230
be called data bonded context.

312
00:20:34,810 --> 00:20:38,842
And this mesh is made of nodes represented by those

313
00:20:38,896 --> 00:20:43,034
bonded context or data domain and vertices by

314
00:20:43,072 --> 00:20:46,234
the different communications we have between those

315
00:20:46,272 --> 00:20:50,540
domains. So let's not forget that each domain has

316
00:20:51,250 --> 00:20:54,590
an ownership of a given ubiquitous language,

317
00:20:54,930 --> 00:20:58,606
but is not alone. In a way, we have to

318
00:20:58,628 --> 00:21:02,506
consume data coming from other domains in order to produce the

319
00:21:02,548 --> 00:21:05,762
different analytics we need and

320
00:21:05,816 --> 00:21:08,530
what is inside each domain.

321
00:21:08,950 --> 00:21:12,194
It's up to you. In fact, we can

322
00:21:12,232 --> 00:21:16,322
get black inside this domain to the very legacy

323
00:21:16,386 --> 00:21:20,198
way of thinking with the operational world

324
00:21:20,364 --> 00:21:24,146
being bridged to the analytics one through ETL

325
00:21:24,258 --> 00:21:27,720
and this is what we would observe usually.

326
00:21:28,110 --> 00:21:31,402
So data mesh is not saying that

327
00:21:31,456 --> 00:21:34,650
this approach is wrong, it's just saying

328
00:21:34,720 --> 00:21:39,210
that we have to have a step back and

329
00:21:39,280 --> 00:21:43,054
think the same way the DDD tells us

330
00:21:43,172 --> 00:21:46,970
in order to organizationally scale.

331
00:21:47,130 --> 00:21:51,102
What we have to understand is that the

332
00:21:51,156 --> 00:21:55,166
data mesh is a sociotechnical concept which brings

333
00:21:55,358 --> 00:21:58,494
especially an organizational scale

334
00:21:58,622 --> 00:22:02,558
and not really a technical one. The technical scaling

335
00:22:02,654 --> 00:22:06,514
is already solved, in my opinion. We have all the database

336
00:22:06,562 --> 00:22:10,786
and data warehouses we need. The pipelining is scaling

337
00:22:10,898 --> 00:22:14,406
with for instance, Apache beam spark and

338
00:22:14,428 --> 00:22:18,074
so on. So the problem is not here, it's more

339
00:22:18,112 --> 00:22:22,410
about tackling an organizational issue.

340
00:22:22,560 --> 00:22:26,406
And with the data mesh, as we have in the domain

341
00:22:26,438 --> 00:22:30,778
driven design, we have teams in ownership

342
00:22:30,874 --> 00:22:33,882
of well designed data domain,

343
00:22:34,026 --> 00:22:38,170
and we have to apply some pillars

344
00:22:38,330 --> 00:22:41,902
where the domain ownership is the main one

345
00:22:42,036 --> 00:22:46,046
and is backed up by three other pillars. Data as a product,

346
00:22:46,148 --> 00:22:49,790
self serve platform and federated computational governance.

347
00:22:49,870 --> 00:22:53,154
Data ownership is like we specified in

348
00:22:53,192 --> 00:22:56,822
DDD is. But stop thinking your business

349
00:22:56,876 --> 00:23:00,582
domain as a monolith. You have to split it into

350
00:23:00,716 --> 00:23:04,294
bonded context so that we have a

351
00:23:04,332 --> 00:23:08,558
team in charge to keep, to catalyte

352
00:23:08,594 --> 00:23:12,794
like we can say as a product, to provide

353
00:23:12,992 --> 00:23:16,698
SLA SlO, to provide a quality of service

354
00:23:16,864 --> 00:23:20,586
through a self serve platform which provides

355
00:23:20,618 --> 00:23:24,222
you with the technical assets you need

356
00:23:24,356 --> 00:23:26,906
technical assets that would scale,

357
00:23:27,018 --> 00:23:30,510
especially if we consider managed services on the cloud.

358
00:23:30,660 --> 00:23:34,702
But we still need at the same time a federated

359
00:23:34,766 --> 00:23:38,510
computational governance, so that, for instance,

360
00:23:38,590 --> 00:23:41,970
we don't exceed API quota. We keep

361
00:23:42,120 --> 00:23:44,740
in line with the naming policy,

362
00:23:45,210 --> 00:23:48,466
with the communication rules

363
00:23:48,578 --> 00:23:52,006
between all those data domains and so on.

364
00:23:52,188 --> 00:23:57,042
What we have to consider though, is unlike

365
00:23:57,106 --> 00:24:01,130
the service mesh for the DevOps who are listening,

366
00:24:01,470 --> 00:24:04,934
data mesh is not a purely technical concept.

367
00:24:04,982 --> 00:24:08,666
It's not like, okay, I am on my cloud platform and

368
00:24:08,688 --> 00:24:12,526
I will install a data mesh. It's not working like that.

369
00:24:12,628 --> 00:24:15,882
You have to first think your business domain

370
00:24:15,946 --> 00:24:19,774
and have a discussion between all the

371
00:24:19,812 --> 00:24:23,470
different jobs you have, developers, data scientists,

372
00:24:24,370 --> 00:24:27,714
business analysts and so on, in order to

373
00:24:27,752 --> 00:24:31,918
make emerge different data domains

374
00:24:32,094 --> 00:24:35,974
and activities such as event storming could

375
00:24:36,092 --> 00:24:39,846
help you to do so. So like I said, it's a social

376
00:24:39,948 --> 00:24:43,254
technical concrete which solve an

377
00:24:43,452 --> 00:24:47,586
organizational scalability issue. So be

378
00:24:47,628 --> 00:24:52,362
cautious about solutions that sells itself

379
00:24:52,496 --> 00:24:56,070
as data mesh ready solution.

380
00:24:56,230 --> 00:24:59,914
What exists, on the other hand, is our

381
00:24:59,952 --> 00:25:03,582
enablers, but a solution that would tell

382
00:25:03,636 --> 00:25:07,514
you, okay, you just have to put the coin inside the machine

383
00:25:07,562 --> 00:25:11,514
and here it is, you have your data mesh does not exist.

384
00:25:11,642 --> 00:25:15,614
And a data mesh is a path for better collaboration. It's not an

385
00:25:15,652 --> 00:25:19,026
end in itself, it's a means to reach a

386
00:25:19,048 --> 00:25:22,562
better collaboration between your team, a better understanding of your

387
00:25:22,616 --> 00:25:26,614
data. So where it shines, it is

388
00:25:26,652 --> 00:25:30,534
where your domain is complex, your business domain is

389
00:25:30,572 --> 00:25:33,634
complex. Where you have different subdomains,

390
00:25:33,682 --> 00:25:38,170
where you have a rich communication between entities,

391
00:25:38,510 --> 00:25:42,202
their data mesh will shine. But if your business domain is

392
00:25:42,256 --> 00:25:46,122
simple enough, there is nothing wrong to have the

393
00:25:46,256 --> 00:25:50,502
legacy approach. Considering only the operational

394
00:25:50,566 --> 00:25:54,074
world, analytics one, and the bridge

395
00:25:54,122 --> 00:25:57,162
in between represented by pipelines,

396
00:25:57,306 --> 00:26:00,878
it's perfectly fine to act this way.

397
00:26:01,044 --> 00:26:05,294
But once you begin to have organizational

398
00:26:05,342 --> 00:26:08,820
issue, once you begin to not understand

399
00:26:09,510 --> 00:26:13,538
what your business is, to not have the right insight to make

400
00:26:13,624 --> 00:26:17,026
evolve your business, maybe data mesh

401
00:26:17,058 --> 00:26:20,470
should be a solution here. So we were talking about data

402
00:26:20,540 --> 00:26:24,850
mesh from a theoretical

403
00:26:24,930 --> 00:26:29,366
point of view. Let's see what kind of implementations

404
00:26:29,478 --> 00:26:32,650
we can imagine. And I insist,

405
00:26:33,070 --> 00:26:36,806
imagine like I said, there is no data mesh

406
00:26:36,838 --> 00:26:40,560
ready solution. So Charles, it's up to you.

407
00:26:41,490 --> 00:26:45,514
So let's dig into the catalog. The catalog

408
00:26:45,562 --> 00:26:49,150
is the place where all domain can push their

409
00:26:49,220 --> 00:26:52,320
own products by product.

410
00:26:52,710 --> 00:26:56,914
Let's understand that we are talking about the data that

411
00:26:57,032 --> 00:27:01,010
each domain collect, store and

412
00:27:01,080 --> 00:27:04,910
want to make available for

413
00:27:05,080 --> 00:27:08,594
other domain. Let's see it like Marketplace,

414
00:27:08,642 --> 00:27:12,034
a catalog where every producer

415
00:27:12,082 --> 00:27:15,410
of data. So let's understand. A data domain

416
00:27:15,490 --> 00:27:19,066
can push and make available

417
00:27:19,248 --> 00:27:22,490
a product which is an aggregation,

418
00:27:23,150 --> 00:27:26,380
a formatted amount of data

419
00:27:26,990 --> 00:27:30,300
that subscriber can.

420
00:27:31,010 --> 00:27:34,814
So we will find here in the catalog a

421
00:27:34,852 --> 00:27:38,814
place where the data domain, so let's call

422
00:27:38,852 --> 00:27:42,550
them producer can push their own projects

423
00:27:42,650 --> 00:27:46,242
and subscriber people out of the domain can

424
00:27:46,296 --> 00:27:49,826
subscribe to those projects and start pull them.

425
00:27:49,928 --> 00:27:53,550
Each data owner will be in charge

426
00:27:53,630 --> 00:27:57,560
of describing its project and

427
00:27:57,930 --> 00:28:01,478
push describe define few

428
00:28:01,564 --> 00:28:06,242
parameters. Those projects will have a set of characteristics

429
00:28:06,386 --> 00:28:10,074
which will define basically what a product is.

430
00:28:10,192 --> 00:28:13,754
So you will find of course

431
00:28:13,952 --> 00:28:17,578
schema. You can also find information

432
00:28:17,664 --> 00:28:21,334
related to the API where you were pulling the refresh

433
00:28:21,382 --> 00:28:26,186
vacancy and all the information that producer

434
00:28:26,218 --> 00:28:29,840
can find relevant. This will help all the

435
00:28:30,210 --> 00:28:33,842
people from outside so the subscribers to

436
00:28:33,896 --> 00:28:37,886
pull the data correctly and automate

437
00:28:37,998 --> 00:28:42,066
the phases of pulling. We can also consider

438
00:28:42,168 --> 00:28:46,040
that subscriber can build their own project based

439
00:28:46,490 --> 00:28:50,486
on those projects, how it works.

440
00:28:50,588 --> 00:28:53,750
So you can open a contract,

441
00:28:54,250 --> 00:28:58,042
subscribe to a project and from there

442
00:28:58,176 --> 00:29:02,220
build your own set of data based on this

443
00:29:02,750 --> 00:29:06,442
project and enrich this

444
00:29:06,496 --> 00:29:09,258
project and build your own product on top of this.

445
00:29:09,344 --> 00:29:13,182
This means that you will create a project from

446
00:29:13,236 --> 00:29:16,910
other project by aggregating those data and transform those data.

447
00:29:17,060 --> 00:29:20,474
This is definitely doable and needs to be included

448
00:29:20,522 --> 00:29:24,038
into the project characteristic saying that this project is beta

449
00:29:24,074 --> 00:29:27,410
on this one and we are doing transformation

450
00:29:28,070 --> 00:29:31,794
on the first product. So how

451
00:29:31,832 --> 00:29:35,562
it works. We will dig into it into the third chapter.

452
00:29:35,726 --> 00:29:38,470
So now let's talk about the orchestrator.

453
00:29:38,810 --> 00:29:42,550
The orchestrator will be in charge of managing

454
00:29:43,050 --> 00:29:46,406
the data from the moment we pull it

455
00:29:46,428 --> 00:29:49,978
from the data sources and it became available

456
00:29:50,144 --> 00:29:53,062
for pulling by the subscribers.

457
00:29:53,206 --> 00:29:57,210
The orchestrator will also monitor all stages on

458
00:29:57,360 --> 00:30:00,574
the data pipeline, starting from how

459
00:30:00,612 --> 00:30:04,714
the data is ingested, if the old data has been ingested

460
00:30:04,842 --> 00:30:08,394
correctly, if the data is transformed

461
00:30:08,442 --> 00:30:11,966
correctly based on the description of transformation in the

462
00:30:11,988 --> 00:30:16,126
catalog, and if the data is correctly

463
00:30:16,238 --> 00:30:19,874
loaded into the data stores. From this

464
00:30:19,912 --> 00:30:23,730
moment, the orchestrator will work

465
00:30:23,800 --> 00:30:27,366
with the catalog to make sure that the state

466
00:30:27,548 --> 00:30:31,238
of the project is correctly updated, saying for example

467
00:30:31,324 --> 00:30:34,790
that the last refresh time is on

468
00:30:34,940 --> 00:30:38,220
22 of March 2023.

469
00:30:38,590 --> 00:30:42,378
Also, the orchestrator can provide

470
00:30:42,544 --> 00:30:45,702
an administration panel. This is helpful

471
00:30:45,766 --> 00:30:49,514
when you want to debug and see what is

472
00:30:49,552 --> 00:30:52,810
happening into the pipeline. For example, you just notice

473
00:30:52,890 --> 00:30:56,254
that project has not been refreshed as it

474
00:30:56,292 --> 00:30:59,486
should and you want to see what is happening.

475
00:30:59,668 --> 00:31:03,700
So the administrator panel will be able to see

476
00:31:04,470 --> 00:31:09,490
what job is currently running and maybe

477
00:31:09,560 --> 00:31:12,642
why. Also it is taking so much time.

478
00:31:12,776 --> 00:31:16,534
We can for example notice that our

479
00:31:16,572 --> 00:31:20,294
data sources is taking much longer to pull and

480
00:31:20,332 --> 00:31:23,942
to give us the data. If the usage of

481
00:31:23,996 --> 00:31:27,190
data mesh is growing within the organization,

482
00:31:27,770 --> 00:31:31,402
it can become hard to debug and

483
00:31:31,456 --> 00:31:35,274
see all the job and states on all the job at

484
00:31:35,312 --> 00:31:38,982
the current time. So having an administration

485
00:31:39,046 --> 00:31:43,290
panel to help you seeing graphically

486
00:31:43,370 --> 00:31:46,894
through a graphical interfaces what is

487
00:31:46,932 --> 00:31:50,062
happening within the data

488
00:31:50,116 --> 00:31:54,322
mesh infrastructures can help you and

489
00:31:54,376 --> 00:31:57,010
gain a lot of time on debug.

490
00:31:57,750 --> 00:32:01,970
So now let's dig into what kind of architecture

491
00:32:02,630 --> 00:32:05,560
can be built to host those services.

492
00:32:06,170 --> 00:32:09,958
Here we are, this is a zoom on a

493
00:32:09,964 --> 00:32:13,734
data domain. Here on the right hand side you

494
00:32:13,772 --> 00:32:17,454
will see data sources. All those data sources

495
00:32:17,602 --> 00:32:21,802
is basically a set of data like

496
00:32:21,936 --> 00:32:24,460
databases, another application,

497
00:32:24,910 --> 00:32:28,300
CSV files, whatever you want

498
00:32:29,310 --> 00:32:32,830
and this will be used as sources

499
00:32:33,410 --> 00:32:36,650
for our product. So let's

500
00:32:36,730 --> 00:32:40,394
start by the catalog. The catalog

501
00:32:40,522 --> 00:32:44,900
is in charge of creating products and

502
00:32:45,990 --> 00:32:49,730
based on those characteristics,

503
00:32:50,070 --> 00:32:54,034
on those parameters, the application

504
00:32:54,152 --> 00:32:57,414
transformation the orchestrator will

505
00:32:57,452 --> 00:33:01,160
be in charge of making this project available.

506
00:33:01,850 --> 00:33:05,350
So we will start by ingest the data

507
00:33:05,420 --> 00:33:08,978
from data sources. It can come from one

508
00:33:09,084 --> 00:33:11,770
to end sources.

509
00:33:12,430 --> 00:33:15,786
Once they are downloaded they will be pushed to a

510
00:33:15,808 --> 00:33:19,530
cage. So storing the data into the cage

511
00:33:19,890 --> 00:33:23,722
will avoid redownloading all the data from sources.

512
00:33:23,866 --> 00:33:27,838
If there is any issue with later

513
00:33:27,924 --> 00:33:32,138
operations like transformation for the transformation

514
00:33:32,234 --> 00:33:35,970
here we use spark with

515
00:33:36,040 --> 00:33:39,442
EMR on AWS to

516
00:33:39,496 --> 00:33:43,662
help us doing all those transformation. Basically the transformation

517
00:33:43,726 --> 00:33:47,250
that has been defined into the catalog.

518
00:33:47,750 --> 00:33:51,206
Once all those transformations are done, the data will

519
00:33:51,228 --> 00:33:55,554
be pushed to s three, redshift or aurora

520
00:33:55,602 --> 00:33:59,690
postgres. Why using proposing

521
00:34:00,030 --> 00:34:03,980
these three data stores? Because of the difference

522
00:34:04,350 --> 00:34:08,438
we can have in terms of data complexity

523
00:34:08,614 --> 00:34:11,358
or amount of data. For example,

524
00:34:11,444 --> 00:34:15,502
s three will be very useful if we have large amount

525
00:34:15,556 --> 00:34:19,070
of data, but redshift will

526
00:34:19,140 --> 00:34:22,726
also allow us to use SQL

527
00:34:22,778 --> 00:34:26,450
queries. So redshift can be very useful with

528
00:34:26,520 --> 00:34:30,434
large amount of data and if the application of

529
00:34:30,472 --> 00:34:34,830
exploration is using for example GDPC driver

530
00:34:34,910 --> 00:34:39,254
and want to use SQL queries to run against the data store

531
00:34:39,452 --> 00:34:43,510
the same way, aurora postgres can be very useful if

532
00:34:43,580 --> 00:34:47,094
the concurrency is very high. We all know

533
00:34:47,132 --> 00:34:50,746
that Redshift is a very powerful tool, but the

534
00:34:50,768 --> 00:34:54,506
concurrency is very hard to deal with

535
00:34:54,688 --> 00:34:58,874
that kind of data stores. Aura postgres allow us to

536
00:34:58,912 --> 00:35:02,254
be very efficient in terms of queries with

537
00:35:02,292 --> 00:35:06,222
a large amount of data and can give us very

538
00:35:06,276 --> 00:35:10,046
high amount of I ops and also can help us

539
00:35:10,068 --> 00:35:13,930
to have a very high amount of concurrent queries

540
00:35:14,090 --> 00:35:16,830
because of two main features,

541
00:35:17,170 --> 00:35:21,186
the rate auto scaling of course and also the fact that we can

542
00:35:21,208 --> 00:35:25,022
have very big instances and last

543
00:35:25,096 --> 00:35:28,594
but not least the exploration

544
00:35:28,722 --> 00:35:32,294
application. So this application will be

545
00:35:32,332 --> 00:35:36,226
in charge of retrieving efficiently

546
00:35:36,338 --> 00:35:40,220
the data within our data stores and make

547
00:35:40,590 --> 00:35:44,860
all those data so the product available

548
00:35:45,790 --> 00:35:50,522
for all the subscribers. This application will

549
00:35:50,576 --> 00:35:54,042
need to take in charge of those operations,

550
00:35:54,106 --> 00:35:58,126
meaning it needs to control the way it

551
00:35:58,148 --> 00:36:02,026
is retrieving the data to do not put too much pressure on data stores

552
00:36:02,138 --> 00:36:05,006
and do not impact all subscribers.

553
00:36:05,198 --> 00:36:08,820
And it needs to be intelligent enough

554
00:36:09,830 --> 00:36:14,450
to load, balance, shard or optimize

555
00:36:14,950 --> 00:36:19,780
customer queries. This is what

556
00:36:20,330 --> 00:36:23,890
data domain can put in place in AWS,

557
00:36:23,970 --> 00:36:27,442
for example to build

558
00:36:27,516 --> 00:36:30,954
their own data mesh. Here we can

559
00:36:30,992 --> 00:36:34,262
use containers within ecs,

560
00:36:34,326 --> 00:36:37,574
for example, the usage of containers

561
00:36:37,622 --> 00:36:41,742
is recommended as few of those operations can

562
00:36:41,796 --> 00:36:46,106
need data within the containers

563
00:36:46,298 --> 00:36:49,342
or also run for a long time.

564
00:36:49,476 --> 00:36:53,090
So I would recommend to use containers here instead

565
00:36:53,160 --> 00:36:56,978
of lambda as we can use it here

566
00:36:57,144 --> 00:37:01,774
for example to run our application orchestrator

567
00:37:01,902 --> 00:37:05,658
or catalog based on DynamoDB

568
00:37:05,854 --> 00:37:09,762
lambda and API gateway, basically the serverless

569
00:37:09,826 --> 00:37:13,014
framework. So this is all the things that we can

570
00:37:13,052 --> 00:37:16,630
build to make our projects available within

571
00:37:16,700 --> 00:37:20,678
our data domain. Now I will give the hand back to Ismail

572
00:37:20,774 --> 00:37:24,474
who will be introducing a Google project that

573
00:37:24,512 --> 00:37:28,122
is aiming to provide all

574
00:37:28,176 --> 00:37:31,806
those services and manage everything

575
00:37:31,908 --> 00:37:35,578
under the hood on Google side. Thank you Charles.

576
00:37:35,674 --> 00:37:39,406
Before concluding, I want to present you

577
00:37:39,508 --> 00:37:44,106
a quick overview of a product called GCP Dataplex.

578
00:37:44,218 --> 00:37:48,094
As we saw when we talk about data

579
00:37:48,132 --> 00:37:52,162
Mesh 3D product, we have to be very cautious because the

580
00:37:52,216 --> 00:37:56,562
real problem is not technical but more like organizational.

581
00:37:56,626 --> 00:38:00,054
So it requires you more to think about your

582
00:38:00,092 --> 00:38:03,414
business domain than buying another

583
00:38:03,532 --> 00:38:07,550
product. But we saw that we have enablers

584
00:38:07,650 --> 00:38:11,050
that enables you to implement the

585
00:38:11,120 --> 00:38:14,714
pillars of domain ownership data as a product self

586
00:38:14,752 --> 00:38:18,710
services platform and federated computational governance.

587
00:38:18,870 --> 00:38:22,430
We think that GCP dataplex from

588
00:38:22,500 --> 00:38:26,654
Google is a good example of such products

589
00:38:26,852 --> 00:38:30,094
because it

590
00:38:30,132 --> 00:38:34,050
will offer you logical layer that

591
00:38:34,120 --> 00:38:37,826
will federate the different existing services

592
00:38:38,008 --> 00:38:41,250
on GCP, such as bigquery,

593
00:38:41,830 --> 00:38:45,798
dataflow, cloud storage and so on. In order

594
00:38:45,884 --> 00:38:49,526
to give you a sense of what

595
00:38:49,708 --> 00:38:53,846
should be data mesh. If we look at

596
00:38:53,948 --> 00:38:58,070
this logical layer, we have a lake

597
00:38:59,210 --> 00:39:02,566
which represents in fact the data domain

598
00:39:02,758 --> 00:39:06,330
and which relies on certain amount of

599
00:39:06,400 --> 00:39:10,386
services such as the data catalog,

600
00:39:10,518 --> 00:39:14,880
which will store metadata related to the different

601
00:39:15,490 --> 00:39:19,594
data that you will store and make compute

602
00:39:19,642 --> 00:39:22,962
on. And also of course Google Cloud

603
00:39:23,016 --> 00:39:27,246
IAM which will allow you to federate

604
00:39:27,358 --> 00:39:31,278
to give you the federated computational governance

605
00:39:31,454 --> 00:39:36,146
on different assets on different GCP

606
00:39:36,338 --> 00:39:40,274
assets such as bigquery or cloud storage.

607
00:39:40,402 --> 00:39:45,030
Another point is that the lake will be

608
00:39:45,180 --> 00:39:49,334
separated into zones which represent

609
00:39:49,382 --> 00:39:53,146
a kind of logical separation of

610
00:39:53,168 --> 00:39:56,598
your data. We can interpret it as package

611
00:39:56,694 --> 00:40:00,842
if we reason in terms of language

612
00:40:00,986 --> 00:40:04,894
development, for instance, and each of which

613
00:40:05,012 --> 00:40:09,130
will be attached to different kind of assets.

614
00:40:09,290 --> 00:40:13,662
Depending what the team associated

615
00:40:13,726 --> 00:40:17,794
to zone wants to do. Each asset will

616
00:40:17,832 --> 00:40:22,302
benefit from by design technical metadata

617
00:40:22,366 --> 00:40:26,598
such as schema for instance, the type of

618
00:40:26,684 --> 00:40:30,082
the different schema of big rate tables for instance,

619
00:40:30,146 --> 00:40:34,194
and will be automatically reported to the lake.

620
00:40:34,322 --> 00:40:38,042
It is interesting to observe that

621
00:40:38,176 --> 00:40:42,090
the different assets we link

622
00:40:42,240 --> 00:40:45,690
to the lake are not necessarily part

623
00:40:45,760 --> 00:40:49,210
of the same GCP project. In fact,

624
00:40:49,280 --> 00:40:52,814
we can see a dataplex lake as the same

625
00:40:52,852 --> 00:40:55,886
kind of abstraction of a GCP project,

626
00:40:56,068 --> 00:40:59,562
but only for data where the GCP

627
00:40:59,626 --> 00:41:04,606
project allows you to abstract the billing

628
00:41:04,718 --> 00:41:08,322
and API quota. The data lake will allow

629
00:41:08,376 --> 00:41:11,890
you to abstract the notion of data mesh through

630
00:41:11,960 --> 00:41:15,574
this federated computational governance which is

631
00:41:15,612 --> 00:41:19,446
not really per project but

632
00:41:19,548 --> 00:41:23,910
per lake. We do not forget also that given

633
00:41:23,980 --> 00:41:28,200
lake which represents a data domain is not enough.

634
00:41:28,650 --> 00:41:32,474
We also have other lakes which represent other data

635
00:41:32,512 --> 00:41:36,362
domains and as we saw, in the end they will be

636
00:41:36,416 --> 00:41:39,942
able to communicate according the

637
00:41:40,016 --> 00:41:43,978
permission we set in the federated computational

638
00:41:44,074 --> 00:41:48,000
governance layer and also according obviously

639
00:41:48,450 --> 00:41:52,710
the need of communication between those domains.

640
00:41:52,810 --> 00:41:57,762
So let's see short how

641
00:41:57,816 --> 00:42:01,534
it illustrates. So here I am on GCP

642
00:42:01,582 --> 00:42:05,990
console. So obviously when we consider Dataplex,

643
00:42:06,730 --> 00:42:10,402
we have to be familiarized

644
00:42:10,546 --> 00:42:14,280
with the Google environment and

645
00:42:14,730 --> 00:42:18,874
we will have in the manage section the different

646
00:42:19,072 --> 00:42:23,066
lakes that represent our data domain. We can create

647
00:42:23,248 --> 00:42:27,302
new one if we have the right permissions

648
00:42:27,446 --> 00:42:31,070
and inside each lake

649
00:42:31,570 --> 00:42:35,450
we can do certain amount of action. For instance,

650
00:42:35,530 --> 00:42:39,760
we can federate the permissions on the different

651
00:42:40,130 --> 00:42:43,742
zones of this lake and we can grant

652
00:42:43,806 --> 00:42:47,118
access to those specific zone.

653
00:42:47,294 --> 00:42:50,786
Here we have three different zones, one of

654
00:42:50,808 --> 00:42:54,906
which I created two different assets.

655
00:42:55,038 --> 00:42:58,582
So if I go inside the zones, I will be

656
00:42:58,636 --> 00:43:02,310
able to see my different assets. I can also

657
00:43:02,380 --> 00:43:06,374
create and delete the existing assets and of

658
00:43:06,412 --> 00:43:10,582
course add permissions on the zone

659
00:43:10,726 --> 00:43:14,362
itself, but also on the

660
00:43:14,416 --> 00:43:18,358
asset itself. So here my two different assets

661
00:43:18,454 --> 00:43:21,966
are a

662
00:43:21,988 --> 00:43:25,114
bigquery data set and a storage

663
00:43:25,162 --> 00:43:28,910
bucket. In Dataplex,

664
00:43:29,330 --> 00:43:33,380
those are the main asset type.

665
00:43:34,070 --> 00:43:37,810
That doesn't mean that you cannot set

666
00:43:37,880 --> 00:43:42,142
other kind of assets, but it would be necessary

667
00:43:42,286 --> 00:43:45,750
through those pillar assets,

668
00:43:46,570 --> 00:43:50,118
since bigquery, for instance, allows you

669
00:43:50,204 --> 00:43:53,320
to later fetch information from

670
00:43:54,970 --> 00:43:58,920
history, blob objects, or even

671
00:44:00,650 --> 00:44:02,970
on premise databases.

672
00:44:04,350 --> 00:44:08,602
See Bigquery Omni from

673
00:44:08,656 --> 00:44:12,910
the manage section. I won't be able to access the

674
00:44:12,980 --> 00:44:16,560
assets. It would be from the

675
00:44:17,170 --> 00:44:21,258
catalog, the data cataloging feature of Dataplex,

676
00:44:21,354 --> 00:44:25,300
which is a kind of search engine

677
00:44:26,310 --> 00:44:29,540
which will rely on the

678
00:44:30,070 --> 00:44:34,146
metadata, the technical one, of course, the name

679
00:44:34,328 --> 00:44:38,086
of my different schema, the name of the

680
00:44:38,268 --> 00:44:42,054
column and the types, but also on

681
00:44:42,092 --> 00:44:45,894
the business metadata, and we will see how

682
00:44:45,932 --> 00:44:50,410
to provide them to Dataplex.

683
00:44:51,070 --> 00:44:54,970
Here I can see that I indeed

684
00:44:55,310 --> 00:44:58,650
can access to my assets

685
00:44:58,990 --> 00:45:02,270
and I have a certain amount of filter

686
00:45:02,850 --> 00:45:06,880
that allows me to

687
00:45:07,650 --> 00:45:10,814
add more criteria in my

688
00:45:10,852 --> 00:45:14,258
search. If I go on

689
00:45:14,424 --> 00:45:15,650
the assets,

690
00:45:17,990 --> 00:45:22,606
I can have different kind of metadata, technical metadata

691
00:45:22,718 --> 00:45:26,870
on it. I can access the schema

692
00:45:27,930 --> 00:45:31,970
and we can see that I can associate

693
00:45:32,050 --> 00:45:35,960
them to business terms. Those are the specific

694
00:45:36,490 --> 00:45:40,220
business metadata I was talking about

695
00:45:41,790 --> 00:45:45,206
and which will be in fact fed

696
00:45:45,398 --> 00:45:49,450
by a glossary which explains

697
00:45:49,970 --> 00:45:54,398
the data domain we are working on.

698
00:45:54,564 --> 00:45:58,794
This is very important because it will allows new users

699
00:45:58,922 --> 00:46:03,406
to have context

700
00:46:03,518 --> 00:46:07,074
on what kind of business we

701
00:46:07,112 --> 00:46:10,866
are working on. And we can

702
00:46:10,888 --> 00:46:14,850
see that here I created a people domain

703
00:46:15,210 --> 00:46:19,320
that I document and finally

704
00:46:19,690 --> 00:46:23,266
I create a new element,

705
00:46:23,458 --> 00:46:27,410
gods, with a definition on which

706
00:46:27,500 --> 00:46:31,690
I can create relationship. So here a commercial

707
00:46:32,590 --> 00:46:36,282
is related to a customer by this

708
00:46:36,336 --> 00:46:40,560
definition. And I added the link

709
00:46:41,010 --> 00:46:44,446
and I can also add a steward which is

710
00:46:44,468 --> 00:46:48,062
a kind of owner of this definition so

711
00:46:48,116 --> 00:46:52,490
that any people in question

712
00:46:52,660 --> 00:46:56,210
for this notion is able to contact

713
00:46:56,360 --> 00:46:58,100
the right person.

714
00:46:59,750 --> 00:47:05,254
And if we go back to the

715
00:47:05,292 --> 00:47:09,190
assets which consume those elements,

716
00:47:09,530 --> 00:47:12,710
I am able in the catalog

717
00:47:13,290 --> 00:47:17,350
to search for customer, for instance,

718
00:47:19,870 --> 00:47:22,918
and see that the assets

719
00:47:23,014 --> 00:47:27,242
associated to this notion is

720
00:47:27,376 --> 00:47:30,686
bring back the search engine. So this

721
00:47:30,708 --> 00:47:34,990
is very interesting in terms of data exploration and

722
00:47:35,060 --> 00:47:38,030
data and business understanding.

723
00:47:38,850 --> 00:47:43,214
Of course, my data lake goal

724
00:47:43,262 --> 00:47:46,562
is not to just expose my

725
00:47:46,616 --> 00:47:50,878
data to understand the business data domain,

726
00:47:50,974 --> 00:47:55,022
but also to apply transformation on it and to

727
00:47:55,096 --> 00:47:59,158
have quality insights on it.

728
00:47:59,324 --> 00:48:03,158
And this section of process allow you to

729
00:48:03,244 --> 00:48:07,946
create tasks under different data that

730
00:48:08,048 --> 00:48:11,962
you consume, that you stored inside the

731
00:48:12,016 --> 00:48:14,460
assets we just saw.

732
00:48:17,790 --> 00:48:21,360
And those processes are

733
00:48:22,770 --> 00:48:25,582
provided by services,

734
00:48:25,716 --> 00:48:29,742
Google services, which are

735
00:48:29,876 --> 00:48:32,990
not to create by yourself,

736
00:48:33,140 --> 00:48:36,850
but which is possibly

737
00:48:37,510 --> 00:48:41,634
given to you through templates for

738
00:48:41,672 --> 00:48:45,334
the different tasks that are common. But keep in

739
00:48:45,372 --> 00:48:49,110
mind that you are also able to provide your own business

740
00:48:49,180 --> 00:48:52,470
logic through for instance, dataflow pipelines.

741
00:48:53,450 --> 00:48:56,950
You also have the capacity to define

742
00:48:57,470 --> 00:49:01,482
specific processing to have more

743
00:49:01,536 --> 00:49:03,980
insight on the quality of your data.

744
00:49:04,910 --> 00:49:08,346
This feature is still on preview, but relies on

745
00:49:08,448 --> 00:49:12,142
dedicated data quality project from

746
00:49:12,196 --> 00:49:16,206
Google which allows you to expose the different

747
00:49:16,308 --> 00:49:20,142
rules you want to apply on your data through

748
00:49:20,276 --> 00:49:24,386
YamL file. And it

749
00:49:24,408 --> 00:49:27,838
is quite interesting in terms of possibility

750
00:49:28,014 --> 00:49:31,170
we can have on this feature.

751
00:49:31,910 --> 00:49:36,306
And of course we have also the secure section

752
00:49:36,418 --> 00:49:40,518
that allows you to have a broad overview of

753
00:49:40,604 --> 00:49:44,934
what kind of access you can give on the

754
00:49:44,972 --> 00:49:48,874
lake, but also the zones and the

755
00:49:48,912 --> 00:49:52,140
assets associated to them.

756
00:49:52,750 --> 00:49:58,298
So like we saw, this product

757
00:49:58,464 --> 00:50:02,910
is more like an abstraction layer rather

758
00:50:02,980 --> 00:50:07,018
than a real service like bigquery

759
00:50:07,194 --> 00:50:10,334
and gives you pointers to

760
00:50:10,452 --> 00:50:13,918
different assets. So bigquery,

761
00:50:14,014 --> 00:50:17,170
data set and cloud storage bucket

762
00:50:17,590 --> 00:50:21,454
to federate the different processes

763
00:50:21,502 --> 00:50:24,290
you will apply on your data,

764
00:50:24,440 --> 00:50:27,798
but also the different permissions you

765
00:50:27,884 --> 00:50:31,334
will apply on it. And last but not

766
00:50:31,372 --> 00:50:35,602
least, it also enables the exploration

767
00:50:35,746 --> 00:50:39,862
of your entire data set with

768
00:50:39,996 --> 00:50:43,526
elements that are technical, the type of

769
00:50:43,548 --> 00:50:47,126
data you are looking for, but also related to

770
00:50:47,148 --> 00:50:51,134
the business. Thanks to the glossary we

771
00:50:51,172 --> 00:50:55,306
just it will concrete this presentation.

772
00:50:55,498 --> 00:50:59,326
Thank you for your attention and if

773
00:50:59,348 --> 00:51:03,470
you have any questions feel free to join us on the chat.

774
00:51:04,130 --> 00:51:04,620
See you.

