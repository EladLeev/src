1
00:00:25,970 --> 00:00:29,126
Hi everyone. I'm really happy to be here today, and thank you all

2
00:00:29,148 --> 00:00:32,642
for joining. I'll introduce myself. My name is Naomi.

3
00:00:32,706 --> 00:00:36,626
I'm a software developer with previous experience in risk and data analysis.

4
00:00:36,738 --> 00:00:40,514
And this lecture today, strain comparison in real life, is inspired

5
00:00:40,562 --> 00:00:44,674
by various projects that I've conducted at my workplace. One of my main goals

6
00:00:44,722 --> 00:00:48,086
today is to provide you tools and ideas that you can apply at your day

7
00:00:48,108 --> 00:00:51,966
to day work. Now, before we dive in and start talking about all

8
00:00:51,988 --> 00:00:55,374
the different methods that we will discuss today, when they're useful and

9
00:00:55,412 --> 00:00:59,006
why, I want to start by sharing with you a story. This is a

10
00:00:59,028 --> 00:01:01,786
story about me in the world of string comparison.

11
00:01:01,898 --> 00:01:05,882
But most of all, this is a story about why what we will discuss today

12
00:01:06,036 --> 00:01:09,826
is really relevant for you. So my story starts about a

13
00:01:09,848 --> 00:01:13,746
year and a half ago. I was at work, and I noticed that one

14
00:01:13,768 --> 00:01:17,186
of my colleagues was quite struggling with comparing two large sets of

15
00:01:17,208 --> 00:01:21,186
data, or of strings, to be precise. So this colleague

16
00:01:21,218 --> 00:01:24,738
of mine, he knew that the two sets of data he was trying to compare

17
00:01:24,834 --> 00:01:28,466
were very similar to one another, but not quite identical.

18
00:01:28,658 --> 00:01:31,942
And he wanted, for each pair of strings that he was comparing

19
00:01:32,086 --> 00:01:35,290
to be able to determine the similarity level, or difference

20
00:01:35,360 --> 00:01:39,558
level, if you'd like, by producing some sort of a similarity score.

21
00:01:39,734 --> 00:01:42,618
So he decided to come up with his own set of logics,

22
00:01:42,714 --> 00:01:45,834
aiming to tackle different scenarios that we face when we compare

23
00:01:45,882 --> 00:01:49,406
strings. Now, of course, trying to come up with your own set

24
00:01:49,428 --> 00:01:52,846
of logics, it's difficult, it's complicated, it is

25
00:01:52,868 --> 00:01:56,434
not very intuitive. But looking back at what he did,

26
00:01:56,552 --> 00:01:59,998
I have to say that he had a point. Because when we talk about strain

27
00:02:00,014 --> 00:02:03,090
comparison, actually many things would go wrong.

28
00:02:03,240 --> 00:02:06,514
I mean, we have typos and abbreviations. Maybe the

29
00:02:06,552 --> 00:02:09,766
words have been reordered in the sentence or repeated with

30
00:02:09,788 --> 00:02:13,254
no added value to the meaning of the sentence. Or even if we talk about

31
00:02:13,292 --> 00:02:17,094
punctuation, we have dot and comma and a dash, and many

32
00:02:17,132 --> 00:02:20,826
things that could affect the result of the comparison. But most of all,

33
00:02:20,848 --> 00:02:24,714
bring us to one important conclusion. There is no single definition for

34
00:02:24,752 --> 00:02:27,590
similarity or difference of strengths.

35
00:02:27,750 --> 00:02:31,226
So my goal is that by the end of this lecture, you will

36
00:02:31,248 --> 00:02:34,926
not only be familiar with the different obstacles that we face when you go to

37
00:02:34,948 --> 00:02:38,734
work, but you will also have a wide enough set of tools to make

38
00:02:38,772 --> 00:02:42,618
your next data oriented project efficient, elegant and useful.

39
00:02:42,794 --> 00:02:46,466
Because at the end of the day, when we talk about string comparison, we talk

40
00:02:46,488 --> 00:02:50,066
about something that has a lot of real world applications if we only know how

41
00:02:50,088 --> 00:02:53,646
to use that correctly. For example, let's talk about fraud

42
00:02:53,678 --> 00:02:57,726
detection. Fraudsters many times steal other people's identities

43
00:02:57,838 --> 00:03:01,106
in order to obtain credit line and make financial purchases.

44
00:03:01,298 --> 00:03:05,030
Now, if we are a company working in financial services, we would probably

45
00:03:05,100 --> 00:03:08,038
like to know if the person on the other side of the screen is a

46
00:03:08,044 --> 00:03:11,618
fraudster or suspected to be one. This way we would

47
00:03:11,644 --> 00:03:15,338
be able to prevent a purchase from being made or take additional security

48
00:03:15,424 --> 00:03:18,906
measures in order to verify the identity of the person on

49
00:03:18,928 --> 00:03:21,978
the other side of the screen. So let's take this a

50
00:03:21,984 --> 00:03:25,758
bit more to practice. Let's say that we are indeed a company working in

51
00:03:25,764 --> 00:03:29,614
financial services, and we have access to a list of identities that

52
00:03:29,652 --> 00:03:33,662
we know are commonly used by fraudsters, either stolen identities

53
00:03:33,726 --> 00:03:37,214
or made up identities. And in this list of identities

54
00:03:37,342 --> 00:03:41,454
we have access to the people's full names, addresses and other relevant

55
00:03:41,502 --> 00:03:45,678
such identifiers. So let's look at George Forge.

56
00:03:45,854 --> 00:03:49,574
George is a legitimate person whose identity has been stolen and

57
00:03:49,612 --> 00:03:53,206
is now being used by a fraudster. The fraudster wants to open an

58
00:03:53,228 --> 00:03:57,014
account in our service by using the info of George. Now as

59
00:03:57,052 --> 00:04:00,898
you can see, the fraudster decided to tweak a few minor details when opening

60
00:04:00,914 --> 00:04:04,838
the account. If we were trying to compare the info that was provided

61
00:04:04,854 --> 00:04:08,042
to us with the info that we already have by using only exact

62
00:04:08,096 --> 00:04:11,706
match, the comparison would fail. But if we would

63
00:04:11,728 --> 00:04:14,794
be able to use a more flexible string comparison algorithm,

64
00:04:14,922 --> 00:04:18,186
we would also be able to say this looks at usage in the info

65
00:04:18,218 --> 00:04:22,106
of George. Let's dig in a bit deeper. Another usage

66
00:04:22,138 --> 00:04:25,706
that I want to discuss is flexibility for typos. So let's

67
00:04:25,738 --> 00:04:29,406
say that you are developing a guessing game, and in our game the user

68
00:04:29,438 --> 00:04:32,910
needs to guess a word or a sentence, which is practically a string,

69
00:04:32,990 --> 00:04:36,466
by typing in free text. So for long up

70
00:04:36,488 --> 00:04:39,638
strings we would like to have some flexibility because let's say that the

71
00:04:39,644 --> 00:04:42,630
user missed out a character, maybe added one,

72
00:04:42,700 --> 00:04:46,374
removed one, or replaced one character with another one. So in

73
00:04:46,412 --> 00:04:49,638
case a human being would look at the pair of strings and would say,

74
00:04:49,804 --> 00:04:53,386
this practically looks like the same thing for me. We, as the

75
00:04:53,408 --> 00:04:57,366
developers of the game would like to say, let's not reject the input.

76
00:04:57,558 --> 00:05:01,014
Another usage that I want to discuss is in the Meditech industry.

77
00:05:01,142 --> 00:05:04,702
Did you know comparing sequences of DNA is done by

78
00:05:04,756 --> 00:05:08,286
comparing sets of strings? Now we will look more in

79
00:05:08,308 --> 00:05:11,950
depth to this DNA sequencing example later on in this lecture.

80
00:05:12,370 --> 00:05:15,982
And of course the list could go on and on. There are many different

81
00:05:16,036 --> 00:05:19,902
usages for string comparison. We just need to be familiar with the tools,

82
00:05:20,046 --> 00:05:23,390
and Python gives us some built in operations for string comparison.

83
00:05:23,470 --> 00:05:26,674
But I want to briefly discuss them and see why they're not

84
00:05:26,712 --> 00:05:30,194
exactly what we're looking for today. First, but in operation

85
00:05:30,242 --> 00:05:34,166
is exact match. When using exact match, even a single change of

86
00:05:34,188 --> 00:05:37,654
a character, replacing it with another one, or changing it to

87
00:05:37,692 --> 00:05:41,050
a capital letter is already enough to have the exact

88
00:05:41,120 --> 00:05:44,346
match returning the value false. Now, this is great for

89
00:05:44,368 --> 00:05:48,662
many applications, but if we're talking about edge basis typos,

90
00:05:48,806 --> 00:05:52,586
a boolean answer is not good enough for us. Another built in

91
00:05:52,608 --> 00:05:56,410
operation is the ability to check if one string is contained

92
00:05:56,910 --> 00:05:59,854
in the other. So if one string is fully contained in the other one,

93
00:05:59,892 --> 00:06:04,134
this is great. But if we're talking about partial containment typos,

94
00:06:04,282 --> 00:06:08,050
such different edge cases. Again, this is not flexible enough

95
00:06:08,120 --> 00:06:11,874
for our needs. And that is when the comparison methods that

96
00:06:11,912 --> 00:06:15,302
we will discuss today will get into action. So today

97
00:06:15,356 --> 00:06:19,506
we will learn about two python libraries. The first is called jellyfish

98
00:06:19,618 --> 00:06:22,310
and the second is called fuzzywazi.

99
00:06:24,490 --> 00:06:27,658
Yeah, don't worry about the bear, we will

100
00:06:27,664 --> 00:06:30,918
get to it later. So, jellyfish,

101
00:06:31,014 --> 00:06:34,714
let's begin. Jellyfish is a python library that

102
00:06:34,752 --> 00:06:39,254
has two main sections of functions. The first is of Eddie distance functions.

103
00:06:39,382 --> 00:06:43,514
Here, each such function receives two strings and produces a distance

104
00:06:43,562 --> 00:06:47,086
score by counting the amount of steps required in

105
00:06:47,108 --> 00:06:49,998
order to convert string a to string b.

106
00:06:50,164 --> 00:06:53,662
The second section is of phonetic encoding. This is not about

107
00:06:53,716 --> 00:06:57,586
comparison, and we will not discuss this today. But I will briefly mention

108
00:06:57,688 --> 00:07:01,026
that here each such function receives a string and

109
00:07:01,048 --> 00:07:04,382
produces a code which indicates about the phonetic pronunciation

110
00:07:04,446 --> 00:07:08,194
of the string. So under editistance, we will learn

111
00:07:08,232 --> 00:07:11,522
about Levinchen distance and the Marvel Levin distance.

112
00:07:11,666 --> 00:07:15,506
But before doing so, I want to introduce a term, a definition

113
00:07:15,618 --> 00:07:18,934
of string metric. String metric receives two

114
00:07:18,972 --> 00:07:22,646
strings and produces a distance score by applying

115
00:07:22,678 --> 00:07:26,358
some sort of magic. As you can see now, both Levenstein

116
00:07:26,374 --> 00:07:30,278
distance and the Marl Levente distance are different types of string metrics,

117
00:07:30,374 --> 00:07:33,574
each of them with a slightly different logic to calculate

118
00:07:33,622 --> 00:07:37,134
the distance score. Levente distance is

119
00:07:37,172 --> 00:07:40,554
a very fundamental and important string metric

120
00:07:40,602 --> 00:07:44,598
in the string comparison world. Many other string metrics and string comparison

121
00:07:44,634 --> 00:07:48,562
algorithms are based on that one, including all the ones that we will discuss

122
00:07:48,616 --> 00:07:52,286
today. So leverage and distance counts the minimal

123
00:07:52,318 --> 00:07:56,414
amount of steps required in order to convert string a to string

124
00:07:56,462 --> 00:07:59,618
b. A step is considered as addition of a character,

125
00:07:59,714 --> 00:08:03,494
a deletion of a character, or a replacement of one character in

126
00:08:03,532 --> 00:08:07,174
another one. The higher the score is, the bigger the difference.

127
00:08:07,372 --> 00:08:10,770
So let's look at a few examples to make this much more clear.

128
00:08:10,940 --> 00:08:14,890
In our first example, we can see a comparison between exit and exist

129
00:08:15,310 --> 00:08:18,634
here in order to convert string a to string b,

130
00:08:18,752 --> 00:08:21,820
all that we would have to do would be adding the letter s.

131
00:08:22,430 --> 00:08:25,726
Now going the other way around. If you would like to convert string b to

132
00:08:25,748 --> 00:08:29,066
string a, all that we would have to do would be removing the letter

133
00:08:29,098 --> 00:08:33,182
s. So in this case, no matter which direction we choose to take,

134
00:08:33,316 --> 00:08:36,786
the minimal amount of steps required in order to make the conversion would

135
00:08:36,808 --> 00:08:40,706
be one. Now in our second example we can see

136
00:08:40,728 --> 00:08:44,130
a comparison between great and great. In this case,

137
00:08:44,200 --> 00:08:47,426
in order to convert string a to string b, all that

138
00:08:47,448 --> 00:08:51,714
we would have to do would be removing the letter e from the first string

139
00:08:51,842 --> 00:08:55,702
and then adding the letter e again at the end of that string. In this

140
00:08:55,756 --> 00:08:59,674
case, the minimal amount of steps required in order to convert string a to

141
00:08:59,712 --> 00:09:03,322
string b would be two. And on our third

142
00:09:03,376 --> 00:09:07,562
example we can see a comparison between look and lock. In this

143
00:09:07,616 --> 00:09:11,050
case, technically, in order to convert string a to string b,

144
00:09:11,120 --> 00:09:14,326
we could have removed the second o from the word look

145
00:09:14,448 --> 00:09:17,694
and then added the letter c instead. And this

146
00:09:17,732 --> 00:09:21,006
would result in two steps. But we also have a

147
00:09:21,028 --> 00:09:24,266
step of replacement. So we can replace the second

148
00:09:24,308 --> 00:09:28,126
o with c and then the minimal amount of steps

149
00:09:28,158 --> 00:09:31,474
required in order to make the conversion would be one.

150
00:09:31,672 --> 00:09:34,786
We will note that replacement is when we are

151
00:09:34,808 --> 00:09:38,374
looking at a single index and replacing for that same index one

152
00:09:38,412 --> 00:09:41,702
character with another one. That is why replacement only takes

153
00:09:41,756 --> 00:09:45,238
place on the third example and not on the second example. Because in

154
00:09:45,244 --> 00:09:49,062
the second example e, the letter e was relocated

155
00:09:49,126 --> 00:09:51,340
from one index to another one.

156
00:09:52,190 --> 00:09:55,318
Now we are moving on to the moral Levinchen

157
00:09:55,334 --> 00:09:59,190
distance. The moral lensian distance is very similar to levente

158
00:09:59,690 --> 00:10:02,770
and distance, but it also has an added value. The moral

159
00:10:02,790 --> 00:10:06,446
Levin distance counts the swap of two adjacent characters as a

160
00:10:06,468 --> 00:10:09,786
single step instead of two steps. So let's

161
00:10:09,818 --> 00:10:13,326
look at our next example to make this much more clear. Here we can

162
00:10:13,348 --> 00:10:15,966
see a comparison between swap and sub.

163
00:10:16,148 --> 00:10:19,522
Now the Marl Levin distance recognizes that two

164
00:10:19,576 --> 00:10:22,994
adjacent characters a and w have been swapped and

165
00:10:23,032 --> 00:10:26,834
therefore it counts this as a single step.

166
00:10:26,952 --> 00:10:30,466
But for levention distance we can see that this would be two steps

167
00:10:30,578 --> 00:10:33,810
because for levention distance, in order to make the conversion

168
00:10:33,890 --> 00:10:37,174
we would have to remove the letter w and then add it

169
00:10:37,212 --> 00:10:40,780
again, or remove the letter a and then add it again.

170
00:10:41,790 --> 00:10:45,386
So this would be two steps. Now we

171
00:10:45,408 --> 00:10:48,918
are getting to our most important question. When is it useful?

172
00:10:49,014 --> 00:10:52,960
When can we make use of that? So let's look at our next example.

173
00:10:53,410 --> 00:10:56,970
Here we can see dna sequencing. Dna sequencing

174
00:10:57,050 --> 00:11:00,334
is the act of taking two sequences of dna and comparing them

175
00:11:00,372 --> 00:11:04,522
in order to determine the distance level between them. DNA sequencing

176
00:11:04,586 --> 00:11:08,274
is commonly done by using Levinstein distance, which is something that

177
00:11:08,312 --> 00:11:10,660
I, by the way, find as pretty cool.

178
00:11:11,030 --> 00:11:15,026
And now, moving on to our next example, we can see two comparisons where

179
00:11:15,048 --> 00:11:18,882
the first is Mr. Bean versus Mr. Bean, and the second is

180
00:11:18,936 --> 00:11:22,726
Johnny Depp vs. Johnny Depp. In the first example, we can see that the

181
00:11:22,748 --> 00:11:25,654
only difference is a dot, which we, by the way,

182
00:11:25,692 --> 00:11:28,918
know indicates about an abbreviations. And on the second example,

183
00:11:29,004 --> 00:11:32,166
the only difference is a swap of two adjacent characters.

184
00:11:32,358 --> 00:11:36,122
Now, we could say that if we know our expected input is a person's full

185
00:11:36,176 --> 00:11:39,626
name, it is very probable that for small enough changes,

186
00:11:39,728 --> 00:11:43,214
maybe one step, two steps of difference would

187
00:11:43,252 --> 00:11:46,634
be small enough for us to say this is practically

188
00:11:46,682 --> 00:11:49,950
the same thing for me. Let's not reject the input.

189
00:11:50,450 --> 00:11:53,962
Now we're getting to our next question, when is it not useful?

190
00:11:54,106 --> 00:11:57,650
So I want to show you the next example. Here we can see

191
00:11:57,720 --> 00:12:01,586
I love comparing strings versus comparing strings. I love.

192
00:12:01,768 --> 00:12:05,070
Now, if we check out the distance score, we can see that it is 14,

193
00:12:05,150 --> 00:12:08,670
which is quite high. Without any additional information,

194
00:12:08,840 --> 00:12:12,326
we would not be able to determine if the distance score is so high

195
00:12:12,428 --> 00:12:15,526
because the strings are indeed unrelated from one another,

196
00:12:15,708 --> 00:12:19,366
or if this is the case, that a person, a human being, will look at

197
00:12:19,388 --> 00:12:22,634
a pair of strings and would say, this is practically the same thing.

198
00:12:22,672 --> 00:12:25,866
For me, let's not reject the input. And that

199
00:12:25,888 --> 00:12:28,838
is when fuzzy wazi is getting into action.

200
00:12:29,014 --> 00:12:32,654
Now, before we talk about fuzzy wazi, there's one thing that we have to talk

201
00:12:32,692 --> 00:12:36,414
about. I mean, did you know that fuzzywazi was the bear that

202
00:12:36,452 --> 00:12:40,298
had no hair? So fuzzywazi wasn't very fuzzy,

203
00:12:40,394 --> 00:12:43,966
was he? Now apparently fuzzywazi is

204
00:12:43,988 --> 00:12:47,566
a rhyme or a tongue twister, and I was not familiar

205
00:12:47,598 --> 00:12:51,470
to that at all until I started working with Faziwazi Python library.

206
00:12:51,630 --> 00:12:55,122
So I guess there's always something new to learn, and today

207
00:12:55,176 --> 00:12:58,866
we will learn more about this library. So let's begin.

208
00:12:59,048 --> 00:13:02,418
Fuzzywazi is a library that has a few different functions.

209
00:13:02,514 --> 00:13:06,194
Each such function receives two strings and produces a similarity

210
00:13:06,242 --> 00:13:09,814
score. On a scale of zero to 1000 indicates

211
00:13:09,862 --> 00:13:13,242
no match, the strings are completely unrelated from one another,

212
00:13:13,376 --> 00:13:16,970
and 100 indicates either an exact match or something

213
00:13:17,040 --> 00:13:20,450
very close to that. Now, today, under Fuzzy wazzi,

214
00:13:20,470 --> 00:13:23,482
we will learn about three functions, fuzz ratio,

215
00:13:23,626 --> 00:13:27,290
fuzz token sort ratio, and fuzz token set ratio.

216
00:13:27,450 --> 00:13:31,146
So let's start with our first function, fuzz ratio.

217
00:13:31,338 --> 00:13:34,942
Fuzz ratio is a function that receives two strings and produces

218
00:13:35,006 --> 00:13:38,674
a similarity score based on the following equation. Given two

219
00:13:38,712 --> 00:13:42,350
strings where t is the total number of elements, which is practically

220
00:13:42,430 --> 00:13:46,114
characters, and m is the total number of matches where

221
00:13:46,152 --> 00:13:49,574
matches are defined as characters that appear in both sequences and

222
00:13:49,612 --> 00:13:53,206
in the same order. So the similarity score is given by

223
00:13:53,228 --> 00:13:57,094
the equation twice m divided by t all multiplied by

224
00:13:57,132 --> 00:14:00,506
100. Now to give you a bit more sense, a bit more

225
00:14:00,528 --> 00:14:04,198
intuition about this equation, I want to show you what happens when we are comparing

226
00:14:04,294 --> 00:14:07,578
two identical strings. So here when we

227
00:14:07,584 --> 00:14:11,226
are comparing same and same, we can see that m number of

228
00:14:11,248 --> 00:14:14,334
matches is four, t total length is

229
00:14:14,372 --> 00:14:18,346
eight. And then using our equation, we would see that the similarity score

230
00:14:18,378 --> 00:14:21,962
is 100. And if we would be comparing

231
00:14:22,026 --> 00:14:25,394
two strings that are completely unrelated to one another, we would

232
00:14:25,432 --> 00:14:28,642
be able to see that m number of matches is zero,

233
00:14:28,776 --> 00:14:33,054
which is also our numerator. So in this case the similarity

234
00:14:33,102 --> 00:14:36,946
score would also be zero. Now moving on to a more realistic

235
00:14:36,978 --> 00:14:40,790
example, something in between. We can see that the comparison between great

236
00:14:40,860 --> 00:14:44,294
and green is a comparison where m number of

237
00:14:44,332 --> 00:14:47,670
matches is three because gr and e

238
00:14:47,740 --> 00:14:51,562
appear in both sequences and in the same order. T total

239
00:14:51,616 --> 00:14:55,222
length is ten. Therefore, the equation

240
00:14:55,366 --> 00:14:58,758
gives us the twice three divided by ten, all multiplied

241
00:14:58,774 --> 00:15:02,106
by 100 is 60. Now I

242
00:15:02,128 --> 00:15:05,886
want to discuss this example because I think it's a pretty interesting one.

243
00:15:06,068 --> 00:15:09,486
Here we can see two comparisons where the first is

244
00:15:09,588 --> 00:15:13,454
for two other cells that are very similar to one another, let alone a few

245
00:15:13,492 --> 00:15:17,198
minor changes. And the second comparison is a versus

246
00:15:17,294 --> 00:15:20,862
ABCDE. Now if we'd be checking the distance

247
00:15:20,926 --> 00:15:24,414
score, we would see that the distance score for both comparisons

248
00:15:24,462 --> 00:15:27,702
is the same, it is four. But if we would

249
00:15:27,756 --> 00:15:31,014
check the similarity code, we would be able to see that using

250
00:15:31,052 --> 00:15:34,406
the funds ratio function, the similarity score of

251
00:15:34,428 --> 00:15:37,558
the first comparison is 96, which is on a

252
00:15:37,564 --> 00:15:41,594
scale of zero to 100, relatively high. And the similarity code for the

253
00:15:41,632 --> 00:15:45,850
second comparison is 33. And here on a scale of zero to 100,

254
00:15:45,920 --> 00:15:49,514
it is relatively low. So this is interesting because now

255
00:15:49,552 --> 00:15:53,722
we can see the added value that fuzz ratio gives us where the

256
00:15:53,776 --> 00:15:56,880
jellyfish functions that we saw before could not give us.

257
00:15:57,330 --> 00:16:00,958
Another thing that we want to take into account when we're using fuzz ratio function

258
00:16:01,044 --> 00:16:04,110
is that it is case sensitive. As you can see here,

259
00:16:04,180 --> 00:16:08,530
capital a and non capital a are considered as two different strings.

260
00:16:08,950 --> 00:16:12,466
So if we do not care about capitalization, we would want to

261
00:16:12,488 --> 00:16:15,966
apply lower on our strings before making the comparison.

262
00:16:16,158 --> 00:16:19,494
The case sensitivity is true for fuzz ratio function,

263
00:16:19,612 --> 00:16:22,630
but it is not true for all fuzzywazi functions.

264
00:16:23,610 --> 00:16:27,126
Now when we are talking about fuzzywazi that we know, all of

265
00:16:27,148 --> 00:16:30,614
the fuzzywazi functions produce similarity scores on scale of zero

266
00:16:30,652 --> 00:16:34,058
to 100. We also want to be familiar with an important

267
00:16:34,144 --> 00:16:37,162
concept which is called a threshold score. So,

268
00:16:37,216 --> 00:16:41,302
let's say that we have a project, and in our project we made three comparisons.

269
00:16:41,446 --> 00:16:44,238
The first comparison resulted a score of 90.

270
00:16:44,404 --> 00:16:47,262
The second comparison resulted a score of 95.

271
00:16:47,396 --> 00:16:50,926
And the third comparison resulted a score of 96 or 97 or

272
00:16:50,948 --> 00:16:54,622
98. So we would have to ask ourselves

273
00:16:54,766 --> 00:16:58,414
for which score are two given strings considered as identical

274
00:16:58,542 --> 00:17:01,906
or similar enough to one another? And for which score are

275
00:17:01,928 --> 00:17:05,634
two given strings considered as different. Now the answer for that

276
00:17:05,672 --> 00:17:09,510
is a threshold score. So let's say that in our project

277
00:17:09,660 --> 00:17:13,138
we decided that all comparison with scores of 80 and above

278
00:17:13,234 --> 00:17:16,934
would be considered as similar, and all comparisons with

279
00:17:16,972 --> 00:17:20,666
scores of 30 and below would be considered as different.

280
00:17:20,848 --> 00:17:24,138
And all comparisons and resulted scores in the range in between

281
00:17:24,224 --> 00:17:26,198
would be considered as undecisive,

282
00:17:26,294 --> 00:17:29,882
inconclusive. So our next question

283
00:17:29,936 --> 00:17:33,946
now would be how do we choose the threshold code? Now the

284
00:17:33,968 --> 00:17:37,706
full answer for that is a bit more lengthy than the scope of this lecture,

285
00:17:37,818 --> 00:17:41,662
but I will briefly mention that it depends on the requirements and the needs

286
00:17:41,716 --> 00:17:44,850
of our project. For example, it is possible

287
00:17:44,920 --> 00:17:48,526
that in different projects we would have a higher or a lower tolerance

288
00:17:48,558 --> 00:17:52,334
level for what we would call as errors, maybe false positives,

289
00:17:52,382 --> 00:17:55,746
true negatives. So we would want to adjust of

290
00:17:55,768 --> 00:17:59,046
torture scores accordingly. Also, another thing to

291
00:17:59,068 --> 00:18:02,646
take into account is the characteristics of the data that we

292
00:18:02,668 --> 00:18:06,546
are working with. For example, let's say that we have two different projects.

293
00:18:06,658 --> 00:18:10,326
In both projects we would compare people's full names, but in

294
00:18:10,348 --> 00:18:13,626
one project these would be people coming from the US and in

295
00:18:13,648 --> 00:18:17,306
the second project these would be people coming from Europe. So it

296
00:18:17,328 --> 00:18:20,922
is possible that in different continents, people's names would be longer,

297
00:18:21,056 --> 00:18:24,654
shorter or more similar to one another. So it is

298
00:18:24,692 --> 00:18:28,170
also possible that same scores in different projects could indicate

299
00:18:28,250 --> 00:18:31,566
completely different things. Therefore, we will need to

300
00:18:31,588 --> 00:18:34,930
take these considerations into account when we are choosing our first

301
00:18:35,000 --> 00:18:38,690
scores. Now let's move on to our next function.

302
00:18:38,840 --> 00:18:42,846
Fuzz token sort ratio. Fuzz token sort ratio words

303
00:18:42,878 --> 00:18:47,170
the tokens practically substrings before making the comparison.

304
00:18:47,330 --> 00:18:51,446
So here we can see sort and compare versus compare and sort

305
00:18:51,628 --> 00:18:54,760
would produce a similarity code of 100.

306
00:18:55,610 --> 00:18:59,298
Now moving on to a more realistic example, we can see Emma walked home with

307
00:18:59,324 --> 00:19:03,290
Lisa versus Lisa walked with Emma home. In this case,

308
00:19:03,360 --> 00:19:06,060
that similarity score would be again 100.

309
00:19:06,430 --> 00:19:09,674
It also teaches us that fuzz token sort ratio is

310
00:19:09,712 --> 00:19:13,262
not case sensitive. Now moving on to our last function

311
00:19:13,316 --> 00:19:16,666
for today, fuzz token set ratio. Fuzz token

312
00:19:16,698 --> 00:19:19,854
set ratio is very similar to the previous function that we saw.

313
00:19:19,892 --> 00:19:23,166
Now, Fuzz token sort ratio in the meaning that it is

314
00:19:23,188 --> 00:19:26,754
not case sensitive and it sorts the tokens before making

315
00:19:26,792 --> 00:19:30,194
a comparison. But it also has the added value of

316
00:19:30,232 --> 00:19:33,886
a set, which means it does not count duplicates.

317
00:19:34,078 --> 00:19:37,810
So as you can see in our next example, sort lower

318
00:19:37,880 --> 00:19:41,162
and nor Pete versus lower nor peaks.

319
00:19:41,246 --> 00:19:45,320
Sort and sort produces a similarity score of 100.

320
00:19:45,690 --> 00:19:48,974
Now if we would take the same comparison but use the previous

321
00:19:49,042 --> 00:19:52,554
function, we would see that the similarity code is lower

322
00:19:52,672 --> 00:19:56,186
91 in this case because the previous function, as we know,

323
00:19:56,288 --> 00:19:59,686
does not remove duplicates before making a comparison.

324
00:19:59,878 --> 00:20:03,694
Now moving on to a more realistic example, we can see I love

325
00:20:03,732 --> 00:20:07,530
chocolate and ice cream compared to I love ice cream

326
00:20:07,610 --> 00:20:11,294
and I love chocolate. True story by the way.

327
00:20:11,492 --> 00:20:15,620
So in this case we can see that the similarity score is 100.

328
00:20:16,470 --> 00:20:20,462
Now I want to discuss the usages and advantages of fuzzywazi.

329
00:20:20,606 --> 00:20:23,746
Fuzzywazi produces similarity scores on scale of

330
00:20:23,768 --> 00:20:27,602
zero to 100 for pairs of strings. It also

331
00:20:27,656 --> 00:20:31,030
has tolerance for what we would call as typos, which is

332
00:20:31,100 --> 00:20:34,694
minor changes in the strings. So it is possible for two given

333
00:20:34,732 --> 00:20:39,010
strings that are very similar to one another and not quite identical

334
00:20:39,170 --> 00:20:42,246
to produce similarity score of 100.

335
00:20:42,428 --> 00:20:46,298
Because eventually fuzzy wazi is here to answer the question,

336
00:20:46,464 --> 00:20:49,914
would a human being who looks at a pair of strings say,

337
00:20:50,112 --> 00:20:53,710
this practically looks like the same thing for me? And if not so,

338
00:20:53,780 --> 00:20:57,790
to which extent? Fuzzywazi also has tolerance for

339
00:20:57,860 --> 00:21:01,534
changes in order of substrings depending on the function that we choose

340
00:21:01,572 --> 00:21:05,374
to use, and it also has tolerance for repetitions,

341
00:21:05,502 --> 00:21:09,634
again, depending on the functions that we choose to use. Another thing

342
00:21:09,672 --> 00:21:13,730
that we want to know about fuzzywazi is that it simplifies the data preprocessing step

343
00:21:13,800 --> 00:21:17,894
for us, because when we approach a data oriented project, we would

344
00:21:17,932 --> 00:21:21,526
want to go through the data preprocessing step, which means cleaning the

345
00:21:21,548 --> 00:21:24,854
data before making a comparison, and therefore we

346
00:21:24,892 --> 00:21:27,946
would be able to increase the accuracy of our

347
00:21:27,968 --> 00:21:31,370
results. So if our data preprocessing step

348
00:21:31,440 --> 00:21:35,126
goes through removing duplicates, sorting substrings

349
00:21:35,158 --> 00:21:39,050
and applying lower, we know that fuzzywazi can do all of these things

350
00:21:39,120 --> 00:21:42,806
for us. Another thing that we want to know about Fuzzywazi,

351
00:21:42,838 --> 00:21:45,870
which is also true to the jellyfish function that we saw before,

352
00:21:46,020 --> 00:21:49,454
is that it is really easy to use. So in case

353
00:21:49,492 --> 00:21:52,894
we have a feature or maybe a plant feature that is

354
00:21:52,932 --> 00:21:56,770
really nice to have, it is not top priority, not very urgent.

355
00:21:57,190 --> 00:22:01,186
We would want to be familiar with fuzzywazi and jellyfish because

356
00:22:01,368 --> 00:22:05,246
in this case it would not waste expensive time of our software

357
00:22:05,278 --> 00:22:08,270
developers, data scientists, data analysts,

358
00:22:08,430 --> 00:22:12,022
because it is really easy to learn, easy to use, easy to change

359
00:22:12,076 --> 00:22:15,682
whenever we need. Now, if you want to know more about Faziwazi,

360
00:22:15,746 --> 00:22:18,626
I invite you to check out the articles that I wrote about it and posted

361
00:22:18,658 --> 00:22:22,118
on medium. The first article covers the functions that we learned

362
00:22:22,134 --> 00:22:25,210
today and also covers another function that we did not discuss

363
00:22:25,280 --> 00:22:29,242
today. And the second article talks more about the process.

364
00:22:29,376 --> 00:22:32,886
It gets to the before and after of making a comparison,

365
00:22:32,998 --> 00:22:36,826
gets more in depth to the data preprocessing step, as well as explains

366
00:22:36,858 --> 00:22:40,800
what to do after we already made a comparison and have scores with us.

367
00:22:41,970 --> 00:22:45,566
And now let's see what we learned today. Today we learned

368
00:22:45,598 --> 00:22:49,602
about two python libraries. The first is called jellyfish and the second

369
00:22:49,656 --> 00:22:53,918
is called fuzzywazi. Under jellyfish we learn about Levinstein

370
00:22:53,934 --> 00:22:58,054
distance and the moral Levinson distance. And under Fuzzywazi we

371
00:22:58,092 --> 00:23:01,330
learn about fuzz ratio, fuzz token sort ratio,

372
00:23:01,410 --> 00:23:05,126
and fuzz token set ratio. One last thing before we

373
00:23:05,148 --> 00:23:07,842
finish. If you have any questions, comments,

374
00:23:07,906 --> 00:23:11,334
or simply want to keep in touch, I invite you to reach out to me

375
00:23:11,372 --> 00:23:14,758
using my LinkedIn profile. Also, if you want to get more of the

376
00:23:14,764 --> 00:23:18,070
contents that I write, I invite you to follow me on medium.

377
00:23:18,490 --> 00:23:21,994
That's it. That's all for today. Thank you all for listening

378
00:23:22,042 --> 00:23:25,774
and I hope you enjoyed the lecture. I was Naomi Kriger explain about

379
00:23:25,812 --> 00:23:27,500
strain comparison in real life.

