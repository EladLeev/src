1
00:00:25,890 --> 00:00:29,238
Hello, my name is Pavos Kripak and today

2
00:00:29,404 --> 00:00:33,974
with Anna Warno we will present how

3
00:00:34,012 --> 00:00:37,682
to use the complex AI based forecasting methods

4
00:00:37,746 --> 00:00:41,254
for investments portfolio optimization. Anya is

5
00:00:41,292 --> 00:00:44,354
senior data scientist in seven Bullscom.

6
00:00:44,482 --> 00:00:48,262
She is working with the time series AI forecasting methods,

7
00:00:48,396 --> 00:00:52,506
also other machine learning methods and I'm

8
00:00:52,538 --> 00:00:56,270
a cto in the AI investments company which

9
00:00:56,340 --> 00:01:00,058
aims to build the complete portfolio optimization

10
00:01:00,154 --> 00:01:03,986
platform using the latest achievements in machine learning

11
00:01:04,088 --> 00:01:07,662
methods. Even more, we have built the first version

12
00:01:07,726 --> 00:01:11,074
of this software which is currently deployed for one

13
00:01:11,112 --> 00:01:15,082
of the investment funds from the Luxembourg.

14
00:01:15,166 --> 00:01:18,374
Today we want to tell you about the

15
00:01:18,412 --> 00:01:22,514
time series and what are the most advanced

16
00:01:22,562 --> 00:01:26,150
and available currently method. And in the second part,

17
00:01:26,220 --> 00:01:30,086
Anya will show the real hands on session

18
00:01:30,198 --> 00:01:33,562
how to work with the time series, how to use this

19
00:01:33,616 --> 00:01:36,730
method, what are the results, and even more,

20
00:01:36,800 --> 00:01:40,622
Anya will show how to use the reinforcement learning. For that

21
00:01:40,676 --> 00:01:44,254
purposes, of course I want some toy example, but to

22
00:01:44,292 --> 00:01:47,950
give you feeling and impression I will go through

23
00:01:48,020 --> 00:01:51,646
this method listed here very briefly and in

24
00:01:51,668 --> 00:01:55,182
the details. Anya will show NBIT's method

25
00:01:55,246 --> 00:01:59,230
and complete this NBIT's methods to the typical statistical

26
00:01:59,310 --> 00:02:02,750
method of the forecasting. That's the agenda.

27
00:02:02,910 --> 00:02:06,262
Let's start why we are talking about this

28
00:02:06,316 --> 00:02:09,794
topic. Recently we are observing the dynamic

29
00:02:09,842 --> 00:02:13,330
development of the time series forecasting method,

30
00:02:13,410 --> 00:02:16,722
especially machine learning based methods

31
00:02:16,866 --> 00:02:20,186
which has significantly higher

32
00:02:20,368 --> 00:02:24,374
accuracy than statistical method. Time series

33
00:02:24,422 --> 00:02:28,202
forecasting is applicable for many, many applications from

34
00:02:28,256 --> 00:02:31,994
forecasting sales stock, some make

35
00:02:32,032 --> 00:02:36,042
some forecasting in health and also in visual recognition

36
00:02:36,106 --> 00:02:39,790
and so on. But it is also applicable for

37
00:02:39,860 --> 00:02:43,746
financial time series and for financial time series we

38
00:02:43,768 --> 00:02:47,026
are able to achieve over 60% of

39
00:02:47,048 --> 00:02:51,358
the accuracy, which is a very significant

40
00:02:51,454 --> 00:02:55,454
number for the financial time series. For other time series

41
00:02:55,582 --> 00:02:59,622
which are easier to forecast. Of course it's not a very impressive number,

42
00:02:59,756 --> 00:03:03,330
but this one gives a significant edge

43
00:03:03,490 --> 00:03:07,206
in the portfolio construction and the results of

44
00:03:07,228 --> 00:03:11,094
the investments shows that. What is time

45
00:03:11,132 --> 00:03:15,274
series? Time series is the ordered time list of

46
00:03:15,312 --> 00:03:18,762
all values with the given attribute one or

47
00:03:18,816 --> 00:03:22,782
more, and time series forecasting is the ability to

48
00:03:22,836 --> 00:03:26,862
forecast the future value of that series which is

49
00:03:26,916 --> 00:03:30,798
not known in the given time and machine learning

50
00:03:30,884 --> 00:03:34,702
time series. AI forecasting methods are recently

51
00:03:34,766 --> 00:03:38,674
developed and achieve very good results. There are

52
00:03:38,712 --> 00:03:42,514
methods of the time series forecasting which are based on the

53
00:03:42,552 --> 00:03:45,410
machine learning models and methods.

54
00:03:45,930 --> 00:03:49,270
Till this time or two years ago,

55
00:03:49,340 --> 00:03:52,854
three years ago, there was the domination of the

56
00:03:52,892 --> 00:03:56,354
statistical method of the forecasting arma Rima,

57
00:03:56,402 --> 00:04:00,742
exponential smoothing and other methods. But recent

58
00:04:00,806 --> 00:04:04,154
evolution and recent breakthrough in

59
00:04:04,192 --> 00:04:07,738
forecasting which probably the most significant point

60
00:04:07,824 --> 00:04:11,646
is the M four and M five competition show that

61
00:04:11,748 --> 00:04:15,102
machine learning methods are simply better

62
00:04:15,236 --> 00:04:19,098
than statistical one by significant margin

63
00:04:19,194 --> 00:04:22,410
of the accuracy. M competition is

64
00:04:22,500 --> 00:04:26,110
probably the most prestigious and for sure scientifically

65
00:04:26,190 --> 00:04:29,662
baked competition related to the time series.

66
00:04:29,726 --> 00:04:33,742
AI forecasting methods organized by University of Nicosia

67
00:04:33,806 --> 00:04:37,094
and Professor Spiros Makriakis. And the

68
00:04:37,132 --> 00:04:40,920
fourth edition of that competition, it was

69
00:04:41,290 --> 00:04:45,094
over two years ago, almost three years ago, and in

70
00:04:45,132 --> 00:04:49,094
this fourth edition 1st 2nd

71
00:04:49,212 --> 00:04:52,618
place were taken by machine learning based

72
00:04:52,704 --> 00:04:56,394
method. One of this method will be presented very briefly today

73
00:04:56,512 --> 00:05:01,658
and the M five competition has been finished.

74
00:05:01,754 --> 00:05:05,962
Previous year was dominated by the machine learning method.

75
00:05:06,026 --> 00:05:09,854
What is important about M four competition is that it was

76
00:05:09,972 --> 00:05:13,422
on the very huge number of the time series.

77
00:05:13,486 --> 00:05:17,230
There was 100,000 different time series

78
00:05:17,310 --> 00:05:21,314
with different number of points and different frequency. So the

79
00:05:21,352 --> 00:05:25,798
method which won was really tested on huge

80
00:05:25,884 --> 00:05:30,082
data set. And this winning method is ES hybrid

81
00:05:30,146 --> 00:05:34,150
methods. It was designed by smell

82
00:05:34,490 --> 00:05:38,490
data scientist from Uber and there are some

83
00:05:38,640 --> 00:05:41,990
very unique elements of that method.

84
00:05:42,070 --> 00:05:46,262
First one is to include data preprocessing

85
00:05:46,326 --> 00:05:50,414
into the back propagation. So the parameters of the

86
00:05:50,452 --> 00:05:54,270
data processing which was exponential smoothing were

87
00:05:54,340 --> 00:05:57,834
learned together with learning weights

88
00:05:57,882 --> 00:06:01,790
of the neural networks. Second thing was very

89
00:06:01,860 --> 00:06:05,554
unique approach for the assembling and

90
00:06:05,592 --> 00:06:09,698
assigning models cto the given time series. And third thing

91
00:06:09,784 --> 00:06:13,422
was the neural networks. In this methods

92
00:06:13,486 --> 00:06:17,994
are used LSTM networks, but not typical LSTM,

93
00:06:18,062 --> 00:06:21,442
but residual delighted and with attention.

94
00:06:21,586 --> 00:06:25,590
So this residuality was one of the first

95
00:06:25,740 --> 00:06:29,186
application of this concept. It's very heavy

96
00:06:29,218 --> 00:06:33,058
applicable for the image recognition to the LSTM

97
00:06:33,154 --> 00:06:36,778
and two time series forecasting and it gives

98
00:06:36,864 --> 00:06:40,430
very good results, especially that suave use

99
00:06:40,500 --> 00:06:44,014
different residuality rules for different

100
00:06:44,132 --> 00:06:47,642
frequency of the time series. Second method

101
00:06:47,706 --> 00:06:51,194
which will be described and presented by Anya

102
00:06:51,322 --> 00:06:55,026
later in the session is the NBITS method. This is

103
00:06:55,048 --> 00:06:58,642
the purely machine learning method comparing to the

104
00:06:58,696 --> 00:07:01,890
ES hybrid which is considered hybrid

105
00:07:02,230 --> 00:07:05,406
and it is constructed from

106
00:07:05,448 --> 00:07:09,542
the block. So it is some kind of the stacked architecture with

107
00:07:09,596 --> 00:07:13,014
also very unique construction. There are different type

108
00:07:13,052 --> 00:07:16,946
of the blocks for trends, seasonality and some generic

109
00:07:17,058 --> 00:07:20,262
blocks. And also there are some features related

110
00:07:20,326 --> 00:07:24,186
to the explainability and transfer learning. And also

111
00:07:24,208 --> 00:07:27,542
the NBITs uses advanced model ensembling.

112
00:07:27,686 --> 00:07:30,894
Here we have a brief overview how the

113
00:07:30,932 --> 00:07:34,042
NBITs looks. Here is also the reference

114
00:07:34,106 --> 00:07:37,386
to the paper. More details are provided.

115
00:07:37,498 --> 00:07:41,690
Third method, probably the easiest one to use is Facebook

116
00:07:41,770 --> 00:07:45,646
profit. It is based on the general's additive models.

117
00:07:45,678 --> 00:07:49,042
It is available as a library and

118
00:07:49,096 --> 00:07:52,654
also provide quite good accuracy

119
00:07:52,702 --> 00:07:57,058
levels. And the important thing of that method

120
00:07:57,154 --> 00:08:00,834
is that we are able to calculate also the range

121
00:08:00,882 --> 00:08:04,338
of the uncertainty or confidential levels.

122
00:08:04,434 --> 00:08:07,894
Not only the one value and this method

123
00:08:07,942 --> 00:08:11,306
is available as a library. So it

124
00:08:11,328 --> 00:08:14,794
is very easy to start using this method for your

125
00:08:14,832 --> 00:08:18,374
own proposals. Third method or fourth

126
00:08:18,422 --> 00:08:22,554
method is the glue on TS. It is also complete framework

127
00:08:22,682 --> 00:08:26,318
of the libraries also available to download and use.

128
00:08:26,404 --> 00:08:30,554
It is more difficult to use than just profit,

129
00:08:30,602 --> 00:08:34,202
but much more powerful. There are many methods included,

130
00:08:34,266 --> 00:08:37,614
many way of the preprocessing data. It has

131
00:08:37,652 --> 00:08:40,962
a support for the cloud computing and also

132
00:08:41,016 --> 00:08:45,910
very strong support from the community and scientific

133
00:08:46,490 --> 00:08:50,502
researchers. Also here we have the paper to the

134
00:08:50,636 --> 00:08:54,198
TS. There are very interesting concept of

135
00:08:54,284 --> 00:08:57,630
different methods, different network structures,

136
00:08:57,730 --> 00:09:01,574
probabilistic distributions,

137
00:09:01,702 --> 00:09:05,782
type denveral components, it is constantly evoluted

138
00:09:05,846 --> 00:09:09,494
and new elements are added. And also it's

139
00:09:09,542 --> 00:09:13,550
available, as I said, as a library so it can be download and

140
00:09:13,620 --> 00:09:17,054
use. And the third method is very unique based

141
00:09:17,092 --> 00:09:20,622
on that. Settling machines, also called stochastic learning

142
00:09:20,756 --> 00:09:23,854
automata is very unique for the forecasting.

143
00:09:23,902 --> 00:09:27,346
But stochastic learning automata itself is known for

144
00:09:27,368 --> 00:09:32,030
the long time since the 60s from previous century.

145
00:09:32,190 --> 00:09:35,558
Here we have one of the latest paper

146
00:09:35,644 --> 00:09:39,318
related to that topic and the main

147
00:09:39,404 --> 00:09:43,474
differences of settling machine comparing to the neural networks

148
00:09:43,522 --> 00:09:47,222
is that it learns the probability distribution for each

149
00:09:47,276 --> 00:09:50,842
parameter and each of the feature and based on that

150
00:09:50,976 --> 00:09:54,874
it is able to forecast the given value.

151
00:09:54,992 --> 00:09:58,810
Yeah, that's all from my side. Now Anya will tell

152
00:09:58,880 --> 00:10:01,998
a little bit more, even much more than

153
00:10:02,164 --> 00:10:05,630
my introductionary part, how to use

154
00:10:06,842 --> 00:10:09,982
forecasting method on the example of

155
00:10:10,036 --> 00:10:13,770
the NBITs and compare NBITs with the statistical

156
00:10:13,850 --> 00:10:17,474
methods. And also we'll show the toy example how

157
00:10:17,512 --> 00:10:20,530
to use the reinforcement learning hello.

158
00:10:20,600 --> 00:10:24,014
Today I would like to show you what can be done with financial time series

159
00:10:24,062 --> 00:10:27,494
data, stockpresses and Python. We'll go

160
00:10:27,532 --> 00:10:31,026
through the whole process from downloading data to Python

161
00:10:31,138 --> 00:10:34,290
EdA. It is explanatory data analysis,

162
00:10:34,370 --> 00:10:38,182
time series data, preprocessing modeling for time series forecasting.

163
00:10:38,246 --> 00:10:41,478
And finally we will try create an automatic

164
00:10:41,574 --> 00:10:44,966
investment strategy. So how can we access stock

165
00:10:44,998 --> 00:10:48,822
price data from Python? There exists many rich API,

166
00:10:48,966 --> 00:10:52,326
but for just learning purposes we will use Python

167
00:10:52,358 --> 00:10:55,722
package y finance based on Apache software

168
00:10:55,786 --> 00:10:59,358
license for Yahoo Finance historical data. Thanks to

169
00:10:59,364 --> 00:11:03,194
this package we can easily download daily stock prices

170
00:11:03,322 --> 00:11:06,654
data with columns such as open, high, low,

171
00:11:06,772 --> 00:11:08,510
close volume, etc.

172
00:11:09,590 --> 00:11:13,282
We just need to pass company index. Here I have some

173
00:11:13,336 --> 00:11:16,790
saved example company indices and we will run

174
00:11:16,860 --> 00:11:21,170
one. And here we can see the output with our downloaded

175
00:11:21,250 --> 00:11:25,266
data frame for visualization and indicators

176
00:11:25,298 --> 00:11:29,530
calculation. We don't need to implement everything from scratch because

177
00:11:29,600 --> 00:11:33,466
Python has many libraries which might do the work for us.

178
00:11:33,568 --> 00:11:36,922
For example, I will use Quantstat library which

179
00:11:36,976 --> 00:11:40,866
has around, I think 20 implemented visualizations

180
00:11:40,918 --> 00:11:45,450
and functions useful for stock price analysis.

181
00:11:45,610 --> 00:11:48,830
Here are some of the plots drawn with this library.

182
00:11:50,210 --> 00:11:54,426
So after the general data analysis we will move cto the data preparation.

183
00:11:54,538 --> 00:11:57,854
So the step before the modeling part, because the topic

184
00:11:57,902 --> 00:12:01,778
of this conference is Python, not machine learning, and maybe some of

185
00:12:01,784 --> 00:12:05,106
you are not familiar with data science, I will not discuss with

186
00:12:05,128 --> 00:12:09,074
the details what is the purpose of some of the specific activities.

187
00:12:09,202 --> 00:12:13,206
I will just say that it will help to choose the models and

188
00:12:13,308 --> 00:12:16,642
hyperparameters range during the further steps.

189
00:12:16,786 --> 00:12:20,810
What can be done during the time series analysis and data preprocessing?

190
00:12:21,230 --> 00:12:24,326
Usually we do the stuff like seasonal decomposition

191
00:12:24,438 --> 00:12:28,214
to relate the trend or season from the data. Time series

192
00:12:28,262 --> 00:12:31,374
transformation might be crucial for some of the

193
00:12:31,412 --> 00:12:34,654
statistical models. Outlier detection helps to

194
00:12:34,692 --> 00:12:38,046
understand the data. Most of those tasks can be

195
00:12:38,068 --> 00:12:42,094
done with stats models package, which provides classes and functions for

196
00:12:42,132 --> 00:12:45,518
the estimation of many different statistical models.

197
00:12:45,614 --> 00:12:49,678
There is belief that r is much better, at least for the classical

198
00:12:49,774 --> 00:12:53,790
time series analysis, than Python. But this task model package,

199
00:12:53,870 --> 00:12:57,574
at least in my opinion well, substitutes r and even

200
00:12:57,612 --> 00:13:02,194
has a similar API. And here we have some example transformations

201
00:13:02,322 --> 00:13:06,146
which are often applied when we are dealing with stock prices.

202
00:13:06,258 --> 00:13:10,726
Some of them are extremely easy like lock, but some of them, like Kalman Filter,

203
00:13:10,838 --> 00:13:14,102
are complex and harder to implement from scratch.

204
00:13:14,246 --> 00:13:18,154
Fortunately in Python we have also package for that.

205
00:13:18,272 --> 00:13:22,300
Maybe I will show you this Kalman filter because it's interesting,

206
00:13:22,670 --> 00:13:25,870
it has very nice smoothing features.

207
00:13:29,570 --> 00:13:32,986
And why do we need data transformation?

208
00:13:33,098 --> 00:13:36,974
There is many reasons. Some of the models require special inputs.

209
00:13:37,102 --> 00:13:40,402
Sometimes it helps also with numerical issues. Okay,

210
00:13:40,456 --> 00:13:44,590
so we have very briefly went through examples of data preprocessing

211
00:13:44,750 --> 00:13:48,374
which might be applied to time series data. And we can

212
00:13:48,412 --> 00:13:51,682
now move to more interesting part the modeling.

213
00:13:51,826 --> 00:13:55,574
So our goal will be prediction of closed stock price.

214
00:13:55,692 --> 00:13:59,414
We will use daily and weekly frequency, usually stock

215
00:13:59,462 --> 00:14:02,394
prices. Series with these frequencies are very,

216
00:14:02,432 --> 00:14:06,342
very noisy and very hard to predict. Currently efficient

217
00:14:06,406 --> 00:14:10,118
models use more data than univariate series

218
00:14:10,214 --> 00:14:13,022
from single stock can provide. For example,

219
00:14:13,156 --> 00:14:17,374
recently tech data like financial news or tweets are

220
00:14:17,412 --> 00:14:21,370
taken also as model input for stock price prediction.

221
00:14:21,530 --> 00:14:25,034
But today we just want to show very simple

222
00:14:25,092 --> 00:14:28,722
examples. So the first step would be division of our

223
00:14:28,776 --> 00:14:32,626
closed time series into three sets, train valve and

224
00:14:32,648 --> 00:14:36,462
test. Those familiar with machine learning will know what the purpose

225
00:14:36,526 --> 00:14:39,910
of this division is, but for those who are not on the train

226
00:14:39,980 --> 00:14:43,574
set, we will train. Our model validation set is

227
00:14:43,692 --> 00:14:46,610
for choosing the best hyperparameters,

228
00:14:46,770 --> 00:14:50,346
and on the test set we can see how our model

229
00:14:50,528 --> 00:14:53,194
performs on unseen data.

230
00:14:53,392 --> 00:14:57,062
It's a good practice to firstly try with benchmark

231
00:14:57,126 --> 00:15:00,426
models before starting to train complex state of

232
00:15:00,448 --> 00:15:04,022
the art. We have three benchmarks, name prediction

233
00:15:04,166 --> 00:15:07,182
where we are predicting always the last seen value,

234
00:15:07,316 --> 00:15:11,498
and two classic statistical models, exponential smoothing

235
00:15:11,594 --> 00:15:15,710
and Ariba. Both of them came from stat model package.

236
00:15:15,790 --> 00:15:19,182
So after training the baselines, we can choose more complete

237
00:15:19,246 --> 00:15:23,154
model. Our choice will be NBIT deep learning neural networks which

238
00:15:23,192 --> 00:15:26,386
outperformed all models on famous, prestigious and

239
00:15:26,408 --> 00:15:31,094
four time series competition where the task was prediction on 100,000

240
00:15:31,212 --> 00:15:35,330
time series from different fields including finances.

241
00:15:35,490 --> 00:15:38,806
There exists a few NBIS implementation, but we

242
00:15:38,828 --> 00:15:42,470
will use one from Pytorch forecasting as it's very well developed

243
00:15:42,550 --> 00:15:46,070
and based on very convenient pytorch lighting library.

244
00:15:46,230 --> 00:15:49,834
So after the training, choosing the best hyperparameters and

245
00:15:49,872 --> 00:15:54,026
evaluation of all of our models, we have time for model comparison

246
00:15:54,138 --> 00:15:57,482
in the data frame here we can see calculated metrics.

247
00:15:57,546 --> 00:16:01,422
The best methods are highlighted with green color first. Metrics such as

248
00:16:01,476 --> 00:16:04,510
mean absolutes error or mean squared error

249
00:16:04,590 --> 00:16:07,954
indicate how much values predicted by our model are close.

250
00:16:07,992 --> 00:16:12,430
CTo the reals as you can see, naive method achieves surprisingly

251
00:16:12,510 --> 00:16:16,210
good scores. However, from investor point of view,

252
00:16:16,360 --> 00:16:19,814
its prediction is worthless. It does not give us any

253
00:16:19,852 --> 00:16:23,382
information whether we can actually earn something. It's a very

254
00:16:23,436 --> 00:16:26,210
common mistake. During the stock price prediction,

255
00:16:26,370 --> 00:16:29,346
people look at metrics like mean absolute error,

256
00:16:29,458 --> 00:16:32,646
plot presented values and everything looks fine at

257
00:16:32,668 --> 00:16:35,882
the first sight. But it turns out that their

258
00:16:35,936 --> 00:16:39,130
model is not better than net prediction and

259
00:16:39,200 --> 00:16:42,426
that the predicted values are just shifted.

260
00:16:42,538 --> 00:16:46,250
To overcome that issue, we have to calculate how accurately

261
00:16:46,330 --> 00:16:50,014
model predicts prices, ups and downs. Results are

262
00:16:50,052 --> 00:16:53,566
placed in the last column. Okay, so all

263
00:16:53,588 --> 00:16:57,346
of these results presented here are only example numbers which

264
00:16:57,448 --> 00:17:00,978
differ a lot across stocks. Sometimes they are better,

265
00:17:01,064 --> 00:17:05,518
sometimes they are worse. If we decide that results are promising,

266
00:17:05,614 --> 00:17:09,414
we can try CTO, trust our predictions and build an

267
00:17:09,452 --> 00:17:12,994
investment strategy with larger and more accurate

268
00:17:13,042 --> 00:17:17,014
predictions horizons, our investment strategy will became more and

269
00:17:17,052 --> 00:17:20,438
more complete. Can we have model which will plan everything

270
00:17:20,524 --> 00:17:24,314
for us? Yes, we can. So we will go to the last

271
00:17:24,352 --> 00:17:27,674
step, creating a fully automatic investment strategy with

272
00:17:27,712 --> 00:17:31,626
reinforcement learning. For those who are not familiar with reinforcement learning,

273
00:17:31,728 --> 00:17:35,134
you probably heard about super powerful AI models which

274
00:17:35,172 --> 00:17:38,970
are able to became masters of go. I think that's the most popular

275
00:17:39,050 --> 00:17:42,814
example of usage of reinforcement learning. Or maybe

276
00:17:42,852 --> 00:17:46,602
you heard about models which learned how to play Atari games.

277
00:17:46,746 --> 00:17:50,322
Reinforcement learning is an area of machine learning concerns with

278
00:17:50,376 --> 00:17:54,466
how intelligent agents also take actions in environment in

279
00:17:54,488 --> 00:17:58,126
order to maximize the notion of cumulative reward. So it's like showing

280
00:17:58,158 --> 00:18:01,858
model a game show that it should maximize game score

281
00:18:01,954 --> 00:18:06,034
and using just on that it will learn how to win. Stock resembles

282
00:18:06,082 --> 00:18:10,138
game where profit is the reward. It's a very hard game with

283
00:18:10,224 --> 00:18:14,666
huge randomness factor, but still we can try use

284
00:18:14,768 --> 00:18:17,786
reinforcement learning for that. So first we need to

285
00:18:17,808 --> 00:18:21,506
have an environment. There is several already implemented stock

286
00:18:21,558 --> 00:18:25,818
simulation environments in Python like for example Finerl.

287
00:18:25,914 --> 00:18:29,534
Most of them is based on OpenAI gym library and

288
00:18:29,572 --> 00:18:32,590
we can use them, but we can also create our own

289
00:18:32,660 --> 00:18:36,354
environment. So I just want to emphasize that we will just

290
00:18:36,392 --> 00:18:39,474
make it as simple as possible, just for fun and

291
00:18:39,512 --> 00:18:42,338
in real scenarios to actually work well.

292
00:18:42,424 --> 00:18:45,778
Our solution should be more complete. Take more data,

293
00:18:45,944 --> 00:18:49,762
more training epochs, more agents, more complicated

294
00:18:49,826 --> 00:18:53,474
policies. But just for today we'll build something extremely

295
00:18:53,522 --> 00:18:57,142
simple. We'll use OpenAI gym library and

296
00:18:57,196 --> 00:19:00,746
single stock data will be used. Observation space will

297
00:19:00,768 --> 00:19:04,742
be just a vector with past and values. Currently owned shares

298
00:19:04,806 --> 00:19:08,682
plus currently owned money plus time for end plus

299
00:19:08,736 --> 00:19:12,554
predictions and predictions step ahead as predictions

300
00:19:12,602 --> 00:19:16,494
are created with models from the previous steps and our

301
00:19:16,532 --> 00:19:20,346
reward would be money plus owned shares multiplied

302
00:19:20,378 --> 00:19:23,546
by current stock price. And you have two possible actions,

303
00:19:23,658 --> 00:19:27,234
buying and selling shares in percentages. Each day

304
00:19:27,272 --> 00:19:30,990
we can sell up to 100% of owned shares

305
00:19:31,070 --> 00:19:34,802
and we can buy maximum free shares. Our game

306
00:19:34,856 --> 00:19:38,914
will always end after 20 days. In reality it's extremely

307
00:19:38,962 --> 00:19:42,022
short period for investment, but we'll leave it short

308
00:19:42,076 --> 00:19:46,066
because of the training time. We'll use PPO algorithms

309
00:19:46,098 --> 00:19:49,402
from stable baseline free and in stable baseline free

310
00:19:49,456 --> 00:19:52,874
we can add our own Pytorch models which might be good

311
00:19:52,912 --> 00:19:56,906
for further development. And for neural networks we

312
00:19:56,928 --> 00:20:00,406
will use the simplest option, multilayer perceptrons.

313
00:20:00,518 --> 00:20:04,362
And now I can show you the code. And the environment

314
00:20:04,426 --> 00:20:07,338
is longer, so it's probably less understable.

315
00:20:07,514 --> 00:20:11,198
The most important functions in environment class

316
00:20:11,284 --> 00:20:15,298
are this obligatory from gym amp like

317
00:20:15,384 --> 00:20:17,220
step in step function.

318
00:20:19,670 --> 00:20:22,126
Each day we take action,

319
00:20:22,238 --> 00:20:25,714
calculate the reward, take the next observation from

320
00:20:25,752 --> 00:20:29,240
the environment, and check whether we are not done.

321
00:20:30,410 --> 00:20:34,374
And here we have training and evaluation codes which

322
00:20:34,412 --> 00:20:37,782
are shorter. And these codes also should

323
00:20:37,836 --> 00:20:41,570
not be different if we use another environment.

324
00:20:41,730 --> 00:20:46,938
We evaluate our agent on unseen data last 365

325
00:20:47,024 --> 00:20:50,906
days for one year from our time series, and we are

326
00:20:50,928 --> 00:20:54,800
running our model 365 times

327
00:20:55,570 --> 00:20:58,954
each time with different starting day. And the biggest

328
00:20:59,002 --> 00:21:02,606
question is whether our investment strategy work. Does it have

329
00:21:02,628 --> 00:21:05,730
a potential? Firstly, we can check whether we are

330
00:21:05,800 --> 00:21:09,874
earning anything, or maybe we are just losing money. After that

331
00:21:09,912 --> 00:21:13,886
we should compare our method with other strategies. There is many methods

332
00:21:13,918 --> 00:21:17,554
for backtesting investment strategy. There are even nice package

333
00:21:17,602 --> 00:21:21,590
for that in Python. But for now we will just test two

334
00:21:21,660 --> 00:21:25,826
very simple strategies. Buy and hold and nest strategy.

335
00:21:26,018 --> 00:21:29,734
Nest strategy relies on buying if the

336
00:21:29,772 --> 00:21:32,858
last n values are higher than the current

337
00:21:32,944 --> 00:21:36,166
price, and sell whether the last n values

338
00:21:36,278 --> 00:21:39,846
are lower than today's price, with the same limit

339
00:21:39,878 --> 00:21:42,750
for buying actions as in our environment.

340
00:21:43,490 --> 00:21:47,514
Okay, so here are the results from the four consecutive

341
00:21:47,562 --> 00:21:51,520
runs with four different randomly selected companies.

342
00:21:51,890 --> 00:21:56,066
And in real scenarios here

343
00:21:56,088 --> 00:22:00,002
we have profit in dollars, but the better way is to

344
00:22:00,136 --> 00:22:02,530
calculate it in percentage.

345
00:22:03,190 --> 00:22:06,750
And as we can see, our reinforcement learning agent

346
00:22:06,840 --> 00:22:09,160
is not the worst. Of course,

347
00:22:10,410 --> 00:22:13,846
these plots here, these results cannot be treated as

348
00:22:13,948 --> 00:22:18,678
a proof or anything because it's hard to think about fair

349
00:22:18,844 --> 00:22:22,546
conditions for comparisons of these strategies.

350
00:22:22,658 --> 00:22:26,886
But it looks like there might be potential in

351
00:22:26,908 --> 00:22:30,546
this method. It seems that our model actually learned

352
00:22:30,578 --> 00:22:33,934
something. Okay, and that's all for

353
00:22:33,972 --> 00:22:35,770
today. Thank you for your attention.

