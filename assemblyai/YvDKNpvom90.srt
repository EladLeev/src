1
00:02:14,850 --> 00:02:18,094
Hello everybody, today I will talk

2
00:02:18,132 --> 00:02:21,854
you about Kubernetes monitoring, why it is difficult and

3
00:02:21,892 --> 00:02:25,594
how to improve it. Let's meet I'm

4
00:02:25,642 --> 00:02:29,374
Aliaksandr Valialkin and our telemetrics founder and

5
00:02:29,412 --> 00:02:33,118
code developer. I also known as

6
00:02:33,284 --> 00:02:37,874
Go contributor and our author of popular libraries

7
00:02:38,002 --> 00:02:41,826
for Go such as Fast RTP, fast cache

8
00:02:41,858 --> 00:02:45,350
and Fixing plate. As you can see that

9
00:02:45,420 --> 00:02:48,526
libraries start from fast and quick prefixes.

10
00:02:48,578 --> 00:02:51,340
This means that these libraries are quite fast.

11
00:02:52,670 --> 00:02:55,770
So I'm fond of performance optimizations.

12
00:02:58,430 --> 00:03:02,750
What is Victoria metrics this

13
00:03:02,820 --> 00:03:05,230
time? Services, database and monitoring solution.

14
00:03:06,370 --> 00:03:10,430
It is open source, it is simple to set up and operate,

15
00:03:11,170 --> 00:03:15,170
it is cost efficient, highly scalable and

16
00:03:15,240 --> 00:03:18,754
it is cloud ready. We provide helm charts and

17
00:03:18,872 --> 00:03:22,130
parameters for victory metrics

18
00:03:22,470 --> 00:03:29,176
for kubernetes according

19
00:03:29,208 --> 00:03:32,984
to recent surveys. From this survey,

20
00:03:33,032 --> 00:03:36,204
for instance, you can see that the

21
00:03:36,242 --> 00:03:39,724
amounts of monitoring data increases two

22
00:03:39,762 --> 00:03:44,210
three times faster than the amounts of actual application data

23
00:03:44,820 --> 00:03:48,050
and this not so good.

24
00:03:49,700 --> 00:03:53,588
For instance, some people

25
00:03:53,754 --> 00:03:57,076
twitters also monetized it and

26
00:03:57,178 --> 00:04:01,990
say that these not so good because

27
00:04:03,800 --> 00:04:08,036
these costs for storing monitoring data increases

28
00:04:08,228 --> 00:04:11,736
much faster comparing to cost for storing application

29
00:04:11,838 --> 00:04:12,490
data.

30
00:04:15,340 --> 00:04:19,180
According to the recent CNCF survey,

31
00:04:19,920 --> 00:04:24,712
many users of kubernetes struggle

32
00:04:24,776 --> 00:04:28,760
with the complexity and monitoring

33
00:04:28,840 --> 00:04:32,304
issues. As you can see, 27% of

34
00:04:32,502 --> 00:04:36,224
these users don't like the state of

35
00:04:36,262 --> 00:04:39,280
monitoring in kubernetes.

36
00:04:42,290 --> 00:04:45,410
So why kubernetes monitoring is so challenging?

37
00:04:49,500 --> 00:04:53,512
The first thing is that kubernetes exposes big

38
00:04:53,566 --> 00:04:56,520
amounts of metrics on itself.

39
00:04:56,670 --> 00:05:01,964
You can look at this link and see how

40
00:05:02,002 --> 00:05:06,108
many kubernetes companions expose huge

41
00:05:06,194 --> 00:05:09,688
amounts of metrics and

42
00:05:09,874 --> 00:05:13,520
the number of this exposed metrics grows over

43
00:05:13,590 --> 00:05:17,324
time. Let's look at these graph this graph

44
00:05:17,372 --> 00:05:21,644
shows that the number of unique metric

45
00:05:21,692 --> 00:05:25,684
names which are exposed by Kubernetes companions has

46
00:05:25,722 --> 00:05:29,604
been grown from 150 in

47
00:05:29,722 --> 00:05:33,380
2018 in Kubernetes version one point

48
00:05:33,450 --> 00:05:37,060
ten to more than 500 in Kubernetes

49
00:05:37,140 --> 00:05:40,868
1.24 which has been released

50
00:05:40,964 --> 00:05:41,880
recently.

51
00:05:45,420 --> 00:05:48,764
The number of metrics unique metrics which

52
00:05:48,802 --> 00:05:52,076
are exposed by applications grow not

53
00:05:52,098 --> 00:05:55,884
only in Kubernetes services, by in

54
00:05:55,922 --> 00:06:01,504
any application, for instance, not the export services component which

55
00:06:01,542 --> 00:06:05,104
is usually used in kubernetes and for

56
00:06:05,142 --> 00:06:09,532
monitoring hardware also increases

57
00:06:09,596 --> 00:06:13,584
the number of unique metrics. For instance, the number of metrics in

58
00:06:13,622 --> 00:06:17,990
node exporter increased from 100 to more than 400

59
00:06:18,600 --> 00:06:20,630
in the last five years.

60
00:06:23,960 --> 00:06:27,590
Every kubernetes node exports at least

61
00:06:29,880 --> 00:06:33,956
2500 series

62
00:06:34,068 --> 00:06:37,544
and this doesn't count the

63
00:06:37,582 --> 00:06:41,796
number of application metrics. This metrics

64
00:06:41,828 --> 00:06:45,816
includes nodexperter's metrics, kubernetes and these advisor

65
00:06:45,848 --> 00:06:48,972
metrics. And according to our

66
00:06:49,026 --> 00:06:52,940
study, we see that these average

67
00:06:53,020 --> 00:06:57,164
number of such metrics per each node Kubernetes

68
00:06:57,212 --> 00:07:00,290
node is around 4000.

69
00:07:05,390 --> 00:07:09,340
So if you have 1000 pods then

70
00:07:10,850 --> 00:07:14,878
your kubernetes cluster will expose 4

71
00:07:14,964 --> 00:07:17,790
million metrics which should be collected.

72
00:07:21,670 --> 00:07:25,810
What is these source of such big amounts of metrics?

73
00:07:26,230 --> 00:07:30,046
This is because of multilayer

74
00:07:30,238 --> 00:07:32,870
architecture of modern systems.

75
00:07:34,250 --> 00:07:38,274
Let's look at this picture. You see, you can see that hardware

76
00:07:38,322 --> 00:07:41,794
server contains virtual machine

77
00:07:41,922 --> 00:07:45,414
and each virtual machine contains pods and each pods

78
00:07:45,462 --> 00:07:48,966
contains applications and each application contains

79
00:07:48,998 --> 00:07:53,322
container. And all these levels must

80
00:07:53,376 --> 00:07:57,150
have some observability. This means that

81
00:07:57,220 --> 00:07:59,790
they need to expose some metrics.

82
00:08:02,570 --> 00:08:06,130
And if you have multiple containers in kubernetes,

83
00:08:06,210 --> 00:08:09,834
multiple pods in kubernetes, then the number of

84
00:08:09,952 --> 00:08:13,914
exposed metrics increases with

85
00:08:13,952 --> 00:08:17,290
these numbers of pods and containers.

86
00:08:19,230 --> 00:08:22,958
Let's look at the simple example. When you

87
00:08:23,044 --> 00:08:27,002
deploy Nginx these leprechauns

88
00:08:27,066 --> 00:08:30,590
Genix in kubernetes, they already

89
00:08:30,660 --> 00:08:34,350
generate more than 600 new time series

90
00:08:35,750 --> 00:08:39,294
according to advisor. And these metrics

91
00:08:39,342 --> 00:08:42,926
don't count application metrics. These means that metrics

92
00:08:42,958 --> 00:08:45,140
which are exposed by Nginx itself.

93
00:08:47,450 --> 00:08:51,174
Another issue with kubernetes monitoring is time

94
00:08:51,212 --> 00:08:54,418
series charm when old series

95
00:08:54,514 --> 00:08:58,466
are substituted by new ones. And monitoring solutions

96
00:08:58,498 --> 00:09:02,700
don't like high charm rate because it leads to

97
00:09:03,150 --> 00:09:07,862
memory issues, memory usage issues and cpu

98
00:09:07,926 --> 00:09:08,940
time issues.

99
00:09:13,470 --> 00:09:17,054
Kubernetes tends to generate high churn rate

100
00:09:17,092 --> 00:09:20,702
for active time series became of two things. The first

101
00:09:20,756 --> 00:09:24,630
thing is frequent deployments. When you deploy

102
00:09:24,810 --> 00:09:28,482
new deployment then

103
00:09:28,616 --> 00:09:32,530
a new set of metrics for these deployments are generated

104
00:09:33,270 --> 00:09:36,830
because every such metric usually

105
00:09:37,000 --> 00:09:40,902
contain pods metric and this pod metric is

106
00:09:40,956 --> 00:09:43,670
usually generated automatically by kubernetes.

107
00:09:45,370 --> 00:09:48,662
And another source of high churn rate is

108
00:09:48,796 --> 00:09:55,046
pods autoscale events. When pods

109
00:09:55,238 --> 00:09:58,790
scale then new pod names appear

110
00:09:58,870 --> 00:10:02,338
and metrics for these new pods

111
00:10:02,454 --> 00:10:06,782
should be registered in monitoring system and this generates high

112
00:10:06,836 --> 00:10:13,580
churn rate and

113
00:10:13,650 --> 00:10:17,584
the number of new metrics which are generated with

114
00:10:17,622 --> 00:10:21,344
each deployment or portal scale event can

115
00:10:21,382 --> 00:10:25,596
be estimated as the number of container

116
00:10:25,708 --> 00:10:29,244
start metrics for this application. For each instance

117
00:10:29,292 --> 00:10:33,140
of this application plus the number of

118
00:10:33,210 --> 00:10:36,644
application metrics and this number

119
00:10:36,762 --> 00:10:41,364
should be multiplied by the number of replicas for this

120
00:10:41,562 --> 00:10:44,680
deployment and these number of deployments.

121
00:10:47,100 --> 00:10:51,704
And as you can see if these

122
00:10:51,742 --> 00:10:54,984
churn rate increases with the number of deployment and the number

123
00:10:55,022 --> 00:10:59,340
of replicas, do we need all these metrics?

124
00:10:59,920 --> 00:11:04,604
The answer is not so easy. Like you see some

125
00:11:04,642 --> 00:11:08,592
people say that no we don't need all these metrics because

126
00:11:08,646 --> 00:11:11,760
our monitoring systems uses only a small

127
00:11:11,830 --> 00:11:14,480
fraction of the collected metrics.

128
00:11:15,300 --> 00:11:19,008
But others say yes we need this

129
00:11:19,094 --> 00:11:22,836
collecting all these metrics because these metrics can be

130
00:11:22,858 --> 00:11:24,230
used in the future.

131
00:11:28,280 --> 00:11:33,224
How to determine the exact set of needed metrics there

132
00:11:33,262 --> 00:11:37,508
is a mimir tool from Grafana which scans

133
00:11:37,604 --> 00:11:41,560
your recording and alerts and rules, and also scans

134
00:11:41,900 --> 00:11:45,980
your dashboard queries and decides

135
00:11:46,480 --> 00:11:49,832
which metrics are used and which metrics aren't

136
00:11:49,896 --> 00:11:54,060
used and then it can generate

137
00:11:54,400 --> 00:11:57,588
hollow list for user metrics.

138
00:11:57,784 --> 00:12:01,072
And for instance, Grafana says

139
00:12:01,126 --> 00:12:04,620
that if you have kubernetes cluster

140
00:12:04,780 --> 00:12:10,000
with three pods, this cluster exposes

141
00:12:10,520 --> 00:12:14,180
40,000 active time series by default.

142
00:12:14,680 --> 00:12:18,660
And if you run memir tool and apply all

143
00:12:18,730 --> 00:12:22,484
lists to labeling rules, this always

144
00:12:22,602 --> 00:12:26,968
reduces the number of active time series from 40,000 to 8000.

145
00:12:27,134 --> 00:12:30,250
This means more than five times less.

146
00:12:35,340 --> 00:12:38,644
So what does it mean? It means that existing solution

147
00:12:38,692 --> 00:12:42,332
slots like kubernetes prometheus stack collect too many

148
00:12:42,386 --> 00:12:46,460
metrics and most of these are unused.

149
00:12:48,980 --> 00:12:52,480
This chart shows that only

150
00:12:52,630 --> 00:12:55,868
24% of collected metrics

151
00:12:56,044 --> 00:13:00,428
from Kube Prometheus tech are actually used by alerts

152
00:13:00,444 --> 00:13:05,604
and recording rules and these boards and 75%

153
00:13:05,722 --> 00:13:09,430
of metrics never used by current

154
00:13:09,880 --> 00:13:11,700
monitoring solutions.

155
00:13:13,500 --> 00:13:18,410
This means that you can reduce your spend

156
00:13:18,860 --> 00:13:23,530
expenses on monitoring solutions by 76%.

157
00:13:24,380 --> 00:13:28,270
That's more than five four times.

158
00:13:32,560 --> 00:13:35,000
Let's talk about monitoring standards.

159
00:13:35,160 --> 00:13:38,976
Unfortunately, there is no established standards for

160
00:13:38,998 --> 00:13:40,690
metrics at the moment.

161
00:13:42,660 --> 00:13:46,444
Community and different companies try to invent

162
00:13:46,572 --> 00:13:50,124
own standard and promote them. For instance,

163
00:13:50,252 --> 00:13:53,584
Google promotes four golden

164
00:13:53,632 --> 00:13:57,472
signal standard, Brendan Greg

165
00:13:57,616 --> 00:14:01,696
promotes used standard for monitoring and weave

166
00:14:01,728 --> 00:14:06,388
works promotes red standard but

167
00:14:06,474 --> 00:14:09,684
so many different standards fits to the following

168
00:14:09,732 --> 00:14:13,880
situation that nobody follows

169
00:14:14,540 --> 00:14:17,976
a single standard and everybody follows different standards

170
00:14:18,008 --> 00:14:21,292
or doesn't follow any standard. This leads to

171
00:14:21,346 --> 00:14:26,620
big amounts of metrics in every application and

172
00:14:26,770 --> 00:14:29,730
these metrics changed over time.

173
00:14:32,910 --> 00:14:37,210
And you can read many articles and opinions about

174
00:14:37,360 --> 00:14:40,810
most essential metrics and there is no

175
00:14:40,880 --> 00:14:44,990
single source of truth for monitoring.

176
00:14:47,490 --> 00:14:50,922
This also leads to outdated dashboards

177
00:14:50,986 --> 00:14:54,830
for kubernetes Grafana for instance,

178
00:14:55,270 --> 00:14:59,010
the most popular dashboards in Grafana are now outdated.

179
00:15:00,070 --> 00:15:03,970
Grafana and

180
00:15:04,040 --> 00:15:08,290
Kubernetes provokes

181
00:15:08,370 --> 00:15:12,120
you to generate to use

182
00:15:12,490 --> 00:15:16,402
microservices architecture and microservices

183
00:15:16,466 --> 00:15:20,070
architecture has some challenges.

184
00:15:20,910 --> 00:15:24,170
Every microservice instance needs to own

185
00:15:24,240 --> 00:15:28,006
metrics. The users need to track and correlate events

186
00:15:28,038 --> 00:15:29,580
across multiple services.

187
00:15:31,630 --> 00:15:35,450
FML services also makes

188
00:15:35,520 --> 00:15:38,954
situation worse. FMLC means that

189
00:15:39,072 --> 00:15:42,878
every mega service can be started,

190
00:15:42,964 --> 00:15:49,378
redeployed, stopped at any time and

191
00:15:49,544 --> 00:15:53,410
because of this situation, new entities like distributed traces

192
00:15:54,150 --> 00:15:58,322
needs to be invented and used in founder CTO improve

193
00:15:58,466 --> 00:16:04,630
the observability station for microservice microservice

194
00:16:04,970 --> 00:16:08,870
talk to each other via network

195
00:16:09,210 --> 00:16:13,286
so you need to improve networking to

196
00:16:13,308 --> 00:16:16,966
monitor networking service

197
00:16:17,068 --> 00:16:21,062
allocation on one, not create a nosy neighbor

198
00:16:21,126 --> 00:16:24,670
problem. And this problem also needs to be resolved

199
00:16:25,730 --> 00:16:29,546
and service mesh introduces yet another layer

200
00:16:29,578 --> 00:16:32,830
of complexity which needs to be monitored.

201
00:16:37,320 --> 00:16:41,236
How kubernetes affects the monitoring as

202
00:16:41,258 --> 00:16:45,104
you can see from previous slides, kubernetes increases complexity

203
00:16:45,152 --> 00:16:48,932
and metrics footprint current

204
00:16:48,986 --> 00:16:52,596
monitoring solutions such as parameters, victory metrics,

205
00:16:52,788 --> 00:16:56,804
tunnels, cortex are busy with overcoming

206
00:16:56,852 --> 00:16:59,320
complexities introduced by kubernetes.

207
00:17:01,100 --> 00:17:04,492
These most complexities are active time

208
00:17:04,546 --> 00:17:08,712
series churn rate which are generated

209
00:17:08,776 --> 00:17:12,684
from the

210
00:17:12,722 --> 00:17:16,450
service and huge volumes of metrics for each layer. And service

211
00:17:19,300 --> 00:17:22,972
developers of current monitoring solutions

212
00:17:23,036 --> 00:17:28,100
spent big amounts of efforts

213
00:17:28,440 --> 00:17:32,304
for adopting these monitoring tools

214
00:17:32,352 --> 00:17:34,710
for kubernetes. Because of this,

215
00:17:36,040 --> 00:17:39,964
maybe if there was no kubernetes we won't

216
00:17:40,032 --> 00:17:43,908
need distributed traces and examplers because distributed

217
00:17:43,924 --> 00:17:48,372
traces and examplers are used only solely for microservices

218
00:17:48,436 --> 00:17:52,600
and kubernetes. And maybe if there was no kubernetes

219
00:17:53,360 --> 00:17:57,192
all this time on overcoming difficulties

220
00:17:57,336 --> 00:18:01,372
in current monitoring solution could be invested into more

221
00:18:01,506 --> 00:18:05,060
useful observability tools such as automated protocols

222
00:18:05,080 --> 00:18:10,224
analysis or metrics correlation who

223
00:18:10,262 --> 00:18:13,504
knows how

224
00:18:13,542 --> 00:18:16,560
Kubernetes deals with millions of metrics?

225
00:18:17,080 --> 00:18:20,932
These answer is that kubernetes doesn't deal, doesn't provide

226
00:18:20,986 --> 00:18:25,540
good solutions. It provides only two flux which

227
00:18:25,610 --> 00:18:28,528
can be used for blacklisting,

228
00:18:28,624 --> 00:18:33,544
dissolving some metrics and other

229
00:18:33,662 --> 00:18:37,816
label values. That's not so good

230
00:18:37,998 --> 00:18:41,396
solution. How does Prometheus

231
00:18:41,588 --> 00:18:43,900
deals with Kubernetes challenges?

232
00:18:45,200 --> 00:18:49,070
Actually Prometheus version two

233
00:18:49,840 --> 00:18:53,772
has been created because of

234
00:18:53,906 --> 00:18:57,356
kubernetes because it needs to solve

235
00:18:57,468 --> 00:19:00,720
kubernetes challenges with high number of

236
00:19:00,790 --> 00:19:04,784
time series and high churn rate. You can read the

237
00:19:04,822 --> 00:19:08,390
announcement of Prometheus version two

238
00:19:09,400 --> 00:19:13,568
in order to understand how they redesigned

239
00:19:13,744 --> 00:19:17,780
internal architecture of Prometheus solely for solving

240
00:19:18,200 --> 00:19:19,670
Kubernetes issues.

241
00:19:21,820 --> 00:19:25,636
But still Kubernetes issues such as high churn

242
00:19:25,668 --> 00:19:29,544
rate and cardinality aren't solved in Prometheus and other

243
00:19:29,582 --> 00:19:34,728
metronic solution. Actually Victoria

244
00:19:34,744 --> 00:19:38,648
metrics also deals kubernetes changed. Actually Victoria

245
00:19:38,664 --> 00:19:42,764
metrics has been appeared as

246
00:19:42,802 --> 00:19:47,250
a system which solves cardinality issues in Prometheus version one.

247
00:19:48,020 --> 00:19:52,050
It is optimized for using lower amounts of memory in this space

248
00:19:52,740 --> 00:19:55,680
when working with high card analysis series.

249
00:19:56,420 --> 00:20:00,260
It also provides optimizations

250
00:20:00,920 --> 00:20:04,724
to overcome time series charm which is

251
00:20:04,922 --> 00:20:10,292
common in Kubernetes and

252
00:20:10,346 --> 00:20:12,470
Victoria metrics. Also,

253
00:20:13,800 --> 00:20:17,876
we at Victoria Matrix also don't know how to reduce

254
00:20:17,908 --> 00:20:22,144
the number of time series and Victoria. New versions

255
00:20:22,212 --> 00:20:25,852
of Victoriametrics increased the number of exported time series over

256
00:20:25,906 --> 00:20:30,344
time. You can see that the number of new metric

257
00:20:30,392 --> 00:20:34,808
names which are exposed by Victoria metrics growth around

258
00:20:34,914 --> 00:20:38,210
three times during the last four years

259
00:20:40,880 --> 00:20:44,312
and only 30% of these metrics

260
00:20:44,376 --> 00:20:48,140
are actually used by Victoria metrics,

261
00:20:49,300 --> 00:20:53,200
dashboards and alerts and records and rules.

262
00:20:54,820 --> 00:20:58,272
How can we improve the situation? We believe

263
00:20:58,326 --> 00:21:02,420
that kubernetes monitoring complexity must be reduced

264
00:21:03,160 --> 00:21:06,564
and the number of exposed metrics must be

265
00:21:06,602 --> 00:21:09,956
reduced. The number of histograms must be

266
00:21:09,978 --> 00:21:13,656
reduced because the histogram is the

267
00:21:13,678 --> 00:21:18,120
biggest offender of cardinal which

268
00:21:18,190 --> 00:21:21,480
generates many new time series,

269
00:21:24,460 --> 00:21:27,500
the number of parametric labels must be reduced.

270
00:21:29,520 --> 00:21:34,590
For instance, in kubernetes it is common practice to put

271
00:21:35,440 --> 00:21:39,116
all the labels which are defined at pod level to all

272
00:21:39,138 --> 00:21:42,610
the metrics exposed by this pod and

273
00:21:43,220 --> 00:21:47,184
probably this not correct. We should change these

274
00:21:47,222 --> 00:21:50,444
situation. Time series

275
00:21:50,492 --> 00:21:54,996
churn rate must be reduced the most common time

276
00:21:55,018 --> 00:21:58,832
series churn rate source in kubernetes

277
00:21:58,896 --> 00:22:02,320
is horizontal port auto scaling and deployments.

278
00:22:02,480 --> 00:22:05,972
And we should think hard how to reduce

279
00:22:06,036 --> 00:22:08,600
churn rate for these sources.

280
00:22:10,380 --> 00:22:14,484
And we believe that community will come up with a standard for kubernetes

281
00:22:14,532 --> 00:22:18,632
monitoring which will be much

282
00:22:18,766 --> 00:22:22,360
lightweighter and will need to collect a much lower

283
00:22:22,430 --> 00:22:26,040
number of metrics compared to the current state of

284
00:22:26,110 --> 00:22:29,810
monitoring of kubernetes. So let's do it together.

285
00:22:32,660 --> 00:22:34,610
Now you can ask questions.

