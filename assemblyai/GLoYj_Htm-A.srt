1
00:00:34,930 --> 00:00:38,402
Hello and welcome everyone. My name is Barkatul Mujauddin

2
00:00:38,466 --> 00:00:42,034
and I'm excited to be here today to talk about API

3
00:00:42,082 --> 00:00:46,038
testing made easy. I hope you are all having a great day so far

4
00:00:46,124 --> 00:00:49,846
and are looking forward to learning something new. Thank you for taking the

5
00:00:49,868 --> 00:00:53,754
time me to join me today. So here's a quick introduction.

6
00:00:53,802 --> 00:00:57,566
About me I am a developer relation intern at Keploy. I am

7
00:00:57,588 --> 00:01:01,246
a GSOC 2023 mentor this year only and

8
00:01:01,348 --> 00:01:04,878
I am also the founder of a community called Hack for Code.

9
00:01:04,964 --> 00:01:08,610
I am also a kubesimplifier ambassador which I'm really proud of.

10
00:01:08,760 --> 00:01:11,938
I believe that there are many problems

11
00:01:12,024 --> 00:01:15,874
in developers job and the root of maximum problems are

12
00:01:15,992 --> 00:01:19,926
strict timelines. Let me tell you how with an example.

13
00:01:20,108 --> 00:01:23,446
Yes, you guessed it right, the example is related to

14
00:01:23,468 --> 00:01:26,674
API testing. So here's a situation. Due to a strict

15
00:01:26,722 --> 00:01:30,118
timeline, we may have limited time to perform testing

16
00:01:30,294 --> 00:01:33,914
which can lead to regressions. Right? Again, this is

17
00:01:33,952 --> 00:01:37,722
a problem. So as we all know, for every

18
00:01:37,776 --> 00:01:42,066
problem there is a solution. All we need to reduce

19
00:01:42,118 --> 00:01:45,486
the bugs are just three things. Number one, we need

20
00:01:45,508 --> 00:01:48,766
to do the functional testing. We need a tool that

21
00:01:48,868 --> 00:01:53,166
could create the test cases and update the test cases easily

22
00:01:53,198 --> 00:01:57,346
for us so that we don't have to really write the

23
00:01:57,368 --> 00:02:01,630
test cases and spend time on it. We need something that could automatically

24
00:02:01,710 --> 00:02:05,874
orchestrate the testing infrastructure. So that's

25
00:02:05,922 --> 00:02:09,174
all we need. Now let's look

26
00:02:09,212 --> 00:02:12,486
at what if we can record the

27
00:02:12,508 --> 00:02:16,006
traffic from production and we can replay it in a

28
00:02:16,028 --> 00:02:19,862
non production environment. I know this is kind of something overwhelming

29
00:02:19,926 --> 00:02:23,834
and out of context, so let me explain it to you.

30
00:02:24,032 --> 00:02:27,450
So here's the thing. Think it in this way.

31
00:02:27,520 --> 00:02:31,646
This is an application serving the user traffic. Let's say application v one.

32
00:02:31,748 --> 00:02:35,294
And first we captured the user traffic from

33
00:02:35,332 --> 00:02:38,766
production. Then we replayed it after setting up

34
00:02:38,788 --> 00:02:42,446
the shadow database of the snapshot database. And then we

35
00:02:42,468 --> 00:02:46,174
need to write to compare the responses. Obviously this requires

36
00:02:46,222 --> 00:02:50,530
a huge operational effort because we first write to record,

37
00:02:50,680 --> 00:02:54,574
then we need to set up those snapshot database which are expensive,

38
00:02:54,702 --> 00:02:58,306
and then we need to update these snapshots timely.

39
00:02:58,418 --> 00:03:01,574
And then we need to write to compare the responses as well.

40
00:03:01,612 --> 00:03:04,482
So it is a pretty wise strategy,

41
00:03:04,626 --> 00:03:07,946
but it's expensive and it's included a lot of

42
00:03:07,968 --> 00:03:11,546
effort so far in

43
00:03:11,568 --> 00:03:14,646
this approach. The record and replay approach.

44
00:03:14,838 --> 00:03:18,506
Let's discuss some of the pros and cons we are

45
00:03:18,528 --> 00:03:22,302
discussing about the pros. The first one is it is a low code

46
00:03:22,356 --> 00:03:26,350
approach and you don't really have to write the test cases.

47
00:03:26,690 --> 00:03:30,606
It is easy to achieve high coverage because you

48
00:03:30,628 --> 00:03:33,766
are capturing the user traffic, so there are multiple

49
00:03:33,818 --> 00:03:38,030
flows that you are capturing and will be able to replay,

50
00:03:38,190 --> 00:03:41,394
which increases the coverage of the code base that

51
00:03:41,432 --> 00:03:44,610
your application or your API test are going through.

52
00:03:44,760 --> 00:03:48,998
The third one is sometimes there are unexpected user flows that

53
00:03:49,084 --> 00:03:52,534
you discover just via the real user traffic that

54
00:03:52,572 --> 00:03:56,194
the developer never did code for, but they discovered

55
00:03:56,242 --> 00:03:59,866
it from the real world. If we are

56
00:03:59,888 --> 00:04:03,370
discussing about the cons, then the first cons

57
00:04:03,440 --> 00:04:07,018
are the dependency states are hard to manage.

58
00:04:07,184 --> 00:04:10,774
This approach is good for load testing or stress testing,

59
00:04:10,822 --> 00:04:14,014
but not suitable for functional testing because if

60
00:04:14,052 --> 00:04:17,626
some of the APIs fail, there are multiple user traffic

61
00:04:17,658 --> 00:04:21,454
calls that fail and you have to go to each of

62
00:04:21,492 --> 00:04:25,178
the call and debug it. So that takes a lot of

63
00:04:25,204 --> 00:04:28,466
effort and a lot of time to debug and it

64
00:04:28,488 --> 00:04:32,162
causes a lot of frustration as well. The third cons are

65
00:04:32,296 --> 00:04:35,418
handling writes is always tricky in this approach.

66
00:04:35,614 --> 00:04:39,842
So let's come back. We were doing a record keploy

67
00:04:39,986 --> 00:04:43,686
via snapshot database from a production environment to

68
00:04:43,708 --> 00:04:47,330
a non production environment. So let's think it in this way.

69
00:04:47,500 --> 00:04:51,450
If you are capturing and replaying the API request and response,

70
00:04:51,870 --> 00:04:55,574
how about we also do the same for the database

71
00:04:55,622 --> 00:04:58,954
queries? So what if we

72
00:04:58,992 --> 00:05:02,694
create a virtual database which is just the database

73
00:05:02,742 --> 00:05:06,750
query request and response and not a complete database?

74
00:05:09,570 --> 00:05:13,106
I'll show you a deep dive with an example later in this talk. But in

75
00:05:13,128 --> 00:05:16,414
this approach also, there are some downsides.

76
00:05:16,462 --> 00:05:20,100
Like you have to add support

77
00:05:22,550 --> 00:05:26,150
for each of the dependencies, right, that your application is talking

78
00:05:26,220 --> 00:05:30,630
to. For example, if your application is talking to MongoDB,

79
00:05:31,050 --> 00:05:35,218
then you need to add support mocks the MongoDB queries

80
00:05:35,314 --> 00:05:39,222
or to capture

81
00:05:39,286 --> 00:05:43,446
and replay that. This becomes brittle in case your API

82
00:05:43,478 --> 00:05:46,806
schema completely changes and you have to rerecord

83
00:05:46,838 --> 00:05:50,126
the user traffic and replay it. So these are

84
00:05:50,148 --> 00:05:53,294
the downsides. But the upsides are the

85
00:05:53,412 --> 00:05:57,338
complete database did not need to be replicated

86
00:05:57,434 --> 00:06:01,486
to some other environment, or neither. It is expensive because

87
00:06:01,588 --> 00:06:04,640
we were storing just the query data.

88
00:06:09,590 --> 00:06:13,058
So what we just discussed, we are going to

89
00:06:13,144 --> 00:06:16,646
basically virtualize that. So to

90
00:06:16,668 --> 00:06:20,360
give an example, let's say in an application,

91
00:06:20,890 --> 00:06:24,790
if we request an API with a particular user,

92
00:06:25,130 --> 00:06:28,678
it returns what sports that particular user plays.

93
00:06:28,854 --> 00:06:32,266
So the user has Thompson and we got the response as

94
00:06:32,368 --> 00:06:35,450
cricket, volleyball, karam and boxing.

95
00:06:36,190 --> 00:06:39,482
So in this case we have the application talking

96
00:06:39,536 --> 00:06:43,200
to MongoDB, which has all of the relevant data.

97
00:06:44,130 --> 00:06:48,014
Typically if I am going to record replay into

98
00:06:48,052 --> 00:06:51,546
a different environment, I would capture

99
00:06:51,578 --> 00:06:54,590
the request, run it again in my test environment,

100
00:06:54,670 --> 00:06:58,260
and this time maybe user Thompson isn't there.

101
00:06:59,030 --> 00:07:02,260
What I mean to say is it's not the same state.

102
00:07:03,110 --> 00:07:07,070
So the problem is how do we ensure

103
00:07:07,230 --> 00:07:11,234
that the exact state is consistent

104
00:07:11,362 --> 00:07:13,510
with the test case that we captured.

105
00:07:14,410 --> 00:07:17,400
Now in the same example,

106
00:07:18,510 --> 00:07:22,138
if once we do dependency mocking, we can instead

107
00:07:22,224 --> 00:07:25,946
of maintaining the test database, we instead

108
00:07:26,048 --> 00:07:29,274
kind of maintain a mock. So when we

109
00:07:29,312 --> 00:07:32,794
capture the Getgame request, we capture the queries

110
00:07:32,842 --> 00:07:36,382
and the response that we got from MongoDB and just

111
00:07:36,436 --> 00:07:40,062
package it along the test case. So now

112
00:07:40,116 --> 00:07:43,586
when the same thing happens while we are

113
00:07:43,608 --> 00:07:47,794
testing, we can just return the same response that

114
00:07:47,832 --> 00:07:51,726
we got for the particular request. So it's basically mocking,

115
00:07:51,838 --> 00:07:55,442
but actually kind of replaying that exact database

116
00:07:55,506 --> 00:07:59,160
response that we captured and then of course

117
00:07:59,770 --> 00:08:01,430
things will be consistent.

118
00:08:02,810 --> 00:08:07,030
So as of now, we have

119
00:08:07,180 --> 00:08:11,354
discussed the problems we were facing right now as a

120
00:08:11,392 --> 00:08:15,206
developer and how record, replay and dependency approach can solve

121
00:08:15,238 --> 00:08:18,554
these problems. So what

122
00:08:18,592 --> 00:08:22,234
basically Kipployoy does is keploy converts your API cases

123
00:08:22,282 --> 00:08:24,670
to test cases automatically.

124
00:08:25,730 --> 00:08:29,934
All you just need to integrate keploy in your application and

125
00:08:30,132 --> 00:08:34,180
it automatically mocks external dependencies for you.

126
00:08:34,630 --> 00:08:37,986
It also detects the noise and accurately reduce it

127
00:08:38,088 --> 00:08:41,406
automatically. All of these three can be done by keploy

128
00:08:41,438 --> 00:08:45,220
easily. How? Let me show you a demo.

129
00:08:47,210 --> 00:08:51,062
Let me show you a demo of how keploy can

130
00:08:51,196 --> 00:08:55,074
generate test cases as well as data mocks for your backend

131
00:08:55,122 --> 00:08:58,454
application. So in the demo I will take

132
00:08:58,492 --> 00:09:02,300
a Javasample application which is an employee management application

133
00:09:02,750 --> 00:09:06,906
and it does a crude operation on employee. So this

134
00:09:06,928 --> 00:09:10,474
is the main repository of keploy where the server lies. You can just

135
00:09:10,512 --> 00:09:14,170
simply install it. Now I'm going to follow the samples Java repository.

136
00:09:14,250 --> 00:09:17,866
So I have already installed keploy server and I'm

137
00:09:17,898 --> 00:09:22,206
going to start it. And I have also cloned the sample Java application

138
00:09:22,308 --> 00:09:25,666
here. Now to integrate the Keploy Java SDK with

139
00:09:25,688 --> 00:09:29,010
my application I need to follow certain steps.

140
00:09:29,350 --> 00:09:33,486
First I need to add the keploy dependency. So I have added

141
00:09:33,518 --> 00:09:36,870
it here. Then I need to also import

142
00:09:36,940 --> 00:09:41,400
the keploy middleware with my main class. I have done that here and

143
00:09:42,090 --> 00:09:46,054
I also need to have the agent jar. So how

144
00:09:46,092 --> 00:09:49,510
you can go to it is to going to maven central

145
00:09:50,090 --> 00:09:53,290
download the latest version of this agent jar and

146
00:09:53,360 --> 00:09:55,740
put it in the main directory of your application.

147
00:09:56,750 --> 00:09:59,660
Now the last thing I need to do is to set up this agent.

148
00:10:00,050 --> 00:10:05,178
I have just added the vm

149
00:10:05,274 --> 00:10:09,086
options as Java agent and then gave the absolute path to

150
00:10:09,108 --> 00:10:12,266
this agent jar. And I have now added

151
00:10:12,298 --> 00:10:15,758
the environment variable which is keploy

152
00:10:15,774 --> 00:10:19,602
mode record because I'm going to make

153
00:10:19,656 --> 00:10:23,362
API cases and record

154
00:10:23,416 --> 00:10:27,250
the test cases. And denoise is true, it just means that

155
00:10:27,320 --> 00:10:31,426
it is going to ignore all the random fields like timestamps or random

156
00:10:31,458 --> 00:10:34,886
numbers which are going to change in every API call. Now let's go

157
00:10:34,908 --> 00:10:38,950
ahead and run our postcase instance first.

158
00:10:39,100 --> 00:10:41,910
So I'm just going to run the postgres instance.

159
00:10:42,070 --> 00:10:45,578
It is running now and I'm going to start the sample application.

160
00:10:45,744 --> 00:10:48,970
Now let's just go ahead and make an API call.

161
00:10:49,120 --> 00:10:52,334
So I have this post API call which adds an

162
00:10:52,372 --> 00:10:55,706
employee record. Now here are the test cases and mocks

163
00:10:55,738 --> 00:10:58,526
generated. Yeah,

164
00:10:58,628 --> 00:11:02,414
so in the test case I have the request and

165
00:11:02,452 --> 00:11:06,254
response header and also I have the mocks for the Postgres

166
00:11:06,302 --> 00:11:10,114
instance. Now let's make another API call

167
00:11:10,152 --> 00:11:13,874
to get the user that we put on employee id one and there

168
00:11:13,912 --> 00:11:18,562
was another test case generated. The interesting part is that since Postgres

169
00:11:18,626 --> 00:11:21,734
is a SQL based database, it captured this whole

170
00:11:21,772 --> 00:11:25,794
query and you can generate the test cases and mock file mock

171
00:11:25,842 --> 00:11:29,194
like that. Now let's go ahead and simply run the

172
00:11:29,232 --> 00:11:33,142
test cases that we captured. So first we need to set the environment

173
00:11:33,206 --> 00:11:36,220
variable of keploy mode to test.

174
00:11:38,190 --> 00:11:41,834
Yeah, now let's run the sample

175
00:11:41,882 --> 00:11:45,258
Java application. Yeah, you can see on the keploy

176
00:11:45,274 --> 00:11:49,070
server that both the test case is run and passed.

177
00:11:49,570 --> 00:11:53,194
Yeah. Now let's introduce

178
00:11:53,242 --> 00:11:57,394
an error. Let's say we have a field named changed which

179
00:11:57,432 --> 00:12:01,022
is being queried. So instead of first name it will query

180
00:12:01,086 --> 00:12:05,278
name from the postgres instance. Now let's stop the postgres instance.

181
00:12:05,374 --> 00:12:09,334
Even without the postgres it is going to run because we

182
00:12:09,372 --> 00:12:12,530
already have the mocks. So we didn't need postgres.

183
00:12:12,690 --> 00:12:16,706
So yeah, it has run and one test case is passed and one failed.

184
00:12:16,818 --> 00:12:20,234
Let's go into detail. So it is why? Because the

185
00:12:20,272 --> 00:12:23,574
first name is now different. It was not able to query

186
00:12:23,622 --> 00:12:27,034
the name parameter that we changed and thus the first

187
00:12:27,072 --> 00:12:30,906
name was null. So this is how you can capture

188
00:12:30,938 --> 00:12:34,602
the test cases quickly, run those and achieve

189
00:12:34,746 --> 00:12:38,730
good line coverage with keploy. I hope you have understood

190
00:12:38,810 --> 00:12:40,640
how keploy works.

191
00:12:43,970 --> 00:12:47,434
So I hope I made it clear how you can integrate

192
00:12:47,482 --> 00:12:51,314
keploy in your application and how it

193
00:12:51,352 --> 00:12:54,866
works, how it can generate test cases and data mocks for

194
00:12:54,888 --> 00:12:58,758
you automatically. So thank you and that's basically about it.

195
00:12:58,844 --> 00:13:03,414
And you can also find us on GitHub and

196
00:13:03,452 --> 00:13:07,270
if you want you can star us. Also, we don't mind and

197
00:13:07,420 --> 00:13:11,014
feel free to join our slack channel. We would love to have all of you

198
00:13:11,052 --> 00:13:14,822
in our community. And thanks

199
00:13:14,876 --> 00:13:17,846
again and thanks for joining today.

200
00:13:18,028 --> 00:13:21,054
And feel free to reach to me if you have any

201
00:13:21,092 --> 00:13:22,974
questions. Till then,

