1
00:00:39,090 --> 00:00:43,218
All right, I'm really excited today to share a little bit about Fairwind's

2
00:00:43,234 --> 00:00:47,206
Kubernetes benchmark report for 2024. We're really

3
00:00:47,308 --> 00:00:50,526
kind of proud of some of the work that our team has put together over

4
00:00:50,548 --> 00:00:53,982
the past few weeks, getting this report ready for the new year,

5
00:00:54,116 --> 00:00:58,330
and excited to kind of share some of the findings that we observed

6
00:00:58,490 --> 00:01:01,230
by studying over 330,000 workloads.

7
00:01:01,650 --> 00:01:05,266
So just as a quick introduction, my name is Joe Pelletier. I'm a

8
00:01:05,288 --> 00:01:08,798
product manager here at Fairwinds. Been in the Kubernetes

9
00:01:08,894 --> 00:01:12,500
and container space for a little over five years

10
00:01:12,870 --> 00:01:15,960
and been working on the Fairwinds Insights product.

11
00:01:17,130 --> 00:01:20,582
Like I mentioned, we actually have done this report now

12
00:01:20,636 --> 00:01:25,202
for, this is the third year, and we've analyzed

13
00:01:25,266 --> 00:01:28,710
over 330,000 workloads to inform the data

14
00:01:28,780 --> 00:01:32,074
behind this report. It's the largest number of workloads we've ever

15
00:01:32,112 --> 00:01:35,354
analyzed for the report, and it includes more than

16
00:01:35,392 --> 00:01:38,890
a dozen different types of policies covering kind of reliability,

17
00:01:39,550 --> 00:01:42,010
security, as well as cost efficiency.

18
00:01:42,530 --> 00:01:46,094
And when it comes to Kubernetes, we find that organizations really have

19
00:01:46,132 --> 00:01:50,186
to consider all three types of checks

20
00:01:50,218 --> 00:01:53,614
here in order to make sure whats they are running workloads that align with best

21
00:01:53,652 --> 00:01:57,186
practices. This gives you a little bit of an example of the

22
00:01:57,208 --> 00:02:00,962
types of policies we've evaluated. I won't go through every single one,

23
00:02:01,016 --> 00:02:04,530
but we'll be covering some of these in today's presentation.

24
00:02:05,510 --> 00:02:08,686
The final report actually covers a number

25
00:02:08,728 --> 00:02:12,594
of different categories of policies and provides a lot more depth

26
00:02:12,642 --> 00:02:16,374
than what we'll be able to cover in today's session. So I do recommend that

27
00:02:16,412 --> 00:02:19,906
you download the report. I think what you'll see in today's presentation

28
00:02:19,938 --> 00:02:23,498
is sort of a summary and a bridge version, and hopefully kind of

29
00:02:23,664 --> 00:02:27,322
see a little bit of where the industry is going in terms of both

30
00:02:27,376 --> 00:02:31,146
kind of Kubernetes best practices and sort of how well organizations are aligning to

31
00:02:31,168 --> 00:02:35,086
those best practices as well. Okay, so a common graph that

32
00:02:35,108 --> 00:02:39,358
you're going to see in this report will look like this. And what

33
00:02:39,444 --> 00:02:42,574
I'll do is spend a little bit of time just explaining how to read the

34
00:02:42,612 --> 00:02:45,586
report and how to read the different charts and graphs that you're seeing.

35
00:02:45,768 --> 00:02:49,394
So on the y axis here, you'll see sort of the percentage of

36
00:02:49,432 --> 00:02:52,834
workloads impacted within an organization. And on the

37
00:02:52,872 --> 00:02:56,246
x axis, you'll see the percentage of organizations that

38
00:02:56,268 --> 00:02:59,366
were evaluated in terms of how many

39
00:02:59,388 --> 00:03:03,814
of their workloads were actually impacted as

40
00:03:03,852 --> 00:03:07,062
a percentage. And so a great way to kind of read

41
00:03:07,116 --> 00:03:09,480
this report or this example here is,

42
00:03:09,930 --> 00:03:13,766
if you take this example, the number of organizations with less than 10% of workloads

43
00:03:13,798 --> 00:03:17,274
improved has fallen from 46% in

44
00:03:17,312 --> 00:03:20,218
2022% to 21% in 2023.

45
00:03:20,224 --> 00:03:23,230
And so when you see something like that,

46
00:03:23,300 --> 00:03:26,986
where the number of organizations that have such a small percentage

47
00:03:27,018 --> 00:03:30,574
of workloads impacted decrease, it actually demonstrates that the problem

48
00:03:30,612 --> 00:03:33,818
might be getting harder to control. And so that's some of the Ways that we've

49
00:03:33,834 --> 00:03:36,350
been able to kind of highlight information in this report.

50
00:03:36,500 --> 00:03:40,126
And you will see that in just kind of various different aspects of today's

51
00:03:40,158 --> 00:03:43,940
presentation when you read the final piece.

52
00:03:44,550 --> 00:03:47,654
So let's kick it off with sort of the highlight from

53
00:03:47,692 --> 00:03:50,934
this year's analysis, which is really this really

54
00:03:50,972 --> 00:03:54,582
kind of interesting fact that over one third

55
00:03:54,636 --> 00:03:58,398
of organizations today, and specifically 37% of organizations,

56
00:03:58,594 --> 00:04:03,270
need to actually right size their containers to improve efficiency.

57
00:04:03,430 --> 00:04:07,306
And when we dig into some of those details, we'll notice that

58
00:04:07,488 --> 00:04:11,434
actually this 37% of organizations have 50%

59
00:04:11,472 --> 00:04:14,122
or more of their containers that are over provisioned.

60
00:04:14,266 --> 00:04:17,434
And it's an interesting finding, because what we notice

61
00:04:17,482 --> 00:04:21,306
is that a lot of times developers have to guess their resource requests

62
00:04:21,338 --> 00:04:24,494
and limits when they go to deploy it because

63
00:04:24,532 --> 00:04:28,034
they don't really have the tooling or the feedback loops to tell them what those

64
00:04:28,072 --> 00:04:31,614
resource requests and limits should be. And a lot of times developers

65
00:04:31,662 --> 00:04:34,974
will guess high. They will over provision by giving

66
00:04:35,032 --> 00:04:38,550
their application too much memory or too much compute. And when left

67
00:04:38,620 --> 00:04:42,134
unchecked or unmonitored, organizations end up

68
00:04:42,172 --> 00:04:45,798
incurring lots of additional compute spend as a

69
00:04:45,804 --> 00:04:49,334
result. And so this is the first year

70
00:04:49,372 --> 00:04:53,306
that we actually started looking at this data. And again, you'll see

71
00:04:53,328 --> 00:04:56,682
that the 37% of organizations that need to right size

72
00:04:56,816 --> 00:05:00,154
50% or more of their containers really represents the

73
00:05:00,192 --> 00:05:03,630
bottom part of this chart.

74
00:05:04,130 --> 00:05:07,866
But it's also interesting to see that there's actually a large cohort of organizations,

75
00:05:07,898 --> 00:05:11,440
in this case 57% of organizations, that have

76
00:05:12,770 --> 00:05:17,054
less than or equal to 10% of their workloads impacted. So some organizations

77
00:05:17,102 --> 00:05:20,814
do seem to get this right. And what we're really excited

78
00:05:20,862 --> 00:05:24,946
to do is monitor this progress over

79
00:05:24,968 --> 00:05:28,166
the years. So this being our first year measuring this, we want to see

80
00:05:28,188 --> 00:05:31,954
sort of how well has this trend

81
00:05:32,082 --> 00:05:35,990
improved or not improved going into next year as well. So again,

82
00:05:36,060 --> 00:05:39,434
first year we're kind of baselining this, but next year should help us understand where

83
00:05:39,472 --> 00:05:42,698
that trend is going. Another aspect of kind

84
00:05:42,704 --> 00:05:46,086
of Kubernetes efficiency that's important to solve

85
00:05:46,118 --> 00:05:49,622
is really making sure that both memory and cpu

86
00:05:49,686 --> 00:05:53,566
requests are set on deployments before they're actually set

87
00:05:53,588 --> 00:05:57,482
to running Kubernetes. Now, Kubernetes technically makes these settings

88
00:05:57,546 --> 00:06:00,954
optional, but when you don't set memory or cpu

89
00:06:01,002 --> 00:06:04,234
requests, it can actually make it difficult for Kubernetes

90
00:06:04,282 --> 00:06:07,886
to properly schedule that workload. And so what we're

91
00:06:07,918 --> 00:06:11,566
seeing is that this is becoming more and more of a systemic

92
00:06:11,598 --> 00:06:15,342
issue. This year. We're noticing that 78% of organizations

93
00:06:15,406 --> 00:06:18,946
have at least 10% of their workloads missing cpu requests,

94
00:06:18,978 --> 00:06:22,120
and this is up from about 50% last year.

95
00:06:22,650 --> 00:06:26,502
So again, I think the numbers kind of are all over the map here.

96
00:06:26,636 --> 00:06:29,990
You'll see that pretty much every organization

97
00:06:30,070 --> 00:06:34,122
has some amount of this problem. But interestingly enough,

98
00:06:34,176 --> 00:06:37,594
this is actually a fairly easy thing to solve with

99
00:06:37,632 --> 00:06:41,334
guardrails and policy enforcement mechanisms for kubernetes,

100
00:06:41,382 --> 00:06:44,814
where you can kind of give developers feedback at the time of their pull

101
00:06:44,852 --> 00:06:48,378
request or at the time of their deployment when they are missing these settings,

102
00:06:48,474 --> 00:06:51,486
and use that as an opportunity to educate developers as well.

103
00:06:51,508 --> 00:06:55,582
So we think that while a lot of organizations may be having missing

104
00:06:55,646 --> 00:06:59,122
cpu requests, we also think this could be a potentially easily solved problem

105
00:06:59,256 --> 00:07:02,754
with policy enforcement and guardrail tools as well.

106
00:07:02,952 --> 00:07:06,546
Let's shift a little bit to reliability and kind of look

107
00:07:06,568 --> 00:07:09,640
at a few different trends that we've observed in this year's report.

108
00:07:10,330 --> 00:07:13,986
So just at a very high level, about a quarter of organizations today are relying

109
00:07:14,018 --> 00:07:17,190
on a cache version for 90% of their images.

110
00:07:17,610 --> 00:07:21,274
And what does that mean? Well, it really means that the pull policy is not

111
00:07:21,312 --> 00:07:25,018
set to always, which is a general best practice. You kind

112
00:07:25,024 --> 00:07:28,310
of want to make sure that your containers are pulling

113
00:07:28,390 --> 00:07:31,158
the latest image so that you don't have inconsistency.

114
00:07:31,334 --> 00:07:34,958
And it also can help you from a security perspective as well to make sure

115
00:07:34,964 --> 00:07:38,634
whats, when you do push an image with updates,

116
00:07:38,682 --> 00:07:42,362
that it's actually pulling in that latest image as well, not just the cache version.

117
00:07:42,506 --> 00:07:45,838
So this is just a general best practice that we're

118
00:07:45,854 --> 00:07:49,438
seeing. Whats right now, about 24% of organizations

119
00:07:49,454 --> 00:07:52,914
are relying on this pull policy not being set

120
00:07:52,952 --> 00:07:55,970
to always for 90% of their images.

121
00:07:56,710 --> 00:08:00,326
Another pattern that we're seeing is that container health checks seem to be missing or

122
00:08:00,348 --> 00:08:04,182
ignored in some deployments as well. And so right now

123
00:08:04,236 --> 00:08:07,978
about 66% of liveness and 69% of

124
00:08:07,984 --> 00:08:11,514
readiness probes are missing in

125
00:08:11,552 --> 00:08:14,602
Kubernetes deployments. And it's important to set these

126
00:08:14,656 --> 00:08:17,702
because it helps Kubernetes automatically restart containers

127
00:08:17,766 --> 00:08:20,966
and ensure that the applications are available to receive traffic

128
00:08:20,998 --> 00:08:24,634
and then ultimately serve users. So this is actually considered

129
00:08:24,682 --> 00:08:28,474
one of the more basic ways to ensure application reliability

130
00:08:28,522 --> 00:08:32,074
in Kube and we're still seeing that organization struggle to various degrees

131
00:08:32,122 --> 00:08:36,114
here. I think part of it is because the

132
00:08:36,152 --> 00:08:39,870
configuration does require a little bit of application specific input.

133
00:08:40,030 --> 00:08:43,666
So development teams need to consider what are

134
00:08:43,688 --> 00:08:46,466
the changes they need to make to their application in order to make sure that

135
00:08:46,488 --> 00:08:50,038
the health checks works for Kubernetes. So we're hoping that

136
00:08:50,044 --> 00:08:53,558
this trend kind of improves over the years as well. Another trend that

137
00:08:53,564 --> 00:08:57,174
we identified is that deployments are missing replicas. This is another

138
00:08:57,212 --> 00:09:01,146
general best practice is to make sure that there's a few different, there's a couple

139
00:09:01,168 --> 00:09:04,954
of replicas available for pods and that right

140
00:09:04,992 --> 00:09:08,506
now we're noticing that 30% of organizations actually have less than 10% of their

141
00:09:08,528 --> 00:09:12,118
deployments missing replicas. So this is an

142
00:09:12,144 --> 00:09:15,406
improvement over 2023, but still kind

143
00:09:15,428 --> 00:09:18,558
of highlights whats if you look at the

144
00:09:18,564 --> 00:09:22,478
graphic here, that some organizations have much more than just 10%

145
00:09:22,564 --> 00:09:25,706
impacted. There might be lots

146
00:09:25,738 --> 00:09:29,314
of applications missing replicas. And I think sometimes this is because

147
00:09:29,352 --> 00:09:32,994
of what we call the copy and paste problem. Sometimes a deployment from one

148
00:09:33,032 --> 00:09:36,114
team that's missing this best practice gets copied from another

149
00:09:36,152 --> 00:09:39,366
team who's looking to get their application deployed. And so they may

150
00:09:39,388 --> 00:09:43,110
be propagating these misconfigurations where

151
00:09:43,260 --> 00:09:46,694
replicas aren't set on the previous team, and now the new team is using

152
00:09:46,732 --> 00:09:50,038
that same configuration without replicas. And so you can see that this becomes

153
00:09:50,054 --> 00:09:53,754
sort of a wider problem. Again, this is usually a very quick

154
00:09:53,792 --> 00:09:57,354
fix, like a one line change to your infrastructure as

155
00:09:57,392 --> 00:10:00,746
code. And we're hoping to see, even though

156
00:10:00,768 --> 00:10:04,830
we're on an improved path here, that even more and more organizations

157
00:10:05,170 --> 00:10:08,590
have fewer and fewer deployments missing these replicas.

158
00:10:08,930 --> 00:10:12,222
Shifting gears a little bit, we'll also take a look at security. Now,

159
00:10:12,276 --> 00:10:15,810
security in Kubernetes kind of can mean a lot of different things.

160
00:10:15,960 --> 00:10:19,250
We look at security from two lenses in this report.

161
00:10:19,320 --> 00:10:22,834
One is from image vulnerabilities as well as from

162
00:10:22,872 --> 00:10:26,386
the kind of the configuration itself. So the YaMl or

163
00:10:26,408 --> 00:10:29,654
the helm chart that's being deployed at a very

164
00:10:29,692 --> 00:10:33,382
high level. We're noticing that about 28% of organizations are running about

165
00:10:33,436 --> 00:10:36,854
90% of their workloads with insecure capabilities. So that means

166
00:10:36,892 --> 00:10:40,998
that they're adding some sort of insecure capability like net admin.

167
00:10:41,174 --> 00:10:44,950
And a lot of times it actually might be necessary

168
00:10:45,030 --> 00:10:49,430
for some applications or workloads to have these additional capabilities,

169
00:10:49,590 --> 00:10:53,562
but sometimes it may not be and it could be accidentally

170
00:10:53,626 --> 00:10:56,970
added to apps going back to that original copy

171
00:10:57,050 --> 00:11:00,702
and paste problem where one team copies the configuration from another team

172
00:11:00,836 --> 00:11:03,994
as a starting point and inadvertently

173
00:11:04,122 --> 00:11:06,820
propagates some of these misconfigurations going forward.

174
00:11:07,350 --> 00:11:10,850
So we always look to make sure that applications start with not having

175
00:11:10,920 --> 00:11:13,570
these dangerous or insecure capabilities added,

176
00:11:13,990 --> 00:11:17,286
and that helps ensure kind of a good baseline from a

177
00:11:17,308 --> 00:11:20,674
security perspective. One positive, though trend

178
00:11:20,722 --> 00:11:23,970
coming out of this year's report is that we're actually seeing fewer containers

179
00:11:24,050 --> 00:11:28,146
set to run as root. So 30% of organizations

180
00:11:28,178 --> 00:11:31,226
today are running 70% or more of their

181
00:11:31,248 --> 00:11:34,858
containers as root, which is actually a drop from 44%

182
00:11:35,024 --> 00:11:38,394
in last year's report. And part of me thinks that

183
00:11:38,432 --> 00:11:41,982
this is another example of sort of a low hanging opportunity

184
00:11:42,116 --> 00:11:45,438
to make a quick win,

185
00:11:45,524 --> 00:11:49,066
a quick fix to containers by essentially turning

186
00:11:49,178 --> 00:11:52,834
off the ability to run as root, which again, is a one line

187
00:11:52,872 --> 00:11:56,018
change. And I think we also see that this example,

188
00:11:56,104 --> 00:12:00,130
this type of misconfiguration example is sort of very popular

189
00:12:00,630 --> 00:12:04,814
when talking about the issues of misconfigurations

190
00:12:04,862 --> 00:12:08,406
of Kubernetes. A lot of organizations talk about, as an example, running as

191
00:12:08,428 --> 00:12:11,254
root being a common example of that.

192
00:12:11,372 --> 00:12:14,854
So it's great to see that this trend is going

193
00:12:14,892 --> 00:12:19,494
in the right direction in that fewer and fewer organizations have

194
00:12:19,612 --> 00:12:23,034
a vast majority of their containers running as root, and that seems to be going

195
00:12:23,072 --> 00:12:26,842
in decline, which is awesome. And I think it's important to note that

196
00:12:26,896 --> 00:12:30,454
running a container as root, just overall, it increases the risk of a malicious user

197
00:12:30,502 --> 00:12:33,866
taking advantage of that root privilege as part of a larger

198
00:12:33,898 --> 00:12:36,990
attack. So you want to kind of, from a defense in depth perspective,

199
00:12:38,210 --> 00:12:41,934
by default, have your container not run as root unless it absolutely needs

200
00:12:41,972 --> 00:12:45,534
to because of some special need or use case for

201
00:12:45,572 --> 00:12:49,442
that app. So again, this is going in the right direction, and we hope it

202
00:12:49,576 --> 00:12:53,406
stays that way going forward as well. Switching gears

203
00:12:53,438 --> 00:12:56,866
a little bit away from kind of misconfigurations, we'll talk about image

204
00:12:56,898 --> 00:13:00,754
vulnerabilities. And so this is the image vulnerabilities

205
00:13:00,802 --> 00:13:05,234
that may exist in running containers or as part of scanning container

206
00:13:05,282 --> 00:13:09,240
images, as part of your CI CD process or your shift left process.

207
00:13:09,930 --> 00:13:13,078
And I think this is an ongoing challenge for many organizations.

208
00:13:13,174 --> 00:13:16,886
It's an ongoing problem. But we do see some signs of progress in this year's

209
00:13:16,918 --> 00:13:20,218
report. So if we actually dig into

210
00:13:20,384 --> 00:13:24,110
the first section where we show the percentage of workloads impacted,

211
00:13:24,530 --> 00:13:28,286
26% of organizations have less than 10% of their

212
00:13:28,308 --> 00:13:32,350
workloads affected, which is an improvement from 12%

213
00:13:32,420 --> 00:13:36,174
in 2023. So we're seeing essentially a greater percentage of organizations

214
00:13:36,222 --> 00:13:40,238
with fewer workloads impacted due to image vulnerabilities.

215
00:13:40,334 --> 00:13:44,574
And I think that is a signal of both kind of organizations upgrading

216
00:13:44,622 --> 00:13:48,654
their third party containers to newer, less vulnerable versions, but also

217
00:13:48,712 --> 00:13:52,470
integrating and scanning more of their containers so that they have

218
00:13:52,540 --> 00:13:55,830
a process in place for this. In the report,

219
00:13:55,900 --> 00:13:58,946
you're also going to see a section where we talked about scanned images.

220
00:13:59,058 --> 00:14:02,410
So Fairwinds is able to kind of help companies

221
00:14:02,480 --> 00:14:05,814
identify if there's images running in their cluster that they have not scanned.

222
00:14:05,942 --> 00:14:09,754
And this has greatly improved over the year.

223
00:14:09,792 --> 00:14:12,954
We're actually seeing almost 84% of organizations getting

224
00:14:12,992 --> 00:14:16,446
almost complete scan coverage of containers in their runtime. That's up

225
00:14:16,468 --> 00:14:19,966
from 64% last year. So I think that's a

226
00:14:19,988 --> 00:14:23,486
great sign that organizations are kind of doing the first step, which is

227
00:14:23,668 --> 00:14:27,122
scanned as many of their images as possible so that they understand

228
00:14:27,176 --> 00:14:30,260
their risk and then taking remediation after.

229
00:14:31,190 --> 00:14:35,178
You know, we hope that next year we even see a higher percentage of organizations

230
00:14:35,214 --> 00:14:39,158
with fewer workloads affected. One of the enhancements that

231
00:14:39,164 --> 00:14:42,726
we made to Fairwinds insights last year was we added some specific

232
00:14:42,828 --> 00:14:46,226
checks related to the NSA hardening

233
00:14:46,258 --> 00:14:50,066
guide. So the NSA actually released Kubernetes hardening

234
00:14:50,098 --> 00:14:53,786
guidance, I think, back in 2021, and there

235
00:14:53,808 --> 00:14:57,766
was a number of great recommendations there, and we actually expanded the number of checks

236
00:14:57,798 --> 00:15:01,974
that Fairwinds insights offers to match what the recommendations

237
00:15:02,022 --> 00:15:05,870
were in the NSA hardening guide. So a lot of new security

238
00:15:05,940 --> 00:15:09,280
checks kind of made its way into the Fairwinds insights platform this year.

239
00:15:09,730 --> 00:15:14,414
One of those checks is actually verifying if there's a network policy configured

240
00:15:14,542 --> 00:15:18,194
for workloads. And network policies are

241
00:15:18,232 --> 00:15:21,780
increasingly important because it helps you kind of segment workload traffic and

242
00:15:22,870 --> 00:15:26,434
ensure that you've got controls around which pods can speak to which

243
00:15:26,472 --> 00:15:29,570
pods. And so we want to get a sense of how is the industry

244
00:15:29,730 --> 00:15:32,440
doing on this particular policy.

245
00:15:33,290 --> 00:15:36,866
And so I think we see kind of two types

246
00:15:36,898 --> 00:15:40,246
of organizations, 37%, or about a third of

247
00:15:40,268 --> 00:15:44,246
organizations today, have less than 10% of their workloads without a network

248
00:15:44,278 --> 00:15:47,978
policy. And that's actually a great sign that there's a lot

249
00:15:47,984 --> 00:15:51,146
of network policy adoption happening in some organizations where

250
00:15:51,248 --> 00:15:54,878
they're making sure that their workloads have a network policy

251
00:15:54,964 --> 00:15:58,206
set. But on the other hand, there's still a majority of

252
00:15:58,228 --> 00:16:01,998
organizations that have way more than 50% of their workloads without

253
00:16:02,084 --> 00:16:05,658
a network policy. So it means that they're deploying the Kubernetes.

254
00:16:05,754 --> 00:16:09,186
The workload is running fine, but that workload can

255
00:16:09,208 --> 00:16:12,754
speak to any other workload in the cluster. And so I think it shows that

256
00:16:12,792 --> 00:16:15,554
the industry still has a little bit of ways to go to make sure that

257
00:16:15,672 --> 00:16:19,750
network policy adoption is even more widespread and more adopted.

258
00:16:21,130 --> 00:16:23,446
And so just to kind of give a little bit of an example of why

259
00:16:23,468 --> 00:16:26,646
we think this is important, network policies help you

260
00:16:26,668 --> 00:16:30,394
limit that egress and ingress traffic. And so when

261
00:16:30,432 --> 00:16:32,986
you have that ability to control the traffic, it allows you to kind of,

262
00:16:33,008 --> 00:16:37,180
again, from a defensive depth perspective, prevent any undesirable access

263
00:16:38,510 --> 00:16:42,426
to those pods. So those are some of the summaries and

264
00:16:42,448 --> 00:16:45,854
the highlights from the report. Again, I think it's probably only, we're only

265
00:16:45,892 --> 00:16:49,054
covering about a quarter of the information that the report has this year,

266
00:16:49,172 --> 00:16:52,590
but I wanted to kind of also help organizations understand what is a path forward.

267
00:16:52,660 --> 00:16:56,514
Like, if you're running lots of kubernetes today, how do you ensure that

268
00:16:56,552 --> 00:17:00,254
your teams are following reliable security and cost efficient

269
00:17:00,302 --> 00:17:04,530
best practices? And I think that's really where Fairwinds insights can

270
00:17:04,680 --> 00:17:08,662
provide a lot of value. It can provide guardrails to help you solve your business

271
00:17:08,716 --> 00:17:12,626
problems. Whether it's ensuring that your images

272
00:17:12,658 --> 00:17:17,126
are free of vulnerabilities or that your

273
00:17:17,148 --> 00:17:21,026
workloads are aligned to standards like the NSA hardening guide, or aligned

274
00:17:21,058 --> 00:17:24,026
to standards like SoC two or ISO 27,001,

275
00:17:24,128 --> 00:17:28,278
there's a big security reason to provide developers with guardrails

276
00:17:28,294 --> 00:17:31,530
and feedback around their configuration hygiene.

277
00:17:32,030 --> 00:17:36,506
I think increasingly in 2023 we did notice whats a lot of organizations

278
00:17:36,538 --> 00:17:39,982
were very cost conscious. So they wanted to make sure that they had

279
00:17:40,036 --> 00:17:43,570
a way to measure their container usage, but also right size

280
00:17:43,640 --> 00:17:47,106
containers to properly make sure that it's using the

281
00:17:47,128 --> 00:17:50,542
correct memory and cpu and they're not overspending

282
00:17:50,606 --> 00:17:55,150
in ways whats incurs additional cost or just wastes compute resources.

283
00:17:55,310 --> 00:17:59,282
So Fairwinds does provide sort of both Kubernetes cost allocation

284
00:17:59,346 --> 00:18:03,026
as well as container right sizing recommendations. And that's

285
00:18:03,058 --> 00:18:06,374
helped organizations in some cases save over 25% on their

286
00:18:06,412 --> 00:18:10,246
container costs. And then finally, this notion of guardrails is

287
00:18:10,268 --> 00:18:13,818
sort of core to everything that we do. So in order to make

288
00:18:13,824 --> 00:18:17,462
sure that engineers have the tools to take action on this feedback,

289
00:18:17,606 --> 00:18:21,514
you want to be able to provide guardrails at different steps in the process,

290
00:18:21,712 --> 00:18:25,806
whether it's at time of pull request, when they're making their infrastructure as code changes,

291
00:18:25,988 --> 00:18:29,770
or at the time of deployment, also known as the time of admission,

292
00:18:29,930 --> 00:18:33,498
when applications are being deployed into the Kubernetes environment.

293
00:18:33,594 --> 00:18:36,946
You want to give that feedback to developers and have both like a way

294
00:18:36,968 --> 00:18:40,626
for them to remediate things easily, but also ensure consistency so

295
00:18:40,648 --> 00:18:44,834
that you're not introducing risk or over provisioned applications along

296
00:18:44,872 --> 00:18:48,618
the way. And these are kind of the core capabilities that Fairwinds insights provides

297
00:18:48,654 --> 00:18:52,166
and how our customers are getting value. So I do encourage you

298
00:18:52,188 --> 00:18:55,986
to kind of take a look at the Kubernetes configuration benchmark

299
00:18:56,018 --> 00:18:59,206
report for this year. Like I said, we only really covered about a quarter of

300
00:18:59,228 --> 00:19:03,366
what's in that report, and there's a lot more broken out by security,

301
00:19:03,548 --> 00:19:07,026
cost and reliability, so you can kind of see the different patterns.

302
00:19:07,218 --> 00:19:11,214
So if you're interested, I'd recommend going to reach

303
00:19:11,252 --> 00:19:14,266
out to me on LinkedIn. I'm happy to point you in the right direction,

304
00:19:14,458 --> 00:19:17,902
and I think that's really kind of what we're hoping to cover

305
00:19:17,956 --> 00:19:21,946
today. So thanks again for the time and looking forward to hearing

306
00:19:21,978 --> 00:19:23,500
your thoughts out there in the community.

