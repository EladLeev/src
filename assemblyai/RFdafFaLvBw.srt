1
00:01:28,770 --> 00:01:32,470
Welcome to Cloud native Apache Pulsar Development 101

2
00:01:32,540 --> 00:01:33,670
with Python.

3
00:01:36,410 --> 00:01:39,478
So this is Pulsar for Python people.

4
00:01:39,644 --> 00:01:42,350
And but what is Apache Pulsar?

5
00:01:43,330 --> 00:01:47,114
How do I code against in Python? Three do I consume messages?

6
00:01:47,162 --> 00:01:51,146
How do I produce them? How do I interact via other protocols

7
00:01:51,178 --> 00:01:54,798
like MQTT, websockets, and Kafka? How do

8
00:01:54,804 --> 00:01:58,866
I use Python for making functions? And what

9
00:01:58,888 --> 00:02:02,354
are schemas? How do I use them and why do I care? I'm Tim

10
00:02:02,392 --> 00:02:05,720
Spann. I'm a developer advocate at Stream native. Like mentioned before,

11
00:02:06,570 --> 00:02:10,790
I'm interested in the Flippinstack flink pulsar Nifi,

12
00:02:11,130 --> 00:02:14,946
working together in an ecosystem of lots of cool open sources

13
00:02:14,978 --> 00:02:18,362
projects. I've been doing this for a number of years in the big data

14
00:02:18,416 --> 00:02:21,782
and cloud space. I work at stream Native,

15
00:02:21,926 --> 00:02:25,366
founded by the original developers of Pulsar,

16
00:02:25,558 --> 00:02:29,270
and we're out there to help people with Pulsar

17
00:02:29,350 --> 00:02:32,862
get you into a cloud environment that

18
00:02:32,916 --> 00:02:36,906
scales and is managed for you. I run the Flipstack

19
00:02:36,938 --> 00:02:40,400
weekly weekly newsletter, has a lot of cool content,

20
00:02:40,850 --> 00:02:44,834
videos, articles, code. Sign up and

21
00:02:44,872 --> 00:02:48,434
get it once a week. Free training is available.

22
00:02:48,552 --> 00:02:52,530
If you're really interested in Pulsar, we give you an environment you can code

23
00:02:52,600 --> 00:02:55,830
with and you get some really good training

24
00:02:55,900 --> 00:02:59,574
materials. Walks you through with some quizzes along

25
00:02:59,612 --> 00:03:03,350
the way. Pretty cool way to learn how to do pulsar.

26
00:03:04,250 --> 00:03:08,280
But what is Pulsar? It is a unified messaging platform.

27
00:03:08,890 --> 00:03:12,086
Why? Well, there's a lot of different messaging systems

28
00:03:12,118 --> 00:03:16,106
out there, different types, they do different things. Yahoo saw that and

29
00:03:16,128 --> 00:03:19,786
they couldn't find one that met all their needs. They did not want to

30
00:03:19,808 --> 00:03:24,154
run multiple messaging systems because they wanted to interrupt.

31
00:03:24,282 --> 00:03:27,854
They wanted just one scalable architecture to do

32
00:03:27,892 --> 00:03:31,530
that. Pulsar unifies multiple

33
00:03:31,610 --> 00:03:35,634
messaging protocols, messaging styles, and does

34
00:03:35,672 --> 00:03:39,710
this in a pretty unique way. As expected, with messaging

35
00:03:39,870 --> 00:03:43,506
all your messages are guaranteed very resilient system and

36
00:03:43,528 --> 00:03:46,946
you'll see why. With the underlying architecture, you can scale out

37
00:03:46,968 --> 00:03:50,786
as big as you need to. There are a lot of large companies scaling

38
00:03:50,818 --> 00:03:54,166
out to petabytes, thousands of machines as big as you need

39
00:03:54,188 --> 00:03:57,646
to go. Or if you're just starting out with one docker

40
00:03:57,698 --> 00:04:01,078
container cluster is pretty straightforward.

41
00:04:01,254 --> 00:04:04,730
What's nice is because of things, separation of

42
00:04:04,800 --> 00:04:08,406
components. These are a separation of compute

43
00:04:08,438 --> 00:04:11,726
and storage makes that very easy to get up on.

44
00:04:11,748 --> 00:04:15,086
Kubernetes, we've got a helm chart and operators for

45
00:04:15,108 --> 00:04:18,478
you all in the open source. Don't need to use any

46
00:04:18,564 --> 00:04:21,774
commercial product there. What's nice is the

47
00:04:21,812 --> 00:04:25,122
different layers are isolated and with such

48
00:04:25,176 --> 00:04:29,154
we've isolated the metastore out. The metadata store

49
00:04:29,192 --> 00:04:33,186
is used for metadata that both layers need

50
00:04:33,288 --> 00:04:36,680
and for discovery of services. This is now

51
00:04:37,290 --> 00:04:41,586
independent of a particular implementation.

52
00:04:41,778 --> 00:04:44,370
As of right now we have zookeeper,

53
00:04:44,530 --> 00:04:47,218
RocKsDB and eTCD coded.

54
00:04:47,394 --> 00:04:51,226
And because it's open sources I'm sure people will be adding more.

55
00:04:51,328 --> 00:04:54,934
Got a lot of developers out there. The pulsar brokers

56
00:04:54,982 --> 00:04:59,434
handle all the messages, make sure things are routed connected with

57
00:04:59,472 --> 00:05:04,670
some caching in there. Automatically load balance handles

58
00:05:05,810 --> 00:05:09,214
segmenting the topics. These are very easy to scale out.

59
00:05:09,252 --> 00:05:13,562
They don't need much disk. These are very inexpensive smaller brokers.

60
00:05:13,626 --> 00:05:17,266
They can scale them up and down very easily. Not have to

61
00:05:17,288 --> 00:05:20,734
worry about the storage. Storage is in Apache bookkeeper.

62
00:05:20,862 --> 00:05:24,318
These bookies have the messages and cursors pointing

63
00:05:24,334 --> 00:05:27,702
to everything and they segment them up to

64
00:05:27,756 --> 00:05:31,526
optimize storage. Makes it very easy. You don't have to have

65
00:05:31,708 --> 00:05:35,606
thousands of these giant SSD nodes. And what's cool as

66
00:05:35,628 --> 00:05:39,042
well is you could design your topics to automatically

67
00:05:39,106 --> 00:05:42,810
tier out to storage such as s three or different object

68
00:05:42,880 --> 00:05:46,314
stores or hdfs. Makes it easy for you to store as much

69
00:05:46,352 --> 00:05:49,706
data as you want and not have to worry about that when you need to

70
00:05:49,808 --> 00:05:54,094
consume messages. Even if you're going back through years of data,

71
00:05:54,212 --> 00:05:56,080
petabytes doesn't matter.

72
00:05:57,410 --> 00:06:01,066
Pretty standard in messaging. Publish and subscribe

73
00:06:01,258 --> 00:06:04,686
producers send messages. Ordered name channels

74
00:06:04,718 --> 00:06:08,926
called the topic messages have a payload,

75
00:06:09,038 --> 00:06:12,546
whatever that may be along with some additional metadata and

76
00:06:12,568 --> 00:06:16,214
properties you put on there. Brokers make sure things

77
00:06:16,252 --> 00:06:20,326
are routed properly and everything's connected. You create a

78
00:06:20,348 --> 00:06:23,398
subscription to topics to get what kind of

79
00:06:23,404 --> 00:06:27,538
data you want. These are the Magic way you decide

80
00:06:27,714 --> 00:06:31,050
how I want this system to run. Do I want it

81
00:06:31,120 --> 00:06:34,346
Kafka style streaming? Do I want to fan out?

82
00:06:34,448 --> 00:06:38,410
Lots of options there. Your consumer get these great messages.

83
00:06:39,070 --> 00:06:42,846
These subscription modes are important because this gives you

84
00:06:43,028 --> 00:06:45,962
some cool different styles of messaging.

85
00:06:46,106 --> 00:06:49,546
With exclusive and failover over you get a guaranteed

86
00:06:49,578 --> 00:06:53,518
order. You get a single consumer with failover,

87
00:06:53,694 --> 00:06:57,682
if that consumer can't function the backup takes

88
00:06:57,736 --> 00:07:01,410
over. This is how you do Kafka style streaming.

89
00:07:01,750 --> 00:07:06,454
This is if you need things exactly once in an order your

90
00:07:06,492 --> 00:07:10,566
traditional flink. Things like that work with that type of

91
00:07:10,748 --> 00:07:14,694
streaming. That's a great way to do things. But remember

92
00:07:14,892 --> 00:07:18,498
it's the consumer that decides that. So it doesn't matter how you got

93
00:07:18,524 --> 00:07:21,786
those messages in there. Consumer decides I

94
00:07:21,808 --> 00:07:25,146
want this streaming style, you get it. If you decide I want

95
00:07:25,168 --> 00:07:29,562
a work queue, give me. I'm going to have 1000

96
00:07:29,696 --> 00:07:33,166
little apps running there and as soon as they cloud get something to

97
00:07:33,188 --> 00:07:36,862
run they're going to run it. All of them active. Don't care about order,

98
00:07:36,916 --> 00:07:40,606
just process this stuff as fast as possible. Started is great for

99
00:07:40,628 --> 00:07:44,238
that key shared. Let me split the difference.

100
00:07:44,404 --> 00:07:48,366
Could have multiple active consumers in order,

101
00:07:48,548 --> 00:07:52,654
but it's order for a key so this is good. If maybe you're doing CDC.

102
00:07:52,782 --> 00:07:56,294
Each one has a table. Make that the key. Have each

103
00:07:56,412 --> 00:08:00,406
table consumer get their own table. All those

104
00:08:00,588 --> 00:08:04,486
rows in that table will be in order. Lots of other examples for

105
00:08:04,508 --> 00:08:08,140
that. What's a message? A message

106
00:08:08,670 --> 00:08:11,558
data. Obviously this could be raw bytes,

107
00:08:11,734 --> 00:08:15,114
but you can make sure that it conforms to different

108
00:08:15,152 --> 00:08:18,506
types of schemas. Key highly recommend it.

109
00:08:18,608 --> 00:08:23,230
It's helpful for reducing storage if you need to partition.

110
00:08:24,370 --> 00:08:27,694
If you need to find data randomly later. Having a key is

111
00:08:27,732 --> 00:08:31,166
important. If you can't think of a key, I like to

112
00:08:31,188 --> 00:08:34,766
just auto generate one, just to have one cloud add any kind of key

113
00:08:34,788 --> 00:08:38,226
value map properties. These could be metadata that

114
00:08:38,248 --> 00:08:41,518
you need along with the message without messing with that payload.

115
00:08:41,694 --> 00:08:45,090
We like to name things when it gets produced so you know who sent it.

116
00:08:45,160 --> 00:08:48,706
Good for debugging and other purposes. Auditing sequence

117
00:08:48,738 --> 00:08:52,054
id so you know how it's ordered. Obviously something

118
00:08:52,092 --> 00:08:54,070
you need if you're going to do streaming.

119
00:08:54,890 --> 00:08:58,522
Lots of things you could do. Those are really great basic things.

120
00:08:58,656 --> 00:09:02,006
But I want to get data in and out, especially with Python.

121
00:09:02,118 --> 00:09:05,434
So we support functions. These let me run

122
00:09:05,552 --> 00:09:09,094
basically know in Amazon

123
00:09:09,142 --> 00:09:12,506
you have lambda or you could think of database triggers.

124
00:09:12,618 --> 00:09:15,966
They get triggered by events that occur in

125
00:09:15,988 --> 00:09:20,030
a topic or multiple topics. Pretty straightforward there.

126
00:09:20,180 --> 00:09:24,430
A similar concept are connectors. These are sources and syncs.

127
00:09:24,590 --> 00:09:27,890
These you set. Some configuration data automatically goes

128
00:09:27,960 --> 00:09:32,018
there. That's a great feature. The protocol handler support is

129
00:09:32,104 --> 00:09:36,962
important because this lets us not just do the pulsar libraries.

130
00:09:37,026 --> 00:09:40,390
I could use existing libraries for activemQ,

131
00:09:40,890 --> 00:09:44,658
for RabidmQ, for Kafka,

132
00:09:44,674 --> 00:09:48,326
for MQTT. This makes it very easy to

133
00:09:48,428 --> 00:09:52,054
not have to change code. Lets you support unified

134
00:09:52,102 --> 00:09:56,006
messaging. You want to process data, I'll show you some examples

135
00:09:56,038 --> 00:09:59,738
today. Flink spark presto. Makes it easy for you to

136
00:09:59,744 --> 00:10:02,986
do that. We mentioned that tiered storage important once you get

137
00:10:03,008 --> 00:10:07,514
a lot of data and you don't want to store it. All these expensive SSD

138
00:10:07,562 --> 00:10:10,734
drives put it on that cheap s three storage. So a

139
00:10:10,772 --> 00:10:14,190
function takes some messages in, can log

140
00:10:14,260 --> 00:10:17,774
things to a special log topic, send it to one or more outputs,

141
00:10:17,822 --> 00:10:21,726
or not have an output to a topic. You could update a database

142
00:10:21,758 --> 00:10:25,178
or do something special in there. Get support for Java,

143
00:10:25,214 --> 00:10:29,334
Python and go use any kind of libraries in there, including machine learning.

144
00:10:29,452 --> 00:10:33,030
Pretty straightforward. I have a function that

145
00:10:33,100 --> 00:10:36,760
takes in a chat message, which is just

146
00:10:37,930 --> 00:10:41,190
json, but I take it as a big text file.

147
00:10:41,690 --> 00:10:45,466
If I do use our special library to

148
00:10:45,488 --> 00:10:49,094
build my function, I can get access to a context.

149
00:10:49,142 --> 00:10:52,478
It gives me logging, gives me the message id. It's a

150
00:10:52,484 --> 00:10:56,302
nice feature. And then when I do a return from that, it goes to that

151
00:10:56,356 --> 00:11:00,122
output topic that I specified in my DevOps

152
00:11:00,186 --> 00:11:04,426
setup. If I want an advanced way to run these functions,

153
00:11:04,618 --> 00:11:08,174
by default we just run them as a process or thread

154
00:11:08,222 --> 00:11:12,190
within the broker. You want to do that in a more powerful

155
00:11:12,350 --> 00:11:16,158
mechanism. We can run it in Kubernetes with function mesh.

156
00:11:16,334 --> 00:11:20,386
This makes it very easy to have a scalable architecture

157
00:11:20,418 --> 00:11:24,338
for how are you going to run all these different functions? And there you're

158
00:11:24,354 --> 00:11:27,554
setting a couple of Kubernetes files up,

159
00:11:27,692 --> 00:11:31,430
Kubecontrol deploys them, uses all the standard APIs,

160
00:11:31,510 --> 00:11:35,306
and it sets up a namespace to easily connect with

161
00:11:35,328 --> 00:11:40,122
that pulsar cluster it's associated with for

162
00:11:40,176 --> 00:11:43,694
those different protocols. MQTT is an example. If you

163
00:11:43,732 --> 00:11:47,662
see here, it's not a simple little tiny layer there

164
00:11:47,716 --> 00:11:51,022
or some kind of interpreter or something that

165
00:11:51,076 --> 00:11:55,122
just converts a to b. We have a

166
00:11:55,176 --> 00:11:58,942
full implementation of the MQTT

167
00:11:59,006 --> 00:12:02,626
libraries, so these protocol handlers work as if

168
00:12:02,648 --> 00:12:05,986
you now have an MQTT broker. And what's

169
00:12:06,018 --> 00:12:10,230
cool is it doesn't matter how your data gets into or

170
00:12:10,300 --> 00:12:13,906
out of pulsar, you pick the library you're

171
00:12:13,938 --> 00:12:16,680
comfortable with. I send Kafka in,

172
00:12:17,130 --> 00:12:19,686
I want to get it out as pulsar or I want to get it as

173
00:12:19,708 --> 00:12:23,210
MQTT. Doesn't matter, it's that same

174
00:12:23,360 --> 00:12:27,254
data. An easy way to access your data is to pulsar

175
00:12:27,302 --> 00:12:31,270
SQL, which under the covers is Presto Trino.

176
00:12:31,430 --> 00:12:34,986
And it lets you access the data events if it's already in that tiered

177
00:12:35,018 --> 00:12:38,254
storage. So it lets you do a quick SQL and see

178
00:12:38,292 --> 00:12:41,662
what's in your topics. If you want to do

179
00:12:41,796 --> 00:12:45,934
micro batch coding or maybe some ETL,

180
00:12:46,062 --> 00:12:49,170
spark is great for that. We got a great Spark connector.

181
00:12:50,550 --> 00:12:53,826
Connect right to your topics, do a SQL on

182
00:12:53,848 --> 00:12:56,606
it and then you can figure out where you want to write the stream.

183
00:12:56,638 --> 00:13:00,278
It could just be the console if you're monitoring something. Or you could dump it

184
00:13:00,284 --> 00:13:04,018
to a CSV JSON, Avro, parquet some other file,

185
00:13:04,114 --> 00:13:07,494
maybe combine it with other things, maybe do some machine learning.

186
00:13:07,692 --> 00:13:10,966
Pretty easy to do. You could

187
00:13:10,988 --> 00:13:14,554
do some real time ETL and continuous SQL with

188
00:13:14,592 --> 00:13:19,930
Flink. This is a nice way to process

189
00:13:20,000 --> 00:13:23,722
your data at scale event at a time. What's nice here is

190
00:13:23,776 --> 00:13:27,066
this can be done know either just a SQL

191
00:13:27,098 --> 00:13:30,686
interface or you could use one of Flink's languages like

192
00:13:30,788 --> 00:13:34,002
JavaScript or Python. The queries are pretty

193
00:13:34,056 --> 00:13:38,126
straightforward. Select columns

194
00:13:38,238 --> 00:13:41,762
from a table. That table being a topics you do order

195
00:13:41,816 --> 00:13:45,302
by and group bys Max, all those kind of fun things you expect

196
00:13:45,356 --> 00:13:49,446
in SQL in

197
00:13:49,468 --> 00:13:52,882
the overview of this kind of think this is a universal

198
00:13:52,946 --> 00:13:56,342
Gateway can be used as a buffer while you're doing

199
00:13:56,396 --> 00:13:59,866
processing in different places. Very easy to get in and out of

200
00:13:59,888 --> 00:14:03,626
a lot of different sources and

201
00:14:03,648 --> 00:14:07,078
you cloud do a lot of processing in here, whether it's filtering,

202
00:14:07,174 --> 00:14:10,378
aggregating, enriching, deduping data,

203
00:14:10,544 --> 00:14:13,770
and it's nice now all these systems are decoupled.

204
00:14:14,190 --> 00:14:18,266
You don't have to know if someone wrote something in Java or they wrote Scala

205
00:14:18,298 --> 00:14:20,910
or it's coming from this database or that database.

206
00:14:21,410 --> 00:14:24,638
All I got to know is one of these protocols to get in

207
00:14:24,644 --> 00:14:28,754
and out of pulsar. Pretty straightforward to make things

208
00:14:28,952 --> 00:14:32,514
have a contract. It's great to put a schema in there. It's really

209
00:14:32,552 --> 00:14:36,126
easy to do that in python. If I have a schema, whether it's

210
00:14:36,158 --> 00:14:39,702
Avro, Protobuff or JSON, makes it very easy

211
00:14:39,756 --> 00:14:42,854
for me to know what those fields are, what it should look like.

212
00:14:42,972 --> 00:14:46,482
Makes it easy to format these tables. We might want in Spark,

213
00:14:46,546 --> 00:14:49,866
flink or presto, example of an

214
00:14:49,888 --> 00:14:53,002
architecture here. We touched on those

215
00:14:53,056 --> 00:14:56,634
topics already. I want to show you some code if you want to do this

216
00:14:56,672 --> 00:15:00,326
at home. I've put all the directions

217
00:15:00,438 --> 00:15:03,390
in the slides so you don't have to.

218
00:15:03,460 --> 00:15:06,590
I'm not going to walk through them all because we not

219
00:15:06,660 --> 00:15:10,158
have a super long session. I'll come back to a couple

220
00:15:10,244 --> 00:15:13,314
that are maybe really critical to highlight some

221
00:15:13,352 --> 00:15:16,834
things, but here you could download it if you want to try it as

222
00:15:16,872 --> 00:15:20,466
like standalone on

223
00:15:20,488 --> 00:15:24,414
a small node, like maybe an EC two. There's instructions

224
00:15:24,462 --> 00:15:28,258
for doing it with stream native cloud, which we have a free tier

225
00:15:28,434 --> 00:15:32,760
or some other options there. You could also do it in

226
00:15:33,610 --> 00:15:37,206
Docker, you could use the Kubernetes helm chart or go

227
00:15:37,228 --> 00:15:40,966
to our academy and you get to a cluster for a couple of days connected

228
00:15:40,998 --> 00:15:44,806
to visual code instance to do some development.

229
00:15:44,998 --> 00:15:48,010
But let's look at some real code.

230
00:15:48,080 --> 00:15:51,786
Let's look at some demos. I have a

231
00:15:51,808 --> 00:15:55,306
bunch of open source examples here, fully documented.

232
00:15:55,418 --> 00:15:59,230
This one is for weather and this gives you a little idea of

233
00:15:59,300 --> 00:16:02,314
what's going on here. I show you different runs,

234
00:16:02,362 --> 00:16:05,562
how we interact with things just to make it easy.

235
00:16:05,716 --> 00:16:09,106
All the source code is here. Makes it very easy for you to

236
00:16:09,128 --> 00:16:12,846
run these things. Let's take a look at some of the source

237
00:16:12,878 --> 00:16:16,550
here. This is simple python. The things that you need

238
00:16:16,620 --> 00:16:21,010
here, you need the pulsar library.

239
00:16:21,170 --> 00:16:24,610
You just install that with Pip and the one for schema.

240
00:16:24,690 --> 00:16:28,280
Now, it depends on your infrastructure, lots of different

241
00:16:29,450 --> 00:16:32,280
infrastructure supported. If it's not,

242
00:16:32,590 --> 00:16:36,042
you have to compile it from C Plus plus, but we've got the full build

243
00:16:36,096 --> 00:16:39,862
for that if you need to do that, and it'll even run on raspberry PI.

244
00:16:40,006 --> 00:16:44,202
If I want a schema, which if your data is consistent,

245
00:16:44,346 --> 00:16:47,850
has fields, let's do that. Let's create a schema.

246
00:16:47,930 --> 00:16:51,834
This is an example. I want one for my weather fields.

247
00:16:51,962 --> 00:16:55,486
So this is as simple it is. That's all I have to do.

248
00:16:55,588 --> 00:16:58,286
I don't have to know some kind of special schema language.

249
00:16:58,398 --> 00:17:01,714
Create a class, put the fields in it, we're ready to go.

250
00:17:01,912 --> 00:17:05,330
So here I'm connecting, grabbing some stuff from a sensor.

251
00:17:05,770 --> 00:17:09,442
The part that's important here is connect to my pulsar

252
00:17:09,506 --> 00:17:13,506
cluster. If I have authentication,

253
00:17:13,698 --> 00:17:17,346
I've got that examples in there. We support oauth and tokens

254
00:17:17,378 --> 00:17:20,506
in a bunch of other ways. But just to keep it simple, I've just got

255
00:17:20,528 --> 00:17:24,186
my local hosted environment here.

256
00:17:24,368 --> 00:17:28,182
I've got my topic. If you look at the topic, it's a little unique.

257
00:17:28,326 --> 00:17:31,770
I didn't go into this as we're trying to go through pretty quickly,

258
00:17:31,920 --> 00:17:35,918
but pulsar is multitenant, so I could have

259
00:17:36,084 --> 00:17:39,626
a lot of different people using this, so I can have hundreds of thousands

260
00:17:39,658 --> 00:17:43,374
of topics. And the way to keep that clean and secure is

261
00:17:43,412 --> 00:17:47,234
I have a tenant say that this one's for everybody, but I could

262
00:17:47,272 --> 00:17:50,866
have one specific to your group, your company. Then underneath that

263
00:17:50,888 --> 00:17:54,594
I make any of the namespaces I want, maybe per AOP, per line

264
00:17:54,632 --> 00:17:58,070
of business. And under there are my topic names. So it keeps it nice

265
00:17:58,140 --> 00:18:01,366
and isolated, easy to work through. Here is where

266
00:18:01,388 --> 00:18:04,710
I define the schema. I just say JSON schema

267
00:18:05,050 --> 00:18:09,170
with this class I set up up here for weather.

268
00:18:09,330 --> 00:18:12,726
And if I wanted Avro, I would just say Avro schema. And I've

269
00:18:12,758 --> 00:18:16,090
got an example of that documented there as well. I set a couple

270
00:18:16,160 --> 00:18:19,398
of properties, and that's

271
00:18:19,414 --> 00:18:22,654
really all you have to do to be able to set that up. And then

272
00:18:22,692 --> 00:18:26,302
I have a loop that's just going through the sensor. When it ever gets new

273
00:18:26,356 --> 00:18:29,600
data, I just set up a new weather record,

274
00:18:30,210 --> 00:18:33,762
set all the fields, and then just send them along

275
00:18:33,816 --> 00:18:37,406
with that key that we mentioned before. And I'm

276
00:18:37,438 --> 00:18:40,734
just creating one from a build and then boom,

277
00:18:40,782 --> 00:18:44,594
it's sent. Let's actually go to that device and

278
00:18:44,632 --> 00:18:47,974
I'm going to run that code here. First thing it does

279
00:18:48,012 --> 00:18:51,554
is connect to the cluster, build that producer. And now we're

280
00:18:51,602 --> 00:18:55,094
sending data. I'm printing it out just so I can see

281
00:18:55,132 --> 00:18:58,658
that it's working and it's for debug. Obviously in your code you're

282
00:18:58,674 --> 00:19:01,946
probably not going to want that. That way you have lots of different ways you

283
00:19:01,968 --> 00:19:05,382
might want to run your python. Could be in a notebook,

284
00:19:05,446 --> 00:19:09,338
could be somewhere else. This is good to see that the data is coming in.

285
00:19:09,424 --> 00:19:13,166
I know it's happening. That's great. Now if we

286
00:19:13,188 --> 00:19:16,446
were going to do the Avro version, we could take

287
00:19:16,468 --> 00:19:19,950
a look. The only difference in this whole thing

288
00:19:20,100 --> 00:19:23,774
is here I say schema

289
00:19:23,902 --> 00:19:27,394
is Avro. That's it. To make that

290
00:19:27,432 --> 00:19:31,218
different. Pretty straightforward. Now I can connect to

291
00:19:31,304 --> 00:19:34,530
Presto from the command line, or I could use,

292
00:19:34,680 --> 00:19:38,246
there's some graphical tools I don't know how. Well you could

293
00:19:38,268 --> 00:19:42,280
see this is pretty dark, but I just have a select star here

294
00:19:42,810 --> 00:19:47,010
with just a couple of fields I want. I cloud just browse all the topics

295
00:19:47,170 --> 00:19:51,146
from my tenant namespace. Presto sets that up

296
00:19:51,168 --> 00:19:54,790
for me. And I get access to even all those metadata fields

297
00:19:54,950 --> 00:19:58,698
like the event time, the message id, the key,

298
00:19:58,864 --> 00:20:02,446
and I could even put those in my queries like this message id if

299
00:20:02,468 --> 00:20:04,910
I want. Pretty straightforward.

300
00:20:05,730 --> 00:20:09,194
Presto is pretty quick. I could change this query

301
00:20:09,242 --> 00:20:12,834
and do something like account. Let's see how many rows we have

302
00:20:12,872 --> 00:20:16,834
there. 81, 83. We're adding more

303
00:20:16,872 --> 00:20:21,022
as we're running this, which is the good thing to see. Nothing crashed

304
00:20:21,166 --> 00:20:25,006
here. I'm doing flink and that is a

305
00:20:25,048 --> 00:20:29,590
similar way to do this, but this does this continuously.

306
00:20:30,090 --> 00:20:33,910
Now the thing to show you in that is

307
00:20:33,980 --> 00:20:37,270
that there are a couple of different

308
00:20:37,340 --> 00:20:41,018
settings you need for that. If you're interested in other

309
00:20:41,104 --> 00:20:44,666
examples, I have a lot of different examples, especially for

310
00:20:44,688 --> 00:20:48,794
Python and Java as those are pretty common and

311
00:20:48,832 --> 00:20:53,054
a nice way to be able to go through these things

312
00:20:53,252 --> 00:20:56,670
without too much work.

313
00:20:56,820 --> 00:21:00,826
So I've got some examples here and I've got some functions.

314
00:21:00,938 --> 00:21:04,562
Maybe we should go into a function. I've got one in visual code here.

315
00:21:04,696 --> 00:21:08,750
This one does sentiment analysis. It's pretty straightforward.

316
00:21:08,910 --> 00:21:12,258
I'm using the function here so I get that logger so I

317
00:21:12,264 --> 00:21:16,120
could log it so it comes in. I just

318
00:21:16,810 --> 00:21:20,598
parse the json there and

319
00:21:20,684 --> 00:21:24,690
I just return the sentiment. Pretty straightforward.

320
00:21:24,850 --> 00:21:28,098
And then how I'm running it here is, I have

321
00:21:28,124 --> 00:21:31,980
an example. Let me reload this. It's been sitting here too long.

322
00:21:32,750 --> 00:21:36,362
Hopefully I didn't crash my system here.

323
00:21:36,416 --> 00:21:39,260
Okay, so I'm just going to put a question in here.

324
00:21:40,590 --> 00:21:44,480
Is today the best day for demos ever?

325
00:21:44,850 --> 00:21:48,480
We'll see if everything's working. And it is.

326
00:21:49,250 --> 00:21:52,746
So that's cool. And we can look at the publish time for all the new

327
00:21:52,868 --> 00:21:55,700
messages coming through. That's great.

328
00:21:56,950 --> 00:21:59,300
Is Apache pulsar cool?

329
00:22:01,910 --> 00:22:05,300
Great. How about great? And I'll change my name here.

330
00:22:06,650 --> 00:22:10,166
And it's giving me back a positive. So what you're seeing here

331
00:22:10,188 --> 00:22:13,446
is an HTML form that's just doing a

332
00:22:13,468 --> 00:22:16,774
websockets call to pulsar and

333
00:22:16,812 --> 00:22:20,422
that's going into a topic down here is a separate consumer

334
00:22:20,486 --> 00:22:24,054
that's getting the results of that function. I know it looks like it's

335
00:22:24,102 --> 00:22:27,738
instantly showing up here it is making that Websocket called a

336
00:22:27,744 --> 00:22:31,146
pulsar. Pulsar is triggering that function that does

337
00:22:31,168 --> 00:22:34,974
sentiment analysis. It sends it to another topic and that comes

338
00:22:35,012 --> 00:22:38,538
out here in my HTML page with live websockets.

339
00:22:38,634 --> 00:22:42,334
All the source code for that is out there. Pretty easy to do.

340
00:22:42,452 --> 00:22:45,486
We are running presto. Presto is a nice interface to show

341
00:22:45,508 --> 00:22:49,042
you what's golang on. You can see how long things took,

342
00:22:49,096 --> 00:22:52,626
how much data was accessed. Pretty cool way to do that.

343
00:22:52,728 --> 00:22:55,894
Same thing with Flink as those jobs are running. We could see all

344
00:22:55,932 --> 00:22:59,334
of that happening. Just trying to get you a good example

345
00:22:59,452 --> 00:23:03,606
of flink here. We've got most of another example

346
00:23:03,708 --> 00:23:07,238
documented. It's pretty straightforward to do these things.

347
00:23:07,324 --> 00:23:10,682
But like say we wanted to do some

348
00:23:10,816 --> 00:23:13,914
flink. If you don't know how to work

349
00:23:13,952 --> 00:23:16,518
with Flink and pulsar,

350
00:23:16,694 --> 00:23:20,534
just take a look at my code. I've got a lot of examples

351
00:23:20,582 --> 00:23:24,602
out there. Like here shows you how to build a catalog.

352
00:23:24,746 --> 00:23:28,190
So if I'm logged into flink, I create a catalog.

353
00:23:29,330 --> 00:23:33,230
If I pasted it correctly, it's good to copy and paste

354
00:23:33,830 --> 00:23:36,962
properly here. So let's go to this one. This air quality

355
00:23:37,016 --> 00:23:39,790
is another example. Pretty straightforward.

356
00:23:39,870 --> 00:23:43,502
But I'll create a catalog

357
00:23:43,566 --> 00:23:47,014
for flink. Flink can connect to lots of different data

358
00:23:47,052 --> 00:23:50,722
stores, which is nice. So if I wanted to join,

359
00:23:50,866 --> 00:23:55,334
say, pulsar topics with a table from something else,

360
00:23:55,532 --> 00:23:58,686
I can do that. That's pretty cool. Let's see all the topics

361
00:23:58,738 --> 00:24:02,454
we have. Let's describe one. We're doing PI

362
00:24:02,502 --> 00:24:06,506
weather is the one coming off that device there. So I

363
00:24:06,528 --> 00:24:09,814
could just pick a couple of fields here. If they have

364
00:24:09,952 --> 00:24:14,062
receives name, you'll probably want to make

365
00:24:14,116 --> 00:24:18,202
sure that they're in quotes

366
00:24:18,346 --> 00:24:22,122
so that you don't get those lost.

367
00:24:22,266 --> 00:24:25,622
So this is piweather. When in doubt,

368
00:24:25,706 --> 00:24:29,106
use those quotes. So this is going to launch a

369
00:24:29,128 --> 00:24:32,814
flink job that could be spread across a giant flink cluster.

370
00:24:32,942 --> 00:24:36,398
And again, you could wrap this in some python or Java or scala

371
00:24:36,414 --> 00:24:40,054
code. The job has shown up. It is starting to

372
00:24:40,092 --> 00:24:43,494
process. You can see it's a table source scan because I don't have any

373
00:24:43,532 --> 00:24:47,094
joins with any other topics, but I could. So this is

374
00:24:47,132 --> 00:24:50,554
running. So as long as I'm still sending data here,

375
00:24:50,672 --> 00:24:53,818
should start showing up in flink again.

376
00:24:53,904 --> 00:24:57,914
Lots of different systems can show you this, but as those

377
00:24:57,952 --> 00:25:02,870
records are coming in, they're showing up over here and continuous.

378
00:25:03,030 --> 00:25:06,334
I'm not rerunning the query as an event comes in, comes into

379
00:25:06,372 --> 00:25:09,726
my results. I could have wrote this as an insert into,

380
00:25:09,828 --> 00:25:13,402
put that into another topic or insert this into another catalog.

381
00:25:13,466 --> 00:25:17,746
That could be a data store. And that is as easy as the code is.

382
00:25:17,928 --> 00:25:21,790
Makes it pretty straightforward to run these type of applications

383
00:25:21,950 --> 00:25:24,420
and the code, even if you're doing security,

384
00:25:25,290 --> 00:25:28,662
nothing's too hard here. Do your

385
00:25:28,716 --> 00:25:32,182
processing. That's your hardest part. Connecting into

386
00:25:32,236 --> 00:25:35,686
and out of pulsar is easy.

387
00:25:35,788 --> 00:25:38,906
Whether you want to use MQTT, you want to

388
00:25:38,928 --> 00:25:42,742
use sockets from Python, you want to use the Kafka

389
00:25:42,806 --> 00:25:46,538
protocol. Pretty straightforward. Data comes in,

390
00:25:46,704 --> 00:25:50,474
maybe it goes into a function or to a sync comes out.

391
00:25:50,512 --> 00:25:53,914
If you have an output there and anybody can consume

392
00:25:53,962 --> 00:25:57,838
it. I could have thousands of consumers coming off the same thing,

393
00:25:58,004 --> 00:26:01,902
same data. Just decide on your own which kind of

394
00:26:01,956 --> 00:26:05,650
subscription you want. Pretty straightforward. I have

395
00:26:05,720 --> 00:26:09,442
a lot of examples here we showed you so you can get started pretty

396
00:26:09,496 --> 00:26:12,962
easily. Please join the community if you're interested

397
00:26:13,096 --> 00:26:17,254
or have some more questions. I am available all

398
00:26:17,292 --> 00:26:21,094
the time, which means maybe

399
00:26:21,132 --> 00:26:24,406
I shouldn't tell you that I'm too busy. Don't contact me now.

400
00:26:24,428 --> 00:26:27,974
If you have a question, please reach out. I'm always looking to

401
00:26:28,012 --> 00:26:31,194
put out some cool new demos or help people with some

402
00:26:31,232 --> 00:26:34,780
problems or bring back suggestions to the community.

403
00:26:35,550 --> 00:26:39,066
Thank you for joining me. I'm going to have some meetups soon and

404
00:26:39,088 --> 00:26:42,910
some other talks. If you want to see some different queries,

405
00:26:43,650 --> 00:26:48,174
please feel free to let me know. I'm always looking to

406
00:26:48,372 --> 00:26:50,880
do some cool new stuff.

407
00:26:51,570 --> 00:26:55,266
So here, this is SQL. So if

408
00:26:55,288 --> 00:26:57,570
I want to do something like get a max,

409
00:26:58,310 --> 00:27:00,770
let's do a max on, say, temperature.

410
00:27:01,110 --> 00:27:04,594
Is that a float? Okay, I could cast it

411
00:27:04,632 --> 00:27:07,666
to a number if I defined it as a string.

412
00:27:07,778 --> 00:27:11,414
Sometimes you define things as a string just because you're not

413
00:27:11,452 --> 00:27:14,022
sure. So so far,

414
00:27:14,076 --> 00:27:17,746
82 degrees. I may need to turn my air conditioner

415
00:27:17,778 --> 00:27:20,858
on. It's a little loud. I wish I had central air here. What are you

416
00:27:20,864 --> 00:27:24,186
going to do? So that's not getting any higher.

417
00:27:24,288 --> 00:27:27,866
Pretty consistent if we look here. Hasn't gone up. We could have

418
00:27:27,888 --> 00:27:30,540
done the minimum, could have picked another field.

419
00:27:31,010 --> 00:27:34,080
Lots of options there, depending on what you want to do.

420
00:27:34,450 --> 00:27:37,662
So thanks for attending my

421
00:27:37,716 --> 00:27:42,874
talk. I hope you learned a little bit about pulsar

422
00:27:43,002 --> 00:27:46,846
and why you should start coding today with pulsar and

423
00:27:46,868 --> 00:27:51,178
python. Again, if you want to do it in go or Java

424
00:27:51,274 --> 00:27:55,254
or Scala or Kotlin or C or

425
00:27:55,292 --> 00:27:58,770
C sharp or node js or any library.

426
00:27:58,850 --> 00:28:02,486
Anybody who could speak websockets or someone could

427
00:28:02,508 --> 00:28:04,950
speak MqtT or Kafka,

428
00:28:05,770 --> 00:28:09,590
please start codings. Put your messages in. Pulsar makes

429
00:28:09,660 --> 00:28:13,480
everything easy, regardless of what type of

430
00:28:14,090 --> 00:28:16,100
streaming or messaging you intend to do.

