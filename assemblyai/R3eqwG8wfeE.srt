1
00:00:20,490 --> 00:00:24,254
Welcome to a squeezing ago function. Today I'm going to talk about

2
00:00:24,292 --> 00:00:28,738
how to squeeze is the performance out of your go functions.

3
00:00:28,914 --> 00:00:32,150
First of all, I want to talk about what is

4
00:00:32,220 --> 00:00:35,542
optimizing. If I ask

5
00:00:35,596 --> 00:00:39,634
you what is better orange juice probably

6
00:00:39,692 --> 00:00:43,126
most of you will say that fresh squeeze orange

7
00:00:43,158 --> 00:00:46,826
juice is the best one, but it depends on what you want.

8
00:00:46,928 --> 00:00:50,406
If you want to have a really good taste,

9
00:00:50,518 --> 00:00:53,774
probably fresh squeeze orange juice is probably

10
00:00:53,812 --> 00:00:58,970
the best option. But if you want something that is comfortable,

11
00:00:59,130 --> 00:01:03,026
maybe a brick of orange juice is great. If you

12
00:01:03,048 --> 00:01:07,026
want something that has to last forever, probably a

13
00:01:07,048 --> 00:01:10,606
tank will be a better option. So optimizing

14
00:01:10,638 --> 00:01:14,434
is about that. Optimizing is about finding the right ocean for

15
00:01:14,472 --> 00:01:17,874
you. Sometimes it's being faster, sometimes it's consume

16
00:01:17,922 --> 00:01:21,430
less memory, sometimes it's doing less I o.

17
00:01:21,500 --> 00:01:25,506
But not necessarily is just being faster or just consuming

18
00:01:25,538 --> 00:01:29,174
less cPu. So take that into consideration whenever

19
00:01:29,222 --> 00:01:31,850
you are optimizing.

20
00:01:33,470 --> 00:01:37,020
Another thing is important to optimize at the right level.

21
00:01:37,470 --> 00:01:41,166
You can buy an f one car, but if this is

22
00:01:41,188 --> 00:01:44,606
your road, you are screwed. So there's no way that you

23
00:01:44,628 --> 00:01:49,214
are going to get 300

24
00:01:49,252 --> 00:01:53,274
road. So probably you need to optimize

25
00:01:53,322 --> 00:01:57,454
your infrastructure first. You need to optimize probably your architecture

26
00:01:57,502 --> 00:02:00,766
first until you reach the point where you need to do micro

27
00:02:00,798 --> 00:02:04,226
optimizations at the goal level. Go is

28
00:02:04,248 --> 00:02:07,846
a really fast programming language, so the

29
00:02:07,868 --> 00:02:11,606
runtime is really efficient, the compiler is

30
00:02:11,628 --> 00:02:15,510
really good. So probably your bottleneck is not going to be

31
00:02:15,580 --> 00:02:18,646
in that go function. Probably your bottleneck

32
00:02:18,678 --> 00:02:21,020
is in the architecture, in the I O,

33
00:02:22,670 --> 00:02:25,658
maybe on some SQL queries or things like that.

34
00:02:25,744 --> 00:02:29,530
But normally you don't need to optimize at go level

35
00:02:29,680 --> 00:02:34,126
anyway. Sometimes you have good reasons to

36
00:02:34,148 --> 00:02:36,960
optimize at Go function level.

37
00:02:38,530 --> 00:02:42,560
But first of all, analyze all the other stuff first,

38
00:02:44,390 --> 00:02:47,300
optimize what you need when you need it.

39
00:02:47,750 --> 00:02:51,746
If you think about optimizing a go function, you can optimize it

40
00:02:51,768 --> 00:02:55,590
a lot. You can optimize until a

41
00:02:55,740 --> 00:02:59,480
completely absurd level. But the reality is

42
00:03:00,010 --> 00:03:03,480
you need to know when you need to optimize that function

43
00:03:03,850 --> 00:03:07,560
and when you need to stop. Because sometimes

44
00:03:08,330 --> 00:03:12,186
you are looking for the bottleneck and the bottleneck is in that function and

45
00:03:12,208 --> 00:03:16,330
you get obsessed with optimizing that function until the

46
00:03:16,480 --> 00:03:20,074
best level possible. But the reality is sometimes you

47
00:03:20,112 --> 00:03:24,026
reach the point where there is no longer the bottleneck

48
00:03:24,058 --> 00:03:27,230
there. So if you are over optimizing that function,

49
00:03:27,300 --> 00:03:31,694
you end up wasting time in something that is not needed.

50
00:03:31,812 --> 00:03:35,314
So you just should measure where is your

51
00:03:35,352 --> 00:03:39,010
bottleneck, fix that bottleneck and look for

52
00:03:39,080 --> 00:03:42,690
the next bottleneck to fix not get obsessed with one

53
00:03:42,760 --> 00:03:44,660
specific function.

54
00:03:46,970 --> 00:03:51,030
Do not guess. One of the most important lessons about

55
00:03:51,180 --> 00:03:54,934
optimizing is guessing is not

56
00:03:54,972 --> 00:03:58,646
good. We are not good at guessing. Nobody is good at guessing

57
00:03:58,678 --> 00:03:59,740
in these things.

58
00:04:04,910 --> 00:04:09,066
You can think about estimations in

59
00:04:09,168 --> 00:04:12,334
software. Nobody is good at estimations, nobody is

60
00:04:12,372 --> 00:04:16,830
good at guessing when it's going to be

61
00:04:16,900 --> 00:04:19,790
a performance penalty somewhere,

62
00:04:21,010 --> 00:04:23,682
when the optimization is going to work well,

63
00:04:23,816 --> 00:04:27,422
this is because the systems are really complex.

64
00:04:27,566 --> 00:04:30,226
You have to think about a lot of stuff.

65
00:04:30,408 --> 00:04:32,958
I o, hard drives,

66
00:04:33,134 --> 00:04:36,062
spinning disks, cpu,

67
00:04:36,206 --> 00:04:40,194
other processes working in that same cpu context,

68
00:04:40,242 --> 00:04:42,150
switches of threads,

69
00:04:44,010 --> 00:04:48,262
a lot of crazy stuff that the operating system is doing all the time.

70
00:04:48,396 --> 00:04:51,450
A lot of crazy stuff that other processes can be doing.

71
00:04:51,600 --> 00:04:55,114
There's so many variables, so many things

72
00:04:55,232 --> 00:04:58,986
going on in your computer at the same time that guessing is not

73
00:04:59,008 --> 00:05:02,846
going to help you at all. So the

74
00:05:02,868 --> 00:05:06,206
rule is basically measure everything. If you

75
00:05:06,228 --> 00:05:09,918
want to optimize, the first thing that you need to do is measure it.

76
00:05:10,004 --> 00:05:13,410
You need to know how much time

77
00:05:13,480 --> 00:05:17,026
or how much memory or how much resource that you want

78
00:05:17,048 --> 00:05:21,426
to optimize is consuming your program,

79
00:05:21,608 --> 00:05:24,450
and from there you can start optimizing.

80
00:05:27,290 --> 00:05:30,040
Well, how do you measure things? Well,

81
00:05:30,650 --> 00:05:34,902
the most common way of measuring things when you are doing

82
00:05:34,956 --> 00:05:38,346
macro optimizations in functions in go,

83
00:05:38,448 --> 00:05:42,394
is doing benchmarks. A benchmark is just part

84
00:05:42,432 --> 00:05:45,818
of the go standard library. It's part of

85
00:05:45,824 --> 00:05:49,222
the testing framework and allows you to generate

86
00:05:49,286 --> 00:05:52,810
these kind of similar test functions

87
00:05:52,890 --> 00:05:56,394
that allows you to measure how much time it's consuming

88
00:05:56,442 --> 00:06:00,298
something. For example, here I'm comparing the I'm

89
00:06:00,314 --> 00:06:03,522
doing two benchmarks, one for MD five and one for

90
00:06:03,576 --> 00:06:08,062
Sha two, five, six hashes.

91
00:06:08,206 --> 00:06:11,380
So I'm basically doing a benchmark and see

92
00:06:14,390 --> 00:06:17,560
what function is consuming more time.

93
00:06:18,250 --> 00:06:22,514
If I execute that, I'm going to see that the MD

94
00:06:22,562 --> 00:06:25,874
five is twice faster.

95
00:06:26,002 --> 00:06:30,010
I can basically do double of MD five hashes

96
00:06:30,670 --> 00:06:34,186
than SHA 255 hashes in the

97
00:06:34,208 --> 00:06:37,100
same time. So that's one thing.

98
00:06:37,470 --> 00:06:40,998
Also, I can ask the benchmark to report the allocations.

99
00:06:41,094 --> 00:06:45,178
How many allocations is doing all these hashes functions?

100
00:06:45,274 --> 00:06:49,434
If we see there, basically, in this case, the hashes are very well implemented

101
00:06:49,482 --> 00:06:52,946
and they have zero allocations. So they are really

102
00:06:53,048 --> 00:06:57,150
fast and they don't consume

103
00:06:57,230 --> 00:07:00,130
any heap memory.

104
00:07:03,270 --> 00:07:07,062
Well, let's see another example. Let's see open

105
00:07:07,116 --> 00:07:10,610
file. Whenever I open a file,

106
00:07:10,690 --> 00:07:14,550
it's taking, in this case, it's taking 120

107
00:07:14,620 --> 00:07:17,998
bytes per operation and it's doing three allocations.

108
00:07:18,114 --> 00:07:21,690
So that's an example of something that is really doing allocations.

109
00:07:22,750 --> 00:07:25,740
Later we will see why allocations are important.

110
00:07:27,150 --> 00:07:30,422
Another way of getting information about

111
00:07:30,576 --> 00:07:34,366
performance is profiling. Normally you want

112
00:07:34,388 --> 00:07:38,126
to know what a part of your system is the

113
00:07:38,148 --> 00:07:41,840
bottleneck, and profiling is for that.

114
00:07:42,630 --> 00:07:45,778
So here is

115
00:07:45,784 --> 00:07:48,420
the example of our benchmark before,

116
00:07:49,590 --> 00:07:53,346
so I can run that. But in this case I'm asking for a

117
00:07:53,368 --> 00:07:57,378
mem profile. So I asking hey, give me a mem profile

118
00:07:57,474 --> 00:08:00,934
and I can use the gotool pprov to

119
00:08:00,972 --> 00:08:04,498
get the information about where my memory is getting consumed.

120
00:08:04,594 --> 00:08:08,650
In this case, I can see there that 75

121
00:08:08,720 --> 00:08:12,346
megabytes in

122
00:08:12,368 --> 00:08:16,358
this benchmark is consumed

123
00:08:16,454 --> 00:08:18,460
by the OS new file function.

124
00:08:20,510 --> 00:08:24,302
Also I can see that in an SVG format to

125
00:08:24,356 --> 00:08:28,206
see in a more nice way. And clearly I

126
00:08:28,228 --> 00:08:31,280
see there is a big red block that is saying hey,

127
00:08:32,390 --> 00:08:36,354
most of the memory consumed by your

128
00:08:36,392 --> 00:08:40,146
benchmark, by your binary in this case is because the

129
00:08:40,168 --> 00:08:41,700
OS new file function.

130
00:08:44,230 --> 00:08:48,002
Also I can see exactly the line where that's

131
00:08:48,066 --> 00:08:51,670
happening is whenever I create this file strap.

132
00:08:54,330 --> 00:08:58,234
Also I can check for the CPU profile again,

133
00:08:58,432 --> 00:09:02,406
this is more distributed

134
00:09:02,438 --> 00:09:05,642
in different places, but 30% of the time

135
00:09:05,696 --> 00:09:09,420
is getting in the Cisco six function.

136
00:09:10,370 --> 00:09:14,266
And if I see that in a graph, I see that silhouette

137
00:09:14,298 --> 00:09:17,406
six is consuming 30%.

138
00:09:17,508 --> 00:09:21,098
There is 14% in eple weight

139
00:09:21,194 --> 00:09:24,350
and 22% in eple ctl.

140
00:09:25,990 --> 00:09:29,506
Anyway, let's take a look. This is our syscall six.

141
00:09:29,608 --> 00:09:33,394
That is basically a function that syscalls for

142
00:09:33,432 --> 00:09:37,990
the operating systems and is writing in

143
00:09:38,140 --> 00:09:41,714
go assembly. So probably you are not going to optimize

144
00:09:41,762 --> 00:09:42,360
this,

145
00:09:45,290 --> 00:09:49,394
but you can dig where that is happening and optimize

146
00:09:49,442 --> 00:09:52,954
that. This is from the standard library. So probably it's really

147
00:09:52,992 --> 00:09:56,486
well optimized already. But you can use the same tools

148
00:09:56,518 --> 00:09:59,500
to explore your source code. That is the interesting part.

149
00:10:01,250 --> 00:10:04,798
Okay, well, I'm going to show

150
00:10:04,884 --> 00:10:09,050
some examples of small optimizations.

151
00:10:09,130 --> 00:10:12,706
I want you to think about this more

152
00:10:12,728 --> 00:10:16,674
about the process. I want you to learn things during

153
00:10:16,712 --> 00:10:20,514
the process, learn things about the

154
00:10:20,552 --> 00:10:24,034
different microoptimizations that I'd be using here.

155
00:10:24,152 --> 00:10:27,526
But I think it's more interesting in the process how you create the

156
00:10:27,548 --> 00:10:31,382
benchmark, how you run the benchmark, optimize that,

157
00:10:31,436 --> 00:10:34,760
run to the benchmark again and see the results. And probably

158
00:10:35,130 --> 00:10:38,842
apart from the benchmark, you should have some

159
00:10:38,896 --> 00:10:42,538
unit tests to verify that you keep the code correct.

160
00:10:42,704 --> 00:10:46,474
But I'm showing here only the benchmark and

161
00:10:46,512 --> 00:10:50,410
the microoptimizations. So one of the simplest

162
00:10:51,570 --> 00:10:55,694
optimizations for reducing the cpu usage is basically

163
00:10:55,812 --> 00:10:59,342
doing less. So for example, I have here a find

164
00:10:59,396 --> 00:11:03,002
function that have a needle and a high stack and

165
00:11:03,156 --> 00:11:06,942
basically loop over the high stack and it find the needles,

166
00:11:07,006 --> 00:11:09,458
it returns itself,

167
00:11:09,544 --> 00:11:13,186
the IDX as a result and returns a

168
00:11:13,208 --> 00:11:16,790
result. This is a very naive approach to

169
00:11:16,860 --> 00:11:20,882
do this, but anyway, it's clear that you can optimize

170
00:11:20,946 --> 00:11:25,254
this. Let's start with a benchmark. I create

171
00:11:25,452 --> 00:11:29,034
ton of data for a high stack, and I do

172
00:11:29,072 --> 00:11:33,402
a test searching a value that is a bit over

173
00:11:33,456 --> 00:11:36,250
the half of the high stack.

174
00:11:36,670 --> 00:11:40,346
And if I run that, it's taking two,

175
00:11:40,368 --> 00:11:43,470
eight, nine nanoseconds per operation.

176
00:11:43,970 --> 00:11:47,870
Okay, sounds fast. Let's see if we can optimize that.

177
00:11:48,020 --> 00:11:51,294
If we just return early, return the result,

178
00:11:51,412 --> 00:11:55,042
because I don't need to keep searching if I already found something

179
00:11:55,096 --> 00:11:56,290
in the high stack.

180
00:12:01,030 --> 00:12:04,386
If I return that and run again,

181
00:12:04,488 --> 00:12:08,120
it's going to be 172

182
00:12:09,210 --> 00:12:12,806
nanoseconds per operation. That is a

183
00:12:12,828 --> 00:12:16,390
bit more than the half, because we are looking a but

184
00:12:16,460 --> 00:12:20,938
more than the half in the high stack in

185
00:12:20,944 --> 00:12:23,930
the benchmark. So that makes sense.

186
00:12:24,000 --> 00:12:28,022
Basically what we are doing here is just being a bit smarter

187
00:12:28,086 --> 00:12:31,418
and trying to not keep executing code if it's

188
00:12:31,434 --> 00:12:35,406
not needed. Okay. Another interesting

189
00:12:35,508 --> 00:12:39,342
thing is reducing allocations. Let's see

190
00:12:39,396 --> 00:12:41,870
here. If I have a slice,

191
00:12:43,250 --> 00:12:46,750
and I initialize that slice, and start

192
00:12:46,820 --> 00:12:50,846
adding integrals to that slice,

193
00:12:51,038 --> 00:12:54,306
in this case a million integrals, I'm going to do

194
00:12:54,328 --> 00:12:57,414
a benchmark that. See what happens if I run that.

195
00:12:57,612 --> 00:13:00,440
And the thing is,

196
00:13:02,250 --> 00:13:06,150
each operation is taking 39 allocations.

197
00:13:06,810 --> 00:13:10,858
And if you see there, it's taking 10

198
00:13:11,024 --> 00:13:15,354
million nanoseconds. That is like ten

199
00:13:15,392 --> 00:13:17,770
milliseconds per operation.

200
00:13:18,750 --> 00:13:20,800
So let's see,

201
00:13:21,890 --> 00:13:24,734
I can initialize that. I can say,

202
00:13:24,772 --> 00:13:28,158
hey, I know that I going to have 1 million

203
00:13:28,244 --> 00:13:32,378
integrals there. So I going to just

204
00:13:32,484 --> 00:13:35,426
create upfront that array of 1 million,

205
00:13:35,528 --> 00:13:39,140
that slice of 1 million elements. If I do that

206
00:13:39,750 --> 00:13:45,734
now, we are talking about less

207
00:13:45,772 --> 00:13:49,590
than a millisecond execution and

208
00:13:49,660 --> 00:13:53,670
only one allocation. So that's a huge improvement.

209
00:13:54,410 --> 00:14:00,266
We are reducing in one order of magnitude the

210
00:14:00,288 --> 00:14:04,506
time consuming by the function. But we

211
00:14:04,528 --> 00:14:07,706
can do better. We know upfront in this

212
00:14:07,728 --> 00:14:11,630
case, we know upfront of the compilation time that

213
00:14:11,780 --> 00:14:15,118
we are going to have a million elements there.

214
00:14:15,204 --> 00:14:19,102
So instead of creating a slice with make

215
00:14:19,156 --> 00:14:23,142
and giving original size, I'm going to create an array directly.

216
00:14:23,306 --> 00:14:28,978
In this case, it's now more

217
00:14:28,984 --> 00:14:32,990
or less a third of what we had before, and zero allocations.

218
00:14:33,150 --> 00:14:37,010
We are not allocating anything in the heap. That means

219
00:14:37,080 --> 00:14:40,326
that, it doesn't mean that it's not

220
00:14:40,348 --> 00:14:43,830
in memory. It means that it's in the stack, not in the heap.

221
00:14:44,410 --> 00:14:45,480
That's the idea.

222
00:14:49,150 --> 00:14:52,442
Let's see another kind of optimization. Let's talk about

223
00:14:52,496 --> 00:14:53,370
packing.

224
00:14:55,470 --> 00:14:57,930
Well, whenever you have a structure,

225
00:14:59,710 --> 00:15:05,166
the compiler is going to add

226
00:15:05,348 --> 00:15:09,646
space or add padding to our fields to

227
00:15:09,748 --> 00:15:12,298
make them align with the processor,

228
00:15:12,394 --> 00:15:16,494
basically make it more efficient for the cpu.

229
00:15:16,622 --> 00:15:20,046
In this case we have this struct that have a boolean, a float

230
00:15:20,078 --> 00:15:24,194
and an integer, and the compiler is going to add bytes in between

231
00:15:24,312 --> 00:15:28,242
that things. So if we create a slice

232
00:15:28,306 --> 00:15:31,990
of 1 million elements of that struct

233
00:15:32,970 --> 00:15:36,774
and benchmark, that we are going to see that we

234
00:15:36,812 --> 00:15:40,710
are consuming 24 megabytes per operation.

235
00:15:42,010 --> 00:15:46,700
We found a location in this case what is kind of fast, but still

236
00:15:47,310 --> 00:15:50,330
24 megabytes per operation.

237
00:15:51,570 --> 00:15:55,742
If we go here and just change that struct and

238
00:15:55,796 --> 00:15:59,390
change the order of the fields, the compiler is going to

239
00:15:59,540 --> 00:16:03,246
be better at generating that padding. It's going

240
00:16:03,268 --> 00:16:06,062
to say okay, b float is already aligned,

241
00:16:06,126 --> 00:16:09,346
c in 32 is already aligned, add the

242
00:16:09,368 --> 00:16:12,674
bool is not aligned. But I going to align with three

243
00:16:12,712 --> 00:16:14,900
extra bytes because well,

244
00:16:15,450 --> 00:16:18,566
I can pack that together with the previous int and all

245
00:16:18,588 --> 00:16:23,030
that stuff. From here we have 24 bytes

246
00:16:23,370 --> 00:16:27,502
per instance of the strack, and here we have 16

247
00:16:27,586 --> 00:16:31,882
bytes for each instance. If we run

248
00:16:31,936 --> 00:16:36,006
the benchmark there, it's still one allocation, but we are saving eight megabytes

249
00:16:36,038 --> 00:16:37,850
of memory in the heap.

250
00:16:38,990 --> 00:16:42,298
This can sound silly, but it's

251
00:16:42,314 --> 00:16:46,074
important if you see there, even if you are not caring

252
00:16:46,122 --> 00:16:49,694
that much about memory. In this case you see that there is

253
00:16:49,732 --> 00:16:54,034
consuming two milliseconds and

254
00:16:54,072 --> 00:16:56,100
here is consuming, well,

255
00:16:56,790 --> 00:17:02,162
2600 milliseconds, 2.6 milliseconds here 2.02.6.

256
00:17:02,296 --> 00:17:06,114
So it's not only affecting the memory usage, it's affecting the cpu

257
00:17:06,162 --> 00:17:10,198
time also. So another interesting

258
00:17:10,284 --> 00:17:12,070
thing is function inlining.

259
00:17:14,250 --> 00:17:18,342
I'm going to explain why inlining is important, but here

260
00:17:18,396 --> 00:17:20,940
is an example of not inline function.

261
00:17:22,270 --> 00:17:25,546
The compiler is going to take care of the inlining, and it's going to do

262
00:17:25,568 --> 00:17:29,050
that by analyzing the complexity of the

263
00:17:29,120 --> 00:17:32,894
function. And if the function is simple enough, it's going to

264
00:17:32,932 --> 00:17:36,366
inline that automatically. In this case it's not

265
00:17:36,388 --> 00:17:39,722
in line because I explicitly asked the compiler to not inline

266
00:17:39,786 --> 00:17:43,822
it. So if you have this, this is the not inline

267
00:17:43,886 --> 00:17:47,442
function and this is the inline function. And if I

268
00:17:47,496 --> 00:17:51,314
run a benchmark here, we are saving a whole

269
00:17:51,432 --> 00:17:54,818
nanosecond there. That is not, to be honest,

270
00:17:54,904 --> 00:17:58,502
it's not important at all. If you are

271
00:17:58,556 --> 00:18:02,680
fighting for nanoseconds you are in a very interesting

272
00:18:04,250 --> 00:18:08,194
field. But the reality is nobody cares about that nanosecond

273
00:18:08,242 --> 00:18:11,754
normally. But why is important in line,

274
00:18:11,792 --> 00:18:14,650
and we are going to see why it's important in liner in a minute,

275
00:18:15,070 --> 00:18:18,554
let's talk about escape analysis. Escape analysis is

276
00:18:18,592 --> 00:18:21,966
something that the compiler does to decide if something goes to

277
00:18:21,988 --> 00:18:25,966
the heap, also goes to the stack sometimes.

278
00:18:26,148 --> 00:18:29,310
For example, in this case, this is not in line.

279
00:18:29,380 --> 00:18:33,146
So in this case we have this value, and I return the

280
00:18:33,188 --> 00:18:37,170
pointer to the value that is going to escape

281
00:18:37,590 --> 00:18:42,162
to the heap because I'm returning a pointer. So val

282
00:18:42,296 --> 00:18:45,490
can be in the stack of the function, because I have to return

283
00:18:45,560 --> 00:18:49,494
that and it's going to be accessed from the outside.

284
00:18:49,692 --> 00:18:53,046
So I need to store in somewhere that is not the stack of

285
00:18:53,068 --> 00:18:56,150
the function. So it needs to go into the heap.

286
00:18:56,650 --> 00:18:59,020
So for that reason,

287
00:18:59,470 --> 00:19:03,302
now we have one allocation there, one heap allocation

288
00:19:03,366 --> 00:19:03,980
there.

289
00:19:06,270 --> 00:19:09,750
This is non escape function.

290
00:19:09,920 --> 00:19:11,760
I get the value,

291
00:19:13,170 --> 00:19:16,462
the integral value, and I return the value

292
00:19:16,516 --> 00:19:20,094
as an integer. So I'm not returning the pointer, I'm returning the whole

293
00:19:20,132 --> 00:19:23,386
value. So I'm returning a copy

294
00:19:23,418 --> 00:19:26,914
of the value, a copy of the stack. So it's a copy of the value

295
00:19:26,952 --> 00:19:30,878
that is storing the stack. So it is not requiring

296
00:19:31,054 --> 00:19:33,860
storing memory. So let's see,

297
00:19:36,550 --> 00:19:40,086
what is the impact of that with the escape function

298
00:19:40,188 --> 00:19:43,874
where we need to store something in the heap

299
00:19:44,002 --> 00:19:47,806
we have there that is consuming eight bytes in the heap and is consuming

300
00:19:47,858 --> 00:19:51,898
one allocation per operation. And that means

301
00:19:51,984 --> 00:19:55,366
that in this case it's taking 13 nanoseconds

302
00:19:55,398 --> 00:19:59,322
per operation, and the non escape function

303
00:19:59,456 --> 00:20:03,258
is taking only 1.5 nanosecond approximately.

304
00:20:03,434 --> 00:20:07,082
So that means that we are almost an order of magnitudes

305
00:20:07,146 --> 00:20:10,746
faster with the non escape function, because we don't

306
00:20:10,778 --> 00:20:14,282
need to access the heap, and the heap is expensive

307
00:20:14,426 --> 00:20:18,066
than the stack. So this

308
00:20:18,088 --> 00:20:21,646
is important. But we are going to see, other things that are important relate

309
00:20:21,678 --> 00:20:24,686
to escape analysis, and it's scape analysis plus inlining.

310
00:20:24,798 --> 00:20:28,390
If you combine escape analysis and inlining, you get something

311
00:20:28,460 --> 00:20:32,294
very interesting. For example, here we have a document that have

312
00:20:32,332 --> 00:20:35,590
a path, and I have a function,

313
00:20:35,660 --> 00:20:39,762
and I create a new document with that path. The new document

314
00:20:39,826 --> 00:20:43,850
is just creating the document, doing certain stuff,

315
00:20:44,000 --> 00:20:47,340
and returning the document, the pointer to the document. Okay,

316
00:20:47,870 --> 00:20:51,422
ecpC, right. If I run that,

317
00:20:51,556 --> 00:20:55,326
I get three allocations in

318
00:20:55,348 --> 00:20:59,040
this function. Okay, sounds good, that's fine.

319
00:20:59,490 --> 00:21:02,302
But if I change that, I say okay,

320
00:21:02,356 --> 00:21:05,726
now I create a new document, and then

321
00:21:05,748 --> 00:21:09,730
I call an in function. The new document is super simple function

322
00:21:09,880 --> 00:21:13,970
that have almost zero complexity. It's just returning a new pointer

323
00:21:14,310 --> 00:21:17,766
with an empty version of the document. And then I have an

324
00:21:17,788 --> 00:21:21,366
init function, and that init function is going to

325
00:21:21,388 --> 00:21:25,110
take care of the initialization.

326
00:21:26,970 --> 00:21:30,746
Doing this, what I'm doing is ensuring that the

327
00:21:30,768 --> 00:21:34,394
new document is in line, and because the new

328
00:21:34,432 --> 00:21:37,900
document is in line, we now have

329
00:21:39,630 --> 00:21:42,762
the d value there is not in the heap, it's just

330
00:21:42,816 --> 00:21:46,494
in line there. So it doesn't need to go in the heap goes directly into

331
00:21:46,532 --> 00:21:50,574
the stack of the my function and in the my funk is

332
00:21:50,612 --> 00:21:54,180
going to have their stack and it's going to store that

333
00:21:54,550 --> 00:21:58,450
document directly in the stack, it doesn't need to go in the heap.

334
00:21:59,670 --> 00:22:02,978
So whenever I run in it, I pass the

335
00:22:02,984 --> 00:22:06,054
pointer that is still in the stack and it's going to work

336
00:22:06,092 --> 00:22:09,938
well. So this way I'm going to save allocations.

337
00:22:10,034 --> 00:22:13,026
If I see here now, instead of having three allocation,

338
00:22:13,058 --> 00:22:17,922
I have two. Instead of consuming 56

339
00:22:18,076 --> 00:22:20,954
bytes per operation, I consume 40,

340
00:22:21,072 --> 00:22:25,402
32, and instead of being 128

341
00:22:25,456 --> 00:22:29,174
nanoseconds per operation, I only consuming

342
00:22:29,302 --> 00:22:33,342
73. So this small

343
00:22:33,396 --> 00:22:36,926
optimization in the constructors can lead to

344
00:22:37,028 --> 00:22:40,686
save a lot of allocations. Basically the

345
00:22:40,708 --> 00:22:43,906
lesson learned here is try to make your

346
00:22:44,088 --> 00:22:47,694
constructors really simple, and if you need a complex

347
00:22:47,742 --> 00:22:51,154
initialization, do that complex initialization as

348
00:22:51,192 --> 00:22:55,010
part of a secondary method, something like an init method.

349
00:22:56,630 --> 00:23:00,222
Okay, let's talk about concurrency. We all love concurrency

350
00:23:00,286 --> 00:23:03,526
because in go is so easy and so cool to

351
00:23:03,548 --> 00:23:08,090
do concurrency, but have some important

352
00:23:08,160 --> 00:23:11,850
implications in performance. Of course, here we think

353
00:23:11,920 --> 00:23:15,978
we have a fake I O function

354
00:23:16,064 --> 00:23:20,034
that simulates some I o, some blocking

355
00:23:20,102 --> 00:23:25,534
process. In this case I just doing some

356
00:23:25,572 --> 00:23:28,974
time slips and I have the

357
00:23:29,012 --> 00:23:33,486
fake I o parallel that is going to execute that in Go routines

358
00:23:33,678 --> 00:23:36,850
and the number of go routines passed as parameters.

359
00:23:38,150 --> 00:23:42,446
In this case I execute in three benchmark I executing

360
00:23:42,478 --> 00:23:45,714
one benchmark where I do parallel I

361
00:23:45,752 --> 00:23:49,478
o hundred go routines. Parallel I o based on

362
00:23:49,484 --> 00:23:52,886
the number of Cpus and parallel and

363
00:23:52,988 --> 00:23:56,326
serial I o. If we run the

364
00:23:56,348 --> 00:24:03,754
benchmark on this, we are going to see that the

365
00:24:03,792 --> 00:24:07,914
parallel one that have one

366
00:24:07,952 --> 00:24:11,710
go routine per job is going to have way more allocations,

367
00:24:12,530 --> 00:24:16,254
but the

368
00:24:16,292 --> 00:24:21,070
number of nanoseconds per operation is the smallest.

369
00:24:21,970 --> 00:24:26,530
So we are trading here probably memory and allocations

370
00:24:27,590 --> 00:24:31,314
against cpu time. In one

371
00:24:31,352 --> 00:24:34,958
per cpu you have less allocations and have a more or

372
00:24:34,984 --> 00:24:38,294
less acceptable, well, it's slower, it's ten

373
00:24:38,332 --> 00:24:41,794
times lower, but it's way faster than execute.

374
00:24:41,842 --> 00:24:45,974
That in a serial way and

375
00:24:46,012 --> 00:24:49,500
in a serial way is one allocation only, but it's taking

376
00:24:50,750 --> 00:24:53,866
a lot of time, multiple order

377
00:24:53,968 --> 00:24:58,282
of magnitude slower than

378
00:24:58,336 --> 00:25:00,170
the per CPU.

379
00:25:01,410 --> 00:25:04,398
So let's do another example.

380
00:25:04,484 --> 00:25:08,590
Let's play with the CPU. I create a fake cpu function

381
00:25:08,660 --> 00:25:12,142
that is going to consume cpu. Basically it's doing MD five

382
00:25:12,196 --> 00:25:15,186
sums again,

383
00:25:15,368 --> 00:25:19,170
a method to run that in parallel. And now I'm running that

384
00:25:19,240 --> 00:25:26,262
in parallel for 1000

385
00:25:26,316 --> 00:25:30,230
go routines in one per cpu and in serial.

386
00:25:32,970 --> 00:25:37,240
The interesting thing here is one

387
00:25:37,790 --> 00:25:42,314
go routine per job is

388
00:25:42,352 --> 00:25:46,742
going to be the slowest option here. It's going to be the more memory

389
00:25:46,806 --> 00:25:51,198
consuming, the more cpu consuming. Everything is wrong

390
00:25:51,364 --> 00:25:55,920
with trying to do cpu bound workloads in

391
00:25:56,370 --> 00:26:01,950
unbounded number of go routines. That's completely inefficient.

392
00:26:02,550 --> 00:26:06,494
If I use one go routine

393
00:26:06,542 --> 00:26:10,514
per cpu, that's going to give me the best in this

394
00:26:10,552 --> 00:26:15,734
case, because you can see there that it's taking 65

395
00:26:15,772 --> 00:26:19,910
microseconds and it's only 25 allocations. The co one

396
00:26:19,980 --> 00:26:23,960
is not doing allocations at all, but it's taking

397
00:26:24,410 --> 00:26:28,220
way more time to execute that. It's basically

398
00:26:29,150 --> 00:26:32,810
double of time to execute the same thing that you are executing

399
00:26:33,230 --> 00:26:36,010
when you are executing parallel with cpus.

400
00:26:39,070 --> 00:26:43,022
So that's it. This is more or less the idea of how

401
00:26:43,076 --> 00:26:46,350
you optimize a go function.

402
00:26:46,500 --> 00:26:50,270
There's some tricks that I shared, but I think

403
00:26:50,420 --> 00:26:54,420
the most interesting part is all this flow of

404
00:26:55,270 --> 00:26:58,866
creating a benchmark, trying to optimize that, trying to

405
00:26:58,888 --> 00:27:02,706
measure if your benchmark is doing something interesting

406
00:27:02,808 --> 00:27:06,162
or not, trying to reduce allocation allocations.

407
00:27:06,226 --> 00:27:09,974
Also, it's not only going to reduce the amount of

408
00:27:10,172 --> 00:27:17,126
memory consuming and the cpu spent in

409
00:27:17,148 --> 00:27:20,346
getting that memory allocated, also it's going to

410
00:27:20,368 --> 00:27:23,654
reduce the garbage collector pressure. So the garbage

411
00:27:23,702 --> 00:27:27,466
collector needs to recollect any allocation that you are

412
00:27:27,488 --> 00:27:30,994
doing. Whenever you finish with that allocation, the garbage collector

413
00:27:31,062 --> 00:27:35,520
needs to collect that. So every allocation is going to add

414
00:27:35,970 --> 00:27:41,034
pressure to the garbage collector. So reducing

415
00:27:41,082 --> 00:27:45,010
the allocation is going to have an extra impact

416
00:27:46,470 --> 00:27:51,650
in your program execution.

417
00:27:53,190 --> 00:27:56,838
So some references, there's a very interesting

418
00:27:56,924 --> 00:28:00,630
book called Efficient Go by Bartolomeck Plota.

419
00:28:02,330 --> 00:28:06,070
I really recommend that it

420
00:28:06,140 --> 00:28:10,140
has a very methodic approach around

421
00:28:13,230 --> 00:28:17,020
testing, benchmarking and optimizing. So really interesting

422
00:28:17,390 --> 00:28:20,810
book. High performance growth workshop from Dave Cheney.

423
00:28:20,970 --> 00:28:24,446
Also very interesting and very practical way of

424
00:28:24,548 --> 00:28:28,314
optimizing the go Perth book by Damian Grisky.

425
00:28:28,442 --> 00:28:32,026
And the ultimate goal course from Arden Labs

426
00:28:32,058 --> 00:28:35,518
is a really good one also

427
00:28:35,684 --> 00:28:39,018
for learning a bit more about hardware sympathy

428
00:28:39,114 --> 00:28:40,670
and optimizations.

429
00:28:41,970 --> 00:28:45,826
And well, all the images here are creative commons,

430
00:28:45,858 --> 00:28:49,910
so here is the references to

431
00:28:50,060 --> 00:28:53,506
the outforce and that's

432
00:28:53,538 --> 00:28:54,260
it. Thank you.

