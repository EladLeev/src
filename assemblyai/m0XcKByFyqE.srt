1
00:00:23,130 --> 00:00:26,374
By the time my talk will be over on, almost 10,000

2
00:00:26,492 --> 00:00:30,310
hours of content will be uploaded on YouTube, almost 6

3
00:00:30,380 --> 00:00:34,278
million stories will be posted on Instagram. Almost 700

4
00:00:34,364 --> 00:00:39,074
million messages will be shared across the world across different messaging

5
00:00:39,122 --> 00:00:42,674
platforms, and almost 20 million swipes

6
00:00:42,722 --> 00:00:46,578
will happen on the dating platform. So if you are a content creator,

7
00:00:46,674 --> 00:00:50,254
a brand, a marketeer or an employer, you have to

8
00:00:50,292 --> 00:00:54,814
stand out in front of your audience so

9
00:00:54,852 --> 00:00:58,462
that they can engage with your content. There is no way

10
00:00:58,516 --> 00:01:01,930
about that. Hi, I'm Gorapatra,

11
00:01:02,010 --> 00:01:04,980
founder and CTO of Flurgo. Welcome to my talk.

12
00:01:06,230 --> 00:01:10,354
The title of the talk will lead you to three different aspects of

13
00:01:10,392 --> 00:01:14,034
fields combined together. What is an

14
00:01:14,072 --> 00:01:17,894
interactive interfaces or what is interactive between physical and digital world?

15
00:01:18,012 --> 00:01:21,286
Secondly, the landscape of generative AI and finally

16
00:01:21,388 --> 00:01:24,566
crowdsourced. How does a human to

17
00:01:24,588 --> 00:01:28,134
human interaction or communication? So natural communication

18
00:01:28,182 --> 00:01:31,446
among women consists of primarily

19
00:01:31,558 --> 00:01:35,338
mix of speech, which is verbal and nonverbal, things like

20
00:01:35,424 --> 00:01:39,190
facial expression, hand gestures, eye motion,

21
00:01:39,270 --> 00:01:43,218
smile, touch, body language and everything, which are nonverbal

22
00:01:43,254 --> 00:01:47,290
signals. So these are basically multimodal

23
00:01:47,450 --> 00:01:51,150
communication methodologies and very complex contextual,

24
00:01:52,390 --> 00:01:56,590
which actually provides a very deep complementary

25
00:01:56,670 --> 00:02:00,606
waves than any model methods

26
00:02:00,638 --> 00:02:02,900
or signals of communication. Right,

27
00:02:04,550 --> 00:02:08,114
next, about how a human communicates or interacts

28
00:02:08,162 --> 00:02:11,526
with a machine, either today or in the future.

29
00:02:11,708 --> 00:02:16,098
So the next cofounder could be one that combines forward and nonverbal signals

30
00:02:16,194 --> 00:02:19,894
to enable natural modality of communication between human

31
00:02:20,092 --> 00:02:23,080
to computer, between computer to computer as well.

32
00:02:23,530 --> 00:02:27,862
And it acts as a foundation for collaboration or cooperation between

33
00:02:27,916 --> 00:02:32,266
people and AI systems. It starts

34
00:02:32,298 --> 00:02:35,722
with a graphical interface which can be viewed, clicked,

35
00:02:35,786 --> 00:02:39,594
swiped, touched, pinched, and also that exists

36
00:02:39,722 --> 00:02:42,974
today already. Then there is voice recognition

37
00:02:43,022 --> 00:02:46,914
technology, which is getting improved day by

38
00:02:46,952 --> 00:02:50,846
day. It understands human language, natural language,

39
00:02:50,958 --> 00:02:54,610
much more accurately and take action accordingly, process accordingly.

40
00:02:55,130 --> 00:02:58,630
So it also eliminates the need of inputs,

41
00:02:59,050 --> 00:03:02,310
devices, keyboards and all. So that exists today,

42
00:03:02,380 --> 00:03:06,054
like in the forms of Alexa, which might currently

43
00:03:06,172 --> 00:03:09,478
take your instructions and do actions accordingly,

44
00:03:09,574 --> 00:03:13,354
but has a lot of potential in future to make decisions or

45
00:03:13,392 --> 00:03:16,490
take action on your behalf without your instructions.

46
00:03:17,310 --> 00:03:19,900
Now there is gesture and motion control.

47
00:03:20,210 --> 00:03:23,582
With augmented reality, people are playing or interacting with

48
00:03:23,636 --> 00:03:27,262
digital objects with their gestures in a much

49
00:03:27,316 --> 00:03:30,000
more immersive and interactive way.

50
00:03:30,530 --> 00:03:34,094
And that also includes manipulating the digital objects

51
00:03:34,142 --> 00:03:37,794
in the 3d world. If I haven't heard of the

52
00:03:37,832 --> 00:03:41,314
project Soli from Google, you should check

53
00:03:41,352 --> 00:03:44,642
it out. So it's a multipurpose sensor based

54
00:03:44,696 --> 00:03:48,562
miniature radar, which track human motion. Human has gesture

55
00:03:48,706 --> 00:03:52,546
and take action accordingly. So it's an interpretation of the natural

56
00:03:52,738 --> 00:03:57,538
hand gesture language. You can do

57
00:03:57,564 --> 00:04:00,890
a virtual dialing instead of just

58
00:04:01,040 --> 00:04:04,746
a screen on it, right? So these kind of things are

59
00:04:04,768 --> 00:04:08,298
there. It mimics your action also to do

60
00:04:08,384 --> 00:04:13,882
what you are doing on a virtual plane,

61
00:04:14,026 --> 00:04:17,486
assuming that it is a real one. It also does

62
00:04:17,508 --> 00:04:22,962
not depend upon any ambient light and any

63
00:04:23,016 --> 00:04:26,946
external dependency it has. And also

64
00:04:27,128 --> 00:04:30,766
the sensors are captured in a very compact shape,

65
00:04:30,798 --> 00:04:34,622
which can be even embedded to other devices.

66
00:04:34,766 --> 00:04:38,066
So it comes as a full suit.

67
00:04:38,098 --> 00:04:42,758
SDK another under

68
00:04:42,844 --> 00:04:46,194
research to different types of interfaces

69
00:04:46,242 --> 00:04:49,666
and brain computer interfaces and haptic interfaces. So brain

70
00:04:49,698 --> 00:04:53,898
computer interfaces with neuroscience, with the advancement of neurosciences, people can

71
00:04:53,984 --> 00:04:57,626
actually interact with the machine from their thinking, from directly from

72
00:04:57,648 --> 00:05:01,866
their brain, without doing any verbal or nonverbal

73
00:05:01,898 --> 00:05:04,846
gestures. And haptic interfaces are very interesting.

74
00:05:04,948 --> 00:05:11,566
So this depends upon they

75
00:05:11,588 --> 00:05:14,738
claim that you can actually get physical sensations from a

76
00:05:14,744 --> 00:05:17,986
virtual world, right? So it's a

77
00:05:18,008 --> 00:05:23,970
communication between two set of sensors, and in between there are intelligent

78
00:05:24,390 --> 00:05:29,890
machines, and if you are doing some action

79
00:05:29,970 --> 00:05:33,478
from one side, the other person on the other side can actually feel that.

80
00:05:33,564 --> 00:05:37,914
Right. It's a very interesting thing

81
00:05:37,952 --> 00:05:41,290
that people are actually researching on a lot of sensors and all

82
00:05:41,440 --> 00:05:45,386
are being involved into that another set

83
00:05:45,408 --> 00:05:49,082
of interface, which I feel like it's called no

84
00:05:49,136 --> 00:05:52,222
interface. So it's about a connected world.

85
00:05:52,356 --> 00:05:56,350
Sensors and machine learning algorithm, digital devices

86
00:05:56,690 --> 00:06:00,986
all put together, which can take decisions

87
00:06:01,018 --> 00:06:04,786
on your behalf without you to actually instruct on anything.

88
00:06:04,888 --> 00:06:09,230
It analyzes your environment and augments

89
00:06:09,310 --> 00:06:12,498
your decision making. And it basically

90
00:06:12,584 --> 00:06:16,370
works as an additional brain and additional

91
00:06:16,450 --> 00:06:20,038
hand to what a human being

92
00:06:20,124 --> 00:06:24,054
actually can do, can interact with the

93
00:06:24,092 --> 00:06:28,540
machines. Now, let's go a bit deeper into

94
00:06:29,470 --> 00:06:32,938
generality behind the elephant in the room. So for that, we want

95
00:06:32,944 --> 00:06:36,666
to understand a bit of the history. In 1950s

96
00:06:36,688 --> 00:06:39,910
and 60s, researchers had started

97
00:06:40,000 --> 00:06:43,594
developing simple AI systems to perform smaller

98
00:06:43,642 --> 00:06:47,166
tasks, but these

99
00:06:47,188 --> 00:06:51,438
were very primitive. Then it started maturing in 1980s,

100
00:06:51,524 --> 00:06:55,198
where researchers started creating much more sophisticated AI

101
00:06:55,214 --> 00:06:58,930
algorithms, whereas it actually took

102
00:06:59,080 --> 00:07:02,866
shape in 2010 plus in the last

103
00:07:02,888 --> 00:07:06,230
decade, only with the advent of Gn,

104
00:07:07,130 --> 00:07:10,534
generative adversarial network. So that

105
00:07:10,572 --> 00:07:14,006
was developed by Jan Gottfellow and his colleagues in

106
00:07:14,028 --> 00:07:16,230
2014.

107
00:07:16,890 --> 00:07:20,566
So the major purpose was to generate realistic

108
00:07:20,598 --> 00:07:24,950
images. So it has two models combined

109
00:07:25,030 --> 00:07:28,406
together, one which generates fake images,

110
00:07:28,518 --> 00:07:31,562
another which attempts to distinguish

111
00:07:31,626 --> 00:07:35,466
between a fake image with a real image.

112
00:07:35,578 --> 00:07:38,890
So fundamentally, GN generates

113
00:07:38,970 --> 00:07:42,766
image lookalike with the set of data it

114
00:07:42,788 --> 00:07:46,926
is faded with. That is the fundamental process of Gn.

115
00:07:47,118 --> 00:07:50,786
I'll go deeper into it and discuss on the

116
00:07:50,808 --> 00:07:52,260
architecture a bit.

117
00:07:54,970 --> 00:07:58,440
However, GN is

118
00:07:59,770 --> 00:08:04,246
not the only one in terms of generative like.

119
00:08:04,348 --> 00:08:07,702
Generative, AI can be broadly distinguished into two different

120
00:08:07,756 --> 00:08:11,210
stories, one consisting of GN which used

121
00:08:11,280 --> 00:08:15,254
for text synthesis, image synthesis, music synthesis,

122
00:08:15,302 --> 00:08:18,646
this kind of generative stuff. Another, the language models,

123
00:08:18,678 --> 00:08:22,810
which are mostly used for emerging text. These two are fundamentally

124
00:08:22,890 --> 00:08:26,766
two different set of networks. I'll come into the

125
00:08:26,948 --> 00:08:29,120
much more depth of it.

126
00:08:30,610 --> 00:08:34,466
You are from ML background, you must know the significance of

127
00:08:34,488 --> 00:08:38,226
an objective function. I'm not going into the

128
00:08:38,248 --> 00:08:42,734
detail. So in gams,

129
00:08:42,782 --> 00:08:46,466
what happens? The objective function is based on an accessory training,

130
00:08:46,648 --> 00:08:50,334
where there are two networks. One is the generator network

131
00:08:50,382 --> 00:08:53,718
and other is the discriminator network. While the generator always

132
00:08:53,804 --> 00:08:57,834
stories to minimize the discriminator's ability to distinguish between

133
00:08:57,872 --> 00:09:01,306
the fake and the real image, let's consider image. It can

134
00:09:01,328 --> 00:09:04,300
be for other type of inputs also,

135
00:09:04,750 --> 00:09:08,650
whereas the discriminator always tries to maximize its

136
00:09:08,720 --> 00:09:12,634
ability to distinguish

137
00:09:12,682 --> 00:09:16,046
between the fake and the

138
00:09:16,068 --> 00:09:17,440
real images. Right?

139
00:09:18,930 --> 00:09:22,446
Whereas in language models, what happens? The objective function is

140
00:09:22,468 --> 00:09:26,098
based on maximizing the likelihood of the next word in the

141
00:09:26,104 --> 00:09:31,314
sequence, that is, giving them the previous word. So that's how a

142
00:09:31,352 --> 00:09:34,340
language model, large language model, works.

143
00:09:36,570 --> 00:09:39,814
Also, if I draw an analogy to

144
00:09:39,852 --> 00:09:43,398
how a creative process works for actual human being,

145
00:09:43,564 --> 00:09:47,190
if you think fundamentally everyone

146
00:09:47,260 --> 00:09:50,938
is creative, right? So how do

147
00:09:50,944 --> 00:09:54,870
we leverage creativity? We imagine something, we visualize

148
00:09:54,950 --> 00:09:58,906
that in front of us. There are multiple tools available to

149
00:09:59,008 --> 00:10:03,322
do that. We iterate

150
00:10:03,386 --> 00:10:08,202
on that, we change something, we reimagine

151
00:10:08,266 --> 00:10:11,630
something and visualize again. Then again,

152
00:10:11,700 --> 00:10:15,554
reimagine like that. So that's how creativity is

153
00:10:15,592 --> 00:10:18,340
an iterative process.

154
00:10:18,710 --> 00:10:22,030
If you think fundamentally how a gan

155
00:10:22,110 --> 00:10:25,510
works, it also does the same thing, right? So it

156
00:10:25,580 --> 00:10:29,446
is an iterative process to make the engine more

157
00:10:29,468 --> 00:10:32,854
and more creative. So we are

158
00:10:32,892 --> 00:10:36,594
going towards the direction where GNS

159
00:10:36,642 --> 00:10:40,982
can actually augment the human creativity. Maybe researchers

160
00:10:41,046 --> 00:10:43,340
are going towards that.

161
00:10:45,390 --> 00:10:49,254
Now, a little bit of large language models or language

162
00:10:49,302 --> 00:10:53,674
models. So these are typically neural network called transformers.

163
00:10:53,722 --> 00:10:57,962
Most of the LLMs are based on transformers, which processes

164
00:10:58,026 --> 00:11:01,166
a sequence of data. For this

165
00:11:01,268 --> 00:11:04,818
instance, it is text. What it does, it actually

166
00:11:04,904 --> 00:11:08,274
creates a probability distribution of the next set

167
00:11:08,312 --> 00:11:12,014
of tokens given an already input set of tokens,

168
00:11:12,062 --> 00:11:15,890
right. And from that probability distribution,

169
00:11:15,970 --> 00:11:19,238
it predicts the correct next token in the

170
00:11:19,244 --> 00:11:22,360
sequence, which is given in the previous set of problems.

171
00:11:23,050 --> 00:11:26,406
So fundamentally, LLNs use a function or

172
00:11:26,428 --> 00:11:31,162
a technique called self attention. So it

173
00:11:31,216 --> 00:11:34,618
allows this particular neural network to attend different

174
00:11:34,704 --> 00:11:38,726
parts of the input sequence, or all the input tokens

175
00:11:38,838 --> 00:11:42,970
with different weightage, while making the next prediction.

176
00:11:44,190 --> 00:11:48,110
Elements are generally trained with large amount of data and

177
00:11:48,180 --> 00:11:51,722
sometimes fine tuned for specific use cases, be it legal,

178
00:11:51,786 --> 00:11:55,746
be it writing for your copy or this kind of things,

179
00:11:55,928 --> 00:11:59,140
or it can be a translation or questions and model as well.

180
00:11:59,590 --> 00:12:02,318
So once trained properly,

181
00:12:02,414 --> 00:12:06,694
elements can generative AI next set of

182
00:12:06,812 --> 00:12:10,294
text sampling from the learned probability distribution over

183
00:12:10,332 --> 00:12:13,574
the next set of tokens. So we

184
00:12:13,612 --> 00:12:17,110
discuss about gins and we discuss about language models.

185
00:12:17,630 --> 00:12:21,770
What I fundamentally believe, if these two are combined together,

186
00:12:21,840 --> 00:12:25,894
it has enormous potential to generate actually multimodal

187
00:12:25,942 --> 00:12:30,102
interfaces, which will in turn lead

188
00:12:30,176 --> 00:12:33,520
to creating interactive content and

189
00:12:35,250 --> 00:12:39,120
changing the way how human beings are actually

190
00:12:39,650 --> 00:12:43,982
interacting with the content. Nowadays, which is mostly static

191
00:12:44,046 --> 00:12:47,762
content, I consider video is also

192
00:12:47,816 --> 00:12:51,586
static content because there has not been any major

193
00:12:51,688 --> 00:12:54,958
revolution in the content creation space, in the

194
00:12:54,984 --> 00:12:58,118
content format space. After video,

195
00:12:58,284 --> 00:13:02,006
people have started creating like interactive videos and

196
00:13:02,028 --> 00:13:06,022
those kind of stuff a bit. But there has not been any revolution because

197
00:13:06,156 --> 00:13:09,660
the generation process of that,

198
00:13:10,030 --> 00:13:12,678
including the ideation, creativity,

199
00:13:12,774 --> 00:13:15,558
leveraging till the implementation,

200
00:13:15,654 --> 00:13:19,878
as well as how we consume that is very

201
00:13:19,984 --> 00:13:23,742
complicated. When you talk about generative AI,

202
00:13:23,796 --> 00:13:27,562
multimodal contained as at ease.

203
00:13:27,626 --> 00:13:30,746
So we fundamentally believe at flurgo,

204
00:13:30,778 --> 00:13:34,334
and we have been researching on that, that generative

205
00:13:34,382 --> 00:13:39,090
AI is opening the tools for creating that multimodal

206
00:13:39,910 --> 00:13:43,214
interactivity. Having said that, for large language

207
00:13:43,262 --> 00:13:47,350
models, there are also another school of thought which thinks that

208
00:13:47,500 --> 00:13:50,994
adding more and more parameters to the model, or creating

209
00:13:51,042 --> 00:13:54,342
more and more larger set of models might not be the only

210
00:13:54,396 --> 00:13:57,686
solution for emerging creativity in terms of the

211
00:13:57,708 --> 00:14:01,690
text generation. And I also do fundamentally believe that

212
00:14:01,760 --> 00:14:05,914
because with the likes of data privacy, with the likes of

213
00:14:06,112 --> 00:14:09,782
the need of running a model on your local devices,

214
00:14:09,846 --> 00:14:14,494
on your handheld devices, it is very much necessary for

215
00:14:14,532 --> 00:14:17,120
us as the research community,

216
00:14:18,210 --> 00:14:21,694
as the tech community, to innovate on the type of model or

217
00:14:21,732 --> 00:14:25,102
the type of language

218
00:14:25,166 --> 00:14:28,226
models doing almost the same kind

219
00:14:28,248 --> 00:14:32,174
of purpose. People are thinking towards this direction.

220
00:14:32,222 --> 00:14:36,018
It's just a very exciting and interesting field to

221
00:14:36,104 --> 00:14:41,030
watch out for in the next couple of years into

222
00:14:41,100 --> 00:14:44,760
the architecture of a JN model.

223
00:14:45,770 --> 00:14:49,678
So as I mentioned earlier, it has two different netflows

224
00:14:49,714 --> 00:14:53,462
combined together. One is a generator network, another discriminator

225
00:14:53,526 --> 00:14:57,414
network. So the generator takes a random noise as input

226
00:14:57,462 --> 00:15:01,662
and generative AI data that resembles the training data.

227
00:15:01,796 --> 00:15:05,390
And the discriminator's responsibility is to take

228
00:15:05,460 --> 00:15:11,342
that real training data and distinguish between

229
00:15:11,396 --> 00:15:14,562
the created data, like what is the variance between

230
00:15:14,616 --> 00:15:17,730
that? Right? During the training process,

231
00:15:17,800 --> 00:15:21,714
the generative AI, the discriminator, are trained with an

232
00:15:21,752 --> 00:15:25,446
advisorial manner, where the generative AI to fool the

233
00:15:25,468 --> 00:15:29,090
discriminator, and the discriminator aims to correctly distinguish

234
00:15:29,170 --> 00:15:33,410
between the real and the generated image. And over the time, the generator learns

235
00:15:33,490 --> 00:15:37,320
to generate more increasingly realistic data

236
00:15:38,830 --> 00:15:42,186
that can fool the discriminator. That this

237
00:15:42,208 --> 00:15:46,746
is actually real, but actually it

238
00:15:46,768 --> 00:15:49,690
is a fake data, that is the fundamentals.

239
00:15:50,930 --> 00:15:54,366
And once trained properly, the generator can be

240
00:15:54,388 --> 00:15:57,838
used to generate new set of data, which is similar to the training

241
00:15:57,924 --> 00:16:01,082
data set. So fundamentally,

242
00:16:01,226 --> 00:16:05,346
language models are getting more and more competent in

243
00:16:05,368 --> 00:16:08,862
terms of understanding the human language

244
00:16:08,926 --> 00:16:13,540
and process that. So it is getting like

245
00:16:14,070 --> 00:16:17,766
application across the industries, not only in

246
00:16:17,868 --> 00:16:22,754
generating new text, rather also in predicting

247
00:16:22,802 --> 00:16:26,722
or suggesting on any kind of textual analysis

248
00:16:26,866 --> 00:16:28,140
related data.

249
00:16:31,070 --> 00:16:35,174
For GN, fundamentally, it generates

250
00:16:35,222 --> 00:16:38,698
the output which is lookalike or which is similar

251
00:16:38,784 --> 00:16:42,362
to the training data set. And that's how the entire model is structured,

252
00:16:42,426 --> 00:16:46,222
architected. Right? So we can say

253
00:16:46,276 --> 00:16:51,278
that the model may be actually working

254
00:16:51,364 --> 00:16:54,926
on a crowdsourced data it is presented

255
00:16:54,958 --> 00:16:59,246
with, and understanding how genetic

256
00:16:59,278 --> 00:17:02,834
models work and why they produce the output they

257
00:17:02,872 --> 00:17:07,430
do. Right now, it's very important in terms of the research direction.

258
00:17:09,050 --> 00:17:12,486
This also includes analyzing the internal representation of

259
00:17:12,508 --> 00:17:16,150
the model, how they are learning,

260
00:17:16,300 --> 00:17:20,610
and how they are explaining

261
00:17:20,690 --> 00:17:24,074
or how the model is

262
00:17:24,272 --> 00:17:27,658
getting explained or interpreted in terms of how they

263
00:17:27,664 --> 00:17:31,390
are generative AI output. So that

264
00:17:31,460 --> 00:17:35,726
is the fundamental idea

265
00:17:35,828 --> 00:17:39,200
of generating something

266
00:17:41,090 --> 00:17:44,814
which is lookalike or which is influenced by

267
00:17:44,852 --> 00:17:48,542
a crowdsourced creativity which is fed into the generative

268
00:17:48,606 --> 00:17:50,130
and physical network.

269
00:17:51,830 --> 00:17:55,066
Fundamentally, generative AI is at a very nascent stage,

270
00:17:55,118 --> 00:17:59,026
very fundamental building block stage. And apart

271
00:17:59,058 --> 00:18:02,294
from watching out for the explainability, there are

272
00:18:02,332 --> 00:18:03,800
multiple other things.

273
00:18:05,130 --> 00:18:09,178
From the model understanding perspective, it is worth

274
00:18:09,264 --> 00:18:13,610
watching out for. So people are working

275
00:18:13,680 --> 00:18:17,878
towards generating more improved training techniques.

276
00:18:18,054 --> 00:18:22,078
One area of active research is developing better techniques for

277
00:18:22,244 --> 00:18:26,090
training generative models, such has more stable and efficient optimization

278
00:18:26,170 --> 00:18:30,042
algorithm methods, or avoiding mode of collapse

279
00:18:30,106 --> 00:18:34,062
of gn like that. Multimodal generation

280
00:18:34,126 --> 00:18:37,506
is also being looked at

281
00:18:37,688 --> 00:18:41,646
very closely. Different techniques

282
00:18:41,678 --> 00:18:45,226
are getting developed into that. So how we can generate

283
00:18:45,278 --> 00:18:48,806
an immersive content, which is

284
00:18:48,828 --> 00:18:52,262
a combination of text, video, images, audio, everything putting

285
00:18:52,316 --> 00:18:54,470
together, which is multimodal generation.

286
00:18:55,690 --> 00:18:59,126
People are also researching towards controlled generative

287
00:18:59,238 --> 00:19:03,014
AI means like developing generative

288
00:19:03,062 --> 00:19:06,586
models which can be controlled to generate in a

289
00:19:06,608 --> 00:19:09,850
specific direction or in a specific type of output,

290
00:19:11,090 --> 00:19:15,214
which is characterized based

291
00:19:15,252 --> 00:19:19,310
on some defined criteria. And also people

292
00:19:19,380 --> 00:19:23,390
are researching with respect to incorporating

293
00:19:23,550 --> 00:19:27,122
prior knowledge to that prior knowledge to the

294
00:19:27,176 --> 00:19:30,594
generative models. So this includes developing models that

295
00:19:30,632 --> 00:19:34,142
can learn for structured data such as graphs,

296
00:19:34,206 --> 00:19:37,954
tables, or that can also leverage external knowledge

297
00:19:38,002 --> 00:19:39,990
sources such as knowledge graphs.

298
00:19:42,250 --> 00:19:46,274
So where all we are thinking about crowdsourcing interactivity,

299
00:19:46,402 --> 00:19:52,106
one is definitely for idea generation, where different

300
00:19:52,208 --> 00:19:56,060
variations of iterative creative process is

301
00:19:56,590 --> 00:19:58,970
sourced from different sources,

302
00:19:59,550 --> 00:20:03,950
different people who are creative and who are good

303
00:20:04,020 --> 00:20:07,694
at different areas of adding interactivity. Someone might

304
00:20:07,732 --> 00:20:12,190
be good at generating the interfaces, someone might be good at generating

305
00:20:12,550 --> 00:20:17,410
the background. Generative AI assets generating how different

306
00:20:17,480 --> 00:20:20,980
assets interact. What is the logic between

307
00:20:23,510 --> 00:20:27,046
the interaction between different objects, how people are

308
00:20:27,068 --> 00:20:30,774
interact with the objects, generating content and all those

309
00:20:30,812 --> 00:20:34,806
stuff? Experimentation is definitely one

310
00:20:34,828 --> 00:20:39,110
of the areas of crowdsourced

311
00:20:39,690 --> 00:20:43,438
interactivity through generative

312
00:20:43,554 --> 00:20:47,094
for sure, because rapid experimentation leads

313
00:20:47,142 --> 00:20:50,566
to more and more creative content

314
00:20:50,688 --> 00:20:54,842
and rapid creating

315
00:20:54,906 --> 00:20:58,400
hype in that particular direction, that particular build.

316
00:20:59,170 --> 00:21:02,850
Refinement is definitely another very important area.

317
00:21:02,920 --> 00:21:06,290
So when I get something

318
00:21:06,440 --> 00:21:10,510
in front of me, I use AI

319
00:21:10,590 --> 00:21:13,774
to refine that process. Let's say extending the background

320
00:21:13,822 --> 00:21:17,334
to an infinite image, getting the background much

321
00:21:17,372 --> 00:21:24,834
more crisp, tweaking with the brightness,

322
00:21:24,962 --> 00:21:28,470
the other aspects of the interfaces,

323
00:21:28,550 --> 00:21:33,094
and also with the interactions, how people swipe,

324
00:21:33,142 --> 00:21:36,314
how people do animations on top of it.

325
00:21:36,352 --> 00:21:39,754
Everything needs a lot of refinement. When I able

326
00:21:39,792 --> 00:21:43,082
to see that and I get an AI

327
00:21:43,146 --> 00:21:46,526
assistant with me to work on that,

328
00:21:46,628 --> 00:21:48,830
that process becomes much more smoother.

329
00:21:49,330 --> 00:21:52,706
And one very crucial area is

330
00:21:52,728 --> 00:21:56,414
generating multimodal interfaces. As I mentioned, that generative

331
00:21:56,462 --> 00:22:01,422
AI has a potential to democratize creation

332
00:22:01,486 --> 00:22:03,810
of text, video, images,

333
00:22:04,310 --> 00:22:08,194
audio, everything put together to create a multimodal immersive

334
00:22:08,242 --> 00:22:13,094
experience, both in 2d

335
00:22:13,132 --> 00:22:15,110
as well as in the 3d panel.

336
00:22:16,330 --> 00:22:18,220
Personalization, right?

337
00:22:20,190 --> 00:22:24,330
So the future of content is going to be adaptive and personalized

338
00:22:24,990 --> 00:22:28,522
based on how people are interacting with it, right? For example,

339
00:22:28,576 --> 00:22:31,246
you're creating an employee onboarding process.

340
00:22:31,428 --> 00:22:34,494
It has to be interactive. It has to be personalized for

341
00:22:34,532 --> 00:22:37,760
each of the employee so that they

342
00:22:38,690 --> 00:22:42,160
get that value out of it and they engage with your

343
00:22:42,630 --> 00:22:44,660
particular set of onboarding content.

344
00:22:46,550 --> 00:22:50,974
This is my most favorite area, how we are experimenting

345
00:22:51,022 --> 00:22:54,180
with a particular example.

346
00:22:55,430 --> 00:22:59,526
At the end of the next few

347
00:22:59,628 --> 00:23:02,726
slides, you will get to know what we are trying to build here.

348
00:23:02,828 --> 00:23:06,534
I'm just giving you a few context. So if you are actually trying

349
00:23:06,572 --> 00:23:09,754
to create an engaging and interactive content, you'll need different

350
00:23:09,792 --> 00:23:13,434
set of features which exist today in come form

351
00:23:13,472 --> 00:23:17,414
or other, and put things together with an AI assisted

352
00:23:17,462 --> 00:23:21,650
way. So the first thing is image synthesis.

353
00:23:21,830 --> 00:23:25,774
It can be synthesized from an existing real image that

354
00:23:25,892 --> 00:23:29,482
gets enhanced with AI, or it can be completely generated

355
00:23:29,546 --> 00:23:33,410
with prompts. This is an example that I have created for this particular

356
00:23:33,480 --> 00:23:38,002
talk. So I choose to create this

357
00:23:38,136 --> 00:23:41,806
from scratch with stable diffusion

358
00:23:41,918 --> 00:23:45,906
through prompts. And I have given example of two or

359
00:23:45,928 --> 00:23:47,890
three different prompts.

360
00:23:49,290 --> 00:23:52,790
Next, let's say I have to make this landscape infinite,

361
00:23:53,210 --> 00:23:58,294
so I can imagine

362
00:23:58,422 --> 00:24:02,460
it as a long platformer, sort of

363
00:24:03,470 --> 00:24:07,114
game like interface. So I'm doing

364
00:24:07,152 --> 00:24:10,258
that with AI itself, and that is context.

365
00:24:10,374 --> 00:24:14,490
If you see the infinite landscape,

366
00:24:14,570 --> 00:24:17,946
it is similar to the one image that I've chosen

367
00:24:18,138 --> 00:24:19,760
from the generated one,

368
00:24:23,030 --> 00:24:27,300
which is the unicorn like

369
00:24:28,470 --> 00:24:31,730
walking on the working on running on the surface of Mars.

370
00:24:34,070 --> 00:24:37,602
Now I integrate with a language model like GP

371
00:24:37,666 --> 00:24:41,542
four to generate an idea where

372
00:24:41,596 --> 00:24:45,122
I give the prompt, let's say a gamified startup

373
00:24:45,186 --> 00:24:48,614
idea validator. It gave

374
00:24:48,652 --> 00:24:51,994
me the name unique order. I can choose my own name

375
00:24:52,032 --> 00:24:55,466
as well and I be

376
00:24:55,488 --> 00:24:59,846
able to launch it for the early stage startup founders as

377
00:24:59,968 --> 00:25:04,026
my perfect PG. So language models

378
00:25:04,058 --> 00:25:08,080
are so powerful nowadays that it can not only

379
00:25:08,610 --> 00:25:13,190
give you or generate the questionnaires

380
00:25:13,370 --> 00:25:16,706
of the format of the entire game, but also it

381
00:25:16,728 --> 00:25:20,850
can analyze if the PG

382
00:25:21,270 --> 00:25:25,422
who are interacting with the particular content

383
00:25:25,576 --> 00:25:29,030
is giving some responses, can also analyze,

384
00:25:30,170 --> 00:25:34,040
give them suggestions, and finally, it can

385
00:25:34,490 --> 00:25:38,042
give you a come based on the clarity of idea

386
00:25:38,096 --> 00:25:42,122
that you have as a startup founder for this particular use

387
00:25:42,176 --> 00:25:48,426
case. So I'll just quickly take

388
00:25:48,448 --> 00:25:51,600
you through the next step where

389
00:25:52,610 --> 00:25:56,286
I have taken a screenshot of someone actually playing the

390
00:25:56,308 --> 00:26:00,266
game. Now it asks

391
00:26:00,298 --> 00:26:03,970
you first about creating

392
00:26:04,870 --> 00:26:07,540
write down your startup idea.

393
00:26:08,630 --> 00:26:13,262
Now let's say cofounder

394
00:26:13,326 --> 00:26:16,642
responds with something called I am planning to create a decentralized

395
00:26:16,706 --> 00:26:19,846
investment platform. Now it guides me through the

396
00:26:19,868 --> 00:26:23,174
next set of questions. What are the things I am as

397
00:26:23,212 --> 00:26:26,902
a startup founder once you validate and whether

398
00:26:26,956 --> 00:26:30,470
I have clarity on that, based on that, it gives me come score.

399
00:26:32,190 --> 00:26:35,766
I have questions for you. So how do you the fractional

400
00:26:35,798 --> 00:26:40,174
ownership part, as it is not

401
00:26:40,212 --> 00:26:43,466
legal in India, the cofounder

402
00:26:43,498 --> 00:26:46,686
says okay, I'll form an LLP and it

403
00:26:46,868 --> 00:26:50,446
holds the share in the assets. It validates and

404
00:26:50,468 --> 00:26:53,886
says okay, that is a good thing. Now how you validate

405
00:26:53,918 --> 00:26:56,990
your market size or the growth potential,

406
00:26:57,070 --> 00:27:00,754
right? So it creates as

407
00:27:00,792 --> 00:27:04,594
a creator, if you want to create something which

408
00:27:04,632 --> 00:27:08,120
is a startup idea validator and wants to launch it for your

409
00:27:10,570 --> 00:27:15,106
target audience, which is the startup

410
00:27:15,138 --> 00:27:18,170
founders. So that's about it.

411
00:27:18,240 --> 00:27:22,010
It creates an entire interactive interfaces, even that

412
00:27:22,080 --> 00:27:25,734
UI can be created

413
00:27:25,862 --> 00:27:29,446
through generative. A lot of tools are already existing

414
00:27:29,478 --> 00:27:32,874
like Gallery U and all. So we are integrating everything together at

415
00:27:32,912 --> 00:27:38,366
Florida to create the entire thing. And it

416
00:27:38,388 --> 00:27:41,582
can utilize for employee onboarding, can utilize for customer

417
00:27:41,716 --> 00:27:45,182
acquisition a lot of other places where you

418
00:27:45,236 --> 00:27:48,826
want to stand out as a content creator,

419
00:27:48,858 --> 00:27:52,094
as a brand, has a marketeer, has an employer with

420
00:27:52,132 --> 00:27:56,120
respect to your content form. That's it

421
00:27:58,890 --> 00:28:02,310
like us formally currently

422
00:28:02,380 --> 00:28:05,350
is very in an ascent format.

423
00:28:06,570 --> 00:28:11,430
As a startup founder you are

424
00:28:11,500 --> 00:28:15,046
growing at a 20% easier. It manages

425
00:28:15,078 --> 00:28:18,474
and says has a good potential. Then it

426
00:28:18,512 --> 00:28:23,246
asks about my awareness with respect to the

427
00:28:23,268 --> 00:28:26,974
GPM, how I'm going to create awareness about my

428
00:28:27,012 --> 00:28:29,840
product like that. So it's all about the idea.

429
00:28:32,630 --> 00:28:36,420
Lastly, we're living at a very exciting times where I believe

430
00:28:38,230 --> 00:28:41,934
gone are the years where you generally operate

431
00:28:41,982 --> 00:28:45,970
your machines or operate your systems.

432
00:28:46,570 --> 00:28:49,960
You will rather cooperate with your machines. In the future

433
00:28:50,730 --> 00:28:53,190
it will be an augmented intelligence,

434
00:28:56,490 --> 00:29:01,894
but it

435
00:29:01,932 --> 00:29:08,178
is going towards a direction where AI

436
00:29:08,274 --> 00:29:11,070
will not only act as what assistant,

437
00:29:12,850 --> 00:29:17,370
can also take decisions on your behalf, can enhance your creativity,

438
00:29:17,530 --> 00:29:20,906
can augment your creativity with the craft source

439
00:29:20,938 --> 00:29:24,570
creativity. And we at Turbo

440
00:29:24,650 --> 00:29:28,466
are going towards a direction where we believe the

441
00:29:28,488 --> 00:29:31,982
future of storytelling is going to be immersive and interactive.

442
00:29:32,046 --> 00:29:36,390
And in order to facilitate that, in order to bring more and more creators onto

443
00:29:36,810 --> 00:29:40,760
creating immersive and interactive experience, not only

444
00:29:41,210 --> 00:29:44,694
the formats which are given and restricted by

445
00:29:44,892 --> 00:29:48,722
the social media platforms, nowadays you need to have the native

446
00:29:48,786 --> 00:29:51,554
AI innovating in those directions,

447
00:29:51,602 --> 00:29:55,590
innovating in actually creating multimodal interfaces

448
00:29:56,730 --> 00:30:00,206
for storytelling at different use cases. Thank you

449
00:30:00,228 --> 00:30:01,258
guys for joining.

