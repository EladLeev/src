1
00:00:30,930 --> 00:00:34,354
Hello everyone, and welcome to my talk titled GPT,

2
00:00:34,482 --> 00:00:37,282
revolutionizing monitoring and logging systems.

3
00:00:37,426 --> 00:00:41,506
So the AI you've been hearing about on the news mostly

4
00:00:41,538 --> 00:00:44,946
refers to llms, which includes chat,

5
00:00:44,978 --> 00:00:48,854
GPT, bard, my AI from

6
00:00:48,892 --> 00:00:53,146
Snapchat, and so on. Llms are called large language

7
00:00:53,178 --> 00:00:57,406
models, and they can be roughly thought of as a system that

8
00:00:57,508 --> 00:01:01,514
generates likely strings in the same grammar that follows can input string.

9
00:01:01,642 --> 00:01:05,066
It's important to note that these are not magic,

10
00:01:05,258 --> 00:01:08,718
they are not self aware, and they don't

11
00:01:08,734 --> 00:01:12,658
retain any memory of anything that's been said to them beyond what

12
00:01:12,744 --> 00:01:15,886
was in the prompts that you provided at the

13
00:01:15,928 --> 00:01:19,442
time of this recording. The big LLM providers

14
00:01:19,506 --> 00:01:22,566
are OpenAI, which is the market leader in the space,

15
00:01:22,668 --> 00:01:25,366
offers the most sophisticated models available today,

16
00:01:25,548 --> 00:01:29,094
Google, with their Bard AI and stability AI,

17
00:01:29,222 --> 00:01:33,046
which is open source and can run on your own infrastructure.

18
00:01:33,238 --> 00:01:37,180
It's important to note that logs are liars often,

19
00:01:38,670 --> 00:01:40,140
or sometimes rather.

20
00:01:42,050 --> 00:01:45,162
While they generate likely strings, these don't necessarily

21
00:01:45,226 --> 00:01:49,322
mean accurate strings, and they're best suited to workflows

22
00:01:49,386 --> 00:01:52,694
where there's some human review, and creating

23
00:01:52,762 --> 00:01:55,854
better prompts gives you dramatically better results.

24
00:01:55,982 --> 00:01:58,830
So how do we create those better prompts?

25
00:01:58,990 --> 00:02:03,330
Most people use GPT and other llms

26
00:02:04,070 --> 00:02:08,280
by giving it what are called single shot prompts, where you

27
00:02:08,730 --> 00:02:12,214
give it a question and expect some

28
00:02:12,252 --> 00:02:15,890
output back. Here is an example of a single shot prompt

29
00:02:15,970 --> 00:02:19,846
where we ask it to create a list of characters from

30
00:02:19,868 --> 00:02:24,378
the book Kadoon and the format of first letter of first name

31
00:02:24,544 --> 00:02:27,706
and last name concatenated together as you would

32
00:02:27,728 --> 00:02:31,386
usually do for usernames. As you can see,

33
00:02:31,488 --> 00:02:35,086
it didn't quite give us the results we wanted, though it gave us

34
00:02:35,108 --> 00:02:38,846
something that loosely looks like the results we wanted. So how can

35
00:02:38,868 --> 00:02:42,650
we improve this? We can improve it by

36
00:02:42,740 --> 00:02:44,450
giving it some examples.

37
00:02:45,510 --> 00:02:49,940
Here we give it the example of Paul Atreides. We want the

38
00:02:50,470 --> 00:02:53,698
username P. Atreides for that.

39
00:02:53,864 --> 00:02:57,558
So as you can see, it generated a list

40
00:02:57,644 --> 00:03:00,982
of names that better fit

41
00:03:01,036 --> 00:03:04,438
the output that we're looking for. Knowing that

42
00:03:04,604 --> 00:03:08,166
providing it examples gives us better results. How can

43
00:03:08,188 --> 00:03:11,626
we use that knowledge to improve applications that

44
00:03:11,648 --> 00:03:15,542
we're building? With OpenAI, we can do what is called tuning

45
00:03:15,686 --> 00:03:19,770
as well. A tuned model can be thought of as mini shot.

46
00:03:20,290 --> 00:03:24,266
OpenAI allows you to provide json formatted

47
00:03:24,298 --> 00:03:28,046
data of example prompts and example outputs that

48
00:03:28,068 --> 00:03:31,294
we want. For those prompts, we can send many,

49
00:03:31,332 --> 00:03:35,294
many prompts, prompts, and example outputs this way to OpenAI

50
00:03:35,342 --> 00:03:38,610
to tune our model. And it's highly recommended that you do this,

51
00:03:38,680 --> 00:03:42,274
as it creates much higher quality results and also

52
00:03:42,312 --> 00:03:46,802
reduces costs significantly because we're no longer putting those examples

53
00:03:46,866 --> 00:03:50,070
into our prompts, which counts against our license.

54
00:03:50,490 --> 00:03:54,422
Here's how we can use GPT to

55
00:03:54,476 --> 00:03:57,986
improve our monitoring and logging. Say we have a raw

56
00:03:58,018 --> 00:04:00,954
log file. As you can see,

57
00:04:00,992 --> 00:04:03,740
it's not too easy to work with that as is,

58
00:04:04,270 --> 00:04:08,298
and we can give it a handful of examples of the specific

59
00:04:08,384 --> 00:04:12,106
fields we want it to extract from that raw

60
00:04:12,138 --> 00:04:16,154
log event. As you can see, it pulled

61
00:04:16,202 --> 00:04:20,506
out all of those individual fields. Chat GPT's response

62
00:04:20,618 --> 00:04:23,954
is highlighted in green here. Once we have those, we can then

63
00:04:23,992 --> 00:04:27,394
say let's write some regexes that pull

64
00:04:27,432 --> 00:04:31,294
out the field values for each of those fields,

65
00:04:31,422 --> 00:04:34,546
and you can see it didn't always quite do the

66
00:04:34,568 --> 00:04:38,470
best job. But this is a great jumping off point for building

67
00:04:38,620 --> 00:04:42,838
regexes that we can then use in our application. Here's an example

68
00:04:42,924 --> 00:04:46,066
of where we can use GPT to help us out with our monitoring

69
00:04:46,098 --> 00:04:50,226
and logging. This is can event from Azure ad describing

70
00:04:50,258 --> 00:04:53,766
a user being disabled using that few shot method discussed

71
00:04:53,798 --> 00:04:57,494
earlier. We can give it some fields and their field values

72
00:04:57,542 --> 00:05:00,506
and ask it to get the rest of the fields and field values from that

73
00:05:00,528 --> 00:05:03,502
raw log event. As you can see, it did that.

74
00:05:03,636 --> 00:05:06,814
We can then ask it to write regexes that extract each

75
00:05:06,852 --> 00:05:10,666
of those fields, which it did with mixed

76
00:05:10,698 --> 00:05:14,322
results, but it's a great jumping off point to build

77
00:05:14,376 --> 00:05:17,954
the rest of your field extractions. Where GPT really

78
00:05:17,992 --> 00:05:21,298
shines is summarizing large amounts of data into

79
00:05:21,384 --> 00:05:25,202
something human readable. So here's that raw log

80
00:05:25,256 --> 00:05:28,690
event from Azure ad that describes a user being disabled.

81
00:05:28,850 --> 00:05:32,054
As you can see, it's not too human readable as

82
00:05:32,092 --> 00:05:35,446
is, unless you spent quite a bit

83
00:05:35,468 --> 00:05:39,290
of time looking at azure logs. So what can we do to

84
00:05:39,360 --> 00:05:43,450
make that immediately recognizable to a analyst?

85
00:05:43,790 --> 00:05:47,706
We can ask GPT to summarize it for us, and as you

86
00:05:47,728 --> 00:05:51,930
can see, it did a fantastic job of that described

87
00:05:52,910 --> 00:05:57,018
the core directory service where that law came from. It turned

88
00:05:57,034 --> 00:06:00,506
that isolinear timestamp into something human readable,

89
00:06:00,698 --> 00:06:04,018
and it described what the acting user was and what the

90
00:06:04,024 --> 00:06:06,850
target user was and the result of that operation.

91
00:06:07,190 --> 00:06:11,006
This was done using very little training. It was immediately

92
00:06:11,038 --> 00:06:15,082
useful to us. How can we do this automatically

93
00:06:15,166 --> 00:06:18,614
with Kibana? Kibana plugins are written in

94
00:06:18,652 --> 00:06:21,558
typescript, so they're pretty easy to work with,

95
00:06:21,724 --> 00:06:25,798
and elastic offers a template plugin on

96
00:06:25,804 --> 00:06:29,260
their GitHub page. Highly recommend you take a look at that and

97
00:06:30,190 --> 00:06:33,050
just building off that template,

98
00:06:33,390 --> 00:06:36,460
add the API integrations you want to use.

99
00:06:36,990 --> 00:06:40,986
There's also helpful guides out there if you want to create your own kibana

100
00:06:41,018 --> 00:06:44,206
plugin from scratch. With the kibana plugin that we put

101
00:06:44,228 --> 00:06:47,534
together, we were able to give

102
00:06:47,572 --> 00:06:51,630
it that original log file and then have GPT add

103
00:06:51,700 --> 00:06:55,790
a description to that log, which we then stored.

104
00:06:55,950 --> 00:06:59,794
This is much more readable to a human analyst who would be reading through

105
00:06:59,832 --> 00:07:03,470
this. Some caveats with building plugins

106
00:07:03,550 --> 00:07:06,834
that interface with OpenAI is there is a token

107
00:07:06,882 --> 00:07:11,190
limit, which we'll talk about later, that basically specifies

108
00:07:11,530 --> 00:07:14,806
that we can only send some amount of data

109
00:07:14,908 --> 00:07:18,310
to the API and get some amount back from OpenAI.

110
00:07:19,470 --> 00:07:23,100
So the raw event we're sending it may not fit,

111
00:07:23,790 --> 00:07:27,594
and we want to be careful to trim what data we send to only

112
00:07:27,632 --> 00:07:31,514
what's necessary. It's also worth noting that this raw event may contain

113
00:07:31,562 --> 00:07:35,162
information you don't want to send, including client

114
00:07:35,226 --> 00:07:39,998
ids, specific usernames, and so on. So working

115
00:07:40,084 --> 00:07:43,666
with the OpenAI API, there's a

116
00:07:43,688 --> 00:07:47,458
number of different parameters we can look at the

117
00:07:47,544 --> 00:07:50,020
model, the prompt itself,

118
00:07:50,550 --> 00:07:53,922
temperature max tokens, top p

119
00:07:53,976 --> 00:07:58,230
frequency okay, let's talk about some OpenAI parameters

120
00:07:59,050 --> 00:08:02,886
for each of these. There's often analogous parameters for

121
00:08:02,908 --> 00:08:04,150
other llms.

122
00:08:05,290 --> 00:08:09,610
Temperature is a value that describes how random we want the model

123
00:08:09,680 --> 00:08:13,578
to behave. It's a flute between zero and two,

124
00:08:13,744 --> 00:08:17,094
where zero is instructing the model to behave

125
00:08:17,142 --> 00:08:20,366
completely deterministically, where one prompt will

126
00:08:20,388 --> 00:08:23,742
always give us the same answer back and higher

127
00:08:23,796 --> 00:08:27,278
values will get more random and varied answers back.

128
00:08:27,444 --> 00:08:31,050
It's advisable that you have a low temperature

129
00:08:31,130 --> 00:08:34,786
for things where determinism is valuable. That can

130
00:08:34,808 --> 00:08:38,722
be field extractions, creating regexes, and higher

131
00:08:38,776 --> 00:08:42,718
values, where it's providing

132
00:08:42,894 --> 00:08:46,214
more human readable responses back, that can be

133
00:08:46,252 --> 00:08:50,386
summarizations, and so on. Tokens describe

134
00:08:50,498 --> 00:08:54,066
the max amount of data that can be sent and received

135
00:08:54,098 --> 00:08:55,750
in response to OpenAI.

136
00:08:57,210 --> 00:09:01,238
Both the data sent and the data you get back are summed

137
00:09:01,254 --> 00:09:04,780
together to see if they hit that max token limit or not.

138
00:09:06,590 --> 00:09:09,514
One token is loosely one word,

139
00:09:09,712 --> 00:09:13,674
though sometimes several words can be added together to equal

140
00:09:13,722 --> 00:09:17,374
one token. Top P returns more

141
00:09:17,412 --> 00:09:20,846
probabilistic answers back, where the lower the

142
00:09:20,868 --> 00:09:24,430
value, the more probabilistic

143
00:09:24,510 --> 00:09:28,098
answers are returned. So, for example,

144
00:09:28,184 --> 00:09:31,682
0.1 will represent the top 10% of

145
00:09:31,736 --> 00:09:34,894
possible answers OpenAI might generate,

146
00:09:35,022 --> 00:09:38,406
and it will only give you results that came out

147
00:09:38,428 --> 00:09:41,798
of that top ten. It's recommended that you set

148
00:09:41,884 --> 00:09:45,266
either top P or temperature if you want to increase

149
00:09:45,298 --> 00:09:48,794
or decrease the randomness of your responses, but not

150
00:09:48,832 --> 00:09:52,582
both. Frequency penalty decreases the likeliness

151
00:09:52,646 --> 00:09:54,780
for OpenAI to repeat itself.

152
00:09:56,030 --> 00:10:00,650
It's a float value between zero and one, and presence penalty

153
00:10:01,010 --> 00:10:04,682
decreases the likeliness, or, excuse me, it increases the likeliness

154
00:10:04,746 --> 00:10:07,390
for OpenAI to talk about new topics.

155
00:10:08,050 --> 00:10:11,230
Finally, there's the model of which

156
00:10:11,300 --> 00:10:15,106
OpenAI has many different models worth taking a look at.

157
00:10:15,288 --> 00:10:18,818
DaVinci three is the most

158
00:10:18,984 --> 00:10:22,546
sophisticated GPT-3 model out right now.

159
00:10:22,648 --> 00:10:25,870
It can provide the most detailed and creative responses,

160
00:10:26,030 --> 00:10:30,580
though other models might be faster, they might be lower cost,

161
00:10:32,010 --> 00:10:35,026
they might be more suited to specific subjects,

162
00:10:35,058 --> 00:10:37,830
such as code generation, and so on.

163
00:10:37,980 --> 00:10:41,830
Finally, let's talk about some privacy and confidentiality considerations.

164
00:10:41,990 --> 00:10:45,274
All right, here's some open AI models that would be of

165
00:10:45,312 --> 00:10:48,938
interest to somebody viewing this presentation. GPT four

166
00:10:49,024 --> 00:10:53,086
is by wide margin the most capable large language model out today.

167
00:10:53,188 --> 00:10:56,606
At the time of this recording, it is a bit expensive to

168
00:10:56,628 --> 00:11:00,170
use, so I wouldn't recommend using this for bulk tasks.

169
00:11:00,330 --> 00:11:03,762
Maybe summarization if a high deal

170
00:11:03,816 --> 00:11:06,770
of sophistication is needed in the response.

171
00:11:07,750 --> 00:11:11,362
For the most part, I recommend using GPT-3 five

172
00:11:11,416 --> 00:11:14,990
turbo, which is a very capable three five model.

173
00:11:15,160 --> 00:11:18,566
It's optimized for the sort of work that

174
00:11:18,668 --> 00:11:22,182
this presentation has covered and can be used

175
00:11:22,236 --> 00:11:25,430
very cheaply. However, if you're looking to perform

176
00:11:25,500 --> 00:11:29,378
tasks that would require a large amount of tokens,

177
00:11:29,554 --> 00:11:33,610
that it would be summarizing very large log files.

178
00:11:34,190 --> 00:11:37,002
If you're looking for very detailed responses back,

179
00:11:37,136 --> 00:11:41,766
GPT 432K is also fantastic.

180
00:11:41,878 --> 00:11:45,338
As you can see on this table here, you can use far more tokens

181
00:11:45,354 --> 00:11:49,274
with that than you can with any other model. Finally, let's talk about some privacy

182
00:11:49,322 --> 00:11:51,390
and confidentiality considerations.

183
00:11:52,210 --> 00:11:57,854
OpenAI and their privacy agreement does

184
00:11:57,892 --> 00:12:01,422
say that they don't use any data sent to them via their API for training

185
00:12:01,476 --> 00:12:04,670
new models, so you don't have to fear

186
00:12:05,370 --> 00:12:08,742
secrets sensitive being included in any training

187
00:12:08,796 --> 00:12:12,390
set for future models. However, they do not say

188
00:12:12,460 --> 00:12:16,086
they don't retain logs or other properties about what might have been

189
00:12:16,108 --> 00:12:19,594
sent to them via the API. So always be careful when

190
00:12:19,632 --> 00:12:23,770
sending any sensitive data to a third party that might include

191
00:12:24,910 --> 00:12:27,290
keys, secrets,

192
00:12:27,630 --> 00:12:30,526
client ids, usernames, if you don't want to disclose that,

193
00:12:30,548 --> 00:12:34,554
and so on. If you're looking for privacy, I highly

194
00:12:34,602 --> 00:12:38,894
recommend looking at stability AI's open

195
00:12:38,932 --> 00:12:41,790
source models, which you can run locally on your own hardware.

196
00:12:42,210 --> 00:12:45,502
Hopefully this presentation has been very informative

197
00:12:45,566 --> 00:12:48,994
for you, and if you have any questions, feel free to reach

198
00:12:49,032 --> 00:12:53,378
out to me. Clay Langston at Oak nine

199
00:12:53,544 --> 00:12:57,590
finally, let's talk about some privacy and confidentiality considerations.

200
00:12:58,890 --> 00:13:02,070
With many large language models out there today,

201
00:13:02,140 --> 00:13:05,730
data sent to them is used to train future models,

202
00:13:05,810 --> 00:13:09,698
which you would not want them to do with a lot of the data you'd

203
00:13:09,714 --> 00:13:12,778
be sending them for what we talked about in this video.

204
00:13:12,944 --> 00:13:16,106
OpenAI and their confidentiality agreement says that

205
00:13:16,128 --> 00:13:19,562
they do not use any content sent to them to train future

206
00:13:19,616 --> 00:13:22,810
models if that content was sent to them via their API.

207
00:13:22,970 --> 00:13:26,702
However, they do not say that they do not retain logs or

208
00:13:26,756 --> 00:13:29,694
other properties about what was sent to them.

209
00:13:29,892 --> 00:13:33,202
So be mindful. Sending sensitive data to a third

210
00:13:33,256 --> 00:13:37,410
party is always a risk, especially if that data might include

211
00:13:37,830 --> 00:13:41,138
secrets, tokens, so on,

212
00:13:41,304 --> 00:13:44,594
or usernames, client ids, or other data like that.

213
00:13:44,632 --> 00:13:48,630
If that's not something you wish to disclose. If you are

214
00:13:48,780 --> 00:13:52,454
very concerned about the sensitivity of your data and who has

215
00:13:52,492 --> 00:13:56,360
it, I highly recommend looking at stability AI and

216
00:13:56,970 --> 00:14:00,246
looking at one of their models that you can run locally on your own

217
00:14:00,268 --> 00:14:04,278
hardware. And that is all. My contact info is

218
00:14:04,444 --> 00:14:07,798
C. Langston at Oak nine IO. Feel free to reach out

219
00:14:07,804 --> 00:14:10,750
to me if you have any questions about anything talked about today,

220
00:14:10,820 --> 00:14:14,060
or if you just want to say hello. All right, thanks everyone.

