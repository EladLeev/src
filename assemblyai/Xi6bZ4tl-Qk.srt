1
00:00:27,010 --> 00:00:30,626
Hello and thank you for watching today's session on AI driven DevOps

2
00:00:30,658 --> 00:00:34,434
CI CD pipeline. This video was prepared by Francesco

3
00:00:34,482 --> 00:00:37,922
and myself. We're both part of an Accenture group of highly motivated

4
00:00:37,986 --> 00:00:41,714
SRE specializing the state of the art SRE DevOps practices

5
00:00:41,842 --> 00:00:45,586
with these goal to promote a growth mindset that embraces agility,

6
00:00:45,698 --> 00:00:49,446
DevOps and SRE as the new normal. I myself have

7
00:00:49,468 --> 00:00:52,826
a background, software engineering, AI and industrial automation.

8
00:00:52,938 --> 00:00:56,394
I specialize in several SRE related topics such as AIops

9
00:00:56,442 --> 00:01:00,202
observability, brand chaos engineering. Thank you very much Mikhaila

10
00:01:00,266 --> 00:01:04,066
welcome everybody. I'm Francesco Sbaraglia and I'm based in

11
00:01:04,088 --> 00:01:07,934
Germany. I'm working as SRE and I ops tech lead EMEA

12
00:01:07,982 --> 00:01:11,790
at Accenture. I have over 20 years of experience solving

13
00:01:11,870 --> 00:01:15,634
production problems in corporate, startup and government.

14
00:01:15,832 --> 00:01:19,102
Furthermore, I have deep experience in automation,

15
00:01:19,246 --> 00:01:23,010
observability, multicloud and flows engineering.

16
00:01:23,090 --> 00:01:26,658
I'm currently growing the SRE DevOps capability

17
00:01:26,834 --> 00:01:30,330
at Accenture. Let's have a look at the agenda for today.

18
00:01:30,480 --> 00:01:34,042
First we will have a look at why why

19
00:01:34,096 --> 00:01:37,270
we need to monitor and observe SCi CD

20
00:01:37,350 --> 00:01:41,354
delivery pipeline. Then we will have a look at the classic continuous

21
00:01:41,402 --> 00:01:45,866
delivery pipelines architecture. We will refresh about observability

22
00:01:46,058 --> 00:01:49,806
and how to use open telemetry in this case. Then we

23
00:01:49,828 --> 00:01:53,566
move to the AI driven approach. We will explain what we

24
00:01:53,588 --> 00:01:56,818
are doing, brand, how we are using AI. We will have a

25
00:01:56,824 --> 00:02:00,814
demo and Michaela will go with the conclusion

26
00:02:00,862 --> 00:02:04,050
and takeaways. Let's move now on the first point,

27
00:02:04,200 --> 00:02:07,826
why to monitor a CI CD pipeline and platform? You can

28
00:02:07,848 --> 00:02:11,286
imagine these CI CD platform is really critical. Let's see our

29
00:02:11,308 --> 00:02:14,774
key challenges. SRE and DevOps needs a CI CD platform

30
00:02:14,892 --> 00:02:18,694
that needs to be reliable brand with predictable failures.

31
00:02:18,822 --> 00:02:22,822
SRE and DevOps needs a data driven approach

32
00:02:22,886 --> 00:02:26,762
to run proactive capacity management when brand how

33
00:02:26,816 --> 00:02:30,678
to increase resources SRE and DevOps needs a CICD

34
00:02:30,774 --> 00:02:34,378
to deploy Oddfix and Bugsfix during outages.

35
00:02:34,554 --> 00:02:38,250
Now you can imagine how a CICD platform became really critical.

36
00:02:38,410 --> 00:02:42,094
Most of the time the CI CD platform is black box. So we

37
00:02:42,132 --> 00:02:45,326
wanted to understand what is happening inside, how we can improve

38
00:02:45,358 --> 00:02:49,006
all processes and also how we can improve each steps

39
00:02:49,038 --> 00:02:52,386
of the CI CD pipeline. Thanks Francesco. Now that

40
00:02:52,408 --> 00:02:56,046
we introduce the key motivation for today's talk, I suggest we dive

41
00:02:56,078 --> 00:02:59,390
into the first major topic which is CICD.

42
00:02:59,550 --> 00:03:03,282
What we see here is the architecture for our CI CD flow integrated

43
00:03:03,346 --> 00:03:06,774
with the observability platform. So let's start from the top left.

44
00:03:06,892 --> 00:03:10,726
Imagine a DevOps engineer who's writing some new code and when he's

45
00:03:10,758 --> 00:03:14,310
done, he commits it to the git repository. Subsequently,

46
00:03:14,390 --> 00:03:17,574
this commit will trigger our CI CD pipeline.

47
00:03:17,702 --> 00:03:21,306
As you can see, we are using Jenkins that after the commit will

48
00:03:21,328 --> 00:03:24,846
auto initialize the pipelines. Specifically, Jenkins will

49
00:03:24,868 --> 00:03:28,702
initialize the dynamic agent pod which we also see here and which

50
00:03:28,756 --> 00:03:32,586
is dependent on the resources that Jenkins takes from the Kubernetes cluster.

51
00:03:32,698 --> 00:03:36,206
In the next step, the changes will be deployed to the production environment,

52
00:03:36,318 --> 00:03:39,490
ultimately reaching our end users. So far,

53
00:03:39,560 --> 00:03:42,674
everything I've talked about is pretty much standard I would say.

54
00:03:42,792 --> 00:03:46,578
But what we're really interested in is how we integrated the

55
00:03:46,584 --> 00:03:50,482
pipeline with our observability platform. In the upcoming slides

56
00:03:50,546 --> 00:03:54,134
we will explain more about observability platform and how we're using the open

57
00:03:54,172 --> 00:03:57,778
telemetry standard to collect data from our Jenkins pipelines.

58
00:03:57,874 --> 00:04:01,138
But right now what we're interested in are the KPIs

59
00:04:01,234 --> 00:04:04,346
that we see on the right hand side that we want to derive from

60
00:04:04,368 --> 00:04:08,026
the data that we're collecting with open telemetry. So I suggest we

61
00:04:08,048 --> 00:04:11,494
analyze one by one. Let's start with the first one, which is speed

62
00:04:11,542 --> 00:04:14,894
of the CD pipeline. I want to

63
00:04:14,932 --> 00:04:18,714
stress the CD part here as we're only talking about the deployments.

64
00:04:18,842 --> 00:04:21,998
Now consider the situation in which an outage occurs and the

65
00:04:22,004 --> 00:04:25,474
DevOps engineer needs to deploy a hot fix as soon has possible.

66
00:04:25,672 --> 00:04:29,202
We are talking about a critical situation because the system

67
00:04:29,256 --> 00:04:32,450
is unstable brand, the end user is expecting a fix,

68
00:04:32,600 --> 00:04:35,986
otherwise there is a risk that they might abandon it and that's something we

69
00:04:36,008 --> 00:04:40,054
want to avoid. So as you can see, by measuring the deployment speed, we also

70
00:04:40,092 --> 00:04:43,798
improve our realtime to recovery. The quicker I manage to deploy the

71
00:04:43,804 --> 00:04:46,550
hotfix, the lower will my MTTR be.

72
00:04:46,700 --> 00:04:50,506
The next KPI that we're deriving is build test success rate.

73
00:04:50,608 --> 00:04:53,718
These one gives us an indication on how many core tests

74
00:04:53,814 --> 00:04:57,526
or unit tests are successfully completed once deployed

75
00:04:57,558 --> 00:05:01,782
to production. Furthermore, we also want to count the total amount of deployments

76
00:05:01,846 --> 00:05:05,374
we have each month per pipelines and possibly per application.

77
00:05:05,572 --> 00:05:09,582
We want to know the lead time for a change or a deployment. This one

78
00:05:09,636 --> 00:05:12,926
is relevant for identifying the time between the moment in

79
00:05:12,948 --> 00:05:16,910
which the DevOps engineer executed a commit to when the deployment

80
00:05:16,990 --> 00:05:20,802
actually took place. Furthermore, we're also measuring the change

81
00:05:20,856 --> 00:05:24,878
success rate, which corresponds to the total number of successful deployments

82
00:05:24,974 --> 00:05:28,274
divided by total. Speaking of which, some of these might

83
00:05:28,312 --> 00:05:31,366
fail, which is why we want to identify the success rate.

84
00:05:31,468 --> 00:05:35,026
And the last one we ee in this slide is the availability of our CI

85
00:05:35,058 --> 00:05:38,738
CD platform, which is obviously a critical KPI.

86
00:05:38,914 --> 00:05:42,294
Now, coming back to the big picture of this architecture.

87
00:05:42,342 --> 00:05:46,026
You see the benefit of collecting this kind of data

88
00:05:46,128 --> 00:05:49,686
and feeding it into the observability platform is that these DevOps

89
00:05:49,718 --> 00:05:53,226
engineer who does the deployments doesn't even need to use Jenkins

90
00:05:53,258 --> 00:05:57,326
to monitor the state of his deployments since all of these data will

91
00:05:57,348 --> 00:06:01,102
be readily processed brand available in the observability platform.

92
00:06:01,236 --> 00:06:04,706
So it's there where our DevOps engineer will go and

93
00:06:04,728 --> 00:06:08,142
check the status. Okay, speaking of observability,

94
00:06:08,286 --> 00:06:12,034
to introduce this concept, observability is the measure of how

95
00:06:12,072 --> 00:06:15,406
well internal states of a system can be inferred from knowledge

96
00:06:15,438 --> 00:06:19,094
of its external outputs. This basically translates to understanding how

97
00:06:19,132 --> 00:06:22,440
the various components in our systems are connected to each other.

98
00:06:22,810 --> 00:06:26,034
And we pose questions such as what are these dependencies?

99
00:06:26,082 --> 00:06:29,322
What are their dependencies? How do they work together? So in other

100
00:06:29,376 --> 00:06:32,630
words, we are introducing transparency and visibility

101
00:06:32,790 --> 00:06:35,386
across our entire end to end system.

102
00:06:35,568 --> 00:06:39,686
Why? Because our ultimate goal is to derive actionable insights

103
00:06:39,718 --> 00:06:43,626
from it. Here are a couple of notions relevant for observability.

104
00:06:43,818 --> 00:06:47,418
We have different sources of data illustrated in the center figures

105
00:06:47,434 --> 00:06:50,286
that we need to observe. As we mentioned,

106
00:06:50,388 --> 00:06:54,414
observability is the ability to measure a system current state based

107
00:06:54,452 --> 00:06:58,094
on the data that it generates, such as flows, metrics, and traces

108
00:06:58,222 --> 00:07:01,986
the so called golden triangle of observability on the

109
00:07:02,008 --> 00:07:05,294
left hand side, while on the right hand side we see the golden

110
00:07:05,342 --> 00:07:08,902
signals, which consist of latency, traffic errors and

111
00:07:08,956 --> 00:07:12,246
saturation. Okay, before we move on

112
00:07:12,268 --> 00:07:16,470
to the AI side of things, it's also really important to understand

113
00:07:16,620 --> 00:07:19,974
how we collect all of this data that we just mentioned. We sre

114
00:07:20,012 --> 00:07:23,754
using opentelemetry, which is an open source standard for generating and

115
00:07:23,792 --> 00:07:26,886
capturing what we ee here traces,

116
00:07:26,998 --> 00:07:30,746
metrics, logs. So basically our golden triangle. On the

117
00:07:30,768 --> 00:07:34,686
right hand side you see a diagram illustrating how exactly

118
00:07:34,788 --> 00:07:38,186
open telemetry works. Slo I suggest we quickly analyze

119
00:07:38,218 --> 00:07:42,266
it. Let's start from the top, where logs, traces and metrics

120
00:07:42,298 --> 00:07:46,078
are generated in our specific case by the prod and the container.

121
00:07:46,174 --> 00:07:49,906
But from the diagram, we see that we're talking about raw data

122
00:07:50,008 --> 00:07:54,014
too. In the middle, we have the open telemetry collector, which enriches

123
00:07:54,062 --> 00:07:57,506
and processes all of this data. Specifically, these enrichment

124
00:07:57,538 --> 00:08:00,854
process occurs in a completely uniform manner for all three

125
00:08:00,892 --> 00:08:05,110
signals. Basically, the collector guarantees that the signals have

126
00:08:05,180 --> 00:08:09,106
exactly the same attribute names and values describing the Kubernetes pod

127
00:08:09,138 --> 00:08:12,218
that they come from. So what happens in our case is that

128
00:08:12,224 --> 00:08:15,494
the exporter takes all of this data from Jenkins.

129
00:08:15,622 --> 00:08:19,110
So we're talking about system flows, app flows, traces,

130
00:08:19,190 --> 00:08:22,766
metallics, et cetera. It correlates all of it and it forwards it

131
00:08:22,788 --> 00:08:26,170
to our backend. So the key benefit from this approach,

132
00:08:26,250 --> 00:08:30,030
from these dynamic approaches is that we don't have to install

133
00:08:30,100 --> 00:08:33,994
anything on each single Jenkins agent. Instead we simply

134
00:08:34,042 --> 00:08:37,506
attach open telemetry to these Jenkins master and this ensures that

135
00:08:37,528 --> 00:08:41,442
we receive all three signals with only one agent.

136
00:08:41,576 --> 00:08:45,598
Thank you very much Mikhail. Now let's have a look on the aidriven approach.

137
00:08:45,694 --> 00:08:48,994
What you can see are we have these different stages. Observe,

138
00:08:49,042 --> 00:08:52,182
engage and act. In observe we are using open

139
00:08:52,236 --> 00:08:56,562
telemetry collector agent to collect all the inside flows,

140
00:08:56,626 --> 00:08:59,650
metrics and traces from the CI CD pipeline,

141
00:08:59,730 --> 00:09:03,110
but also from the underlining platform on the engage.

142
00:09:03,190 --> 00:09:06,746
We are going to run a machine learning prediction. So we are

143
00:09:06,768 --> 00:09:10,406
going to try to understand and correlate if there are disruption.

144
00:09:10,518 --> 00:09:13,514
You see here that we have two different KPIs.

145
00:09:13,642 --> 00:09:17,034
The first one is CI CD reliability.

146
00:09:17,162 --> 00:09:20,510
The second one is CI CD pipeline end to end.

147
00:09:20,580 --> 00:09:24,186
So we are running a smock test every 15 minutes and tests all

148
00:09:24,228 --> 00:09:27,906
stages of a fake pipeline. Then we move on

149
00:09:27,928 --> 00:09:31,954
on the correlation brand prediction. Correlation and prediction will

150
00:09:31,992 --> 00:09:35,102
try to understand if there will be a disruption and to predict

151
00:09:35,166 --> 00:09:38,950
the service alerts of the next Alphan hour. Then this will land

152
00:09:39,100 --> 00:09:43,174
on predictive alerting. So we are sending an alert before something will

153
00:09:43,212 --> 00:09:46,534
happen or in zero touch operation or zero

154
00:09:46,572 --> 00:09:50,070
touch automation. In this case we will have SL filling.

155
00:09:50,150 --> 00:09:53,466
All scripts will run automatically to fix the

156
00:09:53,488 --> 00:09:56,554
CI CD pipeline and platform. In this case we can

157
00:09:56,592 --> 00:09:59,494
also increase the performance.

158
00:09:59,622 --> 00:10:03,298
We can increase the resources that we have in youre CI CD

159
00:10:03,334 --> 00:10:06,494
platform to prevent any disruption that

160
00:10:06,532 --> 00:10:09,774
can happen. Okay, this is a funny slide. What we

161
00:10:09,812 --> 00:10:13,934
did is to try to understand if our clai these are correct. We asked

162
00:10:13,972 --> 00:10:17,234
at chart GBT, we asked at chat GBT if they can give

163
00:10:17,272 --> 00:10:20,914
to us a couple of slais and

164
00:10:20,952 --> 00:10:24,594
you can see here what we selected at the end. So the first

165
00:10:24,632 --> 00:10:28,510
one is about a build success rate. Really interesting because it's also the

166
00:10:28,520 --> 00:10:32,162
one that we are using these we have a build lead time. You can imagine

167
00:10:32,226 --> 00:10:35,718
which kind of measure. We have these and is really important

168
00:10:35,884 --> 00:10:39,386
for our SRE and DevOps text asset rate.

169
00:10:39,488 --> 00:10:42,634
We have deployment success rate, deployment lead

170
00:10:42,672 --> 00:10:46,426
time. Really interesting one because it's the one also that we are having in

171
00:10:46,448 --> 00:10:49,914
our prediction and machine learning will run brand.

172
00:10:49,952 --> 00:10:53,854
We'll try to understand if there are correlation with the other event brand

173
00:10:53,892 --> 00:10:57,134
then what we are using here is also change failures rate.

174
00:10:57,252 --> 00:11:01,278
But we will see this running automatically in the next

175
00:11:01,444 --> 00:11:04,990
part because we will see in a demo. Now let's move on.

176
00:11:05,060 --> 00:11:08,370
Our demo will have a two part, the first part I will explain

177
00:11:08,440 --> 00:11:12,206
what we are doing with observability and using open telemetry in a Jenkins

178
00:11:12,238 --> 00:11:15,746
pipeline which kind of insight we will get and then try to

179
00:11:15,768 --> 00:11:19,286
understand how to use these for the

180
00:11:19,308 --> 00:11:22,566
machine learning part. And then the second part of the demo

181
00:11:22,668 --> 00:11:26,482
Michael will explain us what we can do with AI.

182
00:11:26,626 --> 00:11:30,266
Okay let's start first to have a look on our pipeline has you

183
00:11:30,288 --> 00:11:33,914
can see we have our smoke tests pipeline. It will

184
00:11:33,952 --> 00:11:37,370
run every 50 minutes. Is it running over every 50 minutes

185
00:11:37,520 --> 00:11:41,306
and it's covering all stages. Here is everything green. Let's have

186
00:11:41,328 --> 00:11:44,862
a look on something that is broken in the end. So we have one time

187
00:11:44,916 --> 00:11:47,440
that was broken. We click inside,

188
00:11:47,810 --> 00:11:51,502
we have a look on our observability. As I mentioned

189
00:11:51,556 --> 00:11:55,326
we are using open telemetry. We are getting these complexity dynamic and

190
00:11:55,348 --> 00:11:59,118
let's have a look on the inside. Those are collected everything automatically.

191
00:11:59,214 --> 00:12:02,334
This is a trace. So the first stage of our pipeline

192
00:12:02,382 --> 00:12:05,410
we see these name of the pipelines and we see the time that was running

193
00:12:05,480 --> 00:12:08,502
is around 1.4 minutes. We can also

194
00:12:08,636 --> 00:12:12,342
click on the span and we can have a look all strategies which

195
00:12:12,396 --> 00:12:15,846
kind of progression they had and when stopped on the

196
00:12:15,868 --> 00:12:19,014
weatherfall. It's really interesting because youre see from the start

197
00:12:19,132 --> 00:12:22,746
the first stage will be about the agent. So we are running

198
00:12:22,848 --> 00:12:26,874
Jenkins inside our Kubernetes cluster. You can imagine that every

199
00:12:26,912 --> 00:12:30,790
time that the agent will start it will be allocated a new pod.

200
00:12:30,870 --> 00:12:33,920
And this new pod will can the Jenkins agent.

201
00:12:34,370 --> 00:12:38,718
Jenkins agent of course will do something as

202
00:12:38,804 --> 00:12:43,146
first it's these allocation. So we request resources to Kubernetes

203
00:12:43,258 --> 00:12:46,914
and start a new pod. Of course you can imagine here that

204
00:12:46,952 --> 00:12:50,078
if we have a problem with our Kubernetes cluster.

205
00:12:50,174 --> 00:12:53,922
If the Kubernetes cluster is saturated then this

206
00:12:53,976 --> 00:12:57,282
will take longer. So here we can already have a look, we can jump inside

207
00:12:57,336 --> 00:13:00,534
and we can try to understand if there are problems. In this case these

208
00:13:00,572 --> 00:13:03,734
we see a checkout. So there will be the download of our

209
00:13:03,772 --> 00:13:07,506
Jenkins file. So our Jenkins file is the subscription of all

210
00:13:07,548 --> 00:13:10,758
steps. We see these that is getting downloaded.

211
00:13:10,854 --> 00:13:14,998
It's consuming around 16 seconds. So we can do already some improvement.

212
00:13:15,174 --> 00:13:19,462
These the script will deploy brand compile

213
00:13:19,606 --> 00:13:23,030
our Yaml file. So it's the one that we want to kind of deploy

214
00:13:23,110 --> 00:13:27,738
to our Kubernetes cluster in prod. It's taking 47 seconds.

215
00:13:27,834 --> 00:13:31,006
And here you can see the deep dive in all stages we can click inside

216
00:13:31,108 --> 00:13:34,706
and also understand what is going on. Then there is a build of a new

217
00:13:34,728 --> 00:13:38,114
version. We are building a new version for production they will

218
00:13:38,152 --> 00:13:42,226
condense our source code and

219
00:13:42,408 --> 00:13:45,650
in this case we are going to have a build at the end

220
00:13:45,720 --> 00:13:49,382
these we place it these gate because SRe we want to understand if we already

221
00:13:49,436 --> 00:13:53,106
have a problem in production. We already consumed our error budget

222
00:13:53,218 --> 00:13:56,566
and we don't have anymore here we have a block. So this is the

223
00:13:56,588 --> 00:14:00,010
case. So in this case we cannot deploy to production because

224
00:14:00,080 --> 00:14:03,626
our error budget has already consumed. Another interesting

225
00:14:03,728 --> 00:14:06,986
part is about these performance summary what you can see on the

226
00:14:07,008 --> 00:14:10,762
right side. So we see here that the wall time

227
00:14:10,816 --> 00:14:14,726
that is required to run these pipeline. There is only one imputation

228
00:14:14,838 --> 00:14:18,794
is about application. So imagine that we have a database

229
00:14:18,842 --> 00:14:23,130
or we had other different third party software

230
00:14:23,210 --> 00:14:26,990
that we needed to connect here. You would have seen something like network

231
00:14:27,070 --> 00:14:30,674
compute and database. We can

232
00:14:30,712 --> 00:14:33,662
jump now on the graphical overview.

233
00:14:33,806 --> 00:14:37,822
What you can see here from the Jenkins pipeline using the open telemetry.

234
00:14:37,886 --> 00:14:41,814
We sre also getting some of the interesting metrics out of the box. The first

235
00:14:41,852 --> 00:14:45,350
one is about the request rate. It's really important for SRE because

236
00:14:45,420 --> 00:14:48,930
we wanted to understand when there is a peak on these request

237
00:14:49,010 --> 00:14:52,794
and it will happen maybe has. And if

238
00:14:52,832 --> 00:14:56,966
also we need to kind of run brand increase some resources.

239
00:14:57,078 --> 00:15:00,854
This will be also the case. Second part about the request

240
00:15:00,902 --> 00:15:04,846
latency. So this will tell us which kind of problem we

241
00:15:04,868 --> 00:15:08,734
have on this Jenkins master and what

242
00:15:08,772 --> 00:15:12,362
we can see down is also the error rate. So if we have error

243
00:15:12,426 --> 00:15:16,994
on the API then we will have these immediately an

244
00:15:17,032 --> 00:15:20,482
alertment that we can create. Another interesting part

245
00:15:20,616 --> 00:15:24,226
is the overview that we have with

246
00:15:24,408 --> 00:15:28,278
the Apm. Here you see the wall overview of our application.

247
00:15:28,444 --> 00:15:31,554
We are going to select only Jenkins.

248
00:15:31,602 --> 00:15:34,914
In this case we will get filtering only on Jenkins

249
00:15:34,962 --> 00:15:39,258
as we are interested about these service we are going to select of

250
00:15:39,424 --> 00:15:43,338
one day because we want to catch also this problem that we have.

251
00:15:43,504 --> 00:15:47,866
Another view that we see here is the problem on

252
00:15:47,888 --> 00:15:51,594
the timeline and here in this case there will

253
00:15:51,632 --> 00:15:55,294
be one of this problem of this

254
00:15:55,332 --> 00:15:58,986
run of the pipeline. Has I mentioned before, we can also go in deep

255
00:15:59,018 --> 00:16:02,174
dive is another run. And you see in this case

256
00:16:02,212 --> 00:16:05,074
the run was 1.86.

257
00:16:05,112 --> 00:16:09,266
2nd we can also try to understand from a different run

258
00:16:09,368 --> 00:16:12,786
if something changes we can compare all of them.

259
00:16:12,888 --> 00:16:16,630
And this is also really interesting on observability

260
00:16:17,770 --> 00:16:21,286
our metrics, of course they are not standalone. This will

261
00:16:21,308 --> 00:16:24,182
be used mainly for debugging built.

262
00:16:24,236 --> 00:16:27,190
Michele will tell us what we can do with AI.

263
00:16:28,110 --> 00:16:32,074
Thanks Francesco for the observability preview. Now it's time

264
00:16:32,112 --> 00:16:36,054
to jump to the AI part. All of these metrics

265
00:16:36,102 --> 00:16:40,122
that youre saw in splunk observability that Francesco showed have also

266
00:16:40,176 --> 00:16:43,354
been integrated in Splunk IT service intelligence,

267
00:16:43,482 --> 00:16:46,874
which is what youre seeing here and which we're using as our AI

268
00:16:46,922 --> 00:16:50,574
platform. So what you're seeing on this page is

269
00:16:50,612 --> 00:16:54,574
the service tree overview which gives us insights on the structure of the CI CD

270
00:16:54,622 --> 00:16:57,794
platform service. I suggest we look

271
00:16:57,832 --> 00:17:01,634
at a couple of these services just to get an idea. We have

272
00:17:01,672 --> 00:17:04,430
the vault here which is used for secret management.

273
00:17:04,590 --> 00:17:08,546
We got the Kubernetes cluster service mapped, which is if

274
00:17:08,568 --> 00:17:12,870
you remember, we actually saw this in the CI CD architecture slide.

275
00:17:13,850 --> 00:17:17,510
We've got also the GitLab CI CD. However, this is

276
00:17:17,580 --> 00:17:21,194
out of scope for today's demo and we finally get to

277
00:17:21,232 --> 00:17:24,330
our Jenkins CI CD service node.

278
00:17:25,150 --> 00:17:28,614
If we look under it, we have two other nodes

279
00:17:28,662 --> 00:17:32,482
which are Jenkins end to end and the Jenkins reliability

280
00:17:32,566 --> 00:17:35,886
service. If I click into one of

281
00:17:35,908 --> 00:17:39,966
these, I will see on the right hand side, I will get a

282
00:17:39,988 --> 00:17:43,818
drill down, will open with a list of KPIs related to

283
00:17:43,844 --> 00:17:47,602
this service. And if you look at these KPIs, you'll actually

284
00:17:47,656 --> 00:17:51,234
notice that these are some of the ones that we

285
00:17:51,272 --> 00:17:54,318
already saw when we were analyzing CI CD,

286
00:17:54,494 --> 00:17:57,974
specifically when we were looking at its architecture. So we

287
00:17:58,012 --> 00:18:01,174
listed some of the KPIs. So for this demo we

288
00:18:01,212 --> 00:18:02,760
implemented a few of these.

289
00:18:05,450 --> 00:18:09,490
So keep in mind that this data comes from the splunk of observability platform

290
00:18:09,580 --> 00:18:13,066
via the open telemetry collector. And the idea is that all

291
00:18:13,088 --> 00:18:16,362
of these KPIs will contribute to the scoring that we ee

292
00:18:16,416 --> 00:18:21,310
up here, which is currently 100, therefore healthy.

293
00:18:23,090 --> 00:18:27,530
So this was sort of a short overview of the service decomposition,

294
00:18:27,690 --> 00:18:31,200
but I suggest we move now to the actual AI part.

295
00:18:31,810 --> 00:18:35,970
So if I go here into the predictive analytics

296
00:18:36,390 --> 00:18:39,634
section. So this is,

297
00:18:39,672 --> 00:18:42,978
as I said, the predictive analytics feature and I will use it to train a

298
00:18:42,984 --> 00:18:46,638
code based on my service and its KPIs. I will be

299
00:18:46,664 --> 00:18:50,694
using data from the last 14 days and since what

300
00:18:50,732 --> 00:18:53,958
I want to predict is the service health, that is

301
00:18:54,044 --> 00:18:57,706
whether it will be low, medium or high or critical, I will

302
00:18:57,728 --> 00:19:01,354
use the random forest regressor. So I will

303
00:19:01,392 --> 00:19:05,062
choose here random forest regressor. The split

304
00:19:05,126 --> 00:19:08,460
is 70 30, which is fine, and I click on train.

305
00:19:09,150 --> 00:19:12,974
This is now currently training the model based on

306
00:19:13,012 --> 00:19:16,494
my Jenkins reliability service which I've selected and its

307
00:19:16,532 --> 00:19:20,000
KPIs. And this might take a couple of minutes.

308
00:19:21,170 --> 00:19:24,566
Now we see that our model is ready. It has been tamed

309
00:19:24,618 --> 00:19:28,450
but also tested. Let's quickly look at the test results.

310
00:19:28,870 --> 00:19:32,226
Specifically, I want to see how my model performed on

311
00:19:32,248 --> 00:19:35,758
the test set. So if I scroll here to the button I see

312
00:19:35,784 --> 00:19:39,714
different analysis, but what I'm really interested to see is the predicted

313
00:19:39,762 --> 00:19:44,230
average versus the predicted worst case health score.

314
00:19:44,810 --> 00:19:48,120
And we see that in both cases it's 100%,

315
00:19:48,570 --> 00:19:52,166
which is okay. But if this were to go below alerts,

316
00:19:52,198 --> 00:19:56,074
an action would have to take place. This is something we also

317
00:19:56,112 --> 00:19:59,498
implemented. But for today's showcase, this is

318
00:19:59,584 --> 00:20:03,760
out of scope. I will save this model

319
00:20:04,850 --> 00:20:08,782
brand. Now I can finally use it on actual real

320
00:20:08,836 --> 00:20:09,440
data.

321
00:20:12,710 --> 00:20:16,690
So I again choose the Jenkins reliability

322
00:20:16,770 --> 00:20:20,306
service knows it's

323
00:20:20,338 --> 00:20:24,694
loading the model. For this service I

324
00:20:24,732 --> 00:20:27,270
again select the random forest regressor.

325
00:20:30,830 --> 00:20:35,086
And basically now while I'm waiting for the results, just understand

326
00:20:35,188 --> 00:20:38,862
what I'm trying to calculate. Here is the service health score for the next

327
00:20:38,916 --> 00:20:42,400
30 minutes and this is the output that we get.

328
00:20:42,930 --> 00:20:46,850
And basically from this point on, the predictions runs automatically.

329
00:20:49,590 --> 00:20:52,820
Okay, let's summarize everything we learned so far today.

330
00:20:53,590 --> 00:20:57,490
We identified different challenges that come with CI CD pipelines

331
00:20:57,570 --> 00:21:01,174
and therefore concluded that we're talking about a critical platform that

332
00:21:01,212 --> 00:21:03,510
requires endtoend monitoring.

333
00:21:04,010 --> 00:21:08,262
Talking about endtoend monitoring, we also introduced the concept of observability,

334
00:21:08,406 --> 00:21:12,502
which is necessary in order to introduce full transparency brand visibility

335
00:21:12,646 --> 00:21:14,810
across our entire infrastructures.

336
00:21:16,430 --> 00:21:20,302
In order to bring this data from our CI CD platform

337
00:21:20,436 --> 00:21:24,090
into our observability platform, we made use of open telemetry,

338
00:21:24,170 --> 00:21:27,866
an open source standard used to collect telemetry data and which is relevant

339
00:21:27,898 --> 00:21:29,550
in order to ensure reliability.

340
00:21:30,690 --> 00:21:34,526
Furthermore, we identified some relevant APIs that help us derive

341
00:21:34,558 --> 00:21:38,782
the state of our CI CD platform. And finally we applied AI

342
00:21:38,846 --> 00:21:42,734
to all this data in order to create some predictions and derive actionable

343
00:21:42,782 --> 00:21:46,598
insights. Why? Because our ultimate goal is

344
00:21:46,684 --> 00:21:50,326
failures prediction, which refers to the use of historical data

345
00:21:50,428 --> 00:21:54,070
in order to preempt failure before it actually occurs.

346
00:21:55,450 --> 00:21:59,810
And a final takeaway, I would like to point out from today's session,

347
00:21:59,970 --> 00:22:04,006
start simple and scale fast. So perhaps youre don't know

348
00:22:04,028 --> 00:22:07,522
where to start from. Well, maybe start from a simple experiment,

349
00:22:07,586 --> 00:22:11,102
see how the system react, see how it goes. And as you

350
00:22:11,156 --> 00:22:14,286
proceed you can scale, you can basically build more and

351
00:22:14,308 --> 00:22:18,254
more on top of that. Well, it seems it's time to close

352
00:22:18,292 --> 00:22:21,646
the curtains. Thanks a lot for watching. I really hope you

353
00:22:21,668 --> 00:22:24,510
got something from this session. Until next time.

