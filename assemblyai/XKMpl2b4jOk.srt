1
00:02:14,850 --> 00:02:18,174
Hello, I'm Mariah Peterson and I'm really excited to speak

2
00:02:18,212 --> 00:02:21,946
to you today at this site reliability Engineering session

3
00:02:21,978 --> 00:02:25,994
of ComP 42. I am currently a member of technical staff

4
00:02:26,042 --> 00:02:29,546
at Telscale, and we will be talking today on the topic

5
00:02:29,578 --> 00:02:33,630
of data. Data data reliability Engineering. It's crucial to your data organization,

6
00:02:34,210 --> 00:02:38,070
so we'll start with what is data reliability engineering?

7
00:02:38,970 --> 00:02:43,362
Just as Google's published SRE

8
00:02:43,426 --> 00:02:47,350
practices have been implemented and re implemented across

9
00:02:47,420 --> 00:02:50,170
various software engineering organizations,

10
00:02:51,310 --> 00:02:54,810
implementing these same practices to data

11
00:02:54,880 --> 00:02:58,474
organizations and data engineering and

12
00:02:58,512 --> 00:03:03,310
your data services is senior

13
00:03:03,380 --> 00:03:06,522
data reliability engineer. Implement these practices

14
00:03:06,586 --> 00:03:10,602
with these focus of decreasing downtime and improving client

15
00:03:10,666 --> 00:03:13,970
experience or the experience of data

16
00:03:14,040 --> 00:03:15,970
users and data practitioners.

17
00:03:17,430 --> 00:03:21,134
So main key topics are finding

18
00:03:21,182 --> 00:03:24,542
if your data is reliable, decreasing your data downtime,

19
00:03:24,606 --> 00:03:30,530
utilizing data level metrics, and treating

20
00:03:30,690 --> 00:03:34,790
or creating a platform for data observability.

21
00:03:35,850 --> 00:03:39,290
Reliable data is a

22
00:03:39,360 --> 00:03:44,186
little bit different than what

23
00:03:44,208 --> 00:03:47,820
you would say for a reliable web service.

24
00:03:48,350 --> 00:03:51,962
There are many aspects to reliable data. They include accuracy,

25
00:03:52,026 --> 00:03:55,310
freshness, missing data duplication,

26
00:03:55,810 --> 00:03:59,294
but they also include the latency at which data is

27
00:03:59,332 --> 00:04:02,462
available. If you can access data,

28
00:04:02,596 --> 00:04:05,794
it depends on the databases, the data stores, the data

29
00:04:05,832 --> 00:04:09,538
gateways, and you want to be

30
00:04:09,544 --> 00:04:13,266
able to maintain a first class or a

31
00:04:13,448 --> 00:04:16,806
4959 experience for all

32
00:04:16,828 --> 00:04:18,680
of these aspects of your data.

33
00:04:21,370 --> 00:04:24,870
Additionally, when you're building systems for reliable data,

34
00:04:24,940 --> 00:04:29,426
you have to understand that systems

35
00:04:29,458 --> 00:04:32,982
are not perfect. That's why we're not aiming for 100% systems.

36
00:04:33,046 --> 00:04:36,886
We do want to create budgets that allow for failure

37
00:04:36,998 --> 00:04:41,082
and that by leveraging these budgets, we have a first class team

38
00:04:41,216 --> 00:04:44,762
that understands what a data incident is and can respond

39
00:04:44,906 --> 00:04:49,114
quickly, right? So that we have a cushion for intervention.

40
00:04:49,242 --> 00:04:52,750
We can respond quickly, efficiently, so that the customers

41
00:04:52,820 --> 00:04:56,702
don't notice large outages, that we have minimized our downtime

42
00:04:56,846 --> 00:05:00,994
and we have created an

43
00:05:01,032 --> 00:05:04,818
experience that says this data is reliable and we are

44
00:05:04,904 --> 00:05:08,680
willing to create that first class experience.

45
00:05:09,610 --> 00:05:13,426
Data downtime, like I said, can have various parts

46
00:05:13,458 --> 00:05:16,466
to it. Bar Moses,

47
00:05:16,498 --> 00:05:20,380
the CEO of Monte Carlo, which is a data observability platform,

48
00:05:21,390 --> 00:05:25,690
has long proliferated this definition

49
00:05:26,110 --> 00:05:29,078
where it's not just your data being unavailable,

50
00:05:29,174 --> 00:05:32,286
but if your data is erroneous, if it's only part

51
00:05:32,308 --> 00:05:36,094
of the picture, if it's inaccurate in any way,

52
00:05:36,212 --> 00:05:39,758
that is data downtime. And that's what we're trying to,

53
00:05:39,924 --> 00:05:43,514
in can iterative way, leverage signals and

54
00:05:43,572 --> 00:05:46,866
metrics, our error budgets and our

55
00:05:46,888 --> 00:05:51,714
engineering time to minimize that, to make our data as

56
00:05:51,832 --> 00:05:56,100
reliable, as close to

57
00:05:58,010 --> 00:06:02,018
the customer's expectation, the practitioner's expectation

58
00:06:02,194 --> 00:06:05,446
as possible things we

59
00:06:05,468 --> 00:06:08,710
can do, just like with service level metrics.

60
00:06:09,050 --> 00:06:12,826
You can put metrics at a data level to capture the

61
00:06:12,848 --> 00:06:16,774
stability of your data pipelines, of your data stores, of your data gateways

62
00:06:16,902 --> 00:06:20,860
or other data services you can create

63
00:06:22,110 --> 00:06:25,242
pictures of if

64
00:06:25,296 --> 00:06:28,590
your queries on data stores are taking too long,

65
00:06:28,660 --> 00:06:32,142
if you're taking too long to train a model, if your data

66
00:06:32,196 --> 00:06:36,690
gateway has an unexpected latency, errors, returning an inappropriate error,

67
00:06:38,150 --> 00:06:41,806
you can use all of these informations

68
00:06:41,838 --> 00:06:46,290
put together to create a picture and explain visually

69
00:06:47,450 --> 00:06:49,910
through metrics and other metadata.

70
00:06:50,410 --> 00:06:53,990
If your data is reliable and through

71
00:06:54,060 --> 00:06:58,478
anomalies, create lets that allow us to investigate

72
00:06:58,514 --> 00:07:02,490
them. Like I said, we can investigate latency,

73
00:07:03,230 --> 00:07:05,980
see which is our most traffic services,

74
00:07:06,590 --> 00:07:10,442
which services are experiencing more

75
00:07:10,496 --> 00:07:14,906
or less traffic than we expect. If we have error

76
00:07:14,938 --> 00:07:18,170
messages being thrown when they shouldn't be, or serve

77
00:07:18,250 --> 00:07:21,706
being 400 errors, 500 errors on API

78
00:07:21,738 --> 00:07:24,818
gateways, maybe you're having

79
00:07:24,904 --> 00:07:28,594
a situations on your gateway or on your database or on other data

80
00:07:28,632 --> 00:07:31,890
store you don't expect. These four things are

81
00:07:31,960 --> 00:07:37,650
part of the golden signals expected

82
00:07:37,730 --> 00:07:41,734
out of the SRE handbook. Observability is

83
00:07:41,772 --> 00:07:45,186
one step beyond those metrics. We take these metrics

84
00:07:45,218 --> 00:07:48,642
and create slas, slis and slos to make sure

85
00:07:48,796 --> 00:07:52,202
that our data services are

86
00:07:52,256 --> 00:07:55,660
performing to what your data practitioner needs.

87
00:07:58,510 --> 00:08:02,842
That way, as things come up, right. Our metrics

88
00:08:02,906 --> 00:08:06,730
are reporting something unexpected.

89
00:08:06,810 --> 00:08:10,922
Through those slos and slis, we can address that either immediately,

90
00:08:10,986 --> 00:08:14,306
depending on the criticality of that error, we can throw it

91
00:08:14,328 --> 00:08:18,500
into a bug, we can give it to a support or a

92
00:08:19,750 --> 00:08:22,290
support reliability customer engineer.

93
00:08:23,110 --> 00:08:26,386
And using procedures, we can minimize

94
00:08:26,418 --> 00:08:29,990
these downtime and leverage a decreased time to resolution

95
00:08:30,650 --> 00:08:34,200
depending on severity. For example,

96
00:08:34,570 --> 00:08:38,714
say we have a data

97
00:08:38,752 --> 00:08:43,258
pipeline that is taking CRM data in

98
00:08:43,424 --> 00:08:46,778
and it is training a model

99
00:08:46,864 --> 00:08:50,246
on that, then creating a couple of dashboards,

100
00:08:50,438 --> 00:08:54,702
deploying that model, and then releasing those dashboards to

101
00:08:54,836 --> 00:08:58,314
sales staff. Right? The model is used to make predictive

102
00:08:58,362 --> 00:09:02,302
leads, and the dashboards are then used by

103
00:09:02,356 --> 00:09:06,290
sales leaders to motivate, to encourage

104
00:09:07,030 --> 00:09:10,654
salesmen to help with marketing.

105
00:09:10,702 --> 00:09:13,140
There's a variety of things that can be done with those.

106
00:09:13,830 --> 00:09:18,390
So we create an SLA for this CRM pipeline

107
00:09:19,530 --> 00:09:23,286
to have

108
00:09:23,308 --> 00:09:27,366
the data be no less than the

109
00:09:27,388 --> 00:09:31,642
model. The dashboards be 24

110
00:09:31,776 --> 00:09:36,060
hours old, they cannot be any older than 24 hours.

111
00:09:37,550 --> 00:09:39,980
We take that and put that into,

112
00:09:41,970 --> 00:09:45,566
yes, revenue dashboard with minimal freshness of one day.

113
00:09:45,748 --> 00:09:49,422
We take that and we transport that into an

114
00:09:49,476 --> 00:09:53,310
SLO, right. An objective for our data

115
00:09:53,380 --> 00:09:57,314
service or our pipelines that is creating that dashboard. So we

116
00:09:57,352 --> 00:10:01,090
know that we want this pipelines to extract data from the CRM,

117
00:10:01,590 --> 00:10:04,690
complete its transforms, training and analysis at least

118
00:10:04,760 --> 00:10:08,246
once a day. And if something happens, we want to

119
00:10:08,268 --> 00:10:12,120
be able to give us can error within a reasonable time,

120
00:10:12,650 --> 00:10:16,402
right? So that we can manually intervene and our SLA

121
00:10:16,466 --> 00:10:19,942
is never broken. If something comes up and the automation

122
00:10:20,006 --> 00:10:23,594
fails, this allows us to keep that

123
00:10:23,632 --> 00:10:27,820
customer or the practitioner expectation without

124
00:10:31,630 --> 00:10:36,074
compromise with some kind of a fall plan while maintaining

125
00:10:36,202 --> 00:10:39,514
that reliability for our systems. So then we have sois,

126
00:10:39,562 --> 00:10:43,520
right? These are the indicators that alert us if we need to

127
00:10:44,050 --> 00:10:47,486
as data, data reliability engineering involved. These can be errors

128
00:10:47,518 --> 00:10:50,978
if something happens in the pipeline, right? Maybe our model doesn't complete training,

129
00:10:51,064 --> 00:10:55,010
maybe one of the transformations doesn't happen. Maybe we can't connect to the CRM.

130
00:10:55,590 --> 00:10:59,606
Maybe we time out and we sre not able to

131
00:10:59,788 --> 00:11:03,640
write to our data store or we are unable to.

132
00:11:04,170 --> 00:11:07,320
That timeout could be on the CRM or on the training.

133
00:11:08,010 --> 00:11:12,186
And these create alerts, right. These slIs, if they

134
00:11:12,288 --> 00:11:15,738
indicate above your threshold, they create alerts and they allow that

135
00:11:15,904 --> 00:11:19,926
practitioner to know, oh, there needs to be a manual intervention

136
00:11:20,038 --> 00:11:23,994
to maintain that reliability standard that we have with our data practitioners

137
00:11:24,122 --> 00:11:28,510
or any of the stakeholders at the other end of our observability pipelines.

138
00:11:30,210 --> 00:11:33,906
So great data reliability makes sense.

139
00:11:34,008 --> 00:11:38,242
It's transposing your normal reliability practices to data

140
00:11:38,296 --> 00:11:41,620
systems. Why do we need it? Right.

141
00:11:43,270 --> 00:11:47,154
Great, the CRM stuff works. But what

142
00:11:47,192 --> 00:11:50,646
about more? That's not as critical as some

143
00:11:50,668 --> 00:11:54,600
of the other systems that rely on data. We have

144
00:11:55,450 --> 00:11:59,142
in one of my favorite books, designing data intensive

145
00:11:59,206 --> 00:12:02,714
systems, it describes these types of

146
00:12:02,752 --> 00:12:06,202
data projects that all need data

147
00:12:06,256 --> 00:12:10,060
reliability to be infused into their project.

148
00:12:10,670 --> 00:12:13,482
The first one is your software data stack.

149
00:12:13,546 --> 00:12:15,520
Right. So this would be,

150
00:12:16,530 --> 00:12:20,634
we'll get to it later. Your software data stack, your enterprise

151
00:12:20,682 --> 00:12:24,030
data stack and these of course your modern data stack,

152
00:12:24,370 --> 00:12:27,986
the software data stack. Let's start. What is it

153
00:12:28,168 --> 00:12:31,874
that is these data store for any customer having application, right.

154
00:12:31,912 --> 00:12:35,426
This could be a database to a microservice. This could be an in

155
00:12:35,448 --> 00:12:39,000
memory cache. This could be any of your

156
00:12:39,850 --> 00:12:43,480
cookies stored in a browser, other kinds of application information,

157
00:12:44,250 --> 00:12:48,762
but any kind of data that interfaces directly with

158
00:12:48,816 --> 00:12:52,730
your software service. Typically who manages

159
00:12:53,070 --> 00:12:55,530
the reliability of these data stores?

160
00:12:57,070 --> 00:13:00,866
This usually falls within the spectrum. Senior senior data reliability

161
00:13:00,998 --> 00:13:05,054
engineer have tools like they

162
00:13:05,172 --> 00:13:08,526
often come from operations backgrounds. They understand how to spin up

163
00:13:08,548 --> 00:13:12,960
databases. They have hosted databases and

164
00:13:14,390 --> 00:13:17,954
they don't usually case to the point where

165
00:13:17,992 --> 00:13:21,442
you need some kind of specialized knowledge for this

166
00:13:21,496 --> 00:13:24,130
kind of reliability interface.

167
00:13:25,270 --> 00:13:29,814
The enterprise data stack is

168
00:13:29,852 --> 00:13:33,266
quite a bit different than that software data stack.

169
00:13:33,458 --> 00:13:37,314
First thing first, it is your infrastructure structure

170
00:13:37,362 --> 00:13:40,538
or data platforms for enterprise data services.

171
00:13:40,624 --> 00:13:43,834
These are your large scale distributed databases. These are your

172
00:13:43,952 --> 00:13:47,594
large data warehouses that are used for

173
00:13:47,632 --> 00:13:51,834
reporting and bi, these are SAP

174
00:13:51,882 --> 00:13:54,030
databases, oracle databases,

175
00:13:56,050 --> 00:13:59,520
any kind of large data infrastructure that

176
00:14:00,690 --> 00:14:04,718
has grown into what various

177
00:14:04,814 --> 00:14:08,146
enterprise companies large scale enterprise services

178
00:14:08,248 --> 00:14:11,570
need who manages their reliability.

179
00:14:12,390 --> 00:14:15,554
That reliability for this requires a special set

180
00:14:15,592 --> 00:14:19,510
of data skills and it is typically handled by your DBA

181
00:14:20,410 --> 00:14:24,534
or a data reliability engineering. Somebody who has that

182
00:14:24,572 --> 00:14:28,710
specialized database administration knowledge

183
00:14:29,230 --> 00:14:33,894
can do sharding query

184
00:14:33,942 --> 00:14:37,898
optimization, has managed many flavors of

185
00:14:38,064 --> 00:14:41,266
database, SQL or other custom database languages,

186
00:14:41,398 --> 00:14:45,360
but still would be the first line of defense if

187
00:14:46,370 --> 00:14:50,878
you have an error from your metric reliability issue

188
00:14:50,964 --> 00:14:55,214
comes up. Now, what is this modern

189
00:14:55,262 --> 00:14:58,322
data stack? I mean, what's left? We've talked about

190
00:14:58,376 --> 00:15:02,226
software, we've talked about your

191
00:15:02,248 --> 00:15:06,014
large scale data. What's left? These are any

192
00:15:06,072 --> 00:15:10,150
kind of pipeline and analytic that's used for

193
00:15:10,300 --> 00:15:13,762
machine learning. These are your ETL transformations

194
00:15:13,906 --> 00:15:18,120
that are used when you're extracting data for

195
00:15:18,430 --> 00:15:23,082
data scientists, basically anything that

196
00:15:23,136 --> 00:15:26,342
can be used for data, right? Creating data gateways,

197
00:15:26,406 --> 00:15:35,440
creating data mesh, creating custom

198
00:15:35,890 --> 00:15:38,666
machine learning pipelines on top of your data warehouse,

199
00:15:38,698 --> 00:15:42,346
perhaps right where you're taking that data. Another step with extracting

200
00:15:42,378 --> 00:15:44,690
it from that warehouse and maintaining a pipeline,

201
00:15:45,430 --> 00:15:49,342
ETL pipelines that perform platform crucial

202
00:15:49,406 --> 00:15:53,266
transformations, right. Oftentimes you're seeing it very common that information

203
00:15:53,368 --> 00:15:57,202
from your analytics data warehouse is being used in

204
00:15:57,256 --> 00:16:00,786
software applications. You're seeing machine

205
00:16:00,818 --> 00:16:04,914
learning models being shipped to production as part of the software

206
00:16:04,962 --> 00:16:09,594
product that is being maintained by

207
00:16:09,712 --> 00:16:13,542
back end engineers. And they need somebody with more data expertise

208
00:16:13,606 --> 00:16:16,906
to come and step in and help maintain the

209
00:16:16,928 --> 00:16:20,330
reliability of those pipelines. Gateways,

210
00:16:20,670 --> 00:16:24,570
like I've mentioned, with something like your data mesh, where you have data gateways

211
00:16:24,650 --> 00:16:27,822
that protect golden data sets, but allow your data

212
00:16:27,876 --> 00:16:31,454
to be accessed by a variety of teams of

213
00:16:31,492 --> 00:16:34,610
consumers, stakeholders, or other kind of data practitioner.

214
00:16:35,350 --> 00:16:39,058
And this is very different. This is not

215
00:16:39,144 --> 00:16:42,690
a data warehouse maintenance load. This is not a site

216
00:16:42,760 --> 00:16:46,194
reliability maintenance load. Who would manage this data set

217
00:16:46,232 --> 00:16:50,738
and make sure it stays reliable? This is what your data reliability engineer

218
00:16:50,834 --> 00:16:54,758
is trained to do. They have that skill set to understand

219
00:16:54,844 --> 00:16:58,194
what ingress and egress is, to understand ETL

220
00:16:58,242 --> 00:17:02,410
transformations both in streaming and batch, to understand storage

221
00:17:03,390 --> 00:17:06,970
varieties, from databases

222
00:17:07,390 --> 00:17:10,854
to large file

223
00:17:10,902 --> 00:17:15,034
stores, in memory stores, distributed stores,

224
00:17:15,162 --> 00:17:19,166
basic data governance and data analysis. They're not

225
00:17:19,188 --> 00:17:23,120
going to be your DBA and maybe be resharding databases, but they'll have

226
00:17:23,590 --> 00:17:27,442
a variety of skills that prepare them for handling data in

227
00:17:27,496 --> 00:17:31,060
motion as opposed to data at rest.

228
00:17:31,670 --> 00:17:34,862
They're going to focus on automating pipelines,

229
00:17:34,926 --> 00:17:38,770
automating services. They're going to be doing monitoring and observability

230
00:17:38,850 --> 00:17:42,406
on these data in motion systems and services, and they're going to understand

231
00:17:42,508 --> 00:17:46,502
modern architectures that are part

232
00:17:46,556 --> 00:17:49,782
of these systems, so that as they work with data engineers,

233
00:17:49,926 --> 00:17:53,974
they can create these contracts

234
00:17:54,022 --> 00:17:57,462
and slas that the practitioners

235
00:17:57,526 --> 00:17:59,980
need to maintain a quality of service.

236
00:18:03,490 --> 00:18:06,942
So that

237
00:18:06,996 --> 00:18:10,606
is the senior data data, senior data reliability

238
00:18:10,628 --> 00:18:14,226
engineer. You understand the

239
00:18:14,248 --> 00:18:17,860
importance of it. I was on a team

240
00:18:19,030 --> 00:18:23,262
that data reliability became essential

241
00:18:23,326 --> 00:18:26,642
part of the work. And these orchestration

242
00:18:26,706 --> 00:18:31,042
we were doing to provide our data practitioners

243
00:18:31,106 --> 00:18:35,154
with the data they needed to perform daily software tasks.

244
00:18:35,282 --> 00:18:38,270
It was very much fulfilling a contract,

245
00:18:38,370 --> 00:18:40,010
keeping your data fresh,

246
00:18:41,630 --> 00:18:45,158
not letting your data age out, keeping it viable,

247
00:18:45,254 --> 00:18:49,100
non erroneous. And it can quickly

248
00:18:49,950 --> 00:18:53,630
go from something trivial to something that is very

249
00:18:53,700 --> 00:18:57,120
much a requirement for software services

250
00:18:57,650 --> 00:19:00,350
to run successfully.

251
00:19:01,410 --> 00:19:04,946
I think it is applicable to all data centric services.

252
00:19:05,048 --> 00:19:08,498
Any service that relies on data to function

253
00:19:08,664 --> 00:19:12,526
needs to have some kind of data reliability

254
00:19:12,718 --> 00:19:16,462
in place as data is democratized

255
00:19:16,526 --> 00:19:20,098
and used across the stack. We see this with movements like data Mesh,

256
00:19:20,194 --> 00:19:23,654
where more and more teams need data and they

257
00:19:23,692 --> 00:19:27,106
need a variety of data sets. And it's

258
00:19:27,138 --> 00:19:30,422
not just your analytics teams, it is software teams,

259
00:19:30,486 --> 00:19:33,930
it is operations teams, it is finance teams.

260
00:19:34,510 --> 00:19:38,090
And as this data is democratized across transactions,

261
00:19:39,470 --> 00:19:42,990
reliability in that data becomes more crucial

262
00:19:43,570 --> 00:19:46,880
to every step of that process,

263
00:19:47,250 --> 00:19:51,114
so that nobody's getting calls

264
00:19:51,162 --> 00:19:54,686
because dashboards are outdated. But you can know in advance

265
00:19:54,878 --> 00:19:59,330
and have preemptive steps in place like the SRE

266
00:19:59,750 --> 00:20:03,534
practices or handbook subscribes

267
00:20:03,662 --> 00:20:07,334
and a data reliability engineer is specialized and

268
00:20:07,372 --> 00:20:11,254
different from any other reliability engineer because they

269
00:20:11,292 --> 00:20:14,614
work with data stacks and with data engineers, and they

270
00:20:14,652 --> 00:20:18,182
understand the data ecosystem, which is very

271
00:20:18,236 --> 00:20:22,314
different than a lot of software ecosystems. There are

272
00:20:22,352 --> 00:20:25,850
different kinds of optimizations and choices and design

273
00:20:25,920 --> 00:20:28,060
and architecture that need to be made.

274
00:20:29,790 --> 00:20:33,166
I want to thank you guys for coming to

275
00:20:33,188 --> 00:20:36,814
my talk. I hope you enjoyed it and

276
00:20:36,852 --> 00:20:40,478
you can take away a desire to have maybe

277
00:20:40,564 --> 00:20:44,190
some new data reliability practices in your organization.

278
00:20:44,350 --> 00:20:47,922
If you have any questions about data

279
00:20:47,976 --> 00:20:50,100
reliability, where to get started,

280
00:20:51,270 --> 00:20:55,186
what to do, how to do it, please reach out to me. I'm available

281
00:20:55,288 --> 00:20:58,626
on Twitter, on LinkedIn. I do weekly

282
00:20:58,658 --> 00:21:01,830
Twitch teams. I'm happy to talk about that,

283
00:21:01,980 --> 00:21:05,318
brainstorm and go further. I want to thank

284
00:21:05,404 --> 00:21:09,682
Comp 42 again for allowing me to speak on this topic

285
00:21:09,826 --> 00:21:14,214
at this site Reliability engineering conference, and I

286
00:21:14,412 --> 00:21:18,082
want to thank any sponsors and most importantly

287
00:21:18,146 --> 00:21:21,820
the attendees. And I hope to see you guys again next time.

