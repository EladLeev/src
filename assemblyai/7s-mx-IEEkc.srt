1
00:02:15,160 --> 00:02:18,352
Hi, my name is Brian Barkley. I'm an engineer at LinkedIn,

2
00:02:18,416 --> 00:02:21,924
and I'm going to be joined by my colleague Vivek Deshpande to

3
00:02:21,962 --> 00:02:25,300
discuss a framework that we've helped to develop called Hodor.

4
00:02:25,720 --> 00:02:29,728
LinkedIn counts on Hodor to protect our microservices by detecting

5
00:02:29,824 --> 00:02:33,636
when our systems have become overloaded and providing relief to

6
00:02:33,658 --> 00:02:37,276
bring them back to a stable state. So this is

7
00:02:37,298 --> 00:02:41,096
our goal. We want to increase our overall service resiliency,

8
00:02:41,208 --> 00:02:44,956
uptime, and availability, while minimizing impact to

9
00:02:44,978 --> 00:02:48,188
these members who are using the site. We also want to

10
00:02:48,194 --> 00:02:51,596
do this with an out of the box solution that doesn't require service owners

11
00:02:51,628 --> 00:02:55,104
to have to customize things or maintain configuration that's specific to

12
00:02:55,142 --> 00:02:58,956
their service. I'll also note that what we'll be presenting

13
00:02:58,988 --> 00:03:02,524
here is all within the context of code running on the Java virtual

14
00:03:02,572 --> 00:03:05,876
machine, though some of the concepts would translate well to other

15
00:03:05,898 --> 00:03:09,076
runtime environments. So here's our

16
00:03:09,098 --> 00:03:13,056
agenda for the talk. I'll provide an overview of Hodor, then Vivek

17
00:03:13,088 --> 00:03:16,324
will discuss the various types of overload detectors we've developed.

18
00:03:16,452 --> 00:03:20,132
I'll talk about how we remediate overloads situations,

19
00:03:20,276 --> 00:03:23,604
how we rolled this out to hundreds of existing services safely.

20
00:03:23,732 --> 00:03:27,276
We'll take a look at a success story from our

21
00:03:27,298 --> 00:03:31,004
production environment and discuss some related work and

22
00:03:31,042 --> 00:03:32,910
what else we have planned for the future.

23
00:03:34,560 --> 00:03:37,692
So, for anyone wondering about the name, as you can see,

24
00:03:37,746 --> 00:03:41,792
Hodor stands for holistic overload detection and

25
00:03:41,846 --> 00:03:45,584
overload remediation. It protects our services and

26
00:03:45,622 --> 00:03:49,920
so also has a similarity in that respect to a certain fictional character.

27
00:03:50,340 --> 00:03:53,644
So let's get into some of the details of Hodor situations

28
00:03:53,692 --> 00:03:56,710
that it's meant to address and how the pieces fit together.

29
00:03:57,240 --> 00:04:01,136
Start with let's talk about what it means for a service to be overloads.

30
00:04:01,248 --> 00:04:04,416
We define it as the point at which it's unable to serve traffic

31
00:04:04,448 --> 00:04:08,036
with reasonable latency. We want to maximize the goodput

32
00:04:08,068 --> 00:04:11,812
that a service is able to provide. And so when a service has saturated

33
00:04:11,876 --> 00:04:16,256
its usage of a given resource, we need to prevent oversaturation

34
00:04:16,388 --> 00:04:19,756
and degradation of that resource. What sort of

35
00:04:19,778 --> 00:04:23,784
resources are we talking about here? They can be physical or virtual

36
00:04:23,832 --> 00:04:27,784
resources. Some obvious examples of physical resources are cpu

37
00:04:27,832 --> 00:04:31,404
usage, memory usage, network bandwidth,

38
00:04:31,452 --> 00:04:34,752
or disk I O, and all of these have hard

39
00:04:34,806 --> 00:04:38,956
physical limits that are just not possible to push past or increase via

40
00:04:38,988 --> 00:04:42,212
some configuration tweaks. Once these limits are hit,

41
00:04:42,266 --> 00:04:44,950
performance starts to degrade pretty quickly.

42
00:04:45,560 --> 00:04:49,284
Virtual resources are different altogether, but can have the same

43
00:04:49,322 --> 00:04:53,456
impact when they are fully utilized. Some examples include threads

44
00:04:53,488 --> 00:04:56,948
available for execution, pool DB connections,

45
00:04:57,044 --> 00:04:58,920
or send before permits.

46
00:04:59,980 --> 00:05:03,800
These limits can be reached in different ways. A clear case

47
00:05:03,870 --> 00:05:07,156
is increased traffic to a service if the number of requests

48
00:05:07,188 --> 00:05:10,412
per second goes up five x or ten x, normal things

49
00:05:10,466 --> 00:05:13,564
aren't going to go well on that machine if it's not provisioned for that amount

50
00:05:13,602 --> 00:05:17,244
of load. Another more subtle case is if some

51
00:05:17,282 --> 00:05:21,144
service downstream starts to slow down. That effectively

52
00:05:21,192 --> 00:05:24,800
applies backpressure up the call chain, and services higher up

53
00:05:24,870 --> 00:05:28,636
are affected by that added latency in those calls. It'll slow

54
00:05:28,668 --> 00:05:32,176
down the upstream service and causes it to have more requests in

55
00:05:32,198 --> 00:05:35,700
flight, which could lead to memory issues or thread exhaustion.

56
00:05:36,360 --> 00:05:39,684
One more example is if your service is running alongside others

57
00:05:39,722 --> 00:05:43,296
on the same host without proper isolation. If a neighboring

58
00:05:43,328 --> 00:05:46,932
process is using a lot of cpu, disk or network bandwidth,

59
00:05:46,996 --> 00:05:50,184
your service will be negatively impacted without any change to

60
00:05:50,222 --> 00:05:53,656
traffic or downstream latencies. So this

61
00:05:53,678 --> 00:05:57,136
is where the holistic part comes in. We want to be able to catch

62
00:05:57,188 --> 00:06:00,476
as many of these types of issues as possible, though the

63
00:06:00,498 --> 00:06:04,520
services using hodor can have wildly different traffic patterns,

64
00:06:04,680 --> 00:06:08,396
execution and threading models and workloads, and as

65
00:06:08,418 --> 00:06:12,096
I mentioned before, it shouldn't take any configuration. We have

66
00:06:12,118 --> 00:06:16,092
hundreds and hundreds of services running on tens of thousands of machines, and tweaking

67
00:06:16,156 --> 00:06:20,016
things for each of those just isn't feasible to

68
00:06:20,038 --> 00:06:23,972
addressing the problem once it's been detected. We begin dropping requests and

69
00:06:24,026 --> 00:06:28,340
return 503, which is service unavailable responses.

70
00:06:29,080 --> 00:06:32,756
But we want to minimize these and drop just enough traffic to

71
00:06:32,778 --> 00:06:36,244
mitigate the problem. The tricky part here is that

72
00:06:36,282 --> 00:06:39,976
the amount of traffic that needs to be dropped and the overall capacity of the

73
00:06:39,998 --> 00:06:43,652
service can easily vary depending on the nature of these overload

74
00:06:43,716 --> 00:06:47,364
itself. For example, the amount of traffic that a service

75
00:06:47,422 --> 00:06:51,288
can handle may be much different if the cpu is saturated

76
00:06:51,464 --> 00:06:54,920
compared to if there's back pressure from a downstream

77
00:06:55,000 --> 00:06:57,870
and memory usage is becoming a problem.

78
00:06:58,320 --> 00:07:02,092
So we have to be flexible and dynamic, both in detecting overloads

79
00:07:02,156 --> 00:07:04,880
and also in knowing how much traffic to drop.

80
00:07:05,860 --> 00:07:09,504
So what's hodor made of? There are basically three main

81
00:07:09,542 --> 00:07:13,264
components. First, there are what we call overload

82
00:07:13,312 --> 00:07:16,272
detectors. There could be multiple of these registered,

83
00:07:16,336 --> 00:07:19,476
including any that might be application specific.

84
00:07:19,658 --> 00:07:23,270
Vivek will be talking about the standard ones we provide in a bit.

85
00:07:23,720 --> 00:07:27,236
The detectors are queried for each inbound request with some metadata

86
00:07:27,268 --> 00:07:30,552
about the requests. This allows the detectors to operate on a

87
00:07:30,606 --> 00:07:34,308
context specific level if needed, and potentially only detect

88
00:07:34,404 --> 00:07:38,216
overload and pushed traffic for a targeted subset of requests.

89
00:07:38,408 --> 00:07:41,976
Most of these detects, though, operate on a global level and don't

90
00:07:42,008 --> 00:07:45,096
do any request specific processing. Instead, they're fetching

91
00:07:45,128 --> 00:07:48,504
an asynchronously calculated state indicating whether the detector

92
00:07:48,552 --> 00:07:50,540
considers things to be overloaded.

93
00:07:51,700 --> 00:07:55,292
Second, we have the load shutter, so this decides

94
00:07:55,356 --> 00:07:58,496
which traffic should be dropped. Once a detector is signaled that there's an

95
00:07:58,518 --> 00:08:02,204
overload, the shutter needs to be intelligent about which requests

96
00:08:02,252 --> 00:08:06,464
should be dropped to make sure that too much traffic isn't rejected,

97
00:08:06,592 --> 00:08:09,750
but that enough is to exit the overloaded state.

98
00:08:10,120 --> 00:08:13,344
The load shutter takes the signal from the detects as input,

99
00:08:13,392 --> 00:08:17,880
as well as some contextual information about the request to make these decisions.

100
00:08:18,540 --> 00:08:22,456
Finally, there's a platform specific component that combines the

101
00:08:22,478 --> 00:08:26,388
detectors and load shutter and adapts request metadata into Hodor's

102
00:08:26,404 --> 00:08:30,028
APIs. The detectors and shutter are platform

103
00:08:30,114 --> 00:08:34,296
agnostic. At LinkedIn, we primarily use an open source project we developed

104
00:08:34,328 --> 00:08:38,296
called restly for communication between services. So that's

105
00:08:38,328 --> 00:08:41,424
the first platform we had Hodor running on. We've since

106
00:08:41,462 --> 00:08:44,604
adapted it to work with GRPC as well as HTTP

107
00:08:44,652 --> 00:08:48,464
servers such as the play framework. I'm going to hand things

108
00:08:48,502 --> 00:08:52,284
off to Vivek now, and he will get into more details

109
00:08:52,332 --> 00:08:56,740
of how some of our detectors work to determine when the service is overloads.

110
00:08:57,400 --> 00:09:01,076
Thanks Brian. Hello everyone. Now let's talk about how the

111
00:09:01,098 --> 00:09:03,540
overloads detection is done using different detectors.

112
00:09:05,980 --> 00:09:09,908
The first detector is CPU detector. The objective of CPU detector

113
00:09:09,924 --> 00:09:13,236
is to quickly and accurately detect CPU overloads.

114
00:09:13,348 --> 00:09:16,504
The idea is to have a lightweight background thread running at same

115
00:09:16,542 --> 00:09:20,488
priority as the application threads, which execute business logic.

116
00:09:20,664 --> 00:09:24,348
This background thread, known as the heartbeat thread, is scheduled to

117
00:09:24,354 --> 00:09:28,312
wake up every ten milliseconds. The overall amount of work here is trivial

118
00:09:28,376 --> 00:09:32,000
and adds no major variable impact to application performance.

119
00:09:32,820 --> 00:09:36,716
The heartbeat overload detection algorithm monitors whether the heartbeat

120
00:09:36,748 --> 00:09:40,684
thread is getting cpu every ten milliseconds. Once the heartbeat algorithm

121
00:09:40,732 --> 00:09:44,416
realizes that the heartbeat thread is consistently not getting cpu

122
00:09:44,448 --> 00:09:47,920
time at the expected time intervals, we flag an overload.

123
00:09:48,080 --> 00:09:51,252
It is important to note here that a few variations are not

124
00:09:51,306 --> 00:09:54,536
enough to flag an overload, and that the algorithm flags an

125
00:09:54,558 --> 00:09:57,736
overload only when we have high confidence that a

126
00:09:57,758 --> 00:10:01,844
cpu is overloaded. In concept, this idea is straightforward,

127
00:10:01,972 --> 00:10:05,988
but determining the appropriate variables and parameters

128
00:10:06,084 --> 00:10:09,356
for this algorithm to maximize precision while have high

129
00:10:09,378 --> 00:10:12,936
recall was challenging. To provide a concrete

130
00:10:12,968 --> 00:10:16,412
example, we may have the thread slip for ten milliseconds each

131
00:10:16,466 --> 00:10:19,720
time, and if the 99th percentile in a second's worth of

132
00:10:19,730 --> 00:10:23,452
data is over 55 milliseconds, that window is in violation.

133
00:10:23,596 --> 00:10:26,864
If eight consecutive windows are in violation, the service is

134
00:10:26,902 --> 00:10:30,704
considered overloads. Values for this thresholds that we

135
00:10:30,742 --> 00:10:34,612
use are determined by synthetic testing as well as by sourcing data

136
00:10:34,666 --> 00:10:38,548
from production and comparing it with performance metrics when the

137
00:10:38,554 --> 00:10:42,240
services were considered to be overloaded. The rationale

138
00:10:42,320 --> 00:10:46,460
behind using heartbeat thread is one it directly measures

139
00:10:46,480 --> 00:10:49,930
useful cpu time available to the application in real time.

140
00:10:50,540 --> 00:10:54,660
What we mean by this is that just because you see 30% free cpu

141
00:10:54,740 --> 00:10:57,996
on something like top command does not mean that it is

142
00:10:58,018 --> 00:11:01,644
useful cpu. And number two, the concept of the heartbeat thread is

143
00:11:01,682 --> 00:11:04,908
applicable everywhere irrespective of these environment or the application

144
00:11:04,994 --> 00:11:05,630
type.

145
00:11:08,560 --> 00:11:12,160
So this is an example of the cpu detector in action.

146
00:11:12,500 --> 00:11:16,364
In the top left graph you can observe that the heartbeat detector

147
00:11:16,412 --> 00:11:20,284
is able to capture a cpu overload. Notice that the performance

148
00:11:20,332 --> 00:11:23,636
indicators such as average and p 90 latencies and

149
00:11:23,658 --> 00:11:27,296
p 95 cpu all spike when the heartbeat detector

150
00:11:27,328 --> 00:11:30,868
flags an overload. Now let's move on to

151
00:11:30,874 --> 00:11:34,580
the next detector which focuses on memory bottlenecks.

152
00:11:36,860 --> 00:11:40,312
The objective of GC overload detector is to quickly and

153
00:11:40,366 --> 00:11:43,944
accurately detects increased garbage collection activity for

154
00:11:43,982 --> 00:11:47,992
applications with auto memory management like Java applications,

155
00:11:48,136 --> 00:11:51,192
these idea is to observe overhead of GC activity

156
00:11:51,256 --> 00:11:54,750
in a lightweight manner to detect overload in real time.

157
00:11:55,200 --> 00:11:58,776
On each GC event, we calculate the amortized percentage

158
00:11:58,808 --> 00:12:01,840
of time spent in GC over a given time window.

159
00:12:02,260 --> 00:12:05,840
We call that as GC overhead. A schedule is set

160
00:12:05,910 --> 00:12:09,724
on top of a GC overloads. So for that schedule, the GC overhead

161
00:12:09,772 --> 00:12:13,356
percentage range is divided into tiers called as GC overhead

162
00:12:13,388 --> 00:12:17,084
tiers. If the duration spent in GC overloads tier

163
00:12:17,212 --> 00:12:21,184
exceeds the volatility period for the tier, then the GC overload

164
00:12:21,232 --> 00:12:24,404
is signaled. The volatility period is smaller for

165
00:12:24,442 --> 00:12:28,504
higher GC over a tier as a higher GC over tier indicates more

166
00:12:28,542 --> 00:12:32,552
severe GC activity. For example GC over it

167
00:12:32,686 --> 00:12:36,304
of 10% or more for say 30 seconds

168
00:12:36,372 --> 00:12:40,076
for consecutive gcs is considered overload or

169
00:12:40,178 --> 00:12:43,468
lower tier such as Gc overhead of 8%

170
00:12:43,554 --> 00:12:47,848
or more for say like 60 seconds is considered overload

171
00:12:47,944 --> 00:12:51,484
and so on. So the rationale behind using percentage

172
00:12:51,532 --> 00:12:54,972
time in GC is it causes both GC duration

173
00:12:55,036 --> 00:12:58,210
and GC frequency that can catch different GC issues.

174
00:12:58,580 --> 00:13:02,676
And also setting a common threshold is possible which

175
00:13:02,778 --> 00:13:06,688
work across all the applications with different allocation rates,

176
00:13:06,784 --> 00:13:09,430
old generation occupancy levels and so on.

177
00:13:12,280 --> 00:13:15,844
So similar to cpu detector, this is an example of GC

178
00:13:15,892 --> 00:13:19,880
overload detector in action when GC activity increases

179
00:13:20,300 --> 00:13:24,088
because of increase in traffic. In the top left graph you

180
00:13:24,094 --> 00:13:27,304
can observe that these GC detector is able to capture

181
00:13:27,352 --> 00:13:31,320
a GC overload. Notice that the performance indicators

182
00:13:31,400 --> 00:13:34,744
such as p 90 p 99 latencies both spike

183
00:13:34,792 --> 00:13:37,100
when the GC detector flags an overload.

184
00:13:40,020 --> 00:13:43,884
Now we will look at a virtual resource overload detector.

185
00:13:44,012 --> 00:13:47,300
Study of QA time and its data

186
00:13:47,370 --> 00:13:50,944
suggests that there is a good correlation between increased KPIs,

187
00:13:51,072 --> 00:13:54,632
such as latencies, and increased thread queue at times

188
00:13:54,686 --> 00:13:58,200
for synchronous services. Consider a synchronous service

189
00:13:58,350 --> 00:14:02,280
requests will start spending more time waiting in a queue if current

190
00:14:02,350 --> 00:14:05,736
request processing time increases, either due to

191
00:14:05,758 --> 00:14:09,816
an issue in the service or in one of its downstreams.

192
00:14:09,928 --> 00:14:13,784
The capacity of a service can also be reached when latencies of downstream

193
00:14:13,832 --> 00:14:18,184
traffic increase, which can cause the number of concurrent requests

194
00:14:18,232 --> 00:14:21,952
being handled in the local service to increase with no change

195
00:14:22,006 --> 00:14:25,312
to the incoming request rate. But without knowing anything

196
00:14:25,366 --> 00:14:29,564
about these downstream service, we can assert at upstream by monitoring thread

197
00:14:29,612 --> 00:14:33,472
pull queue time that there is a thread pull starvation,

198
00:14:33,536 --> 00:14:37,232
and by dropped traffic we can help alleviate the downstream.

199
00:14:37,376 --> 00:14:40,724
At LinkedIn we use JT server side

200
00:14:40,762 --> 00:14:44,328
framework extensively and hence we target that as a

201
00:14:44,334 --> 00:14:47,752
first step. But the logic of observing thread pool q wet time

202
00:14:47,886 --> 00:14:49,640
is applicable widely,

203
00:14:53,260 --> 00:14:56,824
similar to the previous detectors. This is an example of the thread pool

204
00:14:56,872 --> 00:15:01,260
overload detector in action, where there is an issue in downstream processing

205
00:15:03,200 --> 00:15:06,316
that causes increase in the thread pool wet time. In the

206
00:15:06,338 --> 00:15:09,696
left top graph you can observe that the thread pool detects is able to

207
00:15:09,718 --> 00:15:12,956
capture an overload. Notice that the performance

208
00:15:12,988 --> 00:15:16,496
indicators such as average p 99 latency, spike when

209
00:15:16,518 --> 00:15:20,428
detector flags an overload. Now back to Brian

210
00:15:20,524 --> 00:15:23,890
who is going to talk about remediation techniques. Thank you.

211
00:15:25,620 --> 00:15:29,216
Thanks for that. So the question now is, once we've

212
00:15:29,248 --> 00:15:33,140
identified there's a problem, how can we address it with minimal impact?

213
00:15:33,960 --> 00:15:37,352
Well, we need to reduce the amount of work that a service is doing,

214
00:15:37,406 --> 00:15:40,904
and we do that by rejecting some requests. The trick here

215
00:15:40,942 --> 00:15:44,836
is to identify the proper amount of requests to reject, since dropping

216
00:15:44,868 --> 00:15:47,780
too many would have a negative impact on our users,

217
00:15:47,940 --> 00:15:51,356
we've tried and tested a few different load shedding approaches and found that

218
00:15:51,378 --> 00:15:55,550
the most effective is to limit the number of concurrent requests handled by a service.

219
00:15:55,920 --> 00:16:00,044
The load shedder adaptively determines the number of requests that need to be dropped

220
00:16:00,092 --> 00:16:03,676
by initially being somewhat aggressive while shedding the traffic,

221
00:16:03,788 --> 00:16:06,944
and then more conservative about allowing more traffic back

222
00:16:06,982 --> 00:16:10,268
in. When the load shedder drops requests, they're returned

223
00:16:10,284 --> 00:16:14,704
as five hundred and three s, and these can be retried on another healthy

224
00:16:14,752 --> 00:16:18,224
host if one is available. We experimented

225
00:16:18,272 --> 00:16:22,336
with other forms of adaptive load shedding, including using a percentage

226
00:16:22,368 --> 00:16:26,164
based threshold to adaptively control the amount of traffic handled

227
00:16:26,212 --> 00:16:29,816
or rejected. But during our tests we found that a percentage base

228
00:16:29,838 --> 00:16:33,316
shutter didn't really do that good of a job, especially when traffic

229
00:16:33,348 --> 00:16:36,740
patterns changed as it was continually needing to adapt

230
00:16:36,820 --> 00:16:40,396
to the new traffic levels, whether they were increasing or decreasing over

231
00:16:40,498 --> 00:16:43,708
previous thresholds. The graphs shown here

232
00:16:43,794 --> 00:16:47,064
are from one of the experiments we ran where the red host

233
00:16:47,112 --> 00:16:51,308
was unprotected and the blue host had load shedding enabled.

234
00:16:51,484 --> 00:16:55,724
They started off by receiving identical traffic levels until becoming overloaded

235
00:16:55,772 --> 00:16:59,120
where the behavior diverged. As you can see in the middle graph.

236
00:16:59,460 --> 00:17:03,776
You can see as the overall queries per second increases,

237
00:17:03,968 --> 00:17:07,744
the protected host is forced to increase the number of requests

238
00:17:07,792 --> 00:17:11,316
that are dropped. You can also see that the overall high

239
00:17:11,338 --> 00:17:15,364
percentile latency is lower on the protected host,

240
00:17:15,492 --> 00:17:19,096
but there are a few spikes where the load shutter is probing to

241
00:17:19,118 --> 00:17:22,436
see if the concurrency limit can be increased by slowly

242
00:17:22,468 --> 00:17:24,200
letting in more traffic.

243
00:17:26,060 --> 00:17:29,324
So I'd mentioned that holder rejects requests with 503s.

244
00:17:29,442 --> 00:17:32,860
This is done early on in the request pipeline before any business

245
00:17:32,930 --> 00:17:36,812
logic is executed so they're safe to retry on another

246
00:17:36,866 --> 00:17:40,380
healthy host. This reduces overall member impact

247
00:17:40,460 --> 00:17:43,600
because the 503 response is returned quickly

248
00:17:43,670 --> 00:17:48,016
to the client, giving it time to retry the request someplace else.

249
00:17:48,198 --> 00:17:51,748
But we don't want to blindly retry all requests that are

250
00:17:51,754 --> 00:17:55,760
dropped by Hodor because if all hosts in these cluster are overloaded,

251
00:17:55,840 --> 00:17:59,348
these sending additional retry traffic can actually make these problem

252
00:17:59,434 --> 00:18:03,184
worse. To prevent this, we've added functionality

253
00:18:03,232 --> 00:18:06,564
on the client and server side to be able to detect issues that are cluster

254
00:18:06,612 --> 00:18:11,032
wide and prevent retry storms. This is done by

255
00:18:11,086 --> 00:18:14,504
defining client and server side budgets and not

256
00:18:14,542 --> 00:18:17,420
retrying when any of these budgets have been exceeded.

257
00:18:19,200 --> 00:18:22,732
I'm going to talk briefly now about how we went about rolling this out to

258
00:18:22,786 --> 00:18:26,680
the hundreds of separate microservices

259
00:18:26,760 --> 00:18:30,440
that we operate at LinkedIn. So we needed to

260
00:18:30,450 --> 00:18:34,380
be cautious when rolling this out to make sure that we weren't causing impact

261
00:18:34,460 --> 00:18:37,536
to our members from any potential false positives from the

262
00:18:37,558 --> 00:18:41,852
detectors. We did this by enabling the detectors in monitoring mode,

263
00:18:41,916 --> 00:18:45,248
where the signal from the detectors is ignored by the load

264
00:18:45,264 --> 00:18:49,440
shutter, but all relevant metrics are still active and collected.

265
00:18:49,600 --> 00:18:53,176
So this allowed us to set up a pipeline for rollout where we could

266
00:18:53,198 --> 00:18:56,664
monitor when detectors were firing and correlate those

267
00:18:56,702 --> 00:19:00,200
events with other service metrics such as latency,

268
00:19:00,620 --> 00:19:04,500
cpu usage, garbage collection activity, et cetera.

269
00:19:04,580 --> 00:19:08,568
At the same time periods before enabling load shedding.

270
00:19:08,584 --> 00:19:12,664
Though, we monitored a service for at least a week, which would include

271
00:19:12,712 --> 00:19:16,524
load tests that were running in production during that time, we found that

272
00:19:16,562 --> 00:19:20,316
some services were not good candidates for onboarding using our default

273
00:19:20,348 --> 00:19:24,412
settings. These were almost always due to issues with garbage collection

274
00:19:24,556 --> 00:19:28,544
and could usually be solved by tuning the GC. In some

275
00:19:28,582 --> 00:19:32,616
cases, this actually led to significant discoveries around inefficient memory

276
00:19:32,668 --> 00:19:36,020
allocation and usage patterns, which needed to be addressed in the service

277
00:19:36,090 --> 00:19:39,460
but hadn't been surfaced before. Making changes

278
00:19:39,530 --> 00:19:43,476
to address these ended up being significant wins for these services as they led

279
00:19:43,508 --> 00:19:47,332
to reduced cpu usage, better overall performance

280
00:19:47,396 --> 00:19:50,996
and scalability, and they were able to onboard the hodor's

281
00:19:51,028 --> 00:19:53,000
protection as a side benefit.

282
00:19:53,980 --> 00:19:57,576
So at the bottom here is a quote from one of the teams

283
00:19:57,688 --> 00:20:01,276
after adoption adding overload detectors to

284
00:20:01,298 --> 00:20:05,144
our service has surfaced unexpected behavior that owners were generally

285
00:20:05,192 --> 00:20:09,500
not familiar with, and we've truly found some odd behavior

286
00:20:09,660 --> 00:20:13,696
surfaced by our detectors. For example, in one service

287
00:20:13,878 --> 00:20:17,628
we found that thread dumps were being automatically triggered and written

288
00:20:17,644 --> 00:20:21,488
to disk periodically based on a setting that the owners had enabled

289
00:20:21,504 --> 00:20:24,804
and forgotten about. The manifestation of this

290
00:20:24,842 --> 00:20:28,692
was periodic freezes of the JVM while the thread dumps were happening,

291
00:20:28,826 --> 00:20:32,932
which lasted over a few seconds in some cases, but this

292
00:20:32,986 --> 00:20:35,812
didn't register in our higher percentile metrics,

293
00:20:35,876 --> 00:20:39,908
so the service owners were never aware of the problem. Once onboarded

294
00:20:39,924 --> 00:20:43,236
to Hodor, though, it became very clear when the detectors fired

295
00:20:43,268 --> 00:20:46,812
and the load shutter engaged. There are other examples similar

296
00:20:46,866 --> 00:20:50,920
to this where fairly impactful and usually fairly interesting behavior

297
00:20:51,000 --> 00:20:54,030
went unnoticed until uncovered by our system.

298
00:20:55,440 --> 00:20:58,956
So next I'm going to go through a quick example from one of our production

299
00:20:58,988 --> 00:20:59,570
services.

300
00:21:01,860 --> 00:21:05,712
So this is from our flagship application

301
00:21:05,846 --> 00:21:09,628
which powers these main LinkedIn.com site as well as mobile

302
00:21:09,644 --> 00:21:13,436
clients for iOS and Android. So we periodically do traffic

303
00:21:13,468 --> 00:21:17,268
shifts between our data centers for various reasons. In one of

304
00:21:17,274 --> 00:21:20,820
these cases, there was a new bug that was introduced in the service that only

305
00:21:20,890 --> 00:21:23,780
appeared when the service was extremely stressed.

306
00:21:24,140 --> 00:21:27,172
This traffic shift event triggered the bug,

307
00:21:27,236 --> 00:21:30,330
and Hodor intervened aggressively to handle the situation.

308
00:21:31,100 --> 00:21:34,408
You can see in these top graph that Hodor engaged for a

309
00:21:34,414 --> 00:21:38,316
good amount of time with a few spikes which lined up directly with when

310
00:21:38,338 --> 00:21:41,000
latencies were spiking. Overall,

311
00:21:41,080 --> 00:21:44,280
about 20% of requests were dropped

312
00:21:44,440 --> 00:21:47,660
during this overload period, which sounds bad, but when

313
00:21:47,730 --> 00:21:51,868
sres investigated further, they found that if the load hadn't been shed,

314
00:21:51,964 --> 00:21:55,216
this would likely have become a major site issue instead of a

315
00:21:55,238 --> 00:21:58,832
minor one with a service down instead of still

316
00:21:58,886 --> 00:22:01,972
serving partial amounts of traffic. We currently

317
00:22:02,026 --> 00:22:05,972
have over 100 success stories similar to these where Hodor engaged to protect

318
00:22:06,026 --> 00:22:10,196
a service and mitigate an issue to

319
00:22:10,218 --> 00:22:13,916
end with. I'm going to talk about some related work that integrates

320
00:22:13,968 --> 00:22:17,770
well with Hodor and some things that we have planned for the future.

321
00:22:20,140 --> 00:22:23,896
First up is a project that has been integrated with Hodor and

322
00:22:23,918 --> 00:22:27,944
live for some amount of time. Our term for it is traffic tiering,

323
00:22:27,992 --> 00:22:31,176
but it's also known as traffic prioritization or traffic

324
00:22:31,208 --> 00:22:34,664
shaping. It's a pretty simple concept. Some requests

325
00:22:34,712 --> 00:22:38,364
can be considered more important than others. For example,

326
00:22:38,482 --> 00:22:42,032
our web and mobile applications will often prefetch some data

327
00:22:42,086 --> 00:22:46,464
that they expect might be used soon, but if it's not available, there's no

328
00:22:46,582 --> 00:22:50,528
actual impact to the user, it just gets fetched later

329
00:22:50,614 --> 00:22:54,132
on. Demand requests like this can be considered to be lower

330
00:22:54,186 --> 00:22:58,256
priority than directly fetching data that the user has requested.

331
00:22:58,448 --> 00:23:02,564
Similarly, we have offline jobs that utilize the same services

332
00:23:02,682 --> 00:23:05,956
that take user traffic, but nobody's sitting

333
00:23:05,988 --> 00:23:09,476
behind a screen waiting for that data. It's safe to retry

334
00:23:09,508 --> 00:23:13,332
those offline requests at a later time when the service isn't overloaded.

335
00:23:13,476 --> 00:23:17,016
So with traffic tiering, we're able to categorize different types

336
00:23:17,048 --> 00:23:20,412
of requests into different categories and start

337
00:23:20,466 --> 00:23:24,380
dropping traffic with the lowest importance at first, and only moving

338
00:23:24,530 --> 00:23:27,740
to affect higher priority traffic if necessary.

339
00:23:29,200 --> 00:23:33,196
Secondly, we're working on developing new types of detectors to cover blind

340
00:23:33,228 --> 00:23:36,864
spots in the ones we have. One of these is actually a method of

341
00:23:36,902 --> 00:23:41,052
confirming that the detected overload is impacting core metrics.

342
00:23:41,196 --> 00:23:44,784
So we've had cases of false positives where there is an underlying

343
00:23:44,832 --> 00:23:48,624
issue with these service, usually GC related, which isn't

344
00:23:48,672 --> 00:23:52,384
affecting the user perceived performance, but is impacting the ability

345
00:23:52,432 --> 00:23:55,800
of the app to scale further and maximize its capacity.

346
00:23:56,300 --> 00:23:59,672
In these cases, we don't want to drop traffic, but we do want

347
00:23:59,726 --> 00:24:03,636
to signal that there is an issue. So we're working on correlating CPU

348
00:24:03,668 --> 00:24:07,104
or GCU overload signals with latency metrics

349
00:24:07,172 --> 00:24:11,416
and only dropping traffic when there's a clear degradation in performance.

350
00:24:11,608 --> 00:24:15,384
We're also starting to adapt Hodor to frameworks other than restly,

351
00:24:15,432 --> 00:24:19,552
such as the play framework, as well as Neti. These have

352
00:24:19,606 --> 00:24:23,676
different threading models and in some cases don't work as well with the heartbeat

353
00:24:23,708 --> 00:24:26,992
based cpu detection. So for example, we're working

354
00:24:27,046 --> 00:24:30,988
on detects specifically for Netty's event loop threading

355
00:24:31,004 --> 00:24:35,184
model. Finally, we're looking into leveraging the overload

356
00:24:35,232 --> 00:24:38,628
signals to feed them into our elastic scaling system,

357
00:24:38,794 --> 00:24:42,752
so this seems like an obvious match. If a service cluster has become overloaded,

358
00:24:42,816 --> 00:24:45,800
we can just spin up some more instances to alleviate the problem,

359
00:24:45,870 --> 00:24:49,176
right? Well, it turns out it's not that simple, especially when

360
00:24:49,198 --> 00:24:52,344
there are a mix of stateful and stateless systems within an

361
00:24:52,382 --> 00:24:56,424
overloaded call tree. In many cases, just scaling up one

362
00:24:56,462 --> 00:24:59,684
cluster that is overloaded would just propagate the issue further

363
00:24:59,732 --> 00:25:03,720
downstream and cause even more issues. This is an area

364
00:25:03,790 --> 00:25:07,050
where we're still exploring and hope to address in the future.

365
00:25:08,980 --> 00:25:12,656
So I hope that this presentation was enlightening for you and

366
00:25:12,678 --> 00:25:15,824
you learned something new. Thank you for your time. I'd also

367
00:25:15,862 --> 00:25:19,488
like to thank the different teams at LinkedIn that came together to make this project

368
00:25:19,574 --> 00:25:22,816
possible and successful. That's it from

369
00:25:22,838 --> 00:25:24,460
us. Enjoy the rest of the conference.

