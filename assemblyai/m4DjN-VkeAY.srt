1
00:00:00,250 --> 00:00:01,630
Are you an SRE,

2
00:00:03,570 --> 00:00:04,830
a developer?

3
00:00:06,610 --> 00:00:10,014
A quality engineer who wants to tackle the challenge of

4
00:00:10,052 --> 00:00:13,534
improving reliability in your DevOps? You can enable your

5
00:00:13,572 --> 00:00:16,510
DevOps for reliability with chaos native.

6
00:00:16,930 --> 00:00:20,800
Create your free account at Chaos native Litmus Cloud

7
00:01:17,050 --> 00:01:20,742
hello and welcome to the SRE conference. In my session I'll be talking

8
00:01:20,796 --> 00:01:24,166
about improving observability when you're running your workloads on

9
00:01:24,188 --> 00:01:27,734
AWS. As part of this session, we will talk about the best

10
00:01:27,772 --> 00:01:31,558
practices that you can follow whenever your workloads are running on AWS

11
00:01:31,654 --> 00:01:35,462
by leveraging the native AWS services, and also touch upon

12
00:01:35,606 --> 00:01:39,478
some of the SRE practices like slis, slos and slas,

13
00:01:39,654 --> 00:01:43,214
and what kind of best practices you can follow when you are setting up

14
00:01:43,252 --> 00:01:47,290
these KPIs and slas for your workloads.

15
00:01:47,450 --> 00:01:50,638
With that being said, let's have a look at the agenda that we will be

16
00:01:50,644 --> 00:01:54,530
following today. We'll start with the introduction

17
00:01:54,870 --> 00:01:58,114
where we talk about observability. What are the key

18
00:01:58,152 --> 00:02:02,254
signals that you'll be looking out for whenever you're implementing observability or improving

19
00:02:02,302 --> 00:02:05,814
observability for your workloads? Then we'll see the

20
00:02:05,852 --> 00:02:10,066
different features which are available in AWS,

21
00:02:10,178 --> 00:02:13,906
AWS part of Amazon Cloudwatch and AWS Xray

22
00:02:14,018 --> 00:02:16,920
to understand the state and behavior of your application.

23
00:02:17,690 --> 00:02:20,822
Finally, we will touch upon the importance of slis,

24
00:02:20,886 --> 00:02:24,746
slos and slas, and then we'll close off the session with

25
00:02:24,768 --> 00:02:28,380
summarizing whatever we have learned for the next 30 minutes.

26
00:02:29,470 --> 00:02:33,790
So let's start with the first part. What is observability?

27
00:02:35,090 --> 00:02:39,690
As we are trying to develop more and more applications in a distributed ecosystem,

28
00:02:39,850 --> 00:02:44,530
observability has suddenly become a very important aspect of all these applications,

29
00:02:45,110 --> 00:02:48,466
especially when it comes to defining observability. There are

30
00:02:48,488 --> 00:02:51,970
many definitions that you would find in different

31
00:02:52,040 --> 00:02:55,474
websites and resources which are available. The way

32
00:02:55,512 --> 00:02:59,746
you can put observability in the most simplest form is it helps

33
00:02:59,778 --> 00:03:02,440
you understand what is happening in your system.

34
00:03:03,130 --> 00:03:06,406
As a counterpoint to that, you may ask, oh well, I have logs or

35
00:03:06,428 --> 00:03:10,326
I have metrics or I have tracing. I have implemented

36
00:03:10,358 --> 00:03:14,234
one of these three different aspects which are already there in my environment. Doesn't that

37
00:03:14,272 --> 00:03:17,478
give me observability? Well, look at observability

38
00:03:17,574 --> 00:03:20,954
as a correlation of all these different signals that

39
00:03:20,992 --> 00:03:25,150
you would implement if you are running applications in a distributed environment.

40
00:03:25,970 --> 00:03:29,198
Some of the questions that you would want to ask yourself is

41
00:03:29,364 --> 00:03:32,926
is my system up or is it down? Is it fast or

42
00:03:32,948 --> 00:03:35,490
slow? Based on the experience of the end users,

43
00:03:35,910 --> 00:03:38,994
any kind of KPIs and slas which

44
00:03:39,032 --> 00:03:42,434
I am establishing how would I know that I am meeting them?

45
00:03:42,632 --> 00:03:45,814
These are some of the questions for which observability can

46
00:03:45,852 --> 00:03:49,526
help you answer, especially when you

47
00:03:49,548 --> 00:03:53,270
are running your applications at scale and in the cloud.

48
00:03:53,420 --> 00:03:56,614
You cannot afford to be blind to all the different

49
00:03:56,812 --> 00:04:00,630
aspects of running applications in a distributed environment.

50
00:04:00,790 --> 00:04:04,170
You need to be able to answer a wide range of operational and

51
00:04:04,240 --> 00:04:07,654
business related questions. You should also be able to spot

52
00:04:07,702 --> 00:04:11,134
problems which ideally before they would disrupt your

53
00:04:11,172 --> 00:04:14,766
operations. And it should also be able to

54
00:04:14,788 --> 00:04:18,634
respond quickly to any kind of issues that you would see arising

55
00:04:18,682 --> 00:04:21,902
from a customer. To achieve all this

56
00:04:21,956 --> 00:04:25,598
insight, you need your systems to be extremely observable.

57
00:04:25,774 --> 00:04:29,746
Now, obviously there are many different ways of improving your

58
00:04:29,768 --> 00:04:33,250
system's observability. We will be focusing to begin with

59
00:04:33,320 --> 00:04:36,030
on the three primary signals which are logs,

60
00:04:36,110 --> 00:04:39,334
metrics and traces. Most of you would

61
00:04:39,372 --> 00:04:43,302
be implementing at least one of these in your system, or maybe

62
00:04:43,356 --> 00:04:46,786
all three of them if you have a very mature observability practice

63
00:04:46,818 --> 00:04:49,240
in your organization or in your project.

64
00:04:50,410 --> 00:04:54,330
There is also a white paper from CNCF which describes observability

65
00:04:54,670 --> 00:04:58,058
and it's still a work in progress, but you can surely have a look at

66
00:04:58,064 --> 00:05:01,680
it on their GitHub repo to know more about how

67
00:05:02,130 --> 00:05:05,530
the cloud native foundation is thinking about observability

68
00:05:05,610 --> 00:05:08,160
and what kind of recommendations they are giving.

69
00:05:10,930 --> 00:05:15,070
Now one question that you can ask is is observability

70
00:05:15,150 --> 00:05:18,402
new for these software systems? I would say no,

71
00:05:18,536 --> 00:05:22,654
because the three signals that I spoke about in the previous slide,

72
00:05:22,782 --> 00:05:27,086
which is log metric and traces logs,

73
00:05:27,118 --> 00:05:30,406
have always been used for most of your debugging and for

74
00:05:30,428 --> 00:05:33,974
identifying the root cause of your issues. Take an example of

75
00:05:34,012 --> 00:05:37,334
anything that would go wrong in your application even

76
00:05:37,372 --> 00:05:40,730
before you were running them. In cloud, there will always be a log

77
00:05:40,800 --> 00:05:44,234
which is sitting somewhere in your virtualbox which

78
00:05:44,272 --> 00:05:47,914
can be used for identifying what is the root cause or something that went

79
00:05:47,952 --> 00:05:51,450
wrong. Then you have metrics.

80
00:05:51,610 --> 00:05:54,794
The metrics have always been used whenever your applications

81
00:05:54,842 --> 00:05:58,302
have been running for a very long

82
00:05:58,356 --> 00:06:02,398
time. Or maybe you are trying to understand the different

83
00:06:02,564 --> 00:06:06,098
infrastructure related issues which may happen. Let's say something went wrong,

84
00:06:06,184 --> 00:06:09,266
or the cpu is too high, the memory is too

85
00:06:09,288 --> 00:06:12,642
high, or even during performance testing you would notice that certain

86
00:06:12,696 --> 00:06:16,306
aspects of the application are not behaving the way it was intended

87
00:06:16,338 --> 00:06:20,310
to be. You would be relying on metrics and finally traces.

88
00:06:20,730 --> 00:06:24,642
Traces help you understand how a request is traversing

89
00:06:24,706 --> 00:06:28,074
from point a to point b and what all

90
00:06:28,112 --> 00:06:31,846
different services it is impacting even downstream as that request

91
00:06:31,878 --> 00:06:34,986
is traversing. So all in all,

92
00:06:35,088 --> 00:06:38,938
these signals together or individually have always been used

93
00:06:39,024 --> 00:06:42,334
by different software development teams during their

94
00:06:42,372 --> 00:06:46,282
development, testing, your operations, or even maintenance

95
00:06:46,346 --> 00:06:49,902
of the software systems. So observability is not

96
00:06:49,956 --> 00:06:53,406
something which is leveraging concepts which

97
00:06:53,428 --> 00:06:57,422
you are not aware of. Rather it is correlating

98
00:06:57,486 --> 00:07:00,594
all these different signals which you have always been seeing about the

99
00:07:00,632 --> 00:07:03,986
application and giving you a consolidated picture of what

100
00:07:04,008 --> 00:07:05,460
is the state of your application.

101
00:07:08,360 --> 00:07:11,316
So let's go a little bit in depth into each of these.

102
00:07:11,498 --> 00:07:15,632
What are logs? The way I look at it, the logs

103
00:07:15,696 --> 00:07:18,732
can be split into four different categories.

104
00:07:18,896 --> 00:07:22,228
Application logs, system logs, audit logs,

105
00:07:22,244 --> 00:07:25,492
and infrastructure logs. You would see the same segregation

106
00:07:25,556 --> 00:07:29,524
from the CNCF paper as well. The application logs

107
00:07:29,572 --> 00:07:33,532
are the normal logs that you would see as part of your

108
00:07:33,666 --> 00:07:36,444
Java application or your Python application.

109
00:07:36,562 --> 00:07:40,284
Any kind of log appending that you're doing that becomes your

110
00:07:40,322 --> 00:07:43,896
application logs. What about system logs?

111
00:07:44,088 --> 00:07:47,676
These would be from your oss. Let's say you are having some

112
00:07:47,698 --> 00:07:51,244
kind of AMI which are running and that AMI

113
00:07:51,292 --> 00:07:54,896
is having errors, and those would be the logs which you'll be capturing here.

114
00:07:55,078 --> 00:07:58,596
Audit logs is something like an example of what action has been

115
00:07:58,618 --> 00:08:02,144
done by what user, what was the entire trace

116
00:08:02,192 --> 00:08:05,856
of a specific business outcome which has been done by a user

117
00:08:05,968 --> 00:08:10,068
or even cloud trail. If you have implemented cloud trail in AWS,

118
00:08:10,164 --> 00:08:13,320
that's again audit logging. And finally

119
00:08:13,390 --> 00:08:16,776
infrastructure logs. Whenever you are building out your

120
00:08:16,798 --> 00:08:20,788
infrastructure, maybe by using the cloud native services

121
00:08:20,974 --> 00:08:24,488
or even by using let's say chef

122
00:08:24,584 --> 00:08:28,076
or terraform or cloudformation, any kind

123
00:08:28,098 --> 00:08:31,736
of logs that it would generate from there. Again, those are infrastructure

124
00:08:31,768 --> 00:08:35,512
logs. Once you have segregated

125
00:08:35,576 --> 00:08:38,956
these logs into four different categories, you can ideally

126
00:08:38,988 --> 00:08:42,332
look at the logging levels that you are having, and this applies

127
00:08:42,396 --> 00:08:46,076
predominantly on the application side where you have the trace,

128
00:08:46,188 --> 00:08:49,408
debug, born info and error.

129
00:08:49,584 --> 00:08:53,220
These are different log levels. Depending on the environment that you are

130
00:08:53,370 --> 00:08:56,480
using, you should keep changing these log levels.

131
00:08:56,640 --> 00:09:00,880
Try to have only the error logs at the production environment

132
00:09:00,960 --> 00:09:04,196
and use the rest of the logging levels in the lower environments.

133
00:09:04,388 --> 00:09:08,200
Sometimes too much of logging can also result in performance issues because of

134
00:09:08,350 --> 00:09:11,916
all the file I O actions which are happening behind the

135
00:09:11,938 --> 00:09:15,212
scenes. One more point to be

136
00:09:15,266 --> 00:09:19,020
careful about is avoid printing sensitive information in the logs.

137
00:09:19,520 --> 00:09:23,144
Each business would define their own explanation

138
00:09:23,192 --> 00:09:26,444
of what is a sensitive information, but you

139
00:09:26,482 --> 00:09:29,984
should have mechanisms in place in your project or in your

140
00:09:30,022 --> 00:09:33,328
organization which looks at any kind of data which

141
00:09:33,334 --> 00:09:37,170
has been printed into the logs and to determine if they are sensitive or not.

142
00:09:38,500 --> 00:09:42,420
Next point would be defining a consistent logging pattern. Quite often

143
00:09:42,490 --> 00:09:45,908
it's very difficult to have a schema which you will follow for a

144
00:09:45,914 --> 00:09:49,348
long duration, especially when you are writing logs. So you

145
00:09:49,354 --> 00:09:52,664
can use a log format, maybe by using log four

146
00:09:52,702 --> 00:09:56,072
j or logback or any of the logging frameworks that you have,

147
00:09:56,206 --> 00:09:59,524
and you can define a consistent logging format.

148
00:09:59,652 --> 00:10:03,244
The advantage is it helps you extract the information from

149
00:10:03,282 --> 00:10:07,560
these logs during analytics. So you may be having a centralized

150
00:10:07,720 --> 00:10:11,404
logging location where all these logs are being read by

151
00:10:11,602 --> 00:10:14,728
some team trying to find data out of it,

152
00:10:14,754 --> 00:10:18,764
or do some analytics on the logs. A consistent logging format

153
00:10:18,812 --> 00:10:22,928
will help you ease that entire process of extracting information

154
00:10:23,014 --> 00:10:27,244
from the logs. And finally, when you are running container based workloads,

155
00:10:27,292 --> 00:10:30,704
it is preferred to write your logs into a standard output

156
00:10:30,752 --> 00:10:34,996
or AWS, an SD out. And when you're writing it into an STD out,

157
00:10:35,178 --> 00:10:38,596
all these logs can be aggregated. Let's say

158
00:10:38,618 --> 00:10:41,812
you're running your application in a Kubernetes

159
00:10:41,876 --> 00:10:45,208
cluster. You can have some daemon set operation which

160
00:10:45,214 --> 00:10:48,872
is running behind the scenes to aggregate all these logs by possibly using

161
00:10:48,926 --> 00:10:52,372
fluentbit and send it across to the destination of your choice,

162
00:10:52,436 --> 00:10:56,424
which can be a Cloudwatch logs in the case of AWS,

163
00:10:56,552 --> 00:11:01,048
or you can even run those to send the logs to an elasticsearch

164
00:11:01,144 --> 00:11:04,736
instance, which may be running on cloud or on prem, depending on

165
00:11:04,758 --> 00:11:09,616
whatever is your choice of architecture with

166
00:11:09,638 --> 00:11:13,600
that information on logs. Let's have a look at metrics.

167
00:11:14,180 --> 00:11:17,468
Metrics often define as the performance of

168
00:11:17,494 --> 00:11:20,496
your system. So when we say metrics,

169
00:11:20,528 --> 00:11:23,892
it can be the cpu information that you're having. Let's say

170
00:11:23,946 --> 00:11:27,872
one of my cpu is going beyond the 70% threshold.

171
00:11:28,016 --> 00:11:31,556
That's a metric which says my vm is having too

172
00:11:31,578 --> 00:11:34,964
much of load happening on it, or maybe there is some out of memory

173
00:11:35,092 --> 00:11:39,032
that is going to happen. So there is a threshold of say 70 80%

174
00:11:39,086 --> 00:11:42,440
that whatever threshold you have set up that is exceeding.

175
00:11:42,940 --> 00:11:46,296
That's the metric for you. You can aggregate different types

176
00:11:46,328 --> 00:11:49,960
of data as part of this metric. It can be a numeric representation,

177
00:11:50,040 --> 00:11:52,910
it can be a point in time observation of a system.

178
00:11:53,280 --> 00:11:56,450
It can have different cardinality associated with it.

179
00:11:57,060 --> 00:12:00,556
The metrics are essentially divided into two different types.

180
00:12:00,668 --> 00:12:04,364
One is real time monitoring and alerting, and second is trend

181
00:12:04,412 --> 00:12:07,932
analysis and long term planning as

182
00:12:08,006 --> 00:12:11,824
it's self explanatory. Real time monitoring and alerting will immediately

183
00:12:11,872 --> 00:12:15,696
tell you if something is wrong with your application. So that's

184
00:12:15,728 --> 00:12:19,236
where if you have the right set of alarms, you have the right set

185
00:12:19,258 --> 00:12:22,644
of notification and other bits in place, you would immediately

186
00:12:22,692 --> 00:12:27,396
know that a specific application is not showing

187
00:12:27,428 --> 00:12:30,676
the right metrics, or maybe having too many 404 errors,

188
00:12:30,708 --> 00:12:33,816
or too many 500 errors. Those are again metrics.

189
00:12:33,848 --> 00:12:36,590
It gives you account of what is happening with your application,

190
00:12:37,280 --> 00:12:41,128
the trend analysis over time it helps

191
00:12:41,144 --> 00:12:44,688
you do the right sizing. For example, if you are running a new application

192
00:12:44,854 --> 00:12:48,272
and you know that the application is just getting

193
00:12:48,326 --> 00:12:52,284
started. So possibly you would use a lower configuration

194
00:12:52,332 --> 00:12:55,616
of a vm, let's say two virtual cpus and

195
00:12:55,638 --> 00:12:59,308
four gb of ram. Over time, as your application

196
00:12:59,414 --> 00:13:02,644
scales out, you would want to increase the cpu or maybe

197
00:13:02,682 --> 00:13:05,924
scale out. Those kind of long term planning can

198
00:13:05,962 --> 00:13:09,716
again be done based on the trend of what these metrics are coming in and

199
00:13:09,738 --> 00:13:12,884
what is the observation from the historical data. So metrics

200
00:13:12,932 --> 00:13:16,328
help you guide the future of

201
00:13:16,494 --> 00:13:20,276
what needs to be done for your application and also give a snapshot

202
00:13:20,308 --> 00:13:22,590
of what is currently happening with your application.

203
00:13:24,960 --> 00:13:29,368
The third signal that we have spoken about in the previous slide is tracing.

204
00:13:29,544 --> 00:13:32,892
So distributed tracing has become increasingly important

205
00:13:33,026 --> 00:13:36,080
when you are running your system in a distributed environment.

206
00:13:36,500 --> 00:13:40,284
For example, a request which is initiated by your end user

207
00:13:40,332 --> 00:13:43,616
or your customer from your point a to

208
00:13:43,638 --> 00:13:47,152
point b, by having the right tracing

209
00:13:47,216 --> 00:13:50,964
in place, it shows the effect across all

210
00:13:51,002 --> 00:13:54,230
the different downstream services which are there.

211
00:13:54,760 --> 00:13:58,724
Quite often. When you're running a microservices architecture, it's not always

212
00:13:58,842 --> 00:14:02,344
one microservice, it's a collection. So you would have three or four

213
00:14:02,382 --> 00:14:06,116
microservices interacting with each other behind the scenes. So if you're

214
00:14:06,148 --> 00:14:09,812
going from microservice a to d, the tracing

215
00:14:09,876 --> 00:14:13,708
will help you show the information of what is happening

216
00:14:13,874 --> 00:14:17,804
when the request is going from a to b, b to c, and c

217
00:14:17,842 --> 00:14:21,150
to D. So it gives you information of all the

218
00:14:21,760 --> 00:14:25,552
lag or the latency that you are having at different

219
00:14:25,686 --> 00:14:28,896
levels of your application. And that way it gives you a

220
00:14:28,918 --> 00:14:32,320
holistic picture of a request or a journey of a request.

221
00:14:33,140 --> 00:14:35,570
Okay, moving on.

222
00:14:37,720 --> 00:14:41,140
Now that we have established what observability means

223
00:14:41,210 --> 00:14:44,484
and what are the different signals that we will be talking about in

224
00:14:44,522 --> 00:14:48,576
observability, we will discuss about the

225
00:14:48,618 --> 00:14:52,410
ways by which you can understand the state and behavior of the application.

226
00:14:53,180 --> 00:14:56,424
For this purpose, I will be focusing on two main

227
00:14:56,462 --> 00:14:59,930
services. In AWS. There are different

228
00:15:00,380 --> 00:15:03,916
observability services which are available both in the

229
00:15:03,938 --> 00:15:07,324
open source as well as in AWS. For this

230
00:15:07,362 --> 00:15:10,972
particular presentation, I'll be focusing on Amazon Cloudwatch and

231
00:15:11,026 --> 00:15:14,384
AWS X ray. Amazon Cloudwatch will take care

232
00:15:14,422 --> 00:15:18,396
of your metrics and your logs, and AWS Xray provides

233
00:15:18,428 --> 00:15:20,000
you the tracing capability.

234
00:15:23,860 --> 00:15:27,316
So let's consider a use case. Let's say that

235
00:15:27,338 --> 00:15:31,392
I want to monitor a microservices architecture and this microservices

236
00:15:31,456 --> 00:15:34,816
architecture consists of different distributed parts

237
00:15:34,848 --> 00:15:38,464
which needs to be monitored. Obviously for the logging

238
00:15:38,512 --> 00:15:42,504
aspects, you can make use of Amazon Cloud watch and it can

239
00:15:42,542 --> 00:15:46,740
also be used for collecting the metrics, setting up the alarms

240
00:15:46,820 --> 00:15:51,000
and reacting to certain changes which are happening in your AWS environment,

241
00:15:51,740 --> 00:15:54,988
and also with many different microservices working together.

242
00:15:55,154 --> 00:15:58,444
You want to know what is the chain of

243
00:15:58,482 --> 00:16:00,780
invocation for these microservices?

244
00:16:01,200 --> 00:16:05,516
And that's where the idea of xray comes in, which uses

245
00:16:05,548 --> 00:16:09,708
the correlation ids which are unique identifiers attached

246
00:16:09,724 --> 00:16:12,928
to all the requests and messages related to a specific

247
00:16:13,014 --> 00:16:16,356
event chain. So let's take an

248
00:16:16,378 --> 00:16:19,236
example the same way.

249
00:16:19,338 --> 00:16:23,456
Service ABC if I'm trying to do a get operation

250
00:16:23,648 --> 00:16:27,296
on service a behind the scenes, it may be fetching

251
00:16:27,328 --> 00:16:30,756
the data from service B and C and consolidating it and give

252
00:16:30,778 --> 00:16:34,040
it back to me. That's where Xray will help

253
00:16:34,190 --> 00:16:37,524
relate all these different invocations by using the correlation

254
00:16:37,572 --> 00:16:41,564
id and you can get a consolidated view of how your application is

255
00:16:41,602 --> 00:16:45,420
behaving with the different inter service

256
00:16:45,490 --> 00:16:49,084
communication and also the overall response when it is being

257
00:16:49,122 --> 00:16:50,780
provided back to the user.

258
00:16:55,220 --> 00:16:58,688
So as I mentioned earlier, you have two services

259
00:16:58,774 --> 00:17:01,804
which we'll be focusing here for this presentation,

260
00:17:01,932 --> 00:17:05,632
Amazon Cloudwatch and x ray. So Cloudwatch is

261
00:17:05,686 --> 00:17:09,396
a monitoring and management service that provides the data

262
00:17:09,498 --> 00:17:13,156
and insights into AWS. With the help of

263
00:17:13,178 --> 00:17:16,592
Cloudwatch, you will be able to get all your logs consolidated

264
00:17:16,656 --> 00:17:20,996
into Cloudwatch logs. You can have the container

265
00:17:21,028 --> 00:17:24,228
insights in order to get information around the metrics

266
00:17:24,404 --> 00:17:28,276
on containers which may be running on your ECS or eks

267
00:17:28,468 --> 00:17:32,196
orchestrators. You can make use of service lens

268
00:17:32,228 --> 00:17:35,836
to see how the different services are related to each other. You can make use

269
00:17:35,858 --> 00:17:39,564
of metrics and alarms. The metrics will help you get

270
00:17:39,602 --> 00:17:43,192
data on number of 400 errors

271
00:17:43,256 --> 00:17:47,436
or the 500 errors which you are seeing for an application. Load balancer

272
00:17:47,548 --> 00:17:50,880
and alarms will help you set up threshold. For example,

273
00:17:50,950 --> 00:17:54,572
if I have a cpu which is going beyond

274
00:17:54,636 --> 00:17:57,848
70%, have an alarm

275
00:17:57,884 --> 00:18:01,584
which is going off and telling me that okay, this is an error

276
00:18:01,632 --> 00:18:05,264
and there is something happening consistently where the cpu threshold

277
00:18:05,312 --> 00:18:09,328
is beyond 70%, you can also have anomaly

278
00:18:09,344 --> 00:18:12,884
detection and that's where the earlier part of metrics

279
00:18:12,932 --> 00:18:16,564
being able to help you with the long term trend

280
00:18:16,612 --> 00:18:20,136
analytics comes into picture. Because at any point of time, if there

281
00:18:20,158 --> 00:18:23,032
is an anomaly in your overall operation,

282
00:18:23,176 --> 00:18:26,270
the metrics will help you catch that.

283
00:18:27,360 --> 00:18:31,192
Talking about AWS x ray, that's basically for tracing and analytics.

284
00:18:31,256 --> 00:18:35,056
This will also give you a service map and it helps the

285
00:18:35,078 --> 00:18:39,004
developers to analyze and debug their production workloads,

286
00:18:39,052 --> 00:18:42,464
especially in a distributed environment. It can understand how your

287
00:18:42,502 --> 00:18:45,776
application and the underlying services are performing. And you can

288
00:18:45,798 --> 00:18:49,216
use Xray to both analyze the applications in

289
00:18:49,238 --> 00:18:52,710
production as well as in development. And there are different

290
00:18:53,160 --> 00:18:56,996
environments based on each of the customer. You can surely have x ray do

291
00:18:57,018 --> 00:18:58,900
all that in each of those environments.

292
00:19:03,900 --> 00:19:07,224
Now let's look at the different components that you have with

293
00:19:07,262 --> 00:19:10,984
Amazon Cloudwatch. I did give a brief overview of

294
00:19:11,182 --> 00:19:14,648
Amazon Cloudwatch in the previous slide. Let's take an example

295
00:19:14,734 --> 00:19:17,756
of one of the resource that we have here. Let's say

296
00:19:17,778 --> 00:19:21,580
it's an ALb or maybe you have some kind of custom data

297
00:19:21,650 --> 00:19:25,184
which you need to send. So right in the left you will be sending this

298
00:19:25,222 --> 00:19:28,656
custom data into Cloudwatch, and Cloudwatch will be

299
00:19:28,678 --> 00:19:32,610
responsible for getting all these metrics put together.

300
00:19:33,220 --> 00:19:37,332
So you can see an example here of the cpu percentage hours per

301
00:19:37,386 --> 00:19:40,896
week, and again cpu percentage, et cetera, et cetera.

302
00:19:41,088 --> 00:19:45,220
So all these metrics are being put together by Cloudwatch

303
00:19:45,720 --> 00:19:49,904
at any point of time. If a metric is exceeding

304
00:19:49,952 --> 00:19:53,224
or going beyond a specific threshold that you would have set,

305
00:19:53,342 --> 00:19:56,644
you can have a Cloudwatch alarm go off and that alarm

306
00:19:56,692 --> 00:20:00,664
can integrate with an SNS email notification which

307
00:20:00,702 --> 00:20:03,988
will inform your team that there is something wrong in your application.

308
00:20:04,094 --> 00:20:07,416
You need to possibly go and have a look at that. If it's

309
00:20:07,448 --> 00:20:11,244
not a notification, you can also have auto scaling being

310
00:20:11,282 --> 00:20:15,132
invoked. So let's take an example that I have one virtual

311
00:20:15,196 --> 00:20:18,524
cpu and the threshold

312
00:20:18,572 --> 00:20:22,544
is saying that the cpu percentage has gone beyond 70%.

313
00:20:22,742 --> 00:20:26,210
I would like to have auto scaling kick in at this point of time.

314
00:20:26,580 --> 00:20:29,824
So that's where I can use Amazon Cloud watch alarms

315
00:20:29,952 --> 00:20:33,716
and I can trigger an auto scaling so that my cpu count,

316
00:20:33,818 --> 00:20:37,252
the EC two count for that matter, can go from a single

317
00:20:37,386 --> 00:20:41,344
instance to a fleet consisting of three or four different instances,

318
00:20:41,392 --> 00:20:44,548
and then it can scale in as well once the threshold

319
00:20:44,644 --> 00:20:48,520
is coming back to normal. And as a consumer, you can

320
00:20:48,590 --> 00:20:51,964
have a look at all these statistics. You can plot the graphs which

321
00:20:52,002 --> 00:20:54,990
we will have a look at it in the next slide. Aws well,

322
00:21:00,560 --> 00:21:03,916
now let's look at log insights. I did mention that the

323
00:21:03,938 --> 00:21:07,228
logs are getting pushed to Cloudwatch logs,

324
00:21:07,324 --> 00:21:11,004
and once the logs are available in Cloudwatch logs, you can execute

325
00:21:11,052 --> 00:21:14,560
this query. And this is a query which is

326
00:21:14,630 --> 00:21:17,350
being executed on Cloudwatch logs itself,

327
00:21:17,800 --> 00:21:21,764
which helps you parse certain parts of that

328
00:21:21,962 --> 00:21:25,712
particular log format and filter all the logs

329
00:21:25,776 --> 00:21:29,752
which are having a logging type of error. And this is really

330
00:21:29,806 --> 00:21:33,796
helpful when you have all these logs getting consolidated

331
00:21:33,908 --> 00:21:37,224
as part of your log groups within

332
00:21:37,342 --> 00:21:40,904
Amazon Cloudwatch. And after that you can see in

333
00:21:40,942 --> 00:21:45,032
this particular screenshot how you can show the distribution

334
00:21:45,096 --> 00:21:48,636
of the log events over time. Even the custom log events can

335
00:21:48,658 --> 00:21:52,332
be seen here. So the log insights help

336
00:21:52,386 --> 00:21:55,708
you look at what has happened, say from

337
00:21:55,874 --> 00:21:59,264
01:00 a.m. All the way to 02:00 a.m.

338
00:21:59,382 --> 00:22:02,464
And it will be able to consolidate all that information,

339
00:22:02,662 --> 00:22:06,096
show you the distribution of the events which have happened, and you

340
00:22:06,118 --> 00:22:09,280
can also find specific events which you would have otherwise

341
00:22:09,360 --> 00:22:12,612
missed when the logs are being manually looked at.

342
00:22:12,746 --> 00:22:15,620
So that's the advantage of having the log insights.

343
00:22:17,480 --> 00:22:21,432
The sample query here, it fetches the timestamp and

344
00:22:21,566 --> 00:22:24,916
it fetches the message fields, and it orders the timestamp

345
00:22:24,948 --> 00:22:26,170
in the descending order.

346
00:22:30,340 --> 00:22:33,996
So with the logs in place, let's talk about the metrics.

347
00:22:34,188 --> 00:22:38,304
We did see in the earlier example that the metrics are being exposed

348
00:22:38,352 --> 00:22:42,500
for different AWS services. Now these metrics are being grouped by

349
00:22:42,570 --> 00:22:46,276
namespace and then by the various dimensions which are associated

350
00:22:46,308 --> 00:22:50,004
with that. For example, here in this screenshot,

351
00:22:50,052 --> 00:22:53,972
you can see that all the custom namespaces like the container insights,

352
00:22:54,036 --> 00:22:58,036
the Prometheus related stuff, or the ecs container

353
00:22:58,068 --> 00:23:01,496
insights, all these are custom metrics which have been added

354
00:23:01,528 --> 00:23:05,496
into the custom namespaces. And then you have the AWS related namespace

355
00:23:05,528 --> 00:23:08,856
like API, gateway, application, load balancers,

356
00:23:08,968 --> 00:23:12,524
dynamodb and ebs. These are the default namespaces

357
00:23:12,572 --> 00:23:16,256
from different AWS services which can be used in your account.

358
00:23:16,438 --> 00:23:19,664
So this way the metrics section will

359
00:23:19,702 --> 00:23:23,084
help you find all the related metrics which are

360
00:23:23,222 --> 00:23:26,768
for a specific service or for the custom events

361
00:23:26,784 --> 00:23:29,030
which are being consolidated by your team.

362
00:23:33,640 --> 00:23:37,876
Next we have the graphed metrics. It's very related

363
00:23:37,908 --> 00:23:41,928
to the metrics that you saw in the earlier slide. It helps you

364
00:23:42,014 --> 00:23:45,908
basically run statistics on your metrics which can be average,

365
00:23:46,004 --> 00:23:49,108
minimum, maximum sum, et cetera.

366
00:23:49,284 --> 00:23:52,844
And if you look here at the right side, what you see as

367
00:23:52,882 --> 00:23:56,284
the red circle, it also helps you add the

368
00:23:56,322 --> 00:24:00,044
anomaly detection band where you're saying that this is

369
00:24:00,082 --> 00:24:03,296
what my application is normally behaviours, but at

370
00:24:03,318 --> 00:24:06,252
a certain aspects it helps you identify,

371
00:24:06,316 --> 00:24:09,744
okay, this is an anomaly. So possibly something went wrong, or maybe

372
00:24:09,782 --> 00:24:13,476
you got a very high spike of incoming traffic at

373
00:24:13,498 --> 00:24:17,284
that point in time. So it helps you identify such anomalies in

374
00:24:17,322 --> 00:24:20,516
your regular flow of traffic and obviously in your

375
00:24:20,538 --> 00:24:21,380
metrics.

376
00:24:25,560 --> 00:24:29,112
We have been talking about alarms for quite a while. So there are basically

377
00:24:29,166 --> 00:24:33,204
two types of alarms. One would be the basic metric alarms and others are composite

378
00:24:33,252 --> 00:24:36,664
alarms. So we saw that there

379
00:24:36,702 --> 00:24:40,556
are metrics. So if I am using just one metric, and I

380
00:24:40,578 --> 00:24:44,040
am using that for creating an alarm,

381
00:24:44,120 --> 00:24:47,464
then that would be a metric alarm, a composite

382
00:24:47,512 --> 00:24:51,432
alarm. It would basically include a rule expression,

383
00:24:51,496 --> 00:24:55,168
and it needs to take into account the different states which are there in

384
00:24:55,174 --> 00:24:59,024
the alarm. So the threshold of the alarm, for example,

385
00:24:59,142 --> 00:25:02,656
whenever you are setting an EC two auto scaling event,

386
00:25:02,838 --> 00:25:06,304
you would set up say that it needs to scale out whenever

387
00:25:06,352 --> 00:25:09,892
it is 90% and above cpu utilization. And then it would

388
00:25:09,946 --> 00:25:14,176
scale in whenever it's 50% or something cpu

389
00:25:14,208 --> 00:25:18,120
utilization. So that's a threshold that you would set up for an alarm.

390
00:25:18,700 --> 00:25:22,760
So you can see here that the blue line

391
00:25:22,830 --> 00:25:26,424
that is visible here, that's the threshold that you have set up.

392
00:25:26,622 --> 00:25:30,216
The value is the red one. So the statistic

393
00:25:30,248 --> 00:25:33,836
that you are measuring, that would determine whether your

394
00:25:33,938 --> 00:25:37,580
alarm is enabled or it is okay,

395
00:25:37,650 --> 00:25:42,424
or it is an in alarm. So after three periods

396
00:25:42,472 --> 00:25:46,736
over the threshold, where you can say that, I'm not going to say

397
00:25:46,758 --> 00:25:50,160
that the particular metric is an alarm unless it happens

398
00:25:50,230 --> 00:25:53,728
three times in a specific period. So that's what you're seeing here,

399
00:25:53,814 --> 00:25:57,252
where the value has become three

400
00:25:57,306 --> 00:26:00,932
times more than the threshold. So that's where the alarm will go up.

401
00:26:01,066 --> 00:26:04,416
Or you can put a specific statistic

402
00:26:04,448 --> 00:26:06,950
which says that the moment the value goes up,

403
00:26:08,040 --> 00:26:11,556
consider the alarm to be in action. So depending

404
00:26:11,588 --> 00:26:15,064
on your business case, depending on the use case, you can pick

405
00:26:15,102 --> 00:26:18,876
and choose whether you want to have the value to be above the

406
00:26:18,898 --> 00:26:22,136
threshold for consecutive period or just one period.

407
00:26:22,328 --> 00:26:26,300
And that's how the alarms can be put to work together

408
00:26:26,450 --> 00:26:30,124
with the metrics that you are collecting to give you

409
00:26:30,162 --> 00:26:34,352
this experience of either sending out SNS notification or maybe

410
00:26:34,406 --> 00:26:38,432
even doing automated remediation. Or finally,

411
00:26:38,486 --> 00:26:42,480
you can also have the auto scaling, which is kind of like an automated remediation.

412
00:26:47,720 --> 00:26:51,520
So in this example I wanted to show about automated

413
00:26:51,680 --> 00:26:55,488
remediation where you are leveraging the AWS lambda. So you

414
00:26:55,514 --> 00:26:58,852
can see here that, and we have a blog

415
00:26:58,916 --> 00:27:03,016
about this, about incident management and remediation in the cloud.

416
00:27:03,198 --> 00:27:07,124
In this example, you're monitoring a microservice

417
00:27:07,172 --> 00:27:11,096
API that is sitting behind application load

418
00:27:11,128 --> 00:27:14,876
balancer. The traffic can't reach the microservice, so it

419
00:27:14,898 --> 00:27:18,396
times out. So you could have an alarm that is triggered to

420
00:27:18,418 --> 00:27:22,192
send SNS notification to the topic when a lambda function

421
00:27:22,246 --> 00:27:25,756
is being used. So the moment the alarm kicks

422
00:27:25,788 --> 00:27:29,744
off, this alarm is getting triggered, the SNS topic will

423
00:27:29,862 --> 00:27:33,660
send the notification to lambda. And once the notification

424
00:27:33,740 --> 00:27:37,156
is received by the lambda, it knows that there is something wrong with

425
00:27:37,178 --> 00:27:41,184
the security group. So what it will do is it'll

426
00:27:41,232 --> 00:27:43,956
go back and it will fix this security group,

427
00:27:44,058 --> 00:27:47,764
possibly edit the inbound or the outbound rules which are there.

428
00:27:47,882 --> 00:27:51,672
And that way you will allow the traffic to go through and through.

429
00:27:51,806 --> 00:27:55,336
So that's one way of doing automated remediation. You can also

430
00:27:55,358 --> 00:27:58,936
make use of chat Ops is basically you integrate it with some

431
00:27:58,958 --> 00:28:02,764
kind of a slack integration and instead you post a message to your

432
00:28:02,802 --> 00:28:06,024
slack channel and someone would be made aware.

433
00:28:06,072 --> 00:28:08,430
Okay, here is something which has gone wrong.

434
00:28:09,600 --> 00:28:12,828
So there is multiple ways in which you can leverage

435
00:28:12,924 --> 00:28:16,432
the lambda and you can leverage the Cloudwatch alarm to

436
00:28:16,486 --> 00:28:20,640
together have this automated remediation in place by using the Cloudwatch.

437
00:28:25,800 --> 00:28:29,344
If you have a look at the EKS workshop which is published

438
00:28:29,392 --> 00:28:32,884
by AWS, you will see that there is

439
00:28:32,922 --> 00:28:36,736
a specific chapter which talks about service mesh integration

440
00:28:36,848 --> 00:28:39,400
and how you can use the container insights.

441
00:28:39,820 --> 00:28:43,496
This image is from the container insights which

442
00:28:43,518 --> 00:28:46,776
you can set up. So as you can see here on the left panel there

443
00:28:46,798 --> 00:28:50,376
is a section called Container Insights. And here you can select whichever

444
00:28:50,408 --> 00:28:54,520
is your EKS cluster which is native Kubernetes

445
00:28:54,600 --> 00:28:58,220
AWS managed Kubernetes cluster. And here you can have

446
00:28:58,290 --> 00:29:02,140
different views on cpu utilization, memory utilization,

447
00:29:02,300 --> 00:29:06,160
network, number of nodes, disk and cluster failures.

448
00:29:06,820 --> 00:29:10,816
This is the advantage of having container insights which gives you a

449
00:29:10,838 --> 00:29:14,464
one shot view of different clusters which are there. It also

450
00:29:14,502 --> 00:29:18,560
gives you a view of different services and resources if you're using ecs.

451
00:29:18,640 --> 00:29:22,800
So that way you have all the metrics which are needed for your container workloads

452
00:29:22,880 --> 00:29:26,624
together in Cloudwatch. Now Cloudwatch

453
00:29:26,672 --> 00:29:30,564
can't just be used for container workloads, you can also use it for EC twos.

454
00:29:30,692 --> 00:29:33,480
That would be just your normal virtual Vm.

455
00:29:35,100 --> 00:29:38,532
You'll have to export all the logs which are getting generated

456
00:29:38,596 --> 00:29:42,584
in there by using a Cloudwatch agent. But you can have all those logs

457
00:29:42,632 --> 00:29:46,396
come into Cloudwatch logs and you can do the exact same operations that

458
00:29:46,418 --> 00:29:49,884
we have been talking about in the earlier slides, even by

459
00:29:49,922 --> 00:29:51,250
using EC two.

460
00:29:57,860 --> 00:30:01,756
So next is have a look at the anomaly detection.

461
00:30:01,948 --> 00:30:06,192
So when you enable anomaly detection for a particular metrics, it applies

462
00:30:06,256 --> 00:30:09,684
the statistical and the machine learning models which are the

463
00:30:09,722 --> 00:30:13,700
algorithms which are already in place. So you can see from the graph that

464
00:30:13,850 --> 00:30:17,256
this grayed out area is what is the

465
00:30:17,358 --> 00:30:20,616
anomaly detection band around the different metrics which

466
00:30:20,638 --> 00:30:24,484
have been set up. And in this particular anomaly detection

467
00:30:24,532 --> 00:30:28,084
band, we are saying that anomaly detection between m one

468
00:30:28,142 --> 00:30:31,896
and m two. So this indicates that the anomaly detection

469
00:30:31,928 --> 00:30:36,524
has been enabled for the metric with a one

470
00:30:36,562 --> 00:30:39,820
dimensional m one and with a standard deviation of two

471
00:30:39,970 --> 00:30:43,132
as a default. So by default it will be one, and the moment

472
00:30:43,186 --> 00:30:47,404
it goes to a standard deviation of two, you would have an anomaly being detected

473
00:30:47,452 --> 00:30:51,328
in here. So when you are viewing a graph of a metric data,

474
00:30:51,494 --> 00:30:55,556
overlay the expected values on top of the graph. So I know that

475
00:30:55,658 --> 00:30:59,076
this is where my normal execution of

476
00:30:59,098 --> 00:31:02,436
the graph is, and the moment you see these red areas is

477
00:31:02,458 --> 00:31:06,352
where some anomaly has been detected. So that's generally a pattern

478
00:31:06,416 --> 00:31:09,924
which can be used for your long term trend analytics,

479
00:31:09,972 --> 00:31:13,128
or it can even be used for your long term planning, saying what is

480
00:31:13,134 --> 00:31:16,616
the usage that's going to happen for each of your services,

481
00:31:16,798 --> 00:31:20,744
and also what percentage of those will be used. So depending on whichever metric

482
00:31:20,792 --> 00:31:24,184
you're using, in this case it is cpu utilization.

483
00:31:24,312 --> 00:31:27,356
You can determine the band around which it will

484
00:31:27,378 --> 00:31:30,796
be operating and it helps you detect any anomalies which will come up in

485
00:31:30,818 --> 00:31:34,844
your operation. The advantage

486
00:31:34,892 --> 00:31:38,336
of having this sort of a setup is the moment

487
00:31:38,438 --> 00:31:42,236
there is something wrong from the normal baselines, your ops team will be made aware

488
00:31:42,268 --> 00:31:46,032
of it and you are not being caught off guard

489
00:31:46,176 --> 00:31:49,700
whenever such systems are running at scale in a distributed environment.

490
00:31:53,160 --> 00:31:56,624
This is an example of one thing that I mentioned

491
00:31:56,682 --> 00:32:00,292
in the earliers, where you can consolidate

492
00:32:00,356 --> 00:32:03,848
all your logs by potentially running a Cloudwatch agent,

493
00:32:04,014 --> 00:32:07,336
and then you can have the metrics and everything being shipped to

494
00:32:07,358 --> 00:32:11,320
the Cloudwatch logs by using a fluent bit sidecar

495
00:32:11,400 --> 00:32:15,036
pattern. This pattern is very much common in

496
00:32:15,058 --> 00:32:19,096
the container space, and the container insight also collects

497
00:32:19,128 --> 00:32:23,436
the performance logs, focusing something called as the embedded metric format.

498
00:32:23,628 --> 00:32:27,436
These performance log events are structured JSON schema

499
00:32:27,468 --> 00:32:30,972
basically, and it enables you to send high cardinality

500
00:32:31,036 --> 00:32:35,028
data which can be ingested and it can be stored at scale using

501
00:32:35,114 --> 00:32:36,580
Amazon Cloudwatch.

502
00:32:38,040 --> 00:32:42,144
So just to summarize here, we have seen that Amazon Cloudwatch

503
00:32:42,192 --> 00:32:46,248
can help you take care of your logging as well as your metric needs

504
00:32:46,334 --> 00:32:48,200
when it comes to observability.

505
00:32:51,420 --> 00:32:54,824
And now we have a look at AWS X ray with

506
00:32:54,862 --> 00:32:58,412
AWS X ray, going back to the first point I mentioned,

507
00:32:58,466 --> 00:33:02,350
it's basically about correlation ids. So this is an example

508
00:33:03,040 --> 00:33:06,856
where you can see that this service of scorekeep

509
00:33:06,968 --> 00:33:11,304
is behind the scene, getting the data from DynamoDB.

510
00:33:11,432 --> 00:33:14,748
It is keeping the session, it is updating the item,

511
00:33:14,844 --> 00:33:18,096
and it is ultimately putting the data into there.

512
00:33:18,198 --> 00:33:22,096
So this breakdown, what you're seeing, the step by step breakdown, and also

513
00:33:22,118 --> 00:33:25,264
the time which it takes for each of the stage, that's what

514
00:33:25,302 --> 00:33:28,592
x ray gives you, the trace

515
00:33:28,656 --> 00:33:32,960
id which is the correlation id. It's added to the HTTP request

516
00:33:33,040 --> 00:33:36,836
for specific headers and it has the name called X Amazon

517
00:33:36,868 --> 00:33:40,212
Trace iD. And ultimately you can have this integrated

518
00:33:40,276 --> 00:33:43,636
with Amazon API gateway or AWS

519
00:33:43,748 --> 00:33:46,280
application load balancer and likewise.

520
00:33:50,140 --> 00:33:53,112
So in this section we have a service map,

521
00:33:53,176 --> 00:33:56,988
and x ray uses all the data that you have in your application

522
00:33:57,154 --> 00:34:01,116
to generate this service map. What you're seeing here is

523
00:34:01,218 --> 00:34:04,688
anything which is green is what is up and running, and here

524
00:34:04,774 --> 00:34:08,656
what is red. So there is something wrong with this particular service. So this sort

525
00:34:08,678 --> 00:34:11,984
of a consolidated view for a client, let's say

526
00:34:12,022 --> 00:34:15,876
a user is invoking something. It helps you determine how

527
00:34:15,898 --> 00:34:19,284
your service is operating and also gives you a one shot view

528
00:34:19,402 --> 00:34:22,916
of how different services are integrated, especially in

529
00:34:22,938 --> 00:34:24,550
a large distributed system.

530
00:34:27,980 --> 00:34:31,604
So that covers all the topics around Amazon

531
00:34:31,652 --> 00:34:35,144
Cloudwatch and AWS x ray. So we

532
00:34:35,182 --> 00:34:39,028
touched upon different aspects of metrics, how you can have the

533
00:34:39,054 --> 00:34:43,256
graphs put together, how you can use the logging

534
00:34:43,288 --> 00:34:46,904
aspects of Amazon Cloudwatch for consolidating

535
00:34:46,952 --> 00:34:51,070
all your logging, and ultimately use the x ray for the tracing bit as well.

536
00:34:51,920 --> 00:34:55,936
Now I would like to move a little bit outside of the AWS services

537
00:34:56,038 --> 00:34:59,504
which we have been talking about for the last 1520 minutes,

538
00:34:59,622 --> 00:35:02,940
and just talk about the importance of service

539
00:35:03,030 --> 00:35:06,340
level indicators, objective and agreement.

540
00:35:06,680 --> 00:35:10,272
Quite often whenever you are defining your SRE

541
00:35:10,336 --> 00:35:13,716
practice, it's quite important to have these three

542
00:35:13,898 --> 00:35:17,816
definitions well sorted out. So what is

543
00:35:17,838 --> 00:35:21,092
a service level indicator? The service level indicator

544
00:35:21,156 --> 00:35:24,264
is basically a number. It's a quantitative measure of

545
00:35:24,302 --> 00:35:27,716
some aspect of the service which is being provided

546
00:35:27,748 --> 00:35:31,144
to you. So if I have a service, say a rest API

547
00:35:31,192 --> 00:35:34,444
which I am giving, and I'm saying that it's going to be up

548
00:35:34,562 --> 00:35:37,964
99% or something like that, then I know

549
00:35:38,002 --> 00:35:41,164
that this is the measure that

550
00:35:41,202 --> 00:35:44,444
I'm going to meet and that's the SLA part of it. That's the measure

551
00:35:44,492 --> 00:35:48,192
which I have to meet whenever the service is running

552
00:35:48,246 --> 00:35:52,016
for a long duration, and that's the reliability part

553
00:35:52,038 --> 00:35:56,048
of it. And every time you are trying to set up slas,

554
00:35:56,144 --> 00:36:00,352
SLO or SLI, make sure these are incorporated

555
00:36:00,416 --> 00:36:04,464
in your sare practice. Because ultimately not meeting

556
00:36:04,512 --> 00:36:08,056
a particular SLA or meeting an SLA determines the

557
00:36:08,078 --> 00:36:11,800
success or failure of a service. And most often

558
00:36:11,870 --> 00:36:15,704
the slas are easily recognized with a

559
00:36:15,742 --> 00:36:19,736
financial penalty or some kind of rebate

560
00:36:19,768 --> 00:36:21,470
which is associated with them.

561
00:36:24,720 --> 00:36:28,764
Some of the guidance which you can follow, especially when you

562
00:36:28,802 --> 00:36:32,988
are setting up the SRE practice and for the product development,

563
00:36:33,164 --> 00:36:36,528
these are things which you would obviously find in different resources online.

564
00:36:36,694 --> 00:36:40,208
Do have a look at Google's SRE book

565
00:36:40,374 --> 00:36:44,116
which talks about what are the best practices and how you can have

566
00:36:44,138 --> 00:36:47,488
the SRE practice implemented in your organization.

567
00:36:47,664 --> 00:36:51,856
Also, have a look at AWS provided resiliency

568
00:36:51,968 --> 00:36:55,488
and well architected framework which talks about all the

569
00:36:55,514 --> 00:36:59,076
five pillars that you would need to accomplish if you want to ensure

570
00:36:59,188 --> 00:37:03,368
that your architecture is well architected for the high

571
00:37:03,454 --> 00:37:06,436
availability, resiliency, operational excellence,

572
00:37:06,548 --> 00:37:08,360
security and likewise.

573
00:37:09,360 --> 00:37:13,084
So the first thing is you must not use each and

574
00:37:13,122 --> 00:37:17,150
every metric in any tracking or the monitoring system that you are having.

575
00:37:17,520 --> 00:37:20,972
It's always a concern when sometimes

576
00:37:21,026 --> 00:37:24,796
you want to capture everything that your system is coming in that

577
00:37:24,818 --> 00:37:29,100
is not really good in the long term because you would not be differentiating

578
00:37:29,180 --> 00:37:32,016
as to what it is that you want to know about the system and what

579
00:37:32,038 --> 00:37:34,630
is it that you don't want to know about the system.

580
00:37:35,080 --> 00:37:38,724
The second is have as few slos as

581
00:37:38,762 --> 00:37:42,432
possible and get an agreement from all the stakeholders.

582
00:37:42,496 --> 00:37:46,024
For clarity. These should be measured and the conditions under

583
00:37:46,062 --> 00:37:49,256
which these slos are valid. Even that has

584
00:37:49,278 --> 00:37:52,810
to be clarified with the stakeholders. And finally,

585
00:37:53,260 --> 00:37:56,996
make sure that you have an error budget which provides

586
00:37:57,028 --> 00:38:00,872
the objective metric that determines how unreliable

587
00:38:00,936 --> 00:38:04,524
a service is allowed to be. So let's take an example. If you are saying

588
00:38:04,562 --> 00:38:10,030
that your service is going to be available 99.9%

589
00:38:10,800 --> 00:38:14,336
availability, that's approximately four and

590
00:38:14,358 --> 00:38:18,192
a half hours of downtime in a year. Now, are you

591
00:38:18,246 --> 00:38:22,016
ready to have that kind of a setup, and are you

592
00:38:22,038 --> 00:38:25,236
ready to have that kind of an SLA for your application? That is

593
00:38:25,258 --> 00:38:28,788
something which you have to discuss with your stakeholders because the

594
00:38:28,954 --> 00:38:32,790
more available that you want to have your application,

595
00:38:33,240 --> 00:38:36,676
sometimes the cost associated with it is also high. So you

596
00:38:36,698 --> 00:38:40,328
need to take care of what is the Dr. For your application, how the application

597
00:38:40,414 --> 00:38:44,472
is going to behave if it is not going to be 99.9

598
00:38:44,526 --> 00:38:47,160
or 99.95% availability?

599
00:38:47,740 --> 00:38:51,276
How is the run team going to make sure that it has the

600
00:38:51,298 --> 00:38:54,968
right resources, like the run book and other details about the applications?

601
00:38:55,064 --> 00:38:58,396
How is the handover going to be from the

602
00:38:58,578 --> 00:39:02,256
application team which has been building the service to the run team?

603
00:39:02,358 --> 00:39:05,200
What kind of DevOps methodologies we are following?

604
00:39:05,620 --> 00:39:08,992
Does the team have the autonomy for building

605
00:39:09,046 --> 00:39:12,368
and deploying certain patches or fixes for such an application?

606
00:39:12,534 --> 00:39:16,004
So all those things have to be taken into account when you're defining these

607
00:39:16,042 --> 00:39:17,510
KPIs for your application.

608
00:39:18,760 --> 00:39:23,220
Okay, so with that being said, let's summarize.

609
00:39:23,640 --> 00:39:27,472
So what did we learn here? We spoke about logs,

610
00:39:27,536 --> 00:39:31,480
metrics and traces and how all of them correlate. In order to

611
00:39:31,550 --> 00:39:35,384
give you a better experience in the overall observability of your system.

612
00:39:35,582 --> 00:39:38,876
Make sure you implement the SLIs, SLOs and SLAs for

613
00:39:38,898 --> 00:39:42,904
your application in consultation with the stakeholders,

614
00:39:43,032 --> 00:39:46,584
evaluate how these behaviors are under different circumstances.

615
00:39:46,712 --> 00:39:50,076
If you're running your services on AWS, try and leverage the

616
00:39:50,098 --> 00:39:53,644
native tooling that is already available for logging alarms

617
00:39:53,692 --> 00:39:57,152
and dashboards like Amazon Cloudwatch, and for tracing using

618
00:39:57,206 --> 00:40:00,396
AWS x ray, and for a more deeper

619
00:40:00,588 --> 00:40:04,140
dive into what is being offered. AWS part of observability

620
00:40:04,220 --> 00:40:07,584
from AWS hop over to observability

621
00:40:07,712 --> 00:40:11,700
workshop AWS and you can get a hands on experience of

622
00:40:11,770 --> 00:40:15,172
all the features which are there. It's approximately three to 4 hours

623
00:40:15,226 --> 00:40:19,044
long. Lab. It's a self learn kind of a lab. You can

624
00:40:19,082 --> 00:40:22,852
run this in AWS environment with all the different

625
00:40:22,906 --> 00:40:26,484
templates which are already available. So with that being

626
00:40:26,522 --> 00:40:29,836
said, thank you so much for your time and hope you have

627
00:40:29,858 --> 00:40:30,170
a good day.

