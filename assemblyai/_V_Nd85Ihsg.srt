1
00:00:00,250 --> 00:00:00,800
You.

2
00:00:22,610 --> 00:00:26,262
Hello everyone, welcome to my presentation about the vector search

3
00:00:26,316 --> 00:00:29,606
engine Weaviate. In this presentation, I will explain you

4
00:00:29,628 --> 00:00:33,494
what vector search or vector databases are, and I will show you how

5
00:00:33,532 --> 00:00:37,026
it works in live demos. I am Laura

6
00:00:37,058 --> 00:00:40,818
Ham. I am a machine learning product researcher at semi technologies.

7
00:00:40,914 --> 00:00:45,190
And at semi technologies we build the vector search engine Weaviate.

8
00:00:46,250 --> 00:00:49,314
You can connect to me on LinkedIn,

9
00:00:49,442 --> 00:00:53,466
or you can join our slack channel of Weaviate,

10
00:00:53,498 --> 00:00:57,134
which is a slack channel of the community. So the open

11
00:00:57,172 --> 00:01:00,526
source community. And today I will

12
00:01:00,628 --> 00:01:04,510
talk about vectors search and vector databases.

13
00:01:05,090 --> 00:01:08,738
Before I dive into vector databases, I want to explain you

14
00:01:08,824 --> 00:01:12,226
what structured and unstructured data is and

15
00:01:12,248 --> 00:01:16,338
what the challenges in unstructured data. Then I will explain you

16
00:01:16,504 --> 00:01:19,890
everything about factorsearch and factor search engines,

17
00:01:19,970 --> 00:01:24,630
and I will then continue with showing this in live demos.

18
00:01:26,490 --> 00:01:29,290
So first, what is a factor database,

19
00:01:30,910 --> 00:01:33,974
and before that, so what's difficult about unstructured

20
00:01:34,022 --> 00:01:37,766
data? If we compare structured

21
00:01:37,798 --> 00:01:41,434
data with unstructured data, we see that structured

22
00:01:41,482 --> 00:01:45,434
data is what you find in typical database. So in typical

23
00:01:45,482 --> 00:01:48,798
database, typical database is a relational database with

24
00:01:48,884 --> 00:01:52,350
rows and columns and connections between those tables.

25
00:01:52,690 --> 00:01:56,180
So it's mostly quantitative data.

26
00:01:56,870 --> 00:01:59,874
So we have, in this example, we have an id number, a name,

27
00:01:59,912 --> 00:02:03,330
which is a short string, and these city, which is also a short string.

28
00:02:04,710 --> 00:02:08,166
And this is what the typical quantitative data is in

29
00:02:08,188 --> 00:02:11,382
a relational database. On the other

30
00:02:11,436 --> 00:02:14,966
hand, we have unstructured data, and that is what

31
00:02:14,988 --> 00:02:19,002
I call data, what you find in the wild. So that means

32
00:02:19,056 --> 00:02:22,406
it's like longer text documents with a lot of unstructured

33
00:02:22,438 --> 00:02:25,706
text in there, raw sensor data,

34
00:02:25,808 --> 00:02:29,770
we have images or videos, audio files,

35
00:02:30,110 --> 00:02:33,646
also social media data. And what you see in

36
00:02:33,668 --> 00:02:37,866
all those data sources is that there's

37
00:02:37,898 --> 00:02:41,120
a long piece of text or an image of video.

38
00:02:41,650 --> 00:02:45,266
And in this data, there's a lot of information

39
00:02:45,368 --> 00:02:49,134
hidden. So in one image, there can be a lot of information hidden,

40
00:02:49,182 --> 00:02:52,574
but it's difficult to capture that in a traditional

41
00:02:52,702 --> 00:02:55,954
structured database. So with structured

42
00:02:56,002 --> 00:02:58,600
databases on the left hand side,

43
00:02:59,770 --> 00:03:03,110
it's easy to search through. It's easy to store

44
00:03:03,180 --> 00:03:06,710
data, easy to make conclusions from the data,

45
00:03:06,780 --> 00:03:10,486
because you have quantitative data. But if you have the unstructured

46
00:03:10,518 --> 00:03:13,914
data on the right, it's difficult to find the

47
00:03:13,952 --> 00:03:17,702
information that's hidden in that unstructured

48
00:03:17,766 --> 00:03:21,214
data. So if we take an example of

49
00:03:21,252 --> 00:03:24,750
a search engine on top of data

50
00:03:24,820 --> 00:03:28,026
sources, so in search engine, on structured

51
00:03:28,058 --> 00:03:31,770
data, on relational data, it's really easy because it's quantitative

52
00:03:31,850 --> 00:03:34,290
data stored in those tables.

53
00:03:34,710 --> 00:03:38,494
If you have unstructured data, it becomes a lot more challenging.

54
00:03:38,622 --> 00:03:42,226
So here we have an example of,

55
00:03:42,408 --> 00:03:46,002
let's say we have a data set of news articles and all those news

56
00:03:46,056 --> 00:03:49,542
articles have a title and a longer piece of text,

57
00:03:49,596 --> 00:03:53,542
which is the actual article. So if we have this one

58
00:03:53,676 --> 00:03:57,846
article which is about dogs, so the title is these origins

59
00:03:57,878 --> 00:03:59,850
of dogs, how dogs were domesticated.

60
00:04:00,750 --> 00:04:04,298
If you then have a search engine

61
00:04:04,384 --> 00:04:07,862
and you would search with the query in natural language,

62
00:04:07,926 --> 00:04:12,030
animals, in a traditional search engine with

63
00:04:12,180 --> 00:04:15,614
structured data or relational data, you may

64
00:04:15,652 --> 00:04:18,890
not be able to find the article about dogs.

65
00:04:19,050 --> 00:04:22,666
And this is because we search for the word animals.

66
00:04:22,778 --> 00:04:26,106
While the article is actually not using the word animals, but it's

67
00:04:26,138 --> 00:04:29,534
using the word dogs. As humans, we know that animals are related

68
00:04:29,582 --> 00:04:32,946
to dogs, that dogs are a type of animal, so that maybe we

69
00:04:32,968 --> 00:04:36,706
want to see this article as a result. But if we don't

70
00:04:36,738 --> 00:04:40,546
use any semantic layer or machine learning that understands natural

71
00:04:40,578 --> 00:04:44,134
language in English, you may not be able to

72
00:04:44,172 --> 00:04:46,760
find this article in a traditional search engine.

73
00:04:47,790 --> 00:04:51,370
Then if we have a vector search engine and a vector search engine

74
00:04:51,440 --> 00:04:55,034
uses machine learning to be able to

75
00:04:55,072 --> 00:04:58,170
understand the semantics behind language,

76
00:04:58,830 --> 00:05:02,142
you might be able to find actually this answer these

77
00:05:02,196 --> 00:05:05,470
article if you are searching with the animals,

78
00:05:06,450 --> 00:05:09,280
and that is because as I explained before,

79
00:05:09,970 --> 00:05:13,700
we know as humans that dogs and animals are semantically similar

80
00:05:14,070 --> 00:05:17,902
and this is hidden in the english language

81
00:05:18,046 --> 00:05:21,780
and these machine learning model is able to connect those two.

82
00:05:23,510 --> 00:05:26,582
Yeah. Before I explain in detail so how this actually

83
00:05:26,636 --> 00:05:29,846
works, vector Search, I want to show

84
00:05:29,868 --> 00:05:33,686
you that you already know one vector search engine and

85
00:05:33,708 --> 00:05:37,570
that is Google. So Google is a vector search engine too.

86
00:05:37,740 --> 00:05:41,322
With Google, you can put in a very abstract question

87
00:05:41,456 --> 00:05:47,130
like this example, what color of wine is answer?

88
00:05:47,280 --> 00:05:50,734
So Google will browse through all the open

89
00:05:50,932 --> 00:05:54,750
available web pages and it tries to not only

90
00:05:54,820 --> 00:05:58,062
find the web page that is

91
00:05:58,116 --> 00:06:01,534
interesting to this query, but also extract an exact answer from

92
00:06:01,572 --> 00:06:05,234
this web page. So these question is very

93
00:06:05,272 --> 00:06:08,914
abstract. Google is able to find a

94
00:06:08,952 --> 00:06:13,154
concrete answer from a piece of unstructured data and

95
00:06:13,192 --> 00:06:16,840
it also does it really fast. So in this case,

96
00:06:17,450 --> 00:06:20,290
you put in a query, what color of wine is chevronay?

97
00:06:20,450 --> 00:06:23,942
Google is first browsing through all its

98
00:06:23,996 --> 00:06:27,474
indexed web pages and find web pages that might

99
00:06:27,532 --> 00:06:31,530
contain this answer. So it found like almost

100
00:06:31,600 --> 00:06:36,140
13 million results in less than a second and

101
00:06:37,630 --> 00:06:41,582
it's then also able to extract the answer. So white

102
00:06:41,636 --> 00:06:44,846
wine from a paragraph from all of these web

103
00:06:44,868 --> 00:06:47,550
pages. So that's pretty impressive.

104
00:06:49,170 --> 00:06:52,574
And yeah, we're all really happy with that,

105
00:06:52,612 --> 00:06:56,820
of course, but Google is only able to do this with all these,

106
00:06:57,190 --> 00:07:00,386
or Google is doing this with all the unstructured data that

107
00:07:00,408 --> 00:07:03,666
is available on the public web. So all

108
00:07:03,688 --> 00:07:06,520
the web pages that is accessible by everyone.

109
00:07:07,290 --> 00:07:11,142
And this is only a very small portion of data

110
00:07:11,196 --> 00:07:14,774
that's available on all the world. That's because most

111
00:07:14,812 --> 00:07:18,534
of the data is of course available or in the hands of private

112
00:07:18,582 --> 00:07:22,540
companies or for people themselves.

113
00:07:22,990 --> 00:07:26,746
And we don't want Google to see what your

114
00:07:26,768 --> 00:07:30,220
company has in terms of data.

115
00:07:31,390 --> 00:07:34,960
But then if you want to search through your own data,

116
00:07:36,770 --> 00:07:40,734
so we cannot use Google for searching through your own data. So then the question

117
00:07:40,772 --> 00:07:44,126
becomes is what if you could do the same thing? So searching

118
00:07:44,158 --> 00:07:47,794
through unstructured data, through your own data in

119
00:07:47,832 --> 00:07:50,340
of course a simple and secure way.

120
00:07:51,110 --> 00:07:54,580
And that's where open source vector search engines come in.

121
00:07:56,230 --> 00:07:59,654
VV eight is such a vector search engine, which is

122
00:07:59,692 --> 00:08:02,886
open source. And yeah,

123
00:08:02,908 --> 00:08:06,722
as it says here. So instead of just storing raw data like traditional

124
00:08:06,786 --> 00:08:10,246
databases, do, weaviate leverages power of machine

125
00:08:10,278 --> 00:08:14,326
learning model to factorize data. And what these means factorization

126
00:08:14,438 --> 00:08:17,994
is that machine learning models try to understand what your

127
00:08:18,032 --> 00:08:22,126
data is about while it stores data. And also

128
00:08:22,228 --> 00:08:24,720
when you search through it using VV eight,

129
00:08:26,130 --> 00:08:30,320
your search query will also be put through this machine learning model,

130
00:08:30,690 --> 00:08:34,674
be factorized and it's able to find

131
00:08:34,792 --> 00:08:37,870
data objects that are near your search query.

132
00:08:38,030 --> 00:08:41,714
So you can do discovery or classify similar search

133
00:08:41,752 --> 00:08:45,042
results in your data set. So in short,

134
00:08:45,096 --> 00:08:49,126
weaviate is a factor search engine that tries to understand

135
00:08:49,228 --> 00:08:53,302
what your data is about. On the right you see a

136
00:08:53,356 --> 00:08:57,254
three dimensional space and this is an

137
00:08:57,292 --> 00:09:01,210
example, simplified example of how data

138
00:09:01,280 --> 00:09:04,970
is stored in a vector search engine or a vector database.

139
00:09:05,550 --> 00:09:08,874
So we have in this

140
00:09:08,912 --> 00:09:12,286
case three highdimensional. In reality this goes up

141
00:09:12,308 --> 00:09:15,706
to like 300 or even 3000 highdimensional.

142
00:09:15,898 --> 00:09:19,018
And all these dimensions capture some kind of meaning

143
00:09:19,114 --> 00:09:23,106
of data. So on

144
00:09:23,128 --> 00:09:26,526
the right we have some words and some images,

145
00:09:26,718 --> 00:09:30,014
and these words and images have a meaning

146
00:09:30,062 --> 00:09:34,690
to a human and the meaning

147
00:09:35,270 --> 00:09:39,378
is determining these. This data object is stored in the database.

148
00:09:39,554 --> 00:09:42,726
So for example, on the left we have the

149
00:09:42,748 --> 00:09:46,226
word chicken. We have an image of a chicken.

150
00:09:46,418 --> 00:09:50,422
Those are very close because they have kind of the same meaning.

151
00:09:50,566 --> 00:09:54,774
So they are close together in the database.

152
00:09:54,902 --> 00:09:58,774
It's also relatively close to other animals like wolf,

153
00:09:58,822 --> 00:10:01,360
dog, cat, an image of a cat,

154
00:10:02,210 --> 00:10:05,706
but it's then far away. So all these animals are stored

155
00:10:05,738 --> 00:10:09,306
far away from things like an apple or a banana

156
00:10:09,338 --> 00:10:12,586
or the company's school on apple because

157
00:10:12,628 --> 00:10:16,478
they're not semantically similar. So this is how data objects

158
00:10:16,494 --> 00:10:22,914
are stored in a vector database when

159
00:10:22,952 --> 00:10:26,966
you then do a search query. So like animals, we did before

160
00:10:27,068 --> 00:10:31,350
in the example or in this case, we have the search query kitten.

161
00:10:31,770 --> 00:10:36,310
It also puts these query in the

162
00:10:36,380 --> 00:10:40,482
highdimensional space, and it returns

163
00:10:40,546 --> 00:10:43,946
items or data objects that are close to this search query. So in this

164
00:10:43,968 --> 00:10:47,846
case, if you search for kitten, you will see data objects

165
00:10:47,878 --> 00:10:50,730
like cat or the image of a cat returned.

166
00:10:51,890 --> 00:10:55,440
And I will explain these in more detail in the coming minutes.

167
00:10:56,770 --> 00:11:01,040
But first, I want to show you how vector databases are

168
00:11:01,570 --> 00:11:04,450
slightly different from graph databases.

169
00:11:06,150 --> 00:11:09,060
So, with graph databases, as you might know,

170
00:11:09,990 --> 00:11:13,518
data is also not stored in traditional

171
00:11:13,614 --> 00:11:18,466
rows and columns, but data is stored in models

172
00:11:18,498 --> 00:11:22,226
or classes. And all those data objects

173
00:11:22,258 --> 00:11:26,070
can have relations to each other. So if we take the example

174
00:11:26,140 --> 00:11:29,898
again of news articles, we might have a data object

175
00:11:30,064 --> 00:11:34,170
article with the title the origins of dogs.

176
00:11:34,910 --> 00:11:38,826
And this is written by some author and has some category, which are

177
00:11:38,848 --> 00:11:42,606
also separate data objects. And these are relations

178
00:11:42,708 --> 00:11:46,350
between those data objects. So, article has

179
00:11:46,420 --> 00:11:49,626
author, this person, John Doe,

180
00:11:49,738 --> 00:11:52,430
and this person wrote some articles,

181
00:11:53,670 --> 00:11:57,150
for example, this particular article, and this article

182
00:11:57,230 --> 00:12:01,170
has a category nature in this case.

183
00:12:01,320 --> 00:12:03,250
So this is a graph database.

184
00:12:07,930 --> 00:12:11,174
This is different from vector databases. But what you do with

185
00:12:11,212 --> 00:12:15,042
vector databases is you add highdimensional

186
00:12:15,106 --> 00:12:19,094
to it. So these, I have like added two dimensions

187
00:12:19,142 --> 00:12:23,082
to it. And what happens is these

188
00:12:23,136 --> 00:12:29,020
data objects in these graph database will be stored exactly

189
00:12:31,010 --> 00:12:34,238
where the meaning of this data object is. Also.

190
00:12:34,324 --> 00:12:38,782
So in this case, we have

191
00:12:38,916 --> 00:12:42,866
indexed, like, for example, english language. And you see that

192
00:12:42,888 --> 00:12:46,334
a category nature, this data object will be stored

193
00:12:46,382 --> 00:12:49,986
close to, for example, the concept of environment, concept of

194
00:12:50,008 --> 00:12:54,110
animals, but far away from technology and laptop,

195
00:12:54,270 --> 00:12:57,906
and the concept of article, or this article about dogs.

196
00:12:58,098 --> 00:13:01,926
So it's article. So it's close to newspaper, it's close

197
00:13:01,948 --> 00:13:05,698
to dog, and also to cat and animals because they're

198
00:13:05,714 --> 00:13:08,794
all semantically related. And so by

199
00:13:08,832 --> 00:13:12,758
adding these vectors, so these vectors

200
00:13:12,774 --> 00:13:16,300
are essentially just coordinates in a highdimensional space,

201
00:13:17,710 --> 00:13:21,520
you add some context or some meaning to your data,

202
00:13:22,130 --> 00:13:26,190
and this allows you to also search through it semantically.

203
00:13:27,330 --> 00:13:31,150
So how does this work on a technical perspective?

204
00:13:33,430 --> 00:13:36,734
So, the first step of doing vector

205
00:13:36,782 --> 00:13:40,530
search or storing data in a vector database is

206
00:13:40,680 --> 00:13:44,366
choosing some kind of model that can map

207
00:13:44,478 --> 00:13:48,246
your data to vectors, to coordinates in

208
00:13:48,268 --> 00:13:51,720
this high dimensional space. And for this,

209
00:13:52,730 --> 00:13:56,470
you can, for example, use machine learning models.

210
00:13:58,430 --> 00:14:01,786
So the first step is an encoder model. So from data to

211
00:14:01,888 --> 00:14:04,810
a vector, to a coordinate, this is encoding.

212
00:14:06,590 --> 00:14:10,054
It transforms data into vectors. They're also called

213
00:14:10,112 --> 00:14:13,838
retrieved models. And one example is

214
00:14:13,924 --> 00:14:18,026
dense retrievers. Dense retrievers are deep neural networks

215
00:14:18,058 --> 00:14:22,474
or machine learning models that calculate the vectors from

216
00:14:22,612 --> 00:14:26,466
a word. These can

217
00:14:26,488 --> 00:14:29,874
be language models. So youll see an example here on these,

218
00:14:29,912 --> 00:14:30,500
right.

219
00:14:32,710 --> 00:14:36,914
Can example of language models are bird models or sentence transformers

220
00:14:36,962 --> 00:14:40,498
models. You can also do this to images.

221
00:14:40,674 --> 00:14:43,734
And then one example is a Resnet 50 model.

222
00:14:43,852 --> 00:14:47,346
So all these models, they can calculate

223
00:14:47,378 --> 00:14:49,980
a vector position from a piece of data.

224
00:14:52,830 --> 00:14:56,534
You can also do this by not using deep neural networks

225
00:14:56,582 --> 00:15:00,850
but using sparse retrievers, sparse retrievers

226
00:15:00,870 --> 00:15:04,014
like TD TFIDF or BM 25.

227
00:15:04,212 --> 00:15:07,306
They don't use machine learning models,

228
00:15:07,338 --> 00:15:11,440
but they use word frequencies in the documents instead.

229
00:15:12,130 --> 00:15:15,310
So they are a bit light weighted.

230
00:15:16,710 --> 00:15:20,146
So now

231
00:15:20,168 --> 00:15:23,742
we have a model that is able to transform

232
00:15:23,806 --> 00:15:27,366
data. So natural language or images or videos

233
00:15:27,468 --> 00:15:29,350
whatever into vectors.

234
00:15:30,650 --> 00:15:35,314
The second step is you can use weaviate

235
00:15:35,362 --> 00:15:38,646
for example to import all this data and

236
00:15:38,668 --> 00:15:41,350
to store it actually in this vector database.

237
00:15:42,010 --> 00:15:45,734
So Weaviate looks at all the data objects and uses this encoder

238
00:15:45,782 --> 00:15:50,574
machine learning model to vectors the data and

239
00:15:50,612 --> 00:15:54,074
then it will be placed in this hyperspace,

240
00:15:54,122 --> 00:15:55,840
so this high dimensional space.

241
00:15:58,290 --> 00:16:01,646
And then you end up something like this, what I showed before. So we

242
00:16:01,668 --> 00:16:05,342
have a cat that is semantically

243
00:16:05,406 --> 00:16:08,798
related to dog and to animals and to can image

244
00:16:08,814 --> 00:16:11,650
of a cat, but it's far away from Apple and banana.

245
00:16:12,950 --> 00:16:16,920
Then if you use this database as search engine,

246
00:16:19,050 --> 00:16:22,086
weaviate will also take your search query which

247
00:16:22,108 --> 00:16:25,826
you can put in in natural language or search by an image.

248
00:16:25,858 --> 00:16:29,082
For example, it will also use this encoder model

249
00:16:29,136 --> 00:16:33,130
or retriever model to index or vectorize this query.

250
00:16:34,110 --> 00:16:37,994
And so it also gets a vector position like you see here

251
00:16:38,032 --> 00:16:41,066
on the right. And then it does

252
00:16:41,088 --> 00:16:44,538
an approximate nearest neighbor search,

253
00:16:44,624 --> 00:16:48,138
an answering to retrieved data objects that are close to this query.

254
00:16:48,314 --> 00:16:50,270
So if I search for kitten,

255
00:16:52,050 --> 00:16:55,970
then I will get back results that are for example cat

256
00:16:56,040 --> 00:16:59,458
or image of a cat and so on.

257
00:16:59,624 --> 00:17:02,820
And this is called approximate nearest neighbor search.

258
00:17:03,510 --> 00:17:07,400
And this is for example by calculating the cosine distance between

259
00:17:07,930 --> 00:17:11,638
the data objects and the search query, you only want to retrieve the

260
00:17:11,644 --> 00:17:14,150
results that are most close to the query.

261
00:17:16,890 --> 00:17:20,282
And with a vector search engine like

262
00:17:20,336 --> 00:17:24,182
Weaviate, this is done very efficiently.

263
00:17:24,326 --> 00:17:27,754
Even if you have millions or billions of data objects or

264
00:17:27,792 --> 00:17:33,262
search queries, you can still retrieve it very

265
00:17:33,316 --> 00:17:36,910
efficiently. And this is because it uses for example

266
00:17:37,060 --> 00:17:42,222
a indexing library H and SW to

267
00:17:42,276 --> 00:17:44,770
search for these data objects very efficiently.

268
00:17:46,390 --> 00:17:50,206
So to summarize, you have a pretrained

269
00:17:50,238 --> 00:17:53,986
machine learning model, for example a build or

270
00:17:54,008 --> 00:17:58,280
sentence transformer model from hacking phase. You have your own data

271
00:17:59,050 --> 00:18:02,914
and then with VVH you can index them and store them in this vector

272
00:18:02,962 --> 00:18:06,118
database. You can do a

273
00:18:06,124 --> 00:18:10,086
search query and then it will do can approximate nearest neighbor search to retrieved

274
00:18:10,118 --> 00:18:14,170
the data objects close to this query. So this is how a very

275
00:18:14,320 --> 00:18:17,834
relatively simple search pipeline looks

276
00:18:17,872 --> 00:18:21,706
like. And you

277
00:18:21,728 --> 00:18:25,566
can extend this pipeline. So now it was just a very

278
00:18:25,588 --> 00:18:29,326
simple search and you use retriever models for that.

279
00:18:29,428 --> 00:18:32,858
But you can extend this factor search pipeline by for example

280
00:18:33,044 --> 00:18:36,654
reader or generator models. Reader models

281
00:18:36,702 --> 00:18:41,182
are models that extract information from the retrieved data objects.

282
00:18:41,326 --> 00:18:45,326
So that means you can do question answering. With question answering

283
00:18:45,358 --> 00:18:48,902
you put in a natural language query like really

284
00:18:48,956 --> 00:18:52,310
a question in your search query, and then

285
00:18:52,460 --> 00:18:56,102
VV is able to not only find a relevant data object,

286
00:18:56,156 --> 00:18:59,574
but also extract an answer from these relevant data

287
00:18:59,612 --> 00:19:03,126
objects. And that's done by a reader model on top of the retrieved

288
00:19:03,158 --> 00:19:07,050
model. Another example is named entity recognition.

289
00:19:08,670 --> 00:19:12,698
You can also extend it by using generator models.

290
00:19:12,794 --> 00:19:16,890
Generator models use natural language generation

291
00:19:16,970 --> 00:19:21,066
to generate an answer, for example from retrieved data objects.

292
00:19:21,258 --> 00:19:24,722
So here for question answering. In a reader model

293
00:19:24,776 --> 00:19:28,162
it only searches in a data object for

294
00:19:28,216 --> 00:19:31,634
a particular answer but

295
00:19:31,672 --> 00:19:35,474
doesn't modify it, it just retrieves a piece of data. A generator

296
00:19:35,522 --> 00:19:39,762
model can actually generate

297
00:19:39,826 --> 00:19:43,574
language from this data object. So for example it

298
00:19:43,612 --> 00:19:45,480
can summarize a piece of text.

299
00:19:47,770 --> 00:19:51,514
So now that I explained you a bit how this works,

300
00:19:51,632 --> 00:19:55,770
then the question becomes how do you use it? So how do you interact with

301
00:19:55,840 --> 00:19:59,094
Zach factor database vv

302
00:19:59,142 --> 00:20:02,190
eight has two types of API endpoints.

303
00:20:03,490 --> 00:20:06,986
It firstly has all usual restful API

304
00:20:07,018 --> 00:20:11,054
endpoints for crud operations and

305
00:20:11,092 --> 00:20:14,786
additionally it has a GraphqL API to

306
00:20:14,808 --> 00:20:19,794
do intuitive querying. So you

307
00:20:19,832 --> 00:20:23,474
can of course retrieve all the data objects with

308
00:20:23,512 --> 00:20:27,430
a get query. You can do semantic search or for example question

309
00:20:27,500 --> 00:20:31,830
answering depending on what kind of machine learning models you have attached.

310
00:20:32,730 --> 00:20:36,566
And I will show you this. So there

311
00:20:36,588 --> 00:20:40,442
are two demos that are always available for you. The first one

312
00:20:40,496 --> 00:20:43,914
is a super small or relatively small data set

313
00:20:43,952 --> 00:20:47,674
of news articles. So it's only less

314
00:20:47,712 --> 00:20:51,338
than 4000 news articles in there. So that's

315
00:20:51,434 --> 00:20:54,960
really small if we talk about machine learning,

316
00:20:55,890 --> 00:20:58,618
but this is just for demo purposes.

317
00:20:58,794 --> 00:21:02,554
Second one is the complete English Wikipedia indexed.

318
00:21:02,602 --> 00:21:06,894
So that's like billions or

319
00:21:06,932 --> 00:21:11,394
millions of pages from Wikipedia, English Wikipedia and

320
00:21:11,432 --> 00:21:14,866
you can also search for it using Weaviate. But for now I will

321
00:21:14,888 --> 00:21:18,680
just show you the small data set.

322
00:21:21,210 --> 00:21:25,062
So in here I'm connected to this data

323
00:21:25,116 --> 00:21:28,946
set. So this data set has news articles with natural

324
00:21:28,978 --> 00:21:32,786
language text already indexed in Weaviate.

325
00:21:32,978 --> 00:21:35,400
And what youll see here is a,

326
00:21:38,330 --> 00:21:42,842
I'll make it a bit bigger is a graphical user interface

327
00:21:42,906 --> 00:21:46,862
to query and you can query this data set

328
00:21:46,916 --> 00:21:50,720
using graphs. So I will

329
00:21:51,170 --> 00:21:54,898
build this query step by step to explain. So on the

330
00:21:54,904 --> 00:21:58,386
left hand side I built query. On the right hand side you

331
00:21:58,408 --> 00:21:59,810
will see the results.

332
00:22:06,810 --> 00:22:10,290
So I can do a simple query

333
00:22:10,370 --> 00:22:14,246
to get all the

334
00:22:14,268 --> 00:22:18,122
articles. So the news articles that are in the data set and I just

335
00:22:18,176 --> 00:22:20,970
query. It's not these name, it's title.

336
00:22:22,910 --> 00:22:26,582
Just query the title. So this is a really simple get query

337
00:22:26,646 --> 00:22:28,860
basically just to show you how it works.

338
00:22:30,350 --> 00:22:33,502
And on the right you see the result. So now

339
00:22:33,556 --> 00:22:36,446
I have a list of all the articles that are in there and I just

340
00:22:36,468 --> 00:22:37,550
see the title.

341
00:22:39,890 --> 00:22:43,106
I can ask for more data properties here. So this

342
00:22:43,128 --> 00:22:46,740
is the summary of article. For example.

343
00:22:48,150 --> 00:22:52,126
Now with this query there's of course no semantic

344
00:22:52,158 --> 00:22:55,800
search happening. So there's no machine learning happening here.

345
00:22:59,210 --> 00:23:03,880
But I can add this to the query and

346
00:23:04,730 --> 00:23:08,166
let's say I want to find the articles that are near the concept

347
00:23:08,198 --> 00:23:12,010
of animals like I used in the example in the slides

348
00:23:12,350 --> 00:23:16,540
I can do a near text search. Near text is a

349
00:23:16,910 --> 00:23:20,334
function that we defined which uses the

350
00:23:20,372 --> 00:23:23,310
semantic search principles.

351
00:23:28,210 --> 00:23:32,010
And for example I can go for animals.

352
00:23:32,090 --> 00:23:35,522
So now I want to get all the articles that are

353
00:23:35,576 --> 00:23:39,346
near the concept of animals and I want to see the

354
00:23:39,368 --> 00:23:40,850
title and the summary.

355
00:23:42,230 --> 00:23:46,214
And when I run this on the right, I get back all

356
00:23:46,332 --> 00:23:50,360
the other articles ordered on

357
00:23:51,850 --> 00:23:55,270
the relevancy to the query. So they are ordered on how close

358
00:23:55,340 --> 00:23:58,150
they are in the vector space to the search query.

359
00:23:59,790 --> 00:24:03,100
And as you can see the first result

360
00:24:04,030 --> 00:24:07,706
is the example that I used in the slides. So the oranges of

361
00:24:07,728 --> 00:24:11,550
dogs, a new idea about how dogs were domesticated.

362
00:24:12,530 --> 00:24:17,146
And you can see also in this article it's

363
00:24:17,178 --> 00:24:21,242
about dogs and predating

364
00:24:21,306 --> 00:24:25,550
domestication. It's all about wolves.

365
00:24:25,630 --> 00:24:29,154
So it's all about animals, but it doesn't literally use

366
00:24:29,192 --> 00:24:33,346
the word animals. And this is how you can see that you

367
00:24:33,368 --> 00:24:34,980
really do a semantic search.

368
00:24:37,190 --> 00:24:40,354
So the second result, it's about Nigeria

369
00:24:40,402 --> 00:24:43,606
cattle principles. So cattle, I'm not

370
00:24:43,628 --> 00:24:47,654
sure if the word animals is used here but I don't see it.

371
00:24:47,772 --> 00:24:50,822
So vv eight or the machine learning model behind it,

372
00:24:50,876 --> 00:24:54,522
in this case a sentence transform model understands that

373
00:24:54,576 --> 00:24:58,950
with animals I also want to see results like dogs or cattle,

374
00:24:59,110 --> 00:25:00,300
those kind of things.

375
00:25:04,290 --> 00:25:06,800
I can also show you this.

376
00:25:08,290 --> 00:25:12,142
I can call for certainty. And certainty is

377
00:25:12,196 --> 00:25:16,158
a number ranging from zero to one indicating

378
00:25:16,254 --> 00:25:20,018
how close this data point

379
00:25:20,104 --> 00:25:23,918
is actually related to these search query.

380
00:25:24,014 --> 00:25:27,910
So in this case you can say that Weaviate is 79%

381
00:25:27,980 --> 00:25:31,394
sure that this search query is relevant

382
00:25:31,442 --> 00:25:32,920
to what I'm searching for.

383
00:25:35,530 --> 00:25:36,310
Um,

384
00:25:38,990 --> 00:25:43,514
okay, so as

385
00:25:43,552 --> 00:25:47,526
I showed you, you can extend the search pipeline with reader

386
00:25:47,558 --> 00:25:51,040
models and one of the reader models is a question answering model.

387
00:25:53,250 --> 00:25:57,358
So I can also ask

388
00:25:57,444 --> 00:26:01,546
Weaviate or this data set a detailed

389
00:26:01,578 --> 00:26:02,160
question.

390
00:26:12,470 --> 00:26:16,100
So here I have an exact question.

391
00:26:16,950 --> 00:26:21,142
So who is the king of the Netherlands? I'm really looking for one specific answer.

392
00:26:21,196 --> 00:26:24,374
I don't want to see the whole data item, maybe I just want to know

393
00:26:24,412 --> 00:26:29,386
who is the king of the Netherlands and

394
00:26:29,488 --> 00:26:31,660
I need to ask for the answer here.

395
00:26:34,030 --> 00:26:38,140
So one answer is enough.

396
00:26:38,830 --> 00:26:42,442
So if I do this, what happens now

397
00:26:42,496 --> 00:26:46,590
is that VVT will still do a can query. It gets

398
00:26:46,660 --> 00:26:50,474
articles that are near this search query.

399
00:26:50,522 --> 00:26:54,850
So it will find articles that might contain something

400
00:26:54,920 --> 00:26:57,730
like this about King and the Netherlands.

401
00:26:58,630 --> 00:27:02,354
And then with this search query or with these result. So it found

402
00:27:02,392 --> 00:27:05,750
this result about dutch royals.

403
00:27:06,090 --> 00:27:09,842
And with these search query it uses

404
00:27:09,906 --> 00:27:13,842
a question answering machine learning model to extract

405
00:27:13,986 --> 00:27:18,060
an exact answer. So here it found this answer

406
00:27:18,430 --> 00:27:21,686
and you can see that here it's found already in the first sentence.

407
00:27:21,798 --> 00:27:25,318
So King William Alexander

408
00:27:25,494 --> 00:27:27,530
and then went to Netherlands.

409
00:27:30,370 --> 00:27:34,782
So the machine learning model understands that this

410
00:27:34,836 --> 00:27:37,150
king is the king of the Netherlands.

411
00:27:41,890 --> 00:27:45,246
And yeah, there's also these complete english Wikipedia

412
00:27:45,278 --> 00:27:48,706
index as I mentioned. So you

413
00:27:48,728 --> 00:27:52,482
can also ask questions to Wikipedia and

414
00:27:52,536 --> 00:27:55,650
it works similarly.

415
00:27:56,170 --> 00:27:59,590
So you can play around with this if you want. You can find the links

416
00:28:00,730 --> 00:28:04,550
also on our documentation on Wevy IO.

417
00:28:07,710 --> 00:28:11,994
So this

418
00:28:12,032 --> 00:28:16,826
was a very simple example what I showed you with just indexing text.

419
00:28:17,008 --> 00:28:20,878
But you can do more. You can also

420
00:28:20,964 --> 00:28:24,442
use machine learning models which uses

421
00:28:24,506 --> 00:28:28,586
multimodal search. So you can mix media types within VV

422
00:28:28,618 --> 00:28:31,898
eight. This means you can also index

423
00:28:32,074 --> 00:28:35,154
images at the same time as text. So you can

424
00:28:35,192 --> 00:28:38,626
search by an image and retrieve text or the other way around so

425
00:28:38,648 --> 00:28:40,740
you can mix media types. For example,

426
00:28:44,310 --> 00:28:48,230
this is what I showed youll before. So question answering

427
00:28:53,610 --> 00:28:57,062
then as I explained to you, wefit works with

428
00:28:57,196 --> 00:29:00,902
machine learning models to vectorize

429
00:29:00,966 --> 00:29:04,278
data. We have multiple

430
00:29:04,454 --> 00:29:08,378
machine learning models available out of the box. For example most of the

431
00:29:08,384 --> 00:29:11,918
models from hugging these or our own trained machine learning model

432
00:29:12,004 --> 00:29:15,978
called contextionary. We have models

433
00:29:15,994 --> 00:29:18,480
from OpenAI and so on.

434
00:29:20,770 --> 00:29:24,026
But you can also use weaviate without any machine

435
00:29:24,058 --> 00:29:27,522
learning models. If you want to use it as a pure factor search

436
00:29:27,576 --> 00:29:31,566
engine. Or you can use custom machine learning models

437
00:29:31,598 --> 00:29:35,186
if you have your own trained machine learning model but want

438
00:29:35,208 --> 00:29:38,550
to for example scale it or make search engine from it,

439
00:29:38,620 --> 00:29:40,360
you can use VV eight for that.

440
00:29:43,370 --> 00:29:46,806
And finally, what you can also do with VV eight because it

441
00:29:46,828 --> 00:29:50,054
uses these vectors. You can do classification,

442
00:29:50,182 --> 00:29:53,820
automatic data classification, for example

443
00:29:55,070 --> 00:29:58,810
KNM classification if you have training

444
00:29:58,880 --> 00:30:02,160
data or previously classified data available.

445
00:30:02,530 --> 00:30:06,142
Or you can do zero shot classification if you just

446
00:30:06,196 --> 00:30:09,486
want to use the context in Weaviate and

447
00:30:09,508 --> 00:30:11,600
you don't have any training data available.

448
00:30:14,450 --> 00:30:18,254
So that was my presentation about the vector search

449
00:30:18,292 --> 00:30:21,200
engine VV eight. If you have questions,

450
00:30:22,050 --> 00:30:24,990
I'm available in the chat of this conference.

451
00:30:25,850 --> 00:30:29,174
Or you can of course join our slack channel,

452
00:30:29,372 --> 00:30:32,786
our community slack channel, which is very active,

453
00:30:32,978 --> 00:30:36,520
and there's a lot of people who can help you out with questions.

454
00:30:38,170 --> 00:30:41,734
You can always shoot me an email, of course. And if you want to find

455
00:30:41,772 --> 00:30:46,070
out more, you can go to our website, which is VvT IO.

456
00:30:46,410 --> 00:30:47,780
Okay, thank you very much.

