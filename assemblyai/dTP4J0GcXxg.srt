1
00:00:22,250 --> 00:00:25,398
Welcome. In this session I will discuss as

2
00:00:25,484 --> 00:00:29,186
how you can take your data science and machine learning projects

3
00:00:29,298 --> 00:00:32,626
from idea to production by automating your machine

4
00:00:32,658 --> 00:00:34,710
learning workflows with pipelines.

5
00:00:35,770 --> 00:00:39,922
Before I start, I want to point out two great learning resources

6
00:00:39,986 --> 00:00:43,350
to follow up on this topic after today's session.

7
00:00:43,690 --> 00:00:47,442
Besides working as a developer advocate, I'm also an O'Reilly author

8
00:00:47,506 --> 00:00:51,182
and coursera instructor. The O'Reilly book data

9
00:00:51,236 --> 00:00:54,894
Science on AWS, which I coauthored, discusses in

10
00:00:54,932 --> 00:00:58,186
over 500 pages and hundreds of code samples

11
00:00:58,298 --> 00:01:01,742
how to implement endtoend, continuous AI and machine learning

12
00:01:01,796 --> 00:01:05,806
pipelines. Another great resource is the newly

13
00:01:05,838 --> 00:01:08,846
launched practical data science specialization.

14
00:01:09,038 --> 00:01:12,450
In partnership with deep Learning AI and Casera,

15
00:01:13,190 --> 00:01:16,482
this three core specialization teaches you practical

16
00:01:16,546 --> 00:01:20,114
skills in how to take your data science and ML projects

17
00:01:20,162 --> 00:01:24,086
from idea to production using purpose build tools in

18
00:01:24,108 --> 00:01:28,182
the AWS cloud, and it also includes on demand,

19
00:01:28,246 --> 00:01:30,650
hands on labs for you to practice.

20
00:01:32,670 --> 00:01:36,140
So we're talking about automating machine learning.

21
00:01:36,910 --> 00:01:39,020
Hmm. I have an idea.

22
00:01:41,490 --> 00:01:44,160
Alexa deploy my model.

23
00:01:45,650 --> 00:01:48,990
Which multi armed bandit strategy would you like to use?

24
00:01:49,140 --> 00:01:52,960
Thompson sampling Epsilon Greedy or online cover?

25
00:01:54,710 --> 00:01:58,286
Well, I'm pretty sure someone already thought about developing

26
00:01:58,318 --> 00:02:01,534
this Alexa skill, but unfortunately,

27
00:02:01,662 --> 00:02:05,206
getting your machine learning projects ready for production is

28
00:02:05,228 --> 00:02:09,366
not just about technology. A term you

29
00:02:09,388 --> 00:02:12,674
will likely hear in this context of getting your ML applications

30
00:02:12,722 --> 00:02:15,510
ready for production is mlops.

31
00:02:16,170 --> 00:02:20,118
MLOPs builds on DevOps practices that encompasses

32
00:02:20,214 --> 00:02:22,620
people, process and technology.

33
00:02:23,710 --> 00:02:27,366
However, MLOPs also includes considerations

34
00:02:27,478 --> 00:02:31,210
and practices that are really unique to machine learning

35
00:02:31,280 --> 00:02:34,974
workflows. So while most of the time we

36
00:02:35,012 --> 00:02:38,766
tend to focus on the technology, people and process

37
00:02:38,868 --> 00:02:41,120
are equally, if not more important.

38
00:02:42,390 --> 00:02:46,430
Let's take a look at a few key considerations in ensuring

39
00:02:46,510 --> 00:02:49,714
your models and machine learning workloads have

40
00:02:49,752 --> 00:02:51,330
a path to production.

41
00:02:53,750 --> 00:02:57,510
First of all, the machine learning development lifecycle is very

42
00:02:57,580 --> 00:03:00,550
different from a software development lifecycle.

43
00:03:01,290 --> 00:03:05,314
For example, model development includes longer experimentation

44
00:03:05,362 --> 00:03:08,586
cycles compared to what you would typically see

45
00:03:08,688 --> 00:03:11,100
in an agile software development process.

46
00:03:12,350 --> 00:03:15,254
You need to consider choosing the right data meets,

47
00:03:15,382 --> 00:03:19,290
perform data transformations, and feature engineering.

48
00:03:19,890 --> 00:03:23,502
So besides the actual model training code, you also

49
00:03:23,556 --> 00:03:26,240
need to develop the data processing code.

50
00:03:27,570 --> 00:03:31,070
Next. The model is typically only a small part

51
00:03:31,140 --> 00:03:34,610
of an overall machine learning solution, and there are often

52
00:03:34,760 --> 00:03:38,050
more components that need to be built or integrated.

53
00:03:39,110 --> 00:03:43,214
For example, maybe the model needs to be integrated into an existing

54
00:03:43,262 --> 00:03:46,966
application to trigger further process developing on

55
00:03:46,988 --> 00:03:50,454
production results. This leads to the next

56
00:03:50,492 --> 00:03:51,430
consideration.

57
00:03:53,930 --> 00:03:57,474
There are typically multiple personas involved in the

58
00:03:57,532 --> 00:04:01,722
machine learning development lifecycle, often with competing needs and

59
00:04:01,776 --> 00:04:06,102
priorities. A data scientist might feel comfortable

60
00:04:06,246 --> 00:04:09,334
in building a models that meets the expected

61
00:04:09,382 --> 00:04:13,134
model performance metrics, but might not know how

62
00:04:13,172 --> 00:04:16,426
to host that model in a way that it can be consumed

63
00:04:16,458 --> 00:04:18,480
by another system or application.

64
00:04:19,570 --> 00:04:23,694
This part might require a DevOps engineer or the infrastructure

65
00:04:23,742 --> 00:04:27,294
team. You also need to integrate

66
00:04:27,342 --> 00:04:30,786
the projects with existing it systems and

67
00:04:30,808 --> 00:04:33,970
practices, such as change management,

68
00:04:34,550 --> 00:04:38,422
for example. This could mean that as part of

69
00:04:38,476 --> 00:04:42,390
the pipelines, you automatically open a change ticket

70
00:04:42,970 --> 00:04:46,642
anytime a new model is ready to get deployed

71
00:04:46,706 --> 00:04:50,186
into production. Or you might want to

72
00:04:50,208 --> 00:04:54,134
add a manual approval steps in your pipeline before deploying

73
00:04:54,182 --> 00:04:56,250
any model into your production.

74
00:04:57,710 --> 00:05:01,118
If we look at the goal of mlops, you want to move away

75
00:05:01,204 --> 00:05:04,494
from manually building models, which is often still

76
00:05:04,532 --> 00:05:08,346
the status quo. In the first phase,

77
00:05:08,458 --> 00:05:12,394
you can accelerate the path to production by instead of building and

78
00:05:12,452 --> 00:05:16,142
managing individual models, start building and managing

79
00:05:16,206 --> 00:05:19,586
pipelines. You also want

80
00:05:19,608 --> 00:05:23,394
to improve the quality of deployed models. To do

81
00:05:23,432 --> 00:05:27,110
this, you need to be able to detect model decay,

82
00:05:27,530 --> 00:05:30,854
maybe due to a drift in the statistical changes in

83
00:05:30,892 --> 00:05:34,754
data distributions. You should also monitor

84
00:05:34,802 --> 00:05:38,566
the models for any drifts in bias or explainability

85
00:05:38,678 --> 00:05:42,662
from a set baseline. This can be accomplished

86
00:05:42,726 --> 00:05:47,130
in a second phase, and ultimately

87
00:05:48,110 --> 00:05:51,690
this should lead into building AI ML solutions

88
00:05:51,770 --> 00:05:54,970
that are resilient, secure, performant,

89
00:05:55,130 --> 00:05:58,350
operationally efficient, and cost optimized.

90
00:05:59,730 --> 00:06:01,950
Let's have a close look at each phase.

91
00:06:04,310 --> 00:06:08,386
Today we often still manually build and manage individual

92
00:06:08,488 --> 00:06:11,854
models. We also execute

93
00:06:11,902 --> 00:06:15,170
each step in the model development workflow individually.

94
00:06:16,090 --> 00:06:19,942
Here is an example workflow where a data engineer may create

95
00:06:19,996 --> 00:06:24,070
a raw data set and manually send it to a data scientist.

96
00:06:25,130 --> 00:06:29,062
Then the data scientist iteratively performs

97
00:06:29,126 --> 00:06:33,334
data preparation and features. Engineering performs

98
00:06:33,382 --> 00:06:36,682
multiple experiments until a training model

99
00:06:36,736 --> 00:06:40,066
is actually performing well according to the objective

100
00:06:40,118 --> 00:06:44,090
metrics. Then the data scientist

101
00:06:44,170 --> 00:06:47,294
may hand it off to a deployment team or an

102
00:06:47,332 --> 00:06:51,200
ML engineer who is then responsible for deploying the model.

103
00:06:52,690 --> 00:06:56,270
If there has been limited communication between teams,

104
00:06:56,430 --> 00:07:00,274
this part could result in a lot of delays because the model is

105
00:07:00,312 --> 00:07:03,746
essentially intransparent to the deployment engineer or

106
00:07:03,768 --> 00:07:07,766
the DevOps team, meaning there is limited visibility into

107
00:07:07,868 --> 00:07:11,400
how the model is built or how you consume that model.

108
00:07:12,490 --> 00:07:16,374
Then a software engineer potentially needs to make

109
00:07:16,412 --> 00:07:19,866
changes to the application, which consumes that

110
00:07:19,888 --> 00:07:23,290
model for prediction. And finally,

111
00:07:23,440 --> 00:07:26,838
someone ultimately needs to operate the model in production,

112
00:07:26,934 --> 00:07:30,814
which includes making sure the right level of monitoring is

113
00:07:30,852 --> 00:07:34,842
set up. We can see the challenges

114
00:07:34,906 --> 00:07:38,666
in this setup. The workflows includes multiple handoffs

115
00:07:38,698 --> 00:07:42,014
between teams and personas who might not

116
00:07:42,052 --> 00:07:44,930
all be familiar with machine learning workloads.

117
00:07:45,910 --> 00:07:50,030
A limited cross team collaboration could lead to limited visibility

118
00:07:50,190 --> 00:07:53,826
and transparency using increased code

119
00:07:53,928 --> 00:07:57,734
rework, and ultimately slows down the ability to

120
00:07:57,772 --> 00:08:01,414
get the model to production quickly. So what

121
00:08:01,452 --> 00:08:04,870
can we do in a first

122
00:08:04,940 --> 00:08:08,582
phase, we should improve the situation by orchestrating

123
00:08:08,646 --> 00:08:11,370
the individual steps. As a pipeline,

124
00:08:12,270 --> 00:08:15,402
we can also look at automating tasks in each

125
00:08:15,456 --> 00:08:19,178
step. For example,

126
00:08:19,344 --> 00:08:23,114
we can build a model training pipeline that orchestrating

127
00:08:23,162 --> 00:08:27,130
the data preparation, model training, and model evaluation

128
00:08:27,210 --> 00:08:31,242
steps. We could also build a deployed

129
00:08:31,306 --> 00:08:35,122
pipeline which grabs a model from a model registry and

130
00:08:35,176 --> 00:08:37,650
deploys it into a staging environment.

131
00:08:38,630 --> 00:08:42,066
The software engineers could then use this model to

132
00:08:42,168 --> 00:08:45,506
run unit or integration tests before approving

133
00:08:45,538 --> 00:08:47,990
the model for production deployment.

134
00:08:49,610 --> 00:08:52,870
Let's see a demo of a model training pipeline.

135
00:08:54,090 --> 00:08:58,040
All right, here I am in my AWS demo account,

136
00:08:58,430 --> 00:09:02,454
and I want to show you how you can leverage Amazon Sagemaker

137
00:09:02,502 --> 00:09:05,702
pipelines to automate the individual steps

138
00:09:05,766 --> 00:09:09,594
of building a machine learning model. Amazon Sagemaker is

139
00:09:09,632 --> 00:09:12,958
a fully managed service that helps you build, train,

140
00:09:13,044 --> 00:09:15,710
tune, and deploy your machine learning models.

141
00:09:17,090 --> 00:09:20,800
All right, so the use case we're going to build is

142
00:09:21,170 --> 00:09:25,522
I want to train a natural language process model to

143
00:09:25,576 --> 00:09:29,154
classify product reviews. So I'm going to pass in

144
00:09:29,192 --> 00:09:31,890
raw text product review text. For example,

145
00:09:32,040 --> 00:09:36,254
I really enjoyed reading this book, and my NLP

146
00:09:36,302 --> 00:09:40,086
model should classify this into a star rating. So in this case,

147
00:09:40,188 --> 00:09:43,654
hopefully a star rating of a five the best, or a star

148
00:09:43,692 --> 00:09:47,030
rating of 4321, with one being the worst.

149
00:09:47,930 --> 00:09:52,094
And the way we're doing this, I'm going to use a pretrained

150
00:09:52,162 --> 00:09:55,866
bird model. Bird is a very popular model architecture in

151
00:09:55,888 --> 00:09:59,402
the NLP states, and what you can leverage is actually

152
00:09:59,456 --> 00:10:03,334
pre trained models that have been trained on millions of documents

153
00:10:03,382 --> 00:10:07,086
already, for example the Wikipedia. And then you can fine tune it

154
00:10:07,108 --> 00:10:10,558
to your specific data set, which I will do in the training step in

155
00:10:10,564 --> 00:10:15,060
this pipelines, fine tuning it to my specific product reviews text.

156
00:10:16,470 --> 00:10:20,226
So the DAC we're going to build here is first process

157
00:10:20,328 --> 00:10:23,634
the raw text data to generate the embeddings that

158
00:10:23,672 --> 00:10:26,150
the bird model expects as inputs.

159
00:10:26,730 --> 00:10:30,294
Then in the training step, I'm going to fine tune the model to

160
00:10:30,332 --> 00:10:34,402
my data set, and I'm also evaluating

161
00:10:34,466 --> 00:10:37,834
the model performance. So in this case, I'm coding the

162
00:10:37,872 --> 00:10:41,482
validation accuracy of my model and

163
00:10:41,536 --> 00:10:45,498
I'm defining a threshold, a condition. And if my

164
00:10:45,584 --> 00:10:48,630
model performs above this threshold,

165
00:10:48,790 --> 00:10:52,046
then I'm going to register it to a model registry. Think of

166
00:10:52,068 --> 00:10:55,498
it as a catalog of your models to compare the different versions.

167
00:10:55,674 --> 00:10:59,550
And I'm also preparing for deployment by creating a model object

168
00:10:59,620 --> 00:11:03,266
here in Sagemaker. All right,

169
00:11:03,288 --> 00:11:05,380
so let's see how we can build this.

170
00:11:06,630 --> 00:11:10,430
First of all, here I'm importing a couple of sdks

171
00:11:10,510 --> 00:11:14,974
and libraries. One of them is the Sagemaker Python SDK

172
00:11:15,102 --> 00:11:18,614
and a couple of additional libraries used here in the

173
00:11:18,652 --> 00:11:21,942
AWS environment. All right, first of all,

174
00:11:21,996 --> 00:11:25,414
I'm going to import or set the location of the raw data

175
00:11:25,452 --> 00:11:28,902
set, which is my reviews data here

176
00:11:28,956 --> 00:11:31,530
hosted in public s three bucket.

177
00:11:32,270 --> 00:11:35,786
And what I'm going to do here is I'm pulling a subset of the

178
00:11:35,808 --> 00:11:39,882
data just for demo purposes here into my own AWS account.

179
00:11:40,016 --> 00:11:42,890
So I'm setting a path here to my own bucket,

180
00:11:43,890 --> 00:11:47,626
and I'm going to pull in just a subset here so the model training doesn't

181
00:11:47,658 --> 00:11:51,738
run for too long, and I'm just pulling in three different categories

182
00:11:51,834 --> 00:11:53,700
of the product reviews data.

183
00:11:55,430 --> 00:11:58,610
All right, let's start building the actual pipeline.

184
00:11:59,110 --> 00:12:02,878
So first of all, I'm creating a name for the pipeline.

185
00:12:02,974 --> 00:12:07,030
Let's call this my Bird pipeline, and a timestamp.

186
00:12:08,090 --> 00:12:11,366
And then one nice thing about pipelines is that you

187
00:12:11,388 --> 00:12:16,470
can define parameters to parameterize individual executions.

188
00:12:16,810 --> 00:12:20,026
And now let's start with building the first step, which is the

189
00:12:20,048 --> 00:12:23,722
feature engineering. Here's a little bit

190
00:12:23,776 --> 00:12:27,802
of explanation what we're going to do. So my raw data set

191
00:12:27,856 --> 00:12:31,302
here on the left has star ratings

192
00:12:31,366 --> 00:12:34,766
and the review meets. So for example, this is

193
00:12:34,788 --> 00:12:38,202
a great item, or I love this book, and the corresponding label,

194
00:12:38,266 --> 00:12:41,406
which is the star rating. And what I'm going

195
00:12:41,428 --> 00:12:45,566
to do in this first features engineering step is I'm using a sagemaker

196
00:12:45,598 --> 00:12:49,106
processing job, which helps me to execute code

197
00:12:49,208 --> 00:12:52,622
on data, so it's specifically suited if you want to run feature

198
00:12:52,686 --> 00:12:56,582
engineering. And I'm converting this raw input data

199
00:12:56,716 --> 00:13:00,086
into embeddings that the bird model expects as

200
00:13:00,108 --> 00:13:03,766
inputs. All right, so what

201
00:13:03,788 --> 00:13:07,830
I'm going to do here again, I'm making sure I have access

202
00:13:07,900 --> 00:13:11,370
to my input data, which is now in my own bucket.

203
00:13:12,110 --> 00:13:15,386
And I start by preparing a couple of those parameters which I

204
00:13:15,408 --> 00:13:18,842
want to be able to parameterize. So one

205
00:13:18,896 --> 00:13:22,430
is definitely where to find the input data in case the location changes,

206
00:13:22,500 --> 00:13:24,480
or I want to use a different data set.

207
00:13:25,410 --> 00:13:29,742
I'm also specifying the processing job instance count.

208
00:13:29,876 --> 00:13:33,358
So what I could do, depending on how much data I need to process,

209
00:13:33,524 --> 00:13:37,220
I can run this distributed, so I can run it across one

210
00:13:37,590 --> 00:13:41,454
AWS cloud instance. I could run it across two instances, five instances,

211
00:13:41,502 --> 00:13:45,234
et cetera. And it's as easy as just setting the value to one

212
00:13:45,272 --> 00:13:49,126
or five or ten. And the processing job will make sure to

213
00:13:49,148 --> 00:13:53,510
distribute the data across the instances and work in parallel.

214
00:13:53,850 --> 00:13:57,046
In this case, I have a small subset of the data, so I'll stick to

215
00:13:57,068 --> 00:14:01,258
one instance. I can also specify here

216
00:14:01,344 --> 00:14:05,402
the instance type. So this is the AWS easy two instance type

217
00:14:05,456 --> 00:14:09,066
managed by Sagemaker to run this processing job,

218
00:14:09,168 --> 00:14:12,560
and I'm just specifying one particular instance type here.

219
00:14:13,890 --> 00:14:17,562
Then I'm also defining a couple of parameters, which my feature

220
00:14:17,626 --> 00:14:19,710
engineering script requires.

221
00:14:21,090 --> 00:14:24,946
Then I could set parameters such as do I want to balance the

222
00:14:24,968 --> 00:14:28,946
data set before and percentages how to

223
00:14:28,968 --> 00:14:33,060
split the data into a training set, validation and test data set.

224
00:14:33,510 --> 00:14:37,318
So in this case, I'm taking 90% for my training data,

225
00:14:37,484 --> 00:14:40,854
and I keep a split of 5% for validation and

226
00:14:40,892 --> 00:14:43,160
another 5% for test.

227
00:14:44,570 --> 00:14:48,326
All right, then the step actually needs to perform the

228
00:14:48,348 --> 00:14:51,514
feature engineering. So what I've done in preparation is

229
00:14:51,552 --> 00:14:55,766
I wrote a Python script which performs the actual transformations,

230
00:14:55,958 --> 00:14:58,822
and this is here in this Python file.

231
00:14:58,966 --> 00:15:02,634
So I'm not going to go into all of the glory details. If you're curious

232
00:15:02,682 --> 00:15:05,710
how to do this, have a look at the GitHub repo.

233
00:15:06,370 --> 00:15:09,806
All right, then I can start creating this

234
00:15:09,908 --> 00:15:13,226
processing job, and for that I'll

235
00:15:13,258 --> 00:15:17,522
define a processor. Here I'm using a prebuilt process based

236
00:15:17,576 --> 00:15:20,862
on scikit learn. I'm defining the framework

237
00:15:20,926 --> 00:15:24,542
version passing in my IAM role the instance

238
00:15:24,606 --> 00:15:28,482
type and the instance count, and also the region I'm operating

239
00:15:28,546 --> 00:15:32,102
in. And I need one more thing,

240
00:15:32,156 --> 00:15:35,446
because before I wrap it into the official workflow and

241
00:15:35,468 --> 00:15:39,110
pipelines step, I need to define the inputs

242
00:15:39,270 --> 00:15:43,050
for the job and the outputs. The inputs here

243
00:15:43,120 --> 00:15:45,340
is the raw input data,

244
00:15:46,350 --> 00:15:51,098
and the outputs are s three folders

245
00:15:51,194 --> 00:15:54,890
where to store the generated features.

246
00:15:55,050 --> 00:15:57,870
And I'm also going to split this again by training,

247
00:15:57,940 --> 00:16:01,214
validation, and test data meets. So I'm putting here the

248
00:16:01,252 --> 00:16:04,340
three locations where the data will be written to,

249
00:16:05,030 --> 00:16:08,546
and those internal container paths will get mapped to

250
00:16:08,568 --> 00:16:12,114
an s relocation later. All right,

251
00:16:12,232 --> 00:16:15,826
and with that I can define the official step as part of

252
00:16:15,848 --> 00:16:19,414
my pipeline. So here you can see I'm defining a

253
00:16:19,452 --> 00:16:22,694
processing step. I'll give it a name. This is what you

254
00:16:22,732 --> 00:16:25,846
saw in the DAC. I point to

255
00:16:25,868 --> 00:16:29,370
the actual Python code to execute my feature engineering,

256
00:16:29,870 --> 00:16:33,590
and then I'm passing in the scikitlearn processor, which I defined

257
00:16:33,750 --> 00:16:37,878
the inputs and the outputs. And here I'm passing

258
00:16:37,974 --> 00:16:41,626
the specific job arguments that my script requires.

259
00:16:41,738 --> 00:16:44,622
So for example, the training, validation test,

260
00:16:44,756 --> 00:16:46,910
split percentages, et cetera.

261
00:16:47,490 --> 00:16:50,880
All right, this defines my processing step.

262
00:16:52,290 --> 00:16:55,934
Now let's move on to the second steps, which is fine tuning

263
00:16:55,982 --> 00:16:59,380
the model with the help of a sagemaker training job.

264
00:17:00,390 --> 00:17:04,126
And this is pretty similar. So here you can see again I'm defining

265
00:17:04,158 --> 00:17:07,510
parameters, for example, the training instance type and count

266
00:17:07,580 --> 00:17:11,350
again. And then I'm setting up parameters,

267
00:17:12,410 --> 00:17:15,990
the hyperparameters will depend on the models you're using

268
00:17:16,060 --> 00:17:19,958
the use case. So in my case, some general parameters,

269
00:17:20,054 --> 00:17:23,242
number of epics runs throughout the whole data set.

270
00:17:23,376 --> 00:17:27,126
And I'm just going to keep this here to one. For this demo purpose,

271
00:17:27,318 --> 00:17:31,226
I'm setting a learning rate and then additional values,

272
00:17:31,258 --> 00:17:34,362
for example, the Epsilon value train, batch sizes,

273
00:17:34,426 --> 00:17:38,346
validation batch sizes, et cetera. Again, this will highly

274
00:17:38,378 --> 00:17:42,666
depend on the type of model and use case you are

275
00:17:42,788 --> 00:17:45,300
training. All right,

276
00:17:45,670 --> 00:17:48,830
next, what I'm going to do is I'm also going to capture

277
00:17:48,910 --> 00:17:52,034
the performance of my model during training.

278
00:17:52,232 --> 00:17:55,990
So I can specify here regex expressions

279
00:17:56,330 --> 00:18:00,146
that the model training code will output in the logs.

280
00:18:00,258 --> 00:18:04,066
So for example, my script that I use will put validation loss

281
00:18:04,098 --> 00:18:07,726
and validation accuracy as an output. And I'm

282
00:18:07,778 --> 00:18:11,546
using here those regex expressions to capture them from

283
00:18:11,568 --> 00:18:15,514
the locks and then also are available in the UI and

284
00:18:15,552 --> 00:18:18,460
for me to check later on in the evaluation step.

285
00:18:20,030 --> 00:18:23,210
All right, and I've talked about the training script.

286
00:18:23,290 --> 00:18:26,606
So again, I've prepared a Python file which contains the

287
00:18:26,628 --> 00:18:30,602
code to train my model. This is here in the Tfbirdreviews

288
00:18:30,666 --> 00:18:34,026
py file. And again, I'm not going into details. If you're

289
00:18:34,058 --> 00:18:37,186
interested in seeing how to do this, in particular,

290
00:18:37,288 --> 00:18:40,350
how to use this pretrained hugging phase

291
00:18:40,430 --> 00:18:44,146
model and then just fine tune it to the data, please check out

292
00:18:44,168 --> 00:18:47,750
the code in the GitHub repo. All right,

293
00:18:47,820 --> 00:18:51,846
so we can now prepare the training estimator and

294
00:18:51,868 --> 00:18:55,046
then build the model training step. So first

295
00:18:55,068 --> 00:18:58,486
of all, I actually need to define the estimator which performs

296
00:18:58,518 --> 00:19:02,538
the training. And I'm using

297
00:19:02,704 --> 00:19:06,154
a built in Tensorflow estimator with Sagemaker, which has

298
00:19:06,192 --> 00:19:10,038
optimizations to run Tensorflow on AWS. And as

299
00:19:10,064 --> 00:19:13,226
you can see here, I'm defining this Tensorflow estimator,

300
00:19:13,338 --> 00:19:17,338
pointing to the training script, which I just highlighted,

301
00:19:17,514 --> 00:19:21,354
and also passing in the additional parameters, the role,

302
00:19:21,402 --> 00:19:24,718
instance count and type, python version,

303
00:19:24,814 --> 00:19:28,130
tensorflow framework version I want to use. And again,

304
00:19:28,200 --> 00:19:31,250
my hyperparameters, which I defined earlier.

305
00:19:32,230 --> 00:19:36,150
One more step actually, that I'm coding to do is activating

306
00:19:36,490 --> 00:19:39,926
step caching with steps caching. What you

307
00:19:39,948 --> 00:19:43,298
can do is make sure if you're rerunning

308
00:19:43,314 --> 00:19:47,414
the pipeline and individual steps might have not have changed to

309
00:19:47,452 --> 00:19:51,322
reuse the previous results. So Sagemaker will apply

310
00:19:51,376 --> 00:19:54,442
this caching to help you accelerate and

311
00:19:54,496 --> 00:19:56,970
run the different executions more efficient.

312
00:19:57,710 --> 00:20:00,620
All right? And with that, I can define the training step.

313
00:20:01,310 --> 00:20:04,746
So here you can see I define

314
00:20:04,778 --> 00:20:08,110
the official training step. Give it a train. Again, this name,

315
00:20:08,180 --> 00:20:12,106
the train will appear in the DAG as you could see before then I'm passing

316
00:20:12,138 --> 00:20:16,334
in this estimator that has all of the tensorflow and training script

317
00:20:16,382 --> 00:20:20,094
configuration, and I'm also training the inputs.

318
00:20:20,222 --> 00:20:23,506
And here you can see I'm referring to

319
00:20:23,528 --> 00:20:27,310
the previous process step output and using it

320
00:20:27,320 --> 00:20:31,014
as an input for the training. So those are the features that I generated for

321
00:20:31,052 --> 00:20:33,960
bird training, for bird validation, and for bird test.

322
00:20:35,450 --> 00:20:39,050
All right, after the training, there comes the model evaluation.

323
00:20:40,190 --> 00:20:42,140
So let's see how I can do this.

324
00:20:43,710 --> 00:20:47,254
The model evaluation I can also execute as a processing

325
00:20:47,302 --> 00:20:50,658
job again. So I'm going to use this scikitlearn processor

326
00:20:50,694 --> 00:20:54,074
again specifying the framework version instance

327
00:20:54,122 --> 00:20:57,486
types and counts. The difference here is that I

328
00:20:57,508 --> 00:21:00,682
do have another script to execute. So instead of the feature

329
00:21:00,746 --> 00:21:05,410
engineering, I've now written a script that evaluates the model performance.

330
00:21:05,910 --> 00:21:08,930
And again, here is a link to the python script.

331
00:21:09,270 --> 00:21:13,214
Basically what I'm going to do is I'm using the pretrained

332
00:21:13,262 --> 00:21:16,786
and fine tuned model from the previous step, and I'm

333
00:21:16,818 --> 00:21:20,360
running some test predictions and see

334
00:21:20,730 --> 00:21:24,722
how the performance is. So in this case, I'm specifically

335
00:21:24,786 --> 00:21:27,640
looking for the validation performance of my model.

336
00:21:28,830 --> 00:21:33,174
All right, and the results get written into JSON

337
00:21:33,222 --> 00:21:37,430
file, which I call the evaluation JSON, which is the official evaluation

338
00:21:37,510 --> 00:21:41,514
report from the step. And here's the official

339
00:21:41,562 --> 00:21:45,102
definition. So this is actually implemented as another processing step.

340
00:21:45,236 --> 00:21:47,680
In this case I call it evaluate the model.

341
00:21:48,050 --> 00:21:52,270
I'm pointing to this new script which runs the validation.

342
00:21:53,010 --> 00:21:56,722
And again I'm pointing it here to inputs, which in this case

343
00:21:56,856 --> 00:22:00,050
is the fine tuned model, the model artifact from the training

344
00:22:00,120 --> 00:22:03,858
step, and also again, input data

345
00:22:03,944 --> 00:22:05,990
which I could use to run validation.

346
00:22:07,050 --> 00:22:10,742
And I'm also pointing out an output location to store

347
00:22:10,796 --> 00:22:12,310
the evaluation results.

348
00:22:13,930 --> 00:22:17,206
All right, the official model metrics are defined in an

349
00:22:17,228 --> 00:22:20,950
object, which I do here, which contain the evaluation

350
00:22:21,030 --> 00:22:24,074
JSON file. All right,

351
00:22:24,192 --> 00:22:27,674
and the last part now is to define the

352
00:22:27,712 --> 00:22:31,466
condition step. So, checking whether my model meets

353
00:22:31,498 --> 00:22:34,958
the expected highquality gate. And if yes,

354
00:22:35,124 --> 00:22:38,986
then register the model to the model registry and also prepare

355
00:22:39,018 --> 00:22:42,762
for deployment. And what I'm going to do here is first

356
00:22:42,836 --> 00:22:46,594
create those steps afterwards, and then I can reference them

357
00:22:46,632 --> 00:22:49,540
in the actual condition step. So let's see this.

358
00:22:51,030 --> 00:22:54,766
And one nice thing that you can do with sagemaker pipelines

359
00:22:54,878 --> 00:22:58,402
is to actually set a model approval status.

360
00:22:58,466 --> 00:23:02,310
So when you get a model, you evaluate it. You can specify

361
00:23:02,650 --> 00:23:06,050
whether this has a pending, for example, manual approval.

362
00:23:06,130 --> 00:23:09,366
So somebody has to look at the metrics and then approve

363
00:23:09,398 --> 00:23:12,902
it for deployment. You can set it to always approve,

364
00:23:13,046 --> 00:23:16,406
but in this case, I want to show you I keep it to manual approval

365
00:23:16,438 --> 00:23:20,358
only. All right, then I'm

366
00:23:20,374 --> 00:23:23,630
going to define again the instance types and counts, where to later

367
00:23:23,700 --> 00:23:26,670
deploy my model and host it for live predictions.

368
00:23:28,050 --> 00:23:31,790
And I'm also specifying the model package group, which is

369
00:23:31,860 --> 00:23:35,566
registered in the model registry. And I'm

370
00:23:35,598 --> 00:23:39,646
defining an image that is used to later deploy

371
00:23:39,678 --> 00:23:42,786
the endpoint and then run the model and the

372
00:23:42,808 --> 00:23:46,818
inference code. So in this case it's going to be a tensorflow based docker image

373
00:23:46,834 --> 00:23:50,914
again. All right, so here is the step that registers

374
00:23:50,962 --> 00:23:55,062
the model. It's taking the estimator object and

375
00:23:55,116 --> 00:23:57,960
the information about the inference image to use.

376
00:23:58,490 --> 00:24:01,926
It actually points to the s three location of the fine tuned

377
00:24:01,958 --> 00:24:05,286
model, and it also defines specific input

378
00:24:05,398 --> 00:24:09,158
format types. For example, you know that your model expects JSON lines

379
00:24:09,174 --> 00:24:12,398
as input, and also response is going to

380
00:24:12,404 --> 00:24:16,400
be in JSON lines. Then this might vary, of course, depending on your model.

381
00:24:17,170 --> 00:24:21,354
And you also set here the model package group, where to register

382
00:24:21,402 --> 00:24:25,810
it with, and the approval status. This one will be pending manual approval.

383
00:24:26,710 --> 00:24:30,770
All right, then what I'm also going to do here is create a

384
00:24:30,920 --> 00:24:34,100
step to prepare the model for deployment later.

385
00:24:34,470 --> 00:24:37,990
So I'm preparing a model object in sagemaker,

386
00:24:38,570 --> 00:24:42,710
again, pass in the inference image and also the artifact.

387
00:24:44,410 --> 00:24:47,826
All right, and then I define here

388
00:24:47,868 --> 00:24:50,970
the official create model steps for the pipeline

389
00:24:51,390 --> 00:24:53,850
and pass in the model which I just created.

390
00:24:54,670 --> 00:24:58,154
So now we can start in creating this condition check that

391
00:24:58,192 --> 00:25:02,366
comes before. So what

392
00:25:02,388 --> 00:25:05,898
I'm going to do here is I'm importing the conditions

393
00:25:06,074 --> 00:25:09,470
and corresponding functions that are available. For example,

394
00:25:09,540 --> 00:25:13,114
a condition greater than or equal to, and I'm

395
00:25:13,162 --> 00:25:16,594
defining a minimum accuracy value I want to check

396
00:25:16,632 --> 00:25:20,226
against. As this is a demo and I'm just training on a little bit of

397
00:25:20,248 --> 00:25:23,842
data, I'll keep this low so all the model training runs will actually

398
00:25:23,896 --> 00:25:27,266
pass. But obviously in other use cases,

399
00:25:27,298 --> 00:25:30,934
you definitely want to bump up the accuracy threshold. In this case, I'm using

400
00:25:30,972 --> 00:25:35,080
20% as my minimum accuracy to check against.

401
00:25:36,330 --> 00:25:39,462
Then here is the definition. So it's going to execute

402
00:25:39,526 --> 00:25:42,954
this check condition greater or equal to.

403
00:25:43,152 --> 00:25:46,490
And here is my evaluation step.

404
00:25:46,640 --> 00:25:51,246
And I'm also pointing to this report file to generate and

405
00:25:51,348 --> 00:25:53,710
set to this value I created.

406
00:25:54,690 --> 00:25:58,702
And the additional official step is then tuning this

407
00:25:58,756 --> 00:26:02,394
condition. And if I meet the condition.

408
00:26:02,442 --> 00:26:06,510
So if I'm passing my quality threshold, I'm going to register

409
00:26:06,670 --> 00:26:10,146
the model, and I'm also going to create the model in preparation for

410
00:26:10,168 --> 00:26:13,762
later deployment. In the out step you can say,

411
00:26:13,816 --> 00:26:17,300
what else? If I fail the test, right, send a message

412
00:26:17,750 --> 00:26:21,366
to the data scientist or whatever you want to do. In my case, I'm just

413
00:26:21,388 --> 00:26:24,550
keeping it empty so it will fail and end the pipeline.

414
00:26:25,370 --> 00:26:28,946
So what we've done now is defining each and every step from

415
00:26:28,988 --> 00:26:32,234
the preprocessing, to the training, to the

416
00:26:32,272 --> 00:26:36,902
condition and preparation for deployment, if I pass my quality threshold.

417
00:26:37,046 --> 00:26:40,810
So now I can wrap this in the end to end pipelines definition.

418
00:26:41,970 --> 00:26:45,774
First of all, again, I'm importing some of the functions here needed and

419
00:26:45,812 --> 00:26:49,214
objects from the SDK. And as you can see here,

420
00:26:49,252 --> 00:26:53,066
I'm now creating the official pipeline object passing

421
00:26:53,098 --> 00:26:56,994
in the name that we created, and all of the parameters which

422
00:26:57,032 --> 00:27:00,754
I specified in the above code. And then the

423
00:27:00,792 --> 00:27:04,466
steps here will actually line up the individual steps in

424
00:27:04,488 --> 00:27:08,054
this stack. So let's start with the processing, then move

425
00:27:08,092 --> 00:27:11,142
to the training step, do the evaluation step,

426
00:27:11,276 --> 00:27:14,600
and then you have this condition step to evaluate the models,

427
00:27:15,050 --> 00:27:18,440
which will in itself trigger the two different

428
00:27:19,150 --> 00:27:22,540
path depending on if I pass the quality check.

429
00:27:22,990 --> 00:27:26,378
I'm also adding this to an official experiment tracking, so I

430
00:27:26,384 --> 00:27:30,246
can keep track of my pipeline runs, and that's the definition

431
00:27:30,278 --> 00:27:33,950
of my pipelines. And then I can submit the pipeline for execution.

432
00:27:34,290 --> 00:27:37,726
So I'm calling the pipeline create passing a role that has

433
00:27:37,748 --> 00:27:41,166
the permissions to execute everything. And then what I

434
00:27:41,188 --> 00:27:44,020
can do is I can call the pipelines start,

435
00:27:44,630 --> 00:27:48,770
which will start an individual execution run of this pipeline.

436
00:27:49,190 --> 00:27:52,386
And you can see here also you can pass in

437
00:27:52,408 --> 00:27:55,118
now your parameter values,

438
00:27:55,294 --> 00:27:58,722
and that will kick off the pipelines in the background.

439
00:27:58,786 --> 00:28:02,946
But this will run now on AWS with the sagemaker processing

440
00:28:02,978 --> 00:28:06,310
job, the training job, and also run the model evaluation.

441
00:28:06,650 --> 00:28:10,166
And what I also want to show you is that Sagemaker

442
00:28:10,198 --> 00:28:14,026
keeps track of the individual artifacts that are generated in each step.

443
00:28:14,208 --> 00:28:17,334
So what I'm doing here is I'm listing all artifacts

444
00:28:17,382 --> 00:28:21,254
generated by this pipelines. And this is super helpful

445
00:28:21,302 --> 00:28:24,458
if you want to keep track of the individual steps.

446
00:28:24,554 --> 00:28:27,886
And for example, what were the inputs in this case for

447
00:28:27,908 --> 00:28:31,214
the processing job, I do have the raw data. I do

448
00:28:31,252 --> 00:28:34,420
have the docker image that's used to process the data.

449
00:28:35,190 --> 00:28:38,978
And the output is the generated features, and you

450
00:28:38,984 --> 00:28:42,290
can see here contributed to or production.

451
00:28:43,510 --> 00:28:46,574
Then in the training step, you have the generated features,

452
00:28:46,622 --> 00:28:49,926
training features as the input and the image to

453
00:28:49,948 --> 00:28:53,366
execute the training job. And the output here would be

454
00:28:53,388 --> 00:28:56,966
the model artifact. So really nice to keep track of

455
00:28:56,988 --> 00:29:00,762
those artifacts for each step. And with that,

456
00:29:00,816 --> 00:29:04,614
we can come back to actually checking on our pipeline.

457
00:29:04,662 --> 00:29:08,214
So I'm going to go back here in the pipelines.

458
00:29:08,342 --> 00:29:12,170
You can check on the graph, and this is the overall graph that we defined

459
00:29:12,250 --> 00:29:15,870
so it matches our steps.

460
00:29:16,850 --> 00:29:20,458
You can check the parameters. Again, you set the settings,

461
00:29:20,554 --> 00:29:24,574
you can click into the individual graph, and the color coding

462
00:29:24,622 --> 00:29:28,594
here shows green means it completed successfully. You can

463
00:29:28,632 --> 00:29:32,258
click into each step and again see the parameters here

464
00:29:32,344 --> 00:29:35,780
that were used to run and execute the step.

465
00:29:36,390 --> 00:29:39,990
All right, what I also want to show you is the model registry.

466
00:29:40,730 --> 00:29:44,022
So let me go here in the navigation also to the model

467
00:29:44,076 --> 00:29:47,640
registry. And we do see here our model group,

468
00:29:47,950 --> 00:29:52,042
the bird reviews and here is my model

469
00:29:52,096 --> 00:29:55,274
version. And again, I've set this

470
00:29:55,312 --> 00:29:58,474
to manual approval. So this one here

471
00:29:58,672 --> 00:30:02,382
will still need my approval to be deployed into

472
00:30:02,436 --> 00:30:06,714
production. I can update the status

473
00:30:06,842 --> 00:30:10,222
and set it here to approved and

474
00:30:10,276 --> 00:30:14,546
say this is good for

475
00:30:14,728 --> 00:30:17,970
deployment into staging

476
00:30:19,350 --> 00:30:22,946
update. And with that the

477
00:30:22,968 --> 00:30:26,646
model is now approved for deployments and

478
00:30:26,668 --> 00:30:28,920
this completes the first demo.

479
00:30:30,650 --> 00:30:34,658
In the second phase, we could automatically run pipelines

480
00:30:34,834 --> 00:30:37,670
and include automated quality gates.

481
00:30:38,030 --> 00:30:41,910
So here the model training pipeline could automatically

482
00:30:41,990 --> 00:30:45,366
evaluate the model in terms of model performance

483
00:30:45,478 --> 00:30:48,410
or bias metrics and thresholds.

484
00:30:48,990 --> 00:30:52,746
Only models that fall into acceptable performance

485
00:30:52,938 --> 00:30:56,942
metrics get registered in the model registry and

486
00:30:56,996 --> 00:31:00,682
approved for deployment. The deployment pipeline

487
00:31:00,746 --> 00:31:04,782
could leverage deployment strategies such as a b testing

488
00:31:04,926 --> 00:31:08,782
or bandits to evaluate the model in comparison

489
00:31:08,846 --> 00:31:12,850
to existing models. The software engineer can

490
00:31:12,920 --> 00:31:16,562
automate the integration tests with pass fail

491
00:31:16,626 --> 00:31:19,778
highquality gates, and only models that passes

492
00:31:19,874 --> 00:31:23,462
get deployed into production. And finally,

493
00:31:23,596 --> 00:31:27,774
the operation team sets up model monitoring and analyzes

494
00:31:27,842 --> 00:31:30,330
for any data drift or models drift.

495
00:31:31,230 --> 00:31:34,870
If the drift is violating defined threshold values,

496
00:31:35,030 --> 00:31:38,262
this could actually trigger a model retraining pipeline.

497
00:31:38,326 --> 00:31:42,462
Again, another trigger to rerun a model

498
00:31:42,516 --> 00:31:45,854
training and deployment pipeline could be code changes

499
00:31:46,052 --> 00:31:49,674
as part of a continuous integrations and continuous

500
00:31:49,722 --> 00:31:54,766
delivery. Short CI CD automation let's

501
00:31:54,798 --> 00:31:58,574
see another demo of a code change triggered pipelines

502
00:31:58,622 --> 00:32:02,066
run all right, I'm back in

503
00:32:02,088 --> 00:32:05,766
my AWS demo account. Now I want to

504
00:32:05,788 --> 00:32:09,586
show you how you can leverage Sagemaker projects

505
00:32:09,698 --> 00:32:13,634
to automate workflows. Pipelines runs Sagemaker

506
00:32:13,682 --> 00:32:17,358
projects helps you to set up all of the automation

507
00:32:17,474 --> 00:32:20,698
needed in the AWS account to build a

508
00:32:20,704 --> 00:32:24,650
continuous integration, continuous deployed workflow.

509
00:32:25,150 --> 00:32:28,602
The easiest way to get started is if you navigate here

510
00:32:28,656 --> 00:32:32,170
in the menu to projects and then

511
00:32:32,240 --> 00:32:36,078
create project. You can see that it already comes with a

512
00:32:36,084 --> 00:32:39,054
couple of prebuilt templates which you can use.

513
00:32:39,252 --> 00:32:42,678
So one template will set up all of the automation

514
00:32:42,874 --> 00:32:46,770
for a model building and training pipeline and automation.

515
00:32:47,270 --> 00:32:51,134
The other one is for model deployment. And what I've pre provisioned

516
00:32:51,182 --> 00:32:54,974
here is a CI CD environment for

517
00:32:55,032 --> 00:32:58,706
model building, training and the actual model deployment.

518
00:32:58,898 --> 00:33:02,582
I've already pre built everything, so let me walk you

519
00:33:02,636 --> 00:33:04,710
through the steps here. Programmatically,

520
00:33:05,690 --> 00:33:08,890
Sagemaker project is based on an AWS service

521
00:33:08,960 --> 00:33:13,238
catalog product. So the first step is to reusable sagemaker

522
00:33:13,254 --> 00:33:15,770
projects here in the studio environment.

523
00:33:16,990 --> 00:33:20,574
Then I'm again importing all of the needed sdks and

524
00:33:20,612 --> 00:33:24,666
libraries and set the clients, and I'm

525
00:33:24,698 --> 00:33:28,426
coding it here programmatically. But again, if you're coding the template

526
00:33:28,458 --> 00:33:32,590
through the UI, this is done for you. I've provisioned the service

527
00:33:32,660 --> 00:33:36,850
catalog item and the template here programmatically through my notebook.

528
00:33:37,430 --> 00:33:40,174
Once the project is created,

529
00:33:40,302 --> 00:33:43,954
you can see here on the left that I do have this entry. Now for

530
00:33:43,992 --> 00:33:47,574
projects, I can select it. And what

531
00:33:47,612 --> 00:33:51,362
will happen here in the first step is that sagemaker projects

532
00:33:51,426 --> 00:33:54,534
will create two code commit repos for

533
00:33:54,572 --> 00:33:57,994
you in this AWS account you're using. So here

534
00:33:58,032 --> 00:34:00,570
you can see I do have two repos,

535
00:34:00,910 --> 00:34:04,342
one for the model build and training pipelines,

536
00:34:04,486 --> 00:34:07,770
and one for the model deployment pipeline.

537
00:34:08,350 --> 00:34:11,966
And all of the automation is set up. So whenever I

538
00:34:12,148 --> 00:34:16,078
commit code, push code to those repos, that will actually

539
00:34:16,164 --> 00:34:19,246
trigger a pipeline execution. And I'll show you

540
00:34:19,268 --> 00:34:22,910
how this looks like. So, back here in

541
00:34:22,980 --> 00:34:26,306
my main notebook, the first thing I'm also doing in this

542
00:34:26,328 --> 00:34:30,270
notebook here is to clone those code commit repos

543
00:34:30,350 --> 00:34:33,806
locally into my sagemaker studio environment.

544
00:34:33,998 --> 00:34:37,670
So this is what I'm running here for both code repos.

545
00:34:39,050 --> 00:34:42,370
I'm also removing one of the sample repos

546
00:34:42,450 --> 00:34:46,146
encode that are in here. And I'm

547
00:34:46,178 --> 00:34:50,122
triggering the first execution of the pipeline by

548
00:34:50,176 --> 00:34:54,086
actually copying over my sample pipeline that I've

549
00:34:54,118 --> 00:34:57,686
built before in the demo. So if I go to the file

550
00:34:57,718 --> 00:35:01,002
browser here on the left, you can see

551
00:35:01,056 --> 00:35:04,654
I've cloned down those model, build and model, deploy code,

552
00:35:04,692 --> 00:35:08,014
commit repos, and I can click into those.

553
00:35:08,132 --> 00:35:11,470
And I do have all of the needed code already set up

554
00:35:11,540 --> 00:35:15,026
all of the triggers in the AWS account. So what I

555
00:35:15,048 --> 00:35:18,626
can do here is in pipelines, I just need

556
00:35:18,648 --> 00:35:22,162
to add my own pipeline execution code that I want to run

557
00:35:22,296 --> 00:35:25,394
on the code change. So if I go in here,

558
00:35:25,432 --> 00:35:29,234
you will see I'm using the exact same Python scripts for pre

559
00:35:29,272 --> 00:35:32,898
processing, for model training, and for entrance that I've showed

560
00:35:32,914 --> 00:35:36,966
you before when I was manually building this pipelines. So especially

561
00:35:37,068 --> 00:35:41,066
the pipeline py file. If I open this one, this should

562
00:35:41,088 --> 00:35:44,938
look pretty familiar to you. Hopefully this contains exactly

563
00:35:45,024 --> 00:35:48,538
the code that I've been showing you before when I

564
00:35:48,544 --> 00:35:52,046
was building the pipeline in the notebook. The only difference

565
00:35:52,148 --> 00:35:55,582
here is that I'm now tuning this in the

566
00:35:55,636 --> 00:35:59,306
Python file. Programmatically, when I trigger the pipeline,

567
00:35:59,338 --> 00:36:03,102
run back to my notebook

568
00:36:03,166 --> 00:36:07,166
here. So what I've done is I've cloned

569
00:36:07,198 --> 00:36:10,914
the repos here, and I'm coding over my

570
00:36:10,952 --> 00:36:13,250
sample code into those repos,

571
00:36:14,230 --> 00:36:18,194
and then I'm committing them into the code commit.

572
00:36:18,322 --> 00:36:21,798
And you can see here it detected that I have removed some sample code.

573
00:36:21,964 --> 00:36:25,794
And I've also added my own pipeline

574
00:36:25,842 --> 00:36:30,170
code. So that should be enough changes hopefully, to the repo.

575
00:36:30,830 --> 00:36:34,060
I'm making sure I keep track of all the variables I'm using here.

576
00:36:35,070 --> 00:36:38,314
And again, here's the pipeline py file, which I just showed you,

577
00:36:38,352 --> 00:36:42,026
which contains the pipeline code to run. And this is

578
00:36:42,048 --> 00:36:43,760
exactly what I have set up before.

579
00:36:45,090 --> 00:36:48,810
And when I was doing the code commit

580
00:36:48,890 --> 00:36:52,446
and the code push to the repo, this set up all

581
00:36:52,468 --> 00:36:56,542
of the CI CD automation that the template set up for you in the AWS

582
00:36:56,606 --> 00:37:00,306
account and started the pipeline run. So if I

583
00:37:00,328 --> 00:37:04,450
go down here into my projects again and

584
00:37:04,520 --> 00:37:07,798
click on pipelines, I can actually see

585
00:37:07,964 --> 00:37:12,086
that here is a pipeline that got started and

586
00:37:12,108 --> 00:37:15,522
I can select it and it has a succeeded execution

587
00:37:15,586 --> 00:37:20,140
run already. So I've started this sometime before

588
00:37:20,750 --> 00:37:24,266
the session. All right, so let me show

589
00:37:24,288 --> 00:37:28,074
you how this automation works. So what happens

590
00:37:28,192 --> 00:37:32,446
is that projects integrate with the

591
00:37:32,468 --> 00:37:36,430
developer tools, for example with code pipelines and

592
00:37:36,500 --> 00:37:40,222
has this automation built in. So the first step, as you can see here,

593
00:37:40,356 --> 00:37:43,986
is creating the source code needed,

594
00:37:44,168 --> 00:37:48,462
and then it's going to build and start the pipelines execution

595
00:37:48,526 --> 00:37:51,762
run. And this is done with code

596
00:37:51,816 --> 00:37:55,410
build. So if I now jump into the build projects,

597
00:37:55,990 --> 00:37:59,042
you can see here that our build projects

598
00:37:59,106 --> 00:38:02,326
already in place. And the first one is

599
00:38:02,348 --> 00:38:05,894
the model training pipelines, which just succeeded. You can

600
00:38:05,932 --> 00:38:08,970
also see that the model deploy is currently in a failed state,

601
00:38:09,120 --> 00:38:12,666
and this is because it doesn't have an approved model yet

602
00:38:12,768 --> 00:38:16,154
to actually deploy. This template is

603
00:38:16,192 --> 00:38:18,902
also set up to have a two phase deployed.

604
00:38:19,046 --> 00:38:22,058
Phase one is deploying in a staging environment,

605
00:38:22,234 --> 00:38:26,062
which for example a data scientist could approve after

606
00:38:26,116 --> 00:38:29,374
evaluating the model. And then also it comes with a

607
00:38:29,412 --> 00:38:32,822
second stage to deploy into a production environment,

608
00:38:32,906 --> 00:38:35,966
which would most likely be another team to approve.

609
00:38:36,078 --> 00:38:39,934
For example, the integrations team, the DevOps team, the infrastructure

610
00:38:39,982 --> 00:38:43,602
teams. So I can click here into the code

611
00:38:43,656 --> 00:38:47,602
pipeline and I can see that my latest run here succeeded.

612
00:38:47,666 --> 00:38:51,506
The first one is the one I stopped and deployed from the sample

613
00:38:51,538 --> 00:38:54,918
code. So let's go back

614
00:38:55,004 --> 00:38:58,690
here to my environment.

615
00:38:58,850 --> 00:39:02,426
So what I need to do once the pipeline has

616
00:39:02,528 --> 00:39:05,514
executed, I can list the steps again here.

617
00:39:05,552 --> 00:39:09,066
I can see everything looks good. I can also list

618
00:39:09,088 --> 00:39:13,006
the artifacts again. And this looks familiar to

619
00:39:13,028 --> 00:39:15,950
the one I showed before. It's the exact same pipeline,

620
00:39:16,530 --> 00:39:19,280
all the artifacts that contributed to each steps.

621
00:39:19,810 --> 00:39:23,274
What I do have here now is a last step.

622
00:39:23,412 --> 00:39:27,294
That is an approval that is needed to actually deploy

623
00:39:27,342 --> 00:39:29,890
this model into the staging environment.

624
00:39:30,470 --> 00:39:34,754
And if you can remember in the previous demo, I showed you how to

625
00:39:34,872 --> 00:39:37,910
approve the model here through the studio UI.

626
00:39:38,730 --> 00:39:42,262
What I can do now here as well is to approve it

627
00:39:42,316 --> 00:39:45,698
through the API programmatically. And this is what I'm

628
00:39:45,714 --> 00:39:49,370
going to do here. So, in here, I'm looking for

629
00:39:49,520 --> 00:39:53,350
the executions and I'm grabbing

630
00:39:53,430 --> 00:39:56,986
the model package arn where we registered the

631
00:39:57,008 --> 00:40:00,226
model. And then I'm going to update

632
00:40:00,278 --> 00:40:04,010
the model package here and approve it for deployment

633
00:40:04,090 --> 00:40:07,870
into this first stage, which is the staging environment.

634
00:40:08,450 --> 00:40:12,314
So here I'm going to update the model package. I'm setting the

635
00:40:12,452 --> 00:40:15,794
status to approved, and then I can check

636
00:40:15,832 --> 00:40:19,086
here for the model name, and we'll

637
00:40:19,118 --> 00:40:22,894
see that the model starts to get deployed into the staging

638
00:40:22,942 --> 00:40:26,342
environment. Let's see the

639
00:40:26,396 --> 00:40:30,342
deployed pipeline. So what happens here is

640
00:40:30,476 --> 00:40:33,538
once I've approved the model for staging,

641
00:40:33,714 --> 00:40:38,102
it actually started the second pipelines

642
00:40:38,166 --> 00:40:41,050
here, which is the model deploy.

643
00:40:41,390 --> 00:40:44,966
And you can see here it started building the source, and it's

644
00:40:44,998 --> 00:40:49,770
currently in progress deploying the model into the defined staging environment.

645
00:40:50,610 --> 00:40:54,510
I can also have a look here. So I'm looking

646
00:40:54,580 --> 00:40:58,318
at the endpoint that this pipelines will set up.

647
00:40:58,484 --> 00:41:00,160
So if I click here,

648
00:41:01,970 --> 00:41:05,714
I will see now that here is an endpoint being created on

649
00:41:05,752 --> 00:41:09,666
Sagemaker for the staging environment. So this

650
00:41:09,688 --> 00:41:13,138
will take a few minutes for the endpoint to be

651
00:41:13,224 --> 00:41:16,934
ready. All right, the endpoint is

652
00:41:16,972 --> 00:41:20,998
now in service. And if I click in here, I can see

653
00:41:21,164 --> 00:41:24,630
the rest API I could call to get

654
00:41:24,700 --> 00:41:28,294
predictions from my model. Now, let's check

655
00:41:28,332 --> 00:41:32,154
this. In the notebook here, you can see the endpoint is in

656
00:41:32,192 --> 00:41:35,658
service. And what I do here is I pull in

657
00:41:35,664 --> 00:41:39,100
a Tensorflow predictor object, which I can create,

658
00:41:39,550 --> 00:41:43,414
and then I'm going to pass in some sample reviews.

659
00:41:43,462 --> 00:41:46,574
Let's say this is great, and I can run this,

660
00:41:46,692 --> 00:41:50,670
pass it to the predictor, and you can see I get a prediction result

661
00:41:50,740 --> 00:41:53,454
back from my model deployed in the staging environment.

662
00:41:53,582 --> 00:41:57,490
Predicting this is a five star rating. Let's have a look at

663
00:41:57,560 --> 00:42:00,210
the code pipeline that we executed.

664
00:42:02,230 --> 00:42:06,466
So you can see here that the staging

665
00:42:06,658 --> 00:42:10,514
succeeded. But there is one more approval

666
00:42:10,562 --> 00:42:14,166
needed here for the actual deployment into a

667
00:42:14,188 --> 00:42:17,350
production environment. So this could really be something

668
00:42:17,500 --> 00:42:21,174
that another team handles. So if I'm the DevOps

669
00:42:21,222 --> 00:42:24,826
engineer, the integration engineer, I could make sure I'm running all

670
00:42:24,848 --> 00:42:28,458
of the tests that I need with this model. Now that is hosted in the

671
00:42:28,464 --> 00:42:32,046
staging environment. And if I agree

672
00:42:32,148 --> 00:42:35,774
that it's good to be deployed into production, I could

673
00:42:35,812 --> 00:42:39,790
either use here the code pipelines to

674
00:42:39,860 --> 00:42:43,758
approve the model and deploy to production,

675
00:42:43,934 --> 00:42:47,170
or I can also obviously do this programmatically,

676
00:42:47,670 --> 00:42:51,826
which is what I'm doing here now in the notebook. So again,

677
00:42:52,008 --> 00:42:55,926
I review the pipelines and what I'm doing here,

678
00:42:56,028 --> 00:42:59,410
exact same thing is that I'm programmatically approving

679
00:42:59,490 --> 00:43:02,994
this for deployment in production.

680
00:43:03,122 --> 00:43:06,582
You can see this succeeded. Let's actually check our

681
00:43:06,636 --> 00:43:09,866
pipelines and there we go.

682
00:43:09,888 --> 00:43:13,626
You can see it took the approval and is currently now

683
00:43:13,728 --> 00:43:17,014
working on deploying this model into the production

684
00:43:17,062 --> 00:43:20,698
environment. As pointed out earlier,

685
00:43:20,794 --> 00:43:24,254
the ultimate goal is to build AI ML solutions that are

686
00:43:24,292 --> 00:43:27,194
secure, resilient, cost optimized,

687
00:43:27,322 --> 00:43:30,190
performant and operationally efficient.

688
00:43:31,010 --> 00:43:34,946
So in addition to the operational excellence which we discussed in

689
00:43:34,968 --> 00:43:38,738
the context of mlops, you also need to incorporate standard

690
00:43:38,824 --> 00:43:41,170
practices in each of these areas.

691
00:43:42,870 --> 00:43:46,334
Here are a few links to get you started. First,

692
00:43:46,392 --> 00:43:49,974
a link to the data science on AWS resources and

693
00:43:50,012 --> 00:43:53,750
the GitHub repo which contains all of the code samples I've showed.

694
00:43:54,570 --> 00:43:58,182
Also here are links to the Amazon Sagemaker pipelines

695
00:43:58,326 --> 00:43:59,900
and the great blog post.

696
00:44:01,470 --> 00:44:05,926
Again, if you are looking for more comprehensive learning resources,

697
00:44:06,118 --> 00:44:10,490
check out the O'Reilly book data Science on AWS, which covers

698
00:44:10,570 --> 00:44:14,586
how to implement endtoend, continuous AI and machine learning pipelines

699
00:44:14,698 --> 00:44:18,922
in over twelve chapters, 500 pages, and hundreds of additional

700
00:44:18,986 --> 00:44:22,622
code samples. Another great

701
00:44:22,676 --> 00:44:26,202
training resource is our newly launched practical data science

702
00:44:26,266 --> 00:44:30,830
specialization in partnership with deep learning AI and Coursera.

703
00:44:31,650 --> 00:44:34,978
This three course specialization teaches you practicals

704
00:44:35,034 --> 00:44:38,770
skills in how to take your data science and machine learning projects

705
00:44:38,850 --> 00:44:42,646
from idea to production using purpose built tools in the

706
00:44:42,668 --> 00:44:46,006
AWS cloud. This also includes on

707
00:44:46,028 --> 00:44:48,870
demand, hands on labs for you to practice.

708
00:44:50,250 --> 00:44:52,820
This concludes the session. Thanks for watching.

