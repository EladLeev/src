1
00:00:39,730 --> 00:00:43,526
Everyone, welcome to DevOps 2024. My name

2
00:00:43,548 --> 00:00:46,854
is Indika Wimalasuriya. Today I am going to walk you through

3
00:00:46,972 --> 00:00:50,474
about how you can maximize the speed and reduce

4
00:00:50,522 --> 00:00:54,014
cost and increase the user experience.

5
00:00:54,212 --> 00:00:57,758
Leveraging AWS elastic cache serverless so

6
00:00:57,844 --> 00:01:01,598
AWS elastic cache serverless was one of the main service

7
00:01:01,684 --> 00:01:05,854
offerings AWS has released as part of Reinvent

8
00:01:05,902 --> 00:01:09,726
2023 a couple of weeks back. It is a grace feature.

9
00:01:09,838 --> 00:01:14,434
I'm pretty sure that all of you probably are aware of the

10
00:01:14,472 --> 00:01:18,290
elastic cache which was initially provided as cache

11
00:01:18,370 --> 00:01:21,698
and redis cache. But now with elastic cache serverless

12
00:01:21,794 --> 00:01:26,070
it's offer the benefits, especially the kind of benefits

13
00:01:26,220 --> 00:01:30,330
which you can obtain by using serverless capabilities.

14
00:01:30,910 --> 00:01:34,426
As part of my presentation I will work

15
00:01:34,448 --> 00:01:38,422
you through importance of performance. So I'm pretty sure that everyone is aware,

16
00:01:38,486 --> 00:01:41,742
but we'll quick touch upon that why it is important

17
00:01:41,876 --> 00:01:45,210
what it mean for the modern businesses

18
00:01:45,370 --> 00:01:48,974
and then we'll dive on. This problem is not

19
00:01:49,012 --> 00:01:52,510
something new. This has been there for

20
00:01:52,660 --> 00:01:56,286
few decades even and we have found some solutions.

21
00:01:56,398 --> 00:02:00,334
And one of the solution is obviously caching. So caching

22
00:02:00,382 --> 00:02:03,778
have its own pros and cons and we'll go through

23
00:02:03,864 --> 00:02:07,606
some of the challenges, the typical, the standard or

24
00:02:07,628 --> 00:02:11,186
the traditional caching is having, and then we'll

25
00:02:11,218 --> 00:02:15,202
dive on to our key topic, what is elastic cache serverless,

26
00:02:15,346 --> 00:02:19,398
how it is working and then what are the capabilities it's providing.

27
00:02:19,574 --> 00:02:22,902
Then we will move into implementing overview.

28
00:02:23,046 --> 00:02:26,474
I promise you its implementation is very

29
00:02:26,592 --> 00:02:30,842
very quickly, you can do it in very

30
00:02:30,896 --> 00:02:34,474
fast matter of minutes and that is what AWS

31
00:02:34,522 --> 00:02:37,866
is promising as well. So that is again a great feature.

32
00:02:38,058 --> 00:02:41,470
And then finally we'll wrap this up

33
00:02:41,540 --> 00:02:45,074
with some of the anti patterns and the best practices I would

34
00:02:45,112 --> 00:02:48,450
think you should follow when you are starting this journey.

35
00:02:48,870 --> 00:02:52,498
So moving on, I think you all understand the

36
00:02:52,504 --> 00:02:56,150
importance of performance. So today's world, the business is

37
00:02:56,220 --> 00:03:00,518
very competitive. So every industry the competition is very high.

38
00:03:00,684 --> 00:03:04,582
So when our customers comes to our systems and they are using our

39
00:03:04,636 --> 00:03:08,070
systems, performance is key. Example, if a customer

40
00:03:08,140 --> 00:03:11,574
is going through order journey and customer sees a sluggish

41
00:03:11,622 --> 00:03:15,530
performance, it's matter of seconds where our customer

42
00:03:15,600 --> 00:03:18,522
just give up and move into a competitor side.

43
00:03:18,656 --> 00:03:22,410
So the performance is very important and it's directly correlated

44
00:03:22,490 --> 00:03:26,190
with your revenue. So if you have a sluggish system,

45
00:03:26,340 --> 00:03:30,174
you can't expect high revenue because your customers will jump the ship and

46
00:03:30,212 --> 00:03:34,434
go into the competitors very quickly because there

47
00:03:34,632 --> 00:03:38,754
might be someone who is offering a better, faster service.

48
00:03:38,872 --> 00:03:42,820
So speed is key. In today's world, speed is everything.

49
00:03:43,190 --> 00:03:47,206
So if you look at some of the research findings we have

50
00:03:47,388 --> 00:03:51,094
related to the performance and how it correlates with the

51
00:03:51,132 --> 00:03:54,838
revenue. One of the famous research

52
00:03:54,924 --> 00:03:58,440
items or the top research

53
00:03:59,610 --> 00:04:02,578
results which we have is the Walmart.

54
00:04:02,674 --> 00:04:06,730
So sometime back, Walmart has found that every 1 second

55
00:04:06,800 --> 00:04:10,614
improvement they do in their page loading time, their conversation

56
00:04:10,662 --> 00:04:13,822
rate increased by 2%. So that is remarkable, right?

57
00:04:13,876 --> 00:04:17,102
So you do a 1 second improvement and that is

58
00:04:17,156 --> 00:04:20,814
getting correlated with revenue. And Cook finds something very

59
00:04:20,852 --> 00:04:24,974
similar, so that they understood that they are able to increase their

60
00:04:25,172 --> 00:04:28,526
conversion rate by 7% by reducing paid load

61
00:04:28,558 --> 00:04:31,620
time to zero point 85. Very specific.

62
00:04:31,990 --> 00:04:35,686
Mobify found very similar statistics as

63
00:04:35,708 --> 00:04:39,494
well. A study done by Hubford found that even few

64
00:04:39,532 --> 00:04:43,282
milliseconds can have a greater

65
00:04:43,426 --> 00:04:47,570
end user impact and conversion rate and ultimately revenue.

66
00:04:47,650 --> 00:04:51,222
So you should understand by now that performance is

67
00:04:51,276 --> 00:04:55,110
key and it's directly correlated with revenue.

68
00:04:55,190 --> 00:04:58,602
It's not about just operations and we are doing a good job

69
00:04:58,656 --> 00:05:02,266
or customer experience. At the end of the day, everyone is here to make

70
00:05:02,288 --> 00:05:06,494
some bucks and then revenue is important. So if you are focusing on revenue and

71
00:05:06,532 --> 00:05:09,646
if you want to make your systems speed, then you'll have to

72
00:05:09,668 --> 00:05:13,550
think about ways and approaches

73
00:05:13,630 --> 00:05:17,300
to give that customers that speed they are looking for.

74
00:05:17,670 --> 00:05:21,042
So moving on, as I said when I starting this

75
00:05:21,096 --> 00:05:24,594
presentation, this is not a new problem

76
00:05:24,792 --> 00:05:28,470
industry. We all knew that we have a problem,

77
00:05:28,620 --> 00:05:33,014
we have to speed up our systems, we have to ensure our customers are getting

78
00:05:33,212 --> 00:05:35,958
what we have designed for them.

79
00:05:36,124 --> 00:05:39,400
So when we went through this problem,

80
00:05:39,770 --> 00:05:44,250
we identified caching is one of the main

81
00:05:44,320 --> 00:05:47,866
solution which we can give. So typically if

82
00:05:47,888 --> 00:05:51,454
you have a system and if you identify that there are some

83
00:05:51,492 --> 00:05:55,386
data which are getting repeatedly accessed by end users,

84
00:05:55,498 --> 00:05:58,942
and those data are generally not getting

85
00:05:58,996 --> 00:06:02,426
changed for a certain time, then we call them frequently accessed

86
00:06:02,458 --> 00:06:05,874
data. And it makes sense to put

87
00:06:05,912 --> 00:06:09,278
this frequently accessed data in a particular layer.

88
00:06:09,374 --> 00:06:13,234
Instead of every time this data is being requested, go to

89
00:06:13,272 --> 00:06:16,638
database, make a database call and then run a query

90
00:06:16,734 --> 00:06:20,454
and then get the data and then travel around the network and then

91
00:06:20,492 --> 00:06:23,926
send that data to the end users. So we can cut down lot of

92
00:06:23,948 --> 00:06:27,606
database processing time and transactional round trip time

93
00:06:27,708 --> 00:06:30,934
by caching this frequently accessed data in a caching

94
00:06:30,982 --> 00:06:34,458
layer. So primary advantages of this design

95
00:06:34,544 --> 00:06:37,398
approach is it's definitely provide low latency,

96
00:06:37,494 --> 00:06:41,920
it's real time responses are much faster, we are completely

97
00:06:42,530 --> 00:06:46,830
eliminating the database overhead and then it's high throughput.

98
00:06:50,130 --> 00:06:54,002
We are able to support large number

99
00:06:54,056 --> 00:06:57,298
of data and this data,

100
00:06:57,384 --> 00:07:00,686
we are able to put that in our cache system cache layers.

101
00:07:00,798 --> 00:07:04,334
So this is again a very high throughput design approach.

102
00:07:04,462 --> 00:07:08,098
And since this is go beyond the database layer,

103
00:07:08,194 --> 00:07:12,274
we have more focus, more opportunity to focusly

104
00:07:12,402 --> 00:07:15,830
scale this layer. So in summary,

105
00:07:16,650 --> 00:07:19,706
caching, frequently accessed data is allowing us

106
00:07:19,728 --> 00:07:23,510
to achieve low latency, high throughput and high scalability.

107
00:07:23,670 --> 00:07:27,894
And generally for last several

108
00:07:27,942 --> 00:07:32,410
years, from the time where this design pattern was identified,

109
00:07:32,570 --> 00:07:36,286
we have been using this caching solution. So some

110
00:07:36,308 --> 00:07:40,314
of the top user cases, like some of the top users cases

111
00:07:40,362 --> 00:07:44,286
like systems depend on real time analytics, financial trading

112
00:07:44,318 --> 00:07:48,606
systems, online transaction processing, recommendation endings,

113
00:07:48,718 --> 00:07:52,434
leaderboards, IoT data processing and so much

114
00:07:52,472 --> 00:07:56,420
of high value use cases are using

115
00:07:57,670 --> 00:08:00,962
the solution of caching because it's provide

116
00:08:01,016 --> 00:08:04,662
us a great benefit and it's able to cut

117
00:08:04,716 --> 00:08:07,830
down the overhead and increase our performance.

118
00:08:09,230 --> 00:08:13,574
So with this, so AWS as a cloud provider,

119
00:08:13,702 --> 00:08:18,170
it has its own mechanism for

120
00:08:18,240 --> 00:08:21,498
offering how to implement cache solutions.

121
00:08:21,594 --> 00:08:25,994
So three of the main famous AWS cache

122
00:08:26,042 --> 00:08:28,858
solutions are elastic cache for memcache,

123
00:08:28,954 --> 00:08:32,138
elastic cache for redis and memorydb for redis.

124
00:08:32,234 --> 00:08:35,666
So these are three of the main offering AWS is

125
00:08:35,688 --> 00:08:38,946
offering with having lot of capabilities and it will

126
00:08:38,968 --> 00:08:42,690
match lot of use cases and it will match lot of your needs.

127
00:08:42,840 --> 00:08:46,690
So example, elastic cache for memcache is very simple,

128
00:08:46,840 --> 00:08:50,134
non persistent caching layer and

129
00:08:50,172 --> 00:08:53,766
elastic cache for redis. It's a persistence and

130
00:08:53,788 --> 00:08:57,266
it has the replication and it has more wide range

131
00:08:57,298 --> 00:09:01,258
of capabilities than the memcache. So it's more of

132
00:09:01,264 --> 00:09:04,646
like enterprise level I would say. And then memory DB

133
00:09:04,678 --> 00:09:08,714
for redis, it's optimized and provide ultra low sub

134
00:09:08,752 --> 00:09:12,634
millisecond latency. So if you are requiring that kind

135
00:09:12,672 --> 00:09:16,014
of performance, then you can go with memory DB for

136
00:09:16,052 --> 00:09:19,742
redis. So I have put a table comparing these

137
00:09:19,796 --> 00:09:23,170
three, the opportunities, the services the

138
00:09:23,240 --> 00:09:27,582
AWS is performing. So if you can see some of the capabilities,

139
00:09:27,646 --> 00:09:31,394
depends on the use cases. And then

140
00:09:31,512 --> 00:09:35,214
example, elastic cache, it's providing caching session

141
00:09:35,262 --> 00:09:38,886
storage, elastic cache for redis. On top of that

142
00:09:39,068 --> 00:09:42,742
we are able to do some queries, leaderboards, transition data

143
00:09:42,876 --> 00:09:46,486
and memory DB for redis. On top of everything it's able

144
00:09:46,508 --> 00:09:50,422
to do real time maps and ultra performance. So typically

145
00:09:50,566 --> 00:09:54,234
great use cases. And we can

146
00:09:54,272 --> 00:09:57,674
also base on our needs like whether need multi ac support,

147
00:09:57,792 --> 00:10:01,354
whether we need replicas, durability, data persistence,

148
00:10:01,482 --> 00:10:04,826
the backups and the automatic failovers,

149
00:10:04,938 --> 00:10:09,130
sharding and architecture security monitoring.

150
00:10:09,210 --> 00:10:12,654
There are a lot of capabilities AWS is offering and

151
00:10:12,692 --> 00:10:16,338
based on lot of your design,

152
00:10:16,504 --> 00:10:20,194
what is your design is calling for?

153
00:10:20,312 --> 00:10:23,746
You are able to pick one of this and then that will give you a

154
00:10:23,768 --> 00:10:27,480
better solution and better end user product for your customers.

155
00:10:28,090 --> 00:10:31,960
So this has been, we have been using this for

156
00:10:32,650 --> 00:10:36,182
the time when AWS has launched these services and these are very

157
00:10:36,236 --> 00:10:40,586
famous things which every design is using and

158
00:10:40,688 --> 00:10:45,370
every enterprise level system is leveraging.

159
00:10:45,870 --> 00:10:49,382
So moving on. There are some challenges

160
00:10:49,526 --> 00:10:53,050
in serverless based memory cache implementations

161
00:10:53,210 --> 00:10:56,442
and one of the challenges being managing capacity.

162
00:10:56,586 --> 00:11:00,286
So capacity is a very challenging thing, right? Because this

163
00:11:00,308 --> 00:11:03,662
is server based. When we are provisioning, when we are coming

164
00:11:03,716 --> 00:11:07,266
up with our cluster, either memcache or Redis, we had to

165
00:11:07,288 --> 00:11:10,398
identify the capacity. So that is again challenging.

166
00:11:10,494 --> 00:11:14,238
So whenever someone asks you to do a capacity assessment

167
00:11:14,334 --> 00:11:18,466
and come up with a capacity plan, it has lot of unknown

168
00:11:18,498 --> 00:11:22,246
variables which can go against you. So capacity is

169
00:11:22,348 --> 00:11:26,178
a very challenging thing. So I'll be talking through capacity

170
00:11:26,274 --> 00:11:29,498
in another one or two slides. So you have to

171
00:11:29,504 --> 00:11:33,446
be remembered that it's not easy. Second one is scaling

172
00:11:33,478 --> 00:11:37,594
complexities. Even though this memcache and

173
00:11:37,632 --> 00:11:40,470
the redis is offering great flexibility,

174
00:11:40,630 --> 00:11:43,946
the scaling is a challenge. Scaling require

175
00:11:43,978 --> 00:11:47,418
whether you have to decide based on which metrics I'm

176
00:11:47,434 --> 00:11:51,038
going to do the scaling, whether it's cpu, whether it's going to be something else

177
00:11:51,124 --> 00:11:54,786
and that require you some overhead effort to

178
00:11:54,888 --> 00:11:58,354
make some decisions. And making decision is not easy

179
00:11:58,472 --> 00:12:01,806
and that require a lot of input variables.

180
00:12:01,918 --> 00:12:05,826
So again scaling is complex when you are doing

181
00:12:05,848 --> 00:12:10,046
it in a server based memory cache implementation and there's

182
00:12:10,078 --> 00:12:12,998
lot of operations law where you will have to take care of security, you will

183
00:12:13,004 --> 00:12:16,658
have to take care of backups, you'll have to take care of the maintenance perspective.

184
00:12:16,754 --> 00:12:20,250
So that's again a lot of overhead and

185
00:12:20,320 --> 00:12:24,234
high availability is a challenge, right? So the

186
00:12:24,272 --> 00:12:27,626
server base means that you have control of your selecting your

187
00:12:27,648 --> 00:12:31,626
availability zones and you have to ability to go into regions.

188
00:12:31,738 --> 00:12:35,470
But the challenge is that still there are some degree of availability,

189
00:12:36,130 --> 00:12:39,886
the challenges which you have and certain things like

190
00:12:39,908 --> 00:12:43,890
predict implementation, you'll have to put a bit of effort to ensure you

191
00:12:43,960 --> 00:12:47,700
achieve that availability and then cost.

192
00:12:48,710 --> 00:12:51,922
While this solution seems very nice and

193
00:12:51,976 --> 00:12:55,734
it's a very good and decent, it involves a lot of cost. You will

194
00:12:55,772 --> 00:12:59,186
have to manage a separate cluster and then separate cluster

195
00:12:59,218 --> 00:13:04,386
will have to have some high performance hardware

196
00:13:04,578 --> 00:13:08,082
and some great software you have to install and

197
00:13:08,236 --> 00:13:11,962
fine tune and operate. So this is again naturally means

198
00:13:12,016 --> 00:13:15,180
there's lot of overhead and then obviously

199
00:13:15,550 --> 00:13:18,858
when you say cost, there's infrastructure you have to manage

200
00:13:18,944 --> 00:13:24,640
and this infrastructure is costly as well that again you'll have to support

201
00:13:25,090 --> 00:13:28,286
and because we are

202
00:13:28,308 --> 00:13:32,510
bringing in a caching layer in front, there will be lot of caching,

203
00:13:34,450 --> 00:13:37,938
I would say designs which we can use.

204
00:13:38,024 --> 00:13:41,966
So this again slow down your development side. Now when you are doing development,

205
00:13:42,078 --> 00:13:45,718
you will have to be mindful that there's a caching layer on top of

206
00:13:45,804 --> 00:13:50,246
the database and then what data to cache and

207
00:13:50,428 --> 00:13:53,586
what are our cache designs patterns we are going to leverage.

208
00:13:53,698 --> 00:13:57,362
So that involves quite a lot of planning,

209
00:13:57,426 --> 00:14:00,794
preparation and designing and implementing as well. So that

210
00:14:00,832 --> 00:14:04,714
will definitely slow down your development activities. So these are

211
00:14:04,752 --> 00:14:09,098
some of the key challenges in the standard server based

212
00:14:09,184 --> 00:14:12,814
memory cache implementing. So if I have

213
00:14:12,852 --> 00:14:16,270
to select one or two, I would see capacity is the key.

214
00:14:16,340 --> 00:14:20,222
So it's not easy to identify capacity and then this

215
00:14:20,276 --> 00:14:23,774
capacity directly correlate with our cost.

216
00:14:23,972 --> 00:14:28,082
Because end of the day, first slide we discussed, speed is everything

217
00:14:28,216 --> 00:14:31,742
because we want to make our systems more speedy.

218
00:14:31,806 --> 00:14:35,646
So end users are happy, they are able to do the conversions that

219
00:14:35,688 --> 00:14:38,962
generate lot of money and then it's about revenue,

220
00:14:39,026 --> 00:14:43,122
right? So we don't want to unnecessarily spend. So capacity

221
00:14:43,186 --> 00:14:47,174
is key here. So how we manage capacity is what

222
00:14:47,212 --> 00:14:51,050
the advantage or the benefit serverless is offering.

223
00:14:51,470 --> 00:14:55,254
So when you come to capacity, so what you will normally

224
00:14:55,302 --> 00:14:59,338
do is you will look at all your future forecast and you

225
00:14:59,344 --> 00:15:03,242
will do kind of like do assessment, then you will baseline

226
00:15:03,306 --> 00:15:07,086
and you will do a provision capacity. You will say this is what

227
00:15:07,108 --> 00:15:10,894
I want, right? This is my kind of like the

228
00:15:10,932 --> 00:15:14,626
upper capacity limits. So if you have ever done

229
00:15:14,648 --> 00:15:18,238
a capacity planning and actually done a production implementing,

230
00:15:18,414 --> 00:15:21,794
you will be in two groups. One group is that you

231
00:15:21,832 --> 00:15:25,526
always do an over provision and then there

232
00:15:25,548 --> 00:15:28,886
will be extra cost, right. I request for

233
00:15:29,068 --> 00:15:32,582
certain capacity by my systems are not hitting this

234
00:15:32,716 --> 00:15:36,454
area, so I'm continuously using

235
00:15:36,572 --> 00:15:40,678
less. So that is good. That means that our end user's

236
00:15:40,694 --> 00:15:44,266
perspective, there are no performance issues, they are apps.

237
00:15:44,448 --> 00:15:47,754
But we are bleeding unnecessary cost

238
00:15:47,872 --> 00:15:51,306
because we are paying extra because it's our provision,

239
00:15:51,418 --> 00:15:54,442
right. Other side of the coin is under provision.

240
00:15:54,506 --> 00:15:57,886
So if we think that we'll have to

241
00:15:57,988 --> 00:16:01,610
ensure that we don't over provision and we do a provisioning,

242
00:16:01,690 --> 00:16:05,358
but if we have some sort of spikes like as you see in this graph,

243
00:16:05,454 --> 00:16:09,582
there are several spikes where capacity need has gone beyond

244
00:16:09,646 --> 00:16:13,586
our provisioned capacity limit. This is literally called

245
00:16:13,688 --> 00:16:16,978
under provisioning. Every time there's a peak there's

246
00:16:16,994 --> 00:16:20,166
a customer impact because our systems are not able to

247
00:16:20,188 --> 00:16:23,782
serve that customer request because we have hit the

248
00:16:23,836 --> 00:16:27,298
maximum capacity. So this is a very

249
00:16:27,404 --> 00:16:31,306
challenging thing to provide the

250
00:16:31,408 --> 00:16:34,822
solution because either way you will under provision

251
00:16:34,886 --> 00:16:38,794
or over provision with

252
00:16:38,832 --> 00:16:42,494
repetitive things and you can come up with some strategies, but still it's very

253
00:16:42,532 --> 00:16:46,446
challenges and it's a big overhead and that is where I

254
00:16:46,468 --> 00:16:50,142
believe serverless caching will be the next big thing

255
00:16:50,196 --> 00:16:53,994
in industry. So moving on. Amazon elastic

256
00:16:54,042 --> 00:16:57,746
cache service is having a great set of features. So if

257
00:16:57,768 --> 00:17:01,106
I go through a few of them, you are able to create a cache in

258
00:17:01,128 --> 00:17:04,654
1 minute. So if you have created a cache or redis

259
00:17:04,702 --> 00:17:08,566
cache, you know, there are a lot of design aspects, lot of

260
00:17:08,668 --> 00:17:12,306
decisions you have to take, lot of capacity related decisions

261
00:17:12,338 --> 00:17:16,182
you have to take. But now here burden is AWS, it's not

262
00:17:16,236 --> 00:17:19,706
yours. By simply doing a couple of clicks you

263
00:17:19,728 --> 00:17:22,954
can bring up either redis or memcache under a

264
00:17:22,992 --> 00:17:26,170
minute. And that is a promise AWS is keeping.

265
00:17:26,510 --> 00:17:29,754
And you don't have to do the capacity management. So you just

266
00:17:29,792 --> 00:17:33,562
provision and then you start using it. And AWS is

267
00:17:33,616 --> 00:17:37,034
take care of it so that there's no question of over provision

268
00:17:37,082 --> 00:17:40,446
or under provision. So you are using what you are using and

269
00:17:40,468 --> 00:17:43,474
you are getting paid what you are getting what you are using.

270
00:17:43,592 --> 00:17:47,554
So that is a hustle free and it takes lot

271
00:17:47,592 --> 00:17:51,918
of overhead out from us. AWS is guaranteed 17

272
00:17:52,014 --> 00:17:56,594
700 microsecond at p 51.3

273
00:17:56,632 --> 00:18:00,690
millisecond and p 99. Those are great performance

274
00:18:00,770 --> 00:18:04,614
stats. Like most of our systems are

275
00:18:04,652 --> 00:18:07,510
happy to operate at those levels.

276
00:18:08,010 --> 00:18:11,974
And AWS is offering five terabyte of storage

277
00:18:12,022 --> 00:18:15,446
so you can have five terabyte of data cache

278
00:18:15,478 --> 00:18:18,986
in your system. And one of the beauty is this

279
00:18:19,008 --> 00:18:22,362
is paper use and this is nothing new. So when you say serverless you

280
00:18:22,416 --> 00:18:25,726
understand it's paper, we only pay what you are using,

281
00:18:25,828 --> 00:18:29,246
so you just don't have to worry. So this is some of

282
00:18:29,268 --> 00:18:32,958
the great benefit the serverless are coming

283
00:18:33,124 --> 00:18:36,482
and AWS is guaranteeing 99.99

284
00:18:36,536 --> 00:18:40,142
availability. So that is something very decent

285
00:18:40,206 --> 00:18:43,746
and that is something even sometimes challenges for us to achieve in

286
00:18:43,768 --> 00:18:47,730
our own implementations. And one of the other major

287
00:18:47,800 --> 00:18:51,586
thing is it provides single endpoint experience, so we don't

288
00:18:51,698 --> 00:18:55,654
have to worry about multiple endpoints. Just by using one

289
00:18:55,692 --> 00:18:59,574
endpoint we are able to plugging in and start using it. And it

290
00:18:59,612 --> 00:19:03,162
also provides lot of compliance as well. So that is also taken

291
00:19:03,216 --> 00:19:07,222
care of. So in summary, these are some of the great features

292
00:19:07,286 --> 00:19:10,606
AWS elastic cache serverless is offering. So this

293
00:19:10,628 --> 00:19:14,026
is allowing you to do rapid development, rapid deployment

294
00:19:14,138 --> 00:19:18,334
and rapid maintenance. And your application is live and

295
00:19:18,372 --> 00:19:21,822
using cache. So one question

296
00:19:21,876 --> 00:19:25,442
is when it comes to AWS services and serverless is how is the cost?

297
00:19:25,576 --> 00:19:29,134
Cost is again AWS is very transparent as usual

298
00:19:29,182 --> 00:19:33,300
with their cost. So here they are introducing two new,

299
00:19:34,310 --> 00:19:38,162
it's actually one new pricing unit,

300
00:19:38,306 --> 00:19:41,766
something called elastic cache processing unit. And before that we have

301
00:19:41,788 --> 00:19:45,330
this data taught so you are getting charged

302
00:19:45,410 --> 00:19:49,100
for how much data you are storing. And at the time of

303
00:19:50,190 --> 00:19:54,346
this presentation for onegb hour they

304
00:19:54,368 --> 00:19:58,090
are charging around zero point 125 us dollars.

305
00:19:58,240 --> 00:20:01,866
And then they have come up with something called elastic cache processing

306
00:20:01,898 --> 00:20:05,486
units ecpus that is for you pay for

307
00:20:05,508 --> 00:20:09,626
the request in ecpus. So which covers VCPU

308
00:20:09,658 --> 00:20:13,698
time and data transfer. So each read or write

309
00:20:13,864 --> 00:20:17,486
consume one ecpu per kilobyte.

310
00:20:17,678 --> 00:20:21,902
Additionally VCPU time or data transfer over one kb

311
00:20:22,046 --> 00:20:24,766
scale ecpus proportionally.

312
00:20:24,878 --> 00:20:28,678
So that is again a very good way

313
00:20:28,844 --> 00:20:33,058
which will ensure that it's aligned with this paper use approach.

314
00:20:33,154 --> 00:20:36,514
So what you have to be mindful is again this is based on region.

315
00:20:36,562 --> 00:20:40,186
So here I am looking at us east. So be

316
00:20:40,208 --> 00:20:43,978
mindful. So when you are going with the implementing be mindful of this data.

317
00:20:44,064 --> 00:20:47,580
So that is how AWS is going to charge you.

318
00:20:48,110 --> 00:20:51,258
So moving on. I think this is the important part.

319
00:20:51,344 --> 00:20:54,846
Now let's see that how we can do an implementation. So all

320
00:20:54,868 --> 00:20:59,082
you have to do is go to AWS console, search for elastic cache

321
00:20:59,146 --> 00:21:02,674
and then you will come to the dashboard. From here you can

322
00:21:02,712 --> 00:21:07,060
select cache and then

323
00:21:08,310 --> 00:21:11,474
you are just starting your journey. So once

324
00:21:11,512 --> 00:21:15,106
you go to journeys you will have this view. So this

325
00:21:15,128 --> 00:21:18,354
is all the information you have to provide, right? So it's initially

326
00:21:18,402 --> 00:21:22,946
asking whether it's serverless or whether you want to design your own cache.

327
00:21:22,978 --> 00:21:26,742
But it's serverless which we are going to test here. So we will select

328
00:21:26,796 --> 00:21:30,026
serverless. You will have to give a name and if required you

329
00:21:30,048 --> 00:21:33,466
can do some changes into default setting, but otherwise you can

330
00:21:33,488 --> 00:21:37,114
make a create and under minute AWS will

331
00:21:37,152 --> 00:21:39,686
create your elastic memcache.

332
00:21:39,878 --> 00:21:43,280
So like this, right? So you can see it will come up here.

333
00:21:43,810 --> 00:21:47,402
So once that cache is available it's

334
00:21:47,466 --> 00:21:51,040
as business as usual. If you are someone who have used

335
00:21:51,410 --> 00:21:54,766
Memcache or Redis previously, then it's a matter of

336
00:21:54,788 --> 00:21:58,138
we are taking the endpoint and plug it with your application, then you

337
00:21:58,164 --> 00:22:01,026
start using. So you are it, right? You don't have to wait a lot of

338
00:22:01,048 --> 00:22:04,494
time and there's no overhead, there's no about fine tuning,

339
00:22:04,542 --> 00:22:08,326
there's no about thinking about capacity, it's just

340
00:22:08,508 --> 00:22:12,374
starting using it and that is about serverless. Serverless is

341
00:22:12,572 --> 00:22:16,566
AWS will take care of the infrastructure and everything and you clear about

342
00:22:16,668 --> 00:22:20,938
actual implementation and development work. So here

343
00:22:21,104 --> 00:22:24,918
you can use some of the easiest command like connect with OpenSSL client

344
00:22:25,014 --> 00:22:28,794
and then you can give some set and set a

345
00:22:28,832 --> 00:22:32,318
standard set of variable in

346
00:22:32,324 --> 00:22:35,902
the cache and then you can get that variable as well.

347
00:22:36,036 --> 00:22:39,774
So this is easy. So this doesn't require actually live

348
00:22:39,812 --> 00:22:43,742
demo, but with this couple of screenshots you get that picture

349
00:22:43,806 --> 00:22:47,860
and I'm pretty sure that this is as easy as

350
00:22:49,110 --> 00:22:52,802
services AWS is offering. So moving

351
00:22:52,856 --> 00:22:56,054
on. Now that you have kind of a decent understanding of

352
00:22:56,172 --> 00:22:59,202
AWS serverless caching solutions,

353
00:22:59,346 --> 00:23:02,358
let's go and see. Even though it's easy,

354
00:23:02,444 --> 00:23:06,422
there are some anti patterns just like everything in our know

355
00:23:06,476 --> 00:23:09,770
this beautiful and one of the great

356
00:23:09,840 --> 00:23:14,214
solution have some challenges

357
00:23:14,342 --> 00:23:17,386
which you have to be mindful not to fall into some

358
00:23:17,408 --> 00:23:19,798
of these standard antipadent traps.

359
00:23:19,974 --> 00:23:24,186
And one of the traps I feel like is over reliance on caching.

360
00:23:24,298 --> 00:23:28,430
So now when the days where we have to spend significant of time

361
00:23:28,580 --> 00:23:32,462
building Memcache or Redis. So we will always think about

362
00:23:32,596 --> 00:23:35,950
whether caching is really needed for our application.

363
00:23:36,100 --> 00:23:39,426
So we would have just spent bit of time to understand the

364
00:23:39,448 --> 00:23:44,226
pros and cons and then we will come

365
00:23:44,248 --> 00:23:47,762
up with the decision. But here, now that we can make a cache within

366
00:23:47,816 --> 00:23:51,298
less than 1 minute, I would say someone would just go and create a cache

367
00:23:51,314 --> 00:23:55,062
and rely on that. And I believe that is not a good thing. You should

368
00:23:55,116 --> 00:23:58,534
always understand for your application for

369
00:23:58,572 --> 00:24:02,074
your use case whether caching is a required thing and

370
00:24:02,112 --> 00:24:05,434
if it is, then of course you should go. But if it is not,

371
00:24:05,472 --> 00:24:10,098
you should do a lot of due diligence to ensure that it's

372
00:24:10,134 --> 00:24:13,742
right for you. Otherwise it can have some

373
00:24:13,796 --> 00:24:17,470
unexpected fallbacks for you.

374
00:24:17,620 --> 00:24:21,134
The second one is not handling cash misses. So once

375
00:24:21,172 --> 00:24:24,830
you have that cache based on that cash

376
00:24:24,900 --> 00:24:28,970
implementing, there can be operations where there are some cache misses,

377
00:24:29,050 --> 00:24:32,274
right? You had to be very mindful. So in order to get the

378
00:24:32,312 --> 00:24:35,522
benefit out of this solution you had figured out way

379
00:24:35,656 --> 00:24:39,800
how you can identify cache misses quickly and how can turn it around.

380
00:24:40,330 --> 00:24:43,480
And the other important thing is security, right? This is again

381
00:24:44,730 --> 00:24:49,062
just like everything AWS providing. So AWS is take care of certain

382
00:24:49,116 --> 00:24:53,366
things and the infrastructure and other things, but the data residing

383
00:24:53,398 --> 00:24:56,730
here, it's your data. You will have to figure out where.

384
00:24:56,800 --> 00:25:00,474
You'll have to implement security policies, processes and procedures to

385
00:25:00,512 --> 00:25:03,902
ensure that you manage the security of data, which is very

386
00:25:03,956 --> 00:25:07,326
important. Moving on. Now that you have

387
00:25:07,348 --> 00:25:10,654
some understanding about antipatterns, these are some of the best

388
00:25:10,692 --> 00:25:14,302
practices you should follow. So one thing is optimize

389
00:25:14,366 --> 00:25:18,466
your data with access patterns. Like you

390
00:25:18,488 --> 00:25:22,322
can get the maximum benefits of serverless caching. By doing this,

391
00:25:22,456 --> 00:25:26,854
you can come up with some caching designs like read through,

392
00:25:26,972 --> 00:25:30,230
write through, write behind,

393
00:25:30,380 --> 00:25:32,758
cache aside, refresh ahead,

394
00:25:32,924 --> 00:25:36,982
cache aside. Seeing things like write behind this

395
00:25:37,036 --> 00:25:40,470
will really give you high benefit,

396
00:25:40,550 --> 00:25:43,962
right? So these are some of the standard and

397
00:25:44,096 --> 00:25:47,306
the famous design patterns and you should really use it

398
00:25:47,328 --> 00:25:51,214
when you are doing your development. And next one is think about your

399
00:25:51,252 --> 00:25:55,054
serialization. You can reduce data transfer cost and

400
00:25:55,092 --> 00:25:59,034
improve all performance by focusing on the efficient

401
00:25:59,082 --> 00:26:03,138
serialization. So think about the message

402
00:26:03,224 --> 00:26:07,330
path protocol buffers, cache, auro or jSon,

403
00:26:07,990 --> 00:26:11,842
some of the famous examples. So that will actually

404
00:26:11,896 --> 00:26:15,960
give you a greater performance benefit. Another thing is

405
00:26:16,330 --> 00:26:20,466
come up with some good cache key in the naming

406
00:26:20,498 --> 00:26:24,790
convention. So that will actually save you a lot of trouble

407
00:26:25,290 --> 00:26:28,590
and effort when you are doing a long stretch

408
00:26:28,610 --> 00:26:31,610
of development or when you are doing maintenance.

409
00:26:32,030 --> 00:26:36,182
So other thing is just like everything, monitor and analyst

410
00:26:36,246 --> 00:26:39,590
continuously monitoring, understanding your cache hits,

411
00:26:39,670 --> 00:26:43,546
missed rates, latencies, response times, cache eviction

412
00:26:43,578 --> 00:26:46,590
rates, data transfer volumes, cache size,

413
00:26:46,740 --> 00:26:50,574
those will be really helpful. And those data points will

414
00:26:50,612 --> 00:26:54,026
give you some ideas like in your development cache,

415
00:26:54,138 --> 00:26:57,982
what you should do and how you can optimize it. So by

416
00:26:58,036 --> 00:27:02,334
following these four best practices you can get the

417
00:27:02,372 --> 00:27:05,890
more benefits the AWS elastic cache offering.

418
00:27:05,970 --> 00:27:09,590
And then this will about to allow you to develop

419
00:27:09,660 --> 00:27:13,160
a more fast system, a better system which is

420
00:27:13,770 --> 00:27:16,770
providing a world class service to end users,

421
00:27:16,850 --> 00:27:19,926
and you can of course reduce your operation cost.

422
00:27:20,108 --> 00:27:23,410
So with this, I'm going to wind up my presentation.

423
00:27:23,570 --> 00:27:27,334
Thank you very much for spending this time and have

424
00:27:27,372 --> 00:27:28,020
a nice day everyone.

