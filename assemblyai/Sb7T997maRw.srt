1
00:00:36,210 --> 00:00:40,198
Hello, and welcome to applied security at

2
00:00:40,204 --> 00:00:42,790
the Conf 42 site reliability conference.

3
00:00:43,530 --> 00:00:48,518
My name is Aaron Reinhardt. I am the CTO and founder of a

4
00:00:48,604 --> 00:00:51,854
company called Verica. And I am

5
00:00:51,892 --> 00:00:55,760
joined here with my co speaker, Jamie Dicken. Do you want to introduce yourself?

6
00:00:56,850 --> 00:01:00,894
Sure. I'm Jamie Dicken. I'm the manager of security engineering at

7
00:01:00,932 --> 00:01:04,766
Cardinal Health, which is a Fortune

8
00:01:04,798 --> 00:01:08,514
20 healthcare company in the United States. A little bit more about me and my

9
00:01:08,552 --> 00:01:12,206
background. I am the former chief security architect at UnitedHealth

10
00:01:12,238 --> 00:01:14,180
Group. At the company,

11
00:01:15,430 --> 00:01:19,634
I led the DevOps transformation, as well as pioneered

12
00:01:19,682 --> 00:01:22,790
the application of chaos engineering to cybersecurity,

13
00:01:23,130 --> 00:01:26,758
wrote the first tool called Chaos Slinger in the space. And we'll talk more

14
00:01:26,764 --> 00:01:29,306
about Jamie, and I'll talk more about that and the work that Cardinal health is

15
00:01:29,328 --> 00:01:33,398
also doing in the space. Jamie and I are also authors,

16
00:01:33,494 --> 00:01:37,194
O'Reilly authors on the topic. Our O'Reilly book on

17
00:01:37,232 --> 00:01:41,440
security kiosks engineering comes out this fall.

18
00:01:42,130 --> 00:01:45,242
And I also have a background at NASA safety and reliability

19
00:01:45,306 --> 00:01:48,926
engineering. That's my background. Oh,

20
00:01:48,948 --> 00:01:52,170
and you notice kind of an 80s theme to this presentation.

21
00:01:52,250 --> 00:01:54,980
So that was what I looked like in the 80s.

22
00:01:57,590 --> 00:02:01,742
That's awesome. I do not have nearly as awesome of a profile picture

23
00:02:01,806 --> 00:02:05,074
as that. But at Cardinal Health, I was brought into

24
00:02:05,112 --> 00:02:09,030
cybersecurity a little over a year ago, specifically lead a team

25
00:02:09,100 --> 00:02:13,320
focused on security chaos engineering. And we called ourselves applied security.

26
00:02:13,850 --> 00:02:17,122
Prior to that, I had spent ten years in software development,

27
00:02:17,186 --> 00:02:20,874
both as an engineer myself and then as a

28
00:02:20,912 --> 00:02:23,446
manager leading multiple teams.

29
00:02:23,638 --> 00:02:26,966
And with that, what I like to say now is that I spent

30
00:02:26,998 --> 00:02:30,730
the first ten years of my career building new features that added value

31
00:02:30,800 --> 00:02:33,886
to healthcare. And now in the next part of my

32
00:02:33,908 --> 00:02:37,946
career, I'm really focused on securing their legacy. So I've

33
00:02:37,978 --> 00:02:41,566
had the awesome opportunity to be a contributing author on the

34
00:02:41,588 --> 00:02:44,962
O'Reilly report that Erin mentioned. And what I'm really

35
00:02:45,016 --> 00:02:49,102
passionate about is using my experience both in software development

36
00:02:49,166 --> 00:02:53,022
and information security to really unite multiple disciplines.

37
00:02:53,166 --> 00:02:57,358
So today you'll hear me talk about SRE and Chaos Engineering.

38
00:02:57,454 --> 00:03:00,806
Other days, you may hear me talk about championing application security

39
00:03:00,908 --> 00:03:04,086
among software development, but in the end, all of

40
00:03:04,108 --> 00:03:07,414
the disciplines are really related and one can't be successful without

41
00:03:07,452 --> 00:03:10,700
the others. So it's crazy. Excited to be here today.

42
00:03:11,870 --> 00:03:15,162
So next, Erin and I have a really fun talk

43
00:03:15,216 --> 00:03:18,874
that's lined up today. So we're going to first talk

44
00:03:18,912 --> 00:03:22,894
about why a new approach to security, chaos engineering and

45
00:03:23,012 --> 00:03:26,046
continuous learning is really vital to us.

46
00:03:26,228 --> 00:03:29,562
We're going to discuss what the core of the security chaos

47
00:03:29,626 --> 00:03:33,418
engineering discipline is, and talk about the use cases that you can target.

48
00:03:33,594 --> 00:03:37,198
I'm going to discuss my real world experience leading

49
00:03:37,294 --> 00:03:41,294
a team at Cardinal Health as we began our security chaos engineering

50
00:03:41,342 --> 00:03:44,766
journey, and how you can do that, too. And we're going to tie a bow

51
00:03:44,798 --> 00:03:48,758
on everything at the end and unite both SRE and

52
00:03:48,844 --> 00:03:52,422
security chaos engineering. So,

53
00:03:52,476 --> 00:03:55,942
next, Aaron, whatever brought everybody here to

54
00:03:56,076 --> 00:03:59,878
comp 42 today, whether that's to learn what SRE

55
00:03:59,974 --> 00:04:03,866
is from real world practitioners or to gain exposure to

56
00:04:03,888 --> 00:04:07,462
world class thought leadership, or really see some technical

57
00:04:07,526 --> 00:04:10,926
demos and expand your team's horizons, I think it's safe to

58
00:04:10,948 --> 00:04:14,750
say that we can all align on one basic truth,

59
00:04:15,170 --> 00:04:18,922
and next, that is that systems engineering

60
00:04:18,986 --> 00:04:22,954
is messy. So, on the next slide,

61
00:04:23,002 --> 00:04:26,686
I love this picture because I think that it's one that we can all relate

62
00:04:26,718 --> 00:04:30,114
to. It's one that if we've ever been

63
00:04:30,152 --> 00:04:34,034
in some of our server rooms, we recognize that our

64
00:04:34,072 --> 00:04:37,720
systems, no matter how beautifully they were designed at first,

65
00:04:38,250 --> 00:04:40,600
we can quickly sour on those.

66
00:04:41,370 --> 00:04:45,030
So, in the next slide, you see that we sometimes begin

67
00:04:45,100 --> 00:04:48,466
with these beautifully simplistic representations,

68
00:04:48,658 --> 00:04:52,458
either of what we want to build or what it is that we think that

69
00:04:52,464 --> 00:04:56,154
we did build. But as I said, it doesn't take

70
00:04:56,192 --> 00:04:58,940
long for complexity to sink in.

71
00:04:59,550 --> 00:05:03,102
So on the next slide, you can see that it's not always

72
00:05:03,156 --> 00:05:06,462
our fault. So, throughout a system's life,

73
00:05:06,516 --> 00:05:10,174
complexity has a way of sneaking in, whether that's new

74
00:05:10,212 --> 00:05:13,678
business requirements come and force tight deadlines that

75
00:05:13,764 --> 00:05:17,294
have us just throwing on and bolting on new microservices and

76
00:05:17,332 --> 00:05:21,218
using into spaghetti code. Or maybe there is this huge focus

77
00:05:21,304 --> 00:05:24,702
on new customer acquisition that forces

78
00:05:24,766 --> 00:05:28,546
us to really rapidly resize our environment faster than we were planning

79
00:05:28,578 --> 00:05:31,954
on it, without all of the dedication

80
00:05:32,002 --> 00:05:35,494
and the forethought that went into it. Or maybe even there are new

81
00:05:35,532 --> 00:05:38,746
security requirements that come into play, and all of a

82
00:05:38,768 --> 00:05:42,490
sudden, we find ourselves adding in layers of infrastructure.

83
00:05:43,470 --> 00:05:46,890
And then on the next slide, you see that as time goes

84
00:05:46,960 --> 00:05:50,574
on, those problems just compound as new change

85
00:05:50,612 --> 00:05:53,882
is introduced into the system and as our processes

86
00:05:53,946 --> 00:05:57,166
change. And so next,

87
00:05:57,268 --> 00:06:00,814
we see that the reality is that our

88
00:06:00,852 --> 00:06:04,258
systems are so much more complex than we remember them,

89
00:06:04,424 --> 00:06:08,398
and we know that the outages and the experiences

90
00:06:08,494 --> 00:06:11,874
in terms of using the system or not using

91
00:06:11,912 --> 00:06:15,606
the system, in the case of a sev one, we start

92
00:06:15,628 --> 00:06:20,130
CTO realize they're not simple at all. It's actually through failure

93
00:06:20,290 --> 00:06:23,522
that we discover that we have far more dependencies

94
00:06:23,586 --> 00:06:26,710
and complexities than we ever could have imagined.

95
00:06:28,190 --> 00:06:32,346
So, on the next slide, we recognize that

96
00:06:32,528 --> 00:06:36,422
our approach in the past has failed us, and that's because it's

97
00:06:36,486 --> 00:06:38,060
very much old school.

98
00:06:38,750 --> 00:06:42,554
So what we used to do when we wanted to build for security

99
00:06:42,672 --> 00:06:46,810
in mind is that we would pull up an infrastructure diagram

100
00:06:46,890 --> 00:06:50,334
of one of our systems, and we would start to threat model and point out

101
00:06:50,372 --> 00:06:54,010
single points of failure and identify opportunities

102
00:06:54,090 --> 00:06:56,900
for latency or for somebody to get in there.

103
00:06:57,510 --> 00:07:00,526
But we all know the problems with this approach.

104
00:07:00,718 --> 00:07:04,370
So first we think about how and when that documentation is actually

105
00:07:04,440 --> 00:07:07,826
created. So maybe that documentation is created

106
00:07:07,858 --> 00:07:11,586
at the beginning of the project, in the planning phase,

107
00:07:11,698 --> 00:07:15,698
before any lines of code are written, and before any infrastructure is deployed,

108
00:07:15,714 --> 00:07:18,922
CTO production. So if that's the case,

109
00:07:19,056 --> 00:07:22,698
maybe our documentation really matches the

110
00:07:22,784 --> 00:07:26,486
ideal system at that point in time. But maybe it doesn't

111
00:07:26,518 --> 00:07:30,090
even represent the system that was actually deployed to production,

112
00:07:31,010 --> 00:07:34,702
or if that documentation was created after

113
00:07:34,756 --> 00:07:38,174
the fact. Well, it's still dependent on the

114
00:07:38,212 --> 00:07:41,466
memory of the architecture, the engineer that created

115
00:07:41,498 --> 00:07:46,302
it. And you're banking that that architect or engineer isn't misremembering

116
00:07:46,446 --> 00:07:50,020
based on their experience in creating that system.

117
00:07:50,950 --> 00:07:55,006
And even if you do have documentation, you need to wonder about how

118
00:07:55,048 --> 00:07:58,440
it gets updated and when, if that even happens.

119
00:07:59,690 --> 00:08:03,206
If we have our documentation that is

120
00:08:03,308 --> 00:08:07,174
fully updated, do we really know?

121
00:08:07,292 --> 00:08:10,554
Do we have all of our integration points mapped? Do we know

122
00:08:10,592 --> 00:08:14,506
the downstream effects? Do we

123
00:08:14,528 --> 00:08:18,214
know if somebody's actually using our system in an automated manner?

124
00:08:18,262 --> 00:08:22,318
If they're just leveraging our APIs behind the scenes? We have no idea.

125
00:08:22,404 --> 00:08:25,502
Right? So before

126
00:08:25,556 --> 00:08:28,746
you start hating on me, let's be clear. I am not saying don't

127
00:08:28,778 --> 00:08:32,526
create documentation or don't update documentation. I mean,

128
00:08:32,628 --> 00:08:36,322
for the love of anybody who works on your systems, if you have any

129
00:08:36,376 --> 00:08:40,050
respect for your coworkers or your customers at all, please create

130
00:08:40,120 --> 00:08:43,730
that documentation. But what I am saying, though,

131
00:08:43,800 --> 00:08:47,350
is that if your process of evaluating a system for

132
00:08:47,420 --> 00:08:50,966
security vulnerabilities is dependent on

133
00:08:51,148 --> 00:08:54,998
an outdated or just a flat out incorrect representation of

134
00:08:55,004 --> 00:08:59,494
a system, that misremembering that we taken about our

135
00:08:59,532 --> 00:09:02,810
evaluation is going to fail. And there are going to be plenty of problems

136
00:09:02,880 --> 00:09:06,506
that you don't even anticipate. And quite honestly, if I look

137
00:09:06,528 --> 00:09:10,018
at it, the number of industrywide publicly disclosed

138
00:09:10,054 --> 00:09:13,886
breaches or DDoS attacks proves that we're really failing to

139
00:09:13,908 --> 00:09:16,510
keep up in our understanding of our systems.

140
00:09:17,490 --> 00:09:21,070
So, on the next slide, I really like this quote.

141
00:09:21,490 --> 00:09:25,838
As complexity scientist Dave Snowden on nuts.

142
00:09:25,934 --> 00:09:29,298
Dang it, I messed up. I forgot that slide entirely. Oh, yeah,

143
00:09:29,384 --> 00:09:33,140
here it is. Here it is. We got it. That's my bad.

144
00:09:33,590 --> 00:09:36,280
Okay, no, sorry. It was on me.

145
00:09:37,610 --> 00:09:39,830
So I really like this quote.

146
00:09:40,330 --> 00:09:44,150
As complexity scientist Dave Snowden says, it's actually

147
00:09:44,220 --> 00:09:48,242
impossible for a human to model or document

148
00:09:48,386 --> 00:09:51,594
a complex system. And if you're like me,

149
00:09:51,632 --> 00:09:54,826
you hear, challenge accepted. But what

150
00:09:54,848 --> 00:09:58,106
he says really makes sense. And that is the only way to

151
00:09:58,128 --> 00:10:01,614
really understand that complex system is to interact with

152
00:10:01,652 --> 00:10:05,534
it. So then our answer is not

153
00:10:05,572 --> 00:10:09,354
CTO rely on our recollection of our systems or our memory

154
00:10:09,402 --> 00:10:13,782
of the system, but rather to learn from them and use empirical

155
00:10:13,866 --> 00:10:18,158
data. And that's really the heart of chaos engineering,

156
00:10:18,254 --> 00:10:22,162
just implementing experiments that clearly show us what our real system

157
00:10:22,216 --> 00:10:25,818
landscape looks like so we can tease out those false assumptions.

158
00:10:26,014 --> 00:10:28,600
And, Aaron, I'm going to hand it over to you. Right?

159
00:10:29,770 --> 00:10:34,486
Yeah. And it's really about a

160
00:10:34,508 --> 00:10:37,786
new approach to learning. That's really

161
00:10:37,808 --> 00:10:39,180
what we're using to talk about here.

162
00:10:42,990 --> 00:10:46,550
There's a vast difference between what we believe our systems

163
00:10:46,630 --> 00:10:50,006
are and what they are in reality. And Jamie

164
00:10:50,038 --> 00:10:55,966
makes a good point about the number of breaches that are growing. And I

165
00:10:55,988 --> 00:10:59,950
think there's a quote that says we should be surprised that our systems

166
00:11:01,330 --> 00:11:05,338
work as well as they do, given the little amount of little information

167
00:11:05,524 --> 00:11:08,978
we know about them already. The same is true of security.

168
00:11:09,144 --> 00:11:12,994
We know very little about how our systems actually really function in

169
00:11:13,032 --> 00:11:16,802
reality, and that's kind of what's being exposed in breaches and incidents. I mean,

170
00:11:16,856 --> 00:11:19,846
if that wasn't the case, then we would have fixed it, and they never would

171
00:11:19,868 --> 00:11:23,574
have became breaches or incidents. So, moving on here.

172
00:11:23,692 --> 00:11:27,080
So I like to talk about continuous learning in terms of,

173
00:11:27,530 --> 00:11:31,958
it's important to understand that there's a difference between continuous fixing

174
00:11:32,054 --> 00:11:35,194
and continuous learning. And what we're trying to do is continuously learn

175
00:11:35,232 --> 00:11:38,986
about our systems actually work, so we can make better decisions and have more

176
00:11:39,088 --> 00:11:43,854
better concept proactively to

177
00:11:43,892 --> 00:11:47,086
reduce customer pain, because that's really what we're trying to come after here,

178
00:11:47,108 --> 00:11:50,894
is, in the end, we're trying to build more

179
00:11:50,932 --> 00:11:54,126
performant, reliable, and safe systems.

180
00:11:54,318 --> 00:11:59,986
And the customers are the ones that typically feel the impact of

181
00:12:00,008 --> 00:12:03,906
our misgivings, like I said.

182
00:12:04,008 --> 00:12:07,558
So there is a difference between continuous fixing and continuous learning. A lot of

183
00:12:07,564 --> 00:12:11,046
what we're doing now is continuous fixing. Were fixing what we

184
00:12:11,068 --> 00:12:15,510
believe the problem to be, instead of continuously trying to learn deeper insights

185
00:12:16,490 --> 00:12:20,442
and develop a better understanding about how our systems actually

186
00:12:20,496 --> 00:12:21,100
function.

187
00:12:23,550 --> 00:12:26,838
So I love this. I'm using to share a brief story. So, my co founder

188
00:12:26,854 --> 00:12:30,374
of Erica is Casey Rosenthal. He's the creator of chaos engineering at Netflix,

189
00:12:30,422 --> 00:12:33,774
and I was at lunch with him one time and

190
00:12:33,892 --> 00:12:37,710
I witnessed sort of this conversation with a potential customer in large

191
00:12:37,780 --> 00:12:41,226
payment processing company. And they're

192
00:12:41,258 --> 00:12:46,410
talking about, they have this legacy system with

193
00:12:46,420 --> 00:12:50,798
the core applications for the company, and it processed all the organizations,

194
00:12:50,974 --> 00:12:54,258
so they trusted it. The engineers were competent. They really had an

195
00:12:54,264 --> 00:12:57,506
outage, and they wanted to move it all over to Kubernetes,

196
00:12:57,538 --> 00:13:00,840
and they needed help with that. And I started thinking to myself, like,

197
00:13:01,210 --> 00:13:04,998
was that legacy system always stable? Was there a

198
00:13:05,004 --> 00:13:08,866
point in time where it was incurring outages and it wasn't

199
00:13:08,898 --> 00:13:10,470
as widely understood?

200
00:13:13,630 --> 00:13:17,754
We kind of learn about how the system really functions over

201
00:13:17,792 --> 00:13:21,050
time through a series of unforeseen, unplanned, or surprise

202
00:13:21,130 --> 00:13:24,110
events. With chaos engineering.

203
00:13:24,610 --> 00:13:29,006
It's a methodology where we can learn about the system in

204
00:13:29,028 --> 00:13:32,398
a controlled way and not incur pain.

205
00:13:32,564 --> 00:13:36,718
Because when you learned about that legacy system and it became stable through those unforeseen

206
00:13:36,734 --> 00:13:40,020
events, there was pain as part of that process.

207
00:13:40,390 --> 00:13:44,770
Your service was not available. You might have made public headlines.

208
00:13:46,470 --> 00:13:50,038
We don't have to build things that way and run and run our

209
00:13:50,044 --> 00:13:53,782
systems that way. We can be proactive and use

210
00:13:53,836 --> 00:13:57,714
techniques like chaos engineering to develop better understanding of how our systems really

211
00:13:57,772 --> 00:13:58,380
work.

212
00:14:01,630 --> 00:14:04,410
It's also kind of about a change in mindset.

213
00:14:10,270 --> 00:14:14,126
It is kind of a different way of thinking in that a

214
00:14:14,148 --> 00:14:17,758
lot of people say that chaos is a

215
00:14:17,764 --> 00:14:21,182
very provocative term. Right, Casey and I like to

216
00:14:21,236 --> 00:14:24,874
sort of reframe, and so does Jamie. She uses this terminology

217
00:14:24,922 --> 00:14:28,606
as, like, we try to reframe chaos engineering more in terms of continuous

218
00:14:28,638 --> 00:14:32,050
verification, is that as we build things, as we move to the cloud,

219
00:14:32,120 --> 00:14:35,566
as we build new applications, as we move legacy applications

220
00:14:35,598 --> 00:14:38,806
to kubernetes, as we're building them, as part

221
00:14:38,828 --> 00:14:42,994
of the process of building and operating them, we're continuously verifying

222
00:14:43,042 --> 00:14:46,166
that the system still does what it's supposed to do. We do that in the

223
00:14:46,188 --> 00:14:49,320
form of a hypothesis, in asking the system a question,

224
00:14:49,930 --> 00:14:53,594
but it's a different way of thinking.

225
00:14:53,792 --> 00:14:57,098
And just like DevOps was a different way of thinking, cloud was a

226
00:14:57,104 --> 00:15:00,022
different way of thinking. Chaos engineering is right along with it,

227
00:15:00,176 --> 00:15:03,966
but it is

228
00:15:03,988 --> 00:15:05,310
a change of mindset.

229
00:15:07,730 --> 00:15:11,630
Also, it's important to understand this is important.

230
00:15:11,700 --> 00:15:15,498
Caveat to chaos engineering is that when incidents occur

231
00:15:15,674 --> 00:15:19,154
in outages and breaches, people operate differently in those

232
00:15:19,192 --> 00:15:22,578
conditions, but people also in reverse, operate differently when

233
00:15:22,584 --> 00:15:25,794
they expect things to fail. So what do I mean by that? What I mean

234
00:15:25,832 --> 00:15:29,286
is, so chaos engineering, we don't do it

235
00:15:29,308 --> 00:15:32,562
when people are freaking out during an active incident. During an active incident,

236
00:15:32,626 --> 00:15:35,702
people, they're worrying about the blame name. Shame game,

237
00:15:35,756 --> 00:15:39,282
right? Was that my system? Is this the breach?

238
00:15:39,426 --> 00:15:42,966
Did I cause this? And people

239
00:15:43,068 --> 00:15:46,186
think their jobs are on the line. People are freaking out. This is not a

240
00:15:46,208 --> 00:15:50,006
good way to learn. So we don't do chaos engineering

241
00:15:50,038 --> 00:15:53,770
here, okay? We do it when, essentially,

242
00:15:53,850 --> 00:15:56,686
there is nothing to worry about. Right. We do it when it's all rainbows and

243
00:15:56,708 --> 00:16:00,426
sunshine, right? We do it proactively when nobody's

244
00:16:00,458 --> 00:16:04,290
freaking out. It's a better learning environment. People's eyes are more

245
00:16:04,360 --> 00:16:08,500
wide open, and their cognitive loads are not bore down

246
00:16:09,910 --> 00:16:13,026
on all that load of worrying about the

247
00:16:13,048 --> 00:16:16,546
breach and affecting the company and their jobs,

248
00:16:16,738 --> 00:16:20,520
remember? So, we do chaos engineering here? Not here,

249
00:16:21,450 --> 00:16:24,738
here. A little lag.

250
00:16:24,834 --> 00:16:26,840
There we go. All right.

251
00:16:32,030 --> 00:16:35,674
Chaos engineering is about establishing order. We do that by

252
00:16:35,712 --> 00:16:38,794
instrumenting the chaos inherently within the system.

253
00:16:38,992 --> 00:16:42,666
And what we're trying to do is try to proactively introduce turbulent

254
00:16:42,698 --> 00:16:46,366
conditions into the system to try to determine the

255
00:16:46,388 --> 00:16:49,658
conditions by which it will fail before it fails.

256
00:16:49,754 --> 00:16:53,570
So, in security terms, we're trying to introduce the conditions

257
00:16:54,390 --> 00:16:57,250
that we expect our security to operate upon,

258
00:16:57,830 --> 00:17:00,962
and we validate them. And most often,

259
00:17:01,096 --> 00:17:04,962
almost every chaos experiment, security, availability, that I have

260
00:17:05,016 --> 00:17:08,386
either run myself or stories I've heard from other people. I don't

261
00:17:08,418 --> 00:17:11,558
think I've ever heard anyone say it all worked the first time,

262
00:17:11,724 --> 00:17:15,414
because it rarely does. Were almost always wrong about how our systems really

263
00:17:15,452 --> 00:17:16,040
work.

264
00:17:20,330 --> 00:17:23,990
Chaos. So we like to sort of put chaos

265
00:17:24,070 --> 00:17:27,882
or instrumentation in two loose domains. One is

266
00:17:27,936 --> 00:17:30,922
testing and experimentation, right? So,

267
00:17:30,976 --> 00:17:34,254
testing is a verification or validation of something we know to be true or

268
00:17:34,292 --> 00:17:37,680
false. It's a binary sort of thing. It is or it isn't, right?

269
00:17:42,210 --> 00:17:47,054
We know what we're looking for before we go looking for it. Whereas experimentation,

270
00:17:47,182 --> 00:17:50,338
we're trying to derive new information that we previously did not know

271
00:17:50,424 --> 00:17:54,100
about the system beforehand. New insights, new information.

272
00:17:54,630 --> 00:17:57,860
And so, example of sort of testing would be like,

273
00:17:58,870 --> 00:18:02,166
we go in looking for attack patterns or signatures or things

274
00:18:02,188 --> 00:18:05,446
like our scves, whereas experimentation, we're trying to ask

275
00:18:05,468 --> 00:18:08,390
the computer specifically a question. When x occurs.

276
00:18:09,390 --> 00:18:13,066
I know I built y to be the response, and we

277
00:18:13,088 --> 00:18:16,266
actually do that, and we try to find out whether or not that is actually

278
00:18:16,288 --> 00:18:17,260
true or not.

279
00:18:20,270 --> 00:18:24,074
So, chaos engineering in general

280
00:18:24,272 --> 00:18:28,106
is really. It's about establishing order. It's a very practical technique,

281
00:18:28,138 --> 00:18:31,086
if you really think about it. We're just trying to validate what we already know

282
00:18:31,108 --> 00:18:34,430
to be true. You never do. A chaos experiment you know is going to fail.

283
00:18:34,510 --> 00:18:39,170
You'll never learn anything from it. The point is to learn proactively.

284
00:18:43,790 --> 00:18:46,998
So, what is security? Chaos engineering. So security chaos.

285
00:18:47,094 --> 00:18:50,526
Newsflash, security chaos engineering is exactly the same thing as applied to

286
00:18:50,548 --> 00:18:54,350
security. It's just a little bit different in terms of

287
00:18:54,420 --> 00:18:58,174
the use cases. The use cases we're coming after

288
00:18:58,212 --> 00:19:01,634
for security are really focused around. I have some slides on that,

289
00:19:01,672 --> 00:19:05,726
but they're focused around instant response control validation.

290
00:19:05,838 --> 00:19:09,118
That's a big one. I believe Jamie's going to talk about a bit about cardinal

291
00:19:09,134 --> 00:19:12,370
health and control validation, increasing observability.

292
00:19:13,510 --> 00:19:16,760
You'd be surprised how poor your logs really are.

293
00:19:17,130 --> 00:19:20,278
And during an incident, is not a good time to be evaluating the quality of

294
00:19:20,284 --> 00:19:24,134
log data. Right. But if we can inject these

295
00:19:24,172 --> 00:19:26,934
signals and incidents proactively, we can kind of find out,

296
00:19:26,972 --> 00:19:30,806
hey, did the technology in question actually give

297
00:19:30,828 --> 00:19:33,754
good log data to make sense of what the heck happened? Right. Do we have

298
00:19:33,792 --> 00:19:36,220
the right log data we thought we were supposed to have?

299
00:19:36,670 --> 00:19:39,774
And instead of trying to scramble things together, try to figure out what happened,

300
00:19:39,812 --> 00:19:43,086
because you're being attacked, that's not conducive to

301
00:19:43,108 --> 00:19:44,590
a good way of operating.

302
00:19:47,010 --> 00:19:49,150
So I love this quote.

303
00:19:50,850 --> 00:19:54,146
I've always said, in general, that engineers don't believe in two things. They don't believe

304
00:19:54,168 --> 00:19:57,490
in hope or luck. Right. It either works or it doesn't.

305
00:19:57,990 --> 00:20:01,406
And hoping your security works, hoping your systems

306
00:20:01,438 --> 00:20:04,802
work the way you think they do, it's not an effective strategy, especially in engineering.

307
00:20:04,866 --> 00:20:06,694
It worked in Star wars, but it's not going to work.

308
00:20:06,732 --> 00:20:12,642
Here's.

309
00:20:12,786 --> 00:20:16,066
It's also about. So with, you know, with the change of.

310
00:20:16,108 --> 00:20:21,398
This goes hand in hand with change of mindset, is that it's

311
00:20:21,414 --> 00:20:25,126
really about security. Chaos experiments are focused, really on accidents

312
00:20:25,158 --> 00:20:28,118
and mistakes that engineers make.

313
00:20:28,304 --> 00:20:31,386
The size, scale, speed, and complexity of modern software.

314
00:20:31,498 --> 00:20:32,560
It's hard.

315
00:20:36,370 --> 00:20:40,766
We are building the equivalent of complex adaptive systems in

316
00:20:40,788 --> 00:20:44,674
terms of software, and it's impossible for a human to mentally keep track of everything.

317
00:20:44,792 --> 00:20:48,494
It's also very easy to make mistakes when you can't, when you have poor visibility

318
00:20:48,622 --> 00:20:52,130
and your mind can't mentally model that behavior.

319
00:20:53,190 --> 00:20:57,086
So do we sit around, we blame people for making mistakes

320
00:20:57,118 --> 00:21:00,502
and accidents, or do we proactively try CTO, discover them and

321
00:21:00,556 --> 00:21:03,766
fix them before an adversary has a chance to take advantage of

322
00:21:03,788 --> 00:21:06,806
it? And that's really all we're doing, is we're injecting the low hanging fruit by

323
00:21:06,828 --> 00:21:10,634
which most malware and most attacks are successful to begin with.

324
00:21:10,832 --> 00:21:14,426
If you look at most malicious code out there, a majority of it is

325
00:21:14,448 --> 00:21:18,390
junk. It's really junk code. It requires some kind of low hanging fruit.

326
00:21:18,470 --> 00:21:22,342
Open port, permissive account, a deprecated

327
00:21:22,406 --> 00:21:26,170
version or dependency, that's applaud.

328
00:21:27,570 --> 00:21:30,994
What we're trying to do is proactively clean up the low hanging fruit so those

329
00:21:31,032 --> 00:21:33,730
attacks can't become successful.

330
00:21:34,630 --> 00:21:40,706
That's kind of the focus we

331
00:21:40,728 --> 00:21:43,940
often. Is this you? No,

332
00:21:44,390 --> 00:21:48,142
that's this.

333
00:21:48,216 --> 00:21:52,146
What we're talking about here is that, as Jamie said earlier, we often do misremember

334
00:21:52,178 --> 00:21:56,426
what our systems really are. And that's what

335
00:21:56,448 --> 00:21:59,562
I talked about, size, scale, speed, complexity earlier, is that

336
00:21:59,696 --> 00:22:03,962
it's easy to misremember how

337
00:22:04,016 --> 00:22:07,802
our systems work versus how they work

338
00:22:07,856 --> 00:22:11,150
now, we love to remember in terms of that beautiful

339
00:22:11,300 --> 00:22:14,720
three d Amazon diagram that was produced two years ago,

340
00:22:15,730 --> 00:22:18,862
but our system has evolved way beyond that

341
00:22:18,916 --> 00:22:22,130
state and solely drifting in a state of unknown.

342
00:22:27,350 --> 00:22:30,210
So it is about continuous security verification.

343
00:22:31,990 --> 00:22:35,214
And that's really the best way to really frame up chaos

344
00:22:35,262 --> 00:22:38,706
engineering in general, is it's about continuously verifying

345
00:22:38,738 --> 00:22:42,306
that things work the way they're supposed to. That's what we're trying to really achieve

346
00:22:42,338 --> 00:22:45,480
with it. But there's a lot of different ways you can use it.

347
00:22:47,310 --> 00:22:51,014
But predominantly what we're trying to achieve is combating

348
00:22:51,062 --> 00:22:55,020
the inherent uncertainty of

349
00:22:56,350 --> 00:22:59,566
breaches, outages and incidents and the state of

350
00:22:59,588 --> 00:23:02,750
our systems by building confidence slowly through instrumentation.

351
00:23:04,290 --> 00:23:08,206
So here's some of the use cases. There are many more in

352
00:23:08,228 --> 00:23:11,614
the O'Reilly book. There are several accounts of people in different

353
00:23:11,652 --> 00:23:15,618
use cases. We're going to go in depth into all of that, but some

354
00:23:15,624 --> 00:23:19,298
of the main use cases here that I've used before, and I know

355
00:23:19,304 --> 00:23:22,850
Jamie's used it for a variety of these as well, is instant

356
00:23:25,110 --> 00:23:28,326
great. We'll talk about instant response next. It's a great way to sort

357
00:23:28,348 --> 00:23:31,782
of manage and measure incident response, security control,

358
00:23:31,836 --> 00:23:35,346
validation. That's where I began my journey, I think were a team beyond

359
00:23:35,378 --> 00:23:39,210
hers. It's a great way, like I said earlier, to increase

360
00:23:40,590 --> 00:23:44,202
the understanding of how observable your events are within

361
00:23:44,256 --> 00:23:48,574
the system. And every chaos engineering experiment has

362
00:23:48,612 --> 00:23:49,680
compliance value.

363
00:23:52,130 --> 00:23:55,982
So instant response. So the problem with instant response is

364
00:23:56,036 --> 00:23:59,760
the fact that it's always a response. And no matter

365
00:24:00,530 --> 00:24:03,758
how much money you spend, how many fancy tools you have,

366
00:24:03,844 --> 00:24:07,726
you still don't know very much about an

367
00:24:07,748 --> 00:24:10,578
event when it happens. Right? You don't know when it's going to happen, you don't

368
00:24:10,584 --> 00:24:12,626
know where it's going to happen, you don't know why it's happening or who's doing

369
00:24:12,648 --> 00:24:16,226
it, how they're going to get in. You don't really know until you're being tested,

370
00:24:16,258 --> 00:24:20,262
until somebody's actually trying to expose that, well, that's not a good time.

371
00:24:20,316 --> 00:24:23,730
CTO, start understanding whether or not those things are effective.

372
00:24:23,890 --> 00:24:27,002
With chaos engineering, we're not waiting for something to happen.

373
00:24:27,056 --> 00:24:30,234
We're proactively introducing a signal into the system and

374
00:24:30,272 --> 00:24:34,106
saying, hey, do we have enough people on call

375
00:24:34,208 --> 00:24:37,340
in terms of incident response? Were the runbooks correct? Right.

376
00:24:38,430 --> 00:24:42,666
Did the technologies give us the right contextual telemetry?

377
00:24:42,698 --> 00:24:45,914
We needed to make a decision or to conduct a triage

378
00:24:45,962 --> 00:24:49,422
process. When you're constantly kind of waiting for

379
00:24:49,556 --> 00:24:53,166
an event to happen, it's so subjective. It's hard to even compare

380
00:24:53,198 --> 00:24:57,250
two events on a similar system in the same context.

381
00:24:58,630 --> 00:25:02,174
And you're assuming that when you caught the event is when it began.

382
00:25:02,302 --> 00:25:07,254
We know when we started chaos experiment that we began that

383
00:25:07,292 --> 00:25:11,510
failure injection. It's really a great way to manage its response.

384
00:25:14,250 --> 00:25:17,802
So what we're essentially doing, instead of making the post mortem, a three hour

385
00:25:17,856 --> 00:25:21,978
exercise that I'm sure everyone really does, does it

386
00:25:22,144 --> 00:25:26,074
after an incident where people get together in the room and talk

387
00:25:26,112 --> 00:25:30,006
about what they think happened after already knowing

388
00:25:30,118 --> 00:25:33,646
what happened, there are several biases in the whole process of

389
00:25:33,668 --> 00:25:36,814
doing it. And after sort of a post mortem after the fact,

390
00:25:37,012 --> 00:25:40,094
because you already know what the outcome. So people start shooting all over the place.

391
00:25:40,132 --> 00:25:43,460
They say, should have done this, should have done that, they should have known this.

392
00:25:43,910 --> 00:25:47,266
Whereas chaos engineering, we're flipping the model, right?

393
00:25:47,368 --> 00:25:50,270
We're making the post mortem a preparation exercise,

394
00:25:50,350 --> 00:25:52,660
verifying how battle ready we are.

395
00:25:55,590 --> 00:25:59,366
So, Chaoslinger. So Chaos Slinger was a

396
00:25:59,388 --> 00:26:02,706
project about four years ago that I wrote at UnitedHealth

397
00:26:02,738 --> 00:26:05,846
Group, and there's a series of us that

398
00:26:05,868 --> 00:26:09,226
wrote it. But the idea of it was that we

399
00:26:09,248 --> 00:26:13,354
wanted to proactively sort of verify that the

400
00:26:13,392 --> 00:26:17,066
security we were building AWS was actually as good as we thought it

401
00:26:17,088 --> 00:26:21,046
was. I'm going to give a quick example, but the tool chaoslinger

402
00:26:21,078 --> 00:26:24,238
is actually a deprecated. Now, in GitHub, I'm no longer at UnitedHealth Group. They have

403
00:26:24,244 --> 00:26:27,594
their own version internally that they use. But the framework

404
00:26:27,642 --> 00:26:31,246
for how to write experiences is still in there, and that's really all you need

405
00:26:31,268 --> 00:26:33,330
in order to understand how to write experiments.

406
00:26:34,790 --> 00:26:37,394
So a real quick example was Portslinger. So,

407
00:26:37,432 --> 00:26:41,346
Portslinger. We did an example so people could understand the

408
00:26:41,368 --> 00:26:44,738
open source tool and how it works, and a basic experiment. So we picked a

409
00:26:44,744 --> 00:26:48,326
misconfigured, unauthorized port change. For some reason, it still happens all the

410
00:26:48,348 --> 00:26:51,190
time. It could be that somebody didn't understand flow.

411
00:26:52,170 --> 00:26:55,466
They filled out the wrong information in a ticket, or they made the

412
00:26:55,488 --> 00:26:58,380
change wrong. Lots of different reasons why.

413
00:27:00,350 --> 00:27:03,546
What we expected, the security expectation was that we

414
00:27:03,568 --> 00:27:07,754
expected to immediately detect and block the

415
00:27:07,792 --> 00:27:10,586
change with our firewall, and it would be a non issue. What we found out

416
00:27:10,608 --> 00:27:13,280
was that only was true about 60% of the time.

417
00:27:15,730 --> 00:27:18,286
That was the first thing we learned. We learned that the firewalls that we were

418
00:27:18,308 --> 00:27:21,538
paying for this fancy solution wasn't as good as we thought it was.

419
00:27:21,624 --> 00:27:25,490
Right. But we learned this proactively. This was not learning through failure,

420
00:27:26,230 --> 00:27:30,100
through pain. So a second thing we learned was

421
00:27:33,350 --> 00:27:36,706
that the cloud native configuration management tool called it and blocked it every time.

422
00:27:36,808 --> 00:27:40,146
So something we weren't paying for barely caught

423
00:27:40,178 --> 00:27:43,238
and blocked it every time. The third thing we expected was good log data to

424
00:27:43,244 --> 00:27:46,038
go to, like a sim. We didn't use a sim. We had our own log

425
00:27:46,124 --> 00:27:49,502
solution, but we expected a correlate event to the sock.

426
00:27:49,586 --> 00:27:52,998
Okay, that all happened. But when the sock

427
00:27:53,014 --> 00:27:56,518
got the event, we were very new to AWS at the time, and they couldn't

428
00:27:56,534 --> 00:28:00,186
figure out which account structure the event came from, whether it was our

429
00:28:00,208 --> 00:28:04,350
commercial or non commercial software environments. And you say

430
00:28:04,500 --> 00:28:07,546
that's nontrivial. That's easy to figure that out because you can map a back IP

431
00:28:07,578 --> 00:28:10,526
address to what account it was. Yeah, but that could take 30 minutes. That could

432
00:28:10,548 --> 00:28:13,642
take 3 hours. Right. During an active incident, you lose millions,

433
00:28:13,706 --> 00:28:17,458
millions of dollars. Right. We didn't have to do that. All we had

434
00:28:17,464 --> 00:28:21,106
CTO do was figure out that we had need to

435
00:28:21,128 --> 00:28:24,740
add metadata and a pointer to the event

436
00:28:25,190 --> 00:28:29,062
and fix it. But we did all of this. We learned all these things

437
00:28:29,116 --> 00:28:32,406
about how our system was really working and that we had a drift problem between

438
00:28:32,428 --> 00:28:36,486
commercial and non commercial environments. We learned all this not

439
00:28:36,508 --> 00:28:39,950
through an incident or an outage. We learned it by proactively discovering

440
00:28:39,970 --> 00:28:43,914
it. Just asking the system, are you doing what we designed you CTO do?

441
00:28:44,032 --> 00:28:47,466
And it was an extremely valuable exercise. I'm sure Jamie's going to go

442
00:28:47,488 --> 00:28:50,240
more in depth on that in a little bit,

443
00:28:52,290 --> 00:28:54,880
Jamie. Absolutely.

444
00:28:55,250 --> 00:28:59,194
So there are several companies that are already implemented security chaos

445
00:28:59,242 --> 00:29:03,300
engineering today and were seeing that number increase, and that is awesome.

446
00:29:03,910 --> 00:29:07,822
Cardinal Health, where I work, we began our security chaos journey

447
00:29:07,886 --> 00:29:11,298
in the summer of last year. Of the use cases that

448
00:29:11,304 --> 00:29:15,238
Aaron was talking about, the one that we were most concerned about was security

449
00:29:15,324 --> 00:29:18,726
controls validation. And that's the one that we used to jumpstart the

450
00:29:18,748 --> 00:29:22,582
team. So at Cardinal health, we have this really great

451
00:29:22,636 --> 00:29:25,790
security architecture team. So at Cardinal

452
00:29:25,810 --> 00:29:30,102
Health, we have this awesome security architecture team, and they really have two jobs.

453
00:29:30,246 --> 00:29:33,702
So the first is to really be technical visionaries

454
00:29:33,766 --> 00:29:37,782
across the company and anticipate technology trends

455
00:29:37,846 --> 00:29:41,882
and really help the enterprise adopt them. And their second responsibility

456
00:29:41,946 --> 00:29:45,406
is they consult on projects to make sure that they're delivering security

457
00:29:45,508 --> 00:29:48,686
requirements from the get go. But the

458
00:29:48,708 --> 00:29:52,174
challenge that that team has is that the ultimate effectiveness

459
00:29:52,302 --> 00:29:55,806
of their team relies on other teams

460
00:29:55,918 --> 00:29:59,682
making sure that they are actually implementing the security

461
00:29:59,816 --> 00:30:03,426
strategies or requirements or standards

462
00:30:03,458 --> 00:30:07,762
or whatever. And as most of you probably acknowledge,

463
00:30:07,906 --> 00:30:11,894
very rarely do projects execute 100% to

464
00:30:11,932 --> 00:30:15,342
plan. So whether that's unforeseen

465
00:30:15,426 --> 00:30:19,370
technical limitations or project timelines get cut

466
00:30:19,440 --> 00:30:22,810
close, or just even human error,

467
00:30:23,230 --> 00:30:26,810
any of those things could easily undermine the

468
00:30:26,880 --> 00:30:30,858
security standards that security architecture had set forth.

469
00:30:31,034 --> 00:30:34,606
And even if a project was implemented securely from

470
00:30:34,628 --> 00:30:38,554
the start, changes during a system's lifetime could silently

471
00:30:38,602 --> 00:30:41,940
increase the risk to the company, and none of us would even know.

472
00:30:43,030 --> 00:30:47,026
So what Cardinal Health decided was that we needed to move away

473
00:30:47,128 --> 00:30:50,994
from theoretical design based architecture and

474
00:30:51,032 --> 00:30:55,094
move towards what we called applied security, which to

475
00:30:55,132 --> 00:30:58,834
us, that name really meant bringing continuous verification

476
00:30:58,962 --> 00:31:02,306
and validation to Cardinal health so we could validate

477
00:31:02,338 --> 00:31:05,706
our security posture, just like Aaron said, we wanted to

478
00:31:05,728 --> 00:31:09,594
do that when things were good and not when we were scrambling to

479
00:31:09,632 --> 00:31:13,020
try to deal with any sort of negative event.

480
00:31:13,630 --> 00:31:17,702
And we got our name because chaos

481
00:31:17,766 --> 00:31:21,706
engineering was very new at the time. And what we

482
00:31:21,728 --> 00:31:25,502
knew was that when we partnered with other teams, we wanted a jamie that made

483
00:31:25,556 --> 00:31:28,654
sense and to be reflective of the fact that we were

484
00:31:28,692 --> 00:31:32,162
making sure that those security controls were actually

485
00:31:32,296 --> 00:31:35,060
applied. So we called ourselves applied security.

486
00:31:35,910 --> 00:31:39,666
So it was the summer of last year when I joined our information security

487
00:31:39,768 --> 00:31:43,614
team and was set to lead and form

488
00:31:43,672 --> 00:31:47,058
this brand new team focused on security, chaos engineering.

489
00:31:47,234 --> 00:31:50,710
And after the first month, our mission became clear.

490
00:31:50,860 --> 00:31:54,802
We knew that we were supposed to go after the unknown

491
00:31:54,866 --> 00:31:59,142
technical security gaps at the company and really drive those to remediation

492
00:31:59,206 --> 00:32:02,060
before a bad guy found them and exploited them for us.

493
00:32:02,430 --> 00:32:04,922
And we started off as a team of five of us,

494
00:32:05,056 --> 00:32:08,294
and among us, we had expertise in software

495
00:32:08,342 --> 00:32:11,354
development and system design and architecture,

496
00:32:11,402 --> 00:32:14,606
and privacy and risk and networking. So we knew

497
00:32:14,628 --> 00:32:18,874
that collectively, we were equipped to handle our mission. But as exciting

498
00:32:18,922 --> 00:32:21,742
as it would have been to just go nuts and get access to every single

499
00:32:21,796 --> 00:32:25,426
system and start poking and looking for technical gaps, we knew that

500
00:32:25,448 --> 00:32:28,340
we needed a disciplined and a repeatable process.

501
00:32:28,710 --> 00:32:31,794
And when we tried to define what that process was first,

502
00:32:31,832 --> 00:32:34,806
we knew that we had to meet three key needs.

503
00:32:34,988 --> 00:32:38,790
So the first is that whatever technical security gaps we found

504
00:32:38,940 --> 00:32:42,550
they needed to identify, they needed to be

505
00:32:42,700 --> 00:32:45,718
indisputably, critically important ones.

506
00:32:45,884 --> 00:32:49,114
So it did us no good if we just went and we found a whole

507
00:32:49,152 --> 00:32:52,986
bunch of things that ultimately, given the risk, it would be

508
00:32:53,008 --> 00:32:55,210
determined that they weren't worth fixing.

509
00:32:55,710 --> 00:32:59,054
So to give us some credibility, what we

510
00:32:59,092 --> 00:33:03,082
did was we knew that we needed to establish some benchmarks

511
00:33:03,146 --> 00:33:07,210
that the company all agreed to that weren't arbitrary

512
00:33:07,290 --> 00:33:11,326
and weren't just theoretical best practices that everybody

513
00:33:11,428 --> 00:33:15,090
said was just too lofty to achieve. The second

514
00:33:15,160 --> 00:33:19,614
was that we knew that we needed to have a big picture understanding

515
00:33:19,662 --> 00:33:23,426
of what the technical security gap was and just how widespread it

516
00:33:23,448 --> 00:33:27,062
was. Whereas we would have people in the past who

517
00:33:27,116 --> 00:33:30,646
would hypothesize that we have gaps, or they would know

518
00:33:30,748 --> 00:33:34,614
a little bit here or a little bit there. Ultimately, what they

519
00:33:34,652 --> 00:33:38,714
lacked to be able to get these things proactively fixed was

520
00:33:38,832 --> 00:33:42,218
that big picture understanding and not the details that

521
00:33:42,224 --> 00:33:45,418
were necessary to get that documented. And then

522
00:33:45,504 --> 00:33:49,038
last we knew that whatever we found, we wanted to make sure that it actually

523
00:33:49,124 --> 00:33:53,694
stayed closed. This couldn't just be a point in time audit where

524
00:33:53,892 --> 00:33:57,200
these gaps were unknowingly reopened in the future.

525
00:33:58,130 --> 00:34:01,746
So with those goals in mind, we created a process

526
00:34:01,848 --> 00:34:04,686
called continuous verification and validation,

527
00:34:04,798 --> 00:34:07,954
which is on the next slide. And for

528
00:34:07,992 --> 00:34:11,790
short, we call it CBV. So really simply

529
00:34:11,870 --> 00:34:15,334
at its core, we wanted to continuously verify the

530
00:34:15,372 --> 00:34:18,822
presence of those technical security controls and

531
00:34:18,876 --> 00:34:22,870
validate that they were actually implemented correctly.

532
00:34:23,610 --> 00:34:27,498
So the CVV process includes five steps. So step one,

533
00:34:27,584 --> 00:34:30,906
obviously you have to identify the security control that you want

534
00:34:30,928 --> 00:34:34,794
to validate. Step two was that we

535
00:34:34,832 --> 00:34:38,426
needed to identify what those benchmarks were

536
00:34:38,608 --> 00:34:42,046
so that we could identify and detail

537
00:34:42,148 --> 00:34:45,486
what the technical security gaps were, so to give

538
00:34:45,508 --> 00:34:49,134
us some authority. What we typically like to do is we

539
00:34:49,172 --> 00:34:53,006
start with those security standards that are set forth by our security architecture

540
00:34:53,038 --> 00:34:56,386
team and approved by our CISO. And if

541
00:34:56,488 --> 00:34:59,858
those standards don't yet exist, what we do in that case

542
00:34:59,944 --> 00:35:03,138
is we work with our security architecture team and our security

543
00:35:03,224 --> 00:35:06,854
operations teams to get their recommendations, and we

544
00:35:06,892 --> 00:35:09,400
socialize those with relevant teams first.

545
00:35:09,850 --> 00:35:13,126
And what this does is it gives credibility to any of

546
00:35:13,148 --> 00:35:17,286
the gaps that we find, and it really helps us establish the requirement

547
00:35:17,398 --> 00:35:21,814
that either those fixes need to be prioritized and remediated,

548
00:35:21,942 --> 00:35:25,050
or the organization needs to accept the risk.

549
00:35:25,790 --> 00:35:29,762
The third step is that that's the point where we start to actually implement

550
00:35:29,846 --> 00:35:33,626
those continuous checks. And usually what we'll do is we'll

551
00:35:33,658 --> 00:35:37,226
write custom scripts to make those happen, and those scripts

552
00:35:37,258 --> 00:35:40,314
will either test the APIs of some of our tools,

553
00:35:40,442 --> 00:35:45,278
or we'll take a look at some other configurations

554
00:35:45,374 --> 00:35:48,866
as well. But before we write any code, we do try to

555
00:35:48,888 --> 00:35:52,962
make sure that we're making good decisions. We will evaluate if there's anything open

556
00:35:53,016 --> 00:35:57,110
source that we could just port over and use, or if there are commercial options

557
00:35:57,180 --> 00:36:00,866
that will do the job better and faster

558
00:36:01,058 --> 00:36:05,062
for us at the time, especially if we're talking about something where we

559
00:36:05,116 --> 00:36:08,460
have a lot of different vendors that we're keeping in mind.

560
00:36:09,550 --> 00:36:13,578
Step four is that we create a dashboard to show our real time

561
00:36:13,664 --> 00:36:17,498
technical compliance of all of those configurations with the

562
00:36:17,504 --> 00:36:21,054
benchmarks that we've identified. So that dashboard really

563
00:36:21,092 --> 00:36:24,906
serves two purposes. So one is it allows

564
00:36:24,938 --> 00:36:28,906
us to get that big picture of the technical gap so that we can communicate

565
00:36:28,938 --> 00:36:32,206
what our security posture is on demand, and it's

566
00:36:32,238 --> 00:36:35,746
a really good communication tool that we can use when

567
00:36:35,768 --> 00:36:39,522
we're talking to our leaders. And then finally, this is

568
00:36:39,576 --> 00:36:43,122
key. If at any point we find that

569
00:36:43,176 --> 00:36:46,018
our adherence to those benchmarks decreases,

570
00:36:46,194 --> 00:36:49,510
that's when we can create an issue in our risk register.

571
00:36:49,930 --> 00:36:53,734
So cardinal health is very, very lucky that we have a

572
00:36:53,772 --> 00:36:57,786
risk team who has done a ton of work to establish a process

573
00:36:57,888 --> 00:37:01,258
where any technical security gaps that are

574
00:37:01,344 --> 00:37:05,766
identified can be documented and driven to remediation.

575
00:37:05,958 --> 00:37:09,646
So this is awesome for us because it means that our team doesn't have

576
00:37:09,668 --> 00:37:13,342
to be the organizational babysitters who make sure that these things

577
00:37:13,396 --> 00:37:17,114
get done, but we could actually continue CTO, implement CBB

578
00:37:17,162 --> 00:37:20,526
in parallel, knowing that those remediations are,

579
00:37:20,548 --> 00:37:24,706
in fact, going to take place. So I

580
00:37:24,728 --> 00:37:28,078
told you at the beginning, I jamie from a background in software development,

581
00:37:28,174 --> 00:37:31,694
and if you're familiar with software development, what you'll

582
00:37:31,742 --> 00:37:35,106
start to realize is that this continuous verification,

583
00:37:35,218 --> 00:37:39,014
or verification and validation, or CBV process

584
00:37:39,212 --> 00:37:42,882
is really the system's level equivalent

585
00:37:42,946 --> 00:37:46,150
of a regularly scheduled automated test suite.

586
00:37:46,310 --> 00:37:49,482
It's just that instead of verifying that our

587
00:37:49,536 --> 00:37:52,794
code meets the functional requirements set

588
00:37:52,832 --> 00:37:56,842
forth by our business, this kernel health CVV process

589
00:37:56,976 --> 00:38:00,570
makes sure that our systems are meeting the security requirements

590
00:38:00,650 --> 00:38:04,846
set forward by our security architecture team. So let

591
00:38:04,868 --> 00:38:08,234
me geek out just for a little bit longer on the software development analogies,

592
00:38:08,282 --> 00:38:13,086
because they are fantastic. So one

593
00:38:13,108 --> 00:38:15,778
of the things that you could do is you could start to write some security

594
00:38:15,864 --> 00:38:19,646
chaos tests and run them against a non prod environment

595
00:38:19,758 --> 00:38:23,202
that's awaiting promotion to production. So this is

596
00:38:23,256 --> 00:38:26,934
pretty much exactly the same as an automated regression test

597
00:38:26,972 --> 00:38:30,418
suite that you would run prior to deploying new features

598
00:38:30,434 --> 00:38:34,326
or new code to production. And to

599
00:38:34,348 --> 00:38:38,010
take it one step further, where we actually want to go next at cardinal health

600
00:38:38,080 --> 00:38:41,302
is to actually leverage security chaos

601
00:38:41,366 --> 00:38:45,366
engineering as kind of a form of test driven development

602
00:38:45,558 --> 00:38:48,874
where we could actually partner with our security architecture team,

603
00:38:48,912 --> 00:38:52,190
as they're defining our security requirements or setting forward

604
00:38:52,260 --> 00:38:55,322
their standards, we can write those security chaos

605
00:38:55,386 --> 00:38:58,734
tests right then and there, and then see them start CTO

606
00:38:58,772 --> 00:39:02,770
pass, and continuously pass as that project continues.

607
00:39:03,430 --> 00:39:07,266
So the way that I see it, just as

608
00:39:07,288 --> 00:39:10,834
the world of software engineering really first started to embrace the

609
00:39:10,872 --> 00:39:14,418
concept of testing methodologies and instrumenting

610
00:39:14,434 --> 00:39:17,782
for health metrics, the systems engineering world is really

611
00:39:17,836 --> 00:39:21,574
going to do the Jamie. So, as I

612
00:39:21,612 --> 00:39:26,162
see it, security chaos engineering and just normal chaos engineering

613
00:39:26,306 --> 00:39:29,626
are going to become a part of our new normal. They're going to be

614
00:39:29,648 --> 00:39:33,766
baked into our project timelines. They're going to be part of our success criteria.

615
00:39:33,958 --> 00:39:37,066
Just like it was critically important for us to make sure that

616
00:39:37,088 --> 00:39:40,722
our systems are functioning properly and that the health of the systems

617
00:39:40,806 --> 00:39:44,874
is at its peak, that response times are high, it's serving

618
00:39:44,922 --> 00:39:48,846
customer needs, just like we need a pulse on that

619
00:39:49,028 --> 00:39:52,366
constantly. Right now, we know that we need

620
00:39:52,388 --> 00:39:56,146
to get a constant and consistent pulse on the security posture of our

621
00:39:56,168 --> 00:39:59,646
systems as well. So if I had my magic eight ball,

622
00:39:59,678 --> 00:40:02,786
I would be making the prediction that this is going

623
00:40:02,808 --> 00:40:06,614
to become a part of our new normal. And our engineering practices are

624
00:40:06,652 --> 00:40:08,870
really going to reflect that importance.

625
00:40:10,410 --> 00:40:14,246
So, on the next slide, I'm going to start telling some more of

626
00:40:14,268 --> 00:40:18,166
a story here. So, as I mentioned before, I don't

627
00:40:18,198 --> 00:40:22,022
even come from a background of cybersecurity or even site reliability

628
00:40:22,086 --> 00:40:25,834
engineering. But I did spend ten years in

629
00:40:25,872 --> 00:40:29,786
software development, where I ushered new products and new features

630
00:40:29,818 --> 00:40:33,294
into production for two Fortune 20 healthcare companies.

631
00:40:33,492 --> 00:40:35,920
And I loved it. It was awesome.

632
00:40:36,690 --> 00:40:39,998
In an industry that's honestly so ripe for

633
00:40:40,084 --> 00:40:43,682
innovation and transformation, I really believe that my work

634
00:40:43,736 --> 00:40:47,138
was making a difference. But what always

635
00:40:47,224 --> 00:40:50,434
ate at me and what always kind of nagged me in the back of my

636
00:40:50,472 --> 00:40:54,674
mind is that no matter how valuable the product that I stewarded

637
00:40:54,722 --> 00:40:58,246
was, the idea that a data breach or a

638
00:40:58,268 --> 00:41:01,714
DDoS attack could happen, it persisted.

639
00:41:01,842 --> 00:41:04,840
And really, I knew that if that happened,

640
00:41:05,450 --> 00:41:09,050
it just took one incident to threaten to undo all of the awesome

641
00:41:09,120 --> 00:41:11,340
work that I and my team were doing.

642
00:41:11,870 --> 00:41:15,434
And at the time, I only really had vague notions about how

643
00:41:15,472 --> 00:41:18,814
to go about addressing this. And those focused a lot on

644
00:41:18,852 --> 00:41:22,426
things like static code analysis or theoretical threat modeling.

645
00:41:22,458 --> 00:41:27,182
And we talked about why that can be problematic in

646
00:41:27,236 --> 00:41:30,922
building detections, to identify when something

647
00:41:30,996 --> 00:41:34,830
bad was already underway. Like Aaron said, the problem with incident

648
00:41:34,910 --> 00:41:37,650
response is that it's always a response.

649
00:41:38,790 --> 00:41:42,926
It wasn't really until one day at Cardinal Health, at an internal

650
00:41:42,958 --> 00:41:46,550
company function, that I met the director of security architecture,

651
00:41:46,970 --> 00:41:50,246
and we hit it off. And he said, I know we

652
00:41:50,268 --> 00:41:53,746
just met, but I'm starting this new team and I'd

653
00:41:53,778 --> 00:41:57,702
like you to apply. And we spent the rest of the day talking

654
00:41:57,756 --> 00:42:01,654
about security chaos engineering. And that's kind of when I saw the stars

655
00:42:01,702 --> 00:42:04,566
align, if I want to use that cheesy expression.

656
00:42:04,758 --> 00:42:08,446
So I saw kind of the culmination of my desire to

657
00:42:08,468 --> 00:42:11,534
create something that was super valuable and impactful to

658
00:42:11,572 --> 00:42:15,598
healthcare and the ability to proactively ensure that

659
00:42:15,684 --> 00:42:19,386
the company system stayed secure and continued delivering

660
00:42:19,418 --> 00:42:22,594
on that value that we had promised with

661
00:42:22,632 --> 00:42:26,194
the software systems that we had deployed. So,

662
00:42:26,392 --> 00:42:29,566
like I geeked out on you earlier, the parallels between software

663
00:42:29,598 --> 00:42:32,978
engineering and security chaos engineering, or applied security,

664
00:42:33,144 --> 00:42:37,014
were just too profound to ignore. And so the idea of

665
00:42:37,052 --> 00:42:40,790
testing your own security was just so crazy logical,

666
00:42:42,490 --> 00:42:45,858
I can't even articulate that. Just like I

667
00:42:45,884 --> 00:42:49,354
wouldn't ship new features to production without accompanying them

668
00:42:49,392 --> 00:42:53,082
with tests that verify that the code actually

669
00:42:53,136 --> 00:42:56,486
does what it should do. I was on a journey

670
00:42:56,518 --> 00:43:00,442
at Cardinal Health to help a team write continuous

671
00:43:00,506 --> 00:43:04,010
security tests that we could run on a regular basis

672
00:43:04,170 --> 00:43:07,082
to not only ensure the proper functioning of our systems,

673
00:43:07,146 --> 00:43:10,974
but to proactively protect them. And so I like

674
00:43:11,012 --> 00:43:14,466
to tell that story, because there may be some of you in the

675
00:43:14,488 --> 00:43:18,462
audience who either have never heard of security chaos engineering

676
00:43:18,606 --> 00:43:22,226
or those who just don't even know where to start. And the

677
00:43:22,248 --> 00:43:26,102
beauty of security chaos engineering, if you ask me, is not

678
00:43:26,156 --> 00:43:29,750
only is it just such a simple philosophy,

679
00:43:30,090 --> 00:43:33,474
but it's also crazy accessible

680
00:43:33,522 --> 00:43:37,066
to everybody. So you can practice this on

681
00:43:37,088 --> 00:43:40,934
a system that's either a giant monolith or that's radically distributed

682
00:43:40,982 --> 00:43:42,650
with tons of microservices.

683
00:43:43,870 --> 00:43:47,194
So, on the next slide, if you are looking to get

684
00:43:47,232 --> 00:43:50,698
started and you need more of a foundation before you implemented

685
00:43:50,794 --> 00:43:54,190
anything, the good news is, like Erin said at the beginning,

686
00:43:55,810 --> 00:43:59,358
there is an O'Reilly report coming later this year

687
00:43:59,444 --> 00:44:02,990
about the discipline. So the primary authors

688
00:44:03,070 --> 00:44:06,446
are Erin both here, and Kelly Shortridge,

689
00:44:06,478 --> 00:44:10,674
who is the VP of product strategy at capsule eight.

690
00:44:10,872 --> 00:44:14,270
So I'm super excited for this. It's coming together

691
00:44:14,440 --> 00:44:17,634
very well. And in addition to understanding

692
00:44:17,682 --> 00:44:21,122
what the discipline is, it also has real world stories

693
00:44:21,186 --> 00:44:25,254
of security chaos engineering in practice, not only from

694
00:44:25,292 --> 00:44:28,934
me, but from other really smart people at Verica, Google and

695
00:44:28,972 --> 00:44:32,762
others, and even Yuri, who chaos, a session at Comp 42 today,

696
00:44:32,896 --> 00:44:35,610
is one of the contributing authors for that as well.

697
00:44:35,680 --> 00:44:39,098
So please be on the lookout for this. I think it's one of

698
00:44:39,104 --> 00:44:42,414
those things that's going to add a lot of value, especially if you're looking forward

699
00:44:42,532 --> 00:44:44,800
CTO trying to figure out where to get started.

700
00:44:46,130 --> 00:44:49,882
But also what's awesome is that once you have that foundation

701
00:44:49,946 --> 00:44:53,810
and you understand security chaos engineering, it's really

702
00:44:53,880 --> 00:44:57,762
possible to start just insanely small so

703
00:44:57,896 --> 00:45:01,710
you don't have to have this fully automated, super complex,

704
00:45:01,790 --> 00:45:05,206
system wide experiment that requires vp approval to

705
00:45:05,228 --> 00:45:09,238
even get off the ground. So on the next slide, I want to

706
00:45:09,244 --> 00:45:12,070
remind you of Aaron's example with Portslinger.

707
00:45:13,530 --> 00:45:17,706
So here you can actually look at this as

708
00:45:17,808 --> 00:45:21,222
a variety of different individual tests.

709
00:45:21,286 --> 00:45:23,530
And you can see that on the next slide.

710
00:45:24,350 --> 00:45:27,866
So you could break this down and you could say that

711
00:45:27,888 --> 00:45:31,054
you just want to test the config management utility and know

712
00:45:31,092 --> 00:45:34,798
who has access to everything and things like that. Or you

713
00:45:34,804 --> 00:45:39,150
could decide you're just interested in understanding if the alert actually fired.

714
00:45:39,570 --> 00:45:44,466
You could actually start with a manual experiment and you

715
00:45:44,488 --> 00:45:47,874
could build that detection if observability is your main use case,

716
00:45:47,992 --> 00:45:52,034
and build the detection and experiment by just turning off that log source and

717
00:45:52,072 --> 00:45:56,114
actually validate that what you believe is true, which is that that alert

718
00:45:56,162 --> 00:45:58,920
will actually fire you. Validate that it does.

719
00:45:59,850 --> 00:46:03,186
So if you're still struggling

720
00:46:03,218 --> 00:46:06,758
to figure out where you can begin. So first of

721
00:46:06,764 --> 00:46:10,806
all, take a look at it. Do you know any high value, low effort targets

722
00:46:10,838 --> 00:46:14,842
that you could target right now? And if so, fantastic. Start there.

723
00:46:14,896 --> 00:46:18,854
You don't need something that's crazy complex. But if you're

724
00:46:18,902 --> 00:46:23,194
like me, maybe you have a whole bunch of discrete high value testing

725
00:46:23,242 --> 00:46:26,414
possibilities and it's really hard to make sense of them all.

726
00:46:26,612 --> 00:46:30,606
And in that case, what I like to say is that it's okay to

727
00:46:30,628 --> 00:46:34,434
be a little bit selfish and be what I call the right kind

728
00:46:34,472 --> 00:46:37,890
of lazy if you're an engineer. And what I mean by that

729
00:46:37,960 --> 00:46:42,020
is take a look at things that are painful for you or your team.

730
00:46:42,710 --> 00:46:46,694
Are you logging into a system every day to make sure that it's up and

731
00:46:46,732 --> 00:46:50,038
running and healthy and operating the way that you should?

732
00:46:50,204 --> 00:46:53,414
And even were are you doing this on off hours or

733
00:46:53,452 --> 00:46:56,582
weekends? What you can do,

734
00:46:56,636 --> 00:46:59,062
if you want to be what I call that, right, kind of lazy,

735
00:46:59,206 --> 00:47:03,082
is start to automate some of that boring work.

736
00:47:03,216 --> 00:47:06,774
Start to say, you know what, I'm going to continuously

737
00:47:06,822 --> 00:47:10,398
validate these things and I'll sound the alarm if something goes wrong

738
00:47:10,484 --> 00:47:14,046
or doesn't meet that expectation anymore. But the

739
00:47:14,068 --> 00:47:17,614
beauty with security chaos engineering is that with the

740
00:47:17,652 --> 00:47:21,630
high stakes of a security operations environment,

741
00:47:22,370 --> 00:47:25,586
you don't need to add more stress to the plate. You can

742
00:47:25,608 --> 00:47:29,442
actually start to build your confidence that your security is

743
00:47:29,496 --> 00:47:33,186
working even when you aren't and that it will continue to do

744
00:47:33,208 --> 00:47:36,470
so until that alarm fires.

745
00:47:37,050 --> 00:47:40,278
And so, Aaron, back to you. I love the way

746
00:47:40,284 --> 00:47:42,520
you state that. It's a great way to bring it.

747
00:47:43,450 --> 00:47:46,700
So I just have a couple more things to add to it. Is that

748
00:47:47,230 --> 00:47:50,330
chaos engineering? When I started down this path,

749
00:47:52,510 --> 00:47:55,660
I was really focused on the chaos engineering bits itself

750
00:47:56,030 --> 00:47:59,994
and the technique, but really, it's part of a larger domain of knowledge called resilience

751
00:48:00,042 --> 00:48:03,326
engineering. And there's two other domains that are related to it,

752
00:48:03,348 --> 00:48:07,150
which is cognitive systems engineering and safety engineering.

753
00:48:07,570 --> 00:48:12,050
And I believe the path

754
00:48:12,470 --> 00:48:15,970
to righteousness and improvement as a craft lies within

755
00:48:16,040 --> 00:48:19,406
the knowledge we can obtain from those domains. And I encourage

756
00:48:19,438 --> 00:48:23,714
anyone in security to start exploring the different capabilities

757
00:48:23,762 --> 00:48:25,510
and knowledge set in those domains.

758
00:48:28,890 --> 00:48:32,754
And I leave you with this is the case for security chaos

759
00:48:32,802 --> 00:48:35,638
engineering. Jamie and myself,

760
00:48:35,724 --> 00:48:39,640
Kelly, and a slew of other authors will be

761
00:48:40,890 --> 00:48:44,722
after this book is released in the fall, we will begin writing

762
00:48:44,786 --> 00:48:47,222
the. Well, we brought a lot of it already,

763
00:48:47,356 --> 00:48:50,766
but we begin writing the official animal book on the

764
00:48:50,788 --> 00:48:54,446
topic. So we look forward to anyone who does

765
00:48:54,468 --> 00:48:57,982
this to reach out to us. We'd love to hear your story and

766
00:48:58,116 --> 00:49:01,966
get you involved with the community. Absolutely. Thank you for everything,

767
00:49:02,068 --> 00:49:04,046
everyone. Have a great day. Thank you, everyone.

