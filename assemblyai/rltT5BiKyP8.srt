1
00:01:28,370 --> 00:01:32,422
Hi, and welcome to this talk today on

2
00:01:32,476 --> 00:01:36,486
observability and some of the data science that you may need to consider when

3
00:01:36,508 --> 00:01:39,946
you're looking at an observability solution. My name is Dave

4
00:01:40,018 --> 00:01:43,854
McAllister and I'd like to thank 42 for letting me come and share some

5
00:01:43,892 --> 00:01:47,194
of my observations about living with observability,

6
00:01:47,322 --> 00:01:49,470
especially in production systems.

7
00:01:50,130 --> 00:01:54,446
I currently work for Nginx, the open source web server

8
00:01:54,638 --> 00:01:58,210
and reverse proxy model. I focus on

9
00:01:58,280 --> 00:02:02,418
a lot of outreach. I've been open source geek from way back, actually before

10
00:02:02,504 --> 00:02:05,814
it was even named open source, and have continued through

11
00:02:05,852 --> 00:02:09,302
that period of time for quite a while

12
00:02:09,356 --> 00:02:13,222
now. So with that, let's get started on talking about

13
00:02:13,276 --> 00:02:16,966
some of the data side. Well, first of all,

14
00:02:17,068 --> 00:02:20,522
let's point out that quite often you hear a number of

15
00:02:20,576 --> 00:02:23,626
rules or laws that fall into place.

16
00:02:23,808 --> 00:02:27,546
The 22 immutable laws of marketing by

17
00:02:27,728 --> 00:02:30,986
trout and rays for has the twelve immutable rules

18
00:02:31,018 --> 00:02:35,050
for observability, or the 70 maxims of effectively

19
00:02:35,130 --> 00:02:38,686
maximally effective mercenaries, which is really quite hard to

20
00:02:38,708 --> 00:02:42,826
say. And of course, some of these laws are enforced a little harder

21
00:02:42,858 --> 00:02:46,334
than others, such as the law of gravity is pretty strictly enforced

22
00:02:46,382 --> 00:02:50,146
for this. But we're going to talk about just a couple of rules when

23
00:02:50,168 --> 00:02:53,300
it comes to observability. So let's get started.

24
00:02:53,990 --> 00:02:57,382
Well, first we need to sort of talk about

25
00:02:57,516 --> 00:03:01,062
why we're even interested in this space. And the short

26
00:03:01,116 --> 00:03:04,834
answer is that we're all on this cloud native journey, and we're

27
00:03:04,882 --> 00:03:08,710
designed that way to be able to increase the philosophy,

28
00:03:08,790 --> 00:03:12,090
the way things are transforming, keeping up with customer

29
00:03:12,160 --> 00:03:15,946
demands, outpacing our competition. We need to be agile and

30
00:03:15,968 --> 00:03:19,446
we need to be responsive. This has rapidly

31
00:03:19,558 --> 00:03:23,214
accelerated, especially over the last couple of years, where we've had even

32
00:03:23,252 --> 00:03:27,294
more of an online presence in mind. And more

33
00:03:27,332 --> 00:03:30,494
and more, we are engaging our users and

34
00:03:30,532 --> 00:03:34,434
our customers through these digital channels. So every customer is

35
00:03:34,472 --> 00:03:38,590
on this cloud native journey here, whether their goal is migration,

36
00:03:38,750 --> 00:03:41,854
modernization, new app development,

37
00:03:41,982 --> 00:03:44,954
the companies are doing to increase that velocity,

38
00:03:45,102 --> 00:03:48,920
and they're finding that cloud native technologies help them do so,

39
00:03:51,130 --> 00:03:54,722
but could increases complexity.

40
00:03:54,866 --> 00:03:58,198
There are more things to monitor, if you will.

41
00:03:58,284 --> 00:04:02,262
Cloud is an enabler of transformation, but how do you maximize

42
00:04:02,326 --> 00:04:05,718
this investment you're making in cloud, and make sure you have visibility

43
00:04:05,814 --> 00:04:09,866
into everything that's happening in that environment. So the

44
00:04:09,888 --> 00:04:13,758
farther along you are in adopting kubernetes and using

45
00:04:13,844 --> 00:04:18,106
microservices, looking at service meshes or other cloud native technologies

46
00:04:18,218 --> 00:04:21,802
that don't even fit on this slide, there's thousands of potentials.

47
00:04:21,946 --> 00:04:25,642
It's really hard to have the visibility into everything that's happening.

48
00:04:25,796 --> 00:04:29,534
You can't just monitor a monolithic stack. You can't just look at the flashing

49
00:04:29,582 --> 00:04:32,494
lights on the server. You've got to look at the hybrid,

50
00:04:32,542 --> 00:04:36,054
the ephemeral, the abstracted infrastructure. And this

51
00:04:36,092 --> 00:04:38,920
becomes very hard to manage and understand.

52
00:04:39,370 --> 00:04:43,186
And so that visibility, what we call observability,

53
00:04:43,298 --> 00:04:46,562
says is table stakes. It is the basis

54
00:04:46,626 --> 00:04:50,102
that we need to make sure that we are modernizing successfully

55
00:04:50,166 --> 00:04:54,026
and efficiently. And because of

56
00:04:54,048 --> 00:04:57,500
that, while we've been monitoring for quite some time,

57
00:04:57,870 --> 00:05:01,730
we have to make sure that monitoring involves to use usability,

58
00:05:01,830 --> 00:05:05,406
observability here. The reason is that we need

59
00:05:05,428 --> 00:05:07,978
to be able to look at these new complexities.

60
00:05:08,154 --> 00:05:12,494
Okay, you've heard of observability, obviously, but what

61
00:05:12,532 --> 00:05:15,726
is it? Well, it's the unique data that allows

62
00:05:15,758 --> 00:05:19,630
us to understand just what the heck's going on in these apps and infrastructures.

63
00:05:19,790 --> 00:05:22,658
And it's a proxy for our customer experience,

64
00:05:22,824 --> 00:05:26,086
which we've learned are more important than ever.

65
00:05:26,268 --> 00:05:29,702
So monitoring kind of keeps an eye on things that we

66
00:05:29,756 --> 00:05:34,418
know could fail, or the known knowns. And observable

67
00:05:34,594 --> 00:05:38,314
systems allow teams to look at issues that can

68
00:05:38,352 --> 00:05:42,074
occur, including unknown failures and

69
00:05:42,112 --> 00:05:46,026
failures with root causes that are buried somewhere in a giant field

70
00:05:46,208 --> 00:05:50,118
of maze passageways that all are identical for here,

71
00:05:50,224 --> 00:05:54,814
it alerts us, ask questions that we didn't think of before

72
00:05:55,012 --> 00:05:58,302
it goes wrong. So real ability goes

73
00:05:58,356 --> 00:06:01,434
beyond monitoring to detect these unexpected

74
00:06:01,482 --> 00:06:05,730
failure conditions, as well as making sure that we have the

75
00:06:05,880 --> 00:06:08,994
data behind the scenes to be able to

76
00:06:09,032 --> 00:06:12,722
figure out what went wrong. And so getting

77
00:06:12,776 --> 00:06:15,794
these insights, taking those reactions, are something traditional.

78
00:06:15,842 --> 00:06:19,874
Monitoring tools, and Floyd products in particular, really weren't designed

79
00:06:19,922 --> 00:06:24,626
to handle. So keep in mind that observability and monitoring

80
00:06:24,738 --> 00:06:28,746
work hand in hand. Observability is the data. It allows us

81
00:06:28,768 --> 00:06:32,086
to find things that are unexpecting. Monitoring keeps an eye

82
00:06:32,118 --> 00:06:36,614
on things that we know can go wrong. They both work closely

83
00:06:36,662 --> 00:06:40,702
together. But observability is

84
00:06:40,756 --> 00:06:44,254
built for certain challenges. Here and over here,

85
00:06:44,292 --> 00:06:47,866
you're looking at what's called the Knievan framework. And hat tip

86
00:06:47,898 --> 00:06:51,806
to Kevin Brockhoff, particularly for this drawing here.

87
00:06:51,988 --> 00:06:55,346
When we look at a monolith, we're in a simple environment. You look at it,

88
00:06:55,368 --> 00:06:58,434
you see what's going on, and this is the best practice sense.

89
00:06:58,472 --> 00:07:01,522
Categorize and respond. It's broken. Fix it,

90
00:07:01,576 --> 00:07:05,114
if you will. But we've added two new dimensions.

91
00:07:05,262 --> 00:07:08,834
One, we've made things more complicated. We've added

92
00:07:08,882 --> 00:07:12,898
microservices. We've added loosely coupled communications

93
00:07:12,994 --> 00:07:16,326
pathways. We're building these things so that there are

94
00:07:16,348 --> 00:07:19,990
lots of building blocks, and those building blocks may interact

95
00:07:20,070 --> 00:07:23,194
in very different and complicated ways.

96
00:07:23,392 --> 00:07:26,982
Likewise, at the same time, we've added another totally

97
00:07:27,046 --> 00:07:30,406
different environment, which is the ephemeral or the

98
00:07:30,448 --> 00:07:33,690
elastic approach here. Now things scale

99
00:07:33,770 --> 00:07:37,326
as needed and go away. So when

100
00:07:37,348 --> 00:07:41,258
you go to look for something, it may no longer be there, it's ephemeral.

101
00:07:41,354 --> 00:07:44,686
Think serverless, for instance. A serverless function does its job and

102
00:07:44,708 --> 00:07:47,858
gets out of the way here. And so putting those two together, we end up

103
00:07:47,864 --> 00:07:51,394
in a complex environment. Now, complex environments are not

104
00:07:51,432 --> 00:07:55,054
just sense, it's not passive. We want to actually

105
00:07:55,192 --> 00:07:58,690
probe into what's going on inside of this environment,

106
00:07:58,850 --> 00:08:02,486
then we sense what's going on and respond to it.

107
00:08:02,668 --> 00:08:06,582
Our could native world lives in this complex world,

108
00:08:06,636 --> 00:08:09,814
this emergent world of functionality. And so

109
00:08:09,852 --> 00:08:13,226
when we look through all these certain pieces, it's very clear that we

110
00:08:13,248 --> 00:08:16,438
have some very unique capabilities.

111
00:08:16,614 --> 00:08:20,054
Traditional monitoring cannot save us alone here.

112
00:08:20,192 --> 00:08:23,438
Multitenancy can be really painful inside of here.

113
00:08:23,524 --> 00:08:27,038
And honestly, failures don't exactly

114
00:08:27,124 --> 00:08:30,926
repeat. In fact, it's quite unusual for a failure to repeat exactly the

115
00:08:30,948 --> 00:08:34,222
same way. But of course, observability,

116
00:08:34,286 --> 00:08:37,150
and you've heard this before, better visibility,

117
00:08:37,310 --> 00:08:40,994
precise alerting, and end to end causality. So we know

118
00:08:41,032 --> 00:08:45,006
exactly what happened, where it happened, why, what the system believes

119
00:08:45,038 --> 00:08:48,146
was happening, when it happens, which gives us a reduced

120
00:08:48,178 --> 00:08:51,570
meantime to clue and a meantime to resolution

121
00:08:51,730 --> 00:08:55,174
here. So when this happens, though, we also

122
00:08:55,212 --> 00:08:59,106
can get answers when something goes wrong. Be proactive in adding

123
00:08:59,138 --> 00:09:02,618
that to the monitoring stack so that we can watch for it in the

124
00:09:02,624 --> 00:09:05,946
future. But this is all

125
00:09:06,048 --> 00:09:09,386
data intensive, and the amount of data that's flowing into our

126
00:09:09,408 --> 00:09:13,246
systems now is massively increased. There's just so

127
00:09:13,268 --> 00:09:17,658
much more data coming into that between looking at metrics,

128
00:09:17,754 --> 00:09:21,374
looking at distributed traces, and looking at logs, which make up those

129
00:09:21,412 --> 00:09:25,106
three categories. So rule one, use all of

130
00:09:25,128 --> 00:09:28,530
your data to avoid blind spots. And I kind of mentioned

131
00:09:28,600 --> 00:09:32,354
this, but observability really is a data problem. We have a

132
00:09:32,392 --> 00:09:36,002
ton of data that's coming into here. Generally speaking,

133
00:09:36,056 --> 00:09:39,266
we look at three classes of data and observability metrics,

134
00:09:39,298 --> 00:09:42,566
traces and logs. Or do I have a problem? Where is

135
00:09:42,588 --> 00:09:45,718
the problem, why is the problem happening? And each of

136
00:09:45,724 --> 00:09:48,300
these pieces becomes equally important.

137
00:09:48,670 --> 00:09:51,210
They all actually overlap.

138
00:09:51,550 --> 00:09:55,398
So metrics can yield, or logs can yield metrics,

139
00:09:55,494 --> 00:09:58,634
traces can yield metrics. Metrics can point you at the right

140
00:09:58,672 --> 00:10:02,586
trace of the right log. And each of these pieces gives us additional information for

141
00:10:02,608 --> 00:10:06,014
the monitoring and assist in our recognition of issues to help

142
00:10:06,052 --> 00:10:09,134
finding that underlying root causes here today,

143
00:10:09,252 --> 00:10:13,006
we generate tons of data. I've got hundreds of services that

144
00:10:13,028 --> 00:10:16,066
are calling it here. Every transaction generates orders of

145
00:10:16,088 --> 00:10:19,134
kilobytes of metadata about that transaction.

146
00:10:19,262 --> 00:10:22,686
Multiply that for a number of small number of concurrencies,

147
00:10:22,798 --> 00:10:26,846
and you can suddenly have megabytes per second or 300gb

148
00:10:26,878 --> 00:10:31,426
per day. That's coming in for a single concise

149
00:10:31,538 --> 00:10:35,078
application space. You need all this data to be

150
00:10:35,084 --> 00:10:38,582
able to make, to inform your decision making. In fact,

151
00:10:38,636 --> 00:10:41,898
yeah, it can be a terabytes of data, but you

152
00:10:41,984 --> 00:10:46,010
need to be able to be aware of where your data limitations

153
00:10:46,350 --> 00:10:49,900
are, and your data decisions can make you have problems.

154
00:10:52,210 --> 00:10:55,758
So going forward here,

155
00:10:55,844 --> 00:10:59,054
data is this driving factor, and it drives things like our

156
00:10:59,092 --> 00:11:03,422
new artificial intelligence and machine learning directed troubleshooting.

157
00:11:03,566 --> 00:11:07,630
It also has this thing called cardinality. So cardinality

158
00:11:07,790 --> 00:11:10,606
is the ability to slice and dice.

159
00:11:10,718 --> 00:11:15,022
So it's not just, I got a metric that says my cpus

160
00:11:15,086 --> 00:11:19,190
are running at 78%. It can be broken down into

161
00:11:19,340 --> 00:11:22,614
that this specific cpu is running at this,

162
00:11:22,652 --> 00:11:26,214
or even this specific core, or this core running in this

163
00:11:26,252 --> 00:11:30,230
virtual machine on this particular infrastructure environment

164
00:11:30,310 --> 00:11:34,154
is running in this. We also are now living with streaming data.

165
00:11:34,272 --> 00:11:38,234
Batch just doesn't cut it when we're looking at

166
00:11:38,432 --> 00:11:41,930
analyzing our data for this. And we want

167
00:11:42,000 --> 00:11:45,946
as much data, full fidelity metrics and as much information

168
00:11:46,048 --> 00:11:49,438
our traces we can get, while at the same point in time.

169
00:11:49,524 --> 00:11:54,098
We also want it to be standard space and open source available

170
00:11:54,184 --> 00:11:57,410
so that we're never locked into those various pieces.

171
00:11:59,030 --> 00:12:02,834
But what happens if you miss ephemeral data? What happens

172
00:12:02,872 --> 00:12:06,466
if the data that you need is no longer available, or here,

173
00:12:06,568 --> 00:12:09,718
or your data shows something that you can't drill into?

174
00:12:09,884 --> 00:12:12,040
Well, there's some interesting things.

175
00:12:13,130 --> 00:12:16,534
First of all, let's take a look at this massive amount

176
00:12:16,572 --> 00:12:20,570
of data coming in here. And quite often you'll hear people talk about that.

177
00:12:20,640 --> 00:12:24,470
Data in mass is noise.

178
00:12:24,630 --> 00:12:28,282
Well, and there are ways of dealing with that noise here and in this

179
00:12:28,336 --> 00:12:32,218
observability space, they really come down to either filtering

180
00:12:32,234 --> 00:12:35,614
the signals, which can be linear, which smooths it,

181
00:12:35,812 --> 00:12:38,606
destroys sharp edges around here, blind pass,

182
00:12:38,708 --> 00:12:41,760
which removes the outliers for this,

183
00:12:42,130 --> 00:12:45,422
or even smart aggregations pieces.

184
00:12:45,566 --> 00:12:48,974
We'll go a little bit more into sampling. And sampling can be random.

185
00:12:49,022 --> 00:12:53,102
Grab one headbase, tail, base, post predictive dimensionality

186
00:12:53,166 --> 00:12:56,466
reductions. But the real answer is improving the

187
00:12:56,488 --> 00:13:00,086
visualization. Use the data, but improve the visualization so

188
00:13:00,108 --> 00:13:03,586
that you're not lost in the complexity of the data and that you can drill

189
00:13:03,618 --> 00:13:05,800
the data into what you need it to be.

190
00:13:07,130 --> 00:13:10,538
So I mentioned sampling. So let's talk a

191
00:13:10,544 --> 00:13:13,750
little bit about sampling. So trace sampling

192
00:13:13,830 --> 00:13:18,742
is a common approach to doing this. Traces are noisy.

193
00:13:18,806 --> 00:13:22,410
They produce a tremendous amount of data because the trace is actually

194
00:13:22,480 --> 00:13:26,526
looking at your request as it moves through your system. Especially if

195
00:13:26,548 --> 00:13:30,202
you move from a front end environment, your client's laptop,

196
00:13:30,266 --> 00:13:34,574
for example, all the way through the back end, through the various infrastructure

197
00:13:34,622 --> 00:13:38,242
pieces, there can be a huge amount of data here. And so quite

198
00:13:38,296 --> 00:13:42,482
often you see people do trace sampling, and this

199
00:13:42,616 --> 00:13:45,974
routinely misses troubleshooting edge cases. It can

200
00:13:46,012 --> 00:13:49,542
also miss intermittent failures because it's not

201
00:13:49,596 --> 00:13:54,258
being sampled correctly here. If you don't understand the service dependencies

202
00:13:54,354 --> 00:13:57,886
in a microservices environment, in particular, you can get alert stars

203
00:13:57,938 --> 00:14:01,482
because several things may fail because of

204
00:14:01,536 --> 00:14:07,770
one point. You also need to be able to look at not

205
00:14:07,920 --> 00:14:11,866
being simplistic on your triage. So you're seeing an

206
00:14:11,888 --> 00:14:15,566
error happen. You need the data to be able to determine where the error is

207
00:14:15,588 --> 00:14:19,102
actually occurring. And so if you have too

208
00:14:19,156 --> 00:14:22,638
little data, then you end up with a simple approach to this and you

209
00:14:22,644 --> 00:14:26,334
may find yourself spending a lot of time actually looking for where the underlying

210
00:14:26,382 --> 00:14:30,114
cause lives here. And then if you're just looking

211
00:14:30,152 --> 00:14:33,874
at observability as sampling, you're separating your application

212
00:14:33,992 --> 00:14:38,422
environment and your infrastructure environment and has, we all know those

213
00:14:38,476 --> 00:14:42,710
two pieces work pretty closely together and can have impacts

214
00:14:43,770 --> 00:14:47,570
beyond the traditional oh, I'm out of space,

215
00:14:47,660 --> 00:14:51,446
I'm out of cpus. But it can even have things such as noisy

216
00:14:51,478 --> 00:14:54,938
neighbor problems or communications bandwidth problems.

217
00:14:55,104 --> 00:14:59,290
So all these things become a problem with observability sampling.

218
00:15:00,190 --> 00:15:04,206
So here we're looking at a typical tray sampling. And there

219
00:15:04,228 --> 00:15:07,470
are two types of things here. There's headbase and tail base.

220
00:15:07,620 --> 00:15:11,486
No one in the right mind really totally goes after some

221
00:15:11,508 --> 00:15:15,738
of these others. It's easier not to do this than looking at dimensionality

222
00:15:15,834 --> 00:15:19,106
collapse. So, head base sample, let's look at the

223
00:15:19,128 --> 00:15:21,954
sample and then decide whether to keep it or not.

224
00:15:22,152 --> 00:15:26,218
And so the head based sample catches things that are outliers

225
00:15:26,334 --> 00:15:29,730
and can catch things that, you know, are errors.

226
00:15:29,890 --> 00:15:33,862
However, the problem is that a trace may not lead

227
00:15:33,916 --> 00:15:38,162
to an obvious known error. Remember, we're looking at unknown unknowns.

228
00:15:38,306 --> 00:15:41,914
And so when we do this, all of a sudden a head based

229
00:15:41,952 --> 00:15:45,382
sample, while reducing the data flow, may remove

230
00:15:45,446 --> 00:15:48,746
that single sample. That would answer your question.

231
00:15:48,928 --> 00:15:51,818
When you're looking into the underlying causes here,

232
00:15:51,904 --> 00:15:55,854
tail based sampling, which is a little more random. Okay, I've got 100

233
00:15:55,892 --> 00:15:59,710
traces. I'm going to save these. Ten is a little

234
00:15:59,860 --> 00:16:03,618
worse at this. So think about it. 100 traces. I'm going to save

235
00:16:03,704 --> 00:16:07,394
five. That means I have a 95% chance of

236
00:16:07,432 --> 00:16:11,026
missing that outlier. So head based sampling catches my

237
00:16:11,048 --> 00:16:14,818
outliers, catches my known errors. Tail based sampling may

238
00:16:14,904 --> 00:16:18,610
catch it, but probably misses it, depending on how much you sample.

239
00:16:18,770 --> 00:16:22,726
But when we start pulling all the data into place, we've got

240
00:16:22,748 --> 00:16:25,974
the ability to go back and reconstruct, if you will,

241
00:16:26,012 --> 00:16:30,618
the scene of the crime. So another example.

242
00:16:30,784 --> 00:16:34,106
So this is sampling, and in the first instance here,

243
00:16:34,288 --> 00:16:37,674
data one is sampled, data two, no sampling obvious for

244
00:16:37,712 --> 00:16:41,246
this, and they are running the same environment. They are different times

245
00:16:41,348 --> 00:16:45,370
for this. So in sample one, I'm actually pulling

246
00:16:45,530 --> 00:16:49,454
a sample rate that's only looking at pieces of

247
00:16:49,492 --> 00:16:53,662
this sampling is giving me a selection bias. It's not

248
00:16:53,716 --> 00:16:57,026
showing me necessarily things that are out of range for this.

249
00:16:57,128 --> 00:17:01,122
So in this particular case, sampling came back and said that my duration for

250
00:17:01,256 --> 00:17:04,802
my traces is in the one to two second range, and that

251
00:17:04,856 --> 00:17:08,646
even that high end range, I've got one trace that fell outside of

252
00:17:08,668 --> 00:17:12,582
that. I'm sure you can simply look at the

253
00:17:12,636 --> 00:17:16,770
data there and start getting some interesting error indications

254
00:17:16,850 --> 00:17:20,486
about how much is actually not being looked at. Take the

255
00:17:20,508 --> 00:17:23,994
exact same thing where we're not sampling, this is showing the

256
00:17:24,032 --> 00:17:27,674
same application and impact. And all of a sudden you can start

257
00:17:27,712 --> 00:17:31,930
seeing some of those errors creeping into here. Now, my latency distribution

258
00:17:32,090 --> 00:17:37,182
is showing that I have a trace that's in the 29 to 42nd

259
00:17:37,316 --> 00:17:40,858
range. In other words, this is causing

260
00:17:40,874 --> 00:17:44,586
you to have a blind spot. And in an ecommerce world,

261
00:17:44,708 --> 00:17:48,642
29 seconds is known as a lost sale. I've given

262
00:17:48,696 --> 00:17:53,374
up purchases for less time than that while waiting for them. But nonetheless,

263
00:17:53,502 --> 00:17:56,902
no sampling is giving me better results and a better

264
00:17:56,956 --> 00:18:00,390
understanding of my user experience than the sampling environments.

265
00:18:01,530 --> 00:18:04,918
But I can clear you right now. Wait, my metrics tell me everything,

266
00:18:05,084 --> 00:18:09,094
and that can be true. Your metrics are not usually

267
00:18:09,212 --> 00:18:12,338
sampled, particularly for your infrastructure.

268
00:18:12,514 --> 00:18:15,482
They may still end up being sampled for your tracing data,

269
00:18:15,536 --> 00:18:18,874
because tracing data is, in its own right,

270
00:18:19,072 --> 00:18:22,942
ephemeral. It stops when the request stops, as well

271
00:18:22,996 --> 00:18:26,394
as massive. And it comes in not necessarily

272
00:18:26,442 --> 00:18:30,314
in a metrics basis. It comes in, in a form that you can extract metrics

273
00:18:30,362 --> 00:18:34,500
from, usually rate, error duration, or red monitoring here

274
00:18:35,430 --> 00:18:39,170
leading to this problem is that you start

275
00:18:39,240 --> 00:18:43,346
missing those duration results and you can even miss

276
00:18:43,448 --> 00:18:47,302
some of the alert structures less so if you're looking at a metric space

277
00:18:47,356 --> 00:18:50,482
and you're capturing all the metrics data that's inside your traces.

278
00:18:50,626 --> 00:18:54,306
But your duration data may be impacted

279
00:18:54,418 --> 00:18:58,218
by seeing this. After all, when does a trace end

280
00:18:58,304 --> 00:19:02,522
is one of those always interesting questions. So one

281
00:19:02,576 --> 00:19:05,702
whole third of this observability space resides on tracing

282
00:19:05,766 --> 00:19:09,610
data. Here, duration is probably the single most important

283
00:19:09,680 --> 00:19:13,790
element for a user experience to be looking at

284
00:19:13,860 --> 00:19:15,440
inside that trace data.

285
00:19:16,530 --> 00:19:20,714
So, TLDR, your ability

286
00:19:20,762 --> 00:19:24,114
to use observability is dependent on your data source, and your

287
00:19:24,152 --> 00:19:27,854
data source needs to be as complexity as possible. But don't

288
00:19:27,902 --> 00:19:31,166
let the chosen data bias your results.

289
00:19:31,278 --> 00:19:34,898
Don't force yourself into selection bias before you have a

290
00:19:34,904 --> 00:19:37,862
chance to understand what selection bias is meaning to you,

291
00:19:37,996 --> 00:19:41,446
and keep it all, understand it all. Otherwise you

292
00:19:41,468 --> 00:19:45,126
don't track customer happiness as a proxy here. And finally,

293
00:19:45,308 --> 00:19:49,114
getting the data in real time matters because we need to know when things

294
00:19:49,152 --> 00:19:52,474
go wrong as soon as they possibly can go wrong.

295
00:19:52,672 --> 00:19:56,586
And we'll find out more about that when

296
00:19:56,608 --> 00:20:00,378
we hit rule two. So rule two states,

297
00:20:00,464 --> 00:20:04,142
very simply, operate at the speed and resolution of your app

298
00:20:04,196 --> 00:20:07,386
and infrastructure. And again, this starts falling

299
00:20:07,418 --> 00:20:11,086
into a data problem. But we're going to start by talking about

300
00:20:11,268 --> 00:20:15,250
this thing. Some of you are probably familiar with what's known as the von Neumann

301
00:20:16,870 --> 00:20:20,354
Bottleneck and the blind Newman bottleneck, very simply is the idea

302
00:20:20,392 --> 00:20:24,226
that computer system's throughput is limited due to

303
00:20:24,408 --> 00:20:27,734
relative ability of processors to the top

304
00:20:27,772 --> 00:20:31,606
rates of data transfer. And so our cpus have

305
00:20:31,628 --> 00:20:35,090
gotten a whole lot faster. Our memory is not that much faster,

306
00:20:35,250 --> 00:20:38,646
so the processor can sit idle while it's waiting

307
00:20:38,678 --> 00:20:42,010
for memory to be accessed. And yeah, there are ways around

308
00:20:42,080 --> 00:20:45,914
that, but this is a fairly standard model

309
00:20:46,112 --> 00:20:49,862
for computer systems today, is the

310
00:20:49,936 --> 00:20:53,774
memory and the cpu are basically

311
00:20:53,892 --> 00:20:57,440
competing for a bottleneck of transfer.

312
00:20:58,050 --> 00:21:01,278
When we look at that from observability, there's a similar

313
00:21:01,364 --> 00:21:04,418
issue here, a little different,

314
00:21:04,504 --> 00:21:07,826
but the resolution of our data, the precision and

315
00:21:07,848 --> 00:21:11,774
accuracy coming in is massive. The speed

316
00:21:11,822 --> 00:21:15,670
of our data is the deterministic response. How fast

317
00:21:15,740 --> 00:21:19,254
we can get things in is usually

318
00:21:19,372 --> 00:21:23,174
less than the resolution of our data, and that impacts the way we

319
00:21:23,212 --> 00:21:26,342
aggregate, analyze and visualize this data.

320
00:21:26,476 --> 00:21:30,154
And honestly, by the way, data is pretty worthless unless you can do that

321
00:21:30,192 --> 00:21:32,490
aggregation, analysis and visualization.

322
00:21:33,950 --> 00:21:38,534
And that resolution and speed together impact

323
00:21:38,582 --> 00:21:42,094
the insights you get from the data that's coming

324
00:21:42,132 --> 00:21:45,790
in. Pretty straightforward, makes perfect

325
00:21:45,860 --> 00:21:49,262
sense. But when you start looking at this at volume is where

326
00:21:49,316 --> 00:21:52,878
life gets interesting. So I've mentioned

327
00:21:52,964 --> 00:21:56,578
precision and accuracy, and we need to talk a little bit about this.

328
00:21:56,744 --> 00:21:59,874
We discuss these things as if they were the same,

329
00:22:00,072 --> 00:22:03,234
but quite often we need to understand that

330
00:22:03,272 --> 00:22:06,850
they're not. Accuracy states that the measure

331
00:22:06,930 --> 00:22:10,546
is correct. Precision says it's

332
00:22:10,578 --> 00:22:14,214
consistent with the other measurements. So if I measure the

333
00:22:14,252 --> 00:22:17,814
cpu each time I measure the cpu, I want the

334
00:22:17,852 --> 00:22:22,242
number percentage of its in use category

335
00:22:22,386 --> 00:22:26,138
to be precise. I mean accurate. I want

336
00:22:26,144 --> 00:22:29,722
it to tell me what's actually happening. It's running

337
00:22:29,776 --> 00:22:32,758
at 17%. Is it running at 78%?

338
00:22:32,944 --> 00:22:36,634
I also want it to tell me when the cpu

339
00:22:36,682 --> 00:22:40,094
is in the same state. Give me that number again.

340
00:22:40,212 --> 00:22:43,646
So if it looks at this and says it's 17, now it's

341
00:22:43,678 --> 00:22:47,522
78, now it's back to 17. I want to trust that that

342
00:22:47,656 --> 00:22:51,474
is actually consistent with the measurements that are seeing here.

343
00:22:51,672 --> 00:22:54,962
Aggregation and analysis can actually

344
00:22:55,016 --> 00:22:58,390
skew our precision and accuracy categories.

345
00:22:58,730 --> 00:23:01,750
So here's another example.

346
00:23:01,900 --> 00:23:05,414
And what this is, is looking at the number of requests per second going

347
00:23:05,452 --> 00:23:08,666
through a system. So this one is not duration based. This is literally the number

348
00:23:08,688 --> 00:23:12,394
of requests going through here. And as you can see over the first

349
00:23:12,432 --> 00:23:16,780
ten second viewpoint, my aggregations are

350
00:23:17,550 --> 00:23:20,650
going to happen. If I look at a ten second

351
00:23:20,720 --> 00:23:24,186
aggregation, my average is 13.9 and my 95th

352
00:23:24,218 --> 00:23:27,758
percentile is 27.5. If I look at

353
00:23:27,764 --> 00:23:31,902
just the first 5 seconds, I have a 16.4 and my 2nd

354
00:23:32,036 --> 00:23:34,260
5 seconds I has an 11.4.

355
00:23:34,870 --> 00:23:38,674
Both of these miss the fact that one of my

356
00:23:38,712 --> 00:23:42,114
measurements has gone over 30 requests per

357
00:23:42,152 --> 00:23:45,234
second, which happens to be where I had set the

358
00:23:45,272 --> 00:23:48,614
alert. Now again, easy enough to say bring

359
00:23:48,652 --> 00:23:53,026
in the data, show me things when it crosses 30, alert me. But your aggregation

360
00:23:53,138 --> 00:23:57,094
may never show that. The aggregation may show you at under 20,

361
00:23:57,212 --> 00:24:01,146
nice safe number in all these categories. And so when

362
00:24:01,168 --> 00:24:05,530
you start looking at that functionality, you can see that aggregation

363
00:24:05,950 --> 00:24:08,940
can change that aggregation. So,

364
00:24:09,790 --> 00:24:13,310
simple example, but now multiply this times the

365
00:24:13,380 --> 00:24:17,102
hundreds of thousands of requests that can be rolling through your system at any given

366
00:24:17,156 --> 00:24:20,240
moment. So,

367
00:24:20,610 --> 00:24:24,798
data resolution, the speed at which we pull data in and reporting resolution

368
00:24:24,974 --> 00:24:28,210
are never, well, not never,

369
00:24:28,360 --> 00:24:32,146
seldom ever going to be the same thing here. They both can

370
00:24:32,168 --> 00:24:36,078
be problematic. So throwing away data points

371
00:24:36,264 --> 00:24:39,366
means that you can't go back

372
00:24:39,388 --> 00:24:42,806
and reconstruct. So always deliver those data points regardless of

373
00:24:42,828 --> 00:24:46,690
what your reporting structure looks like here. And the finer your granularity

374
00:24:46,770 --> 00:24:50,726
is, the more you can have potential precision, the more you're likely

375
00:24:50,758 --> 00:24:54,714
to get the same number for the same results. So this is a

376
00:24:54,752 --> 00:24:58,218
simple little kind of drawing. When you measure something,

377
00:24:58,304 --> 00:25:02,762
when you measure it, it's actually in the middle of your granularity

378
00:25:02,826 --> 00:25:05,966
point somewhere, but you're not quite sure where.

379
00:25:06,068 --> 00:25:09,534
So in things case, if I'm looking at that, I'm measuring on

380
00:25:09,572 --> 00:25:13,470
a second by second basis where it falls

381
00:25:13,630 --> 00:25:18,194
relative to that second boundary, varies by the size

382
00:25:18,312 --> 00:25:22,206
of that boundary. So milliseconds give me finer granularity,

383
00:25:22,398 --> 00:25:26,786
and picoseconds give me finer granularity. That granularity

384
00:25:26,818 --> 00:25:30,194
may become more and more important, especially as we scale

385
00:25:30,242 --> 00:25:32,898
things and especially as things get complex,

386
00:25:33,074 --> 00:25:36,710
but at the same point in time, it creates more

387
00:25:36,780 --> 00:25:40,178
data. So there's a trade off between granularity,

388
00:25:40,274 --> 00:25:43,546
data resolution, porting resolution that you need to

389
00:25:43,568 --> 00:25:46,762
consider. And when we bring this in,

390
00:25:46,816 --> 00:25:50,794
we have native resolution. How often do we collect data

391
00:25:50,912 --> 00:25:54,574
and chart resolution, which is the aggregation points that

392
00:25:54,612 --> 00:25:58,206
we use. So in this particular case here, I can be

393
00:25:58,228 --> 00:26:02,282
showing you that I'm bringing in requests per second, but I'm actually aggregating

394
00:26:02,346 --> 00:26:05,870
these on a ten second basis. And so

395
00:26:05,940 --> 00:26:09,186
we want to be able to look at this so they make sense. We get

396
00:26:09,208 --> 00:26:11,890
to watch things move, we can look for this.

397
00:26:11,960 --> 00:26:15,298
Humans are incredibly good at pattern matching, and so we need to be

398
00:26:15,304 --> 00:26:18,710
able to break it down and take a look at what's going on here.

399
00:26:18,860 --> 00:26:22,562
So, native resolution, data collection, chart resolution,

400
00:26:22,626 --> 00:26:25,506
the aggregation that we're using for our charts and graphs,

401
00:26:25,618 --> 00:26:29,574
we want speedy data collection and sufficient chart

402
00:26:29,622 --> 00:26:32,860
resolution so that we can understand what's going on.

403
00:26:34,270 --> 00:26:37,626
Add to that complexity. We talked a little about

404
00:26:37,728 --> 00:26:41,280
this complexity. We now have compute cloud

405
00:26:42,530 --> 00:26:46,158
elasticity, cloud compute elasticity. Spin up more

406
00:26:46,244 --> 00:26:49,674
kubernetes, spin up more, spin down more serverless

407
00:26:49,722 --> 00:26:52,814
functionality, pull in the functions when they need them,

408
00:26:53,012 --> 00:26:56,738
and have those pieces happen. That's the ephemeral side of this.

409
00:26:56,824 --> 00:27:00,290
But we can also have drift and SKU. We are now running in

410
00:27:00,360 --> 00:27:04,158
infrastructure that is not all single based. These are virtual

411
00:27:04,174 --> 00:27:07,950
machines in general in a cloud environment, and we're running wherever,

412
00:27:08,030 --> 00:27:11,686
we actually don't kind of know where we're running inside of here. So when

413
00:27:11,708 --> 00:27:14,982
we start bringing data in to aggregate it,

414
00:27:15,116 --> 00:27:18,870
we can have drift and SKU. And so we need to kind of measure

415
00:27:19,030 --> 00:27:22,698
how far ahead or behind the data

416
00:27:22,784 --> 00:27:26,954
source coming in is. Behind where we currently are.

417
00:27:27,072 --> 00:27:30,950
Drift is a continual something, is showing it becoming faster and faster.

418
00:27:31,030 --> 00:27:34,926
SKU is something that's just out of line with everything else.

419
00:27:35,028 --> 00:27:38,606
And keep in mind, this is not just the computer infrastructure, it can

420
00:27:38,628 --> 00:27:42,282
also be the networking infrastructure. So it's worthwhile

421
00:27:42,346 --> 00:27:45,778
looking for ways to manage drift and SKU so that we understand

422
00:27:45,864 --> 00:27:49,858
the functionality. Easiest way, once again, is to keep all the stupid data

423
00:27:49,944 --> 00:27:53,730
so that you can keep track of where your drifts and skus are handling.

424
00:27:55,190 --> 00:27:58,406
And when we get into this data, we really want to

425
00:27:58,428 --> 00:28:01,670
understand one basic construct.

426
00:28:02,330 --> 00:28:06,726
Most of what we do for the observability and monitoring space in particular is

427
00:28:06,828 --> 00:28:09,746
be predictive on alerts, safety too,

428
00:28:09,868 --> 00:28:12,650
or have an alert based on something.

429
00:28:12,800 --> 00:28:15,942
But predictions are data intensive.

430
00:28:16,006 --> 00:28:19,258
Again here, if something's stationary, straight line.

431
00:28:19,344 --> 00:28:22,826
Oh, yeah, piece of cake. We know what the distribution curve is going to look

432
00:28:22,848 --> 00:28:26,078
like. Yeah, you can set a strategic threshold. If not,

433
00:28:26,164 --> 00:28:29,402
we look for sudden change. If we have a linear trend,

434
00:28:29,466 --> 00:28:32,506
things are always going to be sloping up. And if they don't,

435
00:28:32,698 --> 00:28:36,466
then all of a sudden we want to do this, then we can start looking

436
00:28:36,488 --> 00:28:40,514
for things like resources running out. We're approaching the end of our

437
00:28:40,552 --> 00:28:43,966
block of capabilities for this. If it's seasonal,

438
00:28:43,998 --> 00:28:48,706
we need to look at historical anomalies and historic anomaly

439
00:28:48,898 --> 00:28:52,358
can have some interesting impacts. Here's one where I'm going

440
00:28:52,364 --> 00:28:56,262
to say, don't use the mean, use the median for here. And then

441
00:28:56,396 --> 00:29:00,266
all these pieces give us the ability to do some

442
00:29:00,368 --> 00:29:02,890
level of predicting and alerting.

443
00:29:04,830 --> 00:29:08,454
Sorry. So what we really want to know is things predictive

444
00:29:08,502 --> 00:29:12,282
behavior. We want to know what's coming. We not necessarily really

445
00:29:12,416 --> 00:29:16,062
care as much about what's happened. We can usually

446
00:29:16,116 --> 00:29:19,534
track that, but we want to know what's coming as well. Prediction is

447
00:29:19,572 --> 00:29:22,574
only good as that. Precision and accuracy. Can you trust the data?

448
00:29:22,612 --> 00:29:26,082
And is the data telling you the truth? Right here.

449
00:29:26,216 --> 00:29:30,290
When we look at this, at the historical change environments,

450
00:29:31,190 --> 00:29:34,002
we want to make sure that we're using the right thing.

451
00:29:34,136 --> 00:29:37,718
So a sudden change basically says, I was running along fine and all of

452
00:29:37,724 --> 00:29:41,000
a sudden I've got 70% more demand coming in,

453
00:29:41,450 --> 00:29:44,866
let me know. Historical says on Tuesdays

454
00:29:44,978 --> 00:29:48,662
at noon, my workload drops down

455
00:29:48,716 --> 00:29:52,026
so I can shut things down. But at 01:00 it comes back

456
00:29:52,048 --> 00:29:54,986
up. So let me plan to bring things back up.

457
00:29:55,088 --> 00:29:58,934
Oh, look, this Tuesday at 12:00 the workload didn't

458
00:29:58,982 --> 00:30:02,918
fall off for that. And of course, trend lines being stationary.

459
00:30:03,094 --> 00:30:06,814
In any case, with predictive behavior, you can expect to see some

460
00:30:06,852 --> 00:30:10,686
level of false positives as well as false negatives inside of here.

461
00:30:10,868 --> 00:30:14,130
Does it make sense to compare current signals to the observed value

462
00:30:14,200 --> 00:30:18,130
last week? Or could values from the preceding hour

463
00:30:18,200 --> 00:30:22,082
make a better baseline? Sort of that historic change

464
00:30:22,136 --> 00:30:25,314
versus historic anomaly versus

465
00:30:25,362 --> 00:30:29,506
sudden change structures. However, whenever you're

466
00:30:29,538 --> 00:30:32,662
looking at this, particularly at historic data,

467
00:30:32,796 --> 00:30:36,454
use the median. The means will float up and down. Should you

468
00:30:36,492 --> 00:30:39,834
have outliers, the medians pretty much have

469
00:30:39,872 --> 00:30:42,700
a much more comfortable range that they stay in.

470
00:30:43,310 --> 00:30:47,226
So again, summing up observability is only as

471
00:30:47,248 --> 00:30:50,894
useful as your data's precision and accuracy. If you can't trust it and

472
00:30:50,932 --> 00:30:54,030
can't trust it each time, it's worthless here.

473
00:30:54,180 --> 00:30:57,786
And you need to consider the elastic, ephemeral,

474
00:30:57,818 --> 00:31:02,522
and SKU aspects of these complex infrastructural

475
00:31:02,586 --> 00:31:05,918
environments here. And while we look at prediction has a target,

476
00:31:06,014 --> 00:31:10,702
we need to keep in mind that there's a difference between extrapolation and interpolation,

477
00:31:10,846 --> 00:31:14,354
and keep in mind that in any case we may end

478
00:31:14,392 --> 00:31:17,430
up with false positives or false negatives.

479
00:31:18,170 --> 00:31:22,134
And finally, a closing thought here. The most effective debugging tool

480
00:31:22,172 --> 00:31:25,346
is still careful thought coupled with judiciously praised print

481
00:31:25,378 --> 00:31:29,126
statements. Brian Kernegan said this back in 1979

482
00:31:29,228 --> 00:31:32,534
in Unix for beginners, observability is

483
00:31:32,572 --> 00:31:36,374
the new print statement. And with

484
00:31:36,412 --> 00:31:40,206
that, thanks for listening to me today. Thanks for letting me come to the

485
00:31:40,228 --> 00:31:43,850
Conf 42 conference and enjoy the rest of your conference.

