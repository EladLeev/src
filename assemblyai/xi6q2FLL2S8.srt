1
00:00:25,410 --> 00:00:29,542
You event driven architectures

2
00:00:29,596 --> 00:00:33,254
problem do they actually solve? Hi and welcome to Conf 42.

3
00:00:33,292 --> 00:00:36,674
I'm Bogdan Sucaciu and together we'll demystify event

4
00:00:36,722 --> 00:00:40,626
driven architectures with Apache Kafka. I've been working with event

5
00:00:40,658 --> 00:00:44,054
driven architectures a couple of years now. Currently I'm a tech

6
00:00:44,092 --> 00:00:48,454
lead at Deloitte Digital, a digital consulting firm. And probably some

7
00:00:48,492 --> 00:00:52,362
of you may know me from plural website where I teach about various subjects,

8
00:00:52,426 --> 00:00:55,822
event driven systems being one of them. Speaking about event

9
00:00:55,876 --> 00:00:59,534
driven, this is the subject that we're going to be focusing on today.

10
00:00:59,652 --> 00:01:03,294
My goal is rather simple, to change your perspective about it.

11
00:01:03,332 --> 00:01:06,478
For a long time they have been marketed as the solution

12
00:01:06,574 --> 00:01:09,630
for real time or near real time systems.

13
00:01:09,710 --> 00:01:12,962
But is it true though? I mean, we already have real time

14
00:01:13,016 --> 00:01:16,670
systems that are not based on events. We create a new user,

15
00:01:16,750 --> 00:01:19,974
we make can order, and so on. It's not like

16
00:01:20,012 --> 00:01:23,410
I'm going to create a new user, go and grab a coffee,

17
00:01:23,490 --> 00:01:26,774
and then 5 minutes later I can come back and finish my

18
00:01:26,812 --> 00:01:30,854
order. In fact, only a couple of use cases really require event

19
00:01:30,972 --> 00:01:34,282
architectures, and most of them are related to real time

20
00:01:34,336 --> 00:01:37,482
analytics, fraud detection, sensor readings and

21
00:01:37,536 --> 00:01:40,842
so on. But still, event driven architectures,

22
00:01:40,896 --> 00:01:44,366
something else to the table, something much more powerful than you

23
00:01:44,388 --> 00:01:47,838
expected. And to illustrate all of this, I'm going to use

24
00:01:47,924 --> 00:01:51,914
Apache Kafka. Actually, I'm not going to be using on Kafka

25
00:01:51,962 --> 00:01:55,294
that much. If you've already used Kafka, there is quite a big

26
00:01:55,332 --> 00:01:58,370
chance that you won't learn anything new today. But still,

27
00:01:58,440 --> 00:02:02,258
Kafka is an important piece of technology in the event driven world.

28
00:02:02,344 --> 00:02:06,094
Why? Well, I'm going to drop some clues during the session regarding

29
00:02:06,142 --> 00:02:10,630
why Kafka makes such a great option for event driven architectures.

30
00:02:11,370 --> 00:02:14,902
Without further delay, let's get started. And what better

31
00:02:14,956 --> 00:02:19,062
way to start with than one of the most powerful architecture patterns?

32
00:02:19,206 --> 00:02:22,314
Demonolit. Yeah, you heard me right.

33
00:02:22,432 --> 00:02:25,958
Demonolit is indeed an extremely powerful pattern due

34
00:02:25,974 --> 00:02:29,466
to its simplicity. Basically, we have a

35
00:02:29,488 --> 00:02:32,782
bunch of lines of code, and all that code is running

36
00:02:32,836 --> 00:02:36,526
on a single process. Also, we often have a

37
00:02:36,548 --> 00:02:40,222
database attached to it in order to store various data.

38
00:02:40,356 --> 00:02:43,810
Most of the time is some sort of a relational database.

39
00:02:44,230 --> 00:02:47,342
However, it has quite a major drawback,

40
00:02:47,486 --> 00:02:50,594
scalability. And I'm not talking just from

41
00:02:50,632 --> 00:02:54,210
an operations perspective like virtually scaling a process,

42
00:02:54,360 --> 00:02:57,778
but I'm also talking from a team perspective.

43
00:02:57,954 --> 00:03:01,542
I think we've all been there endless conversations on

44
00:03:01,596 --> 00:03:05,286
collaboration apps about basically changing the color of

45
00:03:05,308 --> 00:03:08,598
a button to pink. So working with monolith

46
00:03:08,694 --> 00:03:12,474
is not really scalable or fun. So how can we

47
00:03:12,512 --> 00:03:15,846
fix this problem? Well, some very smart

48
00:03:15,878 --> 00:03:19,322
people came with the idea, what if we take this

49
00:03:19,376 --> 00:03:22,800
big application and split it into multiple services?

50
00:03:23,330 --> 00:03:27,514
It first started with SoA service oriented architecture,

51
00:03:27,642 --> 00:03:30,810
but it was quite tricky to determine the right bounds

52
00:03:30,890 --> 00:03:34,286
for the applications. So this tendency

53
00:03:34,398 --> 00:03:38,530
naturally evolved into what we call today microservices.

54
00:03:38,950 --> 00:03:42,894
Each microservice gets a different piece of the business domain.

55
00:03:43,022 --> 00:03:46,882
This way separate teams can work on separate applications,

56
00:03:47,026 --> 00:03:50,562
allowing to scale down or up the number of teams

57
00:03:50,626 --> 00:03:54,246
working on these product. Exactly the same concept can

58
00:03:54,268 --> 00:03:57,442
be applied to infrastructure. We can scale down

59
00:03:57,516 --> 00:04:01,046
or up the number of instances only for the applications

60
00:04:01,078 --> 00:04:04,774
we need to. However, there is a slight concerning

61
00:04:04,822 --> 00:04:08,102
issue. As we all know, in software

62
00:04:08,166 --> 00:04:11,726
engineering we don't have good or bad, right or

63
00:04:11,748 --> 00:04:15,390
wrong, we have tradeoffs. So by using

64
00:04:15,460 --> 00:04:18,894
microservices, we now have to solve a different set

65
00:04:18,932 --> 00:04:22,240
of problems related to data access.

66
00:04:22,950 --> 00:04:27,086
A lot of times microservices sound very utopic,

67
00:04:27,198 --> 00:04:30,478
like each microservice can evolve independently

68
00:04:30,654 --> 00:04:34,222
without having the need to coordinate with another microservice.

69
00:04:34,366 --> 00:04:38,114
But I think we've all been there. Practice doesn't

70
00:04:38,162 --> 00:04:42,070
transpose 100% from theory. Most of the time

71
00:04:42,140 --> 00:04:46,210
we're using synchronous APIs such as rest or RPC calls

72
00:04:46,290 --> 00:04:50,170
to exchange data between our services. Let's say that

73
00:04:50,240 --> 00:04:53,722
one of our users makes a request to one of our

74
00:04:53,776 --> 00:04:57,782
services. From there we have an entire cascade

75
00:04:57,846 --> 00:05:01,854
of other calls that need to be done from one single rest

76
00:05:01,892 --> 00:05:05,902
call. We may end up with 20 or even 30 other

77
00:05:05,956 --> 00:05:09,230
calls. This happens because even

78
00:05:09,300 --> 00:05:12,702
if each microservice owns a single piece of the business

79
00:05:12,756 --> 00:05:16,674
domain, we will always need data from other domains to get

80
00:05:16,712 --> 00:05:20,258
the full picture. Let's say we want to make a payment

81
00:05:20,344 --> 00:05:24,606
transaction. It's not like we only need the credit card details.

82
00:05:24,718 --> 00:05:28,630
No, we also need the name of our user or even his

83
00:05:28,700 --> 00:05:31,878
address. And that is usually stored by a different

84
00:05:31,964 --> 00:05:36,226
microservice. So in order to get the full picture,

85
00:05:36,338 --> 00:05:39,770
we always need to somehow compose it by calling different

86
00:05:39,840 --> 00:05:42,570
microservices from our architectures.

87
00:05:43,070 --> 00:05:46,826
Sounds easy, but by doing this we are diving into a

88
00:05:46,848 --> 00:05:50,770
new set of challenges, such as distributed transactions,

89
00:05:50,950 --> 00:05:53,722
timeouts, dependency management,

90
00:05:53,866 --> 00:05:57,178
coupling or even data ownership.

91
00:05:57,354 --> 00:06:00,734
These challenges and many more others have been

92
00:06:00,772 --> 00:06:04,386
introduced by microservices and by the fact that

93
00:06:04,408 --> 00:06:08,450
we are now using distributed systems to perform our tasks.

94
00:06:08,950 --> 00:06:12,082
And that's what event event driven architectures really

95
00:06:12,136 --> 00:06:16,210
good at. They can easily solve distributed systems problems

96
00:06:16,360 --> 00:06:19,294
by introducing new concepts and patterns.

97
00:06:19,422 --> 00:06:23,174
But first, what exactly is an event driven architectures this

98
00:06:23,212 --> 00:06:27,266
is these main talking point of my presentation and I haven't even introduced

99
00:06:27,298 --> 00:06:31,226
it yet. Well, I was curious too, so I went

100
00:06:31,248 --> 00:06:35,606
to Wikipedia and I found this event. Event driven

101
00:06:35,718 --> 00:06:39,494
architectures shortly said EDA is a software architecture

102
00:06:39,542 --> 00:06:42,074
paradigm promoting production,

103
00:06:42,202 --> 00:06:46,014
consumption of and reaction of events. There are

104
00:06:46,052 --> 00:06:49,130
some interesting words being thrown in there, production,

105
00:06:49,210 --> 00:06:51,982
consumption, reaction and so on.

106
00:06:52,116 --> 00:06:55,460
So let's find out what they actually mean.

107
00:06:55,830 --> 00:07:00,066
To do that, we need to go back to our microservices to

108
00:07:00,088 --> 00:07:03,380
keep things simple. I'm going to pick only four services,

109
00:07:03,750 --> 00:07:07,574
blue, gray, purple and red. To make

110
00:07:07,612 --> 00:07:11,654
things interesting. The blue service is using reSt APIs to

111
00:07:11,692 --> 00:07:15,254
communicate with the purple and red services, and an

112
00:07:15,292 --> 00:07:18,460
RPC call to communicate with the gray service.

113
00:07:19,230 --> 00:07:22,394
Rest APIs are based on a request response type

114
00:07:22,432 --> 00:07:25,878
of communication, whereas with RPC calls

115
00:07:25,974 --> 00:07:29,622
we don't always get back a response. This flow

116
00:07:29,686 --> 00:07:33,626
can also be represented in another way using unidirectional

117
00:07:33,658 --> 00:07:37,258
flow of data. Now the blue service calls

118
00:07:37,274 --> 00:07:40,766
the gray service and the flow ends. But when the

119
00:07:40,788 --> 00:07:43,818
blue service calls the purples and the red services,

120
00:07:44,004 --> 00:07:47,538
the flow continues and they will provide some sort of data

121
00:07:47,704 --> 00:07:51,426
back to the blue service. This is interesting,

122
00:07:51,528 --> 00:07:55,242
but we still haven't solved any of the problems mentioned earlier.

123
00:07:55,406 --> 00:07:59,606
To solve them, we need another piece of the puzzle called

124
00:07:59,708 --> 00:08:03,750
event stream. The trick is quite simple.

125
00:08:03,900 --> 00:08:06,594
We now have two types of applications,

126
00:08:06,722 --> 00:08:10,202
producers and consumers. Producers send

127
00:08:10,256 --> 00:08:13,834
data to an event streams and consumers retrieve it

128
00:08:13,872 --> 00:08:17,466
from the same event stream. What's interesting is the

129
00:08:17,488 --> 00:08:21,254
fact that an application can be both a producer and

130
00:08:21,312 --> 00:08:24,766
a consumer at the same time. Just like the purple and

131
00:08:24,788 --> 00:08:28,890
the red services, these consume data from the blue event stream

132
00:08:28,970 --> 00:08:32,254
and produce it into another. Now, if we

133
00:08:32,292 --> 00:08:35,790
isolate only the producers, we can immediately notice

134
00:08:35,870 --> 00:08:39,742
that there is only one producer type per event stream.

135
00:08:39,886 --> 00:08:43,038
The blue service produces on the blue event stream,

136
00:08:43,134 --> 00:08:46,498
the purple service produces on the purple event stream,

137
00:08:46,594 --> 00:08:50,374
and so on. You should definitely keep this in mind when

138
00:08:50,412 --> 00:08:53,926
you're designing your event driven architectures. You could,

139
00:08:54,028 --> 00:08:57,346
for example, use multiple producer types to produce

140
00:08:57,378 --> 00:09:00,682
to one event stream, but that will complicate a lot.

141
00:09:00,736 --> 00:09:04,314
The consuming process. Then if we take a look

142
00:09:04,352 --> 00:09:08,422
over the consuming part, we can notice that multiple consumers

143
00:09:08,486 --> 00:09:11,790
can retrieve data from the same event stream.

144
00:09:12,290 --> 00:09:15,918
Also, one consumer is allowed to consume data

145
00:09:16,004 --> 00:09:19,614
from multiple event streams. So consumers are

146
00:09:19,652 --> 00:09:23,262
much more flexible than producers in terms of connection

147
00:09:23,326 --> 00:09:26,974
to different event streams. And by introducing

148
00:09:27,022 --> 00:09:30,382
only one concept, we actually tackled multiple

149
00:09:30,446 --> 00:09:34,066
issues. First of all, our applications are no

150
00:09:34,088 --> 00:09:37,606
longer coupled since the event stream is meant to serve as a

151
00:09:37,628 --> 00:09:41,702
decoupling factor and then data ownership is

152
00:09:41,756 --> 00:09:45,654
already simplified. Each event stream stores one type of data

153
00:09:45,772 --> 00:09:49,338
and any interesting party can easily consume it.

154
00:09:49,504 --> 00:09:52,646
But what exactly is an event stream?

155
00:09:52,838 --> 00:09:56,650
Well, an event stream can be represented by many things,

156
00:09:56,800 --> 00:09:59,842
such as logs, topics, queues,

157
00:10:00,006 --> 00:10:03,310
websockets or even reactive APIs.

158
00:10:03,810 --> 00:10:06,910
Now I know that some of you may be thinking,

159
00:10:07,060 --> 00:10:10,750
hey, how can a websocket be considered an event stream?

160
00:10:11,170 --> 00:10:15,090
Well, if you come to think of it, websockets or even

161
00:10:15,160 --> 00:10:18,610
reactive APIs are a bit more special scenario.

162
00:10:19,350 --> 00:10:23,186
The event stream and a producing application have merged together

163
00:10:23,288 --> 00:10:27,874
into a single entity. But if you think about the previous deductions,

164
00:10:28,002 --> 00:10:30,840
we still have only one producing application,

165
00:10:31,290 --> 00:10:34,406
and multiple consumers can connect to the

166
00:10:34,428 --> 00:10:37,850
same event stream. The only difference between

167
00:10:37,920 --> 00:10:41,242
a reactive API and a topic is that the

168
00:10:41,296 --> 00:10:44,694
event stream and a producing application are two separate

169
00:10:44,742 --> 00:10:48,966
entities. In fact, anything that you can asynchronously

170
00:10:48,998 --> 00:10:52,766
subscribe to can be can event stream. Any tool,

171
00:10:52,868 --> 00:10:56,826
library or protocol that can produce data in an asynchronous

172
00:10:56,858 --> 00:11:00,106
manner can be considered an event stream.

173
00:11:00,298 --> 00:11:03,970
These are, however, some fundamental differences between

174
00:11:04,040 --> 00:11:07,122
these event streams types. For example,

175
00:11:07,256 --> 00:11:10,690
queues, websockets and reactive APIs treat

176
00:11:10,760 --> 00:11:14,210
event differently compared to topics and logs.

177
00:11:14,550 --> 00:11:17,922
While using the first category, we can only react

178
00:11:17,986 --> 00:11:21,590
to events, we consume them and then they are gone

179
00:11:21,740 --> 00:11:25,494
with no option of getting them back. Topics and

180
00:11:25,532 --> 00:11:29,546
logs, on the other hand, can persist those events and we

181
00:11:29,568 --> 00:11:33,258
can replay them later on. This pattern is a

182
00:11:33,264 --> 00:11:36,700
bit more powerful and we'll see why a bit later.

183
00:11:37,230 --> 00:11:41,206
An important thing to remember is that kafka uses topics as

184
00:11:41,248 --> 00:11:44,746
event streams, and under the hood it uses logs

185
00:11:44,778 --> 00:11:48,202
to persist events. This is one of the main reasons

186
00:11:48,266 --> 00:11:52,394
Kafka is such can excellent choice for building event driven

187
00:11:52,442 --> 00:11:55,954
applications. Of course, Kafka is

188
00:11:55,992 --> 00:11:59,026
not the only system that allows this. These are plenty of

189
00:11:59,048 --> 00:12:02,894
other examples such as AWS kinesis or Apache

190
00:12:02,942 --> 00:12:07,198
Pulser, which work in a similar fashion. But Kafka

191
00:12:07,294 --> 00:12:10,594
has another ace under its sleeve, something that

192
00:12:10,632 --> 00:12:14,338
kinesis and pulser aren't that good at, which we'll

193
00:12:14,354 --> 00:12:15,800
see in just a few minutes.

194
00:12:16,890 --> 00:12:20,486
So in Kafka's world, event streams are represented

195
00:12:20,518 --> 00:12:24,102
by Kafka topics residing on Kafka brokers

196
00:12:24,246 --> 00:12:27,846
producers connect to the Kafka broker in order to produce

197
00:12:27,878 --> 00:12:31,150
data, and consumers have to do the same thing

198
00:12:31,220 --> 00:12:35,146
in order to consume it. This brings some implications

199
00:12:35,258 --> 00:12:38,560
to our way. We exchange data throughout the system.

200
00:12:39,090 --> 00:12:42,974
Firstly, we're now dealing with something called inversion of

201
00:12:43,012 --> 00:12:46,130
control, and it's quite a simple pattern.

202
00:12:46,550 --> 00:12:49,746
While using the classic rest API approach, it is the

203
00:12:49,768 --> 00:12:53,486
job of the producer to send these data to the consumer.

204
00:12:53,678 --> 00:12:56,578
Now, the producer doesn't care anymore.

205
00:12:56,674 --> 00:13:00,738
Its only job is to produce data to an event stream.

206
00:13:00,914 --> 00:13:04,854
Actually, it's the consumer's job to retrieve the data by

207
00:13:04,892 --> 00:13:08,746
subscribing to the event stream. So we are inverting the

208
00:13:08,768 --> 00:13:12,566
control from the producer to the consumer. By empowering

209
00:13:12,598 --> 00:13:16,620
the consumer to retrieve the data that it is interested in,

210
00:13:17,070 --> 00:13:20,794
then we have can out behavior. Multiple consumers

211
00:13:20,842 --> 00:13:24,670
can consume data produced by only one producer.

212
00:13:25,410 --> 00:13:28,782
Talking about producers, it is recommended to have

213
00:13:28,836 --> 00:13:32,010
only one producer type per event stream.

214
00:13:32,170 --> 00:13:36,162
Just to make things clear, I'm referring to the producer type and not

215
00:13:36,216 --> 00:13:39,710
producer instance. Multiple producer instances

216
00:13:39,790 --> 00:13:43,026
can produce to the same event stream as long as they

217
00:13:43,048 --> 00:13:46,342
are on the same type. And finally,

218
00:13:46,476 --> 00:13:51,106
producers don't really know consumers. From an application perspective,

219
00:13:51,218 --> 00:13:54,342
a producer is like hey, my only

220
00:13:54,396 --> 00:13:57,378
job is to produce data to this event stream,

221
00:13:57,474 --> 00:14:00,998
but I don't know who is going to consume it and frankly,

222
00:14:01,094 --> 00:14:04,666
I don't even care. Consumers have the

223
00:14:04,688 --> 00:14:08,042
same mindset. These don't know who is producing this data,

224
00:14:08,176 --> 00:14:11,786
they only care about their connection to the event stream.

225
00:14:11,978 --> 00:14:15,882
However, if we zoom out a bit from an architecture

226
00:14:15,946 --> 00:14:20,126
perspective, consumers and producers are well known in

227
00:14:20,148 --> 00:14:23,250
order to get the full picture of the business flow.

228
00:14:23,910 --> 00:14:27,822
So going back to microservices, our main challenge

229
00:14:27,886 --> 00:14:31,042
was data access. By adopting an

230
00:14:31,096 --> 00:14:34,722
event driven architectures, we're actually solving it.

231
00:14:34,856 --> 00:14:38,374
Our data is now really easy to access.

232
00:14:38,572 --> 00:14:42,450
We just have to make our consumers subscribe to an event stream

233
00:14:42,530 --> 00:14:46,454
and that's it. It's much easier to subscribe to one

234
00:14:46,492 --> 00:14:49,818
or more event streams rather than making 20

235
00:14:49,904 --> 00:14:53,162
or 30 API calls. But still,

236
00:14:53,296 --> 00:14:56,806
just as I mentioned previously, in software engineering,

237
00:14:56,918 --> 00:15:00,342
we don't have right or wrong, we have tradeoffs.

238
00:15:00,486 --> 00:15:04,042
And the tradeoff for event architectures

239
00:15:04,186 --> 00:15:08,522
is consistency. While using synchronous APIs

240
00:15:08,666 --> 00:15:12,270
we have that strong consistency, and we're always sure

241
00:15:12,340 --> 00:15:16,542
that we're getting the latest version of our data in

242
00:15:16,596 --> 00:15:19,794
event driven architectures no longer have

243
00:15:19,832 --> 00:15:24,450
that. Instead, we have something called eventual consistency.

244
00:15:25,370 --> 00:15:28,482
In fact, the key to building successful

245
00:15:28,546 --> 00:15:31,990
event driven architecture is understanding and

246
00:15:32,060 --> 00:15:34,710
accepting eventual consistency.

247
00:15:35,370 --> 00:15:38,886
We can't just simply take a synchronous rest API

248
00:15:38,998 --> 00:15:43,078
and then transpose it in an event driven way. New patterns

249
00:15:43,174 --> 00:15:46,442
require new ways of thinking, so such

250
00:15:46,496 --> 00:15:50,830
attempts may fail due to the mismatch between these patterns.

251
00:15:51,170 --> 00:15:54,330
In fact, not all flows are meant to be event driven.

252
00:15:54,410 --> 00:15:59,002
In the real world, we will always find a mix between synchronous APIs

253
00:15:59,146 --> 00:16:02,878
and eventing patterns. Now,

254
00:16:02,964 --> 00:16:06,658
I've babbled a lot about events, but I haven't really said

255
00:16:06,744 --> 00:16:10,386
what an event really is. Well, to do that

256
00:16:10,488 --> 00:16:14,462
we need to have a look of our communication types between systems

257
00:16:14,606 --> 00:16:18,294
and the best way to start with is a message.

258
00:16:18,492 --> 00:16:22,454
A message is just some data being exchanged between two

259
00:16:22,492 --> 00:16:26,086
different services a and b. A message

260
00:16:26,188 --> 00:16:29,546
has a form, a body, but it doesn't say how the

261
00:16:29,568 --> 00:16:32,826
data is being transferred. That's why we

262
00:16:32,848 --> 00:16:36,186
need some more concrete definitions. The types of

263
00:16:36,208 --> 00:16:40,618
messages can be determined based on when these action resulted

264
00:16:40,714 --> 00:16:44,814
from the data transfer is happening. The first one

265
00:16:44,932 --> 00:16:48,158
is command a. Command will always

266
00:16:48,244 --> 00:16:51,742
happen sometime in the future. Even if we

267
00:16:51,796 --> 00:16:54,994
initiate the action now, the result of it will

268
00:16:55,032 --> 00:16:57,300
be perceived sometime in the future.

269
00:16:57,830 --> 00:17:01,342
Also, a command is a directed instruction,

270
00:17:01,486 --> 00:17:05,022
meaning that we know exactly who we are communicating

271
00:17:05,086 --> 00:17:08,518
with. An example will be rest calls service

272
00:17:08,604 --> 00:17:12,214
a, calls service b. A very important thing

273
00:17:12,252 --> 00:17:16,162
to know is that the fact that we don't always receive a response

274
00:17:16,226 --> 00:17:19,258
while using the command pattern. Sometimes we do,

275
00:17:19,344 --> 00:17:23,194
sometimes we don't. The opposite of a command is the

276
00:17:23,232 --> 00:17:26,854
query. Now, our service does not give instructions

277
00:17:26,902 --> 00:17:29,978
to another, but it actually requests some data,

278
00:17:30,144 --> 00:17:33,870
just like trying to request a database or a search engine.

279
00:17:34,020 --> 00:17:37,646
Queries happen in the present because we are requesting the

280
00:17:37,668 --> 00:17:40,606
current state of our data. Also,

281
00:17:40,788 --> 00:17:45,140
one important fact is that we are always getting a response back.

282
00:17:45,990 --> 00:17:49,794
Finally, we have events. Events are

283
00:17:49,832 --> 00:17:53,710
a bit different compared to the other two. They represent actions

284
00:17:53,790 --> 00:17:57,474
that have happened or have been triggered sometime

285
00:17:57,522 --> 00:18:00,754
in the past. As you've probably noticed,

286
00:18:00,882 --> 00:18:05,238
commands and queries require point to point communication service

287
00:18:05,324 --> 00:18:09,066
a, query service b, while events on

288
00:18:09,088 --> 00:18:12,570
the other hand are undirected, meaning that

289
00:18:12,640 --> 00:18:16,330
anyone can consume events without the producer knowing.

290
00:18:16,750 --> 00:18:20,354
Also can event producer will never receive a response

291
00:18:20,422 --> 00:18:23,886
from its consumers. In fact, there are two types of

292
00:18:23,908 --> 00:18:27,102
events that a producer can send. The first

293
00:18:27,156 --> 00:18:29,790
one is called event notification.

294
00:18:30,370 --> 00:18:33,998
And just like his name is suggesting the event producer

295
00:18:34,094 --> 00:18:37,762
is informing its consumers that an action has

296
00:18:37,816 --> 00:18:41,794
happened. I can actually give you a great example of

297
00:18:41,832 --> 00:18:45,698
this pattern in practice. I'm not really sure if I

298
00:18:45,704 --> 00:18:49,522
can mention their name, but let's say that one of the biggest ecommerce

299
00:18:49,586 --> 00:18:53,922
companies has an extremely interesting use case on their homepage.

300
00:18:53,986 --> 00:18:57,590
They are displaying some random items that you may

301
00:18:57,660 --> 00:19:00,346
or may not be interesting in buying them.

302
00:19:00,528 --> 00:19:04,010
However, at the bottom of the page they also have

303
00:19:04,080 --> 00:19:07,546
a questions section. Do you know how they are

304
00:19:07,568 --> 00:19:10,918
compiling those suggestions? Well, we tend

305
00:19:10,934 --> 00:19:14,570
to move the cursor based on the direction of our eyesight.

306
00:19:14,730 --> 00:19:18,558
If we look to the right, we also move the cursor to the right.

307
00:19:18,724 --> 00:19:22,746
So if we see something that may attract us on that page,

308
00:19:22,868 --> 00:19:27,118
we tend to hover these cursor over that specific item.

309
00:19:27,294 --> 00:19:30,914
The moment we do that can event is being sent to their

310
00:19:30,952 --> 00:19:34,846
questions service notifying that we have hovered

311
00:19:34,878 --> 00:19:38,374
over that product. So as we scroll down the

312
00:19:38,412 --> 00:19:41,954
page we probably hover over a couple of items.

313
00:19:42,082 --> 00:19:45,394
Then at the bottom of the page we are getting suggestions

314
00:19:45,522 --> 00:19:48,440
based on which items we have hovered over.

315
00:19:48,910 --> 00:19:52,426
Sounds really cool, right? This is actually one

316
00:19:52,448 --> 00:19:56,474
of the best examples of the event notification pattern because it

317
00:19:56,512 --> 00:20:00,038
really denotes that eventing aspect. The producer

318
00:20:00,134 --> 00:20:03,786
isn't interested in getting back a response, and it also doesn't

319
00:20:03,818 --> 00:20:07,854
care about its consumers. All the producer does it cares about

320
00:20:07,972 --> 00:20:11,040
is to send hover events to the backend system.

321
00:20:11,730 --> 00:20:14,926
So how would something like this would look in Apache

322
00:20:14,958 --> 00:20:18,098
Kafka? Well, we always have at

323
00:20:18,104 --> 00:20:22,558
least two applications, a producer and at least one consumer.

324
00:20:22,734 --> 00:20:27,094
The first step is to define the event stream, in our case a

325
00:20:27,132 --> 00:20:30,546
Kafka topic. By the way, I'm using Java

326
00:20:30,578 --> 00:20:34,514
here, but there are kafka clients that would work with many other programming

327
00:20:34,562 --> 00:20:38,554
languages like Golang, Python and so on.

328
00:20:38,752 --> 00:20:42,010
Then in the next section we have to define some

329
00:20:42,080 --> 00:20:45,606
configuration. Both our producer and consumer

330
00:20:45,718 --> 00:20:49,434
need to know how to connect to the Kafka cluster and what

331
00:20:49,472 --> 00:20:53,086
serialization format should use. Next we

332
00:20:53,108 --> 00:20:57,290
have to initialize a Kafka producer and a Kafka consumer.

333
00:20:57,450 --> 00:21:01,214
Pretty straightforward so far. Now things start

334
00:21:01,252 --> 00:21:05,138
to divert a bit. The producer has to create a producer record.

335
00:21:05,304 --> 00:21:08,802
You can consider a record as an equivalent to an event.

336
00:21:08,936 --> 00:21:12,526
It's just that record is the official naming in Apache

337
00:21:12,558 --> 00:21:15,594
Kafka. A quick fact about records,

338
00:21:15,662 --> 00:21:19,078
a record is composed of a key and a value.

339
00:21:19,244 --> 00:21:23,302
Keys are used to identify events and values store

340
00:21:23,356 --> 00:21:26,918
the payload of that event. On the other hand,

341
00:21:27,004 --> 00:21:30,966
these consumer has to subscribe to the same topic the producer

342
00:21:30,998 --> 00:21:34,794
is producing too. We can actually pass a collection of

343
00:21:34,832 --> 00:21:38,374
topics meaning that our consumers can subscribe

344
00:21:38,422 --> 00:21:41,440
to multiple event streams at the same time.

345
00:21:41,970 --> 00:21:45,774
Finally, these only thing left to do for our producer is

346
00:21:45,812 --> 00:21:49,710
to send that event to the Kafka cluster. I've talked

347
00:21:49,780 --> 00:21:53,422
earlier about inversion of control and how it's the consumer

348
00:21:53,486 --> 00:21:57,218
job to actually make sure it retrieves the data well.

349
00:21:57,304 --> 00:22:01,570
The way this works in Kafka is by using these poll pattern.

350
00:22:02,150 --> 00:22:05,906
The consumer will actively query these Kafka cluster in

351
00:22:05,928 --> 00:22:09,266
order to find if there are new events to be consumed.

352
00:22:09,458 --> 00:22:13,030
As soon as the poll method returns, we are free to do whatever

353
00:22:13,100 --> 00:22:16,054
we want with those events. However,

354
00:22:16,172 --> 00:22:20,166
the poll action only happens once. That's why we

355
00:22:20,188 --> 00:22:23,962
need to wrap it in a while. True loop the end result

356
00:22:24,096 --> 00:22:27,850
sounds like a broken record. Hey, do you have new events for me?

357
00:22:27,920 --> 00:22:30,906
Do you have new events for me? Do you have new events for me?

358
00:22:31,088 --> 00:22:34,862
If the broker has new events in the event stream, it will pass

359
00:22:34,916 --> 00:22:38,030
them further to the consumer. Let's actually see

360
00:22:38,100 --> 00:22:41,934
how these code would look in action. So I have

361
00:22:41,972 --> 00:22:45,362
four terminal windows. On the left I will be running a single

362
00:22:45,416 --> 00:22:49,282
producer. On the right I will be running not one but three

363
00:22:49,336 --> 00:22:52,994
different consumers. All three will do exactly these same

364
00:22:53,032 --> 00:22:56,850
thing. They will lock the consumed event. In practice,

365
00:22:56,930 --> 00:22:59,880
these would actually consume the events in different ways.

366
00:23:00,250 --> 00:23:03,602
Starting with a producer, we can see that it's producing

367
00:23:03,666 --> 00:23:07,522
messages containing an incremental number as the key and

368
00:23:07,596 --> 00:23:11,114
a random uid as the value. I've used

369
00:23:11,152 --> 00:23:15,030
a while loop to make sure that an event is generated

370
00:23:15,110 --> 00:23:19,190
every 1 second. Now let's start the consumers.

371
00:23:19,350 --> 00:23:23,194
Firstly consumer one, then consumer

372
00:23:23,242 --> 00:23:26,480
two, and finally consumer three.

373
00:23:26,850 --> 00:23:31,226
As you can notice, all three of them are receiving the same events

374
00:23:31,338 --> 00:23:35,346
almost at the exact same time. Just try to think about

375
00:23:35,448 --> 00:23:39,282
how much code it would take to write an application that

376
00:23:39,336 --> 00:23:42,290
does the same thing using synchronous APIs.

377
00:23:42,710 --> 00:23:46,802
Of course we would get some added benefits like strong consistency,

378
00:23:46,946 --> 00:23:49,800
but if we don't really need them, it's fine.

379
00:23:50,330 --> 00:23:54,040
The next type of event is event carried state transfer.

380
00:23:54,810 --> 00:23:58,258
I know it sounds quite fancy, but I think it will

381
00:23:58,284 --> 00:24:01,100
be quite easy to explain it using an example.

382
00:24:01,710 --> 00:24:05,258
Event carried state transfer is all about the changes

383
00:24:05,344 --> 00:24:08,794
that occur in the current state. Let's take for

384
00:24:08,832 --> 00:24:12,878
example, the address changed event. A user changes

385
00:24:12,964 --> 00:24:16,458
his or her address through a user interface.

386
00:24:16,634 --> 00:24:20,222
This is a change in the state. Now our

387
00:24:20,276 --> 00:24:24,350
producer needs to send the address changed event to the event stream.

388
00:24:24,510 --> 00:24:27,966
But how much information should that event contain?

389
00:24:28,158 --> 00:24:30,754
It should contain only the new address,

390
00:24:30,952 --> 00:24:34,226
the old one and the new one. What about the

391
00:24:34,248 --> 00:24:37,890
user information? Let's say we want to go minimal

392
00:24:37,970 --> 00:24:41,906
and pick only the new address. That event is produced

393
00:24:41,938 --> 00:24:46,066
to Kafka, which is then picked up by the consumers.

394
00:24:46,258 --> 00:24:50,262
Only then we realize that one of the consuming parties

395
00:24:50,406 --> 00:24:54,666
is a tax service that requires the oral address as well.

396
00:24:54,848 --> 00:24:57,660
Uhoh, what can we do in this case?

397
00:24:58,030 --> 00:25:02,062
Well, that service can query back to the source system and

398
00:25:02,116 --> 00:25:05,566
get the missing data. Do you notice the

399
00:25:05,588 --> 00:25:09,550
problem? Because we haven't passed enough information into

400
00:25:09,620 --> 00:25:13,410
our event, we have now introduced coupling between

401
00:25:13,480 --> 00:25:17,330
our services. That's why when you're designing your event

402
00:25:17,400 --> 00:25:21,246
model, take into consideration that all consuming parties

403
00:25:21,358 --> 00:25:24,100
should avoid querying back the source system.

404
00:25:25,450 --> 00:25:29,842
One other problem that we frequently encounter while using microservices

405
00:25:29,986 --> 00:25:34,306
is the distributed source of truth. Our data doesn't reside

406
00:25:34,338 --> 00:25:38,054
in a single place, but it is distributed across multiple

407
00:25:38,102 --> 00:25:41,798
data stores. So in order to compute the results,

408
00:25:41,894 --> 00:25:45,050
we actually need to query multiple databases,

409
00:25:45,390 --> 00:25:48,300
and it's quite tricky to do it the right way.

410
00:25:48,750 --> 00:25:52,618
Sometimes we may be tempted to apply some workarounds,

411
00:25:52,714 --> 00:25:56,014
like copying parts of the data from one database to

412
00:25:56,052 --> 00:25:59,450
another. But this also poses some challenges.

413
00:25:59,610 --> 00:26:02,926
How can we make sure the data is in sync? How can

414
00:26:02,948 --> 00:26:06,814
we make sure that the moment we compile our result, we have the latest

415
00:26:06,862 --> 00:26:10,146
version of the data? A lot of effort would have

416
00:26:10,168 --> 00:26:13,630
to go into writing, deploying and maintaining

417
00:26:13,710 --> 00:26:17,714
some database synchronizers. We are now working with distributed

418
00:26:17,762 --> 00:26:21,030
systems, so we need another way to tackle this,

419
00:26:21,100 --> 00:26:24,710
a more efficient one. And these is these event

420
00:26:24,780 --> 00:26:28,662
sourcing comes into play event sourcing is

421
00:26:28,716 --> 00:26:32,154
probably one of the most powerful patterns, but it is

422
00:26:32,192 --> 00:26:35,402
also one of the most demanding. It all comes down

423
00:26:35,456 --> 00:26:39,018
to an event log. You can think of an event log

424
00:26:39,104 --> 00:26:43,214
as a ledger where we keep various actions that have happened.

425
00:26:43,412 --> 00:26:46,640
For example, let's think of a bank account.

426
00:26:47,090 --> 00:26:50,234
When we open that bank account, we start with the initial

427
00:26:50,282 --> 00:26:54,170
amount zero. Then money is being deposited

428
00:26:54,250 --> 00:26:58,114
into that account. First we have a transaction of 500

429
00:26:58,232 --> 00:27:01,698
and these another transaction of 200. Finally,

430
00:27:01,864 --> 00:27:05,414
we have to pay for that new tv and $300 are

431
00:27:05,452 --> 00:27:09,522
subtracted from that account. So event sourcing

432
00:27:09,666 --> 00:27:13,654
works by always appending new records to the log instead

433
00:27:13,772 --> 00:27:17,510
of updating an existing row in a relational database.

434
00:27:17,930 --> 00:27:21,274
If we want to compute the current amount, the only thing

435
00:27:21,312 --> 00:27:24,714
we have to do is to replay the log and go through all

436
00:27:24,752 --> 00:27:28,566
the transactions. By doing so, we can easily obtain

437
00:27:28,598 --> 00:27:32,286
the current balance of our bank account, which in our case

438
00:27:32,388 --> 00:27:36,042
would be $400. These is how event sourcing

439
00:27:36,106 --> 00:27:39,386
works. We always build the current state by replaying

440
00:27:39,418 --> 00:27:43,486
all the events from the log. You may think that these pattern

441
00:27:43,598 --> 00:27:47,074
is rather new, but we're actually using it for

442
00:27:47,112 --> 00:27:50,210
quite some time now. To give you an example,

443
00:27:50,360 --> 00:27:53,298
Git is actually an event sourcing system.

444
00:27:53,464 --> 00:27:57,542
We always retrieve the latest version of our code base by

445
00:27:57,596 --> 00:28:01,494
replaying commits from the log. So why is this

446
00:28:01,532 --> 00:28:05,798
pattern so powerful? Well, it solves our data access

447
00:28:05,884 --> 00:28:09,802
problem. Let's say that we have three applications that

448
00:28:09,856 --> 00:28:13,434
need some data. In the Kafka world, these would be some

449
00:28:13,472 --> 00:28:17,466
simple consumers that subscribe to a Kafka topic. Just a

450
00:28:17,488 --> 00:28:21,658
quick reminder. Kafka uses log files to store events.

451
00:28:21,754 --> 00:28:25,006
That's why it is such a powerful option when it comes to

452
00:28:25,028 --> 00:28:28,746
event driven architectures. Cool. Now let's

453
00:28:28,778 --> 00:28:32,942
say that two events are produced to the event log. The only thing

454
00:28:32,996 --> 00:28:36,194
the consumers would have to do is to consume those two

455
00:28:36,232 --> 00:28:39,922
events. They can be stored either in memory or any

456
00:28:39,976 --> 00:28:43,362
persistence layer. By doing so, we are building

457
00:28:43,416 --> 00:28:46,646
a socalled materialized view. We are building a

458
00:28:46,668 --> 00:28:49,186
current state based on events.

459
00:28:49,378 --> 00:28:52,802
If a new event is being produced, the consumers

460
00:28:52,866 --> 00:28:56,470
will pick it up and update their materialized views.

461
00:28:56,810 --> 00:29:00,774
But this doesn't happen in an instant. There's a slight delay

462
00:29:00,822 --> 00:29:04,342
between actually appending the event in the log and updating

463
00:29:04,406 --> 00:29:08,214
these materialized view. That's why we have to accept

464
00:29:08,262 --> 00:29:11,406
eventual consistency. We know that at some point

465
00:29:11,428 --> 00:29:14,442
in time our data will be consistent,

466
00:29:14,586 --> 00:29:16,974
but we don't know exactly when.

467
00:29:17,172 --> 00:29:20,750
Now, not everything goes perfect in production and

468
00:29:20,820 --> 00:29:25,122
things may fail. Let's say that one of our data stores goes

469
00:29:25,176 --> 00:29:28,786
down and we lose all our data. But that

470
00:29:28,808 --> 00:29:33,006
is totally fine. Why? Because all the other applications

471
00:29:33,118 --> 00:29:36,354
are not going to be impacted. If new events

472
00:29:36,402 --> 00:29:39,602
are being produced, the other applications can consume

473
00:29:39,666 --> 00:29:42,994
them in their own rhythm and update their materialized

474
00:29:43,042 --> 00:29:46,214
views. But what about the application that went

475
00:29:46,252 --> 00:29:49,366
down? Well, we can simply bring back an empty

476
00:29:49,398 --> 00:29:53,254
data store. Kafka consumers also have the ability

477
00:29:53,382 --> 00:29:56,714
to rewind and consume events from the past.

478
00:29:56,912 --> 00:30:00,106
That means we can start consuming from the beginning of these

479
00:30:00,128 --> 00:30:03,040
event log and recreate the current state.

480
00:30:03,730 --> 00:30:07,326
I always like to end something on a positive note. So what I want

481
00:30:07,348 --> 00:30:10,842
to say is that the game is on. We are actually witnessing

482
00:30:10,906 --> 00:30:14,750
a transition period and we are all part of it. A couple of years ago

483
00:30:14,820 --> 00:30:18,654
microservices were really popular and everyone was talking about them.

484
00:30:18,692 --> 00:30:22,046
Right now the same thing happens with event driven system we

485
00:30:22,068 --> 00:30:25,334
are changing our systems to adapt to a new mindset and

486
00:30:25,372 --> 00:30:28,966
event event event driven architectures. Step of all of this I would like

487
00:30:28,988 --> 00:30:32,598
to thank you for joining this session and don't forget I'm available

488
00:30:32,684 --> 00:30:36,294
to answer any questions that you may have. Feel free to connect with

489
00:30:36,332 --> 00:30:40,354
me on Twitter or LinkedIn. I'm always open for interesting discussion.

490
00:30:40,402 --> 00:30:43,380
Thank you again and I hope I'll see you next time. Have fun.

