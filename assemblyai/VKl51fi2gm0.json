{"language_code": "en_us", "audio_url": "https://cdn.assemblyai.com/upload/69dd6878-5e07-41f5-a2ae-37e2dde833e5", "punctuate": true, "format_text": true, "dual_channel": null, "webhook_url": null, "webhook_auth_header_name": null, "webhook_auth_header_value": null, "audio_start_from": null, "audio_end_at": null, "word_boost": ["adding", "alertmanager", "be", "becomes", "better", "by", "can", "challenging", "cluster", "clusters", "conf fourty two", "enhances", "grafana", "kubernetes", "lets", "monitoring", "prometheus", "red hat", "software engineer", "tools", "twinkll sisodia", "understanding", "workflow", "youll"], "boost_param": "high", "filter_profanity": false, "redact_pii": false, "redact_pii_audio": false, "redact_pii_policies": null, "redact_pii_sub": null, "speaker_labels": false, "speakers_expected": null, "content_safety": false, "content_safety_confidence": null, "iab_categories": false, "custom_spelling": null, "disfluencies": false, "sentiment_analysis": false, "auto_chapters": false, "entity_detection": false, "summarization": true, "summary_model": "informative", "summary_type": "bullets_verbose", "auto_highlights": true, "language_detection": false, "speech_threshold": null, "id": "3eaf2384-dc39-486e-b0ca-4349208bbac7", "status": "completed", "error": null, "text": "Everyone, I hope you having a great time in the Kube native conference. I am Twinkll Sisodia, software engineer at Red Hat and I work with red Hat partners to build their robust cloud native architectures. So today we'll be looking into why monitoring is so important. We'll look closely into what Prometheus is, its usage and its components. Then we will look into Grafana, how it can be used to visualize metric data, and lastly in the demo, I'll walk you through how an application is deployed on Openshift and how it can be monitored efficiently. We'll also be using an observability operator which will deploy Prometheus and alert manager instances for us. So as we all know how cctv cameras are used for safety and security purposes. Similar to that, we have few tools like Prometheus Grafana which is like cctvs for our system, say your cpu, your memory utilization reached critical limits, or your Kubernetes resources like pods deployments failed. So in this case monitoring can help and will minimize the risk of server down or unavailability of resources. And with that it will also help in proactive management of clusters. So for monitoring, we have few open source tools and one of them is Prometheus. Prometheus collects and stores its metrics as time series data, and it was designed to monitoring highly dynamic container environments like kubernetes, Docker, swarm, say there are many servers on which containers are running and they are all interconnected. Now, maintaining such complex systems becomes really very challenging and to make sure that everything runs smoothly without downtimes. Now imagine having multiple such infrastructures and you have no idea what's going inside it, either in the hardware level or in the application level, like errors, response latencies, overloaded, hardware down, or maybe running out of resources, et cetera. So this complexity would be minimized if you have a tool which constantly monitors your resources and activities, which is happening inside the cluster and alerts whenever something critical happens. So all this automated monitoring and alerting is what Prometheus offers as part of modern DevOps workflow. Now, for us to enable monitoring, we would need few Prometheus components, and I'll start with service monitors. Service monitors specify which services should be monitored. In place of service monitor. You can also use pod monitors. The difference is it specify which pods the Prometheus should monitor. Next we would need Prometheus rules. Now, Prometheus rule defines recording and alerting rules. Recording rule allow you to pre compute frequently used data and alerting rules specify when should we get the alerts like setting up the thresholds. Next we'll need alert manager config which specifies config for the alerts and custom receivers like Slack, pagerduty, etc. So this is a short glance of service monitors in this namespace selector has all the namespaces it will monitoring. The selector has the label for the app blue which it will match. And lastly the endpoint is HTTP port. Next is the Prometheus rule. It contains alerting and recording rule. In this example the app request per minute is greater than 20, so it will send low load alert and so on, so forth medium high. Next is the alert manager config secret. It has the API URL for the slack workspace and it has the channel name to which all the notifications will be sent. So so far we have seen how Prometheus works and how it collects and stores its metrics as time series data. Now let's see how we can visualize those data effectively on Grafana. And what's Grafana? Grafana is an open source software which enables us to query, visualize, alert on, and also explore metrics, logs, traces, wherever they are stored. Grafana provides us with tools to turn the time series database into insightful graphs and visualizations. Now these are the Grafana operator components we would need. So on the Grafana side we would need Grafana data source and Grafana dashboard. This is a short glimpse of how the data source manifest looks like it takes the Prometheus service URL, it takes the type the database type as Prometheus. So now this is an architecture diagram I'll be implementing in the demo how I monitored an application and got metrics out of it and visualized on Grafana. So here you can see we will deploy on an openshift dedicated cluster. We'll have a blue application in the blue namespace, an observability operator in the monitor namespace which is responsible for creating like instances of Prometheus and alert manager. Here Prometheus will scrape the metrics from the blue app and it will send alerts to alert manager, which will then send the alerts to slack as notifications. And lastly, the Grafana dashboard will visualize metric information in the form of graphs. So now let's move on to our demo. On the right hand side you can see the red hat openshift dedicated cluster, and on the left bottom corner you can see the slack workspace where all the notifications and alerts will be coming. So on the openshift dedicated cluster we have two namespaces, one for the blue application which is deployed already, and the other for the observability operator and its instance which is up and running already. Next I'm going to create the Prometheus components like service monitors, Prometheus rules, and alert manager. So I'll create the service monitors. Service monitor is up. I'll create the Prometheus rules. After that I'll create alert manager secret okay so the Prometheus components are in place. Next I'll create the cluster role and cluster role bindings so that the monitor namespace will have the permission to scrape the metrics from the blue namespace. The cluster role blue view is created. Next I'll create the cluster role binding so the role binding is now created. I'll port forward the Prometheus pod and let's see how the Prometheus dashboard looks like. So this is the Prometheus dashboard. If I navigate to alerts I can see all the lets like high medium, low. If I navigate to rules I can see the recording rules and the alerting rules. And lastly if I go to targets I can see the blue application which we have deployed recently up. Now let's trigger this blue application and see how we get the alerts on slack. So I'll created and curl it for at least 25 times. Youll once the threshold is met we can see the alerts popping up on the Slack channel. This shouldn't take time, should be like 25 to 30 seconds. So you can clearly see that the alerts are getting triggered low load, medium load. So on expanding one of these alerts we can see the metadata like where this alert is coming from. Like the alert name, the container name, endpoint IP address, the namespace path et how. So this is a small use case of how an organization can use all these monitoring tools. Like Prometheus, we can use slack alert manager to enhances the workflow and this is how one can minimize the risk of downtime. So far we have seen how we used Prometheus and alert manager to send alerts to slack. Now let's see how we use that data to turn it into insightful graphs and visualizations using Grafana. Let's move to operator hub and install Grafana operator. I'll install it into the monitor namespace and once the Grafana operator is installed I'll go forward and create its instance and data source. And then we'll port forward the Grafana pod to look how the dashboard looks like. So the Grafana operator is installed, I'll go forward and create its instance. The instance is created. Next I'll create the Grafana data source and the data source is created. Now I'll see if the pods are up and running or not. It is not. Okay, now it's up and running. So I'll port forward the Grafana pod at port nine. At port 3000 it's put forwarded. Let's put forward to 3000 and sign in with the same username and password I provided in the Grafana instance. Now before proceeding, I'll just quickly confirm if my data source is working. See when test my data source is working fine. I'll navigate to import and quickly import my sample dashboard which I created. Now you can create your own or just import it from the Grafana website. I'll rename it to blue dashboard and import it here. You can see we are getting different metrics. Starting with alerts. We can see which alerts are being triggered recently. So high load, low load and medium load are alerts which are triggered. What was the alert state, which container it was, what was the endpoint, et cetera. Next we see that blue request per minute metrics. So this metrics show that how many requests were there per minute for the blue application. Apart from that we can see response status, process, cpu, and lastly the up metrics. The up metrics show that how many containers are up currently. So there are one out manager, one for blue application and one for Prometheus which are up and running. So this is how you can use a data source like Prometheus and convert the data into insightful graphs and visualizations. This will help can sre to be mindful of all the resources and all the costs involved. And with that it will also help organizations to minimize their downtime. And that concludes my presentation. Just to summarize what we have discussed so far. So we have talked about the importance of monitoring. We have discussed about Prometheus Grafana components involved in the demo. We deployed an app, we deployed observability operator which installed Prometheus and alert manager. And finally we sent alerts to slack. Then we deployed Grafana operator and its components. And lastly we imported custom dashboards to see insightful graphs us. So here I would like to thank everyone. I hope you all enjoyed it. If you want to get connected, I'm there on LinkedIn and if anyone wants to do like a hands on you can visit my GitHub repository. It has all the in depth details. Read me for that. So yeah, thanks everyone.", "words": [], "utterances": null, "confidence": 0.950400547466513, "audio_duration": 907.0, "webhook_status_code": null, "webhook_auth": false, "summary": "- Twinkll Sisodia, software engineer at Red Hat, talks about why monitoring is so important. In the demo, he walks you through how an application is deployed on Openshift and how it can be monitored efficiently. Also looks at how Grafana can be used to visualize metric data.\n- Using Prometheus and alert manager to send alerts to slack. Now let's see how we use that data to turn it into insightful graphs and visualizations using Grafana.\n- We deployed an app, we deployed observability operator which installed Prometheus and alert manager. Then we deployed Grafana operator and its components. And lastly we imported custom dashboards to see insightful graphs us.\n- So here I would like to thank everyone. I hope you all enjoyed it. If you want to get connected, I'm there on LinkedIn and if anyone wants to do like a hands on you can visit my GitHub repository.", "auto_highlights_result": {"status": "success", "results": [{"count": 2, "rank": 0.09, "text": "Prometheus Grafana", "timestamps": [{"start": 74232, "end": 75826}, {"start": 860324, "end": 861674}]}, {"count": 1, "rank": 0.09, "text": "Prometheus Grafana components", "timestamps": [{"start": 860324, "end": 862362}]}, {"count": 3, "rank": 0.08, "text": "Prometheus rules", "timestamps": [{"start": 193616, "end": 195046}, {"start": 405866, "end": 406698}, {"start": 420008, "end": 421410}]}, {"count": 2, "rank": 0.08, "text": "Grafana data source", "timestamps": [{"start": 311300, "end": 312838}, {"start": 681648, "end": 684010}]}, {"count": 1, "rank": 0.08, "text": "few Prometheus components", "timestamps": [{"start": 174152, "end": 176050}]}, {"count": 5, "rank": 0.07, "text": "Grafana operator", "timestamps": [{"start": 306388, "end": 307274}, {"start": 629078, "end": 630560}, {"start": 641108, "end": 641914}, {"start": 663174, "end": 663964}, {"start": 876234, "end": 877194}]}, {"count": 2, "rank": 0.07, "text": "Grafana dashboard", "timestamps": [{"start": 313272, "end": 314770}, {"start": 369208, "end": 370334}]}, {"count": 1, "rank": 0.07, "text": "low load alert", "timestamps": [{"start": 253524, "end": 254910}]}, {"count": 1, "rank": 0.07, "text": "slack alert manager", "timestamps": [{"start": 605918, "end": 607652}]}, {"count": 1, "rank": 0.07, "text": "alert manager instances", "timestamps": [{"start": 63368, "end": 64654}]}, {"count": 2, "rank": 0.06, "text": "alert manager config", "timestamps": [{"start": 212116, "end": 213742}, {"start": 260468, "end": 261546}]}, {"count": 16, "rank": 0.06, "text": "alerts", "timestamps": [{"start": 159716, "end": 160154}, {"start": 206548, "end": 206954}, {"start": 215992, "end": 216686}, {"start": 361316, "end": 361738}, {"start": 365208, "end": 365790}, {"start": 386892, "end": 387266}, {"start": 484932, "end": 485722}, {"start": 507628, "end": 508066}, {"start": 542992, "end": 543590}, {"start": 580358, "end": 580716}, {"start": 585482, "end": 586416}, {"start": 618066, "end": 618456}, {"start": 782312, "end": 782766}, {"start": 784476, "end": 784866}, {"start": 790012, "end": 790546}, {"start": 872596, "end": 872986}]}, {"count": 1, "rank": 0.06, "text": "blue dashboard", "timestamps": [{"start": 775128, "end": 776206}]}, {"count": 5, "rank": 0.06, "text": "service monitors", "timestamps": [{"start": 177500, "end": 178786}, {"start": 178978, "end": 180002}, {"start": 224312, "end": 225650}, {"start": 404932, "end": 405802}, {"start": 412808, "end": 413970}]}, {"count": 6, "rank": 0.06, "text": "blue application", "timestamps": [{"start": 345744, "end": 346426}, {"start": 393184, "end": 393818}, {"start": 499544, "end": 500226}, {"start": 505468, "end": 506214}, {"start": 805584, "end": 806460}, {"start": 820308, "end": 821140}]}]}, "content_safety_labels": null, "iab_categories_result": null, "chapters": null, "sentiment_analysis_results": null, "entities": null}