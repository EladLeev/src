1
00:02:10,420 --> 00:02:14,280
Everyone, good morning, good afternoon and good evening.

2
00:02:14,780 --> 00:02:18,360
Happy to be here at Con 42

3
00:02:18,430 --> 00:02:20,360
Chaos Engineering conference.

4
00:02:21,340 --> 00:02:24,836
Before we delve into the topic of continuous resilience,

5
00:02:24,948 --> 00:02:28,196
a bit about me I am Uma Mukara,

6
00:02:28,388 --> 00:02:32,312
head head of Chaos engineering harness. I am

7
00:02:32,366 --> 00:02:36,336
also a maintainer and co founder the

8
00:02:36,358 --> 00:02:40,380
Litmus Chaos CNCF project which is at incubating stage

9
00:02:40,540 --> 00:02:44,636
at harness. I've been developing customers head

10
00:02:44,668 --> 00:02:48,400
head of chaos engineering wider number of use cases

11
00:02:49,160 --> 00:02:52,484
in that process I have learned a little bit

12
00:02:52,522 --> 00:02:56,320
about head of chaos engineering been adopted.

13
00:02:56,400 --> 00:03:00,084
What are the use cases that are

14
00:03:00,122 --> 00:03:03,736
more prominent, more appealing? So here is

15
00:03:03,758 --> 00:03:07,368
an opportunity for me to talk about what I learned in

16
00:03:07,374 --> 00:03:11,444
the last few years of trying to push chaos

17
00:03:11,492 --> 00:03:15,064
engineering to more practical environments in the cloud native

18
00:03:15,112 --> 00:03:19,404
space. Innovation is a continuous process in

19
00:03:19,442 --> 00:03:22,908
software, right? And we're all trying to

20
00:03:22,994 --> 00:03:26,640
innovate something in software either

21
00:03:26,710 --> 00:03:30,108
to improve governance,

22
00:03:30,204 --> 00:03:33,040
quality, efficiency,

23
00:03:35,460 --> 00:03:38,616
control and reliability, et cetera.

24
00:03:38,668 --> 00:03:42,404
So in this specific talk, let's talk how

25
00:03:42,442 --> 00:03:46,004
can we innovate more in the space of reliability or

26
00:03:46,042 --> 00:03:46,900
resilience?

27
00:03:49,720 --> 00:03:53,636
So before we actually reach the topic of innovating

28
00:03:53,748 --> 00:03:57,220
in the space of resilience, or in the area of resilience,

29
00:03:57,380 --> 00:04:01,604
let's talk about the software development costs

30
00:04:01,732 --> 00:04:06,280
that applies to the software developers overall.

31
00:04:06,440 --> 00:04:10,424
In the world today, we have about 20 been million software developers

32
00:04:10,472 --> 00:04:14,696
costing 100k average on annual basis.

33
00:04:14,888 --> 00:04:18,752
That leads to a total spend of about $2.7

34
00:04:18,806 --> 00:04:22,944
trillion. That's a huge money that's being

35
00:04:22,982 --> 00:04:26,464
spent on software development. If that

36
00:04:26,502 --> 00:04:30,308
is so big amount of money that's being spent, what are

37
00:04:30,314 --> 00:04:34,004
the software developers are doing? In this poll you can

38
00:04:34,042 --> 00:04:39,632
see that more than 50% of the software developers indicate

39
00:04:39,696 --> 00:04:44,170
that they actually spend less than 3 hours in a day writing the code.

40
00:04:44,940 --> 00:04:47,850
Where are they spending the remaining time?

41
00:04:48,220 --> 00:04:52,504
They could be spending the time in trying to

42
00:04:52,702 --> 00:04:56,508
build the environments, or build

43
00:04:56,594 --> 00:05:00,700
the deployment environments, or debugging

44
00:05:01,520 --> 00:05:04,556
the existing software, or the software that they

45
00:05:04,578 --> 00:05:07,900
just wrote, or production issues, etc. Cetera, et cetera.

46
00:05:07,980 --> 00:05:11,132
So this all leads to a more toil

47
00:05:11,196 --> 00:05:15,168
for software developers. And is there a way

48
00:05:15,334 --> 00:05:19,524
we can actually reduce this toil to

49
00:05:19,562 --> 00:05:22,260
as much as about 50% less?

50
00:05:22,410 --> 00:05:25,732
Right? So that could actually free

51
00:05:25,786 --> 00:05:29,684
up more money

52
00:05:29,802 --> 00:05:33,540
for the actual software development.

53
00:05:33,620 --> 00:05:37,688
And that's a huge spend, right? So this is the overall market

54
00:05:37,774 --> 00:05:41,112
space for developers, but you can apply the same thing

55
00:05:41,166 --> 00:05:45,116
to your own organization. You're spending a lot of money

56
00:05:45,298 --> 00:05:48,876
on developers, but developers are actually not spending enough

57
00:05:48,978 --> 00:05:52,204
time and effort to writing code, right?

58
00:05:52,322 --> 00:05:56,220
That's an opportunity to reduce that toil and

59
00:05:56,370 --> 00:06:00,288
increase innovation in different, different sres. The opportunity

60
00:06:00,374 --> 00:06:03,856
is to innovate, to increase the developer productivity and

61
00:06:03,878 --> 00:06:07,600
hence save the cost. And you can use that cost back into

62
00:06:07,670 --> 00:06:11,908
more development and ship more products

63
00:06:12,074 --> 00:06:15,680
or more code or faster code, et cetera, et cetera.

64
00:06:15,760 --> 00:06:19,968
Right. So let's see how it applies

65
00:06:20,144 --> 00:06:23,592
to the resilience as a use case.

66
00:06:23,646 --> 00:06:26,996
Right. So you can actually reduce

67
00:06:27,028 --> 00:06:30,196
the developer toil. So this toil comprises

68
00:06:30,228 --> 00:06:34,212
of either build time or deployment

69
00:06:34,276 --> 00:06:37,956
time or a debug time. Right. So you need to basically reduce

70
00:06:37,988 --> 00:06:41,704
this toil and that's where you can actually improve

71
00:06:41,752 --> 00:06:45,420
production. In this specific topic we are going to

72
00:06:45,570 --> 00:06:49,132
look at, where are these developers

73
00:06:49,196 --> 00:06:52,848
spending their time in debugging? Why are they doing that?

74
00:06:52,934 --> 00:06:57,164
And how can we actually reduce that amount of debug

75
00:06:57,212 --> 00:07:01,220
time? And eventually that leads to more

76
00:07:01,290 --> 00:07:04,560
time for innovation. And because you are reducing

77
00:07:04,720 --> 00:07:08,660
the debug time in the area of resilience, that also improves

78
00:07:09,320 --> 00:07:11,690
the resilience of the products.

79
00:07:13,580 --> 00:07:17,380
So why are they spending time in debugging?

80
00:07:17,460 --> 00:07:21,176
Right. So basically, in other words, why the bugs are

81
00:07:21,198 --> 00:07:24,796
being introduced. Right. So it could

82
00:07:24,818 --> 00:07:28,604
be plain oversight. Developers are humans, so there

83
00:07:28,642 --> 00:07:32,712
is possibility that something is overlooked.

84
00:07:32,856 --> 00:07:36,724
Even the smartest developers could overlook

85
00:07:36,872 --> 00:07:40,716
some of the cases and then introduce bugs or leak bugs

86
00:07:40,748 --> 00:07:44,000
to the right, but that cannot be done.

87
00:07:44,150 --> 00:07:48,000
But the more common pattern

88
00:07:48,600 --> 00:07:52,624
that can be observed is a lot of dependencies

89
00:07:52,752 --> 00:07:56,436
in the practical world are not tested and they've been

90
00:07:56,458 --> 00:08:00,340
released to the right. And it's also possible

91
00:08:00,410 --> 00:08:03,800
that developers, there's a lot of churn,

92
00:08:04,540 --> 00:08:08,376
a lot of hiring. It could be a

93
00:08:08,398 --> 00:08:11,224
case of either. In that case,

94
00:08:11,342 --> 00:08:15,112
you are not the guy who has written the product

95
00:08:15,166 --> 00:08:18,284
from the beginning and the product has scaled up so much and

96
00:08:18,322 --> 00:08:21,804
you don't necessarily understand the entire architecture of the product,

97
00:08:21,922 --> 00:08:26,124
but you are rolling out some of the new features, right? So that leads to

98
00:08:26,322 --> 00:08:30,092
a bit of lack of understanding of the product architecture.

99
00:08:30,236 --> 00:08:33,344
So some of the intricacies are not

100
00:08:33,382 --> 00:08:36,480
well understood and then the design

101
00:08:36,550 --> 00:08:39,136
box can be trickling in or the code box,

102
00:08:39,238 --> 00:08:42,420
right? So even if you take care of all that,

103
00:08:42,570 --> 00:08:46,176
you assume that the product will run in certain environment,

104
00:08:46,288 --> 00:08:49,412
but the environment can be totally different or

105
00:08:49,466 --> 00:08:53,016
it can keep changing and that may

106
00:08:53,038 --> 00:08:56,164
not work as expected

107
00:08:56,212 --> 00:08:59,924
in that environment, your code. So these are the reasons

108
00:09:00,052 --> 00:09:03,610
why you end up as a developer spending more time.

109
00:09:04,080 --> 00:09:07,656
And these reasons

110
00:09:07,768 --> 00:09:10,552
become more common in cloud native.

111
00:09:10,696 --> 00:09:14,044
But before that, let's look at the cost of

112
00:09:14,082 --> 00:09:18,064
debugging, right, so you can end

113
00:09:18,102 --> 00:09:22,336
up having costing much more to

114
00:09:22,358 --> 00:09:26,464
the organization if you actually find these bugs in

115
00:09:26,502 --> 00:09:30,500
production and start fixing them. The cost of

116
00:09:30,570 --> 00:09:34,356
fixing bugs in production is almost like more

117
00:09:34,378 --> 00:09:38,420
than ten times than what you incur if you

118
00:09:38,490 --> 00:09:41,824
debug and fix them in QA or within the code,

119
00:09:41,882 --> 00:09:45,464
right? So it's a well known factor, nothing new.

120
00:09:45,582 --> 00:09:49,112
It's always good to find the bugs before

121
00:09:49,166 --> 00:09:52,904
they go into the production, right? So that's another way to

122
00:09:53,102 --> 00:09:57,064
look at this. But the reasons for introducing

123
00:09:57,112 --> 00:10:00,988
this issues or

124
00:10:01,074 --> 00:10:04,872
overlooking this causes

125
00:10:05,016 --> 00:10:08,444
is becoming more and more common in the case of cloud native

126
00:10:08,492 --> 00:10:12,464
developers, right? So in cloud native, two things

127
00:10:12,502 --> 00:10:15,916
are happening. By default, you are assumed

128
00:10:15,948 --> 00:10:20,224
to ship things faster because the total ecosystem is

129
00:10:20,262 --> 00:10:24,456
supporting faster shipment of your bills

130
00:10:24,508 --> 00:10:29,620
because of the small amount of code that each

131
00:10:29,770 --> 00:10:33,512
developer has to look at, and well defined boundaries around

132
00:10:33,566 --> 00:10:37,592
continuous and entire

133
00:10:37,726 --> 00:10:40,968
ecosystem of cloud native, right? So the

134
00:10:40,974 --> 00:10:45,020
pipelines are better, the tools surrounding shipment,

135
00:10:45,360 --> 00:10:48,830
like githubs and all are helping you to

136
00:10:50,000 --> 00:10:53,816
ship things faster. So added

137
00:10:53,848 --> 00:10:57,804
to that, containers are helping

138
00:10:57,932 --> 00:11:01,404
developers to wrap up features faster

139
00:11:01,452 --> 00:11:05,484
because they are microservices. You need to look at things objectively,

140
00:11:05,532 --> 00:11:09,188
only to a limited scope within a

141
00:11:09,194 --> 00:11:12,644
container and surrounded by APIs. So you are able

142
00:11:12,682 --> 00:11:15,920
to look at things very objectively,

143
00:11:16,000 --> 00:11:19,776
finish the coding and ship things faster.

144
00:11:19,968 --> 00:11:23,192
So because you are doing them very

145
00:11:23,246 --> 00:11:27,064
fast, the chances of

146
00:11:27,262 --> 00:11:30,712
not doing the dependency testing or chances of not

147
00:11:30,766 --> 00:11:33,892
understanding the product very well are high.

148
00:11:34,046 --> 00:11:37,644
And that could actually cause a lot of

149
00:11:37,682 --> 00:11:41,576
issues. And if the issues are related, the faults

150
00:11:41,688 --> 00:11:45,084
are happening in infrastructure and the impact of

151
00:11:45,122 --> 00:11:49,072
outage can be very high to

152
00:11:49,206 --> 00:11:52,380
just a fault happening within your container.

153
00:11:52,460 --> 00:11:55,248
And because of that, the outage is happening,

154
00:11:55,414 --> 00:11:58,444
the impact can be very low at that level, right?

155
00:11:58,582 --> 00:12:02,276
So the summary here is you

156
00:12:02,298 --> 00:12:06,132
are testing the code as much as possible and then

157
00:12:06,186 --> 00:12:10,036
shipping as fast as you can, and you may

158
00:12:10,058 --> 00:12:13,384
or may not be looking at the entire set of new

159
00:12:13,422 --> 00:12:17,236
testing that is needed, right? So it's

160
00:12:17,268 --> 00:12:21,156
possible that deep dependencies, the faults happening in the deep dependencies

161
00:12:21,188 --> 00:12:25,240
are not tested. So typically

162
00:12:25,320 --> 00:12:30,472
what's happening in the case of cloud native shipments,

163
00:12:30,616 --> 00:12:34,076
in such cases is the end service

164
00:12:34,178 --> 00:12:38,556
is impacted, right? And developers are then jumping

165
00:12:38,588 --> 00:12:41,968
into debug, and finally they know that,

166
00:12:42,134 --> 00:12:46,960
they come to discover that there's a dependent component

167
00:12:47,860 --> 00:12:51,012
or a service that

168
00:12:51,066 --> 00:12:55,008
incurs fault within it or multiple faults,

169
00:12:55,104 --> 00:12:58,420
and because of that, a given service is affected.

170
00:12:59,240 --> 00:13:02,920
So that's kind of a weakness within a given service.

171
00:13:03,070 --> 00:13:06,392
It's not resilient enough and you find it and fix it, right?

172
00:13:06,526 --> 00:13:10,072
So this is typically a case

173
00:13:10,126 --> 00:13:13,444
of increased cost, and there's a good

174
00:13:13,502 --> 00:13:17,432
opportunity that you can find such issues much earlier

175
00:13:17,496 --> 00:13:18,830
and avoid the cost,

176
00:13:22,240 --> 00:13:25,756
the kind of test that you

177
00:13:25,778 --> 00:13:29,436
need to be doing before you ship to avoid

178
00:13:29,468 --> 00:13:33,424
such cost is you have to

179
00:13:33,622 --> 00:13:36,880
assume that faults can happen within your code or

180
00:13:36,950 --> 00:13:40,784
within the aps that your code or application is consuming,

181
00:13:40,912 --> 00:13:45,030
or other services such as databases or

182
00:13:46,040 --> 00:13:49,536
message queues or other network services. There are faults

183
00:13:49,568 --> 00:13:53,464
that could be happening and your application has to be tested for

184
00:13:53,502 --> 00:13:57,252
such faults. And of course the infrastructure faults

185
00:13:57,316 --> 00:14:01,076
that are pretty common and infrastructure faults

186
00:14:01,108 --> 00:14:04,556
can happen within kubernetes and your code has to

187
00:14:04,578 --> 00:14:08,344
be resilient for such faults,

188
00:14:08,392 --> 00:14:12,456
right? These are the dependent fault testing

189
00:14:12,648 --> 00:14:15,788
kind of set that you need to be aware of and test,

190
00:14:15,874 --> 00:14:19,664
right? So what this really means is cloud

191
00:14:19,702 --> 00:14:23,120
native developers need to do chaos testing, right?

192
00:14:23,270 --> 00:14:27,536
This is exactly what chaos testing typically means. Some fault can happen

193
00:14:27,638 --> 00:14:31,552
in my dependent component or infrastructure,

194
00:14:31,696 --> 00:14:35,508
and my service, that is depending on my code,

195
00:14:35,674 --> 00:14:39,904
need to be resilient enough, right? So chaos

196
00:14:39,952 --> 00:14:43,912
testing is needed by the nature of the

197
00:14:43,966 --> 00:14:47,690
cloud native architecture itself

198
00:14:48,300 --> 00:14:50,760
to achieve high resilience.

199
00:14:52,300 --> 00:14:56,216
And we are basically saying that developers end up spending

200
00:14:56,248 --> 00:14:59,772
a lot of time debugging and that's not good for

201
00:14:59,826 --> 00:15:04,072
developer productivity. So if you need to do chaos testing,

202
00:15:04,216 --> 00:15:07,884
let's actually see what the typical definition of chaos

203
00:15:07,932 --> 00:15:11,596
engineering is. Chaos engineering. Typically it's

204
00:15:11,628 --> 00:15:15,824
been there for quite some time. We all know that, and we

205
00:15:15,862 --> 00:15:19,296
all are kind of told that chaos engineering

206
00:15:19,328 --> 00:15:22,868
is about introduce control faults and

207
00:15:22,954 --> 00:15:26,630
reduce the expensive outages. So if you are

208
00:15:27,800 --> 00:15:30,580
basically reducing the outages,

209
00:15:31,340 --> 00:15:35,320
you are looking at doing chaos testing in production,

210
00:15:36,460 --> 00:15:39,732
right? And that really comes with a high barrier.

211
00:15:39,796 --> 00:15:43,064
And this is one reason, even though

212
00:15:43,102 --> 00:15:46,524
chaos engineering has been there for quite some time, the adoption of

213
00:15:46,562 --> 00:15:50,476
chaos engineering, though it's increasing in the

214
00:15:50,498 --> 00:15:51,550
recent years,

215
00:15:52,880 --> 00:15:57,064
rapidly. The typical chaos testing

216
00:15:57,112 --> 00:16:02,076
or chaos engineering understanding is that it applies to production that's

217
00:16:02,108 --> 00:16:05,410
changing very fast. And that's exactly what we are talking about here.

218
00:16:05,940 --> 00:16:10,496
And the traditional chaos engineering is also about introducing

219
00:16:10,528 --> 00:16:14,196
game days and try to find the

220
00:16:14,218 --> 00:16:18,150
right champions within the organization who are open to

221
00:16:18,760 --> 00:16:22,644
do this game days and find any resilience

222
00:16:22,692 --> 00:16:26,170
issues and then keep doing more game days, right?

223
00:16:26,780 --> 00:16:30,324
That's typically the head head

224
00:16:30,372 --> 00:16:33,888
of chaos engineering. It's been a reactive approach.

225
00:16:34,084 --> 00:16:37,500
Either some major incidents have happened

226
00:16:37,570 --> 00:16:41,100
and has a solution. You're trying to head of chaos engineering.

227
00:16:41,170 --> 00:16:44,590
Sometimes it can be driven by regulations as well,

228
00:16:45,120 --> 00:16:48,544
especially in the case of Dr. And all chaos engineering comes

229
00:16:48,582 --> 00:16:52,176
into the picture in the banking sector and all.

230
00:16:52,278 --> 00:16:56,096
But these are the typical needs

231
00:16:56,198 --> 00:17:00,164
or patterns that head head of chaos

232
00:17:00,202 --> 00:17:04,084
engineering until a couple of years ago.

233
00:17:04,282 --> 00:17:08,212
But the modern chaos engineering is really driven by not

234
00:17:08,266 --> 00:17:10,820
necessarily to reduce the outages,

235
00:17:11,980 --> 00:17:15,428
but also the need to increase developer productivity,

236
00:17:15,524 --> 00:17:18,776
right? So if my developers are spending a lot

237
00:17:18,798 --> 00:17:21,800
of time in debugging the production issues.

238
00:17:21,950 --> 00:17:25,292
That's a major loss, and you need to avoid that.

239
00:17:25,346 --> 00:17:28,860
So how can I do that? Use chaos testing,

240
00:17:29,440 --> 00:17:32,808
and similarly, the QA teams. Right, so QA

241
00:17:32,824 --> 00:17:35,992
teams are coming in and looking at more

242
00:17:36,066 --> 00:17:39,776
ways to test so many components that are coming in the form of

243
00:17:39,798 --> 00:17:43,004
microservices. Earlier, it was easy enough that you're

244
00:17:43,052 --> 00:17:47,356
getting a monolith application, very clear boundaries,

245
00:17:47,468 --> 00:17:51,028
and you can write better, complete set of

246
00:17:51,034 --> 00:17:55,124
test cases. But now microservices can

247
00:17:55,162 --> 00:17:58,340
pose a challenge for QA teams. There's so many

248
00:17:58,490 --> 00:18:02,040
containers and they're coming in so fast.

249
00:18:02,110 --> 00:18:05,784
So how do I actually make sure that the

250
00:18:05,822 --> 00:18:09,992
quality is right in many aspects and that

251
00:18:10,046 --> 00:18:13,568
can be achieved through chaos testing.

252
00:18:13,684 --> 00:18:17,950
Right. And it's also possible that the whole

253
00:18:18,480 --> 00:18:22,204
big monolith or traditional application that's working well,

254
00:18:22,242 --> 00:18:25,768
which is business critical in nature,

255
00:18:25,944 --> 00:18:29,404
is being moved to cloud native. How do you ensure

256
00:18:29,452 --> 00:18:32,924
that everything works fine on the other side, on the cloud native?

257
00:18:33,052 --> 00:18:36,316
So one way to ensure is by employing

258
00:18:36,348 --> 00:18:40,176
the chaos engineering practices. Right. So the need for chaos engineering

259
00:18:40,208 --> 00:18:43,350
in the modern times is really defined or

260
00:18:43,960 --> 00:18:47,412
driven by these needs, rather than just, hey,

261
00:18:47,546 --> 00:18:51,136
I incurred some outages, let's go fix them. Right.

262
00:18:51,258 --> 00:18:54,516
So while that is still true, there are more drivers

263
00:18:54,548 --> 00:18:58,360
that are driving adoption of chaos engineering.

264
00:18:59,580 --> 00:19:02,884
So these needs are leading

265
00:19:02,932 --> 00:19:05,980
to a new concept called continuous resilience.

266
00:19:06,640 --> 00:19:11,384
So what is continuous resilience? It's basically verifying

267
00:19:11,432 --> 00:19:15,112
the resilience of your code or component through automated

268
00:19:15,176 --> 00:19:18,472
testing. Chaos testing. And you do that continuously.

269
00:19:18,616 --> 00:19:21,520
Right. So chaos engineering,

270
00:19:22,020 --> 00:19:25,484
done in automated way across your DevOps

271
00:19:25,532 --> 00:19:28,160
spectrum, is called continuous resilience.

272
00:19:28,600 --> 00:19:32,112
You achieve continuous resilience. That approach is called continuous resilience

273
00:19:32,176 --> 00:19:36,180
approach. Right. So just to summarize,

274
00:19:37,080 --> 00:19:40,180
you head head of chaos engineering,

275
00:19:40,250 --> 00:19:43,252
QA, pre prod and prod,

276
00:19:43,396 --> 00:19:47,348
continuous all the time, involving all the personas.

277
00:19:47,524 --> 00:19:51,400
And that leads to continuous resilience as a concept.

278
00:19:53,260 --> 00:19:57,470
So what are the typical metrics that you look for in the

279
00:19:58,000 --> 00:20:02,028
continuous resilience space or a model

280
00:20:02,194 --> 00:20:05,624
is the resilience score and resilience coverage.

281
00:20:05,752 --> 00:20:09,628
Right. So you always measure the resilience

282
00:20:09,724 --> 00:20:13,388
score of a given experiments, chaos experiment,

283
00:20:13,484 --> 00:20:16,896
or a component or a service itself.

284
00:20:17,078 --> 00:20:20,500
And it can be defined by the

285
00:20:20,570 --> 00:20:24,772
average success of the steady state checks of

286
00:20:24,826 --> 00:20:28,912
whatever you are measuring. Right. The steady state checks

287
00:20:29,056 --> 00:20:32,320
that are done during a given experiment

288
00:20:32,480 --> 00:20:35,576
or of a given component or of a given service.

289
00:20:35,678 --> 00:20:38,776
Right. So this is the resilience score. Typically it can

290
00:20:38,798 --> 00:20:41,892
be out of 100 or a percentage.

291
00:20:42,036 --> 00:20:45,804
And the more important metric in continuous resilience, you can

292
00:20:45,842 --> 00:20:50,332
think of this as resilience coverage, where because

293
00:20:50,386 --> 00:20:53,884
you are looking at the whole spectrum, you can

294
00:20:54,002 --> 00:20:58,140
come up with a total number of possible chaos

295
00:20:58,220 --> 00:21:01,632
tests. Basically you can compute them

296
00:21:01,686 --> 00:21:05,520
as what are the total resources that my service

297
00:21:05,590 --> 00:21:09,430
is comprising of? And you can do multiple combinations of that.

298
00:21:10,120 --> 00:21:14,100
The resources can be infrastructure resources, API resources,

299
00:21:14,440 --> 00:21:18,004
or network resources, or the resources that make up the

300
00:21:18,042 --> 00:21:20,940
service itself, like container resources, et cetera.

301
00:21:21,120 --> 00:21:24,296
And basically you can come up with a

302
00:21:24,318 --> 00:21:27,784
large number of tests that are possible, and then you

303
00:21:27,822 --> 00:21:31,012
start introducing

304
00:21:31,076 --> 00:21:34,756
such chaos tests into your pipelines, and those

305
00:21:34,798 --> 00:21:38,316
are the ones that you actually cover. Right. So you have

306
00:21:38,338 --> 00:21:41,756
a very clear way of measuring what are

307
00:21:41,778 --> 00:21:46,184
the chaos tests that you have done out of the possible chaos

308
00:21:46,232 --> 00:21:49,528
tests. And that leads to a coverage. Think of this as a

309
00:21:49,554 --> 00:21:53,900
code coverage. In the traditional developer spectrum,

310
00:21:54,060 --> 00:21:57,520
resilience coverage is being applied for

311
00:21:57,670 --> 00:22:00,740
resilience and chaos experiments.

312
00:22:02,600 --> 00:22:05,910
So many

313
00:22:06,920 --> 00:22:10,752
people are calling this approach as hey, let's do chaos

314
00:22:10,816 --> 00:22:14,856
in pipelines. That's almost same, right? Except that

315
00:22:15,038 --> 00:22:18,532
continuous resilience does not limit

316
00:22:18,596 --> 00:22:22,216
yourself just to pipelines. You can automate the

317
00:22:22,238 --> 00:22:25,964
chaos test on the production side as

318
00:22:26,002 --> 00:22:29,256
your maturity improves, right? So it's a pipelines

319
00:22:29,288 --> 00:22:33,400
approach. So what are the general differences

320
00:22:33,560 --> 00:22:37,360
between the traditional chaos engineering approach versus

321
00:22:38,020 --> 00:22:41,404
the pipelines or approach, or the continuous

322
00:22:41,452 --> 00:22:46,012
resilience approach? So traditionally

323
00:22:46,076 --> 00:22:50,592
in the game disk model, you are executing on demand

324
00:22:50,736 --> 00:22:55,072
with a lot of preparation. You need to assign

325
00:22:55,136 --> 00:22:58,640
certain dates and take permissions

326
00:22:58,800 --> 00:23:03,044
and then execute this test. Versus pipelines,

327
00:23:03,172 --> 00:23:06,536
you are executing continuous with not much of a

328
00:23:06,558 --> 00:23:09,352
thought or preparation. They are supposed to work,

329
00:23:09,486 --> 00:23:12,996
and if it doesn't work, it doesn't hurt

330
00:23:13,028 --> 00:23:17,004
so much. But it actually is a good thing that you can go and look

331
00:23:17,042 --> 00:23:20,876
at whenever it fails. Right? Maybe just slowing down the delivery of

332
00:23:20,898 --> 00:23:24,216
your builds, but that's

333
00:23:24,248 --> 00:23:28,392
okay. So this leads to greater adoption itself

334
00:23:28,546 --> 00:23:31,964
overall. And game days are targeted

335
00:23:32,012 --> 00:23:35,696
towards sres. Sres are the ones that think of, they're the

336
00:23:35,718 --> 00:23:39,524
ones that budget this entire game

337
00:23:39,562 --> 00:23:44,272
days model. But in the Chaos

338
00:23:44,336 --> 00:23:48,176
pipelines model, all personas are involved.

339
00:23:48,368 --> 00:23:51,690
Shift left is possible, but shift right also is possible

340
00:23:52,140 --> 00:23:56,148
in this approach. Right. So that's

341
00:23:56,164 --> 00:23:59,860
another major difference. So as you can assume,

342
00:23:59,940 --> 00:24:03,572
the chaos gamed model, that option is

343
00:24:03,726 --> 00:24:07,208
very barrier is very high. The barrier

344
00:24:07,304 --> 00:24:11,560
for pipelines is very less because you're doing in a non prod environment

345
00:24:11,720 --> 00:24:14,956
and you have the bandwidth that is

346
00:24:14,978 --> 00:24:18,336
associated to the development, and developers are

347
00:24:18,358 --> 00:24:21,456
the ones who are writing. So it becomes kind

348
00:24:21,478 --> 00:24:25,056
of unnatural for the

349
00:24:25,078 --> 00:24:29,056
adoption of such model. So when it comes to writing

350
00:24:29,168 --> 00:24:32,070
the chaos experiments themselves,

351
00:24:32,840 --> 00:24:36,676
traditionally it's been a challenge because the

352
00:24:36,698 --> 00:24:40,132
code itself is changing. And if

353
00:24:40,186 --> 00:24:44,104
sres are the ones that are writing such

354
00:24:44,302 --> 00:24:47,640
bandwidth is usually not budgeted or planned,

355
00:24:48,220 --> 00:24:52,072
and sres are typically pulled in into the

356
00:24:52,126 --> 00:24:56,156
other pressing needs,

357
00:24:56,258 --> 00:24:59,544
such as attending to incidents and corresponding action tracking,

358
00:24:59,592 --> 00:25:03,708
et cetera, et cetera. So that it may not be always possible to

359
00:25:03,794 --> 00:25:07,868
be proactive in writing a lot of chaos experiment,

360
00:25:07,964 --> 00:25:11,232
right? And in general,

361
00:25:11,286 --> 00:25:15,056
because you are not measuring the resilience coverage kind

362
00:25:15,078 --> 00:25:18,210
of a thing and you are just going and doing game day model,

363
00:25:19,800 --> 00:25:24,560
it's not very clear how many more chaos experiments

364
00:25:24,720 --> 00:25:28,464
I need to develop before I can say that I have covered

365
00:25:28,592 --> 00:25:31,688
all my resilience issues. Right?

366
00:25:31,854 --> 00:25:35,700
But in the continuous resilience approach, these are exactly opposite.

367
00:25:35,780 --> 00:25:40,824
Right? So you are basically looking at each other's help in

368
00:25:40,862 --> 00:25:44,596
a team sport model and you're extending

369
00:25:44,708 --> 00:25:47,944
your regular test. Developers would be writing integration

370
00:25:47,992 --> 00:25:52,092
best. And now you add some more best to

371
00:25:52,146 --> 00:25:54,700
introduce some faults on the dependent components,

372
00:25:55,440 --> 00:25:58,992
and those tests can be reused by QA and QA will add

373
00:25:59,046 --> 00:26:02,304
a little bit more tests. Those can be reused either by

374
00:26:02,342 --> 00:26:05,996
developers or by sres, et cetera, et cetera.

375
00:26:06,108 --> 00:26:10,230
So basically there is an increased sharing of the tests and

376
00:26:11,000 --> 00:26:14,596
in central repositories, or what

377
00:26:14,618 --> 00:26:17,350
you call them as chaos hubs in general.

378
00:26:18,440 --> 00:26:22,276
So you tend to manage these

379
00:26:22,298 --> 00:26:26,216
chaos experiments as code in git, and that increases the

380
00:26:26,238 --> 00:26:29,416
adoption. Right. And with resilience coverage is the

381
00:26:29,438 --> 00:26:33,776
concept, you know exactly how much more coverage

382
00:26:33,828 --> 00:26:37,116
you need to do or how many tests more you need to

383
00:26:37,138 --> 00:26:41,532
write, et cetera, et cetera. So that also helps in general

384
00:26:41,666 --> 00:26:46,196
with planning perspective. Right. So that's

385
00:26:46,248 --> 00:26:50,096
really a kind of a

386
00:26:50,118 --> 00:26:53,516
new pattern to think how to head,

387
00:26:53,548 --> 00:26:57,212
head, head of chaos engineering need to adopt chaos engineering.

388
00:26:57,356 --> 00:27:01,270
That's what I've been observing in the last few years

389
00:27:02,200 --> 00:27:06,196
and also at harness where we are saying there

390
00:27:06,218 --> 00:27:10,260
is a good growth of adoption of chaos

391
00:27:10,600 --> 00:27:15,204
for the purpose of both developer productivity

392
00:27:15,332 --> 00:27:18,404
as well as to increase the resilience as an innovative

393
00:27:18,452 --> 00:27:22,088
metric. Right? So let's take a look at

394
00:27:22,254 --> 00:27:26,444
a couple of DevOps. One on how

395
00:27:26,642 --> 00:27:29,996
you can inject a chaos experiment into a

396
00:27:30,018 --> 00:27:33,550
pipeline and probably cause a rollback depending on

397
00:27:35,120 --> 00:27:38,560
the resilience score that is achieved. And the other one,

398
00:27:38,630 --> 00:27:41,760
a quick demo about how we at chaos,

399
00:27:42,340 --> 00:27:46,320
the development teams are using chaos experiments in the pipeline

400
00:27:46,900 --> 00:27:50,436
a little bit more liberally before the

401
00:27:50,458 --> 00:27:53,584
code is shipped to a preprod environment or a QA

402
00:27:53,632 --> 00:27:54,340
environment.

403
00:28:02,860 --> 00:28:06,632
In this demo, we're going to take a look at how

404
00:28:06,686 --> 00:28:11,640
to achieve continuous resilience using chaos experiments

405
00:28:12,060 --> 00:28:15,470
with a sample chaos engineering tool.

406
00:28:16,240 --> 00:28:19,224
In this case, we are using head, head,

407
00:28:19,272 --> 00:28:22,510
head of chaos engineering,

408
00:28:23,040 --> 00:28:26,828
any other tool, a pipeline tool and a chaos engineering

409
00:28:26,844 --> 00:28:30,192
tool together to achieve the same

410
00:28:30,246 --> 00:28:33,970
continuous resilience. So let's start.

411
00:28:38,440 --> 00:28:41,924
So I have the chaos engineering tool

412
00:28:42,122 --> 00:28:44,820
from harness harness chaos engineering.

413
00:28:45,240 --> 00:28:49,552
This has the concept of chaos experiments,

414
00:28:49,696 --> 00:28:52,520
which are stored in chaos hubs.

415
00:28:53,900 --> 00:28:57,690
These chaos hubs are generally a way to

416
00:28:58,540 --> 00:29:02,724
share the experiments across teams, because in continuous

417
00:29:02,772 --> 00:29:06,508
resilience, you are talking about multiple teams across

418
00:29:06,594 --> 00:29:10,744
different pipeline stages. Either it's

419
00:29:10,792 --> 00:29:14,072
dev or QA or preprod or prod.

420
00:29:14,216 --> 00:29:17,472
So everyone will be using this tool and they will have

421
00:29:17,526 --> 00:29:21,196
access to either common chaos hubs, or they'll be maintaining

422
00:29:21,228 --> 00:29:25,424
their own chaos hubs. This chaos hub can maintain the

423
00:29:25,462 --> 00:29:29,712
chaos faults that are developed and chaos

424
00:29:29,776 --> 00:29:33,300
experiments that are created,

425
00:29:33,800 --> 00:29:36,900
which in turn uses the chaos fault.

426
00:29:37,320 --> 00:29:40,768
So a chaos fault in this case

427
00:29:40,954 --> 00:29:45,156
is nothing but the actual chaos injection

428
00:29:45,348 --> 00:29:49,124
and addition of certain resilience

429
00:29:49,172 --> 00:29:53,144
probes to check a steady state hypothesis. So let me

430
00:29:53,182 --> 00:29:57,064
show how in this harness chaos engineering

431
00:29:57,112 --> 00:30:01,176
tool, a particular chaos experiment

432
00:30:01,288 --> 00:30:05,292
is constructed or been. So let me go

433
00:30:05,346 --> 00:30:08,976
here. If I take a look at a

434
00:30:08,998 --> 00:30:12,524
given chaos experiment, it has multiple chaos

435
00:30:12,572 --> 00:30:16,540
faults. It can have multiple chaos faults

436
00:30:16,620 --> 00:30:20,124
either in series or in parallel. And a given chaos

437
00:30:20,172 --> 00:30:23,556
fault usually will have. Where are

438
00:30:23,578 --> 00:30:27,172
you injecting this vault at your

439
00:30:27,226 --> 00:30:30,996
target application? And what

440
00:30:31,018 --> 00:30:33,370
are the characteristics of the chaos itself?

441
00:30:34,540 --> 00:30:37,736
How long you want to do it, how many times you need

442
00:30:37,758 --> 00:30:41,668
to repeat the chaos, et cetera. And then the probe

443
00:30:41,764 --> 00:30:45,428
in this case is. Different tools call this

444
00:30:45,454 --> 00:30:48,924
probes in different ways. This is basically a way

445
00:30:48,962 --> 00:30:52,344
to check your steady state while this chaos injection

446
00:30:52,392 --> 00:30:56,844
is going on. So in the case of harness chaos

447
00:30:56,892 --> 00:31:00,624
engineering, we use probes to define the resilience of

448
00:31:00,662 --> 00:31:03,936
a given experiment or of

449
00:31:03,958 --> 00:31:07,136
a given service, or of a given module or a

450
00:31:07,158 --> 00:31:11,472
component, right. You can add any number of probes

451
00:31:11,536 --> 00:31:14,500
to a given fault.

452
00:31:15,320 --> 00:31:19,220
So that way you're not just developing on one

453
00:31:19,370 --> 00:31:23,176
probe to check the resilience, you're checking a whole

454
00:31:23,198 --> 00:31:27,384
lot of things while you inject chaos at

455
00:31:27,422 --> 00:31:31,272
any point of time into a given resource, right? Or against

456
00:31:31,326 --> 00:31:36,796
a given resource. So in the case of this

457
00:31:36,898 --> 00:31:39,870
particular chaos experiment, for example,

458
00:31:40,880 --> 00:31:44,990
you can go and see that it has resulted in 100%

459
00:31:46,160 --> 00:31:48,510
resilience, because there were three,

460
00:31:49,700 --> 00:31:53,340
the chaos that was injected was a cpu hog

461
00:31:53,420 --> 00:31:56,636
against a given pod. And while that cpu

462
00:31:56,668 --> 00:32:00,644
hog was injected, there were three process that

463
00:32:00,682 --> 00:32:04,484
were checked whether the pods were okay and some other

464
00:32:04,522 --> 00:32:08,064
service. Was it available or not? The HTTP

465
00:32:08,112 --> 00:32:11,880
endpoint. And it also was checking

466
00:32:12,460 --> 00:32:14,810
a completely different service.

467
00:32:16,060 --> 00:32:20,136
And it's checking for the latency response from

468
00:32:20,158 --> 00:32:24,344
the front end web service. So you should generally

469
00:32:24,392 --> 00:32:28,104
look at the larger picture while gauging

470
00:32:28,152 --> 00:32:31,864
the steady state hypothesis while injecting chaos

471
00:32:31,912 --> 00:32:35,800
fault. So because everything is passed and there's only one fault,

472
00:32:35,880 --> 00:32:39,152
you will see the resilience score as 100%.

473
00:32:39,206 --> 00:32:42,860
So this is how you would generally go and score

474
00:32:43,020 --> 00:32:46,000
the resilience against a given chaos experiments.

475
00:32:46,340 --> 00:32:51,344
And then these chaos experiments generally should be mobile

476
00:32:51,392 --> 00:32:54,624
back into a chaos hub, or you should be able to launch

477
00:32:54,672 --> 00:32:58,368
these experiments from a given chaos hub, et cetera, et cetera.

478
00:32:58,544 --> 00:33:02,244
And in general, the chaos tool should have

479
00:33:02,362 --> 00:33:05,690
the ability to do some access control. For example,

480
00:33:06,220 --> 00:33:09,972
in the case of harness chaos engineering,

481
00:33:10,116 --> 00:33:14,604
you will have default access control against

482
00:33:14,802 --> 00:33:19,128
who can access the centralized library

483
00:33:19,304 --> 00:33:23,432
of chaos hubs and who can execute a given chaos experiment.

484
00:33:23,576 --> 00:33:28,144
And chaos infrastructure is your target agent area.

485
00:33:28,342 --> 00:33:32,944
And if there are game days, who can run these

486
00:33:32,982 --> 00:33:36,656
game days, and typically nobody should have the ability to

487
00:33:36,758 --> 00:33:40,180
remove the reports of game days. So there's no delete option for anyone.

488
00:33:40,250 --> 00:33:44,612
Right? So with this kind of access control and

489
00:33:44,666 --> 00:33:48,196
then the capability of chaos hubs and then the

490
00:33:48,218 --> 00:33:52,984
probes, you will be able to score the

491
00:33:53,022 --> 00:33:56,072
resilience against a given chaos experiment for

492
00:33:56,126 --> 00:33:59,848
a given resource and also be able to share

493
00:34:00,014 --> 00:34:04,068
such developed chaos experiments across multiple different

494
00:34:04,254 --> 00:34:07,964
teams. And now let's go and take a look

495
00:34:08,002 --> 00:34:12,232
at how you can inject these chaos

496
00:34:12,296 --> 00:34:15,020
experiments into pipelines.

497
00:34:16,340 --> 00:34:20,464
Or let's look at the other way. How are you supposed to

498
00:34:20,502 --> 00:34:26,050
achieve continuous resilience during

499
00:34:26,900 --> 00:34:31,270
the deployment stage? Right. So example here,

500
00:34:31,960 --> 00:34:35,716
this pipelines is meant for developing a given service.

501
00:34:35,818 --> 00:34:39,876
That means somebody has kicked off a

502
00:34:39,898 --> 00:34:44,132
deployment of a given service, and once it's deployed,

503
00:34:44,196 --> 00:34:48,040
this could be a complicated process or a complex job in itself.

504
00:34:48,190 --> 00:34:51,560
And once this is deployed, we should

505
00:34:51,630 --> 00:34:55,564
in general add more tests. So this deployment is

506
00:34:55,602 --> 00:34:59,544
supposed to involve some functional tests has. Well, but in addition

507
00:34:59,592 --> 00:35:03,470
to that, you can add more

508
00:35:03,840 --> 00:35:08,320
chaos best. And for example here,

509
00:35:08,390 --> 00:35:12,610
each step in harness pipeline can be

510
00:35:12,980 --> 00:35:18,176
a chaos experiment. And if

511
00:35:18,198 --> 00:35:21,060
you go and look at this chaos experiment,

512
00:35:22,200 --> 00:35:25,844
it's integrated well enough to go and browse in your

513
00:35:25,882 --> 00:35:29,108
same workspace. What are the

514
00:35:29,114 --> 00:35:32,536
chaos experiments that are available? So I'm just going to

515
00:35:32,558 --> 00:35:36,440
go and select certain chaos experiment

516
00:35:37,340 --> 00:35:40,504
here, and then you can set

517
00:35:40,542 --> 00:35:44,092
the expected resilience score against that.

518
00:35:44,226 --> 00:35:47,484
In case that resilience score is

519
00:35:47,522 --> 00:35:51,272
not met, you can go and implement

520
00:35:51,416 --> 00:35:55,272
some failure strategy. Either go and observe, take some manual

521
00:35:55,336 --> 00:35:59,020
intervention, roll back the entire stage,

522
00:35:59,100 --> 00:36:01,810
et cetera, et cetera. So for example,

523
00:36:02,660 --> 00:36:07,612
in this actual case, we have identified

524
00:36:07,756 --> 00:36:11,136
the failure strategy or configured the failure strategy

525
00:36:11,168 --> 00:36:14,564
as a rollback. And typically you can

526
00:36:14,602 --> 00:36:19,364
go and see the past executions of

527
00:36:19,482 --> 00:36:23,828
this pipeline. And let's

528
00:36:23,844 --> 00:36:29,124
say that this has a failed instance

529
00:36:29,172 --> 00:36:33,604
of a pipeline, and you could go and see this

530
00:36:33,662 --> 00:36:36,984
pipeline was deploying

531
00:36:37,032 --> 00:36:40,632
this service and then the chaos

532
00:36:40,696 --> 00:36:44,952
experiment has executed and the expected resilience

533
00:36:45,016 --> 00:36:48,544
has not good enough. And if you go and take a look

534
00:36:48,582 --> 00:36:52,096
at this resilience scores or

535
00:36:52,118 --> 00:36:56,236
probe details, you see that one particular probe

536
00:36:56,428 --> 00:36:59,924
has failed. In this case though,

537
00:37:00,122 --> 00:37:03,248
when cpu was increased,

538
00:37:03,424 --> 00:37:06,996
the pod was good and the court service

539
00:37:07,098 --> 00:37:10,580
where the cpu was injected,

540
00:37:11,100 --> 00:37:14,196
high injection of cpu happened, it was continuing

541
00:37:14,228 --> 00:37:17,332
to available, but some other service provided a latency

542
00:37:17,396 --> 00:37:20,632
issue, so that was not good. And then

543
00:37:20,766 --> 00:37:25,550
it eventually caused it to fail and

544
00:37:26,640 --> 00:37:29,612
the pipeline was rolled back. Right.

545
00:37:29,746 --> 00:37:33,244
So that is an example of how you could

546
00:37:33,282 --> 00:37:37,740
do it, how you could do more and more chaos experiments

547
00:37:37,900 --> 00:37:42,220
into a pipelines and then stop leaking

548
00:37:42,300 --> 00:37:46,140
the resilience bugs to the right. And primarily

549
00:37:46,220 --> 00:37:49,844
what we are trying to say here is we

550
00:37:49,882 --> 00:37:53,504
should encourage the idea of injecting

551
00:37:53,552 --> 00:37:57,444
chaos experiments into the pipelines and sharing these

552
00:37:57,482 --> 00:38:00,660
chaos experiments across teams.

553
00:38:01,180 --> 00:38:04,744
And someone has developed, most likely developers in this case

554
00:38:04,782 --> 00:38:08,570
or QAT members. In any large

555
00:38:09,180 --> 00:38:12,056
deployment or development system,

556
00:38:12,238 --> 00:38:16,300
there are a lot of common services and the teams are distributed,

557
00:38:16,720 --> 00:38:20,076
there are a lot of processes involved. Just like you are

558
00:38:20,098 --> 00:38:23,772
sharing the test cases, common test cases, you could share the

559
00:38:23,826 --> 00:38:27,216
chaos test as well. When you do that, it becomes a

560
00:38:27,238 --> 00:38:33,116
practice. And the practices of injecting

561
00:38:33,148 --> 00:38:36,130
chaos experiments, whenever you test something,

562
00:38:36,660 --> 00:38:39,956
it becomes common and it increases the

563
00:38:39,978 --> 00:38:44,144
adoption of chaos engineering within the organization,

564
00:38:44,272 --> 00:38:47,940
across teams, and it eventually leads to more

565
00:38:48,010 --> 00:38:51,956
stability and less resilience issues or bugs.

566
00:38:52,148 --> 00:38:57,112
So that's a quick way of looking at how

567
00:38:57,166 --> 00:39:00,984
you can use a chaos experimentation tool

568
00:39:01,102 --> 00:39:06,632
and use the chaos experiments to

569
00:39:06,686 --> 00:39:10,136
inject chaos in pipelines and

570
00:39:10,158 --> 00:39:13,496
verify the resilience before they actually go to the

571
00:39:13,518 --> 00:39:16,336
right or go to the next stage. You,

572
00:39:16,478 --> 00:39:17,170
you,

573
00:39:20,500 --> 00:39:24,528
let's look at another demo

574
00:39:24,694 --> 00:39:27,852
for continuous resilience, where you can inject

575
00:39:27,916 --> 00:39:31,750
multiple chaos experiments and

576
00:39:32,360 --> 00:39:36,164
use the resilience score to decide whether to

577
00:39:36,202 --> 00:39:37,590
move forward or not.

578
00:39:39,880 --> 00:39:43,336
So in

579
00:39:43,358 --> 00:39:47,560
this demo, we have a pipelines,

580
00:39:48,060 --> 00:39:51,560
which is being used internally at

581
00:39:51,630 --> 00:39:55,672
harness in one of the module

582
00:39:55,736 --> 00:39:59,516
pipelines. So let's take a look at this particular

583
00:39:59,618 --> 00:40:03,436
pipeline. So what

584
00:40:03,458 --> 00:40:07,376
we have done here is the existing pipeline is

585
00:40:07,478 --> 00:40:10,864
not at all touched, it is

586
00:40:10,902 --> 00:40:14,288
kept as is. Maybe the maintainer of this particular

587
00:40:14,374 --> 00:40:18,096
stage will continue to

588
00:40:18,118 --> 00:40:21,824
focus on the regular deployment and the functional tests

589
00:40:21,872 --> 00:40:25,104
associated with it. And once the functional

590
00:40:25,152 --> 00:40:28,800
tests are completed after deployment,

591
00:40:28,960 --> 00:40:32,616
you can add more chaos tests in

592
00:40:32,718 --> 00:40:36,456
separate stages. In fact, in this particular example,

593
00:40:36,558 --> 00:40:40,744
there are two stages. One to verify the

594
00:40:40,782 --> 00:40:43,960
code changes related to the Chaos module

595
00:40:44,040 --> 00:40:48,280
CE module, and then another stage

596
00:40:48,440 --> 00:40:51,996
that is related to platform

597
00:40:52,098 --> 00:40:55,864
module itself. So you can put all of them into

598
00:40:56,002 --> 00:40:59,970
a group. So here it's called a step group.

599
00:41:01,300 --> 00:41:05,164
So you can just dedicate one single separate

600
00:41:05,212 --> 00:41:08,748
stage to group all the chaos experiments together,

601
00:41:08,934 --> 00:41:12,324
and you can set them up in parallel if

602
00:41:12,362 --> 00:41:17,488
needed. Depending on your use case individually,

603
00:41:17,584 --> 00:41:21,780
each chaos experiments will return some resilience score,

604
00:41:21,940 --> 00:41:25,764
and you can take all the resilience

605
00:41:25,812 --> 00:41:30,472
scores into account and decide at the end whether

606
00:41:30,526 --> 00:41:34,664
you want to continue or take some actions such as rollback.

607
00:41:34,712 --> 00:41:39,640
Right? So in this case, the expected

608
00:41:39,720 --> 00:41:43,020
resilience was all good,

609
00:41:43,090 --> 00:41:46,924
so nothing needs to be done, so it proceeded.

610
00:41:47,052 --> 00:41:51,664
This is another example of how

611
00:41:51,862 --> 00:41:56,508
you can use step groups or multiple chaos experiments

612
00:41:56,684 --> 00:42:00,752
into a separate stage and then take a decision

613
00:42:00,816 --> 00:42:04,240
based on the resilience score. I hope this helps.

614
00:42:04,320 --> 00:42:07,616
This is another simple demo of how do you use multiple

615
00:42:07,648 --> 00:42:17,160
chaos experiments together? You well,

616
00:42:17,230 --> 00:42:19,880
you looked at those two demos.

617
00:42:20,540 --> 00:42:24,664
So in summary, resilience is a real challenge,

618
00:42:24,792 --> 00:42:28,444
and there's an opportunity to increase resilience by

619
00:42:28,482 --> 00:42:32,268
involving developers into the game and start

620
00:42:32,354 --> 00:42:35,812
introducing chaos experimentation in the pipeline.

621
00:42:35,976 --> 00:42:39,856
And you can get ahead of this challenge of resilience by

622
00:42:40,038 --> 00:42:44,640
actually involving the entire DevOps,

623
00:42:45,300 --> 00:42:49,124
rather than just involving on the need basis the

624
00:42:49,162 --> 00:42:52,260
sres alone. Right? So the DevOps

625
00:42:53,480 --> 00:42:57,444
culture of chaos engineering is more scalable and

626
00:42:57,482 --> 00:43:02,116
is actually easy to adopt. Chaos engineering

627
00:43:02,228 --> 00:43:06,088
at scale, it makes it easier. So thank you very much

628
00:43:06,174 --> 00:43:09,544
for watching this talk,

629
00:43:09,742 --> 00:43:13,188
and I'm available at this Twitter

630
00:43:13,204 --> 00:43:17,288
handle or at the Litmus Slack channel.

631
00:43:17,454 --> 00:43:21,096
Feel free to reach out to me if you want to talk to me

632
00:43:21,118 --> 00:43:24,712
about more practical use cases on what I've been seeing

633
00:43:24,846 --> 00:43:28,576
in the field with chaos engineering adoption. Thank you and have

634
00:43:28,598 --> 00:43:29,760
a great conference.

