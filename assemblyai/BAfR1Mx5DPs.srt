1
00:00:23,530 --> 00:00:27,042
Everyone, thank you so much for joining me. My name is Noa Goldman,

2
00:00:27,106 --> 00:00:30,374
and this is the hitchhiker's guide to the ML of experience.

3
00:00:30,572 --> 00:00:34,994
This talk is all about getting into the mind of an ML engineer,

4
00:00:35,122 --> 00:00:38,566
trying to understand their challenges and

5
00:00:38,748 --> 00:00:42,902
trying to think of approaches that

6
00:00:42,956 --> 00:00:46,694
will help them in their day to day jobs. And when

7
00:00:46,732 --> 00:00:50,154
I say approaches, I don't mean specific tools

8
00:00:50,242 --> 00:00:53,566
or technical integrations. I mean in

9
00:00:53,588 --> 00:00:57,840
terms of Ux and in terms of what their experience should be,

10
00:00:58,850 --> 00:01:03,214
so that they will be able to have better and easier workflow.

11
00:01:03,262 --> 00:01:07,282
So let's step into the mind of an ML engineer and

12
00:01:07,336 --> 00:01:11,586
see what are their challenges. So data quality and

13
00:01:11,608 --> 00:01:16,002
availability is these one challenge. They often have to have the

14
00:01:16,056 --> 00:01:19,526
data with the highest quality to create better models, and these need it

15
00:01:19,548 --> 00:01:22,680
to be available for them to use at each time.

16
00:01:23,210 --> 00:01:27,318
They always have to select the most fitted model for them,

17
00:01:27,404 --> 00:01:31,014
the one with the best result, and they constantly have to fine tuning

18
00:01:31,062 --> 00:01:34,666
them. So it's not about developing one model and that's it.

19
00:01:34,688 --> 00:01:38,726
They have to constantly improve it. They have to explain their experiments

20
00:01:38,758 --> 00:01:42,654
and those models. They have to explain the results to people that

21
00:01:42,692 --> 00:01:46,990
are not ML engineers and to other stakeholders.

22
00:01:47,410 --> 00:01:50,622
They need to work with the team. So me

23
00:01:50,676 --> 00:01:54,986
personally, I'm coming from the world of software development, and it's

24
00:01:55,018 --> 00:01:58,514
relatively easy to work in a team. There are a lot of standards and

25
00:01:58,552 --> 00:02:02,754
best practices to use, but I feel like that from ML project,

26
00:02:02,872 --> 00:02:06,674
these standards is not yet to be set. So it's kind of hard,

27
00:02:06,712 --> 00:02:09,590
and it's a challenge to work in a team of ML engineers.

28
00:02:10,250 --> 00:02:13,926
And I think the main challenge is just keeping everything

29
00:02:14,108 --> 00:02:17,202
up to date. This world is constantly evolving,

30
00:02:17,266 --> 00:02:21,162
especially in the past few years, and it's really hard to keep

31
00:02:21,216 --> 00:02:24,486
track of everything and understand what are the new tools

32
00:02:24,518 --> 00:02:27,782
and new technologies. So those are some of the challenges

33
00:02:27,846 --> 00:02:31,294
that ML engineers are experiencing in their day to day

34
00:02:31,332 --> 00:02:34,878
life. And just to give you

35
00:02:34,884 --> 00:02:38,526
a glimpse of how it looks like to try to find

36
00:02:38,628 --> 00:02:41,742
solutions for those challenges. So this is a very

37
00:02:41,796 --> 00:02:47,762
high level, not that technical flow of what

38
00:02:47,816 --> 00:02:51,220
components a model is built out of. So you have the data,

39
00:02:52,230 --> 00:02:55,422
you have experiments, and you have models,

40
00:02:55,486 --> 00:02:58,658
which is the end result almost. There are a

41
00:02:58,664 --> 00:03:01,846
lot of other things on the way, such as deployments and the

42
00:03:01,868 --> 00:03:05,474
code, but I'm trying to keep it really high level. And for each component,

43
00:03:05,522 --> 00:03:08,918
you have a lot of different steps that you have to do

44
00:03:09,084 --> 00:03:12,154
in order to reach your goals. And for each step,

45
00:03:12,272 --> 00:03:16,694
currently, you have tons of tools, you have tons of options, tons of solutions.

46
00:03:16,742 --> 00:03:20,380
Some of them are open sources, some of them are paid products,

47
00:03:21,250 --> 00:03:25,070
and it's really hard to choose and create

48
00:03:25,140 --> 00:03:29,418
the best fitted solution for ML engineers

49
00:03:29,514 --> 00:03:33,086
and their operations. It's almost impossible. You have

50
00:03:33,108 --> 00:03:37,138
to integrate everything. You have to choose the best tools for this specific step.

51
00:03:37,304 --> 00:03:40,366
It's something that takes a lot of time and a lot of effort.

52
00:03:40,478 --> 00:03:43,682
And I think that thinking about

53
00:03:43,736 --> 00:03:47,862
ML engineers problems and challenges will

54
00:03:47,916 --> 00:03:51,558
help you understand which tools you need to use or

55
00:03:51,644 --> 00:03:54,854
which approach you need to take. And I'm here to offer some

56
00:03:54,892 --> 00:03:58,154
approaches to how this experience of

57
00:03:58,192 --> 00:04:02,234
developing a model should look and feels for

58
00:04:02,272 --> 00:04:05,702
ML engineers daily work, that it will be simpler.

59
00:04:05,846 --> 00:04:09,866
So this is what we're here to do before

60
00:04:10,048 --> 00:04:13,566
we'll deep dive into everything a little bit about who I am.

61
00:04:13,668 --> 00:04:17,018
So who am I? I'm Noah Goldman.

62
00:04:17,194 --> 00:04:20,958
As I said, I'm the lead product manager at Dagshub. At Dagshub, we're a

63
00:04:20,964 --> 00:04:23,822
platform that managing and hosting experiments,

64
00:04:23,886 --> 00:04:27,650
code, data annotations, and models all in one place.

65
00:04:27,720 --> 00:04:31,058
We're in mlops tools. I'm an

66
00:04:31,064 --> 00:04:34,974
ex software developer, so I'm coming from the world of software

67
00:04:35,022 --> 00:04:38,694
engineering. I was a developer for around eight years.

68
00:04:38,812 --> 00:04:42,994
I was doing both front end and back end, and then I transitioned

69
00:04:43,042 --> 00:04:46,806
lead lead product manager. Favorite thing in the world would

70
00:04:46,828 --> 00:04:50,474
be to take complex technologies and create simple,

71
00:04:50,672 --> 00:04:55,338
easy to use tools for tech savvy people and

72
00:04:55,424 --> 00:04:58,762
personal think about me. So we will be able to better connect.

73
00:04:58,896 --> 00:05:02,366
Love CrossFit and I recently adopted my new

74
00:05:02,468 --> 00:05:06,666
dog, my new puppy. His name is Hippo. He's really friendly

75
00:05:06,698 --> 00:05:09,600
and really energetic. So this is about me.

76
00:05:10,370 --> 00:05:13,878
And let's deep dive into those approaches that can help ML

77
00:05:13,914 --> 00:05:17,506
engineers improve their day to day lives. So before we do

78
00:05:17,528 --> 00:05:20,450
that, let's talk about what is mlops.

79
00:05:21,270 --> 00:05:25,630
So Mlops is just a set of practices and technologies that

80
00:05:25,720 --> 00:05:28,914
supposed to help ML engineer, develop, deploy,

81
00:05:28,962 --> 00:05:32,850
and manage their models within their lifecycle.

82
00:05:32,930 --> 00:05:36,706
And it's here to ensure that those models are reliable.

83
00:05:36,818 --> 00:05:40,326
So those models supposed to serve users and

84
00:05:40,348 --> 00:05:43,798
they have to be trustworthy eventually that they are scalable,

85
00:05:43,894 --> 00:05:47,798
that we will be able to work at scale. So it can be models

86
00:05:47,814 --> 00:05:51,950
that use other models or large data sets, and they supposed

87
00:05:52,020 --> 00:05:55,934
to help update and maintain all

88
00:05:55,972 --> 00:05:59,754
the lifecycle of developing a model over time. So mlops

89
00:05:59,802 --> 00:06:03,746
is here to help ML engineers focus on what is really important,

90
00:06:03,848 --> 00:06:06,450
which is developing their models.

91
00:06:07,350 --> 00:06:10,142
And it's important for those reasons.

92
00:06:10,206 --> 00:06:13,806
It helps productivity. It's supposed to help ML

93
00:06:13,838 --> 00:06:17,142
engineers focus again on improving their models, and not

94
00:06:17,196 --> 00:06:20,358
on the DevOps or the operations part.

95
00:06:20,444 --> 00:06:23,990
It's supposed to help them collaboration better and be more productive.

96
00:06:24,330 --> 00:06:27,590
It's supposed to help with scale, with reliability,

97
00:06:27,950 --> 00:06:31,434
and to be future proof. So with

98
00:06:31,472 --> 00:06:35,610
this constantly evolving world, mlops approach

99
00:06:36,270 --> 00:06:40,606
will help you to constantly adapt and be

100
00:06:40,628 --> 00:06:43,920
able to be up to date with the new solutions that are out there.

101
00:06:45,970 --> 00:06:48,800
So as I said before,

102
00:06:49,250 --> 00:06:53,074
those are the main components that mlops should cover.

103
00:06:53,192 --> 00:06:57,038
So I specifically these data experiments and models.

104
00:06:57,134 --> 00:07:00,798
Again, this is a very high level flow,

105
00:07:00,974 --> 00:07:04,542
and we'll focus on that. So this is zooming

106
00:07:04,606 --> 00:07:08,354
in for a second, and let's zoom in on data. So what

107
00:07:08,392 --> 00:07:12,150
are the challenges when it's time to data for ML engineers?

108
00:07:12,810 --> 00:07:16,310
So the main challenge is to keep track and manage

109
00:07:16,380 --> 00:07:20,370
and organize all this constantly collected data from various sources.

110
00:07:20,450 --> 00:07:24,010
So I believe, and this is going to be a buzzword here.

111
00:07:24,080 --> 00:07:27,546
So I warned you that to improve your model, you have to be

112
00:07:27,568 --> 00:07:31,910
data centric. So you have to constantly train your models

113
00:07:32,070 --> 00:07:35,680
on new use cases and use as many as you can,

114
00:07:36,770 --> 00:07:40,174
as many use cases as you can, as many data points as you can to

115
00:07:40,212 --> 00:07:43,614
improve your model. So it's less about the code, it's also about the code,

116
00:07:43,652 --> 00:07:47,186
but it's less about the code and more about the data that you use to

117
00:07:47,208 --> 00:07:50,914
train and the variety of use

118
00:07:50,952 --> 00:07:54,514
cases that you use. And when you have tons of data and you collect it

119
00:07:54,552 --> 00:07:58,454
all the time, it's really hard to keep track of it and manage

120
00:07:58,492 --> 00:08:02,390
it and keep it organized, especially when you do it manually. And another

121
00:08:02,460 --> 00:08:06,242
issue with data, it's just, again, to be data centric,

122
00:08:06,306 --> 00:08:10,742
you have to be able to quickly double down on those relevant use cases

123
00:08:10,886 --> 00:08:14,154
on the data point that matters. And you have

124
00:08:14,192 --> 00:08:17,546
to provide an easy approach, an easy solution in

125
00:08:17,568 --> 00:08:20,946
terms of experience for your ML engineers

126
00:08:20,998 --> 00:08:24,414
to do so. When it comes

127
00:08:24,452 --> 00:08:27,886
to the first issue, I think that the

128
00:08:27,908 --> 00:08:31,754
best approach, or at least the approach that worked for me coming from software

129
00:08:31,802 --> 00:08:35,874
development, is data versioning approach. It can help

130
00:08:35,912 --> 00:08:38,962
ML engineers with their daily challenges when it comes to data,

131
00:08:39,016 --> 00:08:42,498
because if they will be able to use the data

132
00:08:42,584 --> 00:08:45,746
like they treat their code, they will be

133
00:08:45,768 --> 00:08:49,554
able to be a lot more productive. They will be able to reproduce specific data

134
00:08:49,592 --> 00:08:52,742
sets that they see that are relevant. So if, for example,

135
00:08:52,796 --> 00:08:56,406
they see their teammate working on a specific data set and coming to a

136
00:08:56,428 --> 00:09:00,474
different result, and they want to test that they can use this specific data set

137
00:09:00,672 --> 00:09:03,818
with the versioning, and they will just supposed to

138
00:09:03,824 --> 00:09:07,514
be able to have just a clear display of the changes that made

139
00:09:07,632 --> 00:09:11,534
over this data set and maybe better make sense of the

140
00:09:11,572 --> 00:09:14,350
progress that was made during evolving a model.

141
00:09:14,420 --> 00:09:18,522
And another thing that versioning is supposed to help, again, with productivity

142
00:09:18,586 --> 00:09:22,062
is teamwork. They will

143
00:09:22,116 --> 00:09:25,426
be able to collaborate faster if we will

144
00:09:25,448 --> 00:09:28,994
use that approach of software development. So being

145
00:09:29,032 --> 00:09:32,626
able to create pull requests or comments or issues and just have

146
00:09:32,648 --> 00:09:36,758
a really organized way to work together within a

147
00:09:36,764 --> 00:09:40,806
team and have it all organized and

148
00:09:40,828 --> 00:09:44,566
managed in one place, and not just manually, it's supposed to

149
00:09:44,588 --> 00:09:48,206
help ML engineers and ML engineers

150
00:09:48,258 --> 00:09:51,978
teams work a lot more productive and better, and just to create

151
00:09:52,064 --> 00:09:55,514
a cleaner, simpler environment. And it

152
00:09:55,552 --> 00:09:59,322
will help them focus on developing better models and not

153
00:09:59,376 --> 00:10:02,586
on these operations around it, and not just manually

154
00:10:02,778 --> 00:10:05,550
changing names and changing data points.

155
00:10:05,620 --> 00:10:09,758
So data versioning is the approach we choose. For example,

156
00:10:09,924 --> 00:10:13,566
showing your data like you would show different in your

157
00:10:13,588 --> 00:10:16,786
code within a software management project. This isn't one way to do this,

158
00:10:16,808 --> 00:10:21,042
and this is one approach, or again, just using comments or

159
00:10:21,096 --> 00:10:24,306
issues with your data set, that's supposed to

160
00:10:24,328 --> 00:10:26,470
help keep things really organized.

161
00:10:28,410 --> 00:10:31,526
Another thing that's supposed to help when it

162
00:10:31,548 --> 00:10:34,806
comes to picking and choosing specific data points and relevant use

163
00:10:34,828 --> 00:10:38,506
cases is visualization, which is one

164
00:10:38,528 --> 00:10:42,678
of my favorite tools to use. So data visualization

165
00:10:42,854 --> 00:10:46,086
and creating a very clear display

166
00:10:46,118 --> 00:10:51,530
of your data, supposed to help data

167
00:10:51,600 --> 00:10:55,278
scientists and ML engineers just focus on these right data points and the

168
00:10:55,284 --> 00:10:58,654
relevant use cases. And this data should not just show the data

169
00:10:58,692 --> 00:11:02,160
itself, but you should also show what matters, which is,

170
00:11:02,530 --> 00:11:06,766
I like to call it enrichment. So for example, it can be metadata,

171
00:11:06,878 --> 00:11:10,754
annotations, predictions, everything that, according to which an

172
00:11:10,792 --> 00:11:14,914
ML engineer can pick and choose what is most relevant for them

173
00:11:15,112 --> 00:11:16,520
to improve the model.

174
00:11:17,770 --> 00:11:21,334
When thinking of how to solve the problem of

175
00:11:21,372 --> 00:11:25,142
quickly picking and choosing and creating training ready

176
00:11:25,196 --> 00:11:29,226
data sets, you have to provide a way, and I'm not

177
00:11:29,248 --> 00:11:32,858
saying which tools exactly, but you have to provide a way to really

178
00:11:32,944 --> 00:11:36,714
fast filtering and sorting the

179
00:11:36,752 --> 00:11:40,494
data sets so they will be able to create the best sub

180
00:11:40,532 --> 00:11:44,110
data sets possible and the most relevant one. And after you

181
00:11:44,180 --> 00:11:48,602
help them create those sub data sets

182
00:11:48,746 --> 00:11:52,574
and these relevant use cases, you need to

183
00:11:52,612 --> 00:11:55,874
create a way for them to use those data sets. So it's not just

184
00:11:55,912 --> 00:11:59,490
about filtering and seeing visually the specific

185
00:11:59,560 --> 00:12:02,738
use case of a data set, it's also what to

186
00:12:02,744 --> 00:12:06,926
do about it. So, for example, a thing that will be really

187
00:12:06,968 --> 00:12:10,658
helpful is to create a quick way to send those data sets

188
00:12:10,674 --> 00:12:14,498
to annotations or to be able to download

189
00:12:14,674 --> 00:12:17,990
a snippet of this data set to use and retrain your model.

190
00:12:18,060 --> 00:12:21,514
So constantly think about, first of all,

191
00:12:21,552 --> 00:12:25,658
how to help ML engineers focus and zoom in

192
00:12:25,824 --> 00:12:29,222
on the most relevant use cases when it comes to huge data sets,

193
00:12:29,286 --> 00:12:33,082
but also help them take action easily.

194
00:12:33,146 --> 00:12:36,682
And go to the next step of the flow to again help them generate

195
00:12:36,746 --> 00:12:38,590
experiments a lot faster.

196
00:12:40,290 --> 00:12:43,554
So for example, show the data side by side to

197
00:12:43,592 --> 00:12:47,534
the metadata that relevant to it, show metadata,

198
00:12:47,582 --> 00:12:51,314
the annotations and on it the predictions and have

199
00:12:51,352 --> 00:12:55,620
a clear way for them to use it to filter and

200
00:12:56,070 --> 00:12:59,474
clear way. For example, some data

201
00:12:59,512 --> 00:13:03,154
scientists like to use Python client, which is cool. They like to syntax

202
00:13:03,202 --> 00:13:06,726
and do all the commands there and also, but just a very clear intuitive way

203
00:13:06,748 --> 00:13:10,714
to just filter and sort things and to be able to focus

204
00:13:10,832 --> 00:13:13,882
on the relevant things and see it visually. So these is

205
00:13:13,936 --> 00:13:17,162
one way that can help data scientists create

206
00:13:17,216 --> 00:13:21,046
experiments a lot faster by creating relevant data sets

207
00:13:21,078 --> 00:13:24,622
or sub data sets a lot faster with those abilities and just help

208
00:13:24,676 --> 00:13:28,154
them go to the next step a lot faster

209
00:13:28,282 --> 00:13:31,342
by helping these send it to annotation in a very clear

210
00:13:31,396 --> 00:13:35,450
and easy way or having those behind the scene integration

211
00:13:35,530 --> 00:13:38,866
relabeling tools that will help them avoid the process of the

212
00:13:38,888 --> 00:13:42,180
operation or just help them download these data set.

213
00:13:43,430 --> 00:13:46,934
Yeah, so we spoke about data, the challenges the data has

214
00:13:47,052 --> 00:13:50,758
and what approaches these should

215
00:13:50,924 --> 00:13:55,506
be taken care when it comes to helping

216
00:13:55,538 --> 00:13:59,670
ML engineers using their time

217
00:13:59,740 --> 00:14:03,478
wisely and avoid those challenges when it

218
00:14:03,484 --> 00:14:06,902
comes to these data. And let's deep dive into

219
00:14:06,956 --> 00:14:10,542
experiments. So the challenge just with

220
00:14:10,596 --> 00:14:13,360
experiments, again, there are a lot of challenges there.

221
00:14:14,130 --> 00:14:17,242
The main one is that ML engineers have to experiment

222
00:14:17,306 --> 00:14:20,190
fast and they have to keep track of those changes.

223
00:14:20,260 --> 00:14:23,870
So it's a lot similar to the challenges for

224
00:14:23,940 --> 00:14:27,822
data components. They just need to constantly experiment

225
00:14:27,886 --> 00:14:31,822
and they need to make sure they are keeping track of those changes, because eventually

226
00:14:31,966 --> 00:14:35,426
you want to create the best model for you. And to do

227
00:14:35,448 --> 00:14:38,470
that, you need to understand what got you there,

228
00:14:38,540 --> 00:14:42,038
like what got to the point where those are these results of

229
00:14:42,044 --> 00:14:45,446
the model. And another main challenge is that

230
00:14:45,468 --> 00:14:49,634
they have to communicate the result for

231
00:14:49,692 --> 00:14:53,014
non ML engineers or for just their stakeholders.

232
00:14:53,062 --> 00:14:56,826
So if they got to a specific result

233
00:14:56,928 --> 00:15:00,970
from a specific model and they believe that this is the right way to go,

234
00:15:01,040 --> 00:15:04,398
this is the best option. It's not enough.

235
00:15:04,484 --> 00:15:07,934
They have to tell their bosses or their managers or

236
00:15:07,972 --> 00:15:11,774
their managers, like, this is the right thing to do, and communicating and

237
00:15:11,812 --> 00:15:15,250
explaining and convincing others that this is the best model

238
00:15:15,320 --> 00:15:17,540
possible. It's not that easy.

239
00:15:18,630 --> 00:15:21,554
So we need to think of the approach that will make it easy for them.

240
00:15:21,592 --> 00:15:23,700
So just a sec.

241
00:15:27,050 --> 00:15:30,950
Experiment tracking, obviously this is the way to go. And this is these approach

242
00:15:31,530 --> 00:15:34,200
and experiment tracking. Again, much like data,

243
00:15:35,130 --> 00:15:38,454
it's all about display and comparison and just

244
00:15:38,492 --> 00:15:42,346
visual output. So experiment displays is not just about the

245
00:15:42,368 --> 00:15:46,026
end result, it's about showing the hyperparameters, these metric, these result,

246
00:15:46,208 --> 00:15:49,626
everything that the data science care about that

247
00:15:49,648 --> 00:15:53,034
these data scientist cares about when it comes to how it got to those results.

248
00:15:53,082 --> 00:15:56,910
So all the collaboration, everything needs to be really well displayed

249
00:15:57,410 --> 00:16:01,278
and you have to have the ability to have comparison of

250
00:16:01,284 --> 00:16:05,102
these results, obviously, and just the visual output.

251
00:16:05,246 --> 00:16:08,638
For non ML stakeholders, I feel like experiment tracking,

252
00:16:08,734 --> 00:16:12,274
this is the most standard way

253
00:16:12,312 --> 00:16:15,940
to go. Like this is the component that has the most

254
00:16:16,310 --> 00:16:19,702
standard approaches these days. So I'm not going

255
00:16:19,756 --> 00:16:23,510
to talk about it tools long, but I really think that the most important

256
00:16:23,580 --> 00:16:27,154
part these is just the visual output for non ML

257
00:16:27,282 --> 00:16:31,174
stakeholders. You have to find a way to help ML engineers

258
00:16:31,302 --> 00:16:34,970
convince these bosses that the job that they were doing up until

259
00:16:35,040 --> 00:16:38,694
now paid it off and to show them the results

260
00:16:38,742 --> 00:16:42,134
clearly. So obviously, for experiment tracking,

261
00:16:42,182 --> 00:16:46,346
you just want to show these experiments, you want to show these hyperparameters, the metrics

262
00:16:46,378 --> 00:16:50,078
and all that. This is very basic, but you also want to make sure

263
00:16:50,244 --> 00:16:53,842
that you have a way to explain to stakeholders what

264
00:16:53,896 --> 00:16:57,150
happened, how different features affected those models.

265
00:16:57,230 --> 00:17:00,530
And you have to think of a way

266
00:17:00,680 --> 00:17:04,498
that an ML engineer won't have to go through tools much

267
00:17:04,584 --> 00:17:09,062
in order to explain those results. So perhaps find

268
00:17:09,116 --> 00:17:12,902
a good way to expert images that explain what

269
00:17:12,956 --> 00:17:16,690
happened in experiments. And always think about non ML engineers

270
00:17:16,850 --> 00:17:19,946
because data scientists pretty much will understand.

271
00:17:20,048 --> 00:17:23,574
But what about these managers and these managers, how you convince

272
00:17:23,622 --> 00:17:27,402
them? Think about a good way to convince others that

273
00:17:27,456 --> 00:17:29,740
this is the best model fitted for you.

274
00:17:30,770 --> 00:17:35,226
Okay, zooming out again. Final thing is models and the approach

275
00:17:35,418 --> 00:17:39,790
spoiler alert is pretty similar to data and experiments.

276
00:17:41,410 --> 00:17:44,546
What are the challenges? With model? Pretty similar. You have

277
00:17:44,568 --> 00:17:48,082
to keep track of your models, you have to

278
00:17:48,136 --> 00:17:51,986
be able to access your model really easily, and you have to be able to

279
00:17:52,168 --> 00:17:55,230
compare the different versions, especially at scale.

280
00:17:55,310 --> 00:17:58,750
When we are developing models, it's not about just one model that

281
00:17:58,760 --> 00:18:02,818
we want to develop. It's often a pipeline, it's often tons

282
00:18:02,834 --> 00:18:06,246
of different versions of a model. So we want to be able to do that

283
00:18:06,268 --> 00:18:10,142
at scale. And again, much like data and much like experiments,

284
00:18:10,226 --> 00:18:13,980
we want to collaborate, we want to have an easy way to reproduce a model

285
00:18:15,310 --> 00:18:18,986
that we want to use. But it's not just about reproducing in

286
00:18:19,008 --> 00:18:22,862
terms of collaboration, there's also reproducing in terms

287
00:18:22,916 --> 00:18:26,126
of production. So this is an

288
00:18:26,148 --> 00:18:29,790
approach that I've taken from software development, which is

289
00:18:29,940 --> 00:18:33,438
okay, we deployed a model to production. That's cool, but it's

290
00:18:33,454 --> 00:18:37,186
in production now, meaning that if something happens, we need to

291
00:18:37,208 --> 00:18:40,500
have a really fast way to go back

292
00:18:41,350 --> 00:18:44,894
and get the model that will fit the solution

293
00:18:44,942 --> 00:18:47,910
and have no problems, especially when it comes to production.

294
00:18:48,250 --> 00:18:52,120
So ML engineers have to have a very clear way to do that

295
00:18:53,210 --> 00:18:57,026
and not to have to go through DevOps. So the mlops

296
00:18:57,138 --> 00:19:00,950
purpose here is really important. It's supposed to help ML engineers

297
00:19:01,030 --> 00:19:03,500
do all that, compare, have access,

298
00:19:04,510 --> 00:19:08,646
collaboration on models at scale, but also to reproduce to production

299
00:19:08,838 --> 00:19:12,622
easily and without having to involve anyone else

300
00:19:12,676 --> 00:19:16,622
in this. Obviously the solution is

301
00:19:16,676 --> 00:19:20,320
model registry, but it's not just about at least of your model.

302
00:19:21,010 --> 00:19:24,622
When you build a model registry, or when you use a specific tool that implements

303
00:19:24,686 --> 00:19:28,958
model registry, you want to help ML engineers collaborate effectively.

304
00:19:29,054 --> 00:19:32,386
So have an easy access to those models. You want

305
00:19:32,408 --> 00:19:35,774
them to have one location where all those models will be managed

306
00:19:35,822 --> 00:19:39,640
at, all the relevant details supposed to be there.

307
00:19:40,410 --> 00:19:44,226
You don't want them to have to look for something. Think of filtering

308
00:19:44,258 --> 00:19:48,242
ways, think about all the relevant data that's supposed to be in this registry.

309
00:19:48,386 --> 00:19:51,666
And I think the most important thing when you're

310
00:19:51,698 --> 00:19:55,386
deploying your models to production and you want to do this in scale and you

311
00:19:55,408 --> 00:19:58,726
want to do this fast, is when you integrate deployment tools

312
00:19:58,758 --> 00:20:02,094
or you think about deployment solution, think about what will

313
00:20:02,132 --> 00:20:05,886
be the easy and most intuitive ways to deploy your

314
00:20:05,908 --> 00:20:09,246
models to productions from the approach of

315
00:20:09,268 --> 00:20:13,246
ML engineer. So I know that usually this process

316
00:20:13,348 --> 00:20:16,990
involves a DevOps or a software

317
00:20:17,070 --> 00:20:21,202
engineer, but we do want to pass from

318
00:20:21,256 --> 00:20:24,626
that, and we do eventually want to have ML engineers be

319
00:20:24,648 --> 00:20:27,718
able to do this on their own, or at least do most of it on

320
00:20:27,724 --> 00:20:31,206
their own, or at least understand what's happening there. So we need to think of

321
00:20:31,228 --> 00:20:34,626
approach that will be really easy and intuitive for ML

322
00:20:34,658 --> 00:20:38,146
engineers to easily deploy their models.

323
00:20:38,178 --> 00:20:41,398
And if not do the full process, at least do most of it and understand

324
00:20:41,484 --> 00:20:44,982
it. This is at least my take on this from software development.

325
00:20:45,046 --> 00:20:48,586
So there are a lot of tools that do this, obviously, but you have

326
00:20:48,608 --> 00:20:52,314
to think of a way that shows the most relevant details,

327
00:20:52,362 --> 00:20:55,040
the status of this model, where it is,

328
00:20:56,930 --> 00:21:00,746
who challenges last, and just an easy way to deploy

329
00:21:00,778 --> 00:21:04,574
it, easy and intuitive way to deploy it. So this is what I

330
00:21:04,612 --> 00:21:07,902
would think about if I think to make ML engineers

331
00:21:07,966 --> 00:21:11,730
lives a lot easier. And the final thing,

332
00:21:11,800 --> 00:21:15,006
this is not part of the components. This is just me as a

333
00:21:15,048 --> 00:21:18,038
product manager for devtools. Bring it with me.

334
00:21:18,124 --> 00:21:21,766
I think a lot of the tools that are available today are

335
00:21:21,868 --> 00:21:25,654
missing simplification, which is they

336
00:21:25,692 --> 00:21:29,106
have tons of functionalities, tools of abilities. They claim

337
00:21:29,138 --> 00:21:32,714
that they do a lot and they forget to keep

338
00:21:32,752 --> 00:21:36,266
things simple. As an ML engineer, you're not

339
00:21:36,288 --> 00:21:39,546
always aware of all those abilities or you not always want to use those

340
00:21:39,568 --> 00:21:43,470
abilities or you don't want the complexity. You just want to keep simple.

341
00:21:43,540 --> 00:21:47,726
So I would say when you build an internal tools for

342
00:21:47,828 --> 00:21:52,126
your ML team, or when you choose to integrate specific tool to

343
00:21:52,148 --> 00:21:56,094
your workflow, make sure you keep things clean. Make sure you don't

344
00:21:56,142 --> 00:21:59,934
add extra abilities and features

345
00:21:59,982 --> 00:22:03,058
just because you think someone thinks it's cool.

346
00:22:03,224 --> 00:22:06,774
Make sure you think about the process and add only the things

347
00:22:06,812 --> 00:22:10,854
that needs to be there. Another thing that

348
00:22:10,892 --> 00:22:14,774
when you integrate tools, make sure your ML engineers love them. Don't just

349
00:22:14,812 --> 00:22:18,250
force them on using them and make them feel like home.

350
00:22:18,320 --> 00:22:21,466
If they like use pandas, integrate things that

351
00:22:21,488 --> 00:22:25,194
are similar in the behavior. Just give them the look

352
00:22:25,232 --> 00:22:28,940
and feel of their home and have them

353
00:22:30,110 --> 00:22:32,800
love it and have it easy to use.

354
00:22:33,890 --> 00:22:37,806
This is more of a product manager's approach which is just

355
00:22:37,828 --> 00:22:41,850
think about the flows, don't think about the features,

356
00:22:41,930 --> 00:22:45,786
don't think about the tools, think about the flow, think about an ML

357
00:22:45,818 --> 00:22:49,182
engineer. What do they

358
00:22:49,236 --> 00:22:52,606
trying to do when they wake up in the morning and they have a task?

359
00:22:52,718 --> 00:22:56,494
What is the step by step flow that they

360
00:22:56,552 --> 00:23:00,946
supposed to reach? And think about that when it comes to creating

361
00:23:00,978 --> 00:23:04,498
your ML models, sorry, when it comes to creating

362
00:23:04,594 --> 00:23:07,350
your mlops workflow.

363
00:23:08,890 --> 00:23:12,680
So those are some examples. Make them look and feel at home.

364
00:23:14,330 --> 00:23:17,894
That's it. I am Noah Goldman. Here is my

365
00:23:17,932 --> 00:23:21,934
email. Feel free to tell me about your mlops experience and

366
00:23:21,972 --> 00:23:25,454
how you overcome those challenges and what

367
00:23:25,492 --> 00:23:29,246
are the challenges of your ML engineers on their daily job.

368
00:23:29,428 --> 00:23:30,220
Thank you so much.

