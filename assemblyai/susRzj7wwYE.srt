1
00:00:24,010 --> 00:00:27,606
Hi there, this is chaos engineering and service ownership at

2
00:00:27,628 --> 00:00:31,106
enterprise Scale. My name's Jay and I'm a lead chaos engineer

3
00:00:31,138 --> 00:00:34,742
at Salesforce. I've been with our site reliability team for about

4
00:00:34,796 --> 00:00:38,802
six years, working on public cloud operations, incident response,

5
00:00:38,866 --> 00:00:42,278
and our Chaos game day program. Now I'm an engineer on

6
00:00:42,284 --> 00:00:45,702
the Chaos platform team where we're delivering a toolkit to service

7
00:00:45,756 --> 00:00:49,106
owners that lets them safely run chaos experiments whenever

8
00:00:49,138 --> 00:00:52,206
they'd like. I'm here today to share with you some of what

9
00:00:52,228 --> 00:00:55,806
we've learned while shifting left and scaling up in the public cloud.

10
00:00:55,988 --> 00:00:59,774
But our story actually starts about nine years ago in

11
00:00:59,812 --> 00:01:03,162
2015. In these days, Salesforce infrastructure

12
00:01:03,226 --> 00:01:06,946
was hosted out of first party data centers. We had

13
00:01:06,968 --> 00:01:10,574
a simpler application footprint, security controls and ownership

14
00:01:10,622 --> 00:01:14,206
models. In short, SRE owned the availability

15
00:01:14,318 --> 00:01:18,386
of pretty much everything, and sres had widely

16
00:01:18,418 --> 00:01:22,066
scoped privileged shell access to be able to log into servers

17
00:01:22,178 --> 00:01:26,054
and take actions for incident mitigation or maintenance and so

18
00:01:26,092 --> 00:01:29,594
forth. Our Chaos game day program made use

19
00:01:29,632 --> 00:01:33,494
of this, and we would rely on someone with that privileged shell

20
00:01:33,542 --> 00:01:37,210
access to log in to a server and shut it off

21
00:01:37,280 --> 00:01:40,746
or kill processes on the node. Or sometimes we

22
00:01:40,768 --> 00:01:44,298
relied on a tight partnership with network and data center engineers

23
00:01:44,394 --> 00:01:48,510
to do things like turn off network switch ports, or remove power

24
00:01:48,580 --> 00:01:52,206
from hosts manually. So a game day back

25
00:01:52,228 --> 00:01:55,470
in 2015 might have looks like this, where an SRE

26
00:01:55,550 --> 00:01:58,706
would shell in and run a privileged command, and at

27
00:01:58,728 --> 00:02:02,654
the same time the game day team would be observing critical

28
00:02:02,702 --> 00:02:06,470
host or app metrics and writing down their findings.

29
00:02:08,410 --> 00:02:12,674
But at the same time, Salesforce was undergoing some other transformations

30
00:02:12,802 --> 00:02:16,018
that involved infrastructure and service ownership.

31
00:02:16,194 --> 00:02:20,026
New business needs demanded larger and more flexible infrastructure, and this

32
00:02:20,048 --> 00:02:24,730
is things like new products and sales growth, as well as new

33
00:02:24,880 --> 00:02:28,906
regulatory requirements across the world. So this leads to

34
00:02:28,928 --> 00:02:32,506
the birth of the public cloud infrastructure known as Hyperforce.

35
00:02:32,618 --> 00:02:36,586
And for internal engineers, Hyperforce enforces a lot of new internal

36
00:02:36,618 --> 00:02:40,394
requirements and operational practices. The new infrastructure

37
00:02:40,442 --> 00:02:44,194
brought a bevy of foundational shared services that

38
00:02:44,232 --> 00:02:47,506
were used to ensure a safe and zero trust sort

39
00:02:47,528 --> 00:02:51,060
of environment that developers would build on top of.

40
00:02:51,430 --> 00:02:55,074
It also eliminated most manual interactive touch

41
00:02:55,112 --> 00:02:58,774
points, such as shell access. Underlying these

42
00:02:58,812 --> 00:03:01,670
changes was also the embrace of service ownership.

43
00:03:02,090 --> 00:03:05,798
Salesforce decided early on in the process that service owners would

44
00:03:05,804 --> 00:03:09,734
be responsible for the end to end operational health of their service and would

45
00:03:09,772 --> 00:03:13,530
no longer be able to sort of give that burden to SRE.

46
00:03:14,190 --> 00:03:17,386
So in short, we all had to learn new ways of working in the

47
00:03:17,408 --> 00:03:21,434
public cloud, which simply didn't apply to our first party data centers.

48
00:03:21,562 --> 00:03:25,066
And while we reimagined the infra and app ownership models.

49
00:03:25,178 --> 00:03:28,570
Our approach needed to be the same for operational

50
00:03:28,650 --> 00:03:33,566
things such AWS chaos Engineering we

51
00:03:33,588 --> 00:03:36,686
started to imagine what chaos engineering as a part of service ownership

52
00:03:36,718 --> 00:03:39,490
would look like, and we uncovered a few challenges.

53
00:03:39,910 --> 00:03:42,898
First off, in a service ownership world,

54
00:03:42,984 --> 00:03:46,520
SRE has less of a centralized role than before.

55
00:03:46,890 --> 00:03:50,646
A centralized game day team would have difficulty learning all

56
00:03:50,668 --> 00:03:53,814
the architectures and edge cases of new or

57
00:03:53,852 --> 00:03:57,566
newly redesigned services for the new public cloud architecture.

58
00:03:57,698 --> 00:04:01,654
And finally, like I mentioned, new technical containers around privileged

59
00:04:01,702 --> 00:04:05,382
access made previous chaos approaches unsuitable

60
00:04:05,446 --> 00:04:06,730
for Salesforce.

61
00:04:09,070 --> 00:04:12,934
But if we started to look at shifting our approach, we could frame

62
00:04:12,982 --> 00:04:16,738
these issues differently. First off, service owners

63
00:04:16,774 --> 00:04:20,862
know their service better than anybody else, so they'll be the best equipped to

64
00:04:20,916 --> 00:04:24,062
talk about potential failure points as well as potential

65
00:04:24,126 --> 00:04:26,866
mediations to those. Also,

66
00:04:26,968 --> 00:04:30,194
shifting left in the development cycle should reduce turnaround time

67
00:04:30,232 --> 00:04:33,602
on discovering and fixing the issues. Moving it closer left

68
00:04:33,656 --> 00:04:37,986
in the development lifecycle will let a service owner respond to the problem faster

69
00:04:38,098 --> 00:04:41,622
than if they had waited until it reached prod and a customer found

70
00:04:41,676 --> 00:04:45,046
it. Finally, we decided that we should deliver a

71
00:04:45,068 --> 00:04:48,398
chaos engineering platform that lets service owners run chaos

72
00:04:48,434 --> 00:04:52,330
experiments safely and easily, eliminating the need for

73
00:04:52,400 --> 00:04:54,300
manual interactive touch points.

74
00:04:56,190 --> 00:04:59,226
And this was a great idea and it's working out well for us. But there

75
00:04:59,248 --> 00:05:02,526
were some major scale and shift left challenges to address

76
00:05:02,708 --> 00:05:06,286
the size and shape of our AWS footprint, the need to

77
00:05:06,308 --> 00:05:09,966
granularly attack multi tenant compute clusters, the need to

78
00:05:09,988 --> 00:05:12,970
discover inventory and manage RBAC access,

79
00:05:13,140 --> 00:05:17,166
and finally, maintaining safety, observability and observing

80
00:05:17,198 --> 00:05:20,530
the outcomes while doing all of the above.

81
00:05:22,550 --> 00:05:26,146
Let's talk specifically about our AWS footprint. The core

82
00:05:26,178 --> 00:05:29,538
CRM product that most people think of when they think of Salesforce

83
00:05:29,634 --> 00:05:33,254
is hundreds of services spanning across at

84
00:05:33,292 --> 00:05:36,950
least 78, about 80 AWS accounts.

85
00:05:37,370 --> 00:05:40,474
And for one contrived example, a service might

86
00:05:40,512 --> 00:05:44,086
have their application container running on one shared

87
00:05:44,118 --> 00:05:47,994
compute cluster in an account over here, but then might have

88
00:05:48,032 --> 00:05:51,654
their database and cache and s three buckets, all in different

89
00:05:51,712 --> 00:05:55,706
accounts. So to begin with, it's infeasible for humans

90
00:05:55,738 --> 00:05:59,662
to log into every AWS account we have for core and do

91
00:05:59,716 --> 00:06:03,634
failure experiments. At the very least, we need to script that.

92
00:06:03,832 --> 00:06:07,186
So we translate to a requirement that says that we need a

93
00:06:07,208 --> 00:06:11,694
privileged chaos engineering platform that can run AWS attacks

94
00:06:11,742 --> 00:06:14,530
in multiple accounts simultaneously.

95
00:06:16,310 --> 00:06:20,006
The second challenge is around our multi tenant Kubernetes clusters that I

96
00:06:20,028 --> 00:06:23,602
alluded to before. Services here are deployed

97
00:06:23,666 --> 00:06:26,466
across many namespaces, but also many clusters,

98
00:06:26,578 --> 00:06:29,894
and so service owners should be able to attack their service and

99
00:06:29,932 --> 00:06:33,498
their service namespace. But in the multiple clusters that

100
00:06:33,504 --> 00:06:37,446
they might be deployed into. At the same time, service owners shouldn't

101
00:06:37,478 --> 00:06:41,360
be allowed to attack the shared services or the cluster worker nodes themselves.

102
00:06:41,970 --> 00:06:45,994
And finally, service owners may know or care less about Kubernetes

103
00:06:46,042 --> 00:06:48,910
infrastructure than we assume.

104
00:06:49,410 --> 00:06:53,374
So to translate to requirements, we need a privileged chaos

105
00:06:53,422 --> 00:06:57,314
engineering platform that can orchestrate attacks in multiple namespaces and

106
00:06:57,352 --> 00:07:00,818
clusters simultaneously, just like how we need to drive

107
00:07:00,904 --> 00:07:04,306
multiple AWS accounts. Also, we need

108
00:07:04,328 --> 00:07:08,114
the platform to provide failures without requiring

109
00:07:08,162 --> 00:07:11,942
ad hoc cluster configs and service accounts. Basically, the service

110
00:07:11,996 --> 00:07:15,522
owner should not need to know all the inner workings of the Kubernetes

111
00:07:15,586 --> 00:07:19,320
cluster to be able to do chaos attacks on their service.

112
00:07:21,370 --> 00:07:25,114
The third challenge is around inventory and role based access,

113
00:07:25,312 --> 00:07:29,242
so it's obviously a challenge to discover an account for all the different

114
00:07:29,296 --> 00:07:32,190
resource types that a service team might own.

115
00:07:32,340 --> 00:07:35,962
As I've said, we're spanning nearly 80 AWS accounts,

116
00:07:36,026 --> 00:07:39,834
and within those we've got EC two instances and s three buckets

117
00:07:39,882 --> 00:07:43,490
and RDS databases in addition to the containers.

118
00:07:44,070 --> 00:07:47,714
So discovering and making a sensible inventory of those

119
00:07:47,752 --> 00:07:50,914
is a challenge. Also, enforcing role based

120
00:07:50,952 --> 00:07:54,558
access could be a challenge. We need to control the blast radius

121
00:07:54,654 --> 00:07:58,054
and know that when a service owner logs into the platform, they should

122
00:07:58,092 --> 00:08:01,302
only be allowed to do chaos attacks on a service that they own,

123
00:08:01,436 --> 00:08:04,566
again removing shared infra and service components from

124
00:08:04,588 --> 00:08:07,774
the blast radius. So for requirements,

125
00:08:07,842 --> 00:08:11,610
the chaos engineering platform should integrate with discover and group

126
00:08:11,680 --> 00:08:15,402
all sorts of infrastructure, resources and

127
00:08:15,456 --> 00:08:18,954
applications. Also, the chaos platform should integrate with

128
00:08:18,992 --> 00:08:22,526
SSO to match service owners to their services. And our

129
00:08:22,548 --> 00:08:26,346
chaos platform needs to make use of the opinionated hyperforce

130
00:08:26,458 --> 00:08:29,518
tagging and labeling scheme to make sure that

131
00:08:29,524 --> 00:08:33,070
we're grouping services and service owners together appropriately.

132
00:08:34,470 --> 00:08:38,350
And the fourth challenge is around safety, observability and outcomes.

133
00:08:38,510 --> 00:08:41,986
We started to ask ourselves questions like what would happen if

134
00:08:42,008 --> 00:08:46,022
there was an ongoing incident or maintenance or patching. It might be

135
00:08:46,076 --> 00:08:49,414
unsafe for service owners to run experiments at this time.

136
00:08:49,612 --> 00:08:52,806
And also, how should service owners measure the success of

137
00:08:52,828 --> 00:08:57,160
their chaos experiments? How do we track improvement so

138
00:08:57,870 --> 00:09:01,206
concretely? Translating to requirements, the chaos engineering

139
00:09:01,238 --> 00:09:05,446
platform should integrate with our change and incident management database

140
00:09:05,558 --> 00:09:08,250
and refuse to attack if it's unsafe.

141
00:09:08,590 --> 00:09:11,718
And for the observability point, service owners should

142
00:09:11,744 --> 00:09:15,278
measure their chaos experiments through the same slos and monitors that

143
00:09:15,284 --> 00:09:18,426
they use in production. This creates a positive feedback

144
00:09:18,458 --> 00:09:22,310
loop so that learnings can be fed back into SRE

145
00:09:22,410 --> 00:09:25,954
principles for production operations, but also used

146
00:09:25,992 --> 00:09:29,394
to evaluate the service health and make tweaks coming

147
00:09:29,432 --> 00:09:31,410
out of the chaos experimentation.

148
00:09:34,070 --> 00:09:37,710
So if you're facing similar challenges, these are my recommendations

149
00:09:37,790 --> 00:09:40,902
for a self service chaos platform. Number one,

150
00:09:41,036 --> 00:09:45,298
pick a tool that is multi substrate and can support future flexibility.

151
00:09:45,474 --> 00:09:49,190
You never know what sort of regulatory or business changes are ahead,

152
00:09:49,340 --> 00:09:52,566
and for example, you might acquire a company that uses

153
00:09:52,598 --> 00:09:55,818
a different sort of cloud. I've talked about AWS quite a

154
00:09:55,824 --> 00:09:59,082
bit today, but hyperforce is designed to run on other public cloud

155
00:09:59,136 --> 00:10:02,810
providers, and if we need to support another provider tomorrow,

156
00:10:02,890 --> 00:10:06,318
we want to be able to do that. The chaos tooling that we've got

157
00:10:06,404 --> 00:10:10,586
supports that also make use of RBAC and tags

158
00:10:10,618 --> 00:10:14,206
and labels, et cetera, to control the blast radius and limit

159
00:10:14,238 --> 00:10:17,886
your attack access. There should be no question who's

160
00:10:17,918 --> 00:10:21,780
logging into a platform and what they are allowed to run an attack against.

161
00:10:22,630 --> 00:10:26,146
Also consider prioritizing extensibility to integrate

162
00:10:26,178 --> 00:10:30,120
with custom systems like we did for our change management system.

163
00:10:30,650 --> 00:10:34,482
Anything that offers custom extensions or webhooks

164
00:10:34,626 --> 00:10:38,330
and subscribe notifications can be good candidates.

165
00:10:38,830 --> 00:10:42,474
Fourth, seek out a sophisticated toolbox of attacks to

166
00:10:42,512 --> 00:10:45,478
support both large scale game day style experiments,

167
00:10:45,574 --> 00:10:49,082
AWS, well as precision granular attacks on individual

168
00:10:49,216 --> 00:10:52,554
services. And finally, use slos.

169
00:10:52,682 --> 00:10:55,754
Make them part of your hypotheses and make sure that service owners

170
00:10:55,802 --> 00:10:59,946
observe experiments, AWS they would observe production anytime

171
00:11:00,058 --> 00:11:03,214
there seems to be a disagreement between the outcome of a chaos

172
00:11:03,262 --> 00:11:06,754
experiment and the SLO is an opportunity to

173
00:11:06,792 --> 00:11:09,730
improve the definition of that SLO.

174
00:11:11,990 --> 00:11:15,998
And I want to talk briefly just about the ongoing role of game day exercises,

175
00:11:16,094 --> 00:11:19,250
because I don't mean to imply that we don't do game days anymore.

176
00:11:19,330 --> 00:11:22,822
We absolutely do. In this case, we optimize game days

177
00:11:22,876 --> 00:11:26,386
for purpose and for expertise. So service owners

178
00:11:26,418 --> 00:11:31,082
can take charge of concrete technical fixes on their own service because

179
00:11:31,136 --> 00:11:34,566
they're so close to it and they understand exactly what the failure

180
00:11:34,598 --> 00:11:38,650
modes can be and what potential solution paths are easiest.

181
00:11:39,230 --> 00:11:42,558
But game day teams that have a wide ranging view of

182
00:11:42,564 --> 00:11:46,314
the infrastructure can support things like compliance exercises

183
00:11:46,442 --> 00:11:49,662
or run organizational and people chaos. So,

184
00:11:49,716 --> 00:11:53,054
for example, you might test your incident response mechanism,

185
00:11:53,182 --> 00:11:57,970
but make sure that the first person paged is unavailable.

186
00:11:58,310 --> 00:12:01,458
That could reveal some interesting insights about your incident response process,

187
00:12:01,544 --> 00:12:05,254
your pager readiness, et cetera. Also the same,

188
00:12:05,292 --> 00:12:08,162
I would suggest, for shared it service chaos.

189
00:12:08,306 --> 00:12:12,562
So consider attacking your wiki or your operational runbook

190
00:12:12,626 --> 00:12:16,274
repository, right? What happens if the document

191
00:12:16,322 --> 00:12:20,458
repository with all of the instructions for incident remediation is

192
00:12:20,544 --> 00:12:23,802
slow or unavailable? I bet you'll come up with some ways

193
00:12:23,856 --> 00:12:27,382
that you can become redundant to that as an organization during incident

194
00:12:27,446 --> 00:12:30,778
response. And finally, game day

195
00:12:30,864 --> 00:12:34,046
teams continue to have a role in tabletop exercises to

196
00:12:34,068 --> 00:12:37,630
help service owners scope their attacks because of their wide ranging view,

197
00:12:37,700 --> 00:12:41,246
game day teams can suggest issues to service owners that

198
00:12:41,268 --> 00:12:45,194
have plagued other services in the past and give them recommendations

199
00:12:45,322 --> 00:12:49,006
on running generically in the public cloud. So with

200
00:12:49,028 --> 00:12:53,162
that, I want to say thank you. I hope this was useful and helped

201
00:12:53,306 --> 00:12:56,946
shine a light on some of the things that we experienced as we shifted

202
00:12:56,978 --> 00:13:00,230
left and scaled up. We're really passionate about giving this

203
00:13:00,300 --> 00:13:04,614
power to service owners because everybody has a hand in chaos and

204
00:13:04,652 --> 00:13:08,566
everybody has a hand in operating their service at scale. Thanks so much and

205
00:13:08,588 --> 00:13:09,650
have a great rest of the conference.

