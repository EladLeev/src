1
00:01:40,210 --> 00:01:43,406
You. Hi. This session is

2
00:01:43,428 --> 00:01:47,226
titled get ready to recover with reliability management. I'm Jeff

3
00:01:47,258 --> 00:01:50,270
Nickoloff. I'm currently with working with Gremlin,

4
00:01:51,010 --> 00:01:54,366
but I have 20 years in industry. I've worked with some of

5
00:01:54,388 --> 00:01:56,990
the largest organizations on the planet,

6
00:01:57,730 --> 00:02:01,706
and I've been working with mission critical business critical systems

7
00:02:01,738 --> 00:02:05,826
for a very, very long time. Some of those companies include at Amazon,

8
00:02:05,938 --> 00:02:08,920
Venmo, and PayPal, and several others.

9
00:02:11,530 --> 00:02:14,230
In my time working on those critical systems,

10
00:02:14,650 --> 00:02:17,430
I've been involved with hundreds of incidents,

11
00:02:18,350 --> 00:02:21,180
both as a person on call,

12
00:02:22,190 --> 00:02:27,226
as well as someone who's responsible for communicating across

13
00:02:27,328 --> 00:02:31,070
an organization, running a man, running an incidents,

14
00:02:32,210 --> 00:02:35,930
communicating with customers, digesting and translating

15
00:02:36,010 --> 00:02:40,302
between engineering information technician information and

16
00:02:40,356 --> 00:02:43,918
customer impact. And there's a

17
00:02:43,924 --> 00:02:47,566
long way of saying I've seen a lot of different kinds of incidents,

18
00:02:47,758 --> 00:02:51,300
but I think the type of incidents that bring us together

19
00:02:52,390 --> 00:02:55,898
really drive us all to be interested in a session

20
00:02:55,934 --> 00:02:59,910
like this, in a conference like this, are really those

21
00:02:59,980 --> 00:03:03,800
with the most pain, and I don't necessarily mean

22
00:03:04,170 --> 00:03:07,320
those with the greatest customer impact, although those are very important.

23
00:03:07,690 --> 00:03:11,414
I'm talking about situations that I like to call scrambling

24
00:03:11,462 --> 00:03:14,614
in the dark. Now, this might be literally

25
00:03:14,662 --> 00:03:18,522
in the dark, where it's either the middle of the night,

26
00:03:18,656 --> 00:03:19,770
you're tired,

27
00:03:21,810 --> 00:03:25,834
but it doesn't have to be. It might also mean situations

28
00:03:25,882 --> 00:03:29,486
where you don't know, your customer doesn't know,

29
00:03:29,588 --> 00:03:33,646
your technicians don't know, no one's sure you're

30
00:03:33,678 --> 00:03:36,610
having communication problems, miscommunication.

31
00:03:37,190 --> 00:03:39,780
You have to do just in time research.

32
00:03:41,670 --> 00:03:45,086
Scrambling in the dark might look like your technicians are

33
00:03:45,128 --> 00:03:48,550
trading dashboards like trading cards.

34
00:03:51,290 --> 00:03:55,000
There's an increased desperation to the situation.

35
00:03:56,330 --> 00:03:59,930
And those are

36
00:04:00,080 --> 00:04:04,186
the moments where the

37
00:04:04,208 --> 00:04:08,058
customer impact is one thing, but it's really those moments that create a

38
00:04:08,064 --> 00:04:12,062
bit of a crisis of conscious, or rather

39
00:04:12,116 --> 00:04:15,950
a crisis of confidence for yourself, for your

40
00:04:16,020 --> 00:04:19,902
team in itself, for your business

41
00:04:20,036 --> 00:04:23,250
partners, in your engineering organization.

42
00:04:23,670 --> 00:04:26,926
People begin to ask the hard question of, can I rely

43
00:04:26,958 --> 00:04:28,210
on our systems?

44
00:04:29,750 --> 00:04:33,762
And many, many uncomfortable questions fall out

45
00:04:33,816 --> 00:04:36,790
of these times where we're scrambling in the dark.

46
00:04:37,450 --> 00:04:40,966
And so this is a problem we

47
00:04:40,988 --> 00:04:44,886
need to solve as much as, and with the

48
00:04:44,908 --> 00:04:48,554
same urgency as we typically talk, but things like

49
00:04:48,592 --> 00:04:52,362
time to recovery or

50
00:04:52,416 --> 00:04:56,650
time to response those sort of like a little bit more concrete metrics.

51
00:04:58,030 --> 00:05:00,640
But in my experience, more often than not,

52
00:05:01,890 --> 00:05:05,070
these have similar solutions.

53
00:05:06,930 --> 00:05:11,710
It really comes down to preparation.

54
00:05:14,390 --> 00:05:18,686
You need to get ready for the incident. Before the incident,

55
00:05:18,878 --> 00:05:22,500
you're going to have incidents that is, that is given.

56
00:05:24,390 --> 00:05:28,290
And especially for systems that undergo regular

57
00:05:28,370 --> 00:05:32,502
change, high velocity change, either in the system itself, the system

58
00:05:32,556 --> 00:05:36,246
design, or in the business context in which you're operating the

59
00:05:36,268 --> 00:05:41,254
system, either side of the coin can

60
00:05:41,292 --> 00:05:44,330
change a system in ways that are difficult to anticipate.

61
00:05:45,710 --> 00:05:48,278
But I don't want to be another one of those people that kind of stands

62
00:05:48,294 --> 00:05:52,174
up and says, well, you need to be better prepared, or to just

63
00:05:52,292 --> 00:05:56,094
be better prepared. Preparation is important,

64
00:05:56,212 --> 00:06:00,282
but it's important not to minimize the level of effort

65
00:06:00,426 --> 00:06:04,542
and the expense that goes into preparation.

66
00:06:04,686 --> 00:06:06,530
Preparing for incidents.

67
00:06:08,950 --> 00:06:12,370
It's nontrivial.

68
00:06:13,510 --> 00:06:16,100
As soon as you dive into it, you have to ask yourself,

69
00:06:16,890 --> 00:06:20,194
what does it mean to be prepared? What level of readiness

70
00:06:20,242 --> 00:06:24,040
do we need and what do we need to prepare for?

71
00:06:26,090 --> 00:06:29,894
And the level of effort increases with the complexity

72
00:06:29,942 --> 00:06:32,940
of your system. The number of individual components of the system,

73
00:06:34,670 --> 00:06:38,220
the number of people on your team, the number of teams you have,

74
00:06:38,830 --> 00:06:42,554
the different ways that your system might interact with other systems

75
00:06:42,602 --> 00:06:46,314
or your partner systems, your upstream dependencies,

76
00:06:46,442 --> 00:06:49,678
or with your customers. How many different ways does your

77
00:06:49,684 --> 00:06:53,266
system interact with your customers? How many different ways can

78
00:06:53,288 --> 00:06:58,046
it fail? And that's

79
00:06:58,078 --> 00:07:01,694
a hard story to tell when you're

80
00:07:01,742 --> 00:07:05,338
asking for funding to invest in programs

81
00:07:05,454 --> 00:07:08,070
so that you can be better prepared,

82
00:07:08,890 --> 00:07:12,214
so that your teams can feel better prepared, so that you can be more

83
00:07:12,252 --> 00:07:15,990
confident before, during and

84
00:07:16,060 --> 00:07:17,510
after an incidents.

85
00:07:18,990 --> 00:07:20,700
Because if you don't have that,

86
00:07:23,070 --> 00:07:26,666
this is table stakes for getting to those places where you

87
00:07:26,688 --> 00:07:29,450
can begin to talk about time to recovery.

88
00:07:30,110 --> 00:07:33,370
It all starts with the people and understanding the space.

89
00:07:33,520 --> 00:07:36,686
And it's critical not to minimize the

90
00:07:36,708 --> 00:07:40,270
level of effort it requires to be better prepared.

91
00:07:42,870 --> 00:07:45,250
So when we talk about preparation,

92
00:07:47,110 --> 00:07:49,970
your investment really falls into two categories.

93
00:07:50,310 --> 00:07:54,370
You have your detective controls,

94
00:07:55,930 --> 00:07:59,526
identifying when your system is failing, potentially identifying what

95
00:07:59,548 --> 00:08:03,030
parts of your system are failing, and potentially identifying

96
00:08:03,770 --> 00:08:05,990
likely resolution.

97
00:08:07,870 --> 00:08:11,946
That all falls into a world that

98
00:08:11,968 --> 00:08:13,850
we typically think of as automation.

99
00:08:15,310 --> 00:08:19,698
Automation. These look like programs or systems

100
00:08:19,894 --> 00:08:24,250
that provide continuous monitoring

101
00:08:24,330 --> 00:08:26,270
and automated recovery.

102
00:08:28,370 --> 00:08:31,866
Things like resilience platforms like kubernetes.

103
00:08:31,978 --> 00:08:35,954
This is specifically what that is intended to

104
00:08:36,072 --> 00:08:40,126
provide. Automated release control, automated rollback

105
00:08:40,158 --> 00:08:43,666
control, automated recovery. If a pottery or

106
00:08:43,688 --> 00:08:47,954
process fails, it will automatically bring reconcile

107
00:08:48,002 --> 00:08:51,000
that desired state back to having it run.

108
00:08:54,170 --> 00:08:58,146
Those types of investment

109
00:08:58,178 --> 00:09:01,850
in automation can be very powerful, but can also end up being

110
00:09:02,000 --> 00:09:06,074
very tool or stack dependent. And this

111
00:09:06,112 --> 00:09:10,018
makes it a little bit more fragile, makes it a little bit more engineering

112
00:09:10,054 --> 00:09:14,190
effort to pursue robust solutions over

113
00:09:14,260 --> 00:09:17,326
the lifetime of your team and your product and your

114
00:09:17,348 --> 00:09:20,586
company. They need a little bit of love, and that's

115
00:09:20,618 --> 00:09:24,722
okay. These are very powerful tools, but they are

116
00:09:24,856 --> 00:09:27,710
expensive to maintain,

117
00:09:27,790 --> 00:09:31,890
to implement, maintain and continuously improve.

118
00:09:33,190 --> 00:09:36,854
They're not bad. They're critical. Right. The other side of

119
00:09:36,892 --> 00:09:40,726
this is getting your team on autopilot. And what

120
00:09:40,748 --> 00:09:44,310
I mean by that is having a consistent,

121
00:09:47,050 --> 00:09:51,302
bringing a high degree of consistency into your incidents

122
00:09:51,446 --> 00:09:54,794
response, the skill set and

123
00:09:54,832 --> 00:09:57,020
context that your technicians bring,

124
00:09:58,510 --> 00:10:01,614
a consistent and logical way of following through

125
00:10:01,652 --> 00:10:04,030
and problem solving the incidents.

126
00:10:05,330 --> 00:10:09,710
A fairly deterministic and consistent set

127
00:10:09,780 --> 00:10:13,700
of remediation options.

128
00:10:14,390 --> 00:10:17,970
How do we recover? Getting that on autopilot?

129
00:10:19,270 --> 00:10:22,660
And when you're not on autopilot, what that looks like is

130
00:10:24,230 --> 00:10:30,214
incident responders and technicians, specifically where

131
00:10:30,252 --> 00:10:33,938
you have a high degree of variation in their readiness

132
00:10:34,034 --> 00:10:37,994
to handle the incident. Some people understand some

133
00:10:38,032 --> 00:10:42,442
systems more than others. Some people are

134
00:10:42,496 --> 00:10:46,362
familiar with recent changes to a system more than others will

135
00:10:46,416 --> 00:10:49,798
be. Sometimes people have

136
00:10:49,904 --> 00:10:54,522
different problem solving responses, different problem solving workflows.

137
00:10:54,666 --> 00:10:55,840
In other times,

138
00:10:58,210 --> 00:11:02,170
different people will be familiar with different sources.

139
00:11:02,250 --> 00:11:05,898
For truth, this might look like dashboards.

140
00:11:05,994 --> 00:11:09,790
It might look like awareness of specific, some alarms

141
00:11:09,870 --> 00:11:13,106
or maybe non alarming monitors that

142
00:11:13,128 --> 00:11:17,014
have also been set up, that are designed to, or that are

143
00:11:17,052 --> 00:11:22,198
in place to try to help triage and

144
00:11:22,284 --> 00:11:26,374
identify a path to recovery. But those

145
00:11:26,412 --> 00:11:30,106
things are not. There is

146
00:11:30,128 --> 00:11:33,766
a missed opportunity if your team can't

147
00:11:33,798 --> 00:11:36,966
use them with consistency. So when I say getting on autopilot,

148
00:11:36,998 --> 00:11:40,342
I mean bringing a high degree of readiness

149
00:11:40,406 --> 00:11:43,678
to the people on your team, making sure that

150
00:11:43,684 --> 00:11:47,246
they're aware, making sure that they understand the systems and how

151
00:11:47,268 --> 00:11:51,246
those systems fail, making sure that they understand the tooling and

152
00:11:51,268 --> 00:11:54,282
everything else that are available to them,

153
00:11:54,436 --> 00:11:57,422
and making sure that they get reps, making sure that they get practice,

154
00:11:57,486 --> 00:12:00,546
making sure that they've seen the various kinds of

155
00:12:00,568 --> 00:12:04,338
failure before they show up in

156
00:12:04,344 --> 00:12:05,170
an incident.

157
00:12:08,250 --> 00:12:12,086
This is an investment, and a regular investment into the

158
00:12:12,108 --> 00:12:15,574
human side of things. But either way,

159
00:12:15,692 --> 00:12:19,418
regardless of which of you're going to end

160
00:12:19,424 --> 00:12:23,530
up investing in both of these things, the question is what

161
00:12:23,600 --> 00:12:26,620
to spend on each side of these things,

162
00:12:27,310 --> 00:12:32,206
and then also identifying what

163
00:12:32,308 --> 00:12:35,582
things to prepare for, which kinds of incidents to

164
00:12:35,636 --> 00:12:40,000
prepare for. Most systems have.

165
00:12:41,010 --> 00:12:44,894
They can become quite complicated quite quickly with the number of dependencies,

166
00:12:45,022 --> 00:12:47,700
the number of ways things can break,

167
00:12:49,590 --> 00:12:53,394
and under which kinds of conditions different parts of the

168
00:12:53,432 --> 00:12:57,078
system may break or may need different kinds of love in

169
00:12:57,084 --> 00:12:58,150
order to recover.

170
00:13:00,730 --> 00:13:03,926
In some cases, you might be asking yourself, like, what can we change about the

171
00:13:03,948 --> 00:13:06,790
system before an incidents?

172
00:13:07,390 --> 00:13:11,494
To either reduce the probability of an incident

173
00:13:11,622 --> 00:13:13,850
or to speed recovery.

174
00:13:15,150 --> 00:13:18,854
But again, coming back to the complexity

175
00:13:18,902 --> 00:13:22,110
and the level of effort that goes into just preparing,

176
00:13:23,890 --> 00:13:27,600
it's going to be very difficult to prepare for everything.

177
00:13:29,250 --> 00:13:32,818
That's a very long tail that we'll all have

178
00:13:32,824 --> 00:13:36,482
to be chasing. So the question really comes down

179
00:13:36,536 --> 00:13:40,162
to not just

180
00:13:40,296 --> 00:13:43,662
do we invest in automation versus autopilot,

181
00:13:43,806 --> 00:13:47,474
but which types of incidents,

182
00:13:47,522 --> 00:13:51,746
which types of failures should we invest in preparing

183
00:13:51,778 --> 00:13:55,078
for? And that's a

184
00:13:55,244 --> 00:13:58,710
nontrivial question. You can answer it trivially.

185
00:13:59,770 --> 00:14:03,226
Some people may have seen that some

186
00:14:03,248 --> 00:14:07,050
people might be more familiar with different types of failures than others, and so

187
00:14:07,200 --> 00:14:10,746
they'll probably lean towards that, naively lean towards the

188
00:14:10,768 --> 00:14:13,966
things that they are familiar with, the failing or the

189
00:14:13,988 --> 00:14:17,342
last thing that they ended up being paged for.

190
00:14:17,396 --> 00:14:21,086
There's typically a strong bias for that. But if

191
00:14:21,108 --> 00:14:25,380
you're standing in a position where you have the opportunity to choose,

192
00:14:27,590 --> 00:14:31,106
there's a better way. I want to

193
00:14:31,128 --> 00:14:34,846
talk about the relationship between incident management

194
00:14:34,878 --> 00:14:40,470
and incident response and reliability,

195
00:14:40,810 --> 00:14:44,326
reliability of your systems and running a reliability program.

196
00:14:44,508 --> 00:14:47,406
These two things are definitely separate efforts,

197
00:14:47,538 --> 00:14:51,100
but there's an inherent relationship between the two.

198
00:14:52,670 --> 00:14:54,380
Your reliability program.

199
00:14:56,190 --> 00:14:59,610
We put these things in place so that we can proactively,

200
00:15:00,130 --> 00:15:03,790
not retrospectively,

201
00:15:04,370 --> 00:15:09,150
looking at what has been breaking, but so we can proactively identify

202
00:15:10,290 --> 00:15:14,030
and regularly assert what incidents

203
00:15:14,110 --> 00:15:17,998
we are at risk for, the probability of those risks,

204
00:15:18,174 --> 00:15:21,426
the severity, if these things break. And we

205
00:15:21,448 --> 00:15:25,446
use that information to inform what incidents we

206
00:15:25,468 --> 00:15:28,854
should prepare for, and we use the information that

207
00:15:28,892 --> 00:15:30,950
comes out of managing incidents,

208
00:15:32,490 --> 00:15:36,150
how prepared are we to handle these incidents?

209
00:15:36,810 --> 00:15:40,646
How long does it take to recover? What's been the financial impact

210
00:15:40,678 --> 00:15:44,090
of the last three times or however

211
00:15:44,160 --> 00:15:47,734
many times this type of failure

212
00:15:47,782 --> 00:15:51,520
has happened? We use that as an input back into the reliability program

213
00:15:52,210 --> 00:15:56,126
so that we can prioritize what to change and

214
00:15:56,148 --> 00:15:59,360
how to measure. So, reliability program.

215
00:16:01,250 --> 00:16:04,418
This is a very high level, abstract idea,

216
00:16:04,584 --> 00:16:08,580
but in general, what this looks like is

217
00:16:09,270 --> 00:16:12,786
having a strong idea of being able to

218
00:16:12,808 --> 00:16:15,300
enumerate the components in your system,

219
00:16:16,730 --> 00:16:20,246
being aware of the ways that they might fail, being aware of your

220
00:16:20,268 --> 00:16:23,960
dependencies, being aware of the value,

221
00:16:24,650 --> 00:16:28,534
usually, like by rate, of how valuable certain systems

222
00:16:28,582 --> 00:16:33,290
are, and then regularly measuring and determining

223
00:16:33,710 --> 00:16:38,634
what types of operational conditions those

224
00:16:38,672 --> 00:16:42,234
components can survive and specifically

225
00:16:42,282 --> 00:16:43,550
where they tip.

226
00:16:46,930 --> 00:16:51,754
And then obviously there's a whole thing around identifying,

227
00:16:51,802 --> 00:16:55,434
funding and staffing for engineering

228
00:16:55,482 --> 00:16:58,906
improvements so that you can hit reliability goals.

229
00:16:59,098 --> 00:17:02,702
But I really want to talk, but to zoom in on the mechanism,

230
00:17:02,766 --> 00:17:06,600
the high level core mechanisms of a solid reliability program,

231
00:17:07,690 --> 00:17:10,770
there's a tool called failure mode and effect analysis,

232
00:17:10,850 --> 00:17:15,202
effects analysis. This is a pretty robust

233
00:17:15,266 --> 00:17:18,934
framework. It's rare that I've seen it

234
00:17:18,972 --> 00:17:22,710
implemented in SaaS and software space in a

235
00:17:22,780 --> 00:17:26,598
deep way, but it's a really important system,

236
00:17:26,684 --> 00:17:30,800
even if you're taking only high level inspiration from

237
00:17:31,650 --> 00:17:36,046
it. A failure mode and effects analysis is

238
00:17:36,148 --> 00:17:40,222
a robust and opinionated framework for

239
00:17:40,276 --> 00:17:42,800
cataloging the components in your system,

240
00:17:43,590 --> 00:17:47,250
the failure modes for each of those components,

241
00:17:47,990 --> 00:17:51,550
the probability of those failures,

242
00:17:51,630 --> 00:17:54,782
and that term starts to get a little bit fuzzy and

243
00:17:54,936 --> 00:17:58,600
really dependent on your business,

244
00:17:59,770 --> 00:18:05,174
the severity impact of

245
00:18:05,212 --> 00:18:06,550
that type of failure.

246
00:18:08,750 --> 00:18:12,214
For many groups, this might look like financial

247
00:18:12,262 --> 00:18:17,414
impact, this might look like downstream.

248
00:18:17,542 --> 00:18:21,126
If this fails, you can begin to talk about cascading

249
00:18:21,158 --> 00:18:25,006
failures, although failure mode and effects analysis is really not so much concerned about

250
00:18:25,028 --> 00:18:28,266
that. It's usually first order impacts. But if you can get to money, it helps

251
00:18:28,298 --> 00:18:30,080
you craft a better story later.

252
00:18:31,410 --> 00:18:35,860
And these analyses also typically discuss

253
00:18:36,550 --> 00:18:40,594
and presents an opportunity for you to determine whether

254
00:18:40,632 --> 00:18:44,078
or not you can detect the type of failure.

255
00:18:44,174 --> 00:18:47,302
But the big idea is you get this information,

256
00:18:47,436 --> 00:18:50,966
you build out a big table, it might be a spreadsheet, whatever it

257
00:18:50,988 --> 00:18:55,302
is. And this helps you identify all

258
00:18:55,356 --> 00:18:59,306
sorts of risks in your system to really identify where the

259
00:18:59,328 --> 00:19:03,514
risks are. I want to talk for

260
00:19:03,552 --> 00:19:07,014
a moment about failure modes and your detective

261
00:19:07,062 --> 00:19:11,214
controls, because this goes directly to your

262
00:19:11,252 --> 00:19:13,230
incident preparedness,

263
00:19:14,610 --> 00:19:18,734
as you're enumerating, as your reliability management program

264
00:19:18,932 --> 00:19:23,066
is enumerating the types of failures for each component

265
00:19:23,178 --> 00:19:26,722
and whether or not they can survive them. Another big question for

266
00:19:26,776 --> 00:19:30,482
each of those types of failure modes is, can you detect it before

267
00:19:30,536 --> 00:19:34,594
your customers do, or how

268
00:19:34,632 --> 00:19:36,600
quickly can you detect it?

269
00:19:41,290 --> 00:19:44,950
And that's really because if you can't,

270
00:19:45,530 --> 00:19:48,854
these are clearly going to be gaps in your

271
00:19:48,892 --> 00:19:52,114
preparedness. If you can't detect

272
00:19:52,162 --> 00:19:55,478
whether or not a failure mode has happened,

273
00:19:55,644 --> 00:19:59,622
you're going to have poor response time. If you

274
00:19:59,676 --> 00:20:02,160
can't detect whether or not this failure has happened,

275
00:20:03,810 --> 00:20:07,246
your incident responders, when they do respond, are going to

276
00:20:07,268 --> 00:20:11,390
have a more difficult time identifying the nature of the failure.

277
00:20:12,850 --> 00:20:16,306
And so it's naive. I could stand up here

278
00:20:16,328 --> 00:20:19,906
and say, make sure that you've got detective controls for everything. But this is

279
00:20:19,928 --> 00:20:23,346
one of those cases where you want to look at

280
00:20:23,448 --> 00:20:25,060
that breakdown to say,

281
00:20:28,570 --> 00:20:32,710
does this type of failure mode warrant investment

282
00:20:33,450 --> 00:20:35,350
into detective controls?

283
00:20:36,810 --> 00:20:40,620
And it's important to be able to test your detective controls to

284
00:20:41,230 --> 00:20:44,282
regularly, I don't mean at one point in time,

285
00:20:44,416 --> 00:20:49,290
but to regularly create

286
00:20:49,360 --> 00:20:52,974
failure conditions in whatever environment to

287
00:20:53,012 --> 00:20:56,734
verify that your detecting controls operate the way

288
00:20:56,852 --> 00:20:58,270
that they're intended.

289
00:21:03,010 --> 00:21:05,650
The next step, and it's not really a step,

290
00:21:05,720 --> 00:21:09,186
but the other part that I'd already discussed a little bit

291
00:21:09,368 --> 00:21:13,362
is, so when we're bringing it back to how do we prioritize what

292
00:21:13,416 --> 00:21:17,400
to invest into, the real big question is,

293
00:21:18,010 --> 00:21:21,462
well, where's our biggest risk? And I mean not just in

294
00:21:21,516 --> 00:21:24,898
like. And when I say risk here is like probability

295
00:21:24,994 --> 00:21:28,374
of failure, but also multiplied by the severity

296
00:21:28,422 --> 00:21:33,162
of the failure. If you have something that is very

297
00:21:33,296 --> 00:21:37,414
expensive, if the failure occurs, but is extremely

298
00:21:37,462 --> 00:21:41,630
unlikely, then this might be a lower priority

299
00:21:43,010 --> 00:21:46,746
than a type of failure. This might be a lower priority

300
00:21:46,778 --> 00:21:50,286
to prepare for than a type of failure that happens

301
00:21:50,388 --> 00:21:53,954
three or four times a day and is

302
00:21:53,992 --> 00:21:58,210
likely to continue that that has a more

303
00:21:58,280 --> 00:22:00,660
mild cost associated with.

304
00:22:04,070 --> 00:22:07,640
But you have to do that reflection activity. You have to actually ask yourself,

305
00:22:08,970 --> 00:22:12,102
how likely is something to happen? And that

306
00:22:12,156 --> 00:22:15,494
typically requires some type of experimentation. You should test it.

307
00:22:15,612 --> 00:22:18,486
Can this happen? Under which conditions can it happen?

308
00:22:18,588 --> 00:22:21,686
And when it does dive into the business. Look at

309
00:22:21,708 --> 00:22:25,078
your volumes, look at your, if it's

310
00:22:25,094 --> 00:22:28,810
a revenue type business, how much revenue is associated with these interactions?

311
00:22:30,050 --> 00:22:33,758
If this type of failure might result

312
00:22:33,844 --> 00:22:36,430
in some breach of contract,

313
00:22:37,570 --> 00:22:41,102
it's important to understand the penalty for those types of violations and bring

314
00:22:41,156 --> 00:22:44,754
that in. Let the business inform your

315
00:22:44,792 --> 00:22:46,290
engineering decisions.

316
00:22:47,990 --> 00:22:51,838
And so there's

317
00:22:51,854 --> 00:22:55,526
a lot of different ways to do this. It's easy to say probability and

318
00:22:55,548 --> 00:22:59,046
severity and talk about risk. I've seen it done a lot of

319
00:22:59,068 --> 00:23:04,438
different ways. And one

320
00:23:04,444 --> 00:23:09,834
of the concerns there is having inconsistency in

321
00:23:09,872 --> 00:23:13,642
your organization. If you have ten

322
00:23:13,696 --> 00:23:16,854
different groups in your organization and each of the ten groups

323
00:23:16,902 --> 00:23:21,470
are doing it slightly differently, it becomes very difficult to

324
00:23:21,540 --> 00:23:25,502
prioritize for your organization because you're often

325
00:23:25,556 --> 00:23:27,310
comparing apples to oranges.

326
00:23:28,690 --> 00:23:32,062
So regardless of what happens, regardless of

327
00:23:32,116 --> 00:23:33,460
how you move forward,

328
00:23:34,950 --> 00:23:38,642
consistency in measurement, consistency in those

329
00:23:38,696 --> 00:23:42,254
metrics that you're using to drive, prioritize decisions

330
00:23:42,382 --> 00:23:45,666
that dictate how you're

331
00:23:45,698 --> 00:23:50,310
going to spend your money in improving your preparation.

332
00:23:51,050 --> 00:23:53,080
Consistency is key.

333
00:23:54,090 --> 00:23:57,598
And this is one of the problems that we're solving at Gremlin that I'm

334
00:23:57,634 --> 00:24:01,334
so passionate about. Our new product reliability management.

335
00:24:01,382 --> 00:24:06,746
Product scoring is really central to it. And at

336
00:24:06,768 --> 00:24:10,220
minimum, this is something that we've learned from

337
00:24:10,910 --> 00:24:15,002
the vast experience in building reliability programs

338
00:24:15,066 --> 00:24:15,920
with companies.

339
00:24:18,530 --> 00:24:22,074
More often than not, from my experience and other conversations

340
00:24:22,122 --> 00:24:25,380
I've had with people at gremlin, more often than not,

341
00:24:25,990 --> 00:24:29,826
these are the dimensions that our customers find great success

342
00:24:29,928 --> 00:24:33,502
with. And like I said, the specific scoring mechanism

343
00:24:33,566 --> 00:24:36,674
you use is less important than having consistency in scoring.

344
00:24:36,722 --> 00:24:40,786
So what we've done is we've gone ahead and built a consistent scoring

345
00:24:40,818 --> 00:24:43,510
mechanism on their behalf.

346
00:24:44,570 --> 00:24:47,080
And so this is just an example.

347
00:24:47,870 --> 00:24:51,174
We do regular testing for redundancy,

348
00:24:51,222 --> 00:24:55,382
scalability and surviving dependency

349
00:24:55,446 --> 00:24:59,338
issues. We combine those into an

350
00:24:59,424 --> 00:25:03,326
easy to understand score and our customers,

351
00:25:03,508 --> 00:25:06,878
and we help present this to customers

352
00:25:07,044 --> 00:25:10,730
in a way that they can understand reliability

353
00:25:10,810 --> 00:25:14,346
issues between different services. Now, if you were to dive

354
00:25:14,378 --> 00:25:17,714
in, you can see specific conditions, and you can use

355
00:25:17,832 --> 00:25:21,006
those types of failures to inform the types

356
00:25:21,038 --> 00:25:24,162
of incidents that you should be prepared for. But the real

357
00:25:24,216 --> 00:25:27,880
power here is being able to know

358
00:25:28,330 --> 00:25:32,326
and identify what things can we survive? What things can

359
00:25:32,348 --> 00:25:35,362
we not survive for? Those things that we can't survive,

360
00:25:35,506 --> 00:25:38,966
what's our impact? Right. So however you end

361
00:25:38,988 --> 00:25:41,980
up implementing it, this is one of those great.

362
00:25:42,670 --> 00:25:45,930
What I believe to be a fantastic example of

363
00:25:46,000 --> 00:25:49,546
what you should end up with at the end of the know.

364
00:25:49,568 --> 00:25:52,430
That's why I'm so excited about what we're building here at Gremlin.

365
00:25:53,970 --> 00:25:57,390
At Gremlin, this has been our focus since the beginning,

366
00:25:57,810 --> 00:26:02,000
but we're really making explicit now

367
00:26:02,930 --> 00:26:06,910
that our mission is to help teams standardize and automate the reliability

368
00:26:07,650 --> 00:26:11,680
one service at a time and to help them understand at the service level

369
00:26:13,330 --> 00:26:17,346
in a consistent, repeat able way what they can tolerate

370
00:26:17,378 --> 00:26:20,614
and what they can't, so that they know this is that you understand how to

371
00:26:20,652 --> 00:26:24,418
prioritize your improvements, either in your product engineering

372
00:26:24,514 --> 00:26:28,034
or in your incidents response preparedness. Your incident

373
00:26:28,082 --> 00:26:31,990
preparedness. Thank you.

374
00:26:32,140 --> 00:26:35,686
That's all I have today, but if you have any other questions, I would love

375
00:26:35,708 --> 00:26:37,894
to see them in the chat. Thank you for everything.

