1
00:00:22,010 --> 00:00:25,606
Hello everyone, welcome to this session on natural language mod link with

2
00:00:25,628 --> 00:00:29,046
Amazon, sagemaker and blazing text algorithm. My name

3
00:00:29,068 --> 00:00:32,374
is Dinesh Kumar. I'm an aspiring ML specialist and

4
00:00:32,412 --> 00:00:36,390
I'm hoping to convince by end of this session that you don't need machine learning

5
00:00:36,460 --> 00:00:39,674
degree to take advantage of tooling AWS has put

6
00:00:39,712 --> 00:00:43,414
at your disposal. Now, as part of this session,

7
00:00:43,542 --> 00:00:47,414
we will pick a language purposefully, a foreign language likely

8
00:00:47,462 --> 00:00:51,526
unfamiliar to you, and apply machine learnings to create some magic.

9
00:00:51,638 --> 00:00:55,754
We will use the language of Tamar. The Tamar language is one of the world's

10
00:00:55,802 --> 00:00:59,294
longest surviving classical language with a history dating back

11
00:00:59,332 --> 00:01:02,974
to 300 bc. Tamar as a language is rich in literature and

12
00:01:03,012 --> 00:01:05,966
one of the language that has evolved over thousands of years.

13
00:01:06,068 --> 00:01:09,634
The reason I picked it is I'm familiar with this language and

14
00:01:09,672 --> 00:01:13,186
I was keen to see whether machine learnings was able to figure out the relationship

15
00:01:13,368 --> 00:01:16,566
between the words of this language and I want to

16
00:01:16,588 --> 00:01:20,022
apply known machine learning algorithm such as lacing text,

17
00:01:20,076 --> 00:01:23,474
in this case within Sagemaker to discover these relationships.

18
00:01:23,602 --> 00:01:26,838
What's cool about today's talk also

19
00:01:26,924 --> 00:01:30,602
is you will be able to apply these

20
00:01:30,656 --> 00:01:33,802
techniques post this session to

21
00:01:33,856 --> 00:01:38,214
the language of your own choice and discover

22
00:01:38,262 --> 00:01:41,694
the relationships yourselves. Now, what are the prerequisites for this

23
00:01:41,732 --> 00:01:45,038
session? It will be good to have fundamentals of the

24
00:01:45,124 --> 00:01:49,354
machine learning or deep learning and understanding

25
00:01:49,402 --> 00:01:52,994
of typical natural language processing problems,

26
00:01:53,112 --> 00:01:57,154
ie search and some familiarity with AWS services.

27
00:01:57,272 --> 00:02:00,466
Not necessarily sagemaker. I will surely cover that as

28
00:02:00,488 --> 00:02:04,718
part of my session and then knowledge of python or numpy.

29
00:02:04,814 --> 00:02:08,466
As long as you know any other coding language, this is straightforward,

30
00:02:08,578 --> 00:02:12,226
you would not be lost and these knowledge

31
00:02:12,258 --> 00:02:16,086
of Jupyter environment notebook environment as such would

32
00:02:16,108 --> 00:02:19,170
be very handy. Now in order to

33
00:02:19,180 --> 00:02:23,338
get this magic happen, we need to introduce word embedding with

34
00:02:23,424 --> 00:02:27,606
word to vec algorithm. Then we need to understand bit about Amazon Sagemaker

35
00:02:27,638 --> 00:02:31,434
and its unique capabilities, especially around NLP area. Then we can

36
00:02:31,472 --> 00:02:35,134
spend bulk of our time with building text algorithm and a quick

37
00:02:35,252 --> 00:02:38,954
demo. So that takes us to these interesting topic

38
00:02:39,002 --> 00:02:42,714
of word embedding. Let's dive into word embedding, natural language,

39
00:02:42,762 --> 00:02:45,982
text, contents of words and so we need to represent

40
00:02:46,046 --> 00:02:49,554
individual words, sentences and collection of

41
00:02:49,592 --> 00:02:53,022
words in some way. Now couldn't we just use strings

42
00:02:53,086 --> 00:02:57,138
containing the words? First of all, words have different lengths

43
00:02:57,234 --> 00:03:00,854
and even written representations differ dramatically from language to

44
00:03:00,892 --> 00:03:05,126
language. So if you look at the

45
00:03:05,148 --> 00:03:09,082
language of Tamil, some words might be mentioned

46
00:03:09,136 --> 00:03:12,406
in a different way in different region of Tamil

47
00:03:12,438 --> 00:03:16,602
Nadu, while the same words will

48
00:03:16,656 --> 00:03:20,922
have different representation or different way of being pronounced

49
00:03:20,986 --> 00:03:24,122
or spelled in different region.

50
00:03:24,266 --> 00:03:27,822
Now, with these complications and

51
00:03:27,956 --> 00:03:31,594
added to that, more importantly, many machine learning techniques

52
00:03:31,642 --> 00:03:35,698
require numeric rather than text input as you would obviously.

53
00:03:35,784 --> 00:03:39,522
You know, computers are good with numbers and

54
00:03:39,576 --> 00:03:43,202
may not be with natural language vocabulary like us.

55
00:03:43,336 --> 00:03:46,494
So that brings us to the topic of representing

56
00:03:46,542 --> 00:03:50,274
these words in a numerical form. So if I want to represent

57
00:03:50,322 --> 00:03:54,274
this particular phrase, you see there to be or not to be in a numerical

58
00:03:54,322 --> 00:03:57,702
form, how can I go about it? Let's say I

59
00:03:57,756 --> 00:04:01,130
represent each of it with an individual label. Let's say

60
00:04:01,200 --> 00:04:04,522
I represent two with zero, b with one, or with two,

61
00:04:04,576 --> 00:04:07,802
not with three. Now that gives me zero, one, two, these zero,

62
00:04:07,856 --> 00:04:11,726
one, a random set of numbers. Now, this particular way

63
00:04:11,748 --> 00:04:15,520
of representing it with just unique labels is actually going to create

64
00:04:15,890 --> 00:04:20,000
or introduce random relationships. For example,

65
00:04:22,290 --> 00:04:25,602
b as a word is now closer to two

66
00:04:25,656 --> 00:04:28,498
and r but away from not.

67
00:04:28,664 --> 00:04:32,146
Is that true? No. So how did we even arrive at

68
00:04:32,168 --> 00:04:34,340
these labels when there is no such relationship?

69
00:04:35,450 --> 00:04:39,206
So just randomly naming them with numerical label may

70
00:04:39,228 --> 00:04:42,466
not be great way of solving the problem. Now let's

71
00:04:42,498 --> 00:04:45,522
say we use vector

72
00:04:45,586 --> 00:04:49,314
instead of single number. Typical approach is to use one, not encoding.

73
00:04:49,362 --> 00:04:52,806
Here each word access an index into a vector of all zeros

74
00:04:52,838 --> 00:04:56,314
with only a single element set to one. Now you

75
00:04:56,352 --> 00:04:59,670
would have already guessed these problem with one out encoding approach.

76
00:04:59,750 --> 00:05:03,702
The example here is having just four words, and a typical

77
00:05:03,766 --> 00:05:07,402
language will have hundreds and thousands of words. In case of Tamar,

78
00:05:07,466 --> 00:05:11,322
I don't have account, but it's a very rich language, as I said earlier,

79
00:05:11,386 --> 00:05:15,554
and there is quite a lot of words to its vocabulary. So representing them

80
00:05:15,592 --> 00:05:19,540
by ones and zeros may not be the efficient way of actually

81
00:05:20,390 --> 00:05:22,802
solving the problem in hand.

82
00:05:22,936 --> 00:05:26,246
Now with that in mind, let's look

83
00:05:26,268 --> 00:05:29,670
at the other side of the coin. Given a sentence,

84
00:05:31,130 --> 00:05:34,418
what is our chances of maximizing the probability

85
00:05:34,514 --> 00:05:38,738
of predicting the context words? Let's say I introduce

86
00:05:38,754 --> 00:05:42,794
this word, Tom Hanks. How can I predict the

87
00:05:42,832 --> 00:05:45,866
context of this word? In this example? Let's say we want

88
00:05:45,888 --> 00:05:49,734
to predict the context of Tom Hanks. What is the probability that somewhere

89
00:05:49,782 --> 00:05:53,226
around Tom Hanks we will find words? Great. Can actor

90
00:05:53,418 --> 00:05:57,354
quite a lot because he's an actor. So there is quite a lot of chances

91
00:05:57,402 --> 00:06:00,558
that those words would appear somewhere near

92
00:06:00,644 --> 00:06:04,370
Tom Hanks. But what is the chances of me finding something like

93
00:06:04,440 --> 00:06:07,940
quantum physics next to Tom Hanks? Maybe very less.

94
00:06:08,470 --> 00:06:12,306
I would not say zero, but it is relatively less

95
00:06:12,408 --> 00:06:15,606
when you compare it to word actor. Now that's the point we are trying

96
00:06:15,628 --> 00:06:19,382
to actually make. How do I figure out that

97
00:06:19,436 --> 00:06:23,414
this particular word has more relationship and

98
00:06:23,452 --> 00:06:27,142
hence contextually more closer to this word.

99
00:06:27,276 --> 00:06:30,682
In typically deep learning world,

100
00:06:30,816 --> 00:06:34,326
we will actually have a fully connected network.

101
00:06:34,438 --> 00:06:38,026
So for every input word it receives, let's say a

102
00:06:38,048 --> 00:06:41,806
vocabulary has 10,000 words, and if we receive one word,

103
00:06:41,988 --> 00:06:45,466
then the output

104
00:06:45,498 --> 00:06:49,520
of this network should be able to figure out what are those

105
00:06:50,210 --> 00:06:53,886
words within those 10,000 words this particular input

106
00:06:53,918 --> 00:06:57,394
word is closer to. So there will be lot of

107
00:06:57,592 --> 00:07:01,026
hidden layers of network that will try to

108
00:07:01,048 --> 00:07:04,834
extract the contents words and then it'll spit out the

109
00:07:04,872 --> 00:07:08,422
probability of the words that might sit

110
00:07:08,476 --> 00:07:11,686
contextually closer to that input word. After training such

111
00:07:11,708 --> 00:07:15,238
a network, we can now quickly compute denser output vector for a

112
00:07:15,244 --> 00:07:18,910
sparse input vector that we had after our 1 hour ten coding.

113
00:07:19,010 --> 00:07:22,730
So we have now grasped a bit about the problem itself

114
00:07:22,800 --> 00:07:26,362
that we are trying to solve. The problem statement is we

115
00:07:26,416 --> 00:07:30,286
want to understand the probability of a

116
00:07:30,308 --> 00:07:33,694
word being in closer relationship with,

117
00:07:33,812 --> 00:07:38,014
or probability of having set of words that

118
00:07:38,052 --> 00:07:41,422
are in close relationship to the input word that is coming

119
00:07:41,476 --> 00:07:45,454
in for inference. Now it's time to understand about the word to veg

120
00:07:45,502 --> 00:07:49,026
algorithm itself. So, dimensionality of the

121
00:07:49,048 --> 00:07:52,814
output vector is a parameter we choose. This is why we said embeddings

122
00:07:52,862 --> 00:07:56,194
high dimensional object, one not encoded word, in this case

123
00:07:56,232 --> 00:07:59,986
into small dimensional space. Turning a sparse vector into a much denser

124
00:08:00,018 --> 00:08:03,814
representation is what we are trying to achieve. Once this representation is

125
00:08:03,852 --> 00:08:07,414
computed, we can simply convert every word into an n

126
00:08:07,452 --> 00:08:10,906
dimensional space. In the end,

127
00:08:11,008 --> 00:08:14,394
words that appear in similar context will likely be

128
00:08:14,432 --> 00:08:17,654
mapped to similar vectors. Words close to each other in vector

129
00:08:17,702 --> 00:08:21,206
space are likely to be similar in meaning as they tend to be used in

130
00:08:21,248 --> 00:08:24,718
similar context. This is where we are getting closer to

131
00:08:24,724 --> 00:08:27,914
these magic a machine learning system automatically discovering

132
00:08:27,962 --> 00:08:31,274
words that appear to have similar meaning. In theory,

133
00:08:31,402 --> 00:08:35,406
we also expect certain vector relationships to hold. This doesn't

134
00:08:35,438 --> 00:08:38,722
have to be exact and depends totally on the carcass being

135
00:08:38,776 --> 00:08:42,402
used for training. We can help to discover word relationships with

136
00:08:42,456 --> 00:08:45,906
transitive properties, as shown on the slide. For example,

137
00:08:46,088 --> 00:08:49,058
if we take a vector that corresponds to word king,

138
00:08:49,234 --> 00:08:52,840
that's a classic example that we usually see.

139
00:08:53,290 --> 00:08:57,534
Let's say I have found a vector for the word king and subtract

140
00:08:57,682 --> 00:09:00,140
vector for the word man.

141
00:09:02,830 --> 00:09:06,426
I get a meaning that says there is some sort

142
00:09:06,448 --> 00:09:10,342
of royalty associated, and then if I add vector

143
00:09:10,406 --> 00:09:13,626
of woman to it, I've arrived at vector of queen.

144
00:09:13,818 --> 00:09:17,502
This is magic here because we have managed to

145
00:09:17,556 --> 00:09:21,562
understand the meaning of a word and then doing typical

146
00:09:21,626 --> 00:09:24,994
addition subtraction that we play with numbers, but in this case

147
00:09:25,032 --> 00:09:28,546
with words and their inherent meaning that

148
00:09:28,568 --> 00:09:31,906
they bring with it. So that's exactly

149
00:09:32,008 --> 00:09:35,982
what word to vec algorithm is actually trying to solve.

150
00:09:36,046 --> 00:09:39,090
You may have heard about new models such as Bert, Roberta,

151
00:09:39,170 --> 00:09:42,226
but the end goal is to achieve the word embeddings.

152
00:09:42,418 --> 00:09:45,880
Yeah, I'll probably show you this

153
00:09:46,890 --> 00:09:50,090
completed word embedding for english language.

154
00:09:50,590 --> 00:09:53,878
And this has been mapped into 100 dimensional vector.

155
00:09:54,054 --> 00:09:57,510
How can we visualize this result high daily on a two dimensional

156
00:09:57,590 --> 00:10:01,550
picture? This is where we probably could use another

157
00:10:01,620 --> 00:10:05,386
trick of trade. T distributed stochastic neighbor

158
00:10:05,418 --> 00:10:09,006
embedding plot. Now that might be a

159
00:10:09,028 --> 00:10:12,374
bunch of words, but it's just simple way of telling

160
00:10:12,522 --> 00:10:16,002
that we now have way to visualize all

161
00:10:16,056 --> 00:10:19,682
those relationships in a two dimensional space.

162
00:10:19,816 --> 00:10:23,154
As you see here, the model has

163
00:10:23,192 --> 00:10:25,842
now figured out american, british, English,

164
00:10:25,906 --> 00:10:29,414
London, England, French, France, German. Now all of

165
00:10:29,452 --> 00:10:33,282
these are closer to each other. It has mapped

166
00:10:33,426 --> 00:10:37,202
that they all can contextually appear closer

167
00:10:37,266 --> 00:10:41,430
or they are all related in some form or shape. And these clusters

168
00:10:41,590 --> 00:10:45,482
are interesting clusters that it has figured out because

169
00:10:45,536 --> 00:10:48,682
of the corpus that was thrown at it. The very

170
00:10:48,736 --> 00:10:52,640
important one I would probably show is if you see there,

171
00:10:53,090 --> 00:10:56,798
it has found out that there is relationship between son,

172
00:10:56,884 --> 00:10:59,520
family, children, father, death, life.

173
00:11:02,130 --> 00:11:06,402
It is able to actually figure out that these

174
00:11:06,536 --> 00:11:09,646
words are related and are closer

175
00:11:09,678 --> 00:11:12,766
in context. And there is more probability

176
00:11:12,878 --> 00:11:17,094
of a word from this particular cluster appearing next

177
00:11:17,132 --> 00:11:21,286
to a word that has just come, that it has just come across.

178
00:11:21,468 --> 00:11:24,934
Now, enough of the word to vec and

179
00:11:25,052 --> 00:11:28,566
word embedding the actual problem statement that we were trying

180
00:11:28,588 --> 00:11:32,150
to solve. Let's try to understand the toolings that are at our disposal

181
00:11:32,230 --> 00:11:35,994
and how sage maker itself as a service is able to help

182
00:11:36,032 --> 00:11:39,882
us with in solving this problem. And our placing text

183
00:11:39,936 --> 00:11:43,866
algorithm then fits into it. This is a typical AAML

184
00:11:43,898 --> 00:11:46,974
stack. In the top you see AI services.

185
00:11:47,172 --> 00:11:51,086
Most of these services are out of box services

186
00:11:51,188 --> 00:11:54,926
in the sense you do not need to have any sort of machine learning skill.

187
00:11:55,038 --> 00:11:59,310
Let's say you are a team of developers

188
00:11:59,470 --> 00:12:02,946
who are having zero skill with machine learning, but want

189
00:12:02,968 --> 00:12:06,342
to actually see how you could use it for your

190
00:12:06,396 --> 00:12:09,942
own application or a problem

191
00:12:10,076 --> 00:12:13,606
or a workload that you have. Then these are

192
00:12:13,628 --> 00:12:17,794
some go to AI services. These are across different areas.

193
00:12:17,842 --> 00:12:21,302
For example, we have AI services for vision, speech language,

194
00:12:21,366 --> 00:12:25,334
chatbots, forecasting recommendations. All you need to do is an API

195
00:12:25,382 --> 00:12:28,762
call. These are models that are up and running and are always

196
00:12:28,896 --> 00:12:32,350
learning because so many of our other customers are using it.

197
00:12:32,420 --> 00:12:35,790
And you will be able to just, with an API call,

198
00:12:35,860 --> 00:12:39,646
get the inferences back. And you do not have to go

199
00:12:39,668 --> 00:12:43,182
down the route of building a model, training it,

200
00:12:43,236 --> 00:12:47,120
validating it and monitoring it of any sort,

201
00:12:47,430 --> 00:12:51,250
you're just going to consume it. But let's say you are a bunch of

202
00:12:51,400 --> 00:12:54,674
developers who are already actually into machine learning and want

203
00:12:54,712 --> 00:12:58,920
to make your life simple, then that's where ML services

204
00:12:59,450 --> 00:13:03,346
with Amazon Sagemaker as a platform would come into picture.

205
00:13:03,538 --> 00:13:07,394
Amazon Sagemaker is a platform that was built ground up

206
00:13:07,532 --> 00:13:10,986
for developers. The primary ambition was

207
00:13:11,008 --> 00:13:14,662
to actually make machine learning development easy for developers,

208
00:13:14,806 --> 00:13:18,186
and hence there are lot of services that

209
00:13:18,208 --> 00:13:22,834
it packs that makes the development lifecycle

210
00:13:22,902 --> 00:13:25,600
lot easier than it was before.

211
00:13:26,290 --> 00:13:29,562
We will dive into that part in detail, but otherwise,

212
00:13:29,626 --> 00:13:33,362
if you are experts and if you want to actually do

213
00:13:33,416 --> 00:13:37,150
it at your own pace,

214
00:13:37,310 --> 00:13:41,170
the frameworks of your choice, and then

215
00:13:41,240 --> 00:13:44,734
we completely support Tensorflow, Mxnet and all those popular

216
00:13:44,782 --> 00:13:48,594
frameworks, and quite a lot of instances

217
00:13:48,722 --> 00:13:52,226
and GPU

218
00:13:52,258 --> 00:13:55,234
based instances that are available for you to leverage.

219
00:13:55,362 --> 00:13:58,858
But we would primarily focus on Sagemaker as a platform

220
00:13:58,944 --> 00:14:02,762
as part of these session, because it's a very big world

221
00:14:02,816 --> 00:14:06,700
to explore on its own. So let's just take ourselves to

222
00:14:07,390 --> 00:14:10,066
the main hero of today's subject,

223
00:14:10,118 --> 00:14:13,680
Sagemaker. So amazing. Sagemaker. As I said,

224
00:14:14,130 --> 00:14:17,706
you can build, train and deploy ML models

225
00:14:17,738 --> 00:14:21,274
at scale. Now at scale is the key term

226
00:14:21,322 --> 00:14:24,094
there. Whatever be the part of your journey,

227
00:14:24,222 --> 00:14:27,950
either it be building or training or deploying.

228
00:14:28,030 --> 00:14:31,234
For each of these stages, Sagemaker as

229
00:14:31,272 --> 00:14:35,106
a platform offers you the right set of APIs with

230
00:14:35,208 --> 00:14:38,662
bunch of python code. You will be able

231
00:14:38,716 --> 00:14:42,294
to build your model, train your model, validate your

232
00:14:42,332 --> 00:14:46,006
model and deploy it. And of course, AWS, part of

233
00:14:46,028 --> 00:14:49,514
today's demo, we will show you how it is being done,

234
00:14:49,632 --> 00:14:53,830
but that's how easy it is to get going in Sagemaker.

235
00:14:53,910 --> 00:14:57,994
So to start with, we offer pre

236
00:14:58,032 --> 00:15:01,754
built notebooks. These pre built notebooks examples

237
00:15:01,802 --> 00:15:05,914
are very much available within sagemaker environment. In your own AWS

238
00:15:05,962 --> 00:15:09,934
console, machine learning developers can just use these examples to

239
00:15:09,972 --> 00:15:13,498
start experimenting for their own specific use cases.

240
00:15:13,594 --> 00:15:16,866
So if you are actually new to sagemaker, you do not have

241
00:15:16,888 --> 00:15:20,542
to start from scratch. You could leverage these pre built notebook

242
00:15:20,606 --> 00:15:24,066
examples and use that as a

243
00:15:24,168 --> 00:15:27,250
starter to explore the sagemaker environment.

244
00:15:27,410 --> 00:15:31,974
Now, let's say you explored it and you

245
00:15:32,012 --> 00:15:35,814
want to actually move forward. Then once you settle in

246
00:15:35,852 --> 00:15:39,718
with your training data, of course, that's these most important bit.

247
00:15:39,804 --> 00:15:44,074
Your models are as good as your training data. So quite

248
00:15:44,112 --> 00:15:47,254
a lot of our customers spend a lot of time in ensuring

249
00:15:47,302 --> 00:15:51,002
they have the right data. Let's say you have the right data, training data

250
00:15:51,056 --> 00:15:54,582
that you need. You then need to choose right algorithms

251
00:15:54,646 --> 00:15:58,174
that will have to actually go with them. Now, when it comes

252
00:15:58,212 --> 00:16:02,506
down to algorithm. We have lots of choices within

253
00:16:02,628 --> 00:16:05,986
these sage maker world. As you see either it

254
00:16:06,008 --> 00:16:09,170
be regression or classification, or image

255
00:16:10,870 --> 00:16:14,818
vision based aa models. You have quite a lot

256
00:16:14,824 --> 00:16:18,642
of built algorithms that comes very handy. These algorithms

257
00:16:18,706 --> 00:16:22,294
are in the form of containers. All you need to do is

258
00:16:22,332 --> 00:16:26,326
actually refer to the container registry and pull the

259
00:16:26,348 --> 00:16:29,946
right container for this algorithms, and then you should

260
00:16:29,968 --> 00:16:33,430
be get going with it. Otherwise,

261
00:16:33,590 --> 00:16:36,986
if none of these existing built in algorithms does

262
00:16:37,008 --> 00:16:41,162
not cater to your needs, you could always bring in your own algorithms

263
00:16:41,306 --> 00:16:45,950
in the form of containers and leverage

264
00:16:46,290 --> 00:16:50,720
the sagemaker as a platform. Now, what's unique about

265
00:16:52,390 --> 00:16:56,062
these inbuilt algorithms is that they could actually take advantage

266
00:16:56,126 --> 00:17:00,254
of distributed computing infrastructure

267
00:17:00,302 --> 00:17:06,806
we have in cloud. And you

268
00:17:06,908 --> 00:17:10,946
might sometimes find the equivalent of these algorithms

269
00:17:11,058 --> 00:17:15,686
in open source as well. But the ones

270
00:17:15,788 --> 00:17:19,958
that are inbuilt with Sagemaker are validated

271
00:17:20,134 --> 00:17:23,526
for their efficiency in utilizing the distributed

272
00:17:23,558 --> 00:17:27,514
compute environment and infrastructure that the cloud offers. And hence it is

273
00:17:27,552 --> 00:17:31,246
always relatively lot performant than the

274
00:17:31,268 --> 00:17:34,414
open source version of the equivalent algorithm you will find out

275
00:17:34,452 --> 00:17:38,314
in the market, in the open source market. So let's

276
00:17:38,362 --> 00:17:41,866
say you determine the algorithm,

277
00:17:41,978 --> 00:17:45,242
then you need to train the

278
00:17:45,316 --> 00:17:48,770
model. So you will want to tell

279
00:17:48,920 --> 00:17:51,710
the number of missions you want to use for your training purpose.

280
00:17:51,790 --> 00:17:55,566
You can then kick start the training with just one line of python

281
00:17:55,598 --> 00:17:58,806
code. Yes, you heard it right, it is just one line of

282
00:17:58,828 --> 00:18:02,854
python code within your sage maker SDK or

283
00:18:03,052 --> 00:18:06,680
click in a console. And that's all it takes to actually

284
00:18:07,290 --> 00:18:10,874
get the training going. And you can actually do

285
00:18:10,912 --> 00:18:13,500
your training and create your model out of it.

286
00:18:14,190 --> 00:18:17,466
These are so many deep learning framework containers that

287
00:18:17,488 --> 00:18:20,666
are supported in Sagemaker. So it

288
00:18:20,688 --> 00:18:23,726
uses docker containers, as I said earlier, that's designed for

289
00:18:23,748 --> 00:18:27,194
that specific algorithm, and that's designed to support that particular framework.

290
00:18:27,242 --> 00:18:30,894
So let's say you have a particular algorithm in Tensorflow that

291
00:18:30,932 --> 00:18:34,174
you want to use for your use case. Then you

292
00:18:34,212 --> 00:18:37,614
will find a container for that particular framework,

293
00:18:37,662 --> 00:18:40,754
for that particular algorithm. And as long as you

294
00:18:40,792 --> 00:18:43,394
actually refer to that in your code, you will be able to pick and run

295
00:18:43,432 --> 00:18:47,380
with it. What you do is when executing the training,

296
00:18:47,830 --> 00:18:51,814
as I said, you just select the right container and provide the data in

297
00:18:51,852 --> 00:18:55,426
form of a file in simple storage service. Now what Sagemaker

298
00:18:55,458 --> 00:18:58,306
would do is launch cluster of training machines.

299
00:18:58,418 --> 00:19:01,574
And again, it will not just launch a lot of machines,

300
00:19:01,622 --> 00:19:04,714
you do not have to worry about the cost, it just launches the

301
00:19:04,752 --> 00:19:08,586
number of machines you are told within your code

302
00:19:08,688 --> 00:19:11,958
and the type of instances that you have advised for it

303
00:19:11,984 --> 00:19:15,182
to pick up for the trading purpose, and then use

304
00:19:15,236 --> 00:19:18,846
those machines to train with the data that it

305
00:19:18,868 --> 00:19:22,442
has from s three. Now, it can also perform distributed training.

306
00:19:22,516 --> 00:19:26,306
As I said earlier, if it

307
00:19:26,328 --> 00:19:29,986
is distributed, you can train it in multiple instances, or you

308
00:19:30,008 --> 00:19:33,438
could always choose to train it in single machine

309
00:19:33,534 --> 00:19:37,478
and end of the day, the output, the resulting model

310
00:19:37,564 --> 00:19:41,640
will be back in s three. So you could always actually

311
00:19:42,410 --> 00:19:46,930
look at the model and see how accurate

312
00:19:47,010 --> 00:19:51,366
is the model giving inferences,

313
00:19:51,478 --> 00:19:54,970
and then decide to go ahead or not, or retrain the model

314
00:19:55,040 --> 00:19:58,220
again with new set of data. Now,

315
00:19:59,390 --> 00:20:02,558
what if the model is not optimal enough? As I said,

316
00:20:02,724 --> 00:20:05,934
there are so many optimization techniques that are available for

317
00:20:05,972 --> 00:20:08,880
you now.

318
00:20:10,050 --> 00:20:13,954
Usually, let's say you create a model and

319
00:20:13,992 --> 00:20:18,482
it is not really performing well. What the

320
00:20:18,616 --> 00:20:22,046
usual ML developers do is actually they tune

321
00:20:22,078 --> 00:20:25,490
the hyperparameters associated with that algorithm.

322
00:20:25,910 --> 00:20:29,206
So for people who may not know what are these? There is usually

323
00:20:29,228 --> 00:20:32,738
a bunch of parameters that contents behavior of a given algorithm.

324
00:20:32,834 --> 00:20:35,894
Often machine learning developers are left in dark as to

325
00:20:35,932 --> 00:20:39,530
what value should they use for these different parameters to get their model

326
00:20:39,600 --> 00:20:42,826
optimized. That's where automated model tuning comes to the

327
00:20:42,848 --> 00:20:46,822
rescue. The reality is even machine learning practitioners

328
00:20:46,886 --> 00:20:49,990
themselves, even experienced ones,

329
00:20:50,080 --> 00:20:53,840
often don't know what to do. In these cases, they just rely on

330
00:20:56,370 --> 00:20:59,790
random grid based hyperparameter

331
00:21:00,210 --> 00:21:01,950
choosing strategy.

332
00:21:03,030 --> 00:21:07,006
Now, Sagemaker allows you to do that by kicking

333
00:21:07,038 --> 00:21:10,754
off the so called hyperparameter optimization job,

334
00:21:10,872 --> 00:21:14,514
and you specify how many machines it needs to run

335
00:21:14,632 --> 00:21:18,278
on to control the cost. And ultimately it helps you identify the

336
00:21:18,284 --> 00:21:21,874
right parameters to optimize your model. It does it with the bayesian

337
00:21:21,922 --> 00:21:25,990
search algorithm. Basically, let's say it

338
00:21:26,140 --> 00:21:30,082
ran the training with bunch of parameters and

339
00:21:30,156 --> 00:21:33,594
it gets a model accuracy and it is not

340
00:21:33,632 --> 00:21:36,906
fitting the built it will now remember what are the parameters that it

341
00:21:36,928 --> 00:21:40,654
ran with in the previous iteration, and hence choose to

342
00:21:40,692 --> 00:21:44,222
deviate away or go towards it based

343
00:21:44,276 --> 00:21:47,520
on the iteration and the learning

344
00:21:48,530 --> 00:21:52,846
that it's picking up. It is not efficient way of actually tuning

345
00:21:52,878 --> 00:21:56,194
the model, and many of our customers leverage this

346
00:21:56,232 --> 00:22:00,210
for their machine learning training

347
00:22:00,280 --> 00:22:04,162
purpose. Amazon Sagemaker Neo is something that I would briefly touch

348
00:22:04,216 --> 00:22:07,610
on with Neo. You can compile your models to be ported

349
00:22:07,630 --> 00:22:11,030
to any of the target processes you may choose to run your model on.

350
00:22:11,100 --> 00:22:14,594
This way your model would not only be smaller

351
00:22:14,642 --> 00:22:18,706
and deployment ready built, also more performant. So it doesn't

352
00:22:18,738 --> 00:22:22,442
matter which end architecture you want to port

353
00:22:22,496 --> 00:22:25,834
your model to. Neo may help you do that and you will also

354
00:22:25,872 --> 00:22:29,526
be actually carry your model with you and deploy

355
00:22:29,558 --> 00:22:33,466
it in lot lighter form in any architecture

356
00:22:33,498 --> 00:22:36,846
of your choice. And the list of architectures to which you could port it

357
00:22:36,868 --> 00:22:39,966
to is always being added on. So you

358
00:22:39,988 --> 00:22:44,354
could always check our AWS pages to figure out what

359
00:22:44,392 --> 00:22:48,866
are the ones that we support. So let's say we

360
00:22:48,888 --> 00:22:52,974
have got the data, we trained the model, we have validated

361
00:22:53,022 --> 00:22:57,554
the model after the fine tuning thanks to hyperparameter

362
00:22:57,602 --> 00:23:01,586
optimization, and we have now got the right accuracy

363
00:23:01,698 --> 00:23:06,354
and performance we needed to deploy this in production.

364
00:23:06,482 --> 00:23:10,554
Now, what we will see as

365
00:23:10,592 --> 00:23:14,138
part of the demo as well is that

366
00:23:14,224 --> 00:23:17,622
with one line of python code, you can take this model to production.

367
00:23:17,766 --> 00:23:21,434
You can then manage the same and easily scale with Amazon

368
00:23:21,482 --> 00:23:25,390
Web services. Now that's the beauty of Sagemaker.

369
00:23:25,970 --> 00:23:29,502
Everything is simplified for the

370
00:23:29,556 --> 00:23:33,294
developers to leverage this distributed

371
00:23:33,342 --> 00:23:38,210
computing platform and focus

372
00:23:38,280 --> 00:23:41,202
on the business outcomes that they want,

373
00:23:41,336 --> 00:23:44,734
rather than all the undifferentiated heavy lifting

374
00:23:44,782 --> 00:23:48,918
they will have to do in building,

375
00:23:49,004 --> 00:23:51,510
training and validating these models.

376
00:23:51,850 --> 00:23:55,878
So that's the bet with

377
00:23:56,044 --> 00:24:00,250
Sage maker that I wanted to cover before we dive into

378
00:24:00,400 --> 00:24:03,622
the world of blazingtext algorithm.

379
00:24:03,766 --> 00:24:07,494
So as I said, blazing text algorithm

380
00:24:07,542 --> 00:24:12,142
was published back in 2017 by

381
00:24:12,196 --> 00:24:15,614
a couple of Amazonians. This is the paper that was

382
00:24:15,652 --> 00:24:20,174
released back in 2017 to

383
00:24:20,212 --> 00:24:25,074
discuss how the blazing text algorithm will go about this

384
00:24:25,112 --> 00:24:28,306
particular problem. Now, key thing to

385
00:24:28,328 --> 00:24:32,174
note these is this algorithm provides highly optimized implementation

386
00:24:32,222 --> 00:24:35,502
of word to Vic and text classification algorithm.

387
00:24:35,646 --> 00:24:39,174
Using blazing text. You can train a model with more than billion of

388
00:24:39,212 --> 00:24:43,302
words in probably a couple of minutes using

389
00:24:43,356 --> 00:24:47,442
multicore cpu or GPU, and you can achieve performance on par with state

390
00:24:47,516 --> 00:24:50,694
of art deep learning text classification algorithms

391
00:24:50,742 --> 00:24:54,954
out there. So the other important thing that

392
00:24:54,992 --> 00:24:58,518
it offers is an implementation of supervised

393
00:24:58,614 --> 00:25:01,866
multiclass, multilabel text classification algorithm,

394
00:25:01,978 --> 00:25:05,598
extending the fast text algorithm implementation by using

395
00:25:05,684 --> 00:25:09,406
GPU acceleration with custom CuDA kernels, but also relying on

396
00:25:09,428 --> 00:25:12,990
multiple cpus for certain modes of operating this algorithm.

397
00:25:14,950 --> 00:25:20,094
In this particular demo, we will be using distributed

398
00:25:20,142 --> 00:25:22,578
training, just so you know.

399
00:25:22,744 --> 00:25:26,130
But there is no hard role.

400
00:25:26,290 --> 00:25:29,586
You could always do it in single machine if that suffice

401
00:25:29,698 --> 00:25:33,638
your needs. These are some of the highlights of

402
00:25:33,724 --> 00:25:36,946
blazingtext that I would love to highlight.

403
00:25:37,058 --> 00:25:41,270
As I said, you can run with single cpu instances, you can run with multiple

404
00:25:41,430 --> 00:25:44,460
GPU acceleration if needed.

405
00:25:45,630 --> 00:25:48,860
And these interesting

406
00:25:49,230 --> 00:25:53,290
thing that you would see from these slide is it can be 21 times faster

407
00:25:53,370 --> 00:25:56,686
and 20% cheaper than fast text on a

408
00:25:56,708 --> 00:26:01,326
single c four. And if

409
00:26:01,348 --> 00:26:05,386
you go down the distributed training route, it can achieve

410
00:26:05,418 --> 00:26:08,546
a training speed of up to 50 million words per second.

411
00:26:08,728 --> 00:26:12,242
Now this is speed of eleven times over one

412
00:26:12,296 --> 00:26:15,606
c four lodge that we saw in the previous one, which is amazing,

413
00:26:15,708 --> 00:26:21,366
actually, the kind of efficiency that

414
00:26:21,388 --> 00:26:25,618
these have actually managed to harness

415
00:26:25,794 --> 00:26:30,300
from blazing text algorithm is amazing. Now how do they do that

416
00:26:30,750 --> 00:26:34,406
in our demo? As I said, we will use blazing

417
00:26:34,438 --> 00:26:38,442
text on multiple cpus. But even within single cpu, blazing text

418
00:26:38,496 --> 00:26:42,506
takes certain steps to optimize its performance. And that's

419
00:26:42,538 --> 00:26:44,400
what we are seeing here.

420
00:26:46,770 --> 00:26:50,586
It uses blast two by intel

421
00:26:50,698 --> 00:26:53,698
and hence it is a

422
00:26:53,704 --> 00:26:57,614
lot more optimized in terms of cpu utilization.

423
00:26:57,742 --> 00:27:02,126
And as you see here, this picture

424
00:27:02,158 --> 00:27:05,446
is showing how we

425
00:27:05,468 --> 00:27:08,994
are optimizing bird to vec by sharing the k negative

426
00:27:09,042 --> 00:27:12,600
samples across using the blast to

427
00:27:12,970 --> 00:27:16,886
advantage that we have from intel. Now this is a

428
00:27:17,068 --> 00:27:20,902
slide that compares the throughput of 1 billion bird

429
00:27:21,046 --> 00:27:24,458
benchmark data set. Over here you are seeing

430
00:27:24,624 --> 00:27:27,946
throughput characteristics of the blazingtext right hand

431
00:27:27,968 --> 00:27:31,226
side you see implementation of fast text, sort of what

432
00:27:31,248 --> 00:27:34,702
is published out there. Because fast text is not able

433
00:27:34,756 --> 00:27:38,478
to be distributed on multiple cpus or gpus, we are

434
00:27:38,564 --> 00:27:41,774
running it for benchmarking on single machine. Now you can

435
00:27:41,812 --> 00:27:45,646
compare and contrast the performance when you look at left hand side where the algorithm

436
00:27:45,678 --> 00:27:49,282
has been run on multiple multicore gpu machines. And in the middle

437
00:27:49,336 --> 00:27:52,574
section where you see the yellow bars, we have performed batch

438
00:27:52,622 --> 00:27:56,034
skip gram benchmarking. And again here you are seeing the results of

439
00:27:56,072 --> 00:27:59,190
running algorithm in distributed fashion on multiple cpus.

440
00:27:59,530 --> 00:28:02,886
But of course throughput is always not the only factor that you

441
00:28:02,908 --> 00:28:06,674
would choose to run with a particular algorithm

442
00:28:06,722 --> 00:28:10,042
because we also need to consider accuracy and cost.

443
00:28:10,176 --> 00:28:13,914
So I would love to actually show you another set

444
00:28:13,952 --> 00:28:17,930
of benchmark results.

445
00:28:18,510 --> 00:28:20,640
Basically, as you see here,

446
00:28:21,650 --> 00:28:25,434
I think the right way to interpret this diagram

447
00:28:25,562 --> 00:28:29,038
is that the circle that we see

448
00:28:29,124 --> 00:28:33,906
here is the throughput. And when

449
00:28:33,928 --> 00:28:37,506
you compare number eight and number two,

450
00:28:37,688 --> 00:28:41,522
sorry, I think these number

451
00:28:41,576 --> 00:28:45,378
eight is the one that was run by

452
00:28:45,464 --> 00:28:49,318
skip Graham blazing text. So yes, what I

453
00:28:49,324 --> 00:28:52,760
said is right, if you compare number eight and number two,

454
00:28:53,290 --> 00:28:56,610
you get lot of throughput, you get lot of accuracy.

455
00:28:56,690 --> 00:29:00,780
In fact, almost same accuracy for very less cost.

456
00:29:01,550 --> 00:29:04,742
The horizontal axis here is cost. The vertical

457
00:29:04,806 --> 00:29:08,870
axis here is accuracy. The size of the circle denotes the throughput

458
00:29:08,950 --> 00:29:12,574
that that particular algorithm is able to achieve with

459
00:29:12,772 --> 00:29:15,146
the given configuration.

460
00:29:15,338 --> 00:29:19,630
So it just goes on to confirm

461
00:29:20,290 --> 00:29:24,178
the previous claim we had that

462
00:29:24,264 --> 00:29:28,414
it is a lot more performant and lot more cheaper when compared

463
00:29:28,462 --> 00:29:32,142
to our other fast text algorithm.

464
00:29:32,286 --> 00:29:35,494
With that we will move on to

465
00:29:35,532 --> 00:29:39,570
the demo part of our discussion,

466
00:29:39,730 --> 00:29:43,398
which is called as semurai in Tambur. By the

467
00:29:43,404 --> 00:29:47,650
way, let me bring the demo

468
00:29:47,740 --> 00:29:51,066
page for you. So hope you're able to

469
00:29:51,168 --> 00:29:54,700
see this. This is the

470
00:29:55,150 --> 00:29:58,726
notebook that I have created in Amazon

471
00:29:58,758 --> 00:30:03,066
Sagemaker. Now you could just go to Amazon

472
00:30:03,098 --> 00:30:07,200
Sagemaker service. I can probably

473
00:30:07,970 --> 00:30:09,520
show you that quickly.

474
00:30:12,610 --> 00:30:16,578
I think we'll just continue with the notebook because

475
00:30:16,744 --> 00:30:19,620
it might be a bit tricky for me to actually share that screen.

476
00:30:20,310 --> 00:30:23,442
So it's very simple.

477
00:30:23,496 --> 00:30:27,106
If you go to Amazon Sagemaker service within AWS console,

478
00:30:27,298 --> 00:30:31,206
you go to notebook that

479
00:30:31,228 --> 00:30:35,106
will be in your left panel and you create a Jupyter notebook.

480
00:30:35,218 --> 00:30:38,834
This is as simple as any Jupyter notebook

481
00:30:38,882 --> 00:30:43,498
that you would have seen, nothing special about it. And it

482
00:30:43,504 --> 00:30:47,194
is just an environment for, if someone is

483
00:30:47,232 --> 00:30:51,182
not aware of it. It is just an environment for data scientists to

484
00:30:51,236 --> 00:30:55,290
share and do more data science

485
00:30:55,450 --> 00:30:57,200
in a collaborative way.

486
00:30:58,690 --> 00:31:01,840
That's all about this environment. Now,

487
00:31:02,210 --> 00:31:03,700
if you look at this,

488
00:31:05,750 --> 00:31:09,378
as I said, we are going to actually take a large corpus of text

489
00:31:09,464 --> 00:31:12,786
in Tamil. We will use this large corpus of

490
00:31:12,808 --> 00:31:16,094
text for data ingestion purpose. Now, in this case,

491
00:31:16,152 --> 00:31:19,494
I have actually taken the dump from this

492
00:31:19,532 --> 00:31:23,654
particular URL. You could very well actually get it from whatever

493
00:31:23,772 --> 00:31:26,854
of your choice. But ideally, Aws, I said your

494
00:31:26,892 --> 00:31:30,620
model is as good as your data. So always please be

495
00:31:32,110 --> 00:31:35,420
careful with the data that you choose.

496
00:31:36,350 --> 00:31:40,106
In my case, I've chosen it from wiki dump. It's totally up

497
00:31:40,128 --> 00:31:43,360
to you where you get it from, but more the data,

498
00:31:43,890 --> 00:31:47,690
more you could learn and actually the inferences

499
00:31:47,770 --> 00:31:52,138
could be a lot better. Now, we have downloaded this wiki

500
00:31:52,154 --> 00:31:55,710
dump. Now there is this wiki extractor py

501
00:31:55,790 --> 00:31:58,610
script created by Atadi.

502
00:31:59,030 --> 00:32:02,754
You could find it in GitHub. What we are doing

503
00:32:02,792 --> 00:32:06,774
here is we are actually passing the dump that we have got and

504
00:32:06,812 --> 00:32:11,078
we are doing extraction of that data.

505
00:32:11,244 --> 00:32:14,642
So we have downloaded that extractor

506
00:32:14,706 --> 00:32:18,650
script and we are using that extractor script to

507
00:32:18,800 --> 00:32:22,934
extract these information from the dump.

508
00:32:23,062 --> 00:32:27,338
So what this extractor would do is just cleanse the data and

509
00:32:27,424 --> 00:32:31,078
make it easy for us to do the machine learning model training.

510
00:32:31,184 --> 00:32:35,070
So as you see, it has picked up that file and it has actually given

511
00:32:35,140 --> 00:32:39,582
us the list of words that we could actually pass

512
00:32:39,636 --> 00:32:43,454
in for our training stage. So we have all

513
00:32:43,492 --> 00:32:47,258
these tamil words, mudarpakam, katirakalai, katirangalin,

514
00:32:47,274 --> 00:32:51,026
patiyal, poviyal, varala, arupuri. So these are

515
00:32:51,048 --> 00:32:54,674
all tamil words that it has picked up from the dump. Now,

516
00:32:54,712 --> 00:32:58,454
this is a very big dump that I had

517
00:32:58,492 --> 00:33:02,546
downloaded, so I didn't want to actually waste

518
00:33:02,578 --> 00:33:06,982
the time during the demo. And hence I have done that hard

519
00:33:07,036 --> 00:33:10,490
task of actually getting this downloaded and training

520
00:33:10,560 --> 00:33:13,974
the model prior to these session. So I'll

521
00:33:14,022 --> 00:33:18,326
just drag to the bottom of extraction

522
00:33:18,438 --> 00:33:21,340
part, or rather the cleanse part,

523
00:33:22,430 --> 00:33:26,190
and these we go. So the extractor is done.

524
00:33:26,340 --> 00:33:29,374
In fact, actually it was just going on for a long time.

525
00:33:29,492 --> 00:33:32,938
I thought I've got enough data, so I just killed it

526
00:33:33,124 --> 00:33:36,610
and to get on with the next stage.

527
00:33:37,510 --> 00:33:41,554
But you can actually leave

528
00:33:41,592 --> 00:33:45,060
it for a long time if you want a lot more data.

529
00:33:46,010 --> 00:33:48,840
As I said, more the data, more it is good.

530
00:33:49,930 --> 00:33:53,378
Now this is where the sagemaker

531
00:33:53,474 --> 00:33:57,870
comes to party. If you see here, we are creating a sagemaker.

532
00:33:57,970 --> 00:34:01,062
We are importing the sagemaker SDK sagemaker.

533
00:34:01,206 --> 00:34:05,174
We are creating a sagemaker session and we are creating

534
00:34:05,222 --> 00:34:09,734
a default bucket that we will use for this

535
00:34:09,872 --> 00:34:11,630
particular training purpose.

536
00:34:12,690 --> 00:34:16,062
Now what we are doing is we are uploading the data.

537
00:34:16,196 --> 00:34:19,966
So the cleansed data that we

538
00:34:19,988 --> 00:34:23,294
now have got after running that wiki extractor Py

539
00:34:23,342 --> 00:34:27,266
is what is now being pushed to the bucket and

540
00:34:27,288 --> 00:34:30,900
we are also setting the output location for s three.

541
00:34:31,430 --> 00:34:35,010
Now those are the basic

542
00:34:37,510 --> 00:34:40,574
constructs that we need from the sage

543
00:34:40,622 --> 00:34:44,680
maker service before we could actually go about

544
00:34:46,330 --> 00:34:49,340
the algorithm side of things.

545
00:34:50,510 --> 00:34:52,540
Now, as I said,

546
00:34:53,950 --> 00:34:58,054
using the inbuilt algorithm is just one line of python

547
00:34:58,102 --> 00:35:02,766
code. And there you see we

548
00:35:02,788 --> 00:35:05,946
are from Sagemaker. We are importing

549
00:35:05,978 --> 00:35:10,538
image URis and we are saying image

550
00:35:10,634 --> 00:35:13,902
Blazingtext. Now this brings

551
00:35:14,046 --> 00:35:19,278
you pointer

552
00:35:19,374 --> 00:35:22,050
to the blazingtext algorithm.

553
00:35:23,910 --> 00:35:27,218
And here you see you are using Sagemaker

554
00:35:27,234 --> 00:35:31,080
blazing text container from EU west one.

555
00:35:33,690 --> 00:35:37,666
After we create the container object, which now essentially

556
00:35:37,778 --> 00:35:41,434
holds the kind of algorithm that it is going to apply for this

557
00:35:41,472 --> 00:35:44,998
training purpose. Again, as I said in this case we have chosen basing

558
00:35:45,014 --> 00:35:49,546
text. You could either go and choose some

559
00:35:49,568 --> 00:35:53,520
other built algorithm of your choice within

560
00:35:54,450 --> 00:35:58,062
Amazon Sagemaker world, or you could bring in your own

561
00:35:58,116 --> 00:36:01,166
containers that you might have in on premise to use that

562
00:36:01,188 --> 00:36:05,262
for your training purpose. And now we are actually creating

563
00:36:05,406 --> 00:36:08,340
the estimator object.

564
00:36:09,350 --> 00:36:12,930
Now estimator is where we pass

565
00:36:13,000 --> 00:36:16,434
the container object. We just created the role

566
00:36:16,482 --> 00:36:21,302
that the training job

567
00:36:21,356 --> 00:36:24,518
would assume when it is actually doing the training.

568
00:36:24,684 --> 00:36:28,742
So this role is what is going to allow it to retrieve the data

569
00:36:28,796 --> 00:36:32,380
from s three, push the data back, or push the model

570
00:36:32,750 --> 00:36:36,154
trained model back to s three and do all that sort of start or

571
00:36:36,272 --> 00:36:39,526
any other service it has to interact with. This is the role

572
00:36:39,558 --> 00:36:43,146
that probably will actually control the permissions associated

573
00:36:43,178 --> 00:36:46,734
with that particular training job. We are also giving

574
00:36:46,772 --> 00:36:49,998
it the number of instances we wanted to use for training. As I

575
00:36:50,004 --> 00:36:54,302
said, it is totally dictated

576
00:36:54,366 --> 00:36:57,300
by you. What instances are being used,

577
00:36:57,990 --> 00:37:01,586
how many instances are being used, what instance type is being used

578
00:37:01,768 --> 00:37:05,830
and what is the input mode.

579
00:37:06,330 --> 00:37:10,566
You can actually choose it to be file or

580
00:37:10,588 --> 00:37:17,394
there is another option of actually another

581
00:37:17,452 --> 00:37:19,900
performance option of input mode. You could go for.

582
00:37:21,710 --> 00:37:25,222
And once you choose these parameters

583
00:37:25,286 --> 00:37:28,922
and create the estimator object, you then pass

584
00:37:28,976 --> 00:37:32,638
the hyperparameters. In this case, we have actually passed the

585
00:37:32,644 --> 00:37:35,902
hyperparameters ourselves. But as I said,

586
00:37:35,956 --> 00:37:40,154
we could actually use hyperparameter tuning or the hyperparameter

587
00:37:40,202 --> 00:37:43,806
optimization option that we had mentioned earlier

588
00:37:43,838 --> 00:37:47,220
and discussed about which could actually do that

589
00:37:48,070 --> 00:37:51,762
iterations to lock in to the

590
00:37:51,816 --> 00:37:58,040
best hyperparameters that will give you the best accuracy and

591
00:37:58,730 --> 00:38:02,600
give you the best performance model that you can choose from.

592
00:38:03,690 --> 00:38:07,640
Once you set the hyperparameters, you then actually

593
00:38:09,370 --> 00:38:13,434
kick start the training by you

594
00:38:13,472 --> 00:38:16,714
point these training data that needs to be used and then you kick start

595
00:38:16,752 --> 00:38:19,050
the training by calling this fit method.

596
00:38:19,790 --> 00:38:23,534
Now, once you say model fit aws, you see here,

597
00:38:23,572 --> 00:38:27,034
it starts the training job, it completes

598
00:38:27,082 --> 00:38:34,126
the training, and you

599
00:38:34,148 --> 00:38:37,506
are going to be just charged for whatever time that

600
00:38:37,608 --> 00:38:40,386
the training has run. As you see here,

601
00:38:40,488 --> 00:38:44,098
the total training time in seconds is 32 86.

602
00:38:44,264 --> 00:38:50,162
So these four instances that you had chosen of

603
00:38:50,216 --> 00:38:53,286
type c, four, two, x, lodge, they are

604
00:38:53,308 --> 00:38:57,426
going to be charged only for those whatever seconds,

605
00:38:57,618 --> 00:39:01,626
36 odd seconds or 32 odd seconds that

606
00:39:01,728 --> 00:39:04,540
the actual training job took. Now,

607
00:39:05,230 --> 00:39:09,050
once the training is completed, the trained model is now uploaded to s three.

608
00:39:09,120 --> 00:39:12,606
It's now going to be residing in s three. And as

609
00:39:12,628 --> 00:39:15,310
I said again earlier during our session,

610
00:39:16,210 --> 00:39:20,350
right after this training is completed,

611
00:39:21,170 --> 00:39:24,962
if you are happy with these accuracy, usually our customers choose

612
00:39:25,016 --> 00:39:28,482
to have a validation stage. So one of your

613
00:39:28,536 --> 00:39:31,774
data engineers or data scientists,

614
00:39:31,822 --> 00:39:35,650
whoever controls what model gets deployed in production,

615
00:39:35,730 --> 00:39:38,706
might get an approval task,

616
00:39:38,818 --> 00:39:42,246
and they will see whether these accuracy of

617
00:39:42,268 --> 00:39:45,526
the model is good enough to be deployed in

618
00:39:45,548 --> 00:39:48,540
production, and then they will give it a go.

619
00:39:48,990 --> 00:39:52,890
So this whole thing actually could be orchestrated

620
00:39:54,030 --> 00:39:57,306
in a CI CD fashion. We have got

621
00:39:57,408 --> 00:40:00,342
something separately called sagemaker pipelines.

622
00:40:00,406 --> 00:40:03,834
It's a feature within Sagemaker that you could leverage. There's no charges

623
00:40:03,882 --> 00:40:07,902
for it, it's just the way you can do CI CD for

624
00:40:08,036 --> 00:40:11,018
machine learning. All of these tasks,

625
00:40:11,114 --> 00:40:15,010
starting from ingestion training

626
00:40:15,080 --> 00:40:18,814
and then validating, deploying, all of this could be orchestrated

627
00:40:18,942 --> 00:40:23,582
in a totally automated fashion if you want, but deployment

628
00:40:23,646 --> 00:40:27,174
itself is just that one line of code that you see

629
00:40:27,212 --> 00:40:30,326
there. So I'm happy with this model and I want

630
00:40:30,348 --> 00:40:33,762
to deploy it in this particular instance

631
00:40:33,826 --> 00:40:37,590
type. And that's it. It gets deployed.

632
00:40:40,270 --> 00:40:43,866
Now, once it is deployed, you now have an

633
00:40:43,888 --> 00:40:47,980
endpoint to do inference against. So if you see here,

634
00:40:50,130 --> 00:40:54,320
I am actually creating set of

635
00:40:55,730 --> 00:40:59,178
words that I want to use for inference.

636
00:40:59,354 --> 00:41:02,834
So these, you see, the first word

637
00:41:02,872 --> 00:41:06,242
is tamar, the second word

638
00:41:06,296 --> 00:41:11,220
is language, or mori music

639
00:41:12,070 --> 00:41:14,450
which is in Tamar isai,

640
00:41:16,010 --> 00:41:20,310
song is another word which in tamaris pardal,

641
00:41:21,610 --> 00:41:24,370
politics in Tamaris Arasiel,

642
00:41:24,530 --> 00:41:28,374
leader in Tamaris, Talibar, year in

643
00:41:28,412 --> 00:41:31,774
tamaris and century in Tamaris Notranda.

644
00:41:31,842 --> 00:41:35,226
So these are random words. Some of them are related, some of them

645
00:41:35,248 --> 00:41:39,850
are not related. We will see how the entrance behaves

646
00:41:41,090 --> 00:41:45,790
based on the context that it has explored

647
00:41:47,010 --> 00:41:51,082
with the blazingtext algorithm that we used for our training purpose.

648
00:41:51,226 --> 00:41:55,838
So we are pointing it to the endpoint that we just created

649
00:41:55,934 --> 00:41:59,410
by doing the deployment. And now

650
00:41:59,480 --> 00:42:02,514
what is happening here is Aws,

651
00:42:02,552 --> 00:42:06,178
I pass these words, it is creating vector

652
00:42:06,274 --> 00:42:09,160
representation of these words. Now for example,

653
00:42:11,210 --> 00:42:14,280
starting from here, you see,

654
00:42:14,650 --> 00:42:18,358
until here is the vector representation of word.

655
00:42:18,524 --> 00:42:22,282
Now what it is doing is it is actually trying to map these word

656
00:42:22,336 --> 00:42:25,626
thumbnail in an n dimensional space.

657
00:42:25,728 --> 00:42:30,146
And that's why you have so many weird numbers like it's

658
00:42:30,198 --> 00:42:32,480
being represented as a list here.

659
00:42:33,730 --> 00:42:37,374
So you will get these kind of list for EAch word

660
00:42:37,492 --> 00:42:41,726
you are trying to vectorize. And then what

661
00:42:41,748 --> 00:42:45,474
you are trying to do is actually map these words that are in

662
00:42:45,512 --> 00:42:48,978
n dimensional space into

663
00:42:49,144 --> 00:42:53,700
two dimensional space for

664
00:42:54,390 --> 00:42:57,958
Your picturization or visualization case.

665
00:42:58,124 --> 00:43:01,394
But otherwise, this is where the word tobac

666
00:43:01,442 --> 00:43:04,950
actually is trying to do the magic.

667
00:43:05,770 --> 00:43:08,570
Once this vectorization is completed,

668
00:43:08,990 --> 00:43:10,540
you can now actually,

669
00:43:12,590 --> 00:43:15,898
now you have the numerical representation of those words.

670
00:43:16,064 --> 00:43:19,594
It's just not zeros and ones, it's this weird list of

671
00:43:19,712 --> 00:43:22,160
array that we see there in the top.

672
00:43:23,970 --> 00:43:28,320
Now this is the real fun. If you see music,

673
00:43:29,650 --> 00:43:34,414
the word isai is closer to the

674
00:43:34,452 --> 00:43:37,550
word song because song is paddle.

675
00:43:37,890 --> 00:43:41,294
They both are closer. And hence, if you

676
00:43:41,332 --> 00:43:45,718
see the relationship or the

677
00:43:45,884 --> 00:43:49,698
vector subtraction is giving you 6.17,

678
00:43:49,794 --> 00:43:53,702
forget about that number. But that's how close

679
00:43:53,756 --> 00:43:57,240
they are is what actually it has inferred. But now

680
00:43:58,010 --> 00:44:00,758
if you see politics and leader,

681
00:44:00,934 --> 00:44:04,554
yes, they are close. And hence if you see vector of

682
00:44:04,592 --> 00:44:07,946
leader minus vector of politics, it's giving you 5.8. They are

683
00:44:07,968 --> 00:44:12,862
a lot closer because these

684
00:44:12,916 --> 00:44:16,910
are words that appear in contents. Now when I try

685
00:44:17,060 --> 00:44:19,710
music and politics,

686
00:44:21,490 --> 00:44:25,614
it has figured out that they are bit away than politics

687
00:44:25,662 --> 00:44:29,202
and leader. So what we have now

688
00:44:29,256 --> 00:44:32,766
achieved is actually we have now created

689
00:44:32,798 --> 00:44:36,946
the vector representation of each of these words

690
00:44:37,128 --> 00:44:41,320
and have now actually identified the distance between them

691
00:44:42,250 --> 00:44:45,622
contextually and where

692
00:44:45,676 --> 00:44:50,666
would they sit in an n dimensional space in

693
00:44:50,688 --> 00:44:54,054
terms of context. So that's what our inferences achieved.

694
00:44:54,102 --> 00:44:58,166
Now, because I restricted

695
00:44:58,198 --> 00:45:02,090
myself to less words within our carpus

696
00:45:04,350 --> 00:45:07,502
and did not bother much about

697
00:45:07,556 --> 00:45:11,038
accuracy, we are

698
00:45:11,044 --> 00:45:14,434
seeing what we are seeing, but with

699
00:45:14,472 --> 00:45:17,550
bit more effort on hyperparameter optimization.

700
00:45:17,710 --> 00:45:22,514
This can be lot accurate and very

701
00:45:22,552 --> 00:45:26,070
interesting inference could be made from this one. Now another

702
00:45:26,140 --> 00:45:30,246
trick that I probably mentioned earlier was you could actually bring in

703
00:45:30,268 --> 00:45:34,054
the model and unpack it and

704
00:45:34,092 --> 00:45:38,214
apply matte plotlib techniques

705
00:45:38,262 --> 00:45:41,578
to actually create a

706
00:45:41,584 --> 00:45:45,962
two dimensional representation of these words later

707
00:45:46,016 --> 00:45:52,334
on. But I think with

708
00:45:52,372 --> 00:45:56,058
that we come to the end of this session. Just to summarize,

709
00:45:56,234 --> 00:46:00,538
we started with an unknown language, the language of Tamar,

710
00:46:00,634 --> 00:46:04,046
and then we understood what is word to wake

711
00:46:04,158 --> 00:46:07,346
and what is word embedding and why do we

712
00:46:07,368 --> 00:46:11,566
need to do that. And then we introduced sagemaker

713
00:46:11,598 --> 00:46:15,714
as a platform and lot of features that comes packed

714
00:46:15,762 --> 00:46:19,030
into it and how sage Maker as a platform could actually

715
00:46:19,100 --> 00:46:22,770
help you in making your machine learning development lifecycle

716
00:46:22,850 --> 00:46:26,450
lot simpler by offloading

717
00:46:26,530 --> 00:46:30,666
the undifferentiated heavy lifting you will be doing at

718
00:46:30,688 --> 00:46:34,074
the moment. And we also explored how

719
00:46:34,112 --> 00:46:38,122
easy it is to actually apply the

720
00:46:38,176 --> 00:46:42,426
placing text algorithm on the data of your choice

721
00:46:42,618 --> 00:46:45,966
by simple demo that we saw at the

722
00:46:45,988 --> 00:46:48,766
later part of the session. Now,

723
00:46:48,948 --> 00:46:53,982
I believe this would have created some sort of interest within

724
00:46:54,036 --> 00:46:57,626
you to actually go and explore the natural language processing

725
00:46:57,818 --> 00:47:01,614
using some of the input algorithms we have within Sagemaker or

726
00:47:01,812 --> 00:47:05,942
the algorithm of your choice and play

727
00:47:05,996 --> 00:47:09,506
with one of your favorite language of your choice

728
00:47:09,698 --> 00:47:12,834
and explore the world of machine learning within the AWS

729
00:47:12,882 --> 00:47:16,134
ecosystem. Thanks for joining the session. It was my pleasure to

730
00:47:16,172 --> 00:47:19,574
actually give this session for you and wishing you a great day ahead.

731
00:47:19,692 --> 00:47:20,100
Bye now.

