1
00:00:23,690 --> 00:00:27,026
Everybody, and welcome to today we're going to be talking about building a plant monitoring

2
00:00:27,058 --> 00:00:30,978
application with influxDb python flask and with edge

3
00:00:31,074 --> 00:00:33,970
to cloud replication, which is an influxDB feature.

4
00:00:34,130 --> 00:00:38,066
So as you know already, my name is Anais Dotisgeorgiou and I'm a developer advocate

5
00:00:38,098 --> 00:00:41,570
at Influxdata, and I encourage you to connect with me on LinkedIn.

6
00:00:41,650 --> 00:00:45,058
So if you have any questions about time series influxDb

7
00:00:45,234 --> 00:00:48,546
flask Python building this plant

8
00:00:48,578 --> 00:00:51,934
buddy application, which we're going to discussed today, I encourage you to reach

9
00:00:51,972 --> 00:00:55,150
out with me there and ask any questions you have.

10
00:00:55,300 --> 00:00:58,490
So before we begin, let's go over a quick agenda.

11
00:00:58,650 --> 00:01:02,426
So first and foremost, I will be talking about the IoT hardware

12
00:01:02,458 --> 00:01:05,646
setup that we use. So this is all the devices that we use to

13
00:01:05,668 --> 00:01:08,926
monitor our plants for our plant buddy or our plant

14
00:01:08,958 --> 00:01:12,434
monitoring application. Next, I'll be going over the tools we use to build this

15
00:01:12,472 --> 00:01:15,742
application. Then I'll be giving an overview of influxdb,

16
00:01:15,886 --> 00:01:19,122
followed by a data ingestion setup overview.

17
00:01:19,266 --> 00:01:22,598
And then I'm going to talk about flux and sql, which are

18
00:01:22,684 --> 00:01:26,258
two languages that you can use to query influxdb. Then I'll

19
00:01:26,274 --> 00:01:30,186
follow that with an understanding of how to set up edge to data replication and

20
00:01:30,208 --> 00:01:34,054
explain what edge to data replication is. I guess a spoiler,

21
00:01:34,102 --> 00:01:37,674
edge to data replication is just the process of replicating data from an

22
00:01:37,712 --> 00:01:41,278
OSS edge instance of influxdb to a cloud instance to

23
00:01:41,284 --> 00:01:44,718
consolidate your data there. Then we'll talk about the

24
00:01:44,724 --> 00:01:47,806
data requests for building the application. And last

25
00:01:47,828 --> 00:01:51,294
built, not least, share the code base. And with

26
00:01:51,332 --> 00:01:54,914
that, let's begin. Let's begin with talking about the setup for

27
00:01:54,952 --> 00:01:58,754
our IoT devices. So this is a diagram of

28
00:01:58,792 --> 00:02:02,690
roughly how our plant body, application and system

29
00:02:02,760 --> 00:02:06,226
works on the edge. So as you can see, we store and manipulate some

30
00:02:06,248 --> 00:02:09,830
of our data here in the open source build, and then

31
00:02:09,900 --> 00:02:13,414
we send a down sampled version of that data to our

32
00:02:13,452 --> 00:02:16,838
cloud instance. So downsampled data is just the process,

33
00:02:16,924 --> 00:02:20,970
or down sampling data is the process of taking high raw precision

34
00:02:21,790 --> 00:02:25,206
time series data and then creating lower precision aggregates

35
00:02:25,238 --> 00:02:29,498
of that data as sort of to provide a summary of your data.

36
00:02:29,664 --> 00:02:33,326
Because oftentimes we don't care about having that high precision data,

37
00:02:33,508 --> 00:02:37,498
especially over long periods of time. And it's

38
00:02:37,514 --> 00:02:40,926
just a way of consolidating data from multiple edge devices to the

39
00:02:40,948 --> 00:02:44,242
cloud. So in order to

40
00:02:44,376 --> 00:02:47,614
successfully build this application, you need the following

41
00:02:47,662 --> 00:02:50,862
things. The first and foremost is a plant, preferably,

42
00:02:50,926 --> 00:02:53,620
although maybe you could monitor your pet instead.

43
00:02:55,030 --> 00:02:58,886
And we used a particle board microcontroller for

44
00:02:58,908 --> 00:03:02,726
this. But you could use any other compatible microcontroller. You need at

45
00:03:02,748 --> 00:03:06,358
least one IoT sensor for your plant and a

46
00:03:06,364 --> 00:03:08,970
breadboard with jump wires and terminal strips.

47
00:03:10,990 --> 00:03:14,730
So here's a look at the breadboard schematics.

48
00:03:15,070 --> 00:03:18,966
So these schematics are for hooking up our four sensors to our breadboard.

49
00:03:19,078 --> 00:03:22,790
And this diagram is just to help break down what ports the microcontrollers and

50
00:03:22,800 --> 00:03:25,966
the sensors are connected to. And hopefully this will make it easier to

51
00:03:25,988 --> 00:03:30,074
set up the exact same setup or a similar one if you're not familiar

52
00:03:30,122 --> 00:03:34,178
with microcontrollers. And for our plant monitoring application,

53
00:03:34,264 --> 00:03:37,758
we decided to also go with four sensors.

54
00:03:37,934 --> 00:03:41,678
The first sensor monitors temperature and humidity. The second sensor

55
00:03:41,774 --> 00:03:45,750
monitors light, then soil moisture, and then temperature.

56
00:03:46,890 --> 00:03:50,482
So you'll notice that all of those measurements

57
00:03:50,546 --> 00:03:53,826
are all time series data, which is why we use influxdB

58
00:03:53,858 --> 00:03:58,466
to store that, because influxdB is a time series database,

59
00:03:58,578 --> 00:04:02,346
and so that's why it's a good use case for it.

60
00:04:02,528 --> 00:04:05,980
So now let's next talk about the tools that we use to build our application.

61
00:04:06,350 --> 00:04:10,406
So at the front and center is flask. So flask is a microweb

62
00:04:10,438 --> 00:04:13,886
framework that's written in Python, and it's going to be doing the heavy lifting for

63
00:04:13,908 --> 00:04:17,790
this project and help run our local application and routing.

64
00:04:18,850 --> 00:04:22,138
We're also going to be using influxdb for storage.

65
00:04:22,234 --> 00:04:26,018
So influxdb is a time series data storage engine,

66
00:04:26,104 --> 00:04:30,622
but it's also much more than that. It contains APIs

67
00:04:30,686 --> 00:04:35,134
and various tools for working with real time data

68
00:04:35,192 --> 00:04:39,190
and applications, and it also has a massive community and ecosystem.

69
00:04:39,690 --> 00:04:41,350
There are forums,

70
00:04:42,570 --> 00:04:45,560
slack channels, and Reddit where you can go and get community support.

71
00:04:46,090 --> 00:04:49,546
Myself, I will be on those channels helping people as

72
00:04:49,568 --> 00:04:53,402
well as other developer advocates and engineers. There is

73
00:04:53,536 --> 00:04:57,546
a ton of support on GitHub as well, and all

74
00:04:57,568 --> 00:05:02,050
of our products have open source offerings. So that's influxdb

75
00:05:02,070 --> 00:05:05,450
in a nutshell. And it's an ideal solution

76
00:05:05,530 --> 00:05:09,374
for storing our sensor data because it is a time series database and

77
00:05:09,412 --> 00:05:12,782
our data is time series data. And then

78
00:05:12,836 --> 00:05:16,578
we will be using telegraph, which is the other product that influxdata, the creators of

79
00:05:16,584 --> 00:05:20,066
influxDB creates. And telegraph is an

80
00:05:20,088 --> 00:05:24,222
open source plugin, sorry, an open source

81
00:05:24,286 --> 00:05:27,938
collection agent that is plugin driven for metrics and events.

82
00:05:28,034 --> 00:05:31,894
There are over 300 plugin options to choose from. So if you

83
00:05:31,932 --> 00:05:35,522
have the task of ingesting data from a source

84
00:05:35,586 --> 00:05:39,574
and sending it somewhere else, while taking advantage of buffing and caching

85
00:05:39,622 --> 00:05:43,750
capabilities and other agent control capabilities

86
00:05:43,910 --> 00:05:47,306
with a lightweight agent that is open source, go check out telegraph. It's a pretty

87
00:05:47,328 --> 00:05:48,140
cool tool,

88
00:05:50,050 --> 00:05:53,294
and it's very simple to configure. It's configurable in a single

89
00:05:53,412 --> 00:05:56,954
ToML config file and downloadable in a single binary,

90
00:05:57,002 --> 00:06:00,000
as well as influxdb oss as well.

91
00:06:02,130 --> 00:06:05,726
Next we'll talk about our client library suite. So we have

92
00:06:05,748 --> 00:06:08,046
client libraries available in multiple languages,

93
00:06:08,238 --> 00:06:11,634
and you can read and write data into influxdb with them.

94
00:06:11,672 --> 00:06:14,338
So if you don't want to use telegraph to ingest data, you can use a

95
00:06:14,344 --> 00:06:18,134
client library. We'll be using the Python client library to query our

96
00:06:18,172 --> 00:06:21,160
data and write our data to influxdb cloud.

97
00:06:22,090 --> 00:06:26,018
You can also use client libraries if there isn't an otelegraph

98
00:06:26,034 --> 00:06:29,722
plugin like I mentioned. And next I'll be showing a code example

99
00:06:29,776 --> 00:06:31,020
for how to do this.

100
00:06:32,750 --> 00:06:36,294
Also, I want you to be aware of the visual studio flux extension.

101
00:06:36,342 --> 00:06:39,290
So flux is a query language in influxdb OSS,

102
00:06:39,630 --> 00:06:43,134
and it allows you to execute sophisticated data

103
00:06:43,172 --> 00:06:46,702
analysis on your time series data. Sophisticated queries also

104
00:06:46,756 --> 00:06:50,334
create checks and alerts and a bunch of different

105
00:06:50,372 --> 00:06:54,254
things. So this extension is particularly useful for

106
00:06:54,452 --> 00:06:57,522
executing some of those queries that you don't have to go back and forth between

107
00:06:57,576 --> 00:07:02,046
vs code, where you're probably building your application, and the influxdb

108
00:07:02,078 --> 00:07:05,558
UI to build your flux queries. You can just

109
00:07:05,724 --> 00:07:09,430
stay in vs code, which is really helpful for developers.

110
00:07:10,650 --> 00:07:14,722
And last but not least, we will be using plotly. So plotly

111
00:07:14,786 --> 00:07:18,482
is a Python graphing library that makes interactive publication

112
00:07:18,546 --> 00:07:22,138
quality graphs. It's open source and free and easy to use. And look at all

113
00:07:22,144 --> 00:07:24,410
the beautiful graphs that you can create with Polly.

114
00:07:25,630 --> 00:07:28,874
So now I'll give an overview of influxdb, and I've already talked some about

115
00:07:28,912 --> 00:07:32,426
influxdb, but let's just make sure we're on the same page. In order to do

116
00:07:32,448 --> 00:07:35,726
that, we need to establish some context, and that context is answering the

117
00:07:35,748 --> 00:07:39,374
question, what exactly is time series data? So essentially, time series data

118
00:07:39,412 --> 00:07:42,766
is any data that has a timestamp that is associated with it.

119
00:07:42,868 --> 00:07:47,102
Examples include weather conditions, stock prices, customer monitoring,

120
00:07:47,166 --> 00:07:50,846
I mean, even point of sale transactions, healthcare logs

121
00:07:50,878 --> 00:07:54,670
and traces. And we usually think of time series data existing in two categories,

122
00:07:54,750 --> 00:07:57,954
metrics, which is time series data that's gathered at a regular interval.

123
00:07:58,002 --> 00:08:01,302
So when we measure the temperature data of our

124
00:08:01,356 --> 00:08:05,186
house, for example, consider that to be a metric, because we're

125
00:08:05,218 --> 00:08:08,698
measuring IoT regularly, and then we have events, and those are measurements that

126
00:08:08,704 --> 00:08:13,274
are gathered at irregular time intervals. So when

127
00:08:13,312 --> 00:08:16,380
something maybe happens, when something's triggered, for example.

128
00:08:18,430 --> 00:08:21,886
So where does time series appear? Well, the short answer is that it appears in

129
00:08:21,908 --> 00:08:25,166
almost any application across a lot of different spaces. So the first

130
00:08:25,188 --> 00:08:28,154
is consumer and industrial IoT, manufacturing,

131
00:08:28,202 --> 00:08:32,058
industrial platforms, renewable energy and fleet management.

132
00:08:32,154 --> 00:08:35,666
These all contain and provide and create a lot

133
00:08:35,688 --> 00:08:38,494
of time series data. So when we think of things like pressure,

134
00:08:38,542 --> 00:08:42,130
concentration, flow rate, rotations per minute,

135
00:08:43,270 --> 00:08:46,446
temperature, humidity, et cetera,

136
00:08:46,478 --> 00:08:49,762
these are all sensors vibrations, these are all sensors that might exist

137
00:08:49,826 --> 00:08:53,494
in those spaces and data that's collected in those

138
00:08:53,532 --> 00:08:57,078
categories. Then we have software infrastructure. You want

139
00:08:57,084 --> 00:09:01,194
to be able to monitor your

140
00:09:01,312 --> 00:09:05,014
API, endpoints and also developer

141
00:09:05,062 --> 00:09:08,378
tools. And then DevOps is a huge DevOps monitoring is a

142
00:09:08,384 --> 00:09:11,782
huge source of time series,

143
00:09:11,926 --> 00:09:14,890
as well as your containers in Kubernetes.

144
00:09:15,050 --> 00:09:18,906
And last but not least, we can think of real time applications. Things like gaming

145
00:09:18,938 --> 00:09:22,222
applications and fintech are really huge sources of time series data,

146
00:09:22,276 --> 00:09:23,970
but also network monitoring.

147
00:09:26,550 --> 00:09:30,302
So now let's talk about the emergence of the time series database category.

148
00:09:30,446 --> 00:09:34,066
So we came from relational databases and

149
00:09:34,088 --> 00:09:37,602
then document databases like MongoDB came on the scene

150
00:09:37,666 --> 00:09:40,950
and more recently search databases.

151
00:09:42,570 --> 00:09:48,246
But there was a unique need for

152
00:09:48,268 --> 00:09:52,074
databases that can specifically handle time series. And what are

153
00:09:52,112 --> 00:09:56,314
these other types of database categories missing that time series databases have

154
00:09:56,352 --> 00:10:00,406
and address? The first one is that when we write

155
00:10:00,448 --> 00:10:04,670
and think and deal with time series data, we're typically only concerned with

156
00:10:04,740 --> 00:10:07,680
ingesting that data and the queryability of that data.

157
00:10:08,050 --> 00:10:11,710
It's very rare that when you are collecting

158
00:10:12,630 --> 00:10:16,290
really high throughput time series data that you are interested in

159
00:10:16,360 --> 00:10:19,598
performing single point deletes or updates.

160
00:10:19,694 --> 00:10:23,540
So time series databases should create

161
00:10:25,370 --> 00:10:29,794
design assumptions around that and essentially

162
00:10:29,922 --> 00:10:33,734
make trade offs in their design that prioritize really high

163
00:10:33,772 --> 00:10:38,330
ingest and really high reads over

164
00:10:38,480 --> 00:10:42,330
updates and deletes, some things that those other

165
00:10:42,400 --> 00:10:45,930
databases are better at performing. Additionally,

166
00:10:46,350 --> 00:10:49,754
you need ways to interact with your time

167
00:10:49,792 --> 00:10:53,530
series effectively like you probably want. It's very helpful

168
00:10:53,690 --> 00:10:57,290
out of the box to be able to have a visualization component

169
00:10:57,370 --> 00:11:01,934
to a database as well, and a UI for that, just because time

170
00:11:01,972 --> 00:11:06,130
series data isn't really that well understood without graphs.

171
00:11:06,630 --> 00:11:10,622
Second component is abilities to manage your time series lifecycle.

172
00:11:10,686 --> 00:11:14,226
So you might need to be able to automatically expire old data

173
00:11:14,408 --> 00:11:18,638
as soon as it's become irrelevant, and also reduce

174
00:11:18,654 --> 00:11:21,926
your data or downsample your data from raw, high precision data to

175
00:11:21,948 --> 00:11:24,950
lower precision aggregates, so that when you view your historical data,

176
00:11:25,020 --> 00:11:29,030
you can view it as a snapshot and see overall trends more effectively.

177
00:11:29,470 --> 00:11:33,206
You also want to be able to have tools that help you work with timestamps

178
00:11:33,238 --> 00:11:36,374
more easily built, work across time zones more easily.

179
00:11:36,502 --> 00:11:39,658
So ideally you have a

180
00:11:39,664 --> 00:11:43,326
database that also contains additional features and is

181
00:11:43,348 --> 00:11:47,658
a whole platform that makes working with time series data easier,

182
00:11:47,754 --> 00:11:52,154
and that's what influxdata and influxDB aims to do. So this is the architecture

183
00:11:52,282 --> 00:11:56,642
diagram for influxdata and

184
00:11:56,696 --> 00:11:59,950
more specifically the OSS version. So with the OSS

185
00:12:00,030 --> 00:12:03,186
version, influxdb itself is a storage engine, but so much more.

186
00:12:03,208 --> 00:12:07,060
It also has that visualization layer and a query and task engine.

187
00:12:07,910 --> 00:12:11,302
Then we have telegraph, which we've talked about, where you have over 300

188
00:12:11,356 --> 00:12:14,966
plus plugins and listed. There are some of the input plugins that you

189
00:12:14,988 --> 00:12:19,666
can use, for example. And so the main goal

190
00:12:19,698 --> 00:12:22,958
of influxDB and influx data, just to resummarize,

191
00:12:22,994 --> 00:12:25,786
is that you can get data from a lot of different sources. You can pull

192
00:12:25,808 --> 00:12:29,526
that data into influxdb with a variety of different tools,

193
00:12:29,718 --> 00:12:34,042
and then you can use influxdb itself to not only transform

194
00:12:34,106 --> 00:12:38,062
and downsample that data, but create triggers and alerts, and even use

195
00:12:38,196 --> 00:12:41,610
flux, which is the query and scripting language for influxdB,

196
00:12:41,690 --> 00:12:45,074
to also collect that data. And then you want to be able to send that

197
00:12:45,112 --> 00:12:49,026
data for additional application workflows to

198
00:12:49,048 --> 00:12:52,222
gain infrastructure insights and even perform IoT

199
00:12:52,286 --> 00:12:55,666
actions. So now let's talk

200
00:12:55,688 --> 00:12:59,622
about our data ingestion setup. So I'm not going to go into depth on

201
00:12:59,676 --> 00:13:02,658
how the setup the microcontroller.

202
00:13:02,754 --> 00:13:05,986
That's only because each one is unique and its setup structures vary,

203
00:13:06,018 --> 00:13:09,618
so just follow the appropriate ones. But for the sake of understanding

204
00:13:09,634 --> 00:13:12,634
the code from here on out, I want you to be aware that mine is

205
00:13:12,672 --> 00:13:16,518
running on a port on my computer because it's plugged in directly,

206
00:13:16,614 --> 00:13:20,186
and this is an example of how the data comes in. When I run the

207
00:13:20,208 --> 00:13:24,080
command particle serial monitor, it shows me the data

208
00:13:24,770 --> 00:13:28,394
that is coming in. And the sensor data is also highly varied.

209
00:13:28,442 --> 00:13:31,706
So I'll be skipping the details for how to clean up that data and tag

210
00:13:31,738 --> 00:13:35,474
it, which we need to do for our sensors. But all

211
00:13:35,512 --> 00:13:38,514
this code is available on GitHub, so you can check that out there in more

212
00:13:38,552 --> 00:13:39,140
detail.

213
00:13:43,830 --> 00:13:47,742
So when you have your open source influxDb

214
00:13:47,806 --> 00:13:51,606
installed and running, even on localhost, you will see a UI like this

215
00:13:51,708 --> 00:13:54,806
where you can set up your bucket and token, and you can

216
00:13:54,828 --> 00:13:58,790
also do this via the CLI. But I find that using the UI is easier,

217
00:13:59,210 --> 00:14:02,186
and so I wanted to show that for this demo. So this video goes over

218
00:14:02,208 --> 00:14:05,386
how to create a bucket. That's where you're going to store your data.

219
00:14:05,568 --> 00:14:09,462
And in this selection you have the ability to set your retention

220
00:14:09,526 --> 00:14:13,150
policy as well. And a retention policy

221
00:14:13,220 --> 00:14:16,880
just describes the amount of time that you want to actually

222
00:14:17,330 --> 00:14:20,800
retain that data. And when you want to automatically expire it.

223
00:14:24,210 --> 00:14:27,694
And this video also shows you how to set up your API

224
00:14:27,742 --> 00:14:31,134
token as well, which we're doing right now. We normally

225
00:14:31,182 --> 00:14:33,490
suggest that you just use an all access token,

226
00:14:35,910 --> 00:14:40,150
but be careful with it. But for development,

227
00:14:40,730 --> 00:14:44,806
it can be useful at first when you're getting started, but you can also set

228
00:14:44,828 --> 00:14:49,414
up a specific read and write token to specify specifically

229
00:14:49,462 --> 00:14:52,758
just your bucket to protect your data and make sure that none

230
00:14:52,774 --> 00:14:55,820
of your tokens are the same.

231
00:15:06,050 --> 00:15:09,354
So we've already seen how to set up a bucket and token in the UI,

232
00:15:09,402 --> 00:15:12,878
but at this point in the code, I have set up my own bucket on

233
00:15:12,884 --> 00:15:16,126
my cloud account, and I put in the appropriate credentials

234
00:15:16,158 --> 00:15:19,054
and tokens to receive data into influx.

235
00:15:19,182 --> 00:15:22,222
And here we're using the influxdb Python client library,

236
00:15:22,366 --> 00:15:25,918
which allows you to write a few lines of code to begin streaming data into

237
00:15:25,944 --> 00:15:29,574
influxdb. The point here is a data point is being

238
00:15:29,612 --> 00:15:33,798
added to the database, and all of these values changes

239
00:15:33,884 --> 00:15:37,302
based on the device and the values. And we'll add

240
00:15:37,356 --> 00:15:41,110
tags to this point that we're writing to influxdb

241
00:15:41,190 --> 00:15:44,422
to help us differentiate between temperature,

242
00:15:44,486 --> 00:15:48,518
humidity. Or we could use a tag to differentiate between multiple users

243
00:15:48,534 --> 00:15:50,730
or multiple plants that we were monitoring.

244
00:15:51,970 --> 00:15:55,534
So that's pretty much it.

245
00:15:55,572 --> 00:15:59,626
We create a point with the point method, and we're

246
00:15:59,658 --> 00:16:03,138
encapsulating this in a write to influx function,

247
00:16:03,304 --> 00:16:08,226
and we are using the

248
00:16:08,248 --> 00:16:11,586
write method to actually write this point to our

249
00:16:11,608 --> 00:16:14,210
bucket and to our organization,

250
00:16:16,390 --> 00:16:20,006
within our organization. And so this is what

251
00:16:20,108 --> 00:16:24,226
writing data to influxdb with telegraph looks like. This is what the Toml configuration

252
00:16:24,418 --> 00:16:28,034
file looks like. So this is a telegraph config file.

253
00:16:28,162 --> 00:16:31,018
The whole thing is quite large, so I'm not going to go in depth on

254
00:16:31,184 --> 00:16:32,460
the entire thing,

255
00:16:34,110 --> 00:16:36,986
but this is a small part of the configuration file, and this is like the

256
00:16:37,008 --> 00:16:39,260
part that you're actually interested in.

257
00:16:40,510 --> 00:16:44,326
And you can either use telegraph or the client libraries to ingest

258
00:16:44,358 --> 00:16:47,470
data into your OSS instance. But in the project example,

259
00:16:47,540 --> 00:16:51,006
we are using telegraph, but we also have

260
00:16:51,028 --> 00:16:54,114
that client library code available for those who prefer to use

261
00:16:54,152 --> 00:16:58,046
that instead. Each telegraph library has its own documentation,

262
00:16:58,158 --> 00:17:00,994
but MOS is very straightforward to install and set up,

263
00:17:01,032 --> 00:17:04,654
and you just run telegraph with single commands.

264
00:17:04,702 --> 00:17:07,830
Like to run this file, you would just simply say telegraph

265
00:17:08,250 --> 00:17:11,430
config and then specify the path for the file.

266
00:17:11,770 --> 00:17:14,630
And here we're using the execd plugin,

267
00:17:15,210 --> 00:17:19,034
and we are essentially passing in a command that says we want to execute this

268
00:17:19,072 --> 00:17:22,314
python three script with the path to that

269
00:17:22,352 --> 00:17:27,242
script and the serial port to use

270
00:17:27,296 --> 00:17:30,570
that to generate

271
00:17:31,070 --> 00:17:35,002
or collect our time series data with Python

272
00:17:35,146 --> 00:17:38,800
and then take advantage of the agent to write it.

273
00:17:40,690 --> 00:17:44,206
So this is an example of how our data

274
00:17:44,308 --> 00:17:47,554
appears in table format once we've written our data

275
00:17:47,592 --> 00:17:51,314
to influxdb and actually queried it. So we have one measurement which is

276
00:17:51,352 --> 00:17:54,574
sensor data, a field which is light and soil moisture,

277
00:17:54,622 --> 00:17:59,766
and there's more fields including temperature and humidity. And we

278
00:17:59,788 --> 00:18:02,966
also have the equivalent value for each field as well as

279
00:18:02,988 --> 00:18:04,920
a timestamp associated with it.

280
00:18:06,250 --> 00:18:10,226
So it's also worth noting that influxDB

281
00:18:10,258 --> 00:18:14,214
data is making a big push to support SQL. So flux

282
00:18:14,262 --> 00:18:17,894
is still supported in OSS, but in influxdB,

283
00:18:17,942 --> 00:18:21,626
cloud flux is being replaced with SQL. And the main reason why this is the

284
00:18:21,648 --> 00:18:24,958
case is because we find that most users don't want to

285
00:18:24,964 --> 00:18:28,394
take on the burden of learning a new language

286
00:18:28,442 --> 00:18:31,566
that is proprietary to a single piece of

287
00:18:31,588 --> 00:18:35,054
technology they use. They're more comfortable with SQL. So that's why we're

288
00:18:35,102 --> 00:18:38,766
working to provide users in the cloud instances with SQL.

289
00:18:38,798 --> 00:18:42,782
However, we recognize that existing users are still taking advantage of flux.

290
00:18:42,846 --> 00:18:46,694
So if you are an OSS user, you can still

291
00:18:46,732 --> 00:18:50,434
continue to use flux, but we will be using SQL to query

292
00:18:50,482 --> 00:18:53,320
data from our influxDB cloud account.

293
00:18:54,970 --> 00:18:58,246
But I do want to just quickly introduce flux. For those

294
00:18:58,268 --> 00:19:01,738
of you who are confused by that, querying a database with

295
00:19:01,744 --> 00:19:05,002
SQL probably makes a lot of sense to a lot of people, but maybe not

296
00:19:05,056 --> 00:19:08,634
with Flux. So flux is a data scripting language that comes

297
00:19:08,672 --> 00:19:12,794
embedded with influxdB OSS, and it allows you to build data pipelines

298
00:19:12,922 --> 00:19:16,186
to query, analyze and transform your data. So that's

299
00:19:16,218 --> 00:19:20,154
a quick example of what flux looks like. It's kind of JavaScript

300
00:19:20,202 --> 00:19:23,826
esque in its syntax, but functionally sort of operates more

301
00:19:23,848 --> 00:19:27,714
like pandas, where the input of one

302
00:19:27,752 --> 00:19:31,410
line gets pipe forwarded into another

303
00:19:31,480 --> 00:19:35,640
function and each function progressively changes or

304
00:19:38,250 --> 00:19:42,134
provides some sort of analysis or transformation option on your

305
00:19:42,172 --> 00:19:42,760
data.

306
00:19:45,450 --> 00:19:48,922
So this is our most basic flux query that we use to retrieve our data

307
00:19:48,976 --> 00:19:52,490
out of influxdb. So specifically,

308
00:19:53,550 --> 00:19:56,970
the device id and field are both variables that we can change.

309
00:19:57,120 --> 00:20:01,066
For example, the device id could be one and the field could

310
00:20:01,088 --> 00:20:04,650
be air temperature, and by having values be variables,

311
00:20:04,730 --> 00:20:08,414
we call the same flux query for all of our graphs. And our

312
00:20:08,452 --> 00:20:11,038
range is currently set to the past 24 hours.

313
00:20:11,204 --> 00:20:14,446
So that's how much data we're

314
00:20:14,478 --> 00:20:18,306
going to be displaying on the graphs. But we cloud change that as well.

315
00:20:18,488 --> 00:20:22,290
And similarly as well, our bucket

316
00:20:26,170 --> 00:20:28,280
is also something that we can change.

317
00:20:32,170 --> 00:20:36,360
So let's talk a little built more about the change that is happening

318
00:20:36,810 --> 00:20:40,614
from flux to SQL. So the future of influxDB

319
00:20:40,662 --> 00:20:45,142
cloud and the future of open source. So we recently

320
00:20:45,286 --> 00:20:49,178
launched influxDB cloud powered by our IoT storage engine

321
00:20:49,344 --> 00:20:53,786
and it will allow storage in parquet file format with unlimited cardinality.

322
00:20:53,898 --> 00:20:57,262
So if you choose to use the edge to cloud replication version of this project,

323
00:20:57,316 --> 00:21:01,658
you will most likely connect with influxDB Cloud's new SQL version.

324
00:21:01,834 --> 00:21:04,766
So the new cloud version also supports SQL.

325
00:21:04,958 --> 00:21:08,178
And if you choose to stay completely in the open source version then

326
00:21:08,184 --> 00:21:12,034
you'll probably using Flux. And the plan is eventually to

327
00:21:12,072 --> 00:21:15,386
roll iocs and SQL capabilities

328
00:21:15,438 --> 00:21:18,040
to our open source actions as well.

329
00:21:19,450 --> 00:21:23,346
But we can use flight SQL plugins presently

330
00:21:23,378 --> 00:21:27,798
in future with influxDB cloud powered by IoT to take advantage of

331
00:21:27,884 --> 00:21:31,162
Apache Superset, tableau power, Bi and Grafana as well.

332
00:21:31,216 --> 00:21:34,666
So another big move to change the storage engine in

333
00:21:34,688 --> 00:21:38,646
influxDB cloud was to offer more interoperability, and that's because it's

334
00:21:38,678 --> 00:21:42,426
largely built on the Apache ecosystem on things like data fusion,

335
00:21:42,458 --> 00:21:45,966
Parquet and Arrow, which is

336
00:21:45,988 --> 00:21:47,070
all really exciting.

337
00:21:48,930 --> 00:21:51,550
But now let's talk about edge data replication.

338
00:21:52,550 --> 00:21:56,706
So edge data replication is the process

339
00:21:56,808 --> 00:22:00,594
of replicating data from an edge instance of oss to

340
00:22:00,632 --> 00:22:04,418
cloud using the edge data replication tool. So what are the

341
00:22:04,424 --> 00:22:08,290
advantages of edge data replication? Well, the first is that you reduce bandwidth

342
00:22:08,370 --> 00:22:11,750
cost of sending high fidelity data to the cloud, and it also

343
00:22:11,820 --> 00:22:15,960
has network resilience for intermittent failures and connectivity to the cloud.

344
00:22:16,890 --> 00:22:20,946
So to summarize, using a hybrid solution for our application provides

345
00:22:20,978 --> 00:22:24,774
the flexibility to move mundane tasks such as down sampling to the edge.

346
00:22:24,902 --> 00:22:28,534
And you can do that down sampling as needed for each type of device

347
00:22:28,582 --> 00:22:32,326
that you are gathering data from. And this provides

348
00:22:32,358 --> 00:22:36,240
more scope for more interesting analysis and data storage to occur in the cloud.

349
00:22:37,490 --> 00:22:40,766
So that's why we're kind of like looking at this hybrid solution and using both

350
00:22:40,788 --> 00:22:45,170
the edge and the cloud instances of influxdb.

351
00:22:46,470 --> 00:22:49,986
So tangibly, the feature of edge data replication consists of two new

352
00:22:50,008 --> 00:22:53,774
API endpoints, the remotes and replication endpoints, and two new CLI

353
00:22:53,822 --> 00:22:56,706
commands, the remotes and replication commands.

354
00:22:56,818 --> 00:23:00,306
And each replicated bucket also gets a disk

355
00:23:00,338 --> 00:23:03,474
back queue for buffering data safely

356
00:23:03,522 --> 00:23:05,830
in case of any disruptions that might exist.

357
00:23:07,610 --> 00:23:10,998
So now we have our setup instructions here,

358
00:23:11,084 --> 00:23:14,186
which can be found on our GitHub readme built. As you can see here we

359
00:23:14,208 --> 00:23:16,890
have a command to set up our edge device,

360
00:23:17,630 --> 00:23:20,860
which for this project is the open source local host I'm running.

361
00:23:22,430 --> 00:23:26,160
So just follow these commands and then you can get it started for yourself.

362
00:23:26,930 --> 00:23:31,118
And these are the two commands that you have to run

363
00:23:31,204 --> 00:23:34,814
to have your edge connected to your cloud instance. So create

364
00:23:34,852 --> 00:23:38,466
your cloud bucket in the exact same steps as your open source, but just in

365
00:23:38,488 --> 00:23:41,790
the cloud. And we have full documentation for edge

366
00:23:41,950 --> 00:23:45,314
data replication or EDR replication as well, that you can check out. That goes

367
00:23:45,352 --> 00:23:49,106
into more detail about the configuration setup. But basically it's

368
00:23:49,138 --> 00:23:52,994
two steps. You first create a remote connection and then you create a replication

369
00:23:53,042 --> 00:23:56,390
rule between localhost and cloud. So you describe basically

370
00:23:56,540 --> 00:23:59,706
what you want and how you want data to be replicated to your

371
00:23:59,728 --> 00:24:02,954
cloud instance. So now we're ready for data

372
00:24:02,992 --> 00:24:06,826
requests and visualization. So in this step we're calling for

373
00:24:06,848 --> 00:24:10,106
the previous flux query and infilling the variables with our

374
00:24:10,128 --> 00:24:13,774
selections, including bucket, sensor and device. And we return the

375
00:24:13,812 --> 00:24:17,534
result that allows us to graph our incoming data.

376
00:24:17,732 --> 00:24:21,630
And we use a query data frame method or function

377
00:24:21,700 --> 00:24:25,038
from the Python library that pulls our data back into a

378
00:24:25,044 --> 00:24:28,734
data frame format that I find easier to work with a lot of python libraries,

379
00:24:28,782 --> 00:24:31,842
especially for visualization, and there are a few different data

380
00:24:31,896 --> 00:24:35,262
outputs options that you can choose from if you prefer a different style.

381
00:24:35,326 --> 00:24:38,546
So it's just my preferred way of working with the Python client

382
00:24:38,578 --> 00:24:42,758
library. So this part of the demo is currently

383
00:24:42,844 --> 00:24:46,818
under construction. Querying with SQL we are working on redoing

384
00:24:46,834 --> 00:24:50,674
this project for the SQL support as well as updating documentation

385
00:24:50,722 --> 00:24:53,194
to go along with the project. That should be done by the end of this

386
00:24:53,232 --> 00:24:56,314
talk, so I encourage you to go check it out.

387
00:24:56,432 --> 00:25:00,300
Basically all you do is use the

388
00:25:04,110 --> 00:25:07,646
aero flight SQL instead and it's just a couple of

389
00:25:07,668 --> 00:25:11,294
lines there. And then you can query directly with SQL instead

390
00:25:11,492 --> 00:25:14,270
and return a data frame as well.

391
00:25:14,340 --> 00:25:17,838
So the process is almost identical. It's just a couple

392
00:25:17,844 --> 00:25:21,614
of lines different. But yeah, go check out the GitHub

393
00:25:21,662 --> 00:25:24,580
repo because that's all up to date there.

394
00:25:26,310 --> 00:25:29,766
And this is the end result of querying data for

395
00:25:29,788 --> 00:25:33,430
the data points and we can now graph them. And as you can see here

396
00:25:33,580 --> 00:25:37,442
things is an example of the hard coded graphs, but in the demo or

397
00:25:37,516 --> 00:25:41,130
I'll also show the selectable graphs.

398
00:25:42,270 --> 00:25:46,214
So this is what our plant body dashboard

399
00:25:46,262 --> 00:25:49,946
looks like. Basically here we have one graph where we

400
00:25:49,968 --> 00:25:52,842
are looking at the light, but we can also see the soil and room temperature

401
00:25:52,906 --> 00:25:56,606
and the humidity and soil moisture. So here's what

402
00:25:56,628 --> 00:26:00,554
the soil and room temperature looks like. And here's what the room humidity

403
00:26:00,602 --> 00:26:02,560
and soil moisture looks like.

404
00:26:04,610 --> 00:26:08,130
So now let's talk about some further resources so you can run this yourself and

405
00:26:08,200 --> 00:26:11,714
get familiar with everything that we talked about today. So try

406
00:26:11,752 --> 00:26:14,786
it for yourself. Follow the following links. Like I said,

407
00:26:14,808 --> 00:26:18,374
it will be updated with the SQL example as well, so should already

408
00:26:18,412 --> 00:26:21,720
be updated. Honestly, I encourage you to go take a look at it

409
00:26:23,210 --> 00:26:27,042
and try both the purely OSS version and OSS

410
00:26:27,106 --> 00:26:31,254
to cloud. I should mention there's also a free tier

411
00:26:31,302 --> 00:26:35,018
cloud version of influxdB. So yeah, you don't have

412
00:26:35,024 --> 00:26:38,602
to pay for anything to try influxdb cloud

413
00:26:38,656 --> 00:26:42,014
powered by IoT and last built.

414
00:26:42,052 --> 00:26:45,758
Not least, I encourage you to please please join us on

415
00:26:45,844 --> 00:26:49,450
either our slack or also our discourse forums,

416
00:26:49,530 --> 00:26:53,054
community influxdata.com, and to participate in any

417
00:26:53,092 --> 00:26:56,814
conversations around influxdb IoT or influxdb cloud

418
00:26:56,852 --> 00:27:00,654
powered by IoT. Specifically, join the influxdb underscore IoT

419
00:27:00,702 --> 00:27:04,020
channel. So again, get started yourself.

420
00:27:04,790 --> 00:27:08,200
You can visit our website and also our influxDB community

421
00:27:09,290 --> 00:27:12,646
organization contains a bunch of examples from the

422
00:27:12,668 --> 00:27:16,022
developer advocate at influxdata and also community members

423
00:27:16,076 --> 00:27:19,362
as well on different projects using influxdb.

424
00:27:19,426 --> 00:27:22,578
So that's just a good place to get inspired as well if you're just wanting

425
00:27:22,594 --> 00:27:26,914
to check out influxDB. And here are some further resources

426
00:27:26,962 --> 00:27:29,998
as well. I've mentioned a lot of these and the last one worth knowing about

427
00:27:30,044 --> 00:27:33,822
is influxdB University there at the bottom where you can get free

428
00:27:33,956 --> 00:27:37,950
instructional courses and earn badges on all things influx.

429
00:27:39,330 --> 00:27:40,060
Thank you so much.

