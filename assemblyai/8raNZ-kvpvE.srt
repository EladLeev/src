1
00:00:25,410 --> 00:00:28,200
You. Hello hello,

2
00:00:28,570 --> 00:00:32,006
my name is Joe Wingard. Today I'm going to talk to you about an

3
00:00:32,028 --> 00:00:35,606
application I've developed, I call service engine. Before I

4
00:00:35,628 --> 00:00:38,966
do that, I'd like to draw your attention to the link at the bottom where

5
00:00:38,988 --> 00:00:42,280
you will be able to download the slides that I'm about to present.

6
00:00:42,730 --> 00:00:46,914
The URL is Tinyurl Comf

7
00:00:46,962 --> 00:00:50,070
42 serviceengin

8
00:00:51,010 --> 00:00:54,000
okay, service engine, what is it?

9
00:00:55,410 --> 00:00:58,846
In short, service engine is an application that

10
00:00:58,868 --> 00:01:02,682
you can run that will connect to many

11
00:01:02,756 --> 00:01:06,670
really popular databases and it will auto provision,

12
00:01:06,750 --> 00:01:10,274
rest, GraphQL and GRPC interfaces that

13
00:01:10,312 --> 00:01:14,206
support full crud to any of the tables, views and materialized views

14
00:01:14,238 --> 00:01:17,286
inside of that database. That's what

15
00:01:17,308 --> 00:01:21,154
it is. The application is available in multiple

16
00:01:21,202 --> 00:01:24,886
ways. You can see this on the right side of this slide where

17
00:01:24,908 --> 00:01:28,086
I have it listed as multiple implementations. This is

18
00:01:28,108 --> 00:01:31,734
a JavaScript conference and I have built this framework

19
00:01:31,782 --> 00:01:36,118
and released it on NPM. So if you were to use NPM

20
00:01:36,214 --> 00:01:40,074
service engine, you would download my framework. So the number one

21
00:01:40,112 --> 00:01:43,326
there service engine, there's the repository that

22
00:01:43,348 --> 00:01:47,598
holds the source for the NPM library. Number two

23
00:01:47,764 --> 00:01:52,474
number two service engine docker that

24
00:01:52,532 --> 00:01:56,766
holds a node application that implements the NPM package and also serves

25
00:01:56,798 --> 00:01:59,570
as the source for the generic docker image.

26
00:02:00,630 --> 00:02:05,010
Then there's a third implementation service engine template.

27
00:02:05,510 --> 00:02:09,078
If you were to go there, you would see an implementation of the public

28
00:02:09,164 --> 00:02:12,726
generic Docker image. And this is what I use when

29
00:02:12,748 --> 00:02:15,080
I need to implement service engine myself.

30
00:02:16,410 --> 00:02:20,346
The key features of service engine are that it auto provisions the

31
00:02:20,368 --> 00:02:24,106
resources. I'm going to go into detail into how this works and some

32
00:02:24,128 --> 00:02:27,706
of the following slides. But the

33
00:02:27,728 --> 00:02:31,246
fastest way to explain it is that when the application

34
00:02:31,348 --> 00:02:34,506
starts up it executes survey,

35
00:02:34,698 --> 00:02:38,106
a database survey query, and it pulls internal tables and it uses

36
00:02:38,138 --> 00:02:42,586
that material to hydrate the interfaces validation

37
00:02:42,618 --> 00:02:47,090
at the source I've used happy joy for a long time as validators.

38
00:02:47,670 --> 00:02:51,118
I think Joy has moved to a different organization, but I've

39
00:02:51,134 --> 00:02:55,060
used joy validators for a long time and this application

40
00:02:56,810 --> 00:03:00,710
supports validating every call, reads and writes.

41
00:03:01,770 --> 00:03:05,042
This application also supports database schema operations.

42
00:03:05,186 --> 00:03:09,100
Kinex has terrific support for schema migrations and I believe

43
00:03:09,790 --> 00:03:13,206
every feature provided by Kinex is supported

44
00:03:13,238 --> 00:03:16,822
by the Docker image. There's some additional

45
00:03:16,966 --> 00:03:20,330
functionality that's fairly detailed and extensive.

46
00:03:20,830 --> 00:03:24,202
There's two and a half hours of documentation on YouTube,

47
00:03:24,346 --> 00:03:27,098
two and a half hours of video that I've put up, and there are links

48
00:03:27,114 --> 00:03:30,894
to those later in the deck. But an example of one such

49
00:03:30,932 --> 00:03:34,238
feature is there's a mechanism that allows

50
00:03:34,254 --> 00:03:37,886
you to intercept a query to the application after it's

51
00:03:37,918 --> 00:03:41,410
validated, but before it gets executed against the database.

52
00:03:44,070 --> 00:03:47,878
This was useful to me. An actual use case I had was appending a

53
00:03:47,884 --> 00:03:51,286
partition key to an inbound query that

54
00:03:51,308 --> 00:03:55,046
was blind to the user, so it optimized the query and the user didn't know

55
00:03:55,068 --> 00:03:57,800
why. All right, moving on.

56
00:04:02,270 --> 00:04:06,586
How it works the

57
00:04:06,608 --> 00:04:10,070
image in the bottom left with the green database and

58
00:04:10,160 --> 00:04:14,186
the blue hexagon and the three horizontal bars.

59
00:04:14,298 --> 00:04:17,722
The database this application supports all of the databases

60
00:04:17,786 --> 00:04:21,470
that are supported by Kinex. That's postgres,

61
00:04:22,710 --> 00:04:25,550
including with the PostGis extension, MySQL,

62
00:04:25,630 --> 00:04:29,250
SQLite, Oracle, Redshift, and SQL server.

63
00:04:30,470 --> 00:04:33,490
The way it works is that the application connects to the database.

64
00:04:37,350 --> 00:04:40,918
It surveys all of the tables, views and materialized views. It collects names of all

65
00:04:40,924 --> 00:04:44,226
the fields. It determines whether or not a null value is acceptable.

66
00:04:44,418 --> 00:04:47,666
It determines whether each field is part of a primary or

67
00:04:47,708 --> 00:04:51,910
unique key, and it uses that information to build happy joy validators,

68
00:04:52,070 --> 00:04:55,500
which is annotated above on number three.

69
00:04:55,950 --> 00:05:00,006
After it builds validators for each database resource, it provisions

70
00:05:00,038 --> 00:05:03,694
rest, GraphQL and GRPC services that

71
00:05:03,732 --> 00:05:07,466
allow reading and writing to the different resources in the database.

72
00:05:07,658 --> 00:05:11,946
I'm going to go into detail on the next slide into the rest interface.

73
00:05:12,058 --> 00:05:16,638
I'm not going to spend any time going over the GraphQL and GRPC implementations,

74
00:05:16,814 --> 00:05:20,702
just know that they're going to be very similar to the rest interfaces

75
00:05:20,846 --> 00:05:24,930
abstractly with the different components that are supported.

76
00:05:25,350 --> 00:05:29,042
If you want to see how the GraphQL and GRPC implementations

77
00:05:29,106 --> 00:05:33,350
are done, you can either check out the resources, code and service

78
00:05:33,420 --> 00:05:37,866
engine, or you can check out the two and a half hours YouTube video on

79
00:05:37,888 --> 00:05:41,754
the right here in those cards, mark two and three.

80
00:05:41,872 --> 00:05:45,142
I have basically the minimum

81
00:05:45,206 --> 00:05:50,298
that's required for you to run this application via public docker image and

82
00:05:50,304 --> 00:05:53,934
give it a whirl. So if you're using a database that's supported by Kinex right

83
00:05:53,972 --> 00:05:57,646
now, there are a couple of considerations which I'll hit at

84
00:05:57,668 --> 00:06:01,226
the very end. But generally speaking, if you're running one of the supported

85
00:06:01,258 --> 00:06:05,166
database dialects, you should be able to build a env file

86
00:06:05,198 --> 00:06:08,546
has indicated in card two and

87
00:06:08,568 --> 00:06:12,820
then execute the command line docker command on line three and

88
00:06:14,070 --> 00:06:17,726
the application will be running. I'm pretty verbose with my

89
00:06:17,768 --> 00:06:21,206
logging statements, so you should know right away and it should give you

90
00:06:21,228 --> 00:06:24,998
some links you should be able to click on. Some of

91
00:06:25,004 --> 00:06:28,502
the links that are available are just very general.

92
00:06:28,556 --> 00:06:32,138
There's a general health check route. So now I'm on the bottom right of

93
00:06:32,144 --> 00:06:35,658
this slide marked with the number four, there's a

94
00:06:35,664 --> 00:06:39,306
health check route that lets you know the application is up. Then there is an

95
00:06:39,328 --> 00:06:43,390
open API route that auto generates

96
00:06:43,730 --> 00:06:47,114
the open API three document, which is really useful

97
00:06:47,162 --> 00:06:50,894
for either rendering the open API three documentation in

98
00:06:50,932 --> 00:06:55,022
a web application, kind of like the swagger user interface.

99
00:06:55,166 --> 00:06:59,186
Or you can also use that endpoint to import the

100
00:06:59,208 --> 00:07:03,518
API documentation into API clients like Insomnia

101
00:07:03,614 --> 00:07:07,802
or Postman. The next endpoint

102
00:07:07,886 --> 00:07:11,714
is the proto file. This is the file that's needed to make GRPC

103
00:07:11,762 --> 00:07:15,346
calls. So you would be able to hit that endpoint, download the file

104
00:07:15,378 --> 00:07:18,854
as text and use it in a client

105
00:07:18,902 --> 00:07:22,540
to make the GRPC calls the final link.

106
00:07:23,230 --> 00:07:26,586
This service only gets provisioned if the

107
00:07:26,608 --> 00:07:29,958
node environment passed to the application is not production, but it's

108
00:07:29,974 --> 00:07:31,310
a graphql playground.

109
00:07:33,810 --> 00:07:37,600
Moving on, what I miss?

110
00:07:37,970 --> 00:07:41,290
Yeah, key concepts.

111
00:07:41,370 --> 00:07:44,642
So earlier I said when you run the application,

112
00:07:44,776 --> 00:07:48,654
it starts a rest service on port 80 80 and a graphql

113
00:07:48,702 --> 00:07:52,050
service on port 80 80 and a GRPC service

114
00:07:52,120 --> 00:07:55,506
on 50 51. So here I'm

115
00:07:55,538 --> 00:07:59,400
going to go into how the rest interface works.

116
00:08:00,490 --> 00:08:04,166
The other interfaces operate in similar manners, but you would

117
00:08:04,188 --> 00:08:06,360
have to take a look at them.

118
00:08:07,850 --> 00:08:09,714
What you want to do is you want to get an idea for the general

119
00:08:09,772 --> 00:08:13,146
pattern because the pattern is what gets defined and

120
00:08:13,168 --> 00:08:16,090
that gets hydrated by the results of the database survey.

121
00:08:16,830 --> 00:08:20,202
So at the top here, the key concept service call to

122
00:08:20,336 --> 00:08:23,958
structured query language. That's the exercise, right? You want

123
00:08:23,984 --> 00:08:27,326
to turn an HTTP call, a rest call into a

124
00:08:27,348 --> 00:08:30,638
SQL statement and then you want to execute it. So you want to

125
00:08:30,644 --> 00:08:34,420
go from card one to card two, card one.

126
00:08:34,950 --> 00:08:38,414
Most of that URL I will explain later. The sample

127
00:08:38,462 --> 00:08:43,218
app name that's a configuration variable you pass into the application service

128
00:08:43,304 --> 00:08:47,154
that has meaning that I'll hit on the next slide. But the

129
00:08:47,192 --> 00:08:51,446
final part of that path schema underscore table the

130
00:08:51,468 --> 00:08:54,854
majority of the databases that are supported by Kinex support the

131
00:08:54,892 --> 00:08:58,902
schema object. So you have a database, then a schema, and that schema holds tables,

132
00:08:58,966 --> 00:09:02,474
views and materialized views, SQLite being the outlier there.

133
00:09:02,512 --> 00:09:06,220
SQLite to my knowledge doesn't support schemas. However,

134
00:09:07,870 --> 00:09:11,534
this is the general structure for how you would call a

135
00:09:11,572 --> 00:09:15,262
specific resource inside of your database. Then next

136
00:09:15,316 --> 00:09:18,538
three, the query string arguments that follow occupation,

137
00:09:18,714 --> 00:09:22,538
state n and handle like these are fields on

138
00:09:22,564 --> 00:09:26,046
the object followed by there's dot notation.

139
00:09:26,238 --> 00:09:29,294
Then what follows the dot is the SQL operator

140
00:09:29,342 --> 00:09:33,022
you want to employ. So occupation

141
00:09:33,086 --> 00:09:36,118
has no dot, there's no operator defined. So it's going to

142
00:09:36,124 --> 00:09:40,134
fall back on the default which is equal state in

143
00:09:40,332 --> 00:09:44,246
it's using the operator and the

144
00:09:44,268 --> 00:09:47,486
value that's passed there. So it's occupation,

145
00:09:47,538 --> 00:09:51,626
engineer, state and New Jersey or Pennsylvania. That's how you would read that and

146
00:09:51,648 --> 00:09:55,430
then handle. Handle is supporting is using the like operator

147
00:09:55,510 --> 00:10:00,330
and I'm passing it pseudo

148
00:10:01,570 --> 00:10:02,960
percentage sign.

149
00:10:05,330 --> 00:10:09,022
All of the operators that are supported are marked in table five,

150
00:10:09,076 --> 00:10:10,400
which is on the top right.

151
00:10:12,370 --> 00:10:15,794
You have support for the logical operators, you have support for

152
00:10:15,832 --> 00:10:19,490
not or like null and not null and not

153
00:10:19,560 --> 00:10:23,634
in. You have support for range and then not

154
00:10:23,672 --> 00:10:27,286
range. If you're using postjis and the

155
00:10:27,308 --> 00:10:30,710
database resource has fields that are of geometry types,

156
00:10:32,810 --> 00:10:37,000
those specific fields, you will also get access to three

157
00:10:38,250 --> 00:10:41,842
geo functions, geo operators, geometry operators.

158
00:10:41,906 --> 00:10:45,626
Right now what's supported is bounding box queries and radius where

159
00:10:45,648 --> 00:10:48,346
you put in the latin long of a center point and then the number of

160
00:10:48,368 --> 00:10:51,802
meters that you want to search. And also a custom polygon.

161
00:10:51,946 --> 00:10:55,642
And the value for custom polygon would be a WKt string,

162
00:10:55,706 --> 00:10:57,230
a well known text string.

163
00:10:58,850 --> 00:11:02,382
Looking at that table on number five, I have the field field

164
00:11:02,436 --> 00:11:05,634
operator and it maps to the SQL operator. And then there's two other

165
00:11:05,672 --> 00:11:08,786
columns, multiple supported arguments and number.

166
00:11:08,888 --> 00:11:12,546
So multiple supported arguments is whether or not you

167
00:11:12,568 --> 00:11:16,146
can submit multiple values separated by default by commas.

168
00:11:16,258 --> 00:11:19,574
So in the example there I have state in New

169
00:11:19,612 --> 00:11:21,910
Jersey, Pennsylvania,

170
00:11:24,250 --> 00:11:27,526
New Jersey Pipe, Pennsylvania. The pipe is being specified as

171
00:11:27,548 --> 00:11:30,786
a separator at the bottom of that list. I'm aware

172
00:11:30,818 --> 00:11:34,426
of the typo, I noticed it earlier today for the

173
00:11:34,448 --> 00:11:37,686
first time. But by default, if you weren't to send that separator

174
00:11:37,718 --> 00:11:41,760
as a pipe, the default separator would be a comma. So state n.

175
00:11:42,450 --> 00:11:46,234
And if you look at table five, n supports multiple arguments.

176
00:11:46,282 --> 00:11:48,000
That's what the third column is.

177
00:11:49,810 --> 00:11:53,274
Some of these operators, not only do they support multiple arguments,

178
00:11:53,322 --> 00:11:57,306
they have a defined number of arguments that can be passed,

179
00:11:57,498 --> 00:12:00,686
the first one being range. And you can think about that when you're

180
00:12:00,718 --> 00:12:03,778
building these queries, you're going to search where the range and then you're going to

181
00:12:03,784 --> 00:12:05,940
pass it a lower and an upper right.

182
00:12:07,670 --> 00:12:11,166
If you were to pass it more arguments

183
00:12:11,198 --> 00:12:14,246
than that. Or yeah, if you were to pass it more arguments than that,

184
00:12:14,268 --> 00:12:17,126
you would get a validation error. You would get a 400 level error. That's very

185
00:12:17,148 --> 00:12:20,858
verbose. I'm going to speak more about validation probably in

186
00:12:20,864 --> 00:12:24,778
the next slide. Returning to card one, if you

187
00:12:24,784 --> 00:12:28,506
look at those query string arguments that begin with a pipe. So I'm looking at

188
00:12:28,528 --> 00:12:32,986
page five, limit 30 order by pipe

189
00:12:33,018 --> 00:12:37,418
fields and separator. All of those query string arguments

190
00:12:37,594 --> 00:12:41,406
are they're not quite operators, but if you look to

191
00:12:41,428 --> 00:12:45,154
the bottom right of the slide marked by six, the header is

192
00:12:45,192 --> 00:12:48,578
additional query context. This is additional material that's needed to build a

193
00:12:48,584 --> 00:12:52,254
query, but it's not related to any specific resource.

194
00:12:52,382 --> 00:12:56,374
It's supported across all of the resources. Why did

195
00:12:56,412 --> 00:12:59,814
I use a prefix of a pipe? I wanted to try

196
00:12:59,852 --> 00:13:03,330
and eliminate the possibility

197
00:13:03,410 --> 00:13:06,840
of collisions between naturally named for

198
00:13:07,210 --> 00:13:10,586
resource fields and what I'm getting at here for

199
00:13:10,608 --> 00:13:12,570
pagination ordering in fields.

200
00:13:13,710 --> 00:13:16,906
So pagination is how you would expect when you run the

201
00:13:16,928 --> 00:13:20,290
application. You can set a pagination limit. If you don't, there's a default.

202
00:13:20,390 --> 00:13:24,240
If you don't send the page, it will call page one

203
00:13:24,850 --> 00:13:28,986
ordering the ordering

204
00:13:29,098 --> 00:13:31,600
context. If you look down on table six,

205
00:13:32,370 --> 00:13:35,726
the example there is field a descending

206
00:13:35,838 --> 00:13:39,570
comma, field B comma, field C descending

207
00:13:39,990 --> 00:13:43,630
field b is ascending. You can send colon ASC,

208
00:13:43,710 --> 00:13:46,958
you don't have to, that's the default. It understands that, but that's

209
00:13:46,974 --> 00:13:50,706
how you would order fields. Speaking of fields,

210
00:13:50,738 --> 00:13:53,186
if you were to move up one in table six and look at the pipe

211
00:13:53,218 --> 00:13:56,454
fields, record the third record, these are the fields to return.

212
00:13:56,572 --> 00:14:00,026
Anytime you're reading a record, you don't have to get all the

213
00:14:00,048 --> 00:14:04,454
fields. Select star from table, you can specify

214
00:14:04,502 --> 00:14:08,742
which fields you actually want, and that's real useful. And the last is separator.

215
00:14:08,886 --> 00:14:11,580
Again, I noted the typo today.

216
00:14:12,430 --> 00:14:15,854
I will put in an issue to get that fixed, but what this

217
00:14:15,892 --> 00:14:19,338
does is by default the separator is a comma. If for whatever reason you didn't

218
00:14:19,354 --> 00:14:23,274
want to use a comma, you could dictate what you want the separator

219
00:14:23,322 --> 00:14:27,026
to be, but that will only apply to the actual fields on the

220
00:14:27,048 --> 00:14:30,706
resource. So in the example back

221
00:14:30,728 --> 00:14:34,494
to card one, the last query string argument is pipe separator,

222
00:14:34,542 --> 00:14:38,118
and we're declaring that it be a pipe. And we're using that pipe on the

223
00:14:38,124 --> 00:14:42,370
second argument state in we're separating New Jersey and Pennsylvania with a pipe.

224
00:14:42,530 --> 00:14:46,086
However, that pipe is not being considered later on with

225
00:14:46,108 --> 00:14:50,490
the order by or fields values, those will always be commas.

226
00:14:52,670 --> 00:14:55,994
Table three, table three API rest endpoints. So when you're building

227
00:14:56,032 --> 00:14:59,110
out these rest APIs to do databases operations,

228
00:14:59,190 --> 00:15:02,678
not only do you have to map out what the query string

229
00:15:02,774 --> 00:15:06,462
and the bodies are going to look like, you also have to consider like

230
00:15:06,516 --> 00:15:09,678
what's the URL structure going to be like. So what I

231
00:15:09,684 --> 00:15:13,140
have in table three are the patterns that are used by this application.

232
00:15:13,990 --> 00:15:16,850
So there are five actions that I have listed there. Create, readme,

233
00:15:16,920 --> 00:15:20,740
update, delete and read. That last read is a search.

234
00:15:25,130 --> 00:15:28,966
I distinguish between a single unique record read and a

235
00:15:28,988 --> 00:15:32,854
table search. So let's look at the first four create,

236
00:15:32,892 --> 00:15:36,786
read, update, delete so create. That's HTTP

237
00:15:36,818 --> 00:15:40,054
post. A note for using this application is

238
00:15:40,092 --> 00:15:44,202
you can create one record or many with a call, and that's across

239
00:15:44,256 --> 00:15:48,038
all the interfaces. It functions the same way for GraphQL and GrPC.

240
00:15:48,134 --> 00:15:49,900
You can create one record or many.

241
00:15:51,390 --> 00:15:54,906
A word about validation if you have a database

242
00:15:54,938 --> 00:15:58,746
table and you're trying to create a record on the table,

243
00:15:58,938 --> 00:16:02,378
and there's a field that's required which is dictated by the GDL

244
00:16:02,394 --> 00:16:05,250
and you don't include it, you're going to get a 400 level error with a

245
00:16:05,400 --> 00:16:09,300
verbose message. If there's a field that

246
00:16:09,830 --> 00:16:13,566
is of numeric type and you send it a string that can't

247
00:16:13,598 --> 00:16:17,618
be quickly cast to an integer or numeric type,

248
00:16:17,704 --> 00:16:21,158
you're going to get a 400 level error. That's the

249
00:16:21,164 --> 00:16:24,038
way the validation works. If you call a resource that doesn't exist, you're going to

250
00:16:24,044 --> 00:16:27,254
get a 400 level error and that's the validation that happens

251
00:16:27,292 --> 00:16:30,538
at the source. And it doesn't just happen on the create, it happens for all

252
00:16:30,544 --> 00:16:33,658
of the operations. So for example with read

253
00:16:33,744 --> 00:16:37,674
the next one there, this is reading a unique record. So read,

254
00:16:37,712 --> 00:16:41,546
update and delete. These are operations that happen on a single unique

255
00:16:41,578 --> 00:16:42,160
record.

256
00:16:44,930 --> 00:16:48,974
These resources don't get provisioned if the database resource doesn't have

257
00:16:49,012 --> 00:16:52,062
keys. So if you have a table with primary key,

258
00:16:52,116 --> 00:16:55,454
that table is going to support reading a single record,

259
00:16:55,652 --> 00:16:59,486
updating and deleting a single record. If you have a table without keys,

260
00:16:59,518 --> 00:17:03,442
or if you have a view or materialized view and it's not keyed, you can

261
00:17:03,496 --> 00:17:08,098
search that table, you can create to that table, but you can't update

262
00:17:08,194 --> 00:17:11,766
and delete and read an individual record if

263
00:17:11,788 --> 00:17:15,126
it is keyed. Let's just think the most simple you create

264
00:17:15,148 --> 00:17:18,470
a table has a primary key. The way you would read that is

265
00:17:18,540 --> 00:17:21,754
you would read that record. So now I'm looking at table three

266
00:17:21,792 --> 00:17:25,500
and I'm looking at the second record read get and then

267
00:17:26,270 --> 00:17:30,610
the path there is service. I'm passing in a resource parameter

268
00:17:30,790 --> 00:17:34,254
and the parameter is going to be what's indicated on card one

269
00:17:34,372 --> 00:17:38,240
schema, underscore table table view, materialized view.

270
00:17:38,770 --> 00:17:42,694
Anyway, back to table three. You would make a get request

271
00:17:42,762 --> 00:17:47,054
to serviceresourcerecord

272
00:17:47,102 --> 00:17:51,570
and record is literally record.

273
00:17:51,640 --> 00:17:54,898
That's what the string literal is going to be. And then you're going to pass

274
00:17:54,984 --> 00:17:57,090
as query string arguments the keys.

275
00:17:57,590 --> 00:18:00,778
So if it's a composite

276
00:18:00,814 --> 00:18:03,506
key, you'll pass all the fields and all the values that make up that composite

277
00:18:03,538 --> 00:18:07,400
key. If it's a single key, you'll pass in the field name and the key

278
00:18:08,250 --> 00:18:11,638
and you'll get the single record back. And that's

279
00:18:11,654 --> 00:18:14,874
also how update works and delete works. Now the last

280
00:18:14,912 --> 00:18:18,522
one is the most interesting one, the read the fifth record there,

281
00:18:18,656 --> 00:18:21,946
because at all the places I've been, that's the one that's

282
00:18:21,978 --> 00:18:25,694
used the most. You're searching tables, trying to get

283
00:18:25,812 --> 00:18:28,990
pages result sets back to calling clients.

284
00:18:30,130 --> 00:18:33,346
And that's the example we went

285
00:18:33,368 --> 00:18:37,086
over first. That's the example marked

286
00:18:37,118 --> 00:18:41,140
in card one. A couple of things I wanted to mention more about

287
00:18:42,070 --> 00:18:45,526
functionality and features. So all of those actions create,

288
00:18:45,548 --> 00:18:48,646
read, update, delete and search. They all support the

289
00:18:48,748 --> 00:18:52,882
additional query context for fields.

290
00:18:52,946 --> 00:18:56,870
So you can declare which fields that you want for all of the calls.

291
00:18:57,950 --> 00:19:02,214
Not all of the databases support returning data on mutations.

292
00:19:02,342 --> 00:19:06,774
I actually mentioned that on a later slide. But think about postgres.

293
00:19:06,822 --> 00:19:10,106
Postgres does. And if you update a record or

294
00:19:10,128 --> 00:19:13,790
you create a record, you can return the record back within the same

295
00:19:13,940 --> 00:19:17,950
request. So you can imagine if you posted

296
00:19:18,530 --> 00:19:22,330
form data, if you posted form data from a user,

297
00:19:22,490 --> 00:19:25,966
if you posted it to this endpoint and the table that it was writing

298
00:19:25,998 --> 00:19:29,298
to had a date and timestamp, or maybe an auto incrementing key,

299
00:19:29,384 --> 00:19:33,422
you can actually return that materialized back within the same request

300
00:19:33,486 --> 00:19:36,774
response lifecycle. The last piece

301
00:19:36,812 --> 00:19:40,790
worth noting here for the rest interface are the headers.

302
00:19:41,850 --> 00:19:44,360
So there are three headers that are used in this application?

303
00:19:47,050 --> 00:19:50,762
Yeah, there are three headers. The first one there always

304
00:19:50,816 --> 00:19:54,954
comes back. I call it request id, right? So when

305
00:19:54,992 --> 00:19:58,010
a request comes into the application, it gets issued a uuId.

306
00:19:58,670 --> 00:20:02,382
I implement COA so that UUID gets attached to

307
00:20:02,436 --> 00:20:06,270
the request state space

308
00:20:06,340 --> 00:20:09,566
on the object and then for the

309
00:20:09,668 --> 00:20:13,022
remaining time that that request is in the application. Anytime there's a log

310
00:20:13,076 --> 00:20:16,914
statement that UUID gets injected into the log. That makes it very easy for

311
00:20:16,952 --> 00:20:18,980
troubleshooting and finding out what happened.

312
00:20:20,950 --> 00:20:24,100
The other headers are optional, you can send them if you want.

313
00:20:25,270 --> 00:20:28,706
So the first one is X getsql. If you chose

314
00:20:28,738 --> 00:20:32,534
to send that key and you send it a truthy value, what would

315
00:20:32,572 --> 00:20:35,494
happen is as we went over with card one and card two,

316
00:20:35,532 --> 00:20:39,066
the exercise is to build a rest query and

317
00:20:39,088 --> 00:20:42,140
generate SQL and then execute it. Well,

318
00:20:43,070 --> 00:20:46,922
whatever SQL gets generated you can actually get

319
00:20:46,976 --> 00:20:50,634
back as text in the header. If you were to send x,

320
00:20:50,672 --> 00:20:53,866
get query with the truth, you value it, actually send you back the SQL string

321
00:20:53,898 --> 00:20:56,350
that was generated for you to inspect.

322
00:20:57,170 --> 00:21:01,454
The last one is get count. It's x get count. And again,

323
00:21:01,572 --> 00:21:04,478
that's optional. If you were to send that and you were to send it a

324
00:21:04,484 --> 00:21:08,578
truthy value, you would get the unpage generated total

325
00:21:08,664 --> 00:21:12,146
back. So the way that would work is going back

326
00:21:12,168 --> 00:21:15,426
to card one there. If we're doing a search based on

327
00:21:15,448 --> 00:21:19,710
this record, maybe we're looking at users or candidates,

328
00:21:19,790 --> 00:21:23,554
maybe it's an applicant tracking system. So I'm going to search for this table,

329
00:21:23,602 --> 00:21:26,758
I'm going to search for occupation as engineer. And the person is in

330
00:21:26,764 --> 00:21:30,330
New Jersey or Pennsylvania and they have a handle of pseudo.

331
00:21:31,310 --> 00:21:34,954
I'm getting page five back and 30 records per page.

332
00:21:34,992 --> 00:21:38,454
But I also need to know the unpaginated

333
00:21:38,502 --> 00:21:41,530
total so that I can render the pagination correctly.

334
00:21:42,350 --> 00:21:46,814
So the way I can get that is I can just send

335
00:21:46,932 --> 00:21:50,414
that header at the very bottom of the slide execute count. If I was to

336
00:21:50,452 --> 00:21:53,440
include that, I would get the unpagenated total count.

337
00:21:53,890 --> 00:21:57,538
So the way I have built this to work is that if you're going

338
00:21:57,544 --> 00:21:59,090
to execute these searches,

339
00:22:01,110 --> 00:22:04,462
when you reach out to get the first page, you would include that header.

340
00:22:04,526 --> 00:22:08,466
So you get the unpaged account. But then for consecutive page lookups

341
00:22:08,498 --> 00:22:12,098
you wouldn't include that. That saves a database lookup.

342
00:22:12,274 --> 00:22:15,960
That's what that saves. Okay, cool. Moving on.

343
00:22:17,210 --> 00:22:21,050
Systems design. The pattern is to normalize a request that comes in,

344
00:22:21,120 --> 00:22:25,094
validate it, and build SQL. So normalize.

345
00:22:25,222 --> 00:22:28,602
Indicated at the top are three buckets. One's red, blue and green.

346
00:22:28,736 --> 00:22:32,766
That's rest, Graphql and GrPC. We just went over in

347
00:22:32,788 --> 00:22:35,998
detail how the rest interface works. The others,

348
00:22:36,084 --> 00:22:38,960
as I said earlier, they work in similar ways.

349
00:22:40,610 --> 00:22:44,002
When they come in, they look different to the application. So the first thing that

350
00:22:44,056 --> 00:22:47,586
happens is that those

351
00:22:47,608 --> 00:22:51,346
unique calls get normalized. So on

352
00:22:51,368 --> 00:22:55,346
the right side of this slide you'll see three cards. Now, one, three and six.

353
00:22:55,528 --> 00:22:59,714
One and six are the same as the two cards from the previous slide.

354
00:22:59,842 --> 00:23:03,094
One is one, that's the HTTP rest get

355
00:23:03,132 --> 00:23:06,854
call. And six here is two on the other. And that's the SQL that gets

356
00:23:06,892 --> 00:23:10,714
generated. What's new on this slide is the addition of

357
00:23:10,832 --> 00:23:13,926
card three. Card three is a standardized

358
00:23:14,038 --> 00:23:16,938
object. I call it a query object.

359
00:23:17,104 --> 00:23:20,890
A user queried the service. This is the payload, this is the information

360
00:23:20,960 --> 00:23:24,174
that they sent. And here it's been standardized. So here's the way it works.

361
00:23:24,292 --> 00:23:27,806
Working down the stack, a rest call comes in through

362
00:23:27,828 --> 00:23:32,254
the red bucket looking much like card one. It goes through the

363
00:23:32,292 --> 00:23:36,340
service call normalizer, which is that layer that's marked with the number two.

364
00:23:37,590 --> 00:23:40,818
If the resource being called exists, it'll be

365
00:23:40,824 --> 00:23:44,046
normalized. If it doesn't exist, it will already respond

366
00:23:44,078 --> 00:23:47,346
back to the user with the 400 low layer. If it does exist,

367
00:23:47,378 --> 00:23:51,240
it'll be normalized. That normalized object

368
00:23:53,850 --> 00:23:57,126
documented by cord three gets validated. This is where

369
00:23:57,148 --> 00:24:00,294
the happy validators come in. They check all the fields that are there.

370
00:24:00,332 --> 00:24:04,234
They check the operators that are being used, as you can see, state in

371
00:24:04,272 --> 00:24:08,310
and handle. Like they're checking to make sure that the operator is supported.

372
00:24:08,470 --> 00:24:11,702
Some of the operations are supported by specific data types,

373
00:24:11,766 --> 00:24:14,458
so think. But the GIS operators.

374
00:24:14,554 --> 00:24:18,462
So if you use a GIS operator here, it's going to check

375
00:24:18,516 --> 00:24:21,886
to see whether the field is of geometry type or not. It's also

376
00:24:21,908 --> 00:24:25,650
going to check to see if the database is postgis.

377
00:24:26,390 --> 00:24:29,634
So that level of validation is happening at the resource level.

378
00:24:29,672 --> 00:24:33,570
At the field level, it's also happening within

379
00:24:33,640 --> 00:24:37,570
the context. So when you're asking for fields back and ordering,

380
00:24:37,990 --> 00:24:40,838
it's going to make sure that the fields that you're ordering by exist on the

381
00:24:40,844 --> 00:24:43,478
object. It's going to make sure that the fields you're asking for exist on the

382
00:24:43,484 --> 00:24:47,590
object. All of that validation is happening at this layer. If it is invalid,

383
00:24:48,010 --> 00:24:51,786
a verbose message is sent back in the

384
00:24:51,808 --> 00:24:54,810
body of the response and you get a 400 level error.

385
00:24:55,470 --> 00:24:59,290
If that request is valid, it continues down this chain to the query builder.

386
00:24:59,790 --> 00:25:03,614
That normalized object in card three gets passed to a function and

387
00:25:03,652 --> 00:25:07,422
it builds the SQL query. That's what happens. And then the

388
00:25:07,476 --> 00:25:11,114
query gets executed against the database. That's actually optional.

389
00:25:11,162 --> 00:25:14,366
I'll mention that at the end there is a debug mode that allows

390
00:25:14,398 --> 00:25:17,682
you to make a call that runs the entire length of

391
00:25:17,736 --> 00:25:20,690
this chain and stops just short of execution.

392
00:25:23,350 --> 00:25:26,382
Documentation, feature,

393
00:25:26,446 --> 00:25:29,974
videos and requirements so we talked about

394
00:25:30,012 --> 00:25:33,734
a lot and we've only done so in a few minutes. I want to take

395
00:25:33,772 --> 00:25:37,014
another moment to mention that there's two and a half

396
00:25:37,052 --> 00:25:41,174
hours of YouTube video that's been up that goes over every feature

397
00:25:41,222 --> 00:25:44,954
and every function. All the features in the application, some of

398
00:25:44,992 --> 00:25:48,486
them, most of the features are explained

399
00:25:48,518 --> 00:25:51,690
in rest. And then there are custom videos

400
00:25:51,760 --> 00:25:55,486
for GraphQL and GRPC that show you how

401
00:25:55,508 --> 00:25:59,182
to implement those same features in the other interfaces. If you're new to

402
00:25:59,236 --> 00:26:03,134
service engine, I would certainly recommend that you run through all

403
00:26:03,172 --> 00:26:07,106
of the features from the rest point of view and then take

404
00:26:07,128 --> 00:26:10,834
a look at the other interfaces running

405
00:26:10,872 --> 00:26:16,318
down that list to make sure that I explain all the features. The quickstart that's

406
00:26:16,334 --> 00:26:19,238
basically what went over in the second slide, how to run the application in a

407
00:26:19,244 --> 00:26:22,886
docker container. The key rest points, I showed you

408
00:26:22,908 --> 00:26:26,418
a subset of those earlier. There are additional key rest endpoints,

409
00:26:26,514 --> 00:26:29,606
specifically like the results of the database survey. That's a

410
00:26:29,628 --> 00:26:33,402
key rest point. There's a video on importing an open

411
00:26:33,456 --> 00:26:36,794
API three document into insomnia. For anyone who hasn't done

412
00:26:36,832 --> 00:26:39,260
that, there's a video that shows you how to do that.

413
00:26:40,110 --> 00:26:43,866
The next feature videos on YouTube are crud operations.

414
00:26:43,978 --> 00:26:47,946
So it's the create, read, update and delete and showing

415
00:26:47,978 --> 00:26:51,870
you how to call the different URLs and how the validation works and the headers.

416
00:26:52,770 --> 00:26:55,986
The next one is SQL operators. So I haven't looked at that

417
00:26:56,008 --> 00:26:59,598
in some time, but I'm thinking it's probably a table search and showing

418
00:26:59,614 --> 00:27:03,140
you how to use all the various operators that are there.

419
00:27:03,830 --> 00:27:07,446
API response metadata. That video goes over

420
00:27:07,468 --> 00:27:11,350
the three different headers that are supported in the application

421
00:27:11,420 --> 00:27:14,934
that we went over. Debug mode. This is

422
00:27:14,972 --> 00:27:18,582
where moving up a couple of slides here.

423
00:27:18,636 --> 00:27:22,394
If you look at, we can look at card one or card

424
00:27:22,432 --> 00:27:26,554
three. Card three. So at the very end we're doing a

425
00:27:26,592 --> 00:27:31,770
read. It's a get request. This is a table search to serviceresource.

426
00:27:31,930 --> 00:27:35,470
If you were to change the literal service,

427
00:27:35,540 --> 00:27:39,006
if you were to actually change that to debug instead so you

428
00:27:39,028 --> 00:27:43,360
were calling debugresource, the application would function

429
00:27:44,690 --> 00:27:48,306
in all the ways that I described. It would just stop short of

430
00:27:48,328 --> 00:27:51,906
calling the database. So the query would get built and then you would

431
00:27:51,928 --> 00:27:55,220
get a verbose response back explaining what happened.

432
00:27:56,150 --> 00:27:59,334
This could be useful in testing. This could be useful for

433
00:27:59,372 --> 00:28:02,966
a number of other things. Like if

434
00:28:02,988 --> 00:28:06,274
you were going to use service engine to build sophisticated complex queries,

435
00:28:06,322 --> 00:28:09,706
maybe you could use it for that but not execute them. It could be

436
00:28:09,728 --> 00:28:13,258
useful there too. What's next?

437
00:28:13,344 --> 00:28:17,366
Permissions. The databases survey pulls internal

438
00:28:17,398 --> 00:28:20,490
tables and lists out all of your tables you use in Michelle's views.

439
00:28:20,990 --> 00:28:24,462
When it starts up, it provides by default crud access

440
00:28:24,516 --> 00:28:28,330
to all of the objects. There is a mechanism

441
00:28:28,410 --> 00:28:31,754
for passing in a configuration that allows you to set resource

442
00:28:31,802 --> 00:28:34,906
level controls over what create, readMe, update and delete

443
00:28:34,938 --> 00:28:38,530
functionality you want to support on any of the resources.

444
00:28:38,950 --> 00:28:42,034
And that video shows you how to do that. So imagine if you wanted to

445
00:28:42,072 --> 00:28:45,346
set an entire all the objects in

446
00:28:45,368 --> 00:28:48,758
the service to read only. But then there were a

447
00:28:48,764 --> 00:28:51,526
couple of tables where you wanted to set as create or read. You could do

448
00:28:51,548 --> 00:28:55,910
that using the permissions database schema migrations.

449
00:28:56,330 --> 00:28:59,594
As I said earlier, I implement the

450
00:28:59,632 --> 00:29:02,714
database schema migrations that are implemented in

451
00:29:02,832 --> 00:29:06,522
connects. I did build a very small custom

452
00:29:06,576 --> 00:29:10,380
module that works to only

453
00:29:11,090 --> 00:29:14,698
push the SQL files into their own directory. When I'm

454
00:29:14,714 --> 00:29:19,040
looking at SQL files, I like to see them as SQL as opposed to having

455
00:29:19,890 --> 00:29:23,722
SQL strings inside of JavaScript files.

456
00:29:23,786 --> 00:29:27,474
But if you watch that video, you'll see the custom work that I put in.

457
00:29:27,512 --> 00:29:30,020
It's very small, but I like it.

458
00:29:30,790 --> 00:29:34,466
GraphQL playground and geo subqueries. And then the next one is

459
00:29:34,488 --> 00:29:38,742
GRPC service and geo subqueries. Those two videos cover

460
00:29:38,796 --> 00:29:42,178
the other interfaces and they go over all of those features

461
00:29:42,274 --> 00:29:46,150
from above, complex resources,

462
00:29:46,650 --> 00:29:50,300
subqueries and aggregate queries. This application,

463
00:29:51,070 --> 00:29:54,794
it's powered by a database survey, so the table view

464
00:29:54,832 --> 00:29:58,714
and materialized view being queried has to exist. Now, a sub

465
00:29:58,752 --> 00:30:02,590
query and an aggregate query doesn't exist as a resource in the database,

466
00:30:03,170 --> 00:30:06,030
but it is built on objects that do exist.

467
00:30:06,370 --> 00:30:09,630
So this application does support

468
00:30:09,700 --> 00:30:13,534
those kind of subqueries. Right now you have to pass in

469
00:30:13,572 --> 00:30:17,042
a configuration. Basically you have to pre build how

470
00:30:17,096 --> 00:30:20,066
the aggregation is going to work, and that video shows you how to do that

471
00:30:20,088 --> 00:30:23,154
in detail. The last feature video there is

472
00:30:23,192 --> 00:30:26,706
middleware and operations redactions. I'll hit first because

473
00:30:26,728 --> 00:30:30,898
it's easier. Think maybe you want to support searching

474
00:30:30,914 --> 00:30:34,786
on a field, but you want to prevent it from being exposed

475
00:30:34,818 --> 00:30:37,878
to the user. Or maybe you just want to prevent it from being exposed to

476
00:30:37,884 --> 00:30:41,446
the user, not even facilitate it for search. So think.

477
00:30:41,468 --> 00:30:45,174
But like maybe you have implemented

478
00:30:45,222 --> 00:30:49,354
ids and keys and you hold those, has different fields, and maybe you want

479
00:30:49,392 --> 00:30:53,006
to publish the id but withhold the key.

480
00:30:53,188 --> 00:30:56,414
Maybe you're thinking about changing keys, maybe it's a partition key

481
00:30:56,452 --> 00:31:00,960
and you want to maintain the ability to

482
00:31:02,130 --> 00:31:07,234
swap those keys or to re optimize your databases without

483
00:31:07,272 --> 00:31:10,658
being concerned that people who have had access to that record in the past are

484
00:31:10,664 --> 00:31:12,450
going to try and query off of that material.

485
00:31:14,230 --> 00:31:17,826
Middleware. Middleware is what I mentioned on the first slide, which is

486
00:31:17,848 --> 00:31:21,366
the ability to intercept a query before it gets executed. So if

487
00:31:21,388 --> 00:31:24,454
we go back up one, if we're running down this stack and a call comes

488
00:31:24,492 --> 00:31:29,350
in and it gets normalized and validated between

489
00:31:29,420 --> 00:31:32,998
validation, but before the query gets built, the middleware

490
00:31:33,094 --> 00:31:36,442
is a function that will receive the object defined in card

491
00:31:36,496 --> 00:31:39,500
three and will be able to modify it. So the example,

492
00:31:40,190 --> 00:31:43,786
the example that I can think of is partition keys. Imagine if you have users

493
00:31:43,818 --> 00:31:47,402
by state, and maybe you have them segmented

494
00:31:47,466 --> 00:31:50,858
into, I don't know, NCAA,

495
00:31:51,034 --> 00:31:55,018
like the SEC. What are those conferences?

496
00:31:55,114 --> 00:31:58,722
Right? So maybe that's it. Maybe you're building an application for

497
00:31:58,776 --> 00:32:02,002
NCAA and you have users and you want to be like okay,

498
00:32:02,056 --> 00:32:05,154
this is the conference. So what you would do is you would take the state

499
00:32:05,192 --> 00:32:08,686
that they're in, or maybe the location

500
00:32:08,718 --> 00:32:10,420
from their IP address, you would drive,

501
00:32:12,250 --> 00:32:15,682
you would be able to do that lookup inside of the middleware and append

502
00:32:15,826 --> 00:32:19,238
the conference as a key to the object

503
00:32:19,324 --> 00:32:23,366
defined in line three using middleware. So whoever's

504
00:32:23,398 --> 00:32:26,566
calling your API would only be calling with something very normal.

505
00:32:26,678 --> 00:32:29,766
What state am I in? They would have no idea that you're doing a conference

506
00:32:29,798 --> 00:32:32,110
lookup in the back and you're pinning that to the query.

507
00:32:33,810 --> 00:32:37,178
Application considerations as I said in the beginning, if you're

508
00:32:37,194 --> 00:32:40,974
using a databases supported by Kinex, generally this

509
00:32:41,012 --> 00:32:43,840
should work. A couple of things to consider.

510
00:32:49,250 --> 00:32:53,134
A couple of things to consider. The first is GraphQL schema

511
00:32:53,182 --> 00:32:56,802
definition language. So the names of your databases,

512
00:32:56,946 --> 00:33:00,722
schemas, tables, views, materialized views, and your fields,

513
00:33:00,786 --> 00:33:04,002
the names matter. This application executes

514
00:33:04,066 --> 00:33:07,686
a database survey and it provisions everything upon that. And one

515
00:33:07,708 --> 00:33:10,902
of the things it provisions are the Graphql schema

516
00:33:10,966 --> 00:33:14,726
object and the GraphQL resolvers. But the schema

517
00:33:14,758 --> 00:33:18,278
definition language supports only a very narrow set of fields.

518
00:33:18,374 --> 00:33:22,110
I believe off the top of my head it is alphanumeric and underscores.

519
00:33:22,610 --> 00:33:25,646
That might not be all of it, but I think that's it. So if you

520
00:33:25,668 --> 00:33:30,000
attempt to run the application and your databases uses different names or

521
00:33:31,970 --> 00:33:35,218
white spaces or things like that, it might not work, but you should get a

522
00:33:35,224 --> 00:33:38,260
message right away indicating what the issue is.

523
00:33:38,870 --> 00:33:43,794
The next thing is database permissions when

524
00:33:43,832 --> 00:33:49,398
the application runs. By default, it runs migrations when

525
00:33:49,404 --> 00:33:53,014
you start the application. Let's go back a couple of slides. We're looking

526
00:33:53,052 --> 00:33:56,470
at the startup sequence. When you run the application by default

527
00:33:57,530 --> 00:34:01,602
on startup it will attempt to run database schema migrations.

528
00:34:01,666 --> 00:34:05,306
So it will check a directory on the disk in the containerized and

529
00:34:05,328 --> 00:34:08,458
it will attempt to run the schema migrations that it finds. Now when

530
00:34:08,464 --> 00:34:11,158
you're, but by default it's not going to find any. But that doesn't mean it's

531
00:34:11,174 --> 00:34:15,114
not going to run the migrations. And part of running the migrations

532
00:34:15,162 --> 00:34:18,846
for the first time, or every time is checking to see if a

533
00:34:18,868 --> 00:34:21,934
database schema migration table exists in the store.

534
00:34:22,052 --> 00:34:25,234
And if it doesn't exist it will attempt to build it. And I'm only going

535
00:34:25,272 --> 00:34:28,354
over that in such detail because if you are

536
00:34:28,392 --> 00:34:32,542
running it, imagine if your database services

537
00:34:32,606 --> 00:34:35,790
user has access to create, read, update and delete

538
00:34:35,870 --> 00:34:38,834
to and from objects, but you don't have the ability to create new objects,

539
00:34:38,882 --> 00:34:42,166
you're probably going to have a problem. So what you can do

540
00:34:42,188 --> 00:34:45,586
is there are methods for running the application and disabling databases

541
00:34:45,618 --> 00:34:49,558
schema migrations at startup and that's what I would recommend.

542
00:34:49,644 --> 00:34:52,842
And there's a video on that to the left that you can go find.

543
00:34:52,976 --> 00:34:56,986
Another reason why you might want to disable database schema migrations when

544
00:34:57,008 --> 00:35:00,318
the application starts is you might want to push your

545
00:35:00,324 --> 00:35:04,174
ddls into version control and set up a

546
00:35:04,212 --> 00:35:07,534
CI CD job that when merges to different

547
00:35:07,572 --> 00:35:10,766
branches occur, the CI CD job pulls down

548
00:35:10,788 --> 00:35:14,730
the changes and executes the migration

549
00:35:14,890 --> 00:35:18,610
only one time, as opposed to deploying a batch of

550
00:35:18,680 --> 00:35:22,066
twelve or 20 different containers in a cluster and

551
00:35:22,088 --> 00:35:26,270
having every single one of them start up and try to execute the same migrations.

552
00:35:26,350 --> 00:35:29,894
I don't think that would be a problem. It's just overhead. You don't have to

553
00:35:30,012 --> 00:35:33,702
deal with the last is returning fields. I did mention

554
00:35:33,756 --> 00:35:36,978
this earlier, not all of the database dialects support returning

555
00:35:36,994 --> 00:35:40,406
fields on mutations. So what I have listed here at the end

556
00:35:40,428 --> 00:35:44,154
is MySQL and SQL Lite. They don't. So if you were

557
00:35:44,272 --> 00:35:47,498
thinking back to our rest example, if you were to create a record or

558
00:35:47,504 --> 00:35:51,030
update a record, if you were using those databases,

559
00:35:51,110 --> 00:35:54,806
you're going to get a 201 with a no body, a 201 status

560
00:35:54,838 --> 00:35:58,398
code, and you're going to get a nobody. But if you're using postgres or one

561
00:35:58,404 --> 00:36:01,290
of the other databases that do support returning fields,

562
00:36:01,370 --> 00:36:05,060
you're actually going to get the fields that you asked for back. That's pretty cool.

563
00:36:06,390 --> 00:36:11,474
Closing notes last SQLite starting

564
00:36:11,512 --> 00:36:14,894
with most obvious. That's me. Hello, my name is Joe Wingard.

565
00:36:14,942 --> 00:36:18,562
There's my contact information, my GitHub, Keybase and LinkedIn.

566
00:36:18,706 --> 00:36:21,606
So if you have any notes about this talk, send me a note. I'd love

567
00:36:21,628 --> 00:36:25,590
to hear it. So project feedback and closing thoughts.

568
00:36:26,330 --> 00:36:29,542
We'll come back to that. First, I want to talk about the imagery here.

569
00:36:29,596 --> 00:36:32,506
So on the left and the right, on the left I have a stack of

570
00:36:32,528 --> 00:36:36,042
five different databases. I have them of

571
00:36:36,176 --> 00:36:39,638
several of the different database dialects supported by connects. They don't

572
00:36:39,654 --> 00:36:43,006
have to be that, they can just be different databases. But here I have

573
00:36:43,028 --> 00:36:46,446
postgres and MySQL, SQLite, SQL server and

574
00:36:46,468 --> 00:36:50,030
Oracle. And each one of those databases is,

575
00:36:50,180 --> 00:36:53,102
there is an instance or many instances,

576
00:36:53,166 --> 00:36:56,418
right? If you're scaling instances of service engine

577
00:36:56,504 --> 00:37:00,094
connecting to that databases, publishing, rest, GraphQL and GRPC

578
00:37:00,142 --> 00:37:04,082
services. On the right side of this slide I have

579
00:37:04,136 --> 00:37:07,566
diagrammed a reverse proxy or an ingress

580
00:37:07,678 --> 00:37:11,718
or yeah, it's a proxy that

581
00:37:11,804 --> 00:37:15,378
enables you to basically virtualize your data access layer.

582
00:37:15,554 --> 00:37:19,074
So at API domain.com that subdomain,

583
00:37:19,122 --> 00:37:22,374
you would be able to make any of the calls using any of the interfaces

584
00:37:22,422 --> 00:37:25,654
to any of the resources across any of the database engines.

585
00:37:25,782 --> 00:37:28,986
And whoever's calling your other engineers who are calling your

586
00:37:29,008 --> 00:37:32,366
data access layer would not have to really be concerned with what the

587
00:37:32,388 --> 00:37:35,520
database. They wouldn't even know what the database is,

588
00:37:36,050 --> 00:37:39,246
possibly. And this is

589
00:37:39,268 --> 00:37:42,842
how I would recommend deploying your data access layer.

590
00:37:42,986 --> 00:37:46,262
There's many reasons why I'm happy to get into if you send me a note,

591
00:37:46,346 --> 00:37:49,118
we can have discussion about it, but you get a lot of wins by deploying

592
00:37:49,134 --> 00:37:52,894
a system in this manner. Project feedback feedback

593
00:37:52,942 --> 00:37:56,820
and recommendations are best received with pull requests and GitHub issues.

594
00:37:57,910 --> 00:38:01,126
The feedback I care about hearing the most is from people who

595
00:38:01,148 --> 00:38:04,486
like to roll up their sleeves and dive in. So if you're one of those

596
00:38:04,508 --> 00:38:08,518
people and you have some feedback, open up a pull request or

597
00:38:08,684 --> 00:38:12,120
a GitHub issue and let me know what we can do to make this better.

598
00:38:12,570 --> 00:38:15,846
And the closing thought I hope this project is useful to

599
00:38:15,868 --> 00:38:19,106
you and your team. If you find it valuable,

600
00:38:19,298 --> 00:38:22,830
it helps you in your world. Please send me a note and

601
00:38:22,900 --> 00:38:26,522
that's it. So this is service engine. I just gave you an overview

602
00:38:26,586 --> 00:38:29,882
of. This is an NPM package. It's my first NPM package,

603
00:38:29,946 --> 00:38:33,438
NPM services engine, and it's available as a

604
00:38:33,444 --> 00:38:37,038
docker container and there's plenty of documentation. I think you can

605
00:38:37,044 --> 00:38:39,920
use it right now, today. So that's all.

606
00:38:40,290 --> 00:38:41,850
Thanks a lot for your time, and cheers.

