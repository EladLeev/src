1
00:00:00,330 --> 00:00:03,422
Hello and welcome to our talk,

2
00:00:03,476 --> 00:00:07,258
chaos in the cloud. I'm going to start by telling

3
00:00:07,274 --> 00:00:10,686
you a little bit about what we are going to do in the

4
00:00:10,708 --> 00:00:14,154
next 45 minutes. We are using to start with a recap

5
00:00:14,202 --> 00:00:17,694
on what is chaos engineering. Then we are going to talk about

6
00:00:17,732 --> 00:00:21,742
chaos engineering in the cloud. Taking an aside into

7
00:00:21,796 --> 00:00:25,670
what the cloud is and what its resiliency features are,

8
00:00:25,820 --> 00:00:29,186
we're going to talk about the AWS fault injection simulator,

9
00:00:29,298 --> 00:00:33,320
which is the AWS fully managed chaos engineering service.

10
00:00:33,690 --> 00:00:37,838
Then we are going to have a demonstration of how that looks in practice.

11
00:00:38,034 --> 00:00:41,434
Then we're going to finally share some best practices on

12
00:00:41,472 --> 00:00:45,766
working with the forge injection simulator and architecting

13
00:00:45,798 --> 00:00:48,300
for resiliency in general.

14
00:00:49,870 --> 00:00:53,600
What is chaos engineering and why do we do it?

15
00:00:53,970 --> 00:00:57,854
I like to start this talk with a quote from

16
00:00:57,892 --> 00:01:01,534
Werner Fogel. He's the CDO of Amazon.com,

17
00:01:01,652 --> 00:01:05,474
and he said, failures are a given, and everything will

18
00:01:05,512 --> 00:01:07,540
eventually fail over time.

19
00:01:08,630 --> 00:01:12,446
And that's so because with the rise of microservices and distributed

20
00:01:12,478 --> 00:01:16,386
architectures, the web and our applications have grown

21
00:01:16,418 --> 00:01:19,446
increasingly complex. And as a result of that,

22
00:01:19,628 --> 00:01:23,154
random failures, random failures have grown

23
00:01:23,202 --> 00:01:27,074
difficult to predict. At the same time, our dependence

24
00:01:27,122 --> 00:01:30,874
on those systems has only ever increased. And so as we

25
00:01:30,912 --> 00:01:35,046
moved away from monolithic architectures towards microservices

26
00:01:35,238 --> 00:01:38,634
to become more agile, build faster, and be able

27
00:01:38,672 --> 00:01:42,430
to scale, we naturally adds some complexity between

28
00:01:42,500 --> 00:01:43,950
those microservices.

29
00:01:46,130 --> 00:01:50,394
And chaos engineering is experimenting on those distributed

30
00:01:50,442 --> 00:01:53,874
systems to build confidence in those systems and make sure they can

31
00:01:53,912 --> 00:01:55,170
survive failure,

32
00:01:56,870 --> 00:02:00,062
and to get a better understanding of their failure

33
00:02:00,126 --> 00:02:06,034
conditions and of the performance envelopes. And that's all contained

34
00:02:06,082 --> 00:02:10,230
in the quote I have on the slide from principlesofchaos.org.

35
00:02:11,530 --> 00:02:14,530
So, diving a little bit deeper into chaos engineering,

36
00:02:14,690 --> 00:02:18,342
what is it? We generally talk about kind of a loop,

37
00:02:18,406 --> 00:02:22,102
stressing a system, observing how it reacts, and then improving

38
00:02:22,166 --> 00:02:26,346
the system. And we do that to improve resiliency and

39
00:02:26,368 --> 00:02:30,034
performance, uncover hidden issues in our architectures,

40
00:02:30,182 --> 00:02:33,914
and expose blind spots, for example, in monitoring observability

41
00:02:34,042 --> 00:02:37,742
and alarming. And usually we also

42
00:02:37,796 --> 00:02:41,198
achieve a degree of improvement in recovery times,

43
00:02:41,364 --> 00:02:45,294
improving our operational skills with the system and

44
00:02:45,332 --> 00:02:48,194
the culture in our technical. Sorry.

45
00:02:48,312 --> 00:02:50,770
And a culture in our tech.org.

46
00:02:53,110 --> 00:02:56,654
And I talked about that being a loop and going

47
00:02:56,712 --> 00:03:00,402
deeper into the loop. We see a loop of setting

48
00:03:00,546 --> 00:03:04,054
an objective. We want to achieve something with our system.

49
00:03:04,252 --> 00:03:08,460
We want to make it resilient against single ac failure, for example.

50
00:03:09,150 --> 00:03:13,094
We set that objective, then we design and implement

51
00:03:13,142 --> 00:03:16,618
our system. We design

52
00:03:16,704 --> 00:03:20,830
an experiment to test if our objective is actually

53
00:03:20,900 --> 00:03:24,206
achieved. We run the test we operate the system,

54
00:03:24,388 --> 00:03:28,426
and from the experiments and the antisystem operation

55
00:03:28,618 --> 00:03:32,486
we learn about the system. We can respond to failure conditions,

56
00:03:32,538 --> 00:03:36,274
improve the system and set new objectives and get

57
00:03:36,312 --> 00:03:37,540
better over time.

58
00:03:40,640 --> 00:03:44,284
So chaos engineering locally on your

59
00:03:44,322 --> 00:03:48,096
local machine, on your server, versus in the cloud I'm using to start

60
00:03:48,198 --> 00:03:51,740
by talking about chaos engineering in Java,

61
00:03:51,900 --> 00:03:55,696
and I just choose Java because it's one

62
00:03:55,718 --> 00:03:59,164
of the most widely used languages and it

63
00:03:59,222 --> 00:04:02,544
has awesome support for chaos testing, both via

64
00:04:02,592 --> 00:04:06,292
libraries and via tooling. What you generally see

65
00:04:06,346 --> 00:04:10,384
here is that a lot of those tests are either JVM

66
00:04:10,432 --> 00:04:13,220
based, they run inside the local JVM,

67
00:04:13,800 --> 00:04:17,396
they might be agent based, they run as a sidecar next to the JVM

68
00:04:17,428 --> 00:04:20,584
on the same machine, or they might be moved into

69
00:04:20,622 --> 00:04:24,476
the service mesh connecting many systems together. And that's really

70
00:04:24,498 --> 00:04:27,548
an awesome way to test your application code and to make

71
00:04:27,554 --> 00:04:31,880
sure it's resilient and to be able to control those experiments

72
00:04:31,960 --> 00:04:33,710
from your application code.

73
00:04:35,600 --> 00:04:39,120
And if that is so awesome and it already works quite well,

74
00:04:39,190 --> 00:04:42,304
it's a great improvement and everybody should do more of it.

75
00:04:42,422 --> 00:04:46,210
Why would we want to move chaos engineering into the cloud?

76
00:04:47,860 --> 00:04:51,350
We believe that using cloud services

77
00:04:51,720 --> 00:04:55,300
we gain a more holistic monitoring view

78
00:04:55,370 --> 00:04:59,488
of how our system works. We can identify

79
00:04:59,664 --> 00:05:04,120
cloud based challenges, for example around limits, around scaling,

80
00:05:04,940 --> 00:05:08,410
around interacting with other accounts. For example,

81
00:05:09,340 --> 00:05:13,320
we can run dedicated scaling experiments to learn

82
00:05:13,470 --> 00:05:17,436
where the failure points of our application is if we scale to

83
00:05:17,458 --> 00:05:20,670
a very large degree or very fast.

84
00:05:22,320 --> 00:05:26,324
And finally, we can do validation of disaster recovery strategies

85
00:05:26,392 --> 00:05:30,060
where we gain the ability to run chaos

86
00:05:30,140 --> 00:05:33,856
experiments on a system and see if it, for example,

87
00:05:33,958 --> 00:05:38,960
can be switched over to another region without

88
00:05:39,030 --> 00:05:42,240
an outtime or in a specified time frame.

89
00:05:44,280 --> 00:05:48,150
And this is the point where I'm going to take an insight into

90
00:05:49,080 --> 00:05:53,272
resiliency features of the cloud of AWS and

91
00:05:53,326 --> 00:05:57,096
how they interact with applications. What we see

92
00:05:57,118 --> 00:06:01,012
on the slide is the region design of AWS,

93
00:06:01,156 --> 00:06:05,320
and we see a region is made up of multiple availability zones

94
00:06:05,660 --> 00:06:08,300
here, abbreviated as AZ.

95
00:06:09,040 --> 00:06:12,236
And each AZ is made up of one or

96
00:06:12,258 --> 00:06:15,564
more data centers. And that's the only time I'm actually

97
00:06:15,602 --> 00:06:17,580
going to talk about data centers.

98
00:06:18,480 --> 00:06:22,676
The fundamental logical part of resilience

99
00:06:22,808 --> 00:06:26,316
in AZ you should think about is an availability zone,

100
00:06:26,508 --> 00:06:30,096
because those availability zones are independent

101
00:06:30,128 --> 00:06:33,988
from each other. And what that means is they have

102
00:06:34,074 --> 00:06:38,500
independent network providers, they have independent power connections,

103
00:06:39,000 --> 00:06:42,592
and if there is a geographic feature

104
00:06:42,736 --> 00:06:45,916
in the city where the region is located, the acs

105
00:06:45,968 --> 00:06:49,192
will be located while being

106
00:06:49,246 --> 00:06:52,488
mindful of that geographic feature. So for example, if there is a

107
00:06:52,494 --> 00:06:56,684
river flowing through the city not all of the AWS will be close to the

108
00:06:56,802 --> 00:07:00,632
river, so that we can be sure that in case of a flooding,

109
00:07:00,776 --> 00:07:03,964
not all of the AWS go down. And so as

110
00:07:04,002 --> 00:07:07,580
you distribute an application across AWS,

111
00:07:08,880 --> 00:07:12,980
it will become resilient against single AZ fiduciary

112
00:07:13,160 --> 00:07:16,800
and gain a degree of resilience.

113
00:07:17,540 --> 00:07:21,316
It will become resilient against single

114
00:07:21,418 --> 00:07:25,200
availability zone failure, even though those failures

115
00:07:25,280 --> 00:07:27,220
are quite unlikely in practice.

116
00:07:30,840 --> 00:07:34,330
What does it actually look like? How do we distribute an application

117
00:07:35,180 --> 00:07:37,720
across multiple availability zones?

118
00:07:38,540 --> 00:07:42,276
What we see here is a simplified architecture

119
00:07:42,308 --> 00:07:45,240
of a web application. We see load balancing,

120
00:07:45,400 --> 00:07:48,812
distributing traffic to different instances, and that means

121
00:07:48,866 --> 00:07:52,476
Ec two instances for us and those instances act as

122
00:07:52,498 --> 00:07:56,504
application servers. And then underneath

123
00:07:56,552 --> 00:08:00,588
that we see a primary and a standby DB instance,

124
00:08:00,764 --> 00:08:04,960
and we see that those are also distributed across availability zones.

125
00:08:05,460 --> 00:08:09,440
And so in the end, like the event that, for example, availability zone r

126
00:08:09,510 --> 00:08:13,124
goes down, the load balancer would distribute traffic to

127
00:08:13,162 --> 00:08:16,624
b and c, and there would be an automatic DB

128
00:08:16,672 --> 00:08:20,644
switch over to make the current standby instance in c,

129
00:08:20,682 --> 00:08:24,344
the primary instance, and your application would continue to

130
00:08:24,382 --> 00:08:24,970
work.

131
00:08:28,700 --> 00:08:32,344
Now that we have a certain understanding of resiliency in the cloud and

132
00:08:32,382 --> 00:08:35,944
why we would want to actually test it, let me go deeper

133
00:08:35,992 --> 00:08:39,976
into the AWS fault injection simulator and how it supports

134
00:08:40,008 --> 00:08:42,860
you in running chaos experiments.

135
00:08:44,480 --> 00:08:48,204
The AWS fault injection simulator is our fully

136
00:08:48,252 --> 00:08:51,776
managed chaos engineering service. It's an

137
00:08:51,798 --> 00:08:55,664
easy way to get started and it lets you reproduce real

138
00:08:55,702 --> 00:08:59,452
world failures, whether they are very simple like stopping an instance,

139
00:08:59,596 --> 00:09:02,240
or more complex like swapping APIs.

140
00:09:04,580 --> 00:09:08,196
And finally, this service fully embraces the

141
00:09:08,218 --> 00:09:12,184
idea of safeguards. So you can make sure that

142
00:09:12,222 --> 00:09:16,840
a running chaos experiments does not impact your productive deployment.

143
00:09:18,300 --> 00:09:21,912
I'm going to go into detail about all of those three

144
00:09:22,046 --> 00:09:25,544
features that I just mentioned. It's easy

145
00:09:25,582 --> 00:09:29,068
to get started because we spent actually a lot

146
00:09:29,074 --> 00:09:32,732
of time making it easy to get started, because when we talk to customers about

147
00:09:32,786 --> 00:09:36,140
chaos engineering, what customers repeatedly told us

148
00:09:36,210 --> 00:09:39,496
is that it is a little bit hard to get started with chaos engineering.

149
00:09:39,528 --> 00:09:42,050
And so we want to make that as easy as possible.

150
00:09:42,980 --> 00:09:46,592
You can use the console to get familiar with the service and actually

151
00:09:46,646 --> 00:09:50,572
try things out, and then you can use the CLI to take advantage

152
00:09:50,636 --> 00:09:54,280
of the templates and integrate the service with your CI

153
00:09:54,300 --> 00:09:57,776
CD pipelines. And those templates are JSon or YAML

154
00:09:57,808 --> 00:10:01,720
files that you can share with your team. You can version control them

155
00:10:01,790 --> 00:10:05,652
and use all of the benefits and best practices associated

156
00:10:05,796 --> 00:10:08,250
with version control, like code review.

157
00:10:10,220 --> 00:10:13,364
And you have conditions.

158
00:10:13,412 --> 00:10:17,084
You can run experiments in sequence, you can run experiments in

159
00:10:17,122 --> 00:10:20,492
parallel sequences. For example are used to test

160
00:10:20,546 --> 00:10:26,584
the impact of gradual degradation, like a sequence latency.

161
00:10:26,712 --> 00:10:30,764
And parallel experiments are to test the impact of multiple concurrent

162
00:10:30,812 --> 00:10:34,752
issues, which is actually how a lot of real world outages happen.

163
00:10:34,806 --> 00:10:37,676
You don't see outages because of a single failure,

164
00:10:37,788 --> 00:10:41,824
but because of a chaos of single failures leading up to

165
00:10:41,862 --> 00:10:45,268
a real world outage. It currently supports services

166
00:10:45,354 --> 00:10:48,660
like EC two RDS, ecs and eks.

167
00:10:49,000 --> 00:10:51,620
So virtual instances, databases,

168
00:10:53,100 --> 00:10:56,120
container runtimes and managed kubernetes.

169
00:10:56,940 --> 00:11:00,648
And we are working all the time to provide support

170
00:11:00,734 --> 00:11:04,296
for more service, for more services, sorry. And for

171
00:11:04,318 --> 00:11:06,120
more complex conditions.

172
00:11:09,500 --> 00:11:12,824
And just to hit the nail on the head here, those faults,

173
00:11:12,872 --> 00:11:16,360
they are really happening at the service control plane level.

174
00:11:16,530 --> 00:11:20,224
So an instance might actually be terminated, memory is actually being

175
00:11:20,262 --> 00:11:24,256
utilized, APIs are actually being throttled. It's not

176
00:11:24,278 --> 00:11:28,540
faking something with metric manipulation, but it's

177
00:11:28,620 --> 00:11:32,212
actually impacting how things work

178
00:11:32,346 --> 00:11:35,972
at the control plane level. So you will have to take extra

179
00:11:36,106 --> 00:11:39,030
or use extra caution when using the service.

180
00:11:39,640 --> 00:11:42,844
And to enable you to do that, we have safeguards.

181
00:11:42,912 --> 00:11:46,168
Safeguards act AWS, the automated stop button,

182
00:11:46,254 --> 00:11:50,264
a way to monitor the blast radius of your experiments and make

183
00:11:50,302 --> 00:11:54,212
sure that it's contained and that failures created with the experiment

184
00:11:54,356 --> 00:11:56,750
are rolled backed if alarms go off.

185
00:11:57,440 --> 00:12:01,036
And that's kind of runtime controls what happens during an

186
00:12:01,058 --> 00:12:04,760
experiment. And the service of course integrates

187
00:12:04,840 --> 00:12:08,284
with identity and access management. IAM and

188
00:12:08,322 --> 00:12:11,644
Im controls can be used to control what fault

189
00:12:11,692 --> 00:12:14,732
types can be used in an experiment and what resource

190
00:12:14,796 --> 00:12:18,700
can be affected. And that's of course working with tag based policies.

191
00:12:18,780 --> 00:12:22,088
So for example, only EC two instances with a tag

192
00:12:22,204 --> 00:12:25,636
environment equals test can be affected. That's one

193
00:12:25,658 --> 00:12:29,284
of many possible safeguards you can

194
00:12:29,322 --> 00:12:30,100
implement.

195
00:12:33,180 --> 00:12:36,280
What kind of targets and actions are supported.

196
00:12:37,020 --> 00:12:40,696
There's a host of targets and

197
00:12:40,718 --> 00:12:43,984
actions supported across the categories of compute, storage,

198
00:12:44,052 --> 00:12:48,428
networking databases and management service services.

199
00:12:48,514 --> 00:12:50,110
Sorry. And management services.

200
00:12:53,200 --> 00:12:57,116
And now we will dive somewhat deeper into the architecture

201
00:12:57,148 --> 00:13:00,864
of default injection service and how it

202
00:13:00,902 --> 00:13:04,412
interacts with the different components of the AWS

203
00:13:04,476 --> 00:13:08,304
cloud. And we see here a

204
00:13:08,342 --> 00:13:12,164
diagram of how the service works. At a high level we start

205
00:13:12,202 --> 00:13:15,796
with an experiment template which

206
00:13:15,818 --> 00:13:19,284
will comprise, sorry. At a high

207
00:13:19,322 --> 00:13:23,832
level we will start with an experiments template which

208
00:13:23,886 --> 00:13:27,576
contains different fault injection actions, targets that

209
00:13:27,598 --> 00:13:31,384
will be affected and safeguards to be run during the experiment. And you can

210
00:13:31,422 --> 00:13:34,990
see that here slightly to the left of center called

211
00:13:36,000 --> 00:13:38,190
experiment template and white.

212
00:13:40,960 --> 00:13:43,704
And then when we start an experiment,

213
00:13:43,832 --> 00:13:48,432
default injection simulator performs the actions. So it

214
00:13:48,486 --> 00:13:53,536
injects faults into supported resources that

215
00:13:53,558 --> 00:13:56,832
are specified as the targets. And then

216
00:13:56,886 --> 00:14:00,864
those faults interact with your resources

217
00:14:00,912 --> 00:14:04,096
and that will change what happens in monitoring,

218
00:14:04,128 --> 00:14:07,588
in monitoring in Amazon Cloudwatch, for example, or in

219
00:14:07,594 --> 00:14:11,584
your third party monitoring solution. And then you can take

220
00:14:11,642 --> 00:14:15,588
action based on those observability

221
00:14:15,684 --> 00:14:19,016
metrics you have there on those alarms you have there on those logs you

222
00:14:19,038 --> 00:14:22,424
have there by using Amazon eventbridge to,

223
00:14:22,462 --> 00:14:25,596
for example, stop an experiment if the wrong alarm is

224
00:14:25,618 --> 00:14:29,512
triggered, or to start a second experiment

225
00:14:29,576 --> 00:14:32,924
at that point in time. And so

226
00:14:32,962 --> 00:14:36,296
now we have an understanding of what chaos engineering

227
00:14:36,328 --> 00:14:40,224
is, why we want to do it in the cloud, and how

228
00:14:40,262 --> 00:14:43,104
the AWS fault injection simulator works.

229
00:14:43,222 --> 00:14:47,504
And it's the point where I'm going to hand over to bent to tell you

230
00:14:47,702 --> 00:14:51,044
about some exciting new scenarios we saw from

231
00:14:51,082 --> 00:14:54,596
reinvent, and to show you a demonstration of how

232
00:14:54,618 --> 00:14:57,300
the fault injection simulator works in practice.

233
00:14:59,080 --> 00:15:02,392
All right, thank you very much, Oliver. We're now

234
00:15:02,446 --> 00:15:06,116
going to take a closer look at those two new scenarios

235
00:15:06,148 --> 00:15:10,200
we launched at reinvent. So the first one is about

236
00:15:10,350 --> 00:15:14,004
cross region connectivity disruptions. So we have

237
00:15:14,062 --> 00:15:17,630
customers who have the requirement of operating their application

238
00:15:18,560 --> 00:15:22,364
at the highest possible availability rates. And those

239
00:15:22,402 --> 00:15:26,324
customers typically tend to architect their applications

240
00:15:26,392 --> 00:15:30,240
to span two regions. And those customers also

241
00:15:30,310 --> 00:15:33,552
asked us that we could maybe help them to make

242
00:15:33,606 --> 00:15:37,392
chaos testing even easier for them to test whether their

243
00:15:37,446 --> 00:15:41,216
applications can really withstand a connectivity disruption

244
00:15:41,248 --> 00:15:45,524
between two regions. So for instance, think of an

245
00:15:45,562 --> 00:15:49,584
active active or active passive kind of setup between two regions

246
00:15:49,712 --> 00:15:53,428
where all of a sudden your database, like a dynamodB,

247
00:15:53,524 --> 00:15:56,904
won't replicate new data. This is now possible

248
00:15:57,022 --> 00:16:00,932
with the new cross region connectivity

249
00:16:00,996 --> 00:16:05,100
disruption scenario that is available with the fault injection simulator.

250
00:16:06,560 --> 00:16:10,328
Another new scenario that we also launch is around availability

251
00:16:10,424 --> 00:16:13,436
zone power interruptions. So we

252
00:16:13,458 --> 00:16:17,180
already mentioned that those kind of scenarios have a really low

253
00:16:17,250 --> 00:16:20,672
likelihood of happening. However, there are customers who still

254
00:16:20,726 --> 00:16:24,864
want to experience how their application would

255
00:16:24,902 --> 00:16:28,252
behave in such an event. So in the event of a power interruption,

256
00:16:28,316 --> 00:16:31,904
you would see, for instance, scenarios like EC, two instances

257
00:16:31,952 --> 00:16:35,924
or containers stopping out of nowhere. And with

258
00:16:35,962 --> 00:16:39,172
the new scenario of ac availability power

259
00:16:39,226 --> 00:16:42,536
disruptions, you will basically be able to test how your application

260
00:16:42,638 --> 00:16:44,760
behaves under those kind of events.

261
00:16:46,780 --> 00:16:50,568
But now we basically want to have a look at

262
00:16:50,654 --> 00:16:53,876
our own demo. So we will take you through

263
00:16:53,918 --> 00:16:57,310
a journey of testing an application

264
00:16:57,680 --> 00:17:00,812
and improving its resiliency when it comes

265
00:17:00,866 --> 00:17:04,156
to availability zones failures. So what

266
00:17:04,178 --> 00:17:07,528
we brought for today here is a simple workload

267
00:17:07,624 --> 00:17:11,052
that is currently running in a single container.

268
00:17:11,196 --> 00:17:14,704
So we have a container that is basically a simple API that

269
00:17:14,742 --> 00:17:18,432
responds with a pong to every request to send to the

270
00:17:18,486 --> 00:17:21,860
container. And this is currently running on eks.

271
00:17:22,680 --> 00:17:26,372
It's a single pod hosted on a single node that we have in our

272
00:17:26,426 --> 00:17:29,552
cluster which is currently running in the availability zone.

273
00:17:29,616 --> 00:17:33,220
A and I personally am not the most Kubernetes

274
00:17:33,300 --> 00:17:37,252
experienced guy, so I'm not really sure how Kubernetes

275
00:17:37,316 --> 00:17:40,856
behaves in the case of

276
00:17:40,878 --> 00:17:44,840
an availability zone disruption. And I personally

277
00:17:45,420 --> 00:17:48,584
want to have a really low cost with my application.

278
00:17:48,702 --> 00:17:52,744
So I would just want to check whether running a single pod

279
00:17:52,872 --> 00:17:56,524
is enough to tolerate the failure of an AC or

280
00:17:56,562 --> 00:18:01,436
if for instance, Kubernetes will automatically for me schedule

281
00:18:01,468 --> 00:18:05,440
this pod on a new node that is hosted in a different availability zone.

282
00:18:06,260 --> 00:18:10,070
But before we jump into the console, I already want to show you

283
00:18:11,080 --> 00:18:14,640
the result of the experiment.

284
00:18:14,800 --> 00:18:19,024
So of course this is not going to happen. We won't see an automatic pod

285
00:18:19,072 --> 00:18:23,480
reassignment to a different node hosted in a different availability zone.

286
00:18:24,140 --> 00:18:27,576
This is not how it actually works. So instead what

287
00:18:27,598 --> 00:18:31,092
we would want to do here to make the system more resilient

288
00:18:31,156 --> 00:18:35,292
is basically by updating our deployment to

289
00:18:35,426 --> 00:18:39,532
at least run in two different

290
00:18:39,666 --> 00:18:43,116
availability zones. And this is actually what we are going to

291
00:18:43,138 --> 00:18:46,476
take a look at now in the demo. Now it's time to look

292
00:18:46,498 --> 00:18:49,856
at the fault injection simulator in action. For this we are

293
00:18:49,878 --> 00:18:53,116
going to first take a look at the application that we deployed

294
00:18:53,148 --> 00:18:56,928
in Kubernetes. Then we are going to set up a load testing tool

295
00:18:57,014 --> 00:19:00,836
to send frequent requests against our application to

296
00:19:00,858 --> 00:19:04,516
inspect some metrics like the availability and also

297
00:19:04,538 --> 00:19:07,220
the response time of our request.

298
00:19:08,280 --> 00:19:12,230
Then we are setting up the fizz to

299
00:19:12,600 --> 00:19:16,724
introduce some chaos in our application. And then we are revisiting

300
00:19:16,772 --> 00:19:20,788
our load testing tool to see the impact of our chaos experiment.

301
00:19:20,964 --> 00:19:24,920
So let's start with taking a look at the Kubernetes manifests

302
00:19:27,280 --> 00:19:30,328
so we can quickly see here with our deployment.

303
00:19:30,424 --> 00:19:34,712
This is basically the Kubernetes resource that you need to deploy

304
00:19:34,776 --> 00:19:38,368
containers in your cluster. We can see here that we have one

305
00:19:38,454 --> 00:19:41,440
container that we are deploying on our cluster.

306
00:19:42,180 --> 00:19:46,210
This is basically pulling the container image from my application

307
00:19:46,900 --> 00:19:50,390
from a elastic container registry of my account.

308
00:19:51,640 --> 00:19:55,796
Then I'm also using a node selector here

309
00:19:55,898 --> 00:20:00,468
to ensure that this container will basically be

310
00:20:00,554 --> 00:20:04,712
scheduled on a node that is running in the US east one a

311
00:20:04,766 --> 00:20:09,144
availability zone. Besides our deployment, we also have a

312
00:20:09,262 --> 00:20:13,268
service and an ingress resource. Those are required

313
00:20:13,364 --> 00:20:17,436
to expose our deployment in the Internet and we need

314
00:20:17,458 --> 00:20:21,164
that in order to run our load testing here for this

315
00:20:21,202 --> 00:20:25,240
use case. One thing to highlight here is the ingress section.

316
00:20:25,320 --> 00:20:29,788
So we are using the AWS load balancer controller

317
00:20:29,964 --> 00:20:33,424
to create an application cloud balancer in the cloud from

318
00:20:33,462 --> 00:20:37,216
this ingress resource that we just deployed. So both the

319
00:20:37,238 --> 00:20:40,416
ingress and the deployment Yaml files are already deployed.

320
00:20:40,448 --> 00:20:42,980
So let's have a look if that was successful.

321
00:20:43,560 --> 00:20:47,472
So I'm jumping in the console now I'm

322
00:20:47,616 --> 00:20:50,340
running kubectl get pod.

323
00:20:51,880 --> 00:20:55,256
This should now return one pod. This looks good.

324
00:20:55,438 --> 00:20:58,952
And now we want to basically double check whether this

325
00:20:59,006 --> 00:21:02,820
node here is really running in the correct availability

326
00:21:02,900 --> 00:21:06,350
zone. And this looks good. So the node is

327
00:21:06,800 --> 00:21:10,844
running in the US east one a availability zone. So that

328
00:21:10,882 --> 00:21:14,476
basically means that the container is running in the availability zone that

329
00:21:14,498 --> 00:21:17,810
we want to run a chaos experiment on.

330
00:21:18,260 --> 00:21:22,896
Now let's get the URL of

331
00:21:22,918 --> 00:21:26,236
the ingress controller or to be more precise

332
00:21:26,268 --> 00:21:30,448
of the cloud balancer. So let's say Kubectl get ingresses.

333
00:21:30,544 --> 00:21:34,096
That looks good. And here's

334
00:21:34,128 --> 00:21:38,230
the address. Let's send a curl request here

335
00:21:39,960 --> 00:21:43,832
and we are getting a response pong this looks good. So now

336
00:21:43,886 --> 00:21:47,624
let's copy this URL and let's set

337
00:21:47,662 --> 00:21:50,712
up the load testing tool. So for this

338
00:21:50,766 --> 00:21:53,124
case I'm using locost.

339
00:21:53,252 --> 00:21:56,844
Locost is basically running on my local machine and will send

340
00:21:56,962 --> 00:22:00,270
a good amount of requests per second to my application.

341
00:22:00,640 --> 00:22:04,604
Let's start this here on

342
00:22:04,642 --> 00:22:08,520
the top right corner we can see the requests per second that are sent against

343
00:22:08,690 --> 00:22:12,220
our application. And here we can see the failures.

344
00:22:12,300 --> 00:22:16,480
So failures would indicate a status code 400 ish.

345
00:22:16,820 --> 00:22:20,016
Now we can take a look at the chart and we can see here that

346
00:22:20,198 --> 00:22:23,684
we slowly ramped up on requests per second. And now we

347
00:22:23,722 --> 00:22:27,248
are around sending around 240 requests

348
00:22:27,264 --> 00:22:31,212
per second. And this looks good. We have no big failures.

349
00:22:31,376 --> 00:22:34,900
Our response time is quite static

350
00:22:34,980 --> 00:22:38,424
and with 120 milliseconds this

351
00:22:38,462 --> 00:22:42,344
looks good. So I would say this is a

352
00:22:42,382 --> 00:22:45,784
successful deployment of our application. So now let's

353
00:22:45,832 --> 00:22:49,932
check how the resiliency of our application really looks like.

354
00:22:49,986 --> 00:22:53,560
And let's see the impact of a chaos experiment.

355
00:22:53,640 --> 00:22:57,472
For this I'm going to the AWS console and here

356
00:22:57,526 --> 00:23:02,940
I'm basically searching for the service called AWS AWS

357
00:23:03,020 --> 00:23:07,120
fizz. I'm going to open

358
00:23:07,190 --> 00:23:10,996
up this one here. I'm basically

359
00:23:11,098 --> 00:23:15,316
taking a look at the experiments templates here

360
00:23:15,338 --> 00:23:19,024
we can see one template that I already created. This is called disrupt

361
00:23:19,072 --> 00:23:22,816
Aza and this is doing exactly that. Let's have a look at it.

362
00:23:22,938 --> 00:23:26,344
So the name is disrupt Aza and this is

363
00:23:26,382 --> 00:23:30,168
exactly what happens here. Let's quickly take a

364
00:23:30,174 --> 00:23:33,464
look at the update wizard. I think this is really good

365
00:23:33,502 --> 00:23:37,596
for the visualization what's happening here. So we can see that

366
00:23:37,618 --> 00:23:41,484
this template is quite small. So we have one action which

367
00:23:41,522 --> 00:23:45,036
is called disrupt Aza let's have a look at

368
00:23:45,058 --> 00:23:48,290
it first. So here we basically

369
00:23:48,660 --> 00:23:52,256
specify the action type.

370
00:23:52,358 --> 00:23:55,932
This is disrupt connectivity. Disrupt connectivity

371
00:23:56,076 --> 00:24:00,448
will prevent packets from leaving

372
00:24:00,544 --> 00:24:04,020
and entering the given subnet.

373
00:24:05,080 --> 00:24:08,276
And down here we can see the duration. In this

374
00:24:08,298 --> 00:24:11,904
case we have two minutes configured and we have configured

375
00:24:11,952 --> 00:24:15,336
the target. So the target are the subnets that are impacted by

376
00:24:15,358 --> 00:24:19,444
this event. Now let's have a look at the targets. So I'm quickly opening

377
00:24:19,492 --> 00:24:23,824
up the target here we can see the subnet

378
00:24:23,892 --> 00:24:27,384
target one. We are using a resource

379
00:24:27,432 --> 00:24:31,336
filter here to ensure that not every single subnet

380
00:24:31,368 --> 00:24:34,990
is targeted but instead we are only taking

381
00:24:35,920 --> 00:24:39,152
subnets in the US east one a

382
00:24:39,206 --> 00:24:42,508
availability zone in scope of this experiment.

383
00:24:42,684 --> 00:24:46,252
So a subnet in AWS is a zonal resource.

384
00:24:46,316 --> 00:24:50,320
So that means that a subnet is deployed in one availability

385
00:24:50,400 --> 00:24:54,004
zone. And this configuration really just makes

386
00:24:54,042 --> 00:24:57,472
sure that no network traffic is leaving or entering

387
00:24:57,536 --> 00:25:00,804
every single subnet in the availability zone. Us east one

388
00:25:00,842 --> 00:25:04,264
a. So I would say let's go back here and start this

389
00:25:04,302 --> 00:25:07,496
experiments to take a look at what's going

390
00:25:07,518 --> 00:25:11,130
on. So I'm clicking start here

391
00:25:13,100 --> 00:25:17,000
and it takes some time until the experiment

392
00:25:17,160 --> 00:25:21,292
is actually executed so you can take a look at the timeline to

393
00:25:21,346 --> 00:25:25,180
see what's going on. So currently this one is pending.

394
00:25:25,600 --> 00:25:29,484
This was just taking a couple of seconds to deploy

395
00:25:29,532 --> 00:25:33,116
the disruption. So let's wait. There we go. Now it's

396
00:25:33,148 --> 00:25:36,556
actually running. So if we go back to the locals

397
00:25:36,588 --> 00:25:40,004
page. Yeah there we go. We should see a drop in

398
00:25:40,042 --> 00:25:44,324
availability. So you can see that

399
00:25:44,442 --> 00:25:48,720
straight away we have a really reduced

400
00:25:48,880 --> 00:25:52,456
amount of requests per seconds and now we can see

401
00:25:52,478 --> 00:25:56,408
that there's no request being

402
00:25:56,494 --> 00:26:00,568
successful anymore. So that's basically

403
00:26:00,654 --> 00:26:04,588
showing us that there's not a single request going through. What's interesting,

404
00:26:04,674 --> 00:26:08,284
if you can go back here we can see that those requests right

405
00:26:08,322 --> 00:26:13,420
now are somehow dangling. So there is no bad

406
00:26:13,490 --> 00:26:18,076
response code like a HTTP 400 ish

407
00:26:18,188 --> 00:26:21,884
but the connection is just not completely opened and cloud.

408
00:26:21,932 --> 00:26:25,964
So this is really the impact of our availability zone

409
00:26:26,012 --> 00:26:29,684
outage here. So let's wait

410
00:26:29,882 --> 00:26:33,056
for some time until this experiment

411
00:26:33,248 --> 00:26:34,660
is finished.

412
00:26:38,460 --> 00:26:41,864
Here we go. The experiment is now

413
00:26:41,902 --> 00:26:45,096
finished. Let's go back to the console and we can see straight

414
00:26:45,128 --> 00:26:49,500
away once the experiment was finished

415
00:26:49,840 --> 00:26:53,436
our network is available again and our application

416
00:26:53,618 --> 00:26:57,376
continues to serve traffic. So here we can

417
00:26:57,398 --> 00:27:00,528
see that our current architecture with

418
00:27:00,694 --> 00:27:04,144
one container being deployed in one availability zone is not

419
00:27:04,182 --> 00:27:07,596
really able to handle the situation well. So let's

420
00:27:07,628 --> 00:27:11,590
see how we can improve the availability. I will first

421
00:27:13,320 --> 00:27:17,012
stop the load test here and we are now jumping back

422
00:27:17,066 --> 00:27:20,956
into the editor, because I already prepared

423
00:27:21,008 --> 00:27:24,612
an updated deployment. So here we have the updated

424
00:27:24,676 --> 00:27:28,264
deployment. It's pretty similar to the previous one,

425
00:27:28,382 --> 00:27:31,972
besides two major updates.

426
00:27:32,036 --> 00:27:35,596
So first of all, we have two replicas. So this basically means

427
00:27:35,618 --> 00:27:39,244
that we deploy two containers. And then what's even more important is

428
00:27:39,282 --> 00:27:42,716
we have an affinity rule created. So this affinity rule will

429
00:27:42,738 --> 00:27:46,512
basically ensure that those containers will

430
00:27:46,566 --> 00:27:50,988
be spread across the nodes and specifically

431
00:27:51,084 --> 00:27:54,012
spread across the different availability zones.

432
00:27:54,156 --> 00:27:56,640
So now let's go back to the terminal.

433
00:27:58,280 --> 00:28:02,420
And now let's delete the old deployment.

434
00:28:05,400 --> 00:28:10,720
And now

435
00:28:10,810 --> 00:28:14,260
let's apply the updated

436
00:28:14,340 --> 00:28:17,992
deployment. So this will

437
00:28:18,046 --> 00:28:22,600
take just a couple of seconds to deploy our pods.

438
00:28:23,840 --> 00:28:27,950
Let's have a look at them. There they are

439
00:28:28,320 --> 00:28:33,132
running on two different nodes. And if we now again open

440
00:28:33,186 --> 00:28:37,250
up the nodes here, we will see that

441
00:28:38,180 --> 00:28:41,712
those two nodes are actually running in two different

442
00:28:41,766 --> 00:28:45,120
availability zones. So now we can test

443
00:28:45,190 --> 00:28:48,932
again. So let's go back to the cloud testing tool. Let's do a new

444
00:28:48,986 --> 00:28:52,324
test. Let's configure the same amount of

445
00:28:52,362 --> 00:28:55,990
users being simulated and run the load test.

446
00:28:57,240 --> 00:29:00,984
Give it some seconds here to create some

447
00:29:01,022 --> 00:29:06,376
stable requests per second. So here

448
00:29:06,398 --> 00:29:09,210
we go. This looks good.

449
00:29:09,740 --> 00:29:13,900
Now let's go back into the AWS console to

450
00:29:13,970 --> 00:29:18,332
rerun the experiments. So we

451
00:29:18,386 --> 00:29:20,140
are going to experiment,

452
00:29:21,760 --> 00:29:25,036
going to the templates, opening up the

453
00:29:25,058 --> 00:29:28,940
template. We're now starting another experiment

454
00:29:29,020 --> 00:29:31,250
here. Let's start this one.

455
00:29:34,100 --> 00:29:38,140
So this again

456
00:29:38,230 --> 00:29:40,660
just takes some time to initiate.

457
00:29:45,720 --> 00:29:49,190
Still pending. Let's see. Now it's running.

458
00:29:49,880 --> 00:29:53,224
So let's also wait for some time

459
00:29:53,262 --> 00:29:55,370
here to check what is happening.

460
00:30:08,020 --> 00:30:10,864
So we can see quite a different result now.

461
00:30:10,982 --> 00:30:14,950
So what we see here is a very, very short

462
00:30:15,640 --> 00:30:19,030
time period where our service wasn't available.

463
00:30:19,400 --> 00:30:23,216
And this is basically because of the load balancer

464
00:30:23,248 --> 00:30:27,108
health check that checks the availability

465
00:30:27,204 --> 00:30:30,564
of its targets every 5 seconds.

466
00:30:30,692 --> 00:30:34,468
And this is just the small time gap where the load

467
00:30:34,484 --> 00:30:38,170
balancer thought that the container is available.

468
00:30:38,620 --> 00:30:41,900
And then after the next evaluation figured out,

469
00:30:41,970 --> 00:30:45,304
no, I cannot send any further request to the target.

470
00:30:45,432 --> 00:30:48,812
And as we can see here with this update, our application

471
00:30:48,946 --> 00:30:52,160
now reacted way better to

472
00:30:52,230 --> 00:30:55,410
the outage. And this would be a.

473
00:30:56,020 --> 00:30:59,616
Yeah, would basically show you the full lifecycle of running an

474
00:30:59,638 --> 00:31:03,124
experiment. So we made an assumption, figured out

475
00:31:03,162 --> 00:31:08,276
that this is not correct, updated our application to

476
00:31:08,298 --> 00:31:11,652
see an improvement in the resiliency. And this

477
00:31:11,706 --> 00:31:15,430
basically here concludes the demo of today.

478
00:31:15,900 --> 00:31:19,656
I now also want to share some best practices to

479
00:31:19,678 --> 00:31:23,348
get started with chaos engineering for your own applications.

480
00:31:23,524 --> 00:31:27,096
So I would recommend you to start with very

481
00:31:27,198 --> 00:31:30,140
small templates in the beginning that maybe only,

482
00:31:30,210 --> 00:31:33,352
as in our demo, only include one single action,

483
00:31:33,416 --> 00:31:37,004
because this allows you to very quickly understand the impact of

484
00:31:37,042 --> 00:31:39,260
a certain template action.

485
00:31:39,920 --> 00:31:43,584
The second tip that I have for you here is testing close

486
00:31:43,622 --> 00:31:46,812
to your production environment. So let's say you have a containerized

487
00:31:46,876 --> 00:31:50,924
workload that is in your staging environment, running on Docker compose,

488
00:31:50,972 --> 00:31:55,344
for instance, and on your production environment, those containers

489
00:31:55,392 --> 00:31:58,020
run on a fully fledged eks cluster.

490
00:31:58,680 --> 00:32:02,564
Here I would recommend you to basically maybe add

491
00:32:02,602 --> 00:32:06,836
a new test environment that is in the architecture more closer

492
00:32:06,868 --> 00:32:10,824
to what you have in production, because else you won't be able to

493
00:32:10,862 --> 00:32:15,284
basically catch flaws in your staging

494
00:32:15,412 --> 00:32:18,584
application architecture that you can basically apply in

495
00:32:18,622 --> 00:32:21,230
production to increase the resiliency of your application.

496
00:32:21,600 --> 00:32:24,828
So always try to test as close as possible

497
00:32:24,914 --> 00:32:27,740
to production as possible, maybe even in production.

498
00:32:28,800 --> 00:32:33,024
The next one is about minimizing the blast radios. So we

499
00:32:33,062 --> 00:32:37,650
mentioned that with the fault injection simulator on AWS, it's possible basically

500
00:32:38,180 --> 00:32:42,016
to minimize the blast radios to two ways. So the first one being

501
00:32:42,118 --> 00:32:45,376
with limiting the access that the service has to your resources.

502
00:32:45,488 --> 00:32:49,204
So for instance, with principle of least privilege in

503
00:32:49,242 --> 00:32:53,236
your IAM policies, you can basically limit the access to the

504
00:32:53,258 --> 00:32:56,376
resources that the fault injection simulator has by for

505
00:32:56,398 --> 00:32:59,896
instance, making sure that only your application servers and your

506
00:32:59,918 --> 00:33:03,764
databases of the staging environment are accessible

507
00:33:03,812 --> 00:33:07,416
by the service. And also we

508
00:33:07,438 --> 00:33:11,304
would recommend you to basically use the health check capabilities,

509
00:33:11,432 --> 00:33:15,212
the emergency brake stops to stop an

510
00:33:15,266 --> 00:33:19,272
experiment when you see that you really have degraded

511
00:33:19,416 --> 00:33:23,040
health in your application when you, for instance, test in production.

512
00:33:24,820 --> 00:33:27,968
To get started with collecting your first hands on experience,

513
00:33:28,054 --> 00:33:31,360
I can recommend this workshop that we have for you here.

514
00:33:31,510 --> 00:33:35,204
With this workshop, you basically have a guided step

515
00:33:35,242 --> 00:33:38,644
by step experience where you basically will

516
00:33:38,762 --> 00:33:42,656
learn and understand those different functionalities of default injection

517
00:33:42,768 --> 00:33:46,420
service firsthand. And I would recommend you to check it out

518
00:33:46,490 --> 00:33:49,976
either by scanning the QR code or visiting the URL on

519
00:33:49,998 --> 00:33:53,956
the screen. And this concludes the session. So you see another QR

520
00:33:53,988 --> 00:33:57,080
code on the screen. This is really important.

521
00:33:57,230 --> 00:34:00,372
So if you scan this QR code or visit the URL,

522
00:34:00,436 --> 00:34:03,624
you can give us feedback. And we really, really need your feedback.

523
00:34:03,672 --> 00:34:07,676
We want to understand if you like the session and what

524
00:34:07,698 --> 00:34:11,276
we could improve next time. So please take a minute and

525
00:34:11,378 --> 00:34:15,084
fill out the form. It would really mean a lot to us

526
00:34:15,202 --> 00:34:18,940
and we thank you really much and we wish you a great day ahead

527
00:34:19,090 --> 00:34:22,476
and also fun with all the other interesting sessions that you have the

528
00:34:22,498 --> 00:34:24,610
chance to explore today. Thank you very much.

