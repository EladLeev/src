1
00:00:26,370 --> 00:00:29,970
Hi, my name's Lorna, I'm a developer advocate at Ivan.

2
00:00:30,130 --> 00:00:34,018
We do interesting open source data platforms

3
00:00:34,114 --> 00:00:37,254
in the cloud. The plan for today's talk

4
00:00:37,372 --> 00:00:40,614
is I'm going to introduce Kafka to you. I'm going to talk

5
00:00:40,652 --> 00:00:44,546
about how you could use Kafka from your go applications.

6
00:00:44,658 --> 00:00:48,566
We'll talk about data formats and schemas, and then I'll also,

7
00:00:48,668 --> 00:00:52,014
just as, as a final teaser, introduce you

8
00:00:52,052 --> 00:00:54,590
to async API.

9
00:00:55,170 --> 00:00:58,798
So let's get started from the

10
00:00:58,804 --> 00:01:02,926
website Apache. Kafka is an opensource distributed

11
00:01:02,958 --> 00:01:04,420
event streaming platform.

12
00:01:05,750 --> 00:01:09,618
It's a massively scalable pub sub

13
00:01:09,704 --> 00:01:13,650
mechanism, and that's typically the type of applications that

14
00:01:13,720 --> 00:01:18,150
use Kafka. It's designed for data streaming.

15
00:01:18,490 --> 00:01:22,182
I see it very widely used in real

16
00:01:22,236 --> 00:01:25,602
time or near real time data applications,

17
00:01:25,666 --> 00:01:29,206
for the finance sector, for manufacturing,

18
00:01:29,318 --> 00:01:32,586
for Internet of things as well, things where the

19
00:01:32,608 --> 00:01:36,010
data needs to be on time. Kafka is super

20
00:01:36,080 --> 00:01:39,978
scalable, so it's designed to handle large volumes of data,

21
00:01:40,144 --> 00:01:43,194
and perhaps this should say a very large volume

22
00:01:43,242 --> 00:01:46,382
of small data sets. The one

23
00:01:46,436 --> 00:01:50,458
thing which I think can be surprising about Kafka is that typically

24
00:01:50,634 --> 00:01:54,510
the maximum payload size for a message will be one meg,

25
00:01:54,670 --> 00:01:58,146
which is actually quite a lot of data. If you're sending files, you're doing it

26
00:01:58,168 --> 00:02:01,362
wrong. But in terms of transmitting data from one place

27
00:02:01,416 --> 00:02:05,266
to another, then it's pretty

28
00:02:05,288 --> 00:02:09,154
good. And we'll talk about data formats as well. Kafka is widely

29
00:02:09,202 --> 00:02:13,202
used in event sourcing applications, in data processing applications,

30
00:02:13,266 --> 00:02:16,790
and you'll see it in your observability tools as well,

31
00:02:16,940 --> 00:02:20,842
where it's often used for moving

32
00:02:20,896 --> 00:02:24,234
metrics, data around, and for log shipping as well.

33
00:02:24,352 --> 00:02:26,986
So now I've said all that, right,

34
00:02:27,168 --> 00:02:30,574
about how modern it is, how well it

35
00:02:30,612 --> 00:02:33,278
scales. Like modern scalable fast.

36
00:02:33,444 --> 00:02:35,360
Remind you of anything?

37
00:02:36,290 --> 00:02:40,106
Right? So go is a really good fit with Kafka.

38
00:02:40,298 --> 00:02:44,302
I see them used together pretty often, especially in

39
00:02:44,436 --> 00:02:48,386
the more modern event driven systems. I think it's a really good fit.

40
00:02:48,568 --> 00:02:52,270
So a quick theory primer or refresher

41
00:02:52,350 --> 00:02:55,974
depending on your context for Kafka. What do we know about

42
00:02:56,012 --> 00:02:59,990
Kafka? We know it isn't a database and

43
00:03:00,060 --> 00:03:03,334
it isn't a queue, although it

44
00:03:03,372 --> 00:03:06,760
can look a bit like both of those concepts. I think

45
00:03:07,130 --> 00:03:10,602
it's a log. It's a distributed log for data.

46
00:03:10,656 --> 00:03:14,054
And we know about logs. We know that we append

47
00:03:14,182 --> 00:03:18,410
data to logs and that the data that we write there

48
00:03:18,560 --> 00:03:22,106
is then immutable. So we

49
00:03:22,128 --> 00:03:25,466
write it once, we never go back and update a line in a log file.

50
00:03:25,658 --> 00:03:28,986
The storage is a topic, and perhaps

51
00:03:29,018 --> 00:03:32,062
in a database this would be a table, or in a queue, it would be

52
00:03:32,116 --> 00:03:35,598
a channel. The topic is what you write to as a producer.

53
00:03:35,694 --> 00:03:39,726
The producer is what sends the data. It's the publisher,

54
00:03:39,758 --> 00:03:43,826
if you like. And then the consumer labeled here as c one

55
00:03:44,008 --> 00:03:47,926
is the subscriber. It's what receives that data.

56
00:03:48,108 --> 00:03:51,446
I've talked about topics, but I've oversimplified it

57
00:03:51,468 --> 00:03:55,570
a little bit, because what we've really got is partitions within topics.

58
00:03:55,730 --> 00:03:58,922
When you create a topic, you'll specify how

59
00:03:58,976 --> 00:04:02,902
many partitions it should have, and this defines

60
00:04:02,966 --> 00:04:06,220
how many shards your data can be shared across.

61
00:04:06,590 --> 00:04:09,946
So usually the partition is defined by the key you

62
00:04:09,968 --> 00:04:13,758
send, a key and a value for kafka. And usually

63
00:04:13,844 --> 00:04:16,970
we use the key. You don't have to. You can add some custom logic,

64
00:04:17,130 --> 00:04:20,718
but there are two reasons to do this. One is

65
00:04:20,884 --> 00:04:24,722
to spread the data out across your lovely big cluster, right?

66
00:04:24,776 --> 00:04:27,460
Because the partitions can be spread apart from one another.

67
00:04:27,830 --> 00:04:32,318
The other one is that we only allow one consumer

68
00:04:32,494 --> 00:04:36,266
per consumer group. Consumer groups in a minute per topic

69
00:04:36,318 --> 00:04:40,230
partition. So if there's a lot of consumer work to do,

70
00:04:40,380 --> 00:04:43,894
you need to spread your data out between the

71
00:04:43,932 --> 00:04:47,494
partitions to allow more consumers to get to the work.

72
00:04:47,612 --> 00:04:51,098
So that's one reason, and another reason is within

73
00:04:51,184 --> 00:04:54,854
the partitions, order is preserved.

74
00:04:54,982 --> 00:04:58,010
So if you have messages that need to be processed in order,

75
00:04:58,080 --> 00:05:01,558
the same item updating multiple times, then you

76
00:05:01,584 --> 00:05:05,322
need to get them into the same partition. Messages with identical keys

77
00:05:05,386 --> 00:05:09,486
will be routed to identical partitions, or you can have your own

78
00:05:09,588 --> 00:05:13,700
logic for that. These messages could be anything,

79
00:05:14,230 --> 00:05:18,546
really. They might be a login event or a click event from

80
00:05:18,728 --> 00:05:22,478
a web application. They might be a sensor reading,

81
00:05:22,574 --> 00:05:25,060
and we'll see some examples of that later.

82
00:05:25,690 --> 00:05:29,398
So one consumer per consumer group

83
00:05:29,484 --> 00:05:32,438
per topic partition. What's a consumer group?

84
00:05:32,524 --> 00:05:35,666
Right. So we have one consumer per partition,

85
00:05:35,778 --> 00:05:39,130
but we might have more than one application

86
00:05:39,280 --> 00:05:43,530
consuming the same data. So, for example, if we have

87
00:05:43,680 --> 00:05:47,146
a sensor beside a railway and it detects that

88
00:05:47,168 --> 00:05:50,346
a train has gone past, that data might be used

89
00:05:50,448 --> 00:05:54,206
by the train arrival times board.

90
00:05:54,388 --> 00:05:58,160
I'm standing on the platform wondering how late my train is now,

91
00:05:58,690 --> 00:06:01,678
and I get the update because we just found out where the train was.

92
00:06:01,844 --> 00:06:05,374
The same data might be used by the application

93
00:06:05,492 --> 00:06:08,914
that draws the map in the control room. So we know where the train got

94
00:06:08,952 --> 00:06:12,162
to. So different apps can read the same data,

95
00:06:12,296 --> 00:06:16,030
and each one of those applications will be in its own consumer group.

96
00:06:16,200 --> 00:06:19,606
So we have one consumer per consumer group

97
00:06:19,708 --> 00:06:23,842
per topic partition. And in the diagram here, you can see that there's

98
00:06:23,986 --> 00:06:27,080
different consumer groups reading from different partitions here.

99
00:06:27,390 --> 00:06:31,670
One more important thing to mention is the replication factors.

100
00:06:31,830 --> 00:06:35,430
When you create the topic, you'll also configure its replication

101
00:06:35,510 --> 00:06:39,178
factor, and that's how many copies of this data

102
00:06:39,344 --> 00:06:42,774
should exist. Replication works at the partition level.

103
00:06:42,832 --> 00:06:46,526
So you create a topic and you say, I need two copies of this

104
00:06:46,548 --> 00:06:49,898
data at all times, and each of its partitions

105
00:06:49,914 --> 00:06:53,662
will be stored on two different nodes. Usually we're working

106
00:06:53,716 --> 00:06:56,786
with one and the other one's just replicating away. We don't need it. But if

107
00:06:56,808 --> 00:07:00,082
something bad happens to the first node, then we've got that second

108
00:07:00,136 --> 00:07:03,380
copy. The more precious your data is,

109
00:07:04,070 --> 00:07:07,954
the higher the replication factor should be for

110
00:07:07,992 --> 00:07:11,510
that data. If it's something that you don't really need,

111
00:07:11,660 --> 00:07:15,106
you don't really need to replicate it. I mean, you let's replicate

112
00:07:15,138 --> 00:07:18,326
it anyway because it's really unusual to run Kafka on a single node. You might

113
00:07:18,348 --> 00:07:21,194
as well have two copies. But if you really can afford to lose it,

114
00:07:21,232 --> 00:07:24,506
then set that number to the number of nodes that you have, and let's give

115
00:07:24,528 --> 00:07:26,940
you the best chance of never losing that data.

116
00:07:27,630 --> 00:07:31,530
Let's talk about using Kafka with

117
00:07:31,600 --> 00:07:35,166
go with our go applications. So I've got a QR code here and

118
00:07:35,188 --> 00:07:38,526
it's just the hyperlink, but you can scan it quickly if you want. You can

119
00:07:38,548 --> 00:07:41,854
pause the video if you're watching the replay. I'm going to

120
00:07:41,892 --> 00:07:45,246
show you some code and I've sort of

121
00:07:45,268 --> 00:07:48,466
pulled some snippets into the slides. But if you want to play with this and

122
00:07:48,488 --> 00:07:51,838
see it working, it's all there in the GitHub repository.

123
00:07:52,014 --> 00:07:55,970
There's also some python code in the GitHub repository, and probably

124
00:07:56,040 --> 00:08:00,070
you all write more than one language anyway, but also, why not?

125
00:08:00,220 --> 00:08:04,386
One of the points of Kafka is that it's an ideal decoupling

126
00:08:04,418 --> 00:08:08,030
point, an ideal scalability point in our distributed

127
00:08:08,130 --> 00:08:11,562
and highly scalable systems. So it is

128
00:08:11,616 --> 00:08:15,226
tech agnostic, it should be tech agnostic, and we

129
00:08:15,248 --> 00:08:19,050
should be using different producers and consumers with whatever

130
00:08:19,120 --> 00:08:22,766
the right tech stack is for the application that's getting that data

131
00:08:22,948 --> 00:08:27,278
or sending it, receiving it, whichever. So this makes complete

132
00:08:27,364 --> 00:08:31,914
sense to me, but we're going to look at the go examples specifically

133
00:08:31,962 --> 00:08:35,090
today I'm using three libraries in particular,

134
00:08:35,240 --> 00:08:39,390
and I want to shout out both these three libraries,

135
00:08:39,470 --> 00:08:43,054
but also the fact that there's a few libraries

136
00:08:43,102 --> 00:08:47,240
around and they're actually all great, which doesn't help you choose.

137
00:08:48,970 --> 00:08:52,486
I can't cover them all. I'm not sure I even

138
00:08:52,508 --> 00:08:56,198
have favorites, but I have examples that use these libraries. I'll give a specific

139
00:08:56,284 --> 00:08:59,814
shout out to Sarama from shopify, which is also pretty great.

140
00:08:59,932 --> 00:09:03,562
Historically it was missing something. That means that I usually

141
00:09:03,616 --> 00:09:07,146
use the confluent library, but I have forgotten what that is and I'm not

142
00:09:07,168 --> 00:09:10,710
sure it's even still missing. So I'm using the confluent library.

143
00:09:10,790 --> 00:09:14,778
They maintain a bunch of sdks for all different tech stacks,

144
00:09:14,874 --> 00:09:18,670
and you'll see that in the first example. The later examples also use

145
00:09:18,740 --> 00:09:22,094
this SR client library for dealing with a

146
00:09:22,132 --> 00:09:25,934
schema registry client and also the Gogen Avro

147
00:09:25,982 --> 00:09:30,574
library, which is going to help me to get some connect structs

148
00:09:30,702 --> 00:09:34,690
to work with from go so that I can more easily go into

149
00:09:34,760 --> 00:09:38,120
and out of the data formats that are flowing around

150
00:09:39,130 --> 00:09:42,418
the example. And you'll see this all through the example repo

151
00:09:42,514 --> 00:09:45,846
is it's an imaginary Internet of things application

152
00:09:46,028 --> 00:09:50,390
for a factory for an imaginary company called Thingam

153
00:09:50,470 --> 00:09:54,294
Industries. They make Thingama jigs

154
00:09:54,422 --> 00:09:56,330
and Thingama bobs.

155
00:09:56,990 --> 00:10:00,522
Examples are hard, but you get the idea. The data

156
00:10:00,576 --> 00:10:04,250
looks like this JSON example here. So each machine

157
00:10:04,330 --> 00:10:07,562
has a bunch of sensors, and those sensors will identify

158
00:10:07,626 --> 00:10:11,614
which machine, which sensor the current reading, and also

159
00:10:11,652 --> 00:10:15,386
the units of that reading, because it makes a big difference.

160
00:10:15,508 --> 00:10:19,666
So now you understand what the imaginary story is and the

161
00:10:19,688 --> 00:10:24,110
format of the data that we're dealing with. Let's look at some code examples.

162
00:10:24,190 --> 00:10:28,246
And I'm going to start with the producer that puts the data into the

163
00:10:28,268 --> 00:10:31,842
topic on Kafka. So we create a new producer

164
00:10:31,906 --> 00:10:36,018
and set some config. Line two has the broker Uri.

165
00:10:36,114 --> 00:10:39,814
I just have this in an environment variable so that I can easily

166
00:10:39,862 --> 00:10:43,306
update it and not paste it into my slides. The rest

167
00:10:43,328 --> 00:10:47,354
is SSL config. If you are running with

168
00:10:47,392 --> 00:10:51,194
the Ivan example, you'll need the SSL certs as

169
00:10:51,232 --> 00:10:54,846
shown here. You can use. I mean, everything we do on

170
00:10:54,868 --> 00:10:58,266
Ivan is open source, right? So you can always do it there or somewhere

171
00:10:58,298 --> 00:11:02,046
else. That's why we do it the way we do. If you're running

172
00:11:02,148 --> 00:11:06,062
the so Apache Kafka also has quite a cool docker setup that's reasonably

173
00:11:06,126 --> 00:11:10,030
easy to get started with. So if you're running that, it's on localhost

174
00:11:10,190 --> 00:11:13,586
and you don't need the SSL config. So this config will

175
00:11:13,608 --> 00:11:17,122
look a little bit different depending handling

176
00:11:17,186 --> 00:11:20,866
some errors and then deferring the close on that producer because we're

177
00:11:20,898 --> 00:11:24,390
about to use it for something that's part one

178
00:11:24,460 --> 00:11:28,310
of the code. Here's part two. I'm creating,

179
00:11:28,890 --> 00:11:32,474
putting some values into a machine sensor struct. We'll talk

180
00:11:32,512 --> 00:11:36,150
about the Avro package later. But anyway, this is my struct.

181
00:11:36,230 --> 00:11:39,450
I'm putting some data in it and then I

182
00:11:39,520 --> 00:11:42,958
am turning it into JSON and sending it

183
00:11:42,964 --> 00:11:46,474
off to Kafka. I asked the producer object to produce

184
00:11:46,522 --> 00:11:51,070
and off it goes. The consumer looks honestly

185
00:11:51,490 --> 00:11:54,810
quite similar. Here is the consumer.

186
00:11:54,890 --> 00:11:58,722
I ripped out the SSL options to show you that. On line three

187
00:11:58,776 --> 00:12:02,414
we use the group id and that's the consumer group concept

188
00:12:02,462 --> 00:12:05,714
that we talked about before. I also have this auto

189
00:12:05,762 --> 00:12:09,570
offset reset setting and by default

190
00:12:09,730 --> 00:12:13,906
when you start consuming from a Kafka

191
00:12:13,938 --> 00:12:17,334
topic, you will get the data that's arriving now. But what

192
00:12:17,372 --> 00:12:21,030
you can do is consume it from the beginning.

193
00:12:21,190 --> 00:12:24,762
So if you need to just re audit or retot up

194
00:12:24,816 --> 00:12:28,086
a bunch of transactions, then you can give it this earliest setting

195
00:12:28,118 --> 00:12:32,422
and it'll read from the beginning. If you're doing vanishingly

196
00:12:32,486 --> 00:12:36,046
simple demos for conference talks, then reading from the

197
00:12:36,068 --> 00:12:39,550
beginning means you don't have to have lots of real time things actually working.

198
00:12:39,620 --> 00:12:42,942
You can just produce some messages in one place and then consume them

199
00:12:42,996 --> 00:12:46,194
in another place. The interesting stuff is happening on

200
00:12:46,232 --> 00:12:49,620
line nine, believe it or not. Doesn't look like the most interesting.

201
00:12:50,470 --> 00:12:54,574
We read the message and it just outputs what we have. So consuming

202
00:12:54,622 --> 00:12:57,814
is also reasonably approachable and you could imagine putting

203
00:12:57,852 --> 00:13:00,918
this into your application and wrapping it up in a

204
00:13:00,924 --> 00:13:04,038
way that would make sense. So we've had a look at the

205
00:13:04,204 --> 00:13:08,038
go code, but let's talk about some of

206
00:13:08,044 --> 00:13:11,274
the other tools in the Kafka space.

207
00:13:11,472 --> 00:13:14,266
I mean you can just do everything from go,

208
00:13:14,368 --> 00:13:18,582
but there are some other tools that I find useful as sort of diagnosis.

209
00:13:18,726 --> 00:13:22,494
Quickly produce or consume messages, so let's give them a mention.

210
00:13:22,612 --> 00:13:27,002
First up, if you download Kafka, it has useful

211
00:13:27,066 --> 00:13:30,574
scripts like list the topics now please run

212
00:13:30,612 --> 00:13:34,414
a consumer on the console now please. It has all of that built

213
00:13:34,452 --> 00:13:37,986
in, so that's quite useful and that's definitely a place to start.

214
00:13:38,088 --> 00:13:41,986
I have two other open source tools that are my favorites that I

215
00:13:42,008 --> 00:13:45,298
want to share. First up, I'm going to show you Kafka cat,

216
00:13:45,384 --> 00:13:48,598
which is a single command line tool that does a

217
00:13:48,604 --> 00:13:51,826
bunch of different things. Here you can see it in consumer

218
00:13:51,858 --> 00:13:55,366
mode. It's just reading in all the data that that producer that you just

219
00:13:55,388 --> 00:13:59,078
saw produced. So for a CLI tool,

220
00:13:59,164 --> 00:14:02,186
and you'll end up with like a little text file with a load of little

221
00:14:02,208 --> 00:14:06,182
scripts or some aliases or something, this is one of my favorites.

222
00:14:06,326 --> 00:14:09,210
If the command line is not so much your thing,

223
00:14:09,280 --> 00:14:13,698
and really, who could blame you, then have a look at Cafdrop,

224
00:14:13,814 --> 00:14:17,870
which is a web UI. This one comes

225
00:14:17,940 --> 00:14:21,918
again, it's got a docker container, so super simple to get started,

226
00:14:22,084 --> 00:14:25,342
give it some configuration and off you go. So you can poke around

227
00:14:25,396 --> 00:14:28,846
at what's going on. Whether you have localhost, Kafka, or somewhere

228
00:14:28,878 --> 00:14:32,318
in the cloud Kafka, you can poke at it and see what's

229
00:14:32,334 --> 00:14:36,030
in your topic and what's happening. If you are using a cloud hosted solution,

230
00:14:36,110 --> 00:14:39,714
probably they have something. This is a screenshot of the Ivan topic

231
00:14:39,762 --> 00:14:43,398
browser that comes just with the online dashboard. So yes,

232
00:14:43,484 --> 00:14:46,886
check out your friendly cloud hosting platform, which may have

233
00:14:46,988 --> 00:14:50,630
options. Let's talk next about schemas,

234
00:14:50,710 --> 00:14:54,858
and I'll open immediately by saying schemas are

235
00:14:55,024 --> 00:14:58,166
not required. So a lot of applications

236
00:14:58,198 --> 00:15:01,982
will use Kafka with no schema at all, and that's fine.

237
00:15:02,116 --> 00:15:05,374
Kafka fundamentally doesn't care what

238
00:15:05,412 --> 00:15:09,550
data you send to it, and it also doesn't care whether

239
00:15:09,620 --> 00:15:12,766
your data is consistent or right or anything.

240
00:15:12,868 --> 00:15:15,394
It's not going to do any validation for you, it's not going to do any

241
00:15:15,432 --> 00:15:18,834
transformation for you. It's rubbish in, rubbish out as far as

242
00:15:18,872 --> 00:15:22,114
Kafka is concerned. So schemas can really help us to get

243
00:15:22,152 --> 00:15:25,602
that right. And I see it as particularly

244
00:15:25,666 --> 00:15:29,526
useful where there are multiple people or teams collaborating on

245
00:15:29,548 --> 00:15:33,650
a project. So schemas are great. I am a fan.

246
00:15:33,810 --> 00:15:37,586
They allow us to describe and then enforce

247
00:15:37,698 --> 00:15:41,754
our data format. Sometimes you will need a

248
00:15:41,792 --> 00:15:45,814
schema. So for example, there are some cool compression formats

249
00:15:45,862 --> 00:15:49,386
such as Protobuff or Avro, and both of those require that there is

250
00:15:49,408 --> 00:15:52,854
a schema that describes the structure of the data

251
00:15:52,992 --> 00:15:56,366
for them to work, they solve the problem in different ways.

252
00:15:56,548 --> 00:15:59,742
Protobuff, you give the schema and it generates some code

253
00:15:59,796 --> 00:16:03,434
and you use that code. So the code generator supports whichever

254
00:16:03,482 --> 00:16:07,074
text stacks it supports, and that's it. Avro is a bit more

255
00:16:07,112 --> 00:16:11,006
open minded and you will encode the schema,

256
00:16:11,118 --> 00:16:14,466
send it to the schema registry, and then the consumer gets the schema back from

257
00:16:14,488 --> 00:16:17,630
the schema registry and turns it

258
00:16:17,640 --> 00:16:21,606
back into something you can use. Because go is

259
00:16:21,628 --> 00:16:25,298
a strongly typed language. Like I've always liked schemas, but I've

260
00:16:25,314 --> 00:16:28,486
worked with Kafka from a bunch of different text stacks. When you come to do

261
00:16:28,508 --> 00:16:32,106
it from go, the schema becomes oh, we should all do

262
00:16:32,128 --> 00:16:35,626
it this way. And it's really influenced the way that I do this

263
00:16:35,728 --> 00:16:39,734
with the other tech stacks that I also know and love. So our favorite strongly

264
00:16:39,782 --> 00:16:42,846
typed language just really finds it useful to

265
00:16:42,868 --> 00:16:46,942
be strict about exactly the data fields and

266
00:16:46,996 --> 00:16:51,162
exactly the data types that will be included. I mentioned Avro.

267
00:16:51,306 --> 00:16:54,626
Avro is today's example.

268
00:16:54,808 --> 00:16:58,926
It's something that I've used quite a bit, again because it's tech stack agnostic

269
00:16:59,038 --> 00:17:03,438
and I find the enforced structure really valuable.

270
00:17:03,534 --> 00:17:08,226
Just that guarantee of payload format is it's

271
00:17:08,258 --> 00:17:11,910
good for my sanity. What Avro does is instead

272
00:17:11,980 --> 00:17:15,106
of including the whole payload verbatim

273
00:17:15,218 --> 00:17:19,222
in every message, it removes the repeated parts

274
00:17:19,286 --> 00:17:22,774
like the field names, and also applies some compression

275
00:17:22,822 --> 00:17:26,106
to it. So your producer has some

276
00:17:26,208 --> 00:17:30,134
Avro capability. You supply a message and your Avro

277
00:17:30,182 --> 00:17:33,438
schema, and it first of all registers the

278
00:17:33,444 --> 00:17:36,638
schema with the schema registry and then

279
00:17:36,724 --> 00:17:40,606
works out which version of the schema we have. Then it creates a

280
00:17:40,628 --> 00:17:44,630
payload which has a bit of information about which version

281
00:17:44,730 --> 00:17:48,210
of the schema we're using, and the specialists

282
00:17:48,280 --> 00:17:51,986
format of the message puts those two things together, sends it into

283
00:17:52,008 --> 00:17:54,574
the topic. The consumer does all that in reverse,

284
00:17:54,622 --> 00:17:57,782
right? Gets the payload, has a look which

285
00:17:57,836 --> 00:18:01,266
version of the schema it needs, fetches that from the schema registry,

286
00:18:01,298 --> 00:18:04,898
and gives you back the message as it was. And that ability

287
00:18:04,994 --> 00:18:08,202
to compress really saves space with the

288
00:18:08,256 --> 00:18:12,086
one meg typical limit, and can be a really efficient

289
00:18:12,118 --> 00:18:15,526
way to transfer things around. So the schema registry

290
00:18:15,638 --> 00:18:19,418
has multiple versions of a schema as things change

291
00:18:19,584 --> 00:18:23,146
on a per topic basis. In my examples, the schema

292
00:18:23,178 --> 00:18:27,066
registry is carapace. It's can open source schema

293
00:18:27,098 --> 00:18:30,746
registry. It's an Ivan project. We use it on our cloud hosted version,

294
00:18:30,778 --> 00:18:34,286
and I also use it for local stuff as well. There are

295
00:18:34,308 --> 00:18:38,130
a few around API curio have one, confluent has one.

296
00:18:38,200 --> 00:18:41,506
There's a bunch I talked about the schema versions, so I just want to

297
00:18:41,528 --> 00:18:44,658
do a small tangent and talk about

298
00:18:44,744 --> 00:18:48,550
evolving those schemas. I mean, don't change your message

299
00:18:48,620 --> 00:18:50,630
format that way lies madness.

300
00:18:51,610 --> 00:18:54,662
Sometimes things happen. I live in the real world

301
00:18:54,716 --> 00:18:58,200
and I know that sometimes our requirements change. When it happens,

302
00:18:58,730 --> 00:19:01,786
then we need to approach it, and the best is

303
00:19:01,808 --> 00:19:04,986
to do it in a backwards compatible way. So if you need

304
00:19:05,008 --> 00:19:08,794
to rename a field instead, add another field

305
00:19:08,912 --> 00:19:12,186
with the same value but the new name, we need to keep the

306
00:19:12,208 --> 00:19:15,726
old one. You can add an optional field that's safe as

307
00:19:15,748 --> 00:19:19,390
well, because if it's not there in a previous version, then that's fine.

308
00:19:19,540 --> 00:19:23,278
Every time you make a change, even a backwards compatible change,

309
00:19:23,364 --> 00:19:26,846
it is a new schema, and we'll need to register a new version

310
00:19:26,878 --> 00:19:30,434
with the schema registry. Avro makes all of this easy because it does

311
00:19:30,472 --> 00:19:34,430
have a support for default values and it does also support aliases

312
00:19:34,510 --> 00:19:38,054
as well. Cool. So that's schemas. That's a little

313
00:19:38,092 --> 00:19:41,430
sanity check on how to evolve them if things do happen.

314
00:19:41,580 --> 00:19:45,026
Let's look at an example of the Avro schema.

315
00:19:45,058 --> 00:19:48,062
How do we describe a payload?

316
00:19:48,226 --> 00:19:52,074
Well, it's a record with a name and

317
00:19:52,112 --> 00:19:55,674
a bunch of fields. Avro supports the name of

318
00:19:55,712 --> 00:19:59,750
a field, the type of a field, and also this doc

319
00:19:59,910 --> 00:20:03,434
string. So you can look at this and know if the

320
00:20:03,472 --> 00:20:06,838
field name isn't quite self explanatory. And you know,

321
00:20:06,944 --> 00:20:09,966
we all have good intentions, but sometimes things happen.

322
00:20:10,068 --> 00:20:13,442
Then the doc string can add just a little bit of

323
00:20:13,496 --> 00:20:17,186
extra connect. Also remember it, because you're going to see it later.

324
00:20:17,288 --> 00:20:22,180
And we can use this schema then to create

325
00:20:22,790 --> 00:20:26,670
a struct using the Gogen Avro library.

326
00:20:26,750 --> 00:20:30,614
So Gogen Avro takes the Avro schema, I've asked

327
00:20:30,652 --> 00:20:34,162
it to make an Avro package here, and it gives me this struct,

328
00:20:34,226 --> 00:20:37,286
which is what you saw in the example code. I can just set values on

329
00:20:37,308 --> 00:20:40,314
this. It knows how to serialize and deserialize itself.

330
00:20:40,512 --> 00:20:43,306
Actually it generates a whole.

331
00:20:43,488 --> 00:20:47,094
The struct comes with a bunch of functionality, so it can be serialized

332
00:20:47,142 --> 00:20:50,662
and deserialized. Amazing magic occurs. It's quite a long file,

333
00:20:50,806 --> 00:20:54,014
but as from the user point of view from userland, I just

334
00:20:54,132 --> 00:20:58,334
go ahead with this machine sensor and set

335
00:20:58,372 --> 00:21:02,046
my values or try and read them back. It's pretty cool. I do want to

336
00:21:02,068 --> 00:21:05,790
show you the producer with the Avro

337
00:21:05,870 --> 00:21:08,260
format and the schema registry piece.

338
00:21:08,710 --> 00:21:12,274
So again, two slides didn't quite

339
00:21:12,312 --> 00:21:15,570
fit. On one we've got the producer already created.

340
00:21:15,990 --> 00:21:19,666
We now have to also connect to this schema registry

341
00:21:19,698 --> 00:21:23,160
client. And here you can see that Sr client library that I mentioned.

342
00:21:23,690 --> 00:21:26,466
We also get the latest schema for a topic.

343
00:21:26,578 --> 00:21:30,770
I'm just assuming that what I'm building here is the latest registered schema.

344
00:21:30,930 --> 00:21:34,586
You know, where the struct came from this time. So we're filling in the

345
00:21:34,608 --> 00:21:38,058
values in the struct, and then we're just getting that ready as a bunch of

346
00:21:38,064 --> 00:21:42,262
bytes that we can add to the payload. The other piece of that payload

347
00:21:42,326 --> 00:21:45,820
is the schema id. We know the schema id.

348
00:21:46,290 --> 00:21:49,662
We turn it into a bunch of bytes and assemble it with the schema id.

349
00:21:49,796 --> 00:21:53,246
And then the main body of the message bytes put

350
00:21:53,268 --> 00:21:56,046
it all together and send it off to Kafka.

351
00:21:56,238 --> 00:21:59,970
So it is a little bit more than you saw in the first example,

352
00:22:00,040 --> 00:22:04,210
but it's quite achievable and sort of fits into the same overall pattern.

353
00:22:04,550 --> 00:22:08,978
I've shown you how to describe the payloads for a machine,

354
00:22:09,154 --> 00:22:12,390
but I want to talk a little bit more about building

355
00:22:12,460 --> 00:22:16,166
on that idea of enforcing structure and

356
00:22:16,348 --> 00:22:19,706
turning it into something that's a bit more

357
00:22:19,808 --> 00:22:23,446
human friendly. We don't

358
00:22:23,558 --> 00:22:27,450
so often publish our streaming integrations

359
00:22:27,950 --> 00:22:31,434
publicly to third parties, as we do with more traditional

360
00:22:31,482 --> 00:22:35,310
HTTP APIs. But in a large enough organization,

361
00:22:35,650 --> 00:22:38,686
having even an intel team integrating with

362
00:22:38,708 --> 00:22:42,634
you is equivalent to a third party if they're a couple of hops away

363
00:22:42,692 --> 00:22:46,690
on the chart. So what can make that

364
00:22:46,760 --> 00:22:50,286
easier? I'd like to introduce you to async

365
00:22:50,318 --> 00:22:53,778
API. It's an open standard

366
00:22:53,944 --> 00:22:57,842
for describing event driven architectures.

367
00:22:57,986 --> 00:23:01,730
If you are already familiar with Open API,

368
00:23:01,890 --> 00:23:05,334
then it's a sister specification to that. If you're not, it's an

369
00:23:05,372 --> 00:23:09,146
open API is another open standard. That's for describing just the

370
00:23:09,168 --> 00:23:12,650
HTTP APIs. Async API works

371
00:23:12,720 --> 00:23:15,894
for all the streaming type platforms,

372
00:23:15,942 --> 00:23:20,082
like messagey things, qish things. If you've got websockets or MQTT

373
00:23:20,166 --> 00:23:24,078
or Kafka or then async API is going

374
00:23:24,084 --> 00:23:27,322
to help you with that. For Kafka, we can describe

375
00:23:27,386 --> 00:23:31,594
the brokers how to authenticate with those endpoints.

376
00:23:31,722 --> 00:23:35,246
We can describe also the name of the topics and whether we

377
00:23:35,268 --> 00:23:38,994
are publishing or subscribing to those. And we can also then

378
00:23:39,112 --> 00:23:42,718
outline the payloads, which is what we did before with the Avro.

379
00:23:42,814 --> 00:23:45,640
And this is where it kind of gets interesting,

380
00:23:46,170 --> 00:23:49,510
because async API is very much

381
00:23:49,580 --> 00:23:52,982
part of the industry. It's something that

382
00:23:53,116 --> 00:23:56,614
integrations and is intended to play nicely with the

383
00:23:56,652 --> 00:24:00,746
other open standards that you're already using. So if you're describing your

384
00:24:00,768 --> 00:24:05,014
payloads with Avro as you've just seen, or cloud events,

385
00:24:05,142 --> 00:24:09,194
if you're using that, then you can refer to those

386
00:24:09,232 --> 00:24:12,990
schemas within your Async API document.

387
00:24:13,650 --> 00:24:16,942
Once you have that description, then you can go ahead

388
00:24:16,996 --> 00:24:20,270
and do all sorts of things. You can generate code,

389
00:24:20,420 --> 00:24:24,434
you can do automatic integrations, you can generate documentation as

390
00:24:24,472 --> 00:24:28,386
well. Let's look at an example. Here's the

391
00:24:28,408 --> 00:24:32,126
interesting bit. Basically from an Async API

392
00:24:32,158 --> 00:24:35,770
document. So straight away you can tell, oh, there's a lot of yaml.

393
00:24:35,950 --> 00:24:40,626
But the magic here is in the last line. The dollar ref syntax

394
00:24:40,738 --> 00:24:43,874
is common across at least OpenAPI

395
00:24:43,922 --> 00:24:47,154
and Async API, and it means that you're referring

396
00:24:47,202 --> 00:24:49,974
to another section in the file,

397
00:24:50,102 --> 00:24:53,766
or another section in another file, or a whole other file,

398
00:24:53,798 --> 00:24:57,242
as I am here. Async API is just as happy

399
00:24:57,296 --> 00:25:00,654
to process an avro schema as it is

400
00:25:00,772 --> 00:25:05,562
to process a payload described in async

401
00:25:05,626 --> 00:25:09,406
API format. One thing you can also do once you

402
00:25:09,428 --> 00:25:12,954
have the payload in place is to add examples.

403
00:25:13,002 --> 00:25:17,282
The Async API document encourages examples and

404
00:25:17,416 --> 00:25:21,106
they say a picture is worth a thousand words, but a good example

405
00:25:21,288 --> 00:25:24,866
is worth at least that many. So that's a feature that

406
00:25:24,888 --> 00:25:28,642
I really appreciate and enjoy. You can generate documentation,

407
00:25:28,786 --> 00:25:32,146
you can see it here with the fields

408
00:25:32,258 --> 00:25:35,666
documented, with the examples, the enum fields,

409
00:25:35,698 --> 00:25:39,222
and then actual examples showing on the right hand side.

410
00:25:39,356 --> 00:25:43,178
You can generate this for no further investment than just creating the

411
00:25:43,184 --> 00:25:46,586
Async API description. And I think there's a lot here that can

412
00:25:46,608 --> 00:25:49,930
really help us to work together. So I've talked today

413
00:25:50,000 --> 00:25:53,694
about Kafka and what it means for us as go

414
00:25:53,732 --> 00:25:57,962
developers, how it can fit alongside our super scalable

415
00:25:58,026 --> 00:26:01,338
and performant tech stack. I think if you're

416
00:26:01,354 --> 00:26:04,766
working in the go space, Kafka is well worth your time.

417
00:26:04,948 --> 00:26:08,558
If you have data flowing between components, especially if it's

418
00:26:08,574 --> 00:26:12,382
eventish or there's a lot of it, Kafka can be a really good addition

419
00:26:12,446 --> 00:26:16,450
to your setup if you don't have it already. It's most common

420
00:26:16,600 --> 00:26:20,982
in the banking and manufacturing industries, but only because they are ahead

421
00:26:21,116 --> 00:26:24,422
of most of the rest of the industries in terms of how much

422
00:26:24,476 --> 00:26:28,390
data they need to collect, keep safe and transfer quickly.

423
00:26:28,540 --> 00:26:32,178
It's got applications in a bunch of other industries, and I'd

424
00:26:32,194 --> 00:26:35,386
be really interested to hear what your experiences are or how you get

425
00:26:35,408 --> 00:26:38,266
on if you go and try it. And I hope that I've given you an

426
00:26:38,288 --> 00:26:41,702
intro today that would let you understand what it is

427
00:26:41,856 --> 00:26:45,214
and get started when you have a need for it.

428
00:26:45,332 --> 00:26:49,422
I'll wrap up then by sharing with you some

429
00:26:49,476 --> 00:26:52,910
resources in case you want them, and also how to reach me.

430
00:26:52,980 --> 00:26:56,938
So there's the example repository again. The Thingam Industries

431
00:26:57,034 --> 00:27:00,782
has everything that you've seen today was copied and pasted out of that repo,

432
00:27:00,846 --> 00:27:03,890
so you can see it in its context. You can run the scripts yourself,

433
00:27:03,960 --> 00:27:07,886
that kind of thing. Compulsory shout out for Ivan,

434
00:27:07,918 --> 00:27:11,106
who is supporting me to be here. Go to Ivan IO.

435
00:27:11,218 --> 00:27:14,342
We have Kafka as a service. Whatever other databases you need.

436
00:27:14,396 --> 00:27:18,562
I mean, go ahead, we have a free trial. So if you are curious

437
00:27:18,626 --> 00:27:22,514
about Kafka, then that's quite an easy onboarding

438
00:27:22,562 --> 00:27:25,306
way to try it out. And of course if you have questions then I would

439
00:27:25,328 --> 00:27:28,634
love to talk to you about those. I mentioned the schema registry, so that is

440
00:27:28,672 --> 00:27:32,650
carapace there's the link to the project. It's an opensource project.

441
00:27:32,720 --> 00:27:36,046
It's one that we use ourselves at Ivan. You're very welcome to use it and

442
00:27:36,068 --> 00:27:40,686
also very welcome to contribute. Here's the link to Async API asyncabi.com

443
00:27:40,788 --> 00:27:44,686
it's an open standard, which means it's a community driven

444
00:27:44,718 --> 00:27:48,290
project. We work in the open. The community

445
00:27:48,440 --> 00:27:51,506
meetings are open, the contributions are welcome.

446
00:27:51,688 --> 00:27:55,666
Discussions are all held in the open. If you are working in this space or

447
00:27:55,688 --> 00:27:59,702
you're interested, it's a very welcoming community and

448
00:27:59,836 --> 00:28:03,346
there's plenty of room for more contributions. So if you have ideas

449
00:28:03,378 --> 00:28:07,014
of how this should work, then I would strongly advocate that as a great place

450
00:28:07,052 --> 00:28:10,374
to go and get more involved in this sort of thing. Finally, that's me.

451
00:28:10,412 --> 00:28:14,018
Lornajane. Net. You can find slide decks,

452
00:28:14,114 --> 00:28:17,574
video recordings, blog posts and my

453
00:28:17,612 --> 00:28:20,862
contact details. So if you need to get in touch with me or you're interested

454
00:28:20,916 --> 00:28:24,846
in keeping up with what I'm doing, then that is a good place to

455
00:28:24,948 --> 00:28:28,986
look. I am done. Thank you so much for your attention.

456
00:28:29,178 --> 00:28:29,820
Stay in touch.

