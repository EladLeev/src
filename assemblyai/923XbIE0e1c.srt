1
00:00:00,250 --> 00:00:04,830
Are you an SRE? A developer?

2
00:00:06,610 --> 00:00:10,474
A quality engineer who wants to tackle the challenge of improving

3
00:00:10,522 --> 00:00:14,254
reliability in your DevOps? You can enable your DevOps for

4
00:00:14,292 --> 00:00:18,286
reliability with chaos native. Create your free account

5
00:00:18,388 --> 00:01:16,982
at Chaos native. Litmus Cloud hi

6
00:01:17,036 --> 00:01:20,534
and welcome to my talk. The topic of this session is about

7
00:01:20,572 --> 00:01:24,550
a new approach SRE can use to automate performance tuning of system

8
00:01:24,620 --> 00:01:28,646
configurations leveraging machine learning techniques we

9
00:01:28,668 --> 00:01:32,102
will start by discussing the SRE challenges for ensuring

10
00:01:32,166 --> 00:01:35,494
application performance and reliability. We will then introduce

11
00:01:35,542 --> 00:01:38,714
a new approach based on machine learning techniques, and then we will see

12
00:01:38,752 --> 00:01:42,570
how the new approach can be used to automatically tune today's complex

13
00:01:42,650 --> 00:01:46,606
applications by focusing on optimizing a microservice application

14
00:01:46,708 --> 00:01:50,238
running on Kubernetes and the Java virtual machine. Finally,

15
00:01:50,324 --> 00:01:54,542
we will conclude by sharing some takeaways on how this approach benefits sres

16
00:01:54,606 --> 00:01:58,210
teams. Before proceeding, allow me to introduce myself.

17
00:01:58,360 --> 00:02:02,094
My name is Stefano Doni and my role is CTO at Akamas.

18
00:02:02,222 --> 00:02:06,398
Performance engineering has always been my topic of interest and passion,

19
00:02:06,574 --> 00:02:10,198
so let's start by discussing why system configuration are so important

20
00:02:10,284 --> 00:02:13,734
to the job and mission of SREs. I would like to

21
00:02:13,772 --> 00:02:17,618
start by reviewing at the high level what really is the job of sres,

22
00:02:17,794 --> 00:02:21,670
citing the seminal SRES book by Google. It's no surprise

23
00:02:21,750 --> 00:02:24,838
that any SRE deeply cares about service performance,

24
00:02:24,934 --> 00:02:28,790
efficiency and reliability to understand in practice.

25
00:02:28,950 --> 00:02:32,698
What does that mean? Let's have a look at some of the core tenets

26
00:02:32,874 --> 00:02:35,882
pursuing maximum change velocity without violating

27
00:02:35,946 --> 00:02:39,438
slos. The tenet is well known and refers to

28
00:02:39,444 --> 00:02:43,374
the goal for sres to accelerate product innovation and releases whilst

29
00:02:43,422 --> 00:02:46,110
in matching agreed service level objectives.

30
00:02:46,270 --> 00:02:50,146
For example, ensuring request latency stays below a target value.

31
00:02:50,328 --> 00:02:53,906
Demand forecasting and capacity planning s three should have

32
00:02:53,928 --> 00:02:57,702
a clear understanding of what is the capacity of the service in terms

33
00:02:57,756 --> 00:03:01,666
of maximum load that can be supported. This is typically achieved

34
00:03:01,698 --> 00:03:04,754
via load testing to determine the right resources

35
00:03:04,802 --> 00:03:08,886
to provision to the service, for example before a product launch.

36
00:03:09,078 --> 00:03:12,602
Efficiency and performance efficiency means being

37
00:03:12,656 --> 00:03:16,154
able to achieve the target load with the required response time

38
00:03:16,272 --> 00:03:20,774
using the least amount of resources, which means lower cost sres

39
00:03:20,822 --> 00:03:24,634
and developers can change service software to improve its efficiency.

40
00:03:24,762 --> 00:03:28,682
And here is a major area where configuration can provide big gains,

41
00:03:28,746 --> 00:03:32,078
as we are going to see in a moment. So what is the role of

42
00:03:32,084 --> 00:03:36,126
system configurations and why are they so important for sres?

43
00:03:36,318 --> 00:03:39,938
System configuration are key for sres as they can really impact service

44
00:03:40,024 --> 00:03:43,746
performance, efficiency and reliability. Let's consider a

45
00:03:43,768 --> 00:03:46,486
real world example related to a Java service.

46
00:03:46,668 --> 00:03:50,354
After a JVM reconfiguration on day three, the cpu usage

47
00:03:50,402 --> 00:03:54,790
of the application service dramatically changed a 75%

48
00:03:54,860 --> 00:03:58,546
reduction while the system was still being

49
00:03:58,588 --> 00:04:02,534
able to support the same traffic load. So this configuration

50
00:04:02,582 --> 00:04:06,518
change translated into significant gains on service efficiency.

51
00:04:06,694 --> 00:04:10,922
On the right, you have another example where an optimal configuration significantly

52
00:04:10,986 --> 00:04:14,206
improved service resiliency. While the

53
00:04:14,228 --> 00:04:18,042
baseline configuration crashed under load, the best configuration

54
00:04:18,106 --> 00:04:21,150
supported the target load with much greater stability.

55
00:04:21,730 --> 00:04:25,426
So tuning system configurations really matters. And this

56
00:04:25,448 --> 00:04:28,974
does not apply only to Java. It also applies to all the infrastructure

57
00:04:29,022 --> 00:04:32,414
levels, from databases to container middleware

58
00:04:32,462 --> 00:04:35,220
and application servers to any other technology.

59
00:04:35,750 --> 00:04:40,066
But how easy it is to do that for modern applications,

60
00:04:40,258 --> 00:04:43,622
well, not so much. Actually, the contrary. The problem

61
00:04:43,676 --> 00:04:47,302
is that performance optimization is getting harder and harder these days.

62
00:04:47,436 --> 00:04:50,826
There are three key factors that I would like to highlight here.

63
00:04:51,008 --> 00:04:54,326
First of all, explosion of configurations parameters

64
00:04:54,518 --> 00:04:57,798
today, a JVM has more than 800 parameters,

65
00:04:57,974 --> 00:05:01,386
a MySQL database more than 500, and on

66
00:05:01,408 --> 00:05:04,782
the cloud AWS just added plus 100,

67
00:05:04,836 --> 00:05:07,966
etc. Two instance types just last year.

68
00:05:08,148 --> 00:05:12,174
Second, configurations can have unpredictable effects. In this

69
00:05:12,212 --> 00:05:16,190
example, allocating less memory to a database, a MongoDB database,

70
00:05:16,270 --> 00:05:19,934
made it run much faster. Third, with its velocity

71
00:05:19,982 --> 00:05:23,310
is increasing, this is well known for sres.

72
00:05:23,470 --> 00:05:27,054
It's worth mentioning that it has negative impacts on performance

73
00:05:27,102 --> 00:05:30,642
tuning also, which is largely a manual task but done

74
00:05:30,696 --> 00:05:33,960
by experts, and as such it takes a lot of time.

75
00:05:34,330 --> 00:05:38,126
So system configurations can play a big role on service performance

76
00:05:38,178 --> 00:05:41,606
and reliability. But optimizing them is a daunting

77
00:05:41,638 --> 00:05:45,274
task for sres, given increasing complexity of technology

78
00:05:45,392 --> 00:05:48,586
and modern delivery processes. This is why we

79
00:05:48,608 --> 00:05:51,886
think we need a new approach. At Akamas, we have

80
00:05:51,908 --> 00:05:55,374
identified four key requirements that we believe are needed to

81
00:05:55,412 --> 00:05:57,760
effectively solve this class of problems.

82
00:05:58,290 --> 00:06:02,042
First, the new approach should allow a full stack optimization,

83
00:06:02,186 --> 00:06:05,774
meaning that it has to be done to be able to optimize several

84
00:06:05,822 --> 00:06:08,466
different technologies at the same time.

85
00:06:08,648 --> 00:06:12,594
For example, the optimization might focus on container parameters and

86
00:06:12,632 --> 00:06:16,420
runtime selectings like the JVM options at the same time.

87
00:06:16,790 --> 00:06:20,006
Second, the new approach CTO allow a smart exploration of the

88
00:06:20,028 --> 00:06:23,398
space of potential configurations. If we consider

89
00:06:23,484 --> 00:06:26,854
all the possible values for all the possible parameters, we really have

90
00:06:26,892 --> 00:06:30,614
billions of configurations. So a brute force approach is

91
00:06:30,652 --> 00:06:34,218
simply not feasible and would not work for this kind of problem.

92
00:06:34,384 --> 00:06:37,882
To be effective, a solution needs to be identified in a reasonable time

93
00:06:37,936 --> 00:06:41,178
frame and at an acceptable cost. Third,

94
00:06:41,264 --> 00:06:44,558
the new approach should allow us to align to each specific need,

95
00:06:44,644 --> 00:06:48,538
priority and use case of interest by defining custom optimization

96
00:06:48,634 --> 00:06:51,886
goals. For example, in some cases the SREs may

97
00:06:51,908 --> 00:06:55,614
want to reduce the service latency and provide the best possible performance

98
00:06:55,662 --> 00:06:58,866
at peak loads. In other cases, reducing costs will

99
00:06:58,888 --> 00:07:02,770
be the key driver for the optimization without violating a service

100
00:07:02,840 --> 00:07:05,858
level objectives. And last but not least,

101
00:07:05,944 --> 00:07:09,126
the new approach should be fully automated to reduce toil as

102
00:07:09,148 --> 00:07:12,786
much as possible and ensure a reliable process with the fastened

103
00:07:12,818 --> 00:07:16,310
convergence to the optimal solution. So this figure

104
00:07:16,380 --> 00:07:20,006
shows the five phases of the automated optimization

105
00:07:20,118 --> 00:07:23,738
process. The first step is to apply a configuration to

106
00:07:23,744 --> 00:07:28,054
the target service of the optimizing. This can be a new JVM

107
00:07:28,102 --> 00:07:31,666
option or a new Kubernetes container resource settings,

108
00:07:31,718 --> 00:07:35,086
for example. The second step is to apply a workload to the

109
00:07:35,108 --> 00:07:38,446
target system so as to assess the impact of

110
00:07:38,468 --> 00:07:41,722
the apply configuration, typically by integrating

111
00:07:41,786 --> 00:07:45,314
with load testing tools. The first step is CTO collect key

112
00:07:45,352 --> 00:07:48,338
performance indicators related to the target system,

113
00:07:48,504 --> 00:07:51,714
that is, information, data and metrics about the behavior of the

114
00:07:51,752 --> 00:07:55,386
system under the workload. This step typically leverages

115
00:07:55,438 --> 00:07:59,174
existing monitoring tools. The fourth step is to score the

116
00:07:59,212 --> 00:08:02,626
configuration against the specified goal by leverages

117
00:08:02,658 --> 00:08:05,926
the collected KPIs. This step is where the

118
00:08:05,948 --> 00:08:09,442
system performance metrics are fed back to the machine learning optimizing,

119
00:08:09,506 --> 00:08:13,030
that is, scoring the tested configuration against the goals.

120
00:08:13,190 --> 00:08:17,114
The last step is where machine learning kicks in by taking this score as

121
00:08:17,152 --> 00:08:21,014
input and producing as output the most optimizing configuration to be tested

122
00:08:21,062 --> 00:08:24,554
in the next iteration of the same process. In a relatively

123
00:08:24,602 --> 00:08:28,618
short amount of time, the machine learning algorithms learns the dependencies

124
00:08:28,714 --> 00:08:32,714
between the configuration and the system behavior, thus identifying

125
00:08:32,762 --> 00:08:36,754
better and better configurations. An important aspect is what

126
00:08:36,792 --> 00:08:39,854
is the role of SRE in this new AI driven

127
00:08:39,902 --> 00:08:43,982
optimization process? SRES has a key role in the new automated

128
00:08:44,046 --> 00:08:48,066
optimization process as it defines the optimization goals

129
00:08:48,178 --> 00:08:51,810
and constraints and then lets the machine learning based optimization

130
00:08:51,890 --> 00:08:55,266
to automatically identify the optimal configurations.

131
00:08:55,458 --> 00:08:58,938
Let's see how this approach applies to a real world example

132
00:08:59,104 --> 00:09:03,110
where we will automate the optimization of a service by tuning Kubernetes

133
00:09:03,190 --> 00:09:06,458
and the JVM options. In our example,

134
00:09:06,544 --> 00:09:10,422
the target system is Google Online boutique, a cloud native

135
00:09:10,486 --> 00:09:14,506
application running on Kubernetes made of ten microservices.

136
00:09:14,698 --> 00:09:18,030
This application features a modern software stack with services

137
00:09:18,100 --> 00:09:21,646
written in Golang, node, JS, Java and Python, and it

138
00:09:21,668 --> 00:09:25,774
also includes a load generator based on locust, which generates realistic

139
00:09:25,822 --> 00:09:29,890
traffic. To test the application, we will leverages this application

140
00:09:30,040 --> 00:09:33,630
to illustrate two different but related use cases.

141
00:09:33,790 --> 00:09:36,706
The first use case is related to tuning Kubernetes,

142
00:09:36,818 --> 00:09:40,594
cpu and memory limits that define how many resources

143
00:09:40,642 --> 00:09:44,946
are assigned to a Kubernetes container to ensure application performance,

144
00:09:45,058 --> 00:09:49,070
cluster efficiency and stability. For SRE, the challenge

145
00:09:49,090 --> 00:09:52,902
is represented by the need to ensure that the overall service will sustain

146
00:09:52,966 --> 00:09:56,614
the target load while also matching the defined slos

147
00:09:56,662 --> 00:10:00,186
on response time and error rate. Service efficiency is

148
00:10:00,208 --> 00:10:03,594
also very important. In this case, we want CTO minimize the overall

149
00:10:03,642 --> 00:10:07,342
cost so that we want to assign more resources that are actually

150
00:10:07,396 --> 00:10:11,022
needed. Also, as a srEs, we want to keep

151
00:10:11,076 --> 00:10:15,010
operational, toil minimal and stay aligned to the delivery milestones.

152
00:10:15,510 --> 00:10:19,022
So this is the overall architecture. The parameter

153
00:10:19,086 --> 00:10:22,542
that we are optimizing is the example are the cpu and memory limits

154
00:10:22,606 --> 00:10:26,062
of each single microservice. These parameters

155
00:10:26,126 --> 00:10:29,286
are applied via the Kubernetes APIs in

156
00:10:29,308 --> 00:10:32,982
order to assess the performance of the overall application with respect to the different

157
00:10:33,036 --> 00:10:37,486
sizes of the containers. We leverage the built in locust load injector.

158
00:10:37,618 --> 00:10:41,706
We then leverage Prometheus and the ISIO service mesh, respectively to

159
00:10:41,728 --> 00:10:45,050
gather pod resource consumption metrics and service level

160
00:10:45,120 --> 00:10:49,158
metrics. The whole optimization process is completely automated.

161
00:10:49,334 --> 00:10:53,326
So what is the goal of this optimization? In our example,

162
00:10:53,428 --> 00:10:57,166
we aim at increasing the efficiency of the online boutique application,

163
00:10:57,348 --> 00:11:00,714
that is, to both increase the service throughput and decrease

164
00:11:00,762 --> 00:11:04,306
the cloud cost. At the same time, we want our service

165
00:11:04,408 --> 00:11:08,174
to always meet its reliability targets, which are expressed

166
00:11:08,222 --> 00:11:11,010
as latency and error rate slos.

167
00:11:11,510 --> 00:11:15,698
Therefore, our optimization goal is to maximize the ratio between

168
00:11:15,784 --> 00:11:18,674
service throughput and the overall service cost.

169
00:11:18,792 --> 00:11:22,046
Service throughput is measured by ISTIO at the front end

170
00:11:22,088 --> 00:11:25,042
layer where all user traffic is processed.

171
00:11:25,186 --> 00:11:28,214
The service cost is the cloud cost that we will pay

172
00:11:28,252 --> 00:11:32,022
to run the application on the cloud considering the cpu and memory

173
00:11:32,086 --> 00:11:35,030
resources allocated to each microservices.

174
00:11:35,190 --> 00:11:39,046
In this example, we cause the pricing of AWS Fargate,

175
00:11:39,158 --> 00:11:42,730
which is a serverless Kubernetes offering by AWS,

176
00:11:42,890 --> 00:11:47,130
which charges $29 per month for each cpu requested

177
00:11:47,210 --> 00:11:50,938
and about $3 per month for each gigabyte of memory.

178
00:11:51,114 --> 00:11:54,754
The machine learning algorithm that we have implemented at akamas also

179
00:11:54,792 --> 00:11:58,274
allow to set constraint on which configuration are

180
00:11:58,312 --> 00:12:01,774
acceptable. In this case, we state that configuration

181
00:12:01,822 --> 00:12:05,762
should have a maximum 90 percentile latency of 500

182
00:12:05,816 --> 00:12:09,446
milliseconds and a error rate lower than 2%.

183
00:12:09,628 --> 00:12:13,814
That is it. At this point, the machine learning based optimization can

184
00:12:13,852 --> 00:12:17,478
run automatically while we are relaxing or doing some other important

185
00:12:17,564 --> 00:12:21,162
stuff. Let's see what results we got. In this

186
00:12:21,216 --> 00:12:24,940
chart, each dot represents the cost efficiency of the application,

187
00:12:25,390 --> 00:12:28,794
which is the service throughput divided by the cloud cost

188
00:12:28,992 --> 00:12:32,266
as a result of different configuration of microservices,

189
00:12:32,378 --> 00:12:36,062
cpu and memory limits as chosen by the machine learning

190
00:12:36,196 --> 00:12:40,170
optimizer. The result of the machine learning based optimization

191
00:12:40,250 --> 00:12:43,554
is quite astonishing. A configuration improving the cost

192
00:12:43,592 --> 00:12:47,726
efficiency by 77% was automatically identified

193
00:12:47,838 --> 00:12:51,598
in about 24 hours. The baseline configuration,

194
00:12:51,694 --> 00:12:55,610
which correspond to the initial sizing of the microservices,

195
00:12:55,790 --> 00:12:59,666
has a cost efficiency of 00:29 transaction

196
00:12:59,698 --> 00:13:04,002
per second per dollar per month the best configuration ML

197
00:13:04,066 --> 00:13:08,542
machine learning found by tuning microservice resources achieved

198
00:13:08,626 --> 00:13:12,182
00:52 transaction per second per dollar

199
00:13:12,246 --> 00:13:15,414
per month. This chart also helps understanding

200
00:13:15,462 --> 00:13:19,098
how our machine learning based optimization works by learning

201
00:13:19,184 --> 00:13:22,686
from each tested configuration to quickly converge toward the

202
00:13:22,708 --> 00:13:25,774
optimal configurations. At this point, I guess

203
00:13:25,812 --> 00:13:29,774
you may want to know what is this best configuration looks like?

204
00:13:29,972 --> 00:13:33,442
Let's inspect the best configuration by focusing on the ten most

205
00:13:33,496 --> 00:13:37,022
important parameters. It's interesting to notice

206
00:13:37,086 --> 00:13:41,246
how our machine learning based optimization reduced cpu

207
00:13:41,278 --> 00:13:44,658
limits for many microservices. This is clearly a

208
00:13:44,664 --> 00:13:47,702
winning move from a cost perspective. However,

209
00:13:47,836 --> 00:13:51,938
it also automatically learned that two particular microservices,

210
00:13:52,114 --> 00:13:55,622
the product catalog and the recommendation, were under

211
00:13:55,676 --> 00:13:59,246
provision. Thus, it increased both their assigned cpu

212
00:13:59,298 --> 00:14:03,382
and memory. All these changes at the microservice level were critical

213
00:14:03,446 --> 00:14:07,210
to achieve our optimization goals that we set on the overall higher

214
00:14:07,280 --> 00:14:10,762
level service, which is to maximize the throughput and lower

215
00:14:10,816 --> 00:14:14,414
cost while still matching the slos. Let's now

216
00:14:14,452 --> 00:14:18,554
see how the overall service performance changes when the best configuration

217
00:14:18,602 --> 00:14:22,426
is applied. The chart on the left shows how the base and

218
00:14:22,468 --> 00:14:26,014
best configurations compare in terms of service throughput

219
00:14:26,142 --> 00:14:30,094
interacting. Besides being much more cost efficient,

220
00:14:30,222 --> 00:14:34,580
the best configuration also improved throughput by 19%.

221
00:14:34,890 --> 00:14:38,610
On the right, we are comparing the service 90 percentile

222
00:14:38,690 --> 00:14:41,922
response time. The best configuration cut the latency

223
00:14:41,986 --> 00:14:46,210
peaks by 60% and made the service latency

224
00:14:46,290 --> 00:14:49,654
much more stable. Let's now consider another use

225
00:14:49,692 --> 00:14:52,854
case of the same target system. Here the

226
00:14:52,892 --> 00:14:56,854
SRes challenge is how to tune the JVM options of the critical

227
00:14:56,902 --> 00:15:00,794
microservice so as to ensure the required service can support

228
00:15:00,912 --> 00:15:04,506
a high target load, a higher target load that is expected

229
00:15:04,618 --> 00:15:08,318
while still matching the defined slos. The container size

230
00:15:08,404 --> 00:15:11,806
is not changed here as we don't want to increase cost

231
00:15:11,988 --> 00:15:15,266
again, we want to keep operational to a minimum and be quick to

232
00:15:15,288 --> 00:15:19,106
stay aligned to product launch milestones. As mentioned,

233
00:15:19,208 --> 00:15:22,446
the target architecture is the same with respect

234
00:15:22,478 --> 00:15:26,774
to the previous picture. Here we are emphasizing the specific JVM options,

235
00:15:26,892 --> 00:15:31,522
about 32 that are taking into account, and the specific microservice,

236
00:15:31,586 --> 00:15:34,802
the ad service that is being targeted.

237
00:15:34,946 --> 00:15:39,466
Let's see how the optimization goal and constraints are defined in this case.

238
00:15:39,648 --> 00:15:44,054
The goal for this use case is CTO increase the number of successful transaction processed

239
00:15:44,102 --> 00:15:47,850
by the Ad microservice. The SLO for this service

240
00:15:47,920 --> 00:15:52,938
is the average response time which should be kept no higher than 100 millisecond.

241
00:15:53,114 --> 00:15:56,670
Let's see the results machines learning achieved for this optimization.

242
00:15:57,010 --> 00:16:01,034
This chart showed the service throughput and response time under increasing

243
00:16:01,082 --> 00:16:04,610
level of load during a load test. First of all,

244
00:16:04,680 --> 00:16:08,462
consider the blue and green lines representing the microservices

245
00:16:08,526 --> 00:16:11,966
throughput and response time for the baseline configuration.

246
00:16:12,158 --> 00:16:15,702
As you can see, the baseline configuration reaches 74

247
00:16:15,756 --> 00:16:19,986
transaction per second before violating the 100 milliseconds

248
00:16:20,018 --> 00:16:24,070
low on response time. Let's now look at the best configuration.

249
00:16:24,650 --> 00:16:28,534
The throughput, which is the black line, reaches 95

250
00:16:28,652 --> 00:16:32,822
transaction per seconds before violating the response time slos.

251
00:16:32,966 --> 00:16:36,886
Therefore, the best configurations identified by machine learning power optimization

252
00:16:36,998 --> 00:16:40,986
provides a 28% increase in transaction per second

253
00:16:41,168 --> 00:16:44,618
while also meeting the defined survey level objectives.

254
00:16:44,794 --> 00:16:47,774
So what makes this configuration so good?

255
00:16:47,972 --> 00:16:52,590
The maximum heap memory was increased at 250%

256
00:16:52,660 --> 00:16:56,622
increase. Actually. It's also interesting to notice that the garbage collection

257
00:16:56,686 --> 00:17:00,482
type was changed from g one to parallel in this case,

258
00:17:00,616 --> 00:17:04,318
and machines learning also decreased the number of garbage collection threads

259
00:17:04,414 --> 00:17:07,854
from a to three as we got the parallel garbage collection thread,

260
00:17:07,902 --> 00:17:11,814
and from a to one as we got the concurrent ones. Machine loading also

261
00:17:11,852 --> 00:17:16,162
adjusted the IP regions and the object aging threshold to maximize

262
00:17:16,226 --> 00:17:19,274
performance. As for the previous case, we also

263
00:17:19,312 --> 00:17:23,334
got eight JVM options automatically selected as the most impactful

264
00:17:23,382 --> 00:17:26,982
with respect to the dozens and dozens, hundreds actually of potential

265
00:17:27,046 --> 00:17:30,410
JVM options to be considered. All right,

266
00:17:30,560 --> 00:17:34,334
it is time to conclude with our takeaways. Our first

267
00:17:34,372 --> 00:17:38,126
takeaway is that tuning modern application is a complex problem that

268
00:17:38,148 --> 00:17:42,074
is hard to solve. Any traditional tuning approach cause a relevant

269
00:17:42,122 --> 00:17:45,934
toil for SRE teams. A second takeaway is

270
00:17:45,972 --> 00:17:49,242
that when our new approach is based on fully automated

271
00:17:49,306 --> 00:17:53,242
machine learning based optimization, it becomes possible for SRE

272
00:17:53,306 --> 00:17:56,694
to really ensure that application with have higher performance and

273
00:17:56,732 --> 00:18:00,054
reliability. And third, that this new

274
00:18:00,092 --> 00:18:03,926
approach also make it possible to reduce the operational toys and

275
00:18:03,948 --> 00:18:08,326
stay aligned to release milestone. A huge improvement for rolexer is

276
00:18:08,508 --> 00:18:11,558
many thanks for your time. I hope you enjoyed the talk.

277
00:18:11,724 --> 00:18:15,602
Please send me any comment and question by leverages the conference

278
00:18:15,746 --> 00:18:19,090
discard channels or these contacts.

