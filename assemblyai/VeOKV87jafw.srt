1
00:00:00,490 --> 00:00:04,506
Okay, so this session is all about reimagining application networking

2
00:00:04,538 --> 00:00:08,074
and security, and I'm going to introduce some new solutions

3
00:00:08,122 --> 00:00:11,850
in this space specifically oriented around cloud native

4
00:00:11,930 --> 00:00:15,582
application networking as well as cloud native application

5
00:00:15,716 --> 00:00:19,354
security. Let's get right into it. These overall

6
00:00:19,402 --> 00:00:22,814
agenda I have three main things I want to talk

7
00:00:22,852 --> 00:00:26,262
about. First of all, how did we get here? Why is there even a need

8
00:00:26,396 --> 00:00:30,022
to reimagining the way we've approached application networking and

9
00:00:30,076 --> 00:00:33,878
security from what we've been doing in the past 1020,

10
00:00:33,964 --> 00:00:37,266
30 years? Then I'm going to deep

11
00:00:37,298 --> 00:00:41,446
dive a little more, double click on what we're doing in the application networking

12
00:00:41,478 --> 00:00:45,078
space. What are the challenges that are unique to that environment,

13
00:00:45,254 --> 00:00:48,970
as well as these, what are solutions that we're bringing forward and these

14
00:00:49,040 --> 00:00:52,106
similarly doing the same exercise. But now in the security

15
00:00:52,208 --> 00:00:55,566
space, we at Cisco have been playing in

16
00:00:55,588 --> 00:00:59,114
both spaces for over 30 years, so we have a lot of expertise

17
00:00:59,162 --> 00:01:02,478
and thought leadership. But how do we apply it to the problems

18
00:01:02,564 --> 00:01:06,402
presented in this new cloud native environment? That's basically these running

19
00:01:06,456 --> 00:01:09,970
theme that you'll see. And finally, I'll summarize everything and just

20
00:01:10,040 --> 00:01:13,714
call out some key takeaways. Let's get right into it then.

21
00:01:13,912 --> 00:01:17,170
So how did we get here? We've seen that traditionally

22
00:01:17,510 --> 00:01:21,254
application has been very tightly coupled to hardware. You wanted

23
00:01:21,292 --> 00:01:24,598
to spin up a new application, you'd have

24
00:01:24,604 --> 00:01:28,298
to stack and rack a new server, install an operating systems,

25
00:01:28,384 --> 00:01:31,180
and then that would run your app. Well,

26
00:01:31,550 --> 00:01:35,334
a big leap forward was taken when we introduced

27
00:01:35,382 --> 00:01:39,754
virtualization technologies that had then the ability to

28
00:01:39,952 --> 00:01:43,482
share a hardware, to say, let's virtualize a hardware

29
00:01:43,546 --> 00:01:46,974
so that you cloud run multiple applications on these

30
00:01:47,012 --> 00:01:50,206
same physical hardware. And that way you didn't have to have

31
00:01:50,228 --> 00:01:54,274
a second or third or multiple instances of

32
00:01:54,312 --> 00:01:57,154
actual physical hardware per each application.

33
00:01:57,352 --> 00:02:00,658
So that as you scaled up and scaled down, you maintain a lot

34
00:02:00,664 --> 00:02:04,606
of efficiency. However, this approach, while it's very flexible

35
00:02:04,638 --> 00:02:08,098
and extensible, it did require a lot of extra overhead.

36
00:02:08,194 --> 00:02:11,878
So if you see here, we have multiple layers of abstraction in

37
00:02:11,884 --> 00:02:15,286
the forms of an operating system for the hardware itself, as well

38
00:02:15,308 --> 00:02:18,774
as virtual machine operating systems. All of these

39
00:02:18,812 --> 00:02:22,486
then would have to be licensed. All of these then would have to be patched,

40
00:02:22,518 --> 00:02:26,266
et cetera. There was a lot more extra overhead as well as

41
00:02:26,288 --> 00:02:29,986
then ultimately the performance overhead, because you're going through all these layers

42
00:02:30,038 --> 00:02:34,110
of abstraction. So an improvement was made here by

43
00:02:34,260 --> 00:02:38,270
virtualizing not the hardware, but the operating system

44
00:02:38,340 --> 00:02:41,354
itself. This was the approach used by containers,

45
00:02:41,482 --> 00:02:44,914
and it's far more efficient, far more effective, and then it

46
00:02:44,952 --> 00:02:48,980
really led to a change in how applications themselves

47
00:02:49,510 --> 00:02:52,882
were developed. For instance, instead of having

48
00:02:52,936 --> 00:02:57,282
what are now known as monolithic application architectures,

49
00:02:57,426 --> 00:03:00,934
these every component within the application would

50
00:03:00,972 --> 00:03:04,774
reside within the application. You have these huge libraries and

51
00:03:04,812 --> 00:03:08,390
files, et cetera. Well, all of these then could be

52
00:03:08,540 --> 00:03:12,154
split apart into atomic parts, self contained and

53
00:03:12,192 --> 00:03:15,910
containerized individually, but then interconnecting

54
00:03:15,990 --> 00:03:19,434
either with application programming interfaces or

55
00:03:19,472 --> 00:03:22,922
direct protocol connections like TCP

56
00:03:22,986 --> 00:03:26,510
or GRPC or whatever the case may be, really doesn't matter.

57
00:03:26,660 --> 00:03:30,126
But the point is that this way you

58
00:03:30,148 --> 00:03:34,002
could bring a lot of benefits. For instance, you don't have to

59
00:03:34,136 --> 00:03:36,882
upgrade an entire application all at once.

60
00:03:37,016 --> 00:03:40,386
You'd have benefits such as portability. You could

61
00:03:40,408 --> 00:03:44,050
have some of these services running on premise,

62
00:03:44,470 --> 00:03:47,606
or some running in the cloud, or you could have some of these running in

63
00:03:47,628 --> 00:03:51,346
one cloud provider and another in another cloud provider.

64
00:03:51,378 --> 00:03:55,122
You have maximum flexibility that way. You could also ensure

65
00:03:55,186 --> 00:03:58,902
continuous delivery. Say okay, I want to upgrade one

66
00:03:58,956 --> 00:04:02,186
service. Well, traditionally when we upgrade an application, you'd have to

67
00:04:02,208 --> 00:04:05,270
take it down, you'd have to install the update,

68
00:04:05,430 --> 00:04:08,534
boot it up, and then you'd have this planned outage.

69
00:04:08,662 --> 00:04:12,166
Well you can do this continuously now by saying, you know what, if I'm upgrading

70
00:04:12,198 --> 00:04:15,518
a specific service when it's ready, I spin that up in a

71
00:04:15,524 --> 00:04:18,030
container and I just change my pointers.

72
00:04:18,450 --> 00:04:21,946
Rollbacks are just as easy. Let's say there's an issue with version

73
00:04:21,978 --> 00:04:26,402
20 of this ML app. I just roll that back to 10

74
00:04:26,536 --> 00:04:29,214
and again it fits instant.

75
00:04:29,342 --> 00:04:33,394
And then when I'm ready for my modified upgrade, I just go

76
00:04:33,432 --> 00:04:37,570
ahead. So tremendous amount of availability realized

77
00:04:37,730 --> 00:04:40,550
through this continuous delivery architectures.

78
00:04:40,890 --> 00:04:44,326
Not only that then, but scalability itself. So when

79
00:04:44,348 --> 00:04:48,202
we containerize these services, we can describe these entire application

80
00:04:48,336 --> 00:04:52,070
environment in a YAML file, we feed that into Kubernetes,

81
00:04:52,150 --> 00:04:56,282
which in turn wraps all these containers and pods and

82
00:04:56,336 --> 00:04:59,946
then manages the orchestration via a

83
00:05:00,048 --> 00:05:03,726
control plane that says, you know what, I'm going to assign work to a

84
00:05:03,748 --> 00:05:07,754
series of worker nodes and I will spin up and spin

85
00:05:07,802 --> 00:05:11,054
down the services as I need them dynamically and

86
00:05:11,092 --> 00:05:14,594
continuously and per my application

87
00:05:14,712 --> 00:05:17,730
requirements or even per the health and availability,

88
00:05:18,550 --> 00:05:22,034
maintaining the environment to whatever was declared in

89
00:05:22,072 --> 00:05:25,734
that Kubernetes file. Okay, so that's just a very high

90
00:05:25,772 --> 00:05:28,886
level overview of how

91
00:05:28,908 --> 00:05:32,322
the application architectures have evolved. It's very simplified,

92
00:05:32,466 --> 00:05:36,194
but it's just for the point of laying some context

93
00:05:36,242 --> 00:05:40,294
for our discussion now to follow. So as we go and look deeper

94
00:05:40,342 --> 00:05:43,482
into what are the challenges that are presented with these new cloud

95
00:05:43,536 --> 00:05:46,810
native architectures specific to networking,

96
00:05:47,150 --> 00:05:50,902
and then we'll repeat for security, at least there's some context

97
00:05:50,966 --> 00:05:54,414
laid there. So as we brought, but we

98
00:05:54,452 --> 00:05:57,806
said, okay, now we have all these microservices within an

99
00:05:57,828 --> 00:06:01,102
application all needing to be interconnecting. And these

100
00:06:01,156 --> 00:06:04,402
may be on Prem, they may be in the cloud, they may be in different

101
00:06:04,456 --> 00:06:07,410
cloud providers, there's all sorts of variations.

102
00:06:07,830 --> 00:06:11,278
Now all of these, then these interconnections

103
00:06:11,454 --> 00:06:14,862
have to be managed and these are net new interconnections

104
00:06:14,926 --> 00:06:19,042
because previously all of these services would be on a single server,

105
00:06:19,186 --> 00:06:23,122
virtual or physical, it doesn't matter, but you wouldn't have to interconnect

106
00:06:23,186 --> 00:06:26,594
them. So now in addition to providing and managing

107
00:06:26,642 --> 00:06:30,250
all your external connections, there's a whole

108
00:06:30,320 --> 00:06:33,674
new series of internal connections that need to be

109
00:06:33,712 --> 00:06:37,146
similarly managed, that need to be authenticated, that need to

110
00:06:37,168 --> 00:06:40,998
be encrypted, that need to be observed, and the traffic and the

111
00:06:41,024 --> 00:06:44,106
loads need to be managed on these, et cetera, et cetera,

112
00:06:44,138 --> 00:06:47,902
et cetera. So all the application challenges you had,

113
00:06:48,036 --> 00:06:50,922
areas now significantly increased,

114
00:06:50,986 --> 00:06:54,834
doubled, maybe even more. It all depends on the complexity of the application

115
00:06:54,952 --> 00:06:58,946
architectures itself. Okay, so this is

116
00:06:58,968 --> 00:07:02,654
time consuming. Obviously this is error prone,

117
00:07:02,782 --> 00:07:06,242
and if it's done individually, it can lead to a lot of inconsistencies,

118
00:07:06,306 --> 00:07:09,590
et cetera. Enter then the service

119
00:07:09,660 --> 00:07:13,046
mesh. So many customers have found that this is a

120
00:07:13,068 --> 00:07:16,514
valuable way then of in maintaining consistent

121
00:07:16,562 --> 00:07:20,214
security. You have one set of encryption

122
00:07:20,262 --> 00:07:23,754
policies for your entire cluster. You have the ability to

123
00:07:23,792 --> 00:07:27,626
observe the traffic pre and post encryption so

124
00:07:27,648 --> 00:07:31,354
that you can see what's actually going on. You can manage the

125
00:07:31,392 --> 00:07:34,986
traffic according to your loads or your policies, et cetera.

126
00:07:35,098 --> 00:07:39,434
And so a lot of benefits are presented with a service mesh architecture,

127
00:07:39,562 --> 00:07:43,342
istio being one of the most popular, but there's others like Linkerd

128
00:07:43,406 --> 00:07:46,770
and so on. But service meshes, like many

129
00:07:46,840 --> 00:07:50,418
technologies, they present some great advantages, but these, at the

130
00:07:50,424 --> 00:07:53,170
same time, as we continue to push these envelope,

131
00:07:53,830 --> 00:07:57,298
are presenting challenges themselves. So some of

132
00:07:57,304 --> 00:08:00,914
the challenges that I'm going to talk about are the lifecycle management of the mesh

133
00:08:00,962 --> 00:08:04,150
itself, how that introduces a lot more work

134
00:08:04,220 --> 00:08:07,910
than was previously needed for managing network connections,

135
00:08:08,410 --> 00:08:11,686
observability challenges, as well as multicluster

136
00:08:11,718 --> 00:08:15,594
challenges and advanced use based challenges. So I'm going to deal with these

137
00:08:15,632 --> 00:08:19,366
one at a time. Let's start with the lifecycle management

138
00:08:19,478 --> 00:08:23,386
in a network. If you wanted to have more functionality from that network,

139
00:08:23,498 --> 00:08:27,354
well then you'd have to upgrade the devices that constitute that network,

140
00:08:27,402 --> 00:08:31,402
the routers, the switches, et cetera, get these latest software versions

141
00:08:31,466 --> 00:08:35,058
on them, and then you have the new capabilities you might have to

142
00:08:35,064 --> 00:08:39,422
do this manually or more recently using controllers

143
00:08:39,486 --> 00:08:43,294
that would manage the software so that you just have everything all kept

144
00:08:43,342 --> 00:08:47,102
to a single standard, golden images, et cetera,

145
00:08:47,246 --> 00:08:51,094
so you can automate that. However, it might be tempting to think

146
00:08:51,132 --> 00:08:54,754
that, okay then, if I want to upgrade a service mesh from one version

147
00:08:54,802 --> 00:08:58,566
to another, it's the same exercise. I just upgrade

148
00:08:58,598 --> 00:09:01,626
my mesh and I'm done. Well, not quite,

149
00:09:01,728 --> 00:09:05,670
because there's a cloud native principle of immutable

150
00:09:05,750 --> 00:09:09,306
infrastructure, which basically says once you establish the

151
00:09:09,328 --> 00:09:13,242
infrastructure which includes these mesh and all the interconnections,

152
00:09:13,386 --> 00:09:17,040
it cannot be changed. And as such,

153
00:09:17,570 --> 00:09:21,326
we have to use a new approach. And the new approach, the way I

154
00:09:21,348 --> 00:09:24,714
like to liken it or compare it, is to changing

155
00:09:24,762 --> 00:09:28,238
a tablecloth on a restaurant. Say you're at a fancy restaurant

156
00:09:28,334 --> 00:09:31,518
and you've been eating, you have all your plates and your wine and et

157
00:09:31,534 --> 00:09:34,130
cetera all on the table, and the tablecloth,

158
00:09:34,710 --> 00:09:38,686
maybe some wine spills on it. Well, how do you replace that tablecloth?

159
00:09:38,798 --> 00:09:42,166
You don't just rip out the old one and try and get a new one

160
00:09:42,268 --> 00:09:45,158
and maybe fastened to the old one as you rip it out.

161
00:09:45,324 --> 00:09:48,582
No, what you do is you set a new table is set.

162
00:09:48,636 --> 00:09:52,374
So an entirely new table with the new dishes,

163
00:09:52,422 --> 00:09:56,234
with the new wine glasses, everything is moved over. And once it's in place,

164
00:09:56,352 --> 00:09:59,674
then actually you also get moved over. So it's the same way

165
00:09:59,712 --> 00:10:03,318
with a service mesh. You lay out a new mesh, you lay out new

166
00:10:03,344 --> 00:10:06,622
instances of the services on that mesh, and then when they're ready,

167
00:10:06,676 --> 00:10:10,314
then you gradually redirect the workloads to the new mesh.

168
00:10:10,362 --> 00:10:14,402
When the confidence is there that everything's working as it should be and

169
00:10:14,536 --> 00:10:17,890
everything is looking good, you can decommission the old mesh.

170
00:10:18,470 --> 00:10:22,162
It's a lot more complicated. And then when you think about

171
00:10:22,296 --> 00:10:26,034
that, when it comes to service meshes, they typically only have

172
00:10:26,072 --> 00:10:29,570
three month lifecycles with only two supported versions,

173
00:10:29,730 --> 00:10:33,714
the current version and these previous. So if you have a production

174
00:10:33,762 --> 00:10:37,442
environment and you want to be on a supported version, this forces

175
00:10:37,506 --> 00:10:41,226
you to upgrade your mesh every three months. And this is done on a

176
00:10:41,248 --> 00:10:45,142
cluster by cluster basis. We deal with customers that have dozens,

177
00:10:45,206 --> 00:10:49,526
if not some have over 100 clusters that they're

178
00:10:49,558 --> 00:10:52,798
managing. And as such, that presents a lot of

179
00:10:52,964 --> 00:10:56,794
toil. So being able to manage that in an efficient

180
00:10:56,842 --> 00:11:00,286
and automated manner is very valuable and can remove all

181
00:11:00,308 --> 00:11:03,458
that toil. A second area of challenge that we want to

182
00:11:03,464 --> 00:11:07,006
touch on is observability. There's a lot of great tools

183
00:11:07,038 --> 00:11:10,434
out in the open source community to give us observability. So you

184
00:11:10,472 --> 00:11:15,838
have prometheus Grafana for Matrix, you have kiali

185
00:11:15,854 --> 00:11:18,966
for topology, you have Jaeger for traces. But then if

186
00:11:18,988 --> 00:11:22,818
you're troubleshooting as an operator, you have to constantly go from one pane

187
00:11:22,834 --> 00:11:26,758
of glass to another pane of glass, and stitch all that information together

188
00:11:26,924 --> 00:11:31,194
can really slow down your troubleshooting and makes it much

189
00:11:31,232 --> 00:11:34,826
more difficult. Whereas if you have to repeat, and all

190
00:11:34,848 --> 00:11:38,266
of these tools, incidentally, are cluster by cluster tools, if you

191
00:11:38,288 --> 00:11:42,558
have multiclusters, then to correlate that information,

192
00:11:42,724 --> 00:11:46,350
aggregate that information, and then integrate that information

193
00:11:46,500 --> 00:11:50,218
areas, presenting some real challenges so that you can troubleshoot

194
00:11:50,234 --> 00:11:53,342
as efficiently as possible. Also,

195
00:11:53,476 --> 00:11:57,458
I talk about advanced use cases. So for instance, we have

196
00:11:57,624 --> 00:12:00,978
what I displayed earlier when upgrading to one version of a

197
00:12:00,984 --> 00:12:04,890
service to another was I illustrated what's called a blue green deployment.

198
00:12:04,990 --> 00:12:08,182
It's just a simple digital cutover. It was

199
00:12:08,236 --> 00:12:12,118
pointing here, now it's pointing there. Now there's a

200
00:12:12,124 --> 00:12:15,234
way that this type of upgrade can be derisked,

201
00:12:15,282 --> 00:12:19,474
and that's termed canary deployment with traffic management.

202
00:12:19,522 --> 00:12:22,666
So rather than just sending all of the traffic to

203
00:12:22,688 --> 00:12:26,314
the new version of the service we could manage, we say, hey,

204
00:12:26,352 --> 00:12:30,198
why don't we start out just sending 20% of the traffic to the

205
00:12:30,224 --> 00:12:34,074
new service, see how it performs, see if there's any unexpected

206
00:12:34,122 --> 00:12:36,922
issues. And these, as our confidence level increases,

207
00:12:36,986 --> 00:12:40,800
we increase these amount of traffic that's directed there.

208
00:12:41,490 --> 00:12:44,802
Another advanced use case is the circuit breaker use case,

209
00:12:44,936 --> 00:12:48,306
and that is now not managing versions of an app, but the

210
00:12:48,328 --> 00:12:51,554
service itself and monitoring its health.

211
00:12:51,672 --> 00:12:55,694
And we can set thresholds and say, below a given threshold,

212
00:12:55,822 --> 00:12:59,766
if the health of that service degrades or deteriorates, we're just

213
00:12:59,788 --> 00:13:03,014
going to cut that service so that no more workloads are

214
00:13:03,052 --> 00:13:06,840
directed to it, and these receive a poor application experience.

215
00:13:07,610 --> 00:13:10,886
As such, we see that when managing a service mesh,

216
00:13:10,998 --> 00:13:15,142
your management solution has a number of requirements. The basic lifecycle

217
00:13:15,206 --> 00:13:19,094
management, managing the security and the encryption of the mesh

218
00:13:19,222 --> 00:13:23,098
provide observability optimally, an integrated observability,

219
00:13:23,194 --> 00:13:26,526
like we're talking about learning about the environment, so that

220
00:13:26,548 --> 00:13:30,190
you know what is normal, what is abnormal, and you can set service level

221
00:13:30,260 --> 00:13:33,642
objectives and then enabling advanced use cases

222
00:13:33,706 --> 00:13:37,166
like circuit breaking or canary deployments, et cetera.

223
00:13:37,278 --> 00:13:41,106
And so to this end, we have a solution service mesh manager that

224
00:13:41,128 --> 00:13:45,634
not only meets all these requirements, like there's a few other offerings

225
00:13:45,682 --> 00:13:49,586
too on the market, but also has a number of unique differentiators.

226
00:13:49,698 --> 00:13:53,030
And again, this is where we're bringing, for instance,

227
00:13:53,610 --> 00:13:57,206
our 30 plus years of networking experience

228
00:13:57,388 --> 00:14:00,874
to now the cloud native domain, for instance. One of the things

229
00:14:00,912 --> 00:14:04,442
that we do that's completely unique is to be able to support

230
00:14:04,576 --> 00:14:07,322
active, active control planes, to say,

231
00:14:07,376 --> 00:14:10,954
even across service meshes that span clusters.

232
00:14:11,082 --> 00:14:14,974
Instead of just having a single primary like a

233
00:14:15,012 --> 00:14:18,222
hot versus a standby control plane, we can

234
00:14:18,276 --> 00:14:21,870
support a multi primary control plane, which means both control

235
00:14:21,940 --> 00:14:26,006
planes areas active, and that maximizes the redundancy

236
00:14:26,138 --> 00:14:29,554
of that service mesh, even across clusters, as well

237
00:14:29,592 --> 00:14:32,946
as takes care of all these service discovery, et cetera. We're the only ones

238
00:14:32,968 --> 00:14:36,834
that can do this, or for instance, taking advantage

239
00:14:36,882 --> 00:14:40,818
of our years of expertise in providing multitenancy segmentation

240
00:14:40,914 --> 00:14:43,766
segregation solutions to recognize that,

241
00:14:43,868 --> 00:14:47,874
okay, there could be multiple customers

242
00:14:48,012 --> 00:14:51,594
leveraging these same resources, or even departments or any

243
00:14:51,632 --> 00:14:55,350
other logical group. How do we maximize the flexibility

244
00:14:55,430 --> 00:14:59,142
of how the traffic is separated across shared resources,

245
00:14:59,206 --> 00:15:02,686
even in these type of environments? And so we're the only ones that

246
00:15:02,708 --> 00:15:06,666
support multi gateways per service mesh

247
00:15:06,698 --> 00:15:10,666
per cluster, as well as direct connections, so that you could have an external client

248
00:15:10,778 --> 00:15:14,830
connecting directly to a workload. The maximum flexibility

249
00:15:14,910 --> 00:15:19,298
when it comes to these types of interconnections. And these also we

250
00:15:19,384 --> 00:15:23,378
have support for asynchronous microservices. We see that

251
00:15:23,544 --> 00:15:27,602
service meshes typically are optimized for request

252
00:15:27,666 --> 00:15:33,842
reply communications, which are synchronous

253
00:15:33,986 --> 00:15:38,214
communications, whereas these are some communications and some applications,

254
00:15:38,262 --> 00:15:41,878
notably approaches kafka, that areas event driven.

255
00:15:41,974 --> 00:15:45,702
So when something happens, then communication is triggered

256
00:15:45,766 --> 00:15:48,780
and it's not synchronous. And as such,

257
00:15:50,110 --> 00:15:54,238
some competitors or some others have used an event based,

258
00:15:54,324 --> 00:15:58,186
an event mesh approach to say you have your service mesh for your synchronous.

259
00:15:58,298 --> 00:16:02,710
Now we have a completely different mesh for asynchronous apps,

260
00:16:02,810 --> 00:16:06,926
whereas that's very redundant. That's a lot of extra architectures

261
00:16:07,118 --> 00:16:11,086
and infrastructure delay, whereas what we do is we optimize

262
00:16:11,198 --> 00:16:14,910
and we've optimized, for instance, specifically istio service

263
00:16:15,000 --> 00:16:18,594
mesh to support both synchronous and asynchronous

264
00:16:18,642 --> 00:16:22,566
communications. And again, we're the only ones that have done this. So a lot of

265
00:16:22,668 --> 00:16:26,280
specific key differentiators in this space.

266
00:16:27,610 --> 00:16:30,854
Let me now show a demonstration of Cisco

267
00:16:30,902 --> 00:16:34,422
service mesh manager. So let's take a look at service Mesh

268
00:16:34,486 --> 00:16:38,262
manager. Service Mesh manager is an istio distribution,

269
00:16:38,326 --> 00:16:41,930
an enterprise grade istio distribution that enables you

270
00:16:42,000 --> 00:16:45,322
a lot of additional features that you wouldn't get with just basic

271
00:16:45,386 --> 00:16:49,022
out of the box istio. We get advantages in terms

272
00:16:49,076 --> 00:16:52,414
of mesh management, integrated observability and

273
00:16:52,452 --> 00:16:56,354
advanced use cases like traffic management circuit breakers or

274
00:16:56,392 --> 00:17:00,478
canary deployments. We're going to look at all of these. In our quick little overview,

275
00:17:00,574 --> 00:17:04,306
here's our main dashboard. We see we have two clusters, 18 services,

276
00:17:04,488 --> 00:17:08,078
eleven workloads, most of them are healthy, but we have some issues.

277
00:17:08,184 --> 00:17:11,350
We don't want everything green for the sake of this demo. It's more interesting,

278
00:17:11,500 --> 00:17:15,362
it gives us these high level stats of our overall environment,

279
00:17:15,506 --> 00:17:18,566
but it's particularly more interesting once we look at

280
00:17:18,588 --> 00:17:22,086
things in, say, a topology view with an open source

281
00:17:22,118 --> 00:17:25,802
tools. You would have to use, say kiali for your topology view,

282
00:17:25,856 --> 00:17:29,494
and then Grafana for metrics and Jaeger for traces.

283
00:17:29,622 --> 00:17:33,434
And it would be cluster by cluster. But here everything is integrated,

284
00:17:33,562 --> 00:17:37,370
not just for the sake of observability, but also configuration.

285
00:17:37,530 --> 00:17:40,990
So I see my overall topology, I have a multi cluster topology,

286
00:17:41,490 --> 00:17:44,778
I have a master cluster, and I have a follower.

287
00:17:44,874 --> 00:17:49,374
They both happen to be in the same cloud provider, in this case AWS,

288
00:17:49,502 --> 00:17:53,006
but they could be on prem, they could be in different providers, et cetera,

289
00:17:53,038 --> 00:17:56,374
it doesn't matter. I get the bird's eye level view of everything.

290
00:17:56,572 --> 00:18:00,006
I can zoom in on any given item, whether it's a

291
00:18:00,028 --> 00:18:03,942
service or workload, et cetera. And I have a tremendous amount

292
00:18:04,076 --> 00:18:07,030
of information. The overall health,

293
00:18:07,180 --> 00:18:10,374
this one is very healthy service. Any service level objectives,

294
00:18:10,422 --> 00:18:13,834
I target the overall key metrics from here.

295
00:18:13,872 --> 00:18:17,610
I could launch, for instance, I could launch a Grafana dashboard

296
00:18:18,510 --> 00:18:22,126
for specifics about the given service or the object that

297
00:18:22,148 --> 00:18:25,726
I'm looking at. Or I can even launch Jaeger if I

298
00:18:25,748 --> 00:18:29,582
like. But having everything integrated gives me a lot

299
00:18:29,636 --> 00:18:33,126
more comprehensive information that's

300
00:18:33,178 --> 00:18:37,250
around that object without having to jump between

301
00:18:37,320 --> 00:18:40,590
tools. So I can not only for instance,

302
00:18:40,670 --> 00:18:44,210
get a view of how things are doing, but I can also change

303
00:18:44,280 --> 00:18:47,726
things. For example, if we're talking about mesh management,

304
00:18:47,838 --> 00:18:51,606
I can do things like inject faults, like for instance, if I

305
00:18:51,628 --> 00:18:54,930
want to take a look, I saw that this service was very healthy,

306
00:18:55,010 --> 00:18:58,662
but what if I start injecting a fault into it on purpose,

307
00:18:58,806 --> 00:19:02,586
to get a sense of, okay, what is that going to do to

308
00:19:02,608 --> 00:19:04,922
my overall environment? I'm going to say, okay,

309
00:19:04,976 --> 00:19:08,970
100% of transactions will now receive

310
00:19:09,710 --> 00:19:13,226
additional 2000 milliseconds of latency, 2 seconds

311
00:19:13,258 --> 00:19:16,862
of latency. What's the effect that that's going to have on that

312
00:19:16,916 --> 00:19:19,840
service? Well, I can now come over to health,

313
00:19:20,690 --> 00:19:24,002
and then within a few moments I'm going to start seeing the effect

314
00:19:24,056 --> 00:19:27,470
of that, particularly when I see latencies

315
00:19:27,550 --> 00:19:30,834
between clusters, I'm going to see that I'm getting

316
00:19:30,872 --> 00:19:34,482
this spike here that's going to just continue rolling out in

317
00:19:34,536 --> 00:19:37,990
real time and then overall degrade the

318
00:19:38,060 --> 00:19:41,958
performance and thus the health metric of that particular service.

319
00:19:42,044 --> 00:19:45,730
And I'm going to see the effects of that in my overall

320
00:19:45,810 --> 00:19:49,734
reporting. And on my dashboard, you can see here the spike in latency

321
00:19:49,782 --> 00:19:52,826
that I've injected. Now I

322
00:19:52,848 --> 00:19:56,294
can see things and configure things from this dashboard,

323
00:19:56,342 --> 00:20:00,054
but I can also have a timeline view. So for instance,

324
00:20:00,102 --> 00:20:03,486
I can see things, not just how they are, but I can go back in

325
00:20:03,508 --> 00:20:07,422
time, whether it's a topology view, or if I want to maybe

326
00:20:07,476 --> 00:20:11,246
zero in on specific services or workloads, I can not

327
00:20:11,268 --> 00:20:14,494
just see the state of affairs right now. But if

328
00:20:14,532 --> 00:20:17,474
I look at my timeline graph, I see that back here,

329
00:20:17,592 --> 00:20:20,786
I did have some issues. So I can go back in time, I can say,

330
00:20:20,808 --> 00:20:24,498
okay, what was the issues that I was experiencing there? And I

331
00:20:24,504 --> 00:20:28,214
can delve further into them. So for instance, I can see that the

332
00:20:28,332 --> 00:20:32,454
booking service here had some health issues.

333
00:20:32,652 --> 00:20:36,566
These overall, the error rates were fine, but the latencies were in

334
00:20:36,588 --> 00:20:40,406
the medium range as well as some of the error rates

335
00:20:40,438 --> 00:20:43,946
were quite high. So I can see what happened even in the

336
00:20:43,968 --> 00:20:48,106
past. So it gives me a tremendous amount of visibility into

337
00:20:48,208 --> 00:20:50,810
the overall health of my environment.

338
00:20:51,550 --> 00:20:55,150
Not only that, but if I return back to, say, my topology,

339
00:20:56,050 --> 00:20:59,294
I'm going to go live, come back to my live view, I'm going to see

340
00:20:59,332 --> 00:21:02,474
that, that service that I injected default is slowly

341
00:21:02,522 --> 00:21:05,746
turning to a lighter shade of green. It's going to

342
00:21:05,768 --> 00:21:09,746
eventually turn yellow and then orange. But some

343
00:21:09,768 --> 00:21:14,002
of the other things that I can do with my service

344
00:21:14,056 --> 00:21:17,922
mesh manager is to even direct traffic

345
00:21:18,066 --> 00:21:21,602
to align, to say, my overall use case objectives.

346
00:21:21,746 --> 00:21:25,606
For example, I have here three versions of the

347
00:21:25,628 --> 00:21:28,934
movie service, version one, version two, version three,

348
00:21:29,052 --> 00:21:33,222
and right now by default, it's about a 33% mixture

349
00:21:33,286 --> 00:21:36,522
of traffic for each of these. But I might say, you know what,

350
00:21:36,576 --> 00:21:41,018
I might want to have the majority of my traffic still going,

351
00:21:41,184 --> 00:21:45,038
let's say 60% of my traffic to version 130

352
00:21:45,124 --> 00:21:48,894
percent of my traffic to version two, and only 10% of my

353
00:21:48,932 --> 00:21:53,342
traffic to version three, to have like a canary style deployment,

354
00:21:53,406 --> 00:21:57,182
so that I'm testing the new versions incrementally

355
00:21:57,326 --> 00:22:00,786
without sacrificing the user experience. And then as I

356
00:22:00,888 --> 00:22:04,542
gain confidence, I can adjust and move the entire load

357
00:22:04,606 --> 00:22:08,006
over. And this is

358
00:22:08,028 --> 00:22:11,842
certainly easier to do than doing it via

359
00:22:11,906 --> 00:22:15,942
editing these Yaml file, such as if I'm working just with

360
00:22:15,996 --> 00:22:19,874
Kubernetes objects like this, and I require a lot more expertise

361
00:22:19,922 --> 00:22:23,654
to go in. But then I'd be changing weights such as highlighted

362
00:22:23,702 --> 00:22:27,286
here in these appropriate fields. But it's just simply far more user

363
00:22:27,318 --> 00:22:30,862
friendly and allows for this type of policy to be set by even

364
00:22:30,916 --> 00:22:34,554
nonexperts. Okay, let's shift

365
00:22:34,602 --> 00:22:38,714
gears now and talk about application security challenges and solutions.

366
00:22:38,762 --> 00:22:42,334
So these same exercise. But now, instead of just focusing on

367
00:22:42,372 --> 00:22:46,462
networking, let's look at general approaches and challenges

368
00:22:46,526 --> 00:22:50,434
for security. Now, in a traditional approach, again, when our

369
00:22:50,472 --> 00:22:53,906
applications and the data for the applications all resided on

370
00:22:53,928 --> 00:22:57,370
a single server, when it was tied very and coupled

371
00:22:57,390 --> 00:23:00,790
very tightly to either physical hardware or

372
00:23:00,860 --> 00:23:04,454
virtual hardware, it was very easy to protect. We just throw in

373
00:23:04,492 --> 00:23:08,342
some firewalls in front of it. Even if the application itself was

374
00:23:08,396 --> 00:23:11,878
lacking in security functionality, we could compensate

375
00:23:12,054 --> 00:23:16,358
via the network. One approach would be that, like I say, to throw some firewalls

376
00:23:16,374 --> 00:23:20,194
in front of it. Another approach, if there wasn't any native encryption,

377
00:23:20,262 --> 00:23:24,234
we could provide encryption again via VPNs

378
00:23:24,282 --> 00:23:28,174
virtual private network head ends on the network, and therefore take

379
00:23:28,212 --> 00:23:31,646
care of that and compensate. But again, this is

380
00:23:31,668 --> 00:23:35,374
our new environment. These cloud native environment applications,

381
00:23:35,422 --> 00:23:39,246
where are they existing because they're so dynamic and ephemeral?

382
00:23:39,358 --> 00:23:43,054
Where do we put the firewalls? Where do we put the VPN

383
00:23:43,102 --> 00:23:47,046
head ends and terminates? And how do we manage then all

384
00:23:47,068 --> 00:23:49,800
of these flows and ensure security?

385
00:23:50,650 --> 00:23:54,290
Not only that, but I presented again a very simplified

386
00:23:54,370 --> 00:23:57,614
view of microservices and interconnections.

387
00:23:57,762 --> 00:24:01,350
But the reality is they're far, far more complex

388
00:24:01,430 --> 00:24:05,094
than this. For example, here's a microservice

389
00:24:05,142 --> 00:24:08,954
dependency graph of just one application, a banking application

390
00:24:09,072 --> 00:24:13,114
by Monzo. Or what about some bigger apps like Netflix?

391
00:24:13,162 --> 00:24:16,442
This is a microservices dependency graph for Netflix.

392
00:24:16,586 --> 00:24:20,282
And these, even more scary, becomes these microservices dependency

393
00:24:20,346 --> 00:24:24,990
graph of Amazon. These just become mind boggling in complexity.

394
00:24:25,430 --> 00:24:28,526
Where is the perimeter of this application that you stick

395
00:24:28,558 --> 00:24:32,622
firewalls around it? Or how do you manage all these dependencies

396
00:24:32,766 --> 00:24:36,530
in such an environment? These are the new challenges that are presented

397
00:24:36,690 --> 00:24:40,342
in cloud native. First of all, recognizing that we have

398
00:24:40,396 --> 00:24:44,866
new security challenges, that's part one. Then the biggest

399
00:24:45,058 --> 00:24:48,866
thing, or the thing that's most lacking often that we hear from customers,

400
00:24:48,988 --> 00:24:52,586
is a lack of visibility. We just don't even know what's going on

401
00:24:52,608 --> 00:24:55,786
and we don't have that insight. Also,

402
00:24:55,888 --> 00:24:59,546
recognizing there's multiple layers of security needed from the

403
00:24:59,568 --> 00:25:02,810
containers, the libraries, the dependencies, the comes,

404
00:25:02,890 --> 00:25:06,362
the orchestration, et cetera, all of these layers,

405
00:25:06,426 --> 00:25:10,414
even the APIs, there's so many elements of security that

406
00:25:10,452 --> 00:25:13,794
need to be examined, inspected and

407
00:25:13,832 --> 00:25:17,886
provisioned for. And finally, also recognizing

408
00:25:17,918 --> 00:25:21,634
that the earlier in the development process, specifically the

409
00:25:21,672 --> 00:25:24,482
continuous integration, continuous delivery process,

410
00:25:24,616 --> 00:25:27,766
that we can identify security risks or

411
00:25:27,788 --> 00:25:31,974
threats and actively address these, these more efficient and

412
00:25:32,012 --> 00:25:36,166
more cost effective and the better for everyone. There's so much time that

413
00:25:36,188 --> 00:25:39,722
could be saved if you spot these earlier in that

414
00:25:39,776 --> 00:25:43,546
cycle and so much frustration as well. So to

415
00:25:43,568 --> 00:25:46,874
that end, we have an

416
00:25:46,912 --> 00:25:50,570
offering, Cisco secure application cloud that provides

417
00:25:50,650 --> 00:25:55,082
these needed capabilities, visibility, policy enforcement,

418
00:25:55,226 --> 00:25:58,574
shifts security left. And I'm going to talk about that in the very

419
00:25:58,612 --> 00:26:02,480
next slide, as well as then offering this continuous security

420
00:26:02,850 --> 00:26:06,194
in a cloud native environment. So what are we talking

421
00:26:06,232 --> 00:26:09,986
about when we say shift left? Well, some analysts have really

422
00:26:10,088 --> 00:26:13,842
coined this term and it's become popular, but maybe not everyone

423
00:26:13,896 --> 00:26:17,554
is familiar with it. The idea is that here we have the CI

424
00:26:17,602 --> 00:26:22,230
CD lifecycle. Many security tools are oriented

425
00:26:22,650 --> 00:26:26,502
towards the runtime. So once the application is up and running

426
00:26:26,636 --> 00:26:30,566
these, we think about security, then we take a look at tools

427
00:26:30,598 --> 00:26:34,362
that address security. Whereas what if we can move

428
00:26:34,416 --> 00:26:38,490
that left in that cycle to say, okay, don't just give SEc Ops

429
00:26:39,070 --> 00:26:43,038
tools, let's also give the DevOps team some security

430
00:26:43,124 --> 00:26:46,714
tools so that they can make good security decisions,

431
00:26:46,762 --> 00:26:50,574
apply good security hygiene, and you know what, not only them, but even the

432
00:26:50,612 --> 00:26:54,466
developer. And we're going to talk. But for instance, how we can enable the

433
00:26:54,488 --> 00:26:58,334
developer to facilitate

434
00:26:58,382 --> 00:27:02,770
and take security into account in the decisions they're making, so that

435
00:27:02,920 --> 00:27:05,846
right at the start when they're coding the app,

436
00:27:05,948 --> 00:27:08,966
they got security but into it rather than after the

437
00:27:08,988 --> 00:27:12,614
fact investigation that now has to result in application

438
00:27:12,732 --> 00:27:16,774
patching and recoding, et cetera. So we

439
00:27:16,812 --> 00:27:19,714
want to shift left and make it continuous.

440
00:27:19,762 --> 00:27:22,380
That's the goal of Cisco secure application.

441
00:27:22,990 --> 00:27:26,406
And what I really like about the architecture

442
00:27:26,518 --> 00:27:29,834
that's been used here is that a lot of competitors use an

443
00:27:29,872 --> 00:27:33,406
agent based architectures. Now what does that mean? Remember we

444
00:27:33,428 --> 00:27:37,342
talked about Kubernetes environment where we would have a control plane and

445
00:27:37,396 --> 00:27:41,486
worker comes and they would then apply security agents

446
00:27:41,588 --> 00:27:45,786
on all of these worker nodes. Now this approach,

447
00:27:45,978 --> 00:27:49,586
it's not very efficient because now you got a lot of extra software, kind of

448
00:27:49,608 --> 00:27:54,050
like one of our earlier slides that showed all these levels of abstraction that

449
00:27:54,120 --> 00:27:58,034
was in a virtualized environment, you got more software

450
00:27:58,082 --> 00:28:01,158
that has to run in order for the applications to function.

451
00:28:01,324 --> 00:28:05,906
And these bogs things down. It puts additional load and expense

452
00:28:06,098 --> 00:28:08,410
onto the overall architecture.

453
00:28:08,830 --> 00:28:12,870
In contrast, we leverage native capabilities

454
00:28:12,950 --> 00:28:15,558
that are already existing within Kubernetes,

455
00:28:15,734 --> 00:28:19,622
specifically the application controller capabilities

456
00:28:19,686 --> 00:28:22,910
of the Kubernetes API server. And as a result,

457
00:28:22,980 --> 00:28:26,254
the only dedicated resource we need in an entire

458
00:28:26,372 --> 00:28:30,346
cluster to run this security solution

459
00:28:30,458 --> 00:28:34,166
is a single pod. So that's very lightweight,

460
00:28:34,218 --> 00:28:37,666
that's very high performance. Like I say, to enforce the

461
00:28:37,688 --> 00:28:41,374
policy we use these native mechanisms so it's fully secure

462
00:28:41,502 --> 00:28:44,946
and a lot of fantastic capabilities and it scales and

463
00:28:44,968 --> 00:28:48,470
it's very inexpensive these to the environment.

464
00:28:49,530 --> 00:28:53,094
Also we can optionally integrate with istio service mesh. So we

465
00:28:53,132 --> 00:28:56,834
talked about the benefits of a service mesh. So by applying

466
00:28:56,882 --> 00:29:00,314
a sidecar like envoy, a proxy, then we

467
00:29:00,352 --> 00:29:03,642
can have some additional capabilities for

468
00:29:03,696 --> 00:29:07,014
each application, such as giving us observability,

469
00:29:07,142 --> 00:29:10,374
providing us with firewalling, providing us with encryption,

470
00:29:10,422 --> 00:29:14,206
et cetera. And then these services are typically managed by

471
00:29:14,308 --> 00:29:17,514
the control plane of the service mesh. So these areas central policies

472
00:29:17,562 --> 00:29:21,486
that are applied to all and it makes it scalable, manages the

473
00:29:21,508 --> 00:29:25,310
traffic, manages the security, manage the observability, et cetera.

474
00:29:25,470 --> 00:29:28,830
Now we patch into this by adding an additional

475
00:29:28,910 --> 00:29:32,142
module within that envoy proxy,

476
00:29:32,206 --> 00:29:35,442
as well as then some additional code

477
00:29:35,496 --> 00:29:39,942
to provide DNS detection, so that we can even set policies that

478
00:29:40,076 --> 00:29:43,446
are limiting which domains that can be

479
00:29:43,468 --> 00:29:46,806
connected to by the workload and report that up to the

480
00:29:46,828 --> 00:29:50,294
controller to enforce policies of that nature

481
00:29:50,342 --> 00:29:53,402
as well. These finally, remember I talked about,

482
00:29:53,456 --> 00:29:57,066
we also want to arm the developer to

483
00:29:57,088 --> 00:30:02,766
help them make security aware decisions early in

484
00:30:02,788 --> 00:30:06,282
the development phase of the CIDC lifecycle.

485
00:30:06,346 --> 00:30:09,566
So we really want to shift left. How do we do that?

486
00:30:09,748 --> 00:30:13,502
Well, one way is that we collect information about various

487
00:30:13,566 --> 00:30:17,182
APIs and their respective security vulnerabilities,

488
00:30:17,246 --> 00:30:20,350
threats, posture, et cetera, from many sources.

489
00:30:20,430 --> 00:30:24,530
From our own Cisco Talos, which is one of the most comprehensive

490
00:30:25,050 --> 00:30:28,438
security resources in the world. We do but

491
00:30:28,524 --> 00:30:32,914
five times the amount of security analysis

492
00:30:32,962 --> 00:30:36,486
of events per day than Google does searches. And not

493
00:30:36,508 --> 00:30:40,034
only that, we gather information from there, but also from Cisco

494
00:30:40,082 --> 00:30:43,994
umbrella as well as bitsite. And these, all these information is then

495
00:30:44,032 --> 00:30:48,262
fed to our system that says, okay, these are the APIs

496
00:30:48,326 --> 00:30:51,654
that we know to be secure or know to be not secure.

497
00:30:51,782 --> 00:30:55,558
And therefore we can present the application developer a

498
00:30:55,584 --> 00:30:59,086
curated list. They say, hey, I need an API that does this.

499
00:30:59,188 --> 00:31:03,262
And rather than just first come, first serve, picking an API that

500
00:31:03,396 --> 00:31:07,554
meets their needs, which is typically the approach, I presented that

501
00:31:07,672 --> 00:31:11,380
security aware curated list to them. They can choose

502
00:31:11,830 --> 00:31:15,362
not only the APIs that meets their needs, but the most

503
00:31:15,416 --> 00:31:19,058
secure one. And you can even set specific compliance rules

504
00:31:19,154 --> 00:31:22,438
like the API must meet this, that and the other thing. And these can

505
00:31:22,444 --> 00:31:25,654
be set globally. So we can ensure that even

506
00:31:25,692 --> 00:31:29,526
in the development process, but not just there, we can also observe

507
00:31:29,558 --> 00:31:32,678
the traffic that's traversing these APIs,

508
00:31:32,774 --> 00:31:36,854
monitor that traffic, and if any of the policies or compliance

509
00:31:36,902 --> 00:31:40,666
rules are violated, we can immediately take action

510
00:31:40,778 --> 00:31:44,702
up to and including termination of that traffic as well. So many different

511
00:31:44,756 --> 00:31:48,254
options available to us via this technology.

512
00:31:48,372 --> 00:31:51,534
So not only presenting container security,

513
00:31:51,732 --> 00:31:55,394
but also API security. And we Cisco are

514
00:31:55,432 --> 00:31:58,210
unique in this overall offering.

515
00:31:59,030 --> 00:32:02,514
Let me now share a demonstration of Cisco application

516
00:32:02,632 --> 00:32:06,406
cloud. Before I

517
00:32:06,428 --> 00:32:09,730
get into these demo, I want to call but this website, this URL

518
00:32:09,810 --> 00:32:14,130
eti cisco.com appsec

519
00:32:14,290 --> 00:32:18,440
so emerging technologies and incubation cisco.com

520
00:32:18,990 --> 00:32:22,602
applicationsecurityabbreviated and this is where this very

521
00:32:22,656 --> 00:32:26,186
software is available for free. For anyone that wants to

522
00:32:26,208 --> 00:32:29,754
run this demo, or even better to run this solution in their

523
00:32:29,792 --> 00:32:33,134
environment, there's no feature limitation, there's no time

524
00:32:33,172 --> 00:32:36,526
limitation. The only limitation is scale. We support up to

525
00:32:36,548 --> 00:32:39,902
five comes for free. We want everyone to take it for a test drive

526
00:32:39,956 --> 00:32:43,546
to see containers, serverless, API and service mesh

527
00:32:43,578 --> 00:32:46,994
security in action in their own environment. So let's get

528
00:32:47,032 --> 00:32:50,434
into the demo. When we log in, we're going to see a dashboard like this

529
00:32:50,472 --> 00:32:54,322
that identifies the top security risks, whether the top risks from

530
00:32:54,376 --> 00:32:58,098
pods or APIs or vulnerabilities or permissions,

531
00:32:58,194 --> 00:33:01,570
whatever the case may be. Or for those who prefer,

532
00:33:01,730 --> 00:33:05,302
we also lay out this security information in the

533
00:33:05,356 --> 00:33:08,802
mitre framework so you can see all the different attack

534
00:33:08,876 --> 00:33:12,714
vectors as well as then all the security best practices that areas

535
00:33:12,752 --> 00:33:16,758
recommended to prevent those specific type of attacks.

536
00:33:16,854 --> 00:33:21,210
But even more than just informing us of a particular vulnerability,

537
00:33:21,550 --> 00:33:25,306
such as in this case, the ability for attackers to hide their tracks

538
00:33:25,338 --> 00:33:28,734
and cover over their activities, we see what are all the

539
00:33:28,772 --> 00:33:32,894
affected elements with our environment that

540
00:33:32,932 --> 00:33:36,618
would be affected by this specific vulnerability. But what

541
00:33:36,644 --> 00:33:39,714
I really like is how easy it is to repair to say okay,

542
00:33:39,832 --> 00:33:43,074
I get it, there's a vulnerability, I have these, there's some

543
00:33:43,112 --> 00:33:46,494
best practices that haven't been implemented. Is there anywhere

544
00:33:46,542 --> 00:33:50,422
I don't want this implemented? Probably not. And then I just

545
00:33:50,476 --> 00:33:53,938
apply now and then I've gone and I've created

546
00:33:53,954 --> 00:33:57,494
the rule to prevent defense evasion. And then now I can see

547
00:33:57,532 --> 00:34:00,650
even that specific vulnerability is plugged.

548
00:34:01,470 --> 00:34:04,554
Now not only can I see my threats as outlined, but I can

549
00:34:04,592 --> 00:34:08,550
see my overall environment from more than one perspective.

550
00:34:08,630 --> 00:34:11,020
For instance, if I'm a DevOps person,

551
00:34:12,610 --> 00:34:15,866
I'd likely be interested in seeing my clusters and pods

552
00:34:15,898 --> 00:34:19,354
and interconnections, et cetera. But if I'm sec ops,

553
00:34:19,402 --> 00:34:22,814
I can just quickly change that view over here. And then now

554
00:34:22,852 --> 00:34:26,306
I have a view of the same environment, but from a security

555
00:34:26,408 --> 00:34:29,602
perspective I see what pods are at risk or

556
00:34:29,656 --> 00:34:33,122
the connections that are regular versus encrypted. If there's any

557
00:34:33,176 --> 00:34:37,714
blocked traffic and I can zoom in on anything according

558
00:34:37,762 --> 00:34:40,982
to my interest, not only this,

559
00:34:41,036 --> 00:34:44,022
but then I can do runtime security and see, okay,

560
00:34:44,076 --> 00:34:47,446
of my workloads that are running which one of these are

561
00:34:47,468 --> 00:34:51,546
at risk. And for instance, I can see that this Nginx workload is quite

562
00:34:51,648 --> 00:34:55,354
risky. And why is that? Well, because it's privileged, it can run

563
00:34:55,392 --> 00:34:59,194
its root, and it's public, placing for a

564
00:34:59,232 --> 00:35:03,146
lot of errors and vulnerabilities associated

565
00:35:03,178 --> 00:35:06,366
with this workload. And therefore I could set policies to

566
00:35:06,388 --> 00:35:10,190
restrict these types of risky workloads from running or having

567
00:35:10,260 --> 00:35:14,080
other actions taken as a result too,

568
00:35:14,450 --> 00:35:18,430
or APIs. For instance, when I come back to my security

569
00:35:18,500 --> 00:35:21,966
risk, I can identify which areas my top security risks

570
00:35:21,998 --> 00:35:25,266
from an API perspective. And so if I take a look and

571
00:35:25,288 --> 00:35:28,774
drill down, I can say, okay, what is risky about this particular

572
00:35:28,892 --> 00:35:32,198
API? Well, I focus in on its security

573
00:35:32,284 --> 00:35:36,086
posture from a network or application or DNS perspective. And I

574
00:35:36,108 --> 00:35:39,430
see from a network perspective there's a vulnerability.

575
00:35:40,030 --> 00:35:43,606
And the specific vulnerability is it's using a deprecated version

576
00:35:43,638 --> 00:35:46,906
of TLS. This leaves it susceptible to man in

577
00:35:46,928 --> 00:35:50,442
the middle attacks such as poodle and beast. I can even identify

578
00:35:50,506 --> 00:35:54,462
the specific endpoints where this vulnerability is

579
00:35:54,516 --> 00:35:57,754
present. Now, I might set compliance

580
00:35:57,802 --> 00:36:01,194
rules either on my connections, my clusters, pods,

581
00:36:01,242 --> 00:36:04,474
et cetera, and even my API policies.

582
00:36:04,602 --> 00:36:07,906
So for instance, I might have a policy that says okay, I'm only going to

583
00:36:07,928 --> 00:36:11,122
allow API policies that are specified as low

584
00:36:11,176 --> 00:36:15,226
risk according to all the sources I have, talus, umbrella and bitsight,

585
00:36:15,278 --> 00:36:19,202
et cetera. Or it can even get very granular and flexible.

586
00:36:19,266 --> 00:36:22,520
For instance, I might have the new policy that says

587
00:36:23,050 --> 00:36:27,590
no Russia based APIs

588
00:36:28,010 --> 00:36:31,254
for the time being. So to implement this,

589
00:36:31,292 --> 00:36:35,126
I'll say ok, I'm going to look at all the API endpoints,

590
00:36:35,238 --> 00:36:38,358
I'm going to select an attribute such as location,

591
00:36:38,534 --> 00:36:42,634
and I'll say it is not equal to and then I'll punch

592
00:36:42,682 --> 00:36:46,334
in Russia. And now I've got a tag that

593
00:36:46,372 --> 00:36:50,574
will look and will geolocate those APIs and then can

594
00:36:50,612 --> 00:36:54,702
enforce them to say okay, I'm no longer leveraging or utilizing

595
00:36:54,766 --> 00:36:58,594
even APIs based on specific location as well as

596
00:36:58,632 --> 00:37:02,382
any other criteria. So basically, I have very powerful

597
00:37:02,446 --> 00:37:06,242
tools for applying security in my cloud

598
00:37:06,296 --> 00:37:09,442
native environment, whether it's container security, APIs,

599
00:37:09,506 --> 00:37:13,314
security, serverless security, so on and so forth. Very comprehensive,

600
00:37:13,442 --> 00:37:16,838
powerful tool, and it's free to use. So by all means, take it

601
00:37:16,844 --> 00:37:20,914
for a spin. Okay, let me wrap

602
00:37:20,962 --> 00:37:24,406
things up and summarize the key points that we've

603
00:37:24,438 --> 00:37:27,814
covered. So first thing, cloud native architectures,

604
00:37:27,862 --> 00:37:31,802
they bring many business benefits. We talked about portability,

605
00:37:31,946 --> 00:37:35,466
flexibility, scalability, containers delivery,

606
00:37:35,578 --> 00:37:39,482
all of these benefits very valuable to the modern

607
00:37:39,546 --> 00:37:41,600
application development and experience.

608
00:37:42,790 --> 00:37:46,702
Not only this, but these architectures

609
00:37:46,766 --> 00:37:50,290
do present some new challenges, which is almost

610
00:37:50,360 --> 00:37:54,306
inevitable with technology. You solve some problems, but sometimes you create some

611
00:37:54,328 --> 00:37:57,486
new ones. And so we're applying our,

612
00:37:57,528 --> 00:38:01,254
like I said, our 30 years plus of experience in networking and

613
00:38:01,292 --> 00:38:05,880
security to this new domain. These new set of challenges, both from

614
00:38:06,650 --> 00:38:11,660
cloud native application networking and cloud application security,

615
00:38:12,030 --> 00:38:15,654
and the two specific solutions I introduced today were Cisco

616
00:38:15,702 --> 00:38:19,978
service mesh manager and Cisco secure application cloud.

617
00:38:20,144 --> 00:38:23,886
And what I really like about the

618
00:38:23,908 --> 00:38:27,274
approach we've taken here is that we really want to drive adoption.

619
00:38:27,322 --> 00:38:30,830
We want people to take these for test drives. So we're offering these

620
00:38:30,900 --> 00:38:34,770
software suites completely free of charge for people

621
00:38:34,840 --> 00:38:38,082
to try. And it's got full

622
00:38:38,136 --> 00:38:41,230
functionality, no time limits whatsoever.

623
00:38:41,390 --> 00:38:44,974
The only limit is these scale. So five nodes

624
00:38:45,022 --> 00:38:49,474
for secure application cloud, ten nodes for service mesh manager,

625
00:38:49,602 --> 00:38:52,854
and then at these steps, you can just start using

626
00:38:52,892 --> 00:38:56,582
them. You download, you log in, you get an account, you sign

627
00:38:56,636 --> 00:39:00,522
up for, like I say, completely free, and you're off and running within

628
00:39:00,576 --> 00:39:03,606
a few minutes. Same with service Mesh manager.

629
00:39:03,638 --> 00:39:07,194
So we really want people to try and adopt and then

630
00:39:07,232 --> 00:39:10,490
see the value that we're bringing here based on

631
00:39:10,560 --> 00:39:14,554
our expertise and thought leadership into these new spaces.

632
00:39:14,602 --> 00:39:17,502
And we really want you to take advantage of that.

633
00:39:17,636 --> 00:39:21,262
We're part of Cisco's research and development team, which is called

634
00:39:21,316 --> 00:39:25,466
emerging technologies and incubation. If you're interested in following

635
00:39:25,498 --> 00:39:28,986
some of the other tools and technologies and solutions

636
00:39:29,018 --> 00:39:32,574
that we're actively working on and developing, please feel

637
00:39:32,612 --> 00:39:35,720
free to follow us here at.

638
00:39:36,250 --> 00:39:39,286
Thanks so much for taking the time for this session. I hope you found it

639
00:39:39,308 --> 00:39:39,990
useful.

