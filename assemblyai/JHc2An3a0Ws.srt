1
00:01:54,090 --> 00:01:57,310
Hey everyone, and thank you for watching my presentation.

2
00:01:57,650 --> 00:02:01,834
Today. I'll tell you about the Mlsecops framework for protecting AI

3
00:02:01,882 --> 00:02:05,446
systems. And before I jump into details, I want

4
00:02:05,468 --> 00:02:08,694
to tell you a story how I started in this field and

5
00:02:08,732 --> 00:02:12,802
actually witnessed how the entire industry was born.

6
00:02:12,946 --> 00:02:16,258
And like many of you at this event, I am a cybersecurity

7
00:02:16,354 --> 00:02:20,186
guy. And for more than ten years I've been doing security research in

8
00:02:20,208 --> 00:02:24,522
various product startups. And after a few years in application security,

9
00:02:24,656 --> 00:02:29,290
I became a product leader for security monitoring and threat detection.

10
00:02:29,630 --> 00:02:33,274
And we've decided to use machine learning for enterprise

11
00:02:33,322 --> 00:02:37,246
security monitoring. And we were quite successful with the models for

12
00:02:37,268 --> 00:02:41,386
insider threat detection. But closer to the release, we tested

13
00:02:41,418 --> 00:02:44,910
our product for vulnerabilities and we have realized

14
00:02:44,990 --> 00:02:48,622
that it's easier to fool our own security models.

15
00:02:48,766 --> 00:02:52,770
So we started diving deeper into the world of AI vulnerabilities

16
00:02:53,190 --> 00:02:56,834
and discovered the entire field called adversarial

17
00:02:56,882 --> 00:03:00,726
machine learning. And during that period, I was interested

18
00:03:00,828 --> 00:03:04,166
in devsecops practices, and at the same time I

19
00:03:04,188 --> 00:03:08,280
was also building an Mlsecops engine for our security platform.

20
00:03:08,890 --> 00:03:12,014
So I have created this name, Mlsecops,

21
00:03:12,162 --> 00:03:16,170
which was quite obvious from my two interests to

22
00:03:16,240 --> 00:03:19,306
separate these two work streams. One is for

23
00:03:19,408 --> 00:03:22,734
operationalizing our security models and another one is

24
00:03:22,772 --> 00:03:26,474
for protecting our own models. And then I witnessed

25
00:03:26,522 --> 00:03:30,346
how the field of AI security was growing with many new attacks

26
00:03:30,378 --> 00:03:34,082
and tools and regulations. So we decided to start

27
00:03:34,136 --> 00:03:37,374
a research company focused exclusively on AI

28
00:03:37,422 --> 00:03:41,250
vulnerabilities and we founded Adversa in

29
00:03:41,320 --> 00:03:45,090
2019. We were the first commercial company

30
00:03:45,240 --> 00:03:48,822
researching AI vulnerabilities, and it also happened

31
00:03:48,876 --> 00:03:52,566
that we did many other things that could be called the

32
00:03:52,588 --> 00:03:56,598
first in the industry. And we started on a mission to protect

33
00:03:56,684 --> 00:04:00,074
AI from cyber threats with applying all

34
00:04:00,112 --> 00:04:04,198
the latest attacks from academia to the industry,

35
00:04:04,294 --> 00:04:08,010
to the real world. And I won't go into details here,

36
00:04:08,080 --> 00:04:11,774
but if you're interested in this topic, I encourage you to visit our

37
00:04:11,812 --> 00:04:15,600
website so you can read a lot of research materials there.

38
00:04:16,450 --> 00:04:19,962
So in this presentation, I'm summarizing my ideas

39
00:04:20,026 --> 00:04:23,522
from years of research, and I'll tell you

40
00:04:23,656 --> 00:04:27,250
why. AI is almost certainly the future

41
00:04:27,320 --> 00:04:31,122
of software. What are the security threats to large

42
00:04:31,176 --> 00:04:35,482
scale AI systems and how to implement security in AI pipelines,

43
00:04:35,646 --> 00:04:39,906
all while shifting security left with Mlsecops

44
00:04:39,938 --> 00:04:43,750
framework as an example. So why

45
00:04:43,820 --> 00:04:46,966
do they call AI as the most likely future of

46
00:04:46,988 --> 00:04:50,266
software? Well, you already can easily find

47
00:04:50,368 --> 00:04:54,150
many exciting AI applications in almost every industry,

48
00:04:54,310 --> 00:04:57,594
and many of them offer quality improvements and

49
00:04:57,632 --> 00:05:01,262
large scale automation. And the simple example here

50
00:05:01,316 --> 00:05:05,226
is that AI is widely used in cybersecurity

51
00:05:05,338 --> 00:05:09,290
so instead of writing regular expressions and signature rules,

52
00:05:09,450 --> 00:05:13,454
we can use machine learning for deciding whether a binary

53
00:05:13,582 --> 00:05:16,894
is malicious or benign, or whether security events

54
00:05:16,942 --> 00:05:20,482
are normal or anomalous. And I believe that

55
00:05:20,536 --> 00:05:24,830
in the next five to ten years, most tech companies will become

56
00:05:25,000 --> 00:05:28,582
AI companies. Similarly to how many

57
00:05:28,636 --> 00:05:32,102
traditional businesses had to go online and become

58
00:05:32,156 --> 00:05:35,110
tech companies just to stay relevant.

59
00:05:35,770 --> 00:05:39,382
But also, AI enables completely new ideas,

60
00:05:39,446 --> 00:05:43,014
like generating visuals or even writing real programs.

61
00:05:43,142 --> 00:05:46,410
And many unexpected things at the intersection can happen,

62
00:05:46,480 --> 00:05:49,466
like turning speech into text,

63
00:05:49,568 --> 00:05:53,102
then this text into visual arts, and this visuals into

64
00:05:53,156 --> 00:05:57,338
code, which essentially becomes a really well designed

65
00:05:57,434 --> 00:06:01,326
web page. And similarly to how the

66
00:06:01,348 --> 00:06:04,734
most popular applications that we use today didn't

67
00:06:04,782 --> 00:06:08,546
exist ten to 20 years ago. In the next

68
00:06:08,728 --> 00:06:12,434
years, a similar shift will happen with very

69
00:06:12,472 --> 00:06:15,960
new types of companies that we cannot even predict now.

70
00:06:17,050 --> 00:06:20,646
So why does this happen? If we dive deeper, we can

71
00:06:20,668 --> 00:06:24,322
see that AI offers not just cool applications,

72
00:06:24,386 --> 00:06:28,746
but a paradigm shift, and that's entire new way of

73
00:06:28,848 --> 00:06:32,630
developing software. With the traditional software,

74
00:06:32,710 --> 00:06:36,838
we create clear algorithms and behaviors

75
00:06:37,014 --> 00:06:40,074
with programming languages. But with

76
00:06:40,112 --> 00:06:44,394
AI in the form of machine learning, we can train the algorithms,

77
00:06:44,522 --> 00:06:48,286
and instead of defining all the rules, we can reduce the

78
00:06:48,308 --> 00:06:52,320
number of instructions and replace it with the trained model.

79
00:06:52,630 --> 00:06:55,854
So a deterministic algorithm designed

80
00:06:55,902 --> 00:06:59,774
by software developers is replaced or augmented

81
00:06:59,902 --> 00:07:04,014
with a probabilistic algorithm designed by AI

82
00:07:04,062 --> 00:07:07,574
developers. And of course, it comes with

83
00:07:07,612 --> 00:07:10,294
the new roles and processes. For example,

84
00:07:10,492 --> 00:07:13,480
data engineers collect and prepare data,

85
00:07:13,930 --> 00:07:18,018
then data scientists do experiments and model training,

86
00:07:18,204 --> 00:07:21,306
and then machine learning. Engineers focus on

87
00:07:21,328 --> 00:07:25,542
model packaging and productizing, and finally, operations handle

88
00:07:25,606 --> 00:07:29,622
deployments and monitoring. And this requires

89
00:07:29,686 --> 00:07:32,910
a whole new process, which is called mlops.

90
00:07:33,330 --> 00:07:37,038
There are many similarities with the devsecops and even

91
00:07:37,204 --> 00:07:41,070
some shared infrastructure and tools. But if we zoom in,

92
00:07:41,140 --> 00:07:44,542
we can see that models and data sets introduce

93
00:07:44,606 --> 00:07:48,942
new types of artifacts, like data set files, model weights,

94
00:07:49,086 --> 00:07:52,514
model code metrics, experiments, and so on.

95
00:07:52,712 --> 00:07:56,918
And all of these things increase complexity and could be

96
00:07:57,084 --> 00:08:01,142
potential attack vectors. So attacks could happen

97
00:08:01,276 --> 00:08:04,614
against data models or against a system in

98
00:08:04,652 --> 00:08:08,346
general. And the field of attacks against

99
00:08:08,448 --> 00:08:11,580
AI is called adversa machine learning.

100
00:08:12,670 --> 00:08:17,094
And as you can imagine, with so much value locked in AI systems,

101
00:08:17,142 --> 00:08:21,182
it's very valuable target for cyberattacks. And in fact,

102
00:08:21,236 --> 00:08:24,238
attacks against AI are already happening.

103
00:08:24,404 --> 00:08:27,658
You can read in the media or in incident

104
00:08:27,754 --> 00:08:31,386
databases of how hackers bypassed

105
00:08:31,498 --> 00:08:36,158
malware detectors, spam filters, or smart firewalls.

106
00:08:36,334 --> 00:08:39,774
And our own projects, penetration testing.

107
00:08:39,822 --> 00:08:43,458
AI systems have 100%

108
00:08:43,544 --> 00:08:46,514
success rate with industries such as automotive,

109
00:08:46,562 --> 00:08:50,582
biometrics, intranet, or finance. And just

110
00:08:50,636 --> 00:08:54,594
recently, we at Adversa, together with our partners

111
00:08:54,722 --> 00:08:58,070
organized the ML Sec AI hacking competition.

112
00:08:58,590 --> 00:09:02,634
My team has created the facial recognition track and

113
00:09:02,832 --> 00:09:05,962
all the top ten contestants used very

114
00:09:06,016 --> 00:09:09,858
different strategies and all of them successfully

115
00:09:10,054 --> 00:09:13,630
fooled AI models. And if you're interested

116
00:09:13,700 --> 00:09:17,386
to learn more about this contest, you can read the blog

117
00:09:17,418 --> 00:09:19,470
post with technical details.

118
00:09:20,850 --> 00:09:24,034
So in adversarial machine learning today, there are

119
00:09:24,072 --> 00:09:27,234
over 5000 research papers about attacking and

120
00:09:27,272 --> 00:09:30,542
defending AI systems. In this presentation,

121
00:09:30,606 --> 00:09:34,466
we don't look at individual attacks, so if you're interested

122
00:09:34,568 --> 00:09:37,842
in details you can watch one of my previous talks that covers

123
00:09:37,906 --> 00:09:41,160
the past ten years of AI vulnerability research.

124
00:09:42,170 --> 00:09:45,266
But I want to give you a quick idea about such attacks.

125
00:09:45,298 --> 00:09:48,906
So you know what we should protect from the

126
00:09:48,928 --> 00:09:52,790
most common AI threats that we see are infection.

127
00:09:52,870 --> 00:09:56,314
That happens at the development stage when

128
00:09:56,352 --> 00:10:00,134
attackers maliciously modify training data with

129
00:10:00,192 --> 00:10:03,646
poisoning attacks or simply trick devsecops to

130
00:10:03,668 --> 00:10:05,870
use infected models.

131
00:10:06,450 --> 00:10:09,902
Then manipulation happens at runtime when

132
00:10:09,956 --> 00:10:13,822
attackers full model decisions or bypass

133
00:10:13,966 --> 00:10:17,810
some detections with adversa examples,

134
00:10:18,390 --> 00:10:22,420
or simply perform a denial of service attack.

135
00:10:23,030 --> 00:10:26,994
An exfiltration happens at the runtime

136
00:10:27,042 --> 00:10:30,598
stage as well when attackers steal model

137
00:10:30,764 --> 00:10:34,662
algorithm details or extract private data

138
00:10:34,716 --> 00:10:38,742
from the model with various types of inference attacks.

139
00:10:38,886 --> 00:10:43,158
So if you want to know more, you can read our report that covers

140
00:10:43,334 --> 00:10:46,406
all known AI vulnerabilities and explains

141
00:10:46,438 --> 00:10:49,270
them from different angles based on industries,

142
00:10:49,350 --> 00:10:53,134
applications, data types and so on. And I just

143
00:10:53,172 --> 00:10:57,034
want to conclude this section reminding

144
00:10:57,082 --> 00:11:00,990
that essentially AI expands the attack surface.

145
00:11:01,410 --> 00:11:05,730
On the one hand, AI systems are just yet another example of software.

146
00:11:06,230 --> 00:11:09,630
That's why you can expect them to have all the traditional

147
00:11:09,710 --> 00:11:13,374
types of vulnerabilities, such as problems with input

148
00:11:13,422 --> 00:11:17,190
validation, access control issues, or misconfigurations.

149
00:11:17,770 --> 00:11:21,826
On the other hand, AI systems have new types of behaviors.

150
00:11:21,938 --> 00:11:25,682
So on top of the traditional vulnerabilities, we should worry

151
00:11:25,746 --> 00:11:29,046
about unique AI threats like infection,

152
00:11:29,158 --> 00:11:31,210
manipulation or exfiltration.

153
00:11:32,990 --> 00:11:36,374
So, as you can see, the security of AI

154
00:11:36,422 --> 00:11:40,278
is a fundamentally new problem, and current application

155
00:11:40,384 --> 00:11:44,638
security and other security solutions cannot help

156
00:11:44,724 --> 00:11:48,640
with AI vulnerabilities. So the new approaches are needed.

157
00:11:49,250 --> 00:11:52,646
But despite these unique problems, the core

158
00:11:52,698 --> 00:11:55,700
security principles still apply here.

159
00:11:57,190 --> 00:12:01,006
Security should enable AI teams build and scale

160
00:12:01,118 --> 00:12:04,740
AI software reliably and with more trust.

161
00:12:05,290 --> 00:12:08,834
And as AI is fundamentally vulnerable,

162
00:12:08,962 --> 00:12:12,486
when you scale it without security, you actually scale problems for

163
00:12:12,508 --> 00:12:16,162
your company. So instead, AI teams should be confident

164
00:12:16,226 --> 00:12:19,350
in AI reliability because it's backed by preparation

165
00:12:19,430 --> 00:12:23,690
and security processes, and then they can scale it successfully.

166
00:12:24,590 --> 00:12:28,730
Similarly to how security shouldn't slow down

167
00:12:28,880 --> 00:12:33,070
AI development. Also, the manual model validation

168
00:12:33,570 --> 00:12:37,562
shouldn't slow down deployment. So automation,

169
00:12:37,626 --> 00:12:41,120
playbooks and best practices are also required here.

170
00:12:41,970 --> 00:12:46,162
Essentially, we want to build security into

171
00:12:46,216 --> 00:12:49,822
the way software is built, not after software

172
00:12:49,886 --> 00:12:52,786
is built. So that's called shifting left,

173
00:12:52,888 --> 00:12:56,754
because we start doing security earlier in the pipeline,

174
00:12:56,882 --> 00:12:59,350
which is technically on the left side.

175
00:12:59,500 --> 00:13:03,830
And this idea is also very practical because

176
00:13:03,900 --> 00:13:08,550
for AI systems it's deeply connected to the levels of complexity

177
00:13:08,630 --> 00:13:12,282
added after each stage. For instance, if in later

178
00:13:12,336 --> 00:13:16,154
stages we decide that the model should be fixed with the

179
00:13:16,192 --> 00:13:20,418
retraining, then it will require redoing all the stages,

180
00:13:20,534 --> 00:13:22,750
starting from the data packaging,

181
00:13:23,170 --> 00:13:26,606
redoing all the experiments, choosing the best model,

182
00:13:26,708 --> 00:13:28,270
and so on and so forth.

183
00:13:29,730 --> 00:13:33,646
So the earlier you start, the more reliable AI

184
00:13:33,678 --> 00:13:37,346
systems you can build because you dramatically decrease the

185
00:13:37,368 --> 00:13:40,974
number of problems at later stages. And it's

186
00:13:41,022 --> 00:13:44,946
beneficial to use combinations of security controls at

187
00:13:44,968 --> 00:13:48,440
different stages over the model lifecycle as well.

188
00:13:49,690 --> 00:13:54,134
So now I want to give you a high level overview of my

189
00:13:54,252 --> 00:13:58,046
Mlsecops framework. I'll focus more on concepts

190
00:13:58,178 --> 00:14:02,154
rather than on technologies, so we could build your intuition about

191
00:14:02,192 --> 00:14:05,958
this topic and based on your knowledge of devsecops.

192
00:14:06,134 --> 00:14:09,930
Later you can do your own research on specific tools.

193
00:14:10,850 --> 00:14:14,762
So first I will visualize original Mlsecops

194
00:14:14,826 --> 00:14:18,330
tasks this way. Then I will highlight

195
00:14:18,490 --> 00:14:22,686
key security problems like this, and finally I

196
00:14:22,708 --> 00:14:26,130
will suggest solutions for MLC Ops pipeline.

197
00:14:27,430 --> 00:14:31,022
So the first stage is planning, and here you collect

198
00:14:31,086 --> 00:14:34,610
all types of requirements to understand what you are building

199
00:14:34,680 --> 00:14:38,530
and why, including business and technical requirements.

200
00:14:39,430 --> 00:14:43,542
Then cross functional dependencies across departments or

201
00:14:43,676 --> 00:14:47,518
even across companies. Then internal

202
00:14:47,554 --> 00:14:50,502
and external policies, regulation and compliance.

203
00:14:50,646 --> 00:14:54,122
So the final outcome of this stage is a set of plans and

204
00:14:54,176 --> 00:14:57,050
policies shaping all the later stages.

205
00:14:58,750 --> 00:15:02,042
What is often overlooked during the planning stage

206
00:15:02,106 --> 00:15:06,426
is understanding of risks and security requirements.

207
00:15:06,618 --> 00:15:09,962
If you don't do risk assessment, you don't know what assets

208
00:15:10,026 --> 00:15:13,634
worth protecting from, what you want to protect them, and what

209
00:15:13,672 --> 00:15:17,106
risks you can accept. Sometimes you won't even

210
00:15:17,288 --> 00:15:21,090
think what assets you'll create during development, and therefore

211
00:15:21,670 --> 00:15:24,580
you'll not be able to plan protection for it.

212
00:15:25,290 --> 00:15:28,502
If you don't do threat modeling, you'll be blind about

213
00:15:28,556 --> 00:15:32,550
what attackers can do to our system. This step

214
00:15:32,620 --> 00:15:36,230
doesn't apply any advanced knowledge about attacking algorithms

215
00:15:36,890 --> 00:15:40,570
or ability to perform such attacks. This is about

216
00:15:40,640 --> 00:15:44,054
finding the weakest links and bottlenecks in the AI

217
00:15:44,102 --> 00:15:47,766
system design and how users interact with the AI,

218
00:15:47,878 --> 00:15:51,662
how AI outputs can impact users or business

219
00:15:51,716 --> 00:15:55,598
decisions. And finally, if you don't manage security,

220
00:15:55,684 --> 00:15:59,242
you basically don't have it. So depending on the scale,

221
00:15:59,306 --> 00:16:02,914
it could be just a team level or system level security process,

222
00:16:03,032 --> 00:16:07,022
or a wider program affecting multiple teams,

223
00:16:07,086 --> 00:16:09,140
departments or entire company.

224
00:16:10,870 --> 00:16:14,306
So when you start implementing the practices of

225
00:16:14,328 --> 00:16:17,666
risk assessment threat modeling and security governance.

226
00:16:17,778 --> 00:16:21,734
You'll start producing some artifacts which will affect the

227
00:16:21,772 --> 00:16:25,426
entire Mlsecops pipeline. Like risk

228
00:16:25,458 --> 00:16:28,586
register helps you track AI risks and

229
00:16:28,608 --> 00:16:32,550
mitigations, and it keeps you up to date with the latest potential

230
00:16:32,630 --> 00:16:36,426
threats and helps you make decisions later

231
00:16:36,608 --> 00:16:38,010
during development.

232
00:16:40,130 --> 00:16:43,630
Knowing your attack surface means having

233
00:16:43,700 --> 00:16:48,202
all the parts of your models, data sets and user interactions analyzed

234
00:16:48,266 --> 00:16:51,946
for attack scenarios, and this could be linked

235
00:16:51,978 --> 00:16:55,460
to the risk register or tracked in the

236
00:16:56,150 --> 00:16:59,170
model management system with some notes and references.

237
00:17:01,190 --> 00:17:04,706
Having security baselines essentially means that you know

238
00:17:04,728 --> 00:17:07,970
how to quantify security risks and can decide

239
00:17:08,050 --> 00:17:11,734
whether these metrics go or no

240
00:17:11,772 --> 00:17:13,910
go situation for deployment.

241
00:17:16,250 --> 00:17:19,750
The next stage is data collection and preparation,

242
00:17:19,910 --> 00:17:23,622
and the process starts with understanding business requirements

243
00:17:23,766 --> 00:17:26,986
and what data should be collected for it,

244
00:17:27,088 --> 00:17:30,382
and then understanding existing data and whether something

245
00:17:30,436 --> 00:17:34,538
is missing. Then, according to the requirements,

246
00:17:34,714 --> 00:17:38,110
the data is collected, cleaned and enriched.

247
00:17:38,450 --> 00:17:42,174
Alternatively, it could be sourced from data vendors or

248
00:17:42,212 --> 00:17:46,082
partners or contractors. And finally, all this data

249
00:17:46,136 --> 00:17:49,614
is packaged for reusing later in the pipeline.

250
00:17:49,742 --> 00:17:53,874
So the outcome of this stage is basically releasing a data set as

251
00:17:53,912 --> 00:17:57,718
a little product and it will be used for model training

252
00:17:57,804 --> 00:17:58,440
later.

253
00:18:00,730 --> 00:18:04,642
Clearly, not knowing your data and business requirements

254
00:18:04,706 --> 00:18:08,490
can get you into big trouble, especially in regulated industries.

255
00:18:08,910 --> 00:18:12,220
You should be very knowledgeable about the structure of your data.

256
00:18:12,910 --> 00:18:17,354
Very often you have some private information that

257
00:18:17,392 --> 00:18:20,380
is absolutely not required for model training,

258
00:18:21,170 --> 00:18:24,430
but brings many privacy and compliance risks.

259
00:18:25,090 --> 00:18:29,054
The next common problem is unreliable sources of data and

260
00:18:29,092 --> 00:18:33,034
essentially supply and chain risk. And regardless

261
00:18:33,162 --> 00:18:36,514
whether you collected it on your own or sourced from

262
00:18:36,552 --> 00:18:40,114
data vendors, often you can't guarantee the correctness of

263
00:18:40,152 --> 00:18:44,350
methods for data collection, whether it was collected without violations,

264
00:18:44,510 --> 00:18:48,470
and whether the actual data matches the data specification.

265
00:18:49,690 --> 00:18:52,994
And another more advanced risk is data poisoning

266
00:18:53,042 --> 00:18:56,614
that injects some malicious entries in the data set.

267
00:18:56,732 --> 00:19:00,758
It could happen by working with compromised data set providers

268
00:19:00,854 --> 00:19:04,682
or by injecting individual examples that

269
00:19:04,736 --> 00:19:08,474
eventually end up in the data set. So the risk could be

270
00:19:08,512 --> 00:19:12,534
that you don't verify the integrity of individual entries

271
00:19:12,662 --> 00:19:16,618
and don't confirm or cannot confirm the data sets

272
00:19:16,714 --> 00:19:20,862
were not maliciously modified. The main

273
00:19:20,916 --> 00:19:24,354
overall principle here is careful curation of

274
00:19:24,392 --> 00:19:27,774
data sets. So for data privacy,

275
00:19:27,902 --> 00:19:31,634
the main piece of advice is to think whether you need this

276
00:19:31,672 --> 00:19:35,682
private data at all. Often it's absolutely not required

277
00:19:35,746 --> 00:19:39,126
for model training, so sensitive entries can be

278
00:19:39,148 --> 00:19:41,910
just removed. And if it's not feasible,

279
00:19:42,410 --> 00:19:46,194
then private details can be anonymized or tokenized.

280
00:19:46,322 --> 00:19:50,486
And sometimes other methods like differential privacy

281
00:19:50,678 --> 00:19:55,014
could also be helpful. Then to address supply

282
00:19:55,062 --> 00:19:59,226
chain risks and ensure data integrity, you should use

283
00:19:59,408 --> 00:20:02,926
reliable data sources, those that you can control

284
00:20:03,028 --> 00:20:06,602
or verify and data should also have verifiable

285
00:20:06,666 --> 00:20:10,126
quality based on specifications, and you

286
00:20:10,148 --> 00:20:14,530
can verify it with individual entries, with metadata, with metrics.

287
00:20:15,590 --> 00:20:19,582
And finally, against data poisoning, you should avoid

288
00:20:19,726 --> 00:20:21,780
inconsistencies in your data.

289
00:20:22,630 --> 00:20:26,162
Like first is like filtering out anomalous

290
00:20:26,226 --> 00:20:29,730
entries, and also filtering potentially

291
00:20:29,890 --> 00:20:34,114
malicious entries that are often anomalous

292
00:20:34,162 --> 00:20:38,230
too. And you should use secure

293
00:20:38,390 --> 00:20:41,834
data set version control to ensure that the data was

294
00:20:41,872 --> 00:20:46,010
not maliciously modified. If you checked that, it's correct previously.

295
00:20:47,550 --> 00:20:51,194
The next stage is model building. At this stage,

296
00:20:51,242 --> 00:20:54,782
we take a specific data set version and start

297
00:20:54,836 --> 00:20:58,234
building a model, including experiments with processing

298
00:20:58,282 --> 00:21:01,866
data in different ways, known as feature engineering,

299
00:21:02,058 --> 00:21:06,066
and also experimenting with model parameters and

300
00:21:06,088 --> 00:21:09,426
architectures. And it's important to remember that we

301
00:21:09,448 --> 00:21:13,166
are working with the data set that was previously supplied.

302
00:21:13,278 --> 00:21:16,840
So technically, it's kind of an external artifacted model.

303
00:21:17,210 --> 00:21:21,042
And also in many cases, for highly effective

304
00:21:21,106 --> 00:21:25,074
big neural networks, there are already pretrained models,

305
00:21:25,202 --> 00:21:28,094
so the model brains could be also external.

306
00:21:28,242 --> 00:21:32,358
And this model building stage essentially becomes writing

307
00:21:32,454 --> 00:21:35,900
code for connecting data set and the model.

308
00:21:37,630 --> 00:21:41,310
At the next stage, we focus on training the model

309
00:21:41,380 --> 00:21:44,954
and hoping that results meet our expectations. Otherwise,

310
00:21:45,002 --> 00:21:48,442
we'll repeat experiments with features and architecture,

311
00:21:48,506 --> 00:21:51,774
and train it again until the results comply with the

312
00:21:51,812 --> 00:21:55,454
business requirements. And finally, for model packaging,

313
00:21:55,502 --> 00:21:58,834
we are finalizing code from the early experiments and

314
00:21:58,872 --> 00:22:02,286
model training, and if necessary, we convert

315
00:22:02,318 --> 00:22:05,460
it from Jupiter notebooks to production ready code.

316
00:22:06,390 --> 00:22:09,700
The outcome of this entire stage is that

317
00:22:10,230 --> 00:22:13,234
the model is ready and could be reused.

318
00:22:13,362 --> 00:22:17,160
Essentially, it could be imported into the main application and used

319
00:22:17,550 --> 00:22:19,770
just like any other module.

320
00:22:20,990 --> 00:22:24,874
So this stage is probably the most dangerous one

321
00:22:25,072 --> 00:22:28,380
because it's so easy to make expensive mistakes here.

322
00:22:29,070 --> 00:22:32,266
So let's start with the model itself. If you're using an

323
00:22:32,288 --> 00:22:36,206
external model, you should be really mindful where you take it

324
00:22:36,228 --> 00:22:40,014
from. Just like with external data sets, you can just download a big

325
00:22:40,052 --> 00:22:43,310
model from GitHub and hope that it's reliable.

326
00:22:44,070 --> 00:22:47,538
And it actually becomes quite common

327
00:22:47,624 --> 00:22:51,218
to use models released by big corporations like Google and

328
00:22:51,224 --> 00:22:54,942
Microsoft, who spent millions of dollars for training those

329
00:22:55,016 --> 00:22:58,770
models. And it's very realistic scenario for cybercriminals

330
00:22:58,850 --> 00:23:02,374
to spread malicious models that look

331
00:23:02,412 --> 00:23:06,226
like benign. If the model is created

332
00:23:06,258 --> 00:23:09,690
from scratch, it's also quite easy to get into trouble.

333
00:23:10,350 --> 00:23:14,342
Production level machine learning is a new area for many teams,

334
00:23:14,406 --> 00:23:18,918
and sometimes due to lack of resources or role misunderstanding,

335
00:23:19,094 --> 00:23:22,554
data scientists are expected to write secure

336
00:23:22,602 --> 00:23:26,046
and production ready code, which is often disconnected from

337
00:23:26,068 --> 00:23:29,614
reality. And on top of this, there are many tools and

338
00:23:29,652 --> 00:23:32,706
frameworks that emerged from academia that

339
00:23:32,728 --> 00:23:36,706
were not designed for production and that were not designed with

340
00:23:36,728 --> 00:23:38,020
security in mind.

341
00:23:39,590 --> 00:23:44,302
One of the most widespread examples is importing

342
00:23:44,366 --> 00:23:48,310
a model with Python pickle files, which is

343
00:23:48,460 --> 00:23:51,682
object serialization, and it's a well known

344
00:23:51,746 --> 00:23:54,950
insecure practice. Also,

345
00:23:55,100 --> 00:23:58,582
the core machine learning frameworks like Tensorflow

346
00:23:58,726 --> 00:24:02,090
and other libraries may have their own vulnerabilities.

347
00:24:03,470 --> 00:24:07,162
And finally, one of the most unique problems to the security

348
00:24:07,296 --> 00:24:10,734
of AI is a non robust training of

349
00:24:10,772 --> 00:24:14,090
models that enables model manipulation

350
00:24:14,170 --> 00:24:16,910
and data exfiltration attacks.

351
00:24:17,570 --> 00:24:20,938
There are various fundamental design flaws

352
00:24:21,034 --> 00:24:24,654
of neural networks that like weak decision

353
00:24:24,702 --> 00:24:28,034
boundaries, lack of diverse data examples that

354
00:24:28,072 --> 00:24:30,420
affect nonrobust learning.

355
00:24:31,510 --> 00:24:35,650
So to address all these risks, the main principle

356
00:24:35,730 --> 00:24:40,374
is to work with external models, as with external code,

357
00:24:40,572 --> 00:24:44,342
and conduct your own due diligence. Here, for model

358
00:24:44,396 --> 00:24:47,678
integrity to address supply chain

359
00:24:47,714 --> 00:24:50,822
risks, you should use only reliable sources

360
00:24:50,966 --> 00:24:54,886
of pretrained models to reduce the risk

361
00:24:54,918 --> 00:24:58,662
of backdoors and validate that models you downloaded

362
00:24:58,726 --> 00:25:02,910
match like hashes of original models.

363
00:25:03,410 --> 00:25:07,486
You should also use secure model version control to

364
00:25:07,508 --> 00:25:10,990
ensure that models were not maliciously modified,

365
00:25:11,490 --> 00:25:14,574
because it's easy to hijack model artifacts,

366
00:25:14,622 --> 00:25:18,334
especially when you work in different environments and switching

367
00:25:18,382 --> 00:25:21,810
back and forth between experimenting and productizing.

368
00:25:23,590 --> 00:25:27,522
Next, secure coding is actually the closest

369
00:25:27,586 --> 00:25:29,720
to traditional application security.

370
00:25:30,730 --> 00:25:34,386
Model code is often written by experts in data science

371
00:25:34,418 --> 00:25:37,854
who are not necessarily experts in software engineering.

372
00:25:38,002 --> 00:25:42,634
And the most common place where problems arise is the

373
00:25:42,672 --> 00:25:46,502
process of converting experiments code to the production

374
00:25:46,566 --> 00:25:50,474
code. Then you should avoid known

375
00:25:50,522 --> 00:25:54,858
unsafe functions like insecure pickling, object serialization,

376
00:25:54,954 --> 00:25:58,430
and other things. And you should only use

377
00:25:58,580 --> 00:26:03,726
known good libraries and diversions and check dependencies

378
00:26:03,758 --> 00:26:07,474
for vulnerabilities. And finally,

379
00:26:07,592 --> 00:26:11,106
for robust and secure machine learning, you should remember

380
00:26:11,208 --> 00:26:14,414
about algorithms based threats

381
00:26:14,462 --> 00:26:18,582
scenarios. So if your primary concern is manipulation of

382
00:26:18,636 --> 00:26:22,134
decisions with adversarial examples, then you should

383
00:26:22,172 --> 00:26:25,474
think about training a model with adversarial

384
00:26:25,522 --> 00:26:28,810
examples. And if you care more about privacy,

385
00:26:29,230 --> 00:26:32,906
you could try training with federated learning or even train on

386
00:26:32,928 --> 00:26:37,050
encrypted data. The next stage is

387
00:26:37,120 --> 00:26:40,662
model validation. At this stage, we take the model built

388
00:26:40,736 --> 00:26:44,346
from the previous stage and conduct model evaluations

389
00:26:44,378 --> 00:26:48,762
and acceptance testing in terms of model accuracy

390
00:26:48,826 --> 00:26:52,362
and business requirements. And if it meets the target baseline,

391
00:26:52,426 --> 00:26:55,854
then it's usually all. In some rare

392
00:26:55,902 --> 00:26:59,694
cases, we see robustness testing that is mostly focused

393
00:26:59,742 --> 00:27:03,650
on some non standard inputs or back testing.

394
00:27:03,990 --> 00:27:07,602
And our experience shows that the most mature

395
00:27:07,666 --> 00:27:11,094
industries are the banks in the United States because of

396
00:27:11,132 --> 00:27:14,902
the model. Risk management regulations also

397
00:27:14,956 --> 00:27:19,382
self driving companies because of the obvious safety concerns,

398
00:27:19,526 --> 00:27:23,306
and we see that some biometric and Internet companies

399
00:27:23,488 --> 00:27:27,126
also fighting some real attacks.

400
00:27:27,318 --> 00:27:30,306
Finally, some companies do compliance checks,

401
00:27:30,438 --> 00:27:33,870
but again mostly in regulated industries,

402
00:27:34,530 --> 00:27:37,806
and the outcome of this stage is essentially a

403
00:27:37,828 --> 00:27:42,014
green red light for deployment based on test reports with

404
00:27:42,052 --> 00:27:45,682
the evaluation metrics. The most common

405
00:27:45,736 --> 00:27:49,214
problem in productizing and protecting AI

406
00:27:49,262 --> 00:27:53,314
systems is that they're barely tested because there is very

407
00:27:53,352 --> 00:27:57,602
little knowledge and no actionable frameworks or best practices.

408
00:27:57,746 --> 00:28:01,554
And it's very rare that testing goes beyond model performance

409
00:28:01,602 --> 00:28:05,794
metrics. Next, even when there is robustness

410
00:28:05,842 --> 00:28:09,258
testing, we still see few problems

411
00:28:09,344 --> 00:28:13,402
here, like not all models are covered by

412
00:28:13,456 --> 00:28:16,986
tests. It could be a limited number of

413
00:28:17,008 --> 00:28:20,194
tests or limited depth of these tests,

414
00:28:20,342 --> 00:28:23,582
and the formal approach happens even

415
00:28:23,636 --> 00:28:26,954
when companies do it for regulation or safety

416
00:28:27,002 --> 00:28:31,434
concerns. And it's even more rare that they do adversarial

417
00:28:31,482 --> 00:28:34,290
testing, which is obviously very dangerous.

418
00:28:35,750 --> 00:28:39,742
Another common problem is that AI heavily relies

419
00:28:39,806 --> 00:28:43,342
on traditional infrastructure and software environments,

420
00:28:43,486 --> 00:28:46,570
and unless companies use some proprietary

421
00:28:46,670 --> 00:28:50,546
end to end platform, it's very common to put models into docker

422
00:28:50,578 --> 00:28:54,038
containers and hosted somewhere in the Amazon cloud.

423
00:28:54,204 --> 00:28:58,390
And as you know, most containers are vulnerable by default

424
00:28:58,470 --> 00:29:01,980
and most clouds are never responsible for your security.

425
00:29:03,710 --> 00:29:06,810
There are several ways this could be addressed.

426
00:29:07,470 --> 00:29:11,198
To work on testing coverage, you really need to have security

427
00:29:11,284 --> 00:29:14,906
governance, so the testing should be informed by asset

428
00:29:14,938 --> 00:29:18,794
registers with models and data with assigned risk

429
00:29:18,842 --> 00:29:22,822
levels and based on security specification,

430
00:29:22,906 --> 00:29:26,242
including attack surface and

431
00:29:26,296 --> 00:29:29,860
threats models that we created before the development started,

432
00:29:30,310 --> 00:29:34,254
and also check for compliance with the functional

433
00:29:34,302 --> 00:29:37,830
specification like model

434
00:29:37,900 --> 00:29:41,254
cards or data sheets or service information. All of these

435
00:29:41,292 --> 00:29:45,094
artifacts define the AI system behavior and

436
00:29:45,132 --> 00:29:47,910
should affect the scope of security testing.

437
00:29:49,070 --> 00:29:53,606
For better security validation, you should have testing playbooks.

438
00:29:53,798 --> 00:29:58,102
Scenarios I can suggest include basic adversarial testing,

439
00:29:58,166 --> 00:30:01,710
like a sanity check. If it fails on the most

440
00:30:01,780 --> 00:30:05,070
basic tests, you probably shouldn't test further.

441
00:30:05,730 --> 00:30:09,982
Then you can use a bigger repository of attacks with

442
00:30:10,036 --> 00:30:13,438
the most common tests, similar to how application

443
00:30:13,524 --> 00:30:16,450
security has OS top ten attacks,

444
00:30:16,950 --> 00:30:21,742
and finish with custom and threat based scenarios

445
00:30:21,886 --> 00:30:26,290
for the kind of real world penetration

446
00:30:26,370 --> 00:30:30,086
testing. And of course you

447
00:30:30,108 --> 00:30:33,654
need to secure the

448
00:30:33,692 --> 00:30:37,000
environment and infrastructure. As for any other application,

449
00:30:37,390 --> 00:30:40,298
and it depends a lot on your AI system design,

450
00:30:40,384 --> 00:30:44,106
but commonly you should scan containers for

451
00:30:44,128 --> 00:30:47,402
known vulnerabilities, be conscious about

452
00:30:47,456 --> 00:30:50,718
any external dependencies in the system,

453
00:30:50,804 --> 00:30:54,750
and ideally scan the entire infrastructure as code

454
00:30:54,820 --> 00:30:56,750
for security and compliance.

455
00:30:59,250 --> 00:31:03,294
The next stage is model deployment. If the validation stage

456
00:31:03,342 --> 00:31:07,214
gave us a green light and the model code with fixed

457
00:31:07,262 --> 00:31:10,046
weights, probably packaged in a container,

458
00:31:10,158 --> 00:31:13,460
is deployed and ready to accept connections. Here,

459
00:31:14,310 --> 00:31:18,086
the model inference step is essentially the

460
00:31:18,108 --> 00:31:22,386
model runtime processing inputs and responding and outputs

461
00:31:22,498 --> 00:31:26,034
that AI users incorporate in their decision

462
00:31:26,082 --> 00:31:29,366
making. And by model serving I mean

463
00:31:29,468 --> 00:31:32,620
like general operations of the AI system.

464
00:31:33,550 --> 00:31:37,034
So the final outcome of this entire stage is that a

465
00:31:37,072 --> 00:31:40,246
concrete model is deployed, the model behaves

466
00:31:40,278 --> 00:31:43,630
as expected, and the system is in a good condition.

467
00:31:45,490 --> 00:31:48,718
Let's see what can go wrong here. First of all,

468
00:31:48,804 --> 00:31:52,334
the model that is actually deployed could be different from what you

469
00:31:52,372 --> 00:31:56,114
wanted to deploy before the deployment. The model

470
00:31:56,232 --> 00:32:00,580
could have been maliciously modified in the model store

471
00:32:00,950 --> 00:32:04,546
by changing its code or replacing its weights. Essentially, you could

472
00:32:04,568 --> 00:32:07,670
deploy a Trojan here, not the model you developed.

473
00:32:09,450 --> 00:32:13,990
The next problem is the classic problem of adversa attacks.

474
00:32:14,410 --> 00:32:18,342
The three main attack groups here are manipulation

475
00:32:18,406 --> 00:32:22,054
attacks that include evasion, reprogramming and resource

476
00:32:22,102 --> 00:32:26,060
exhaustion, which is essentially a denial of service attack,

477
00:32:26,830 --> 00:32:30,570
then exfiltration attacks that steal model

478
00:32:30,640 --> 00:32:33,600
algorithm details, extract private data,

479
00:32:34,130 --> 00:32:38,346
and infection attacks. And even though infection is usually considered

480
00:32:38,378 --> 00:32:42,474
as a train time attack, it's possible that those infected

481
00:32:42,522 --> 00:32:45,966
examples are received during model runtime.

482
00:32:46,078 --> 00:32:49,826
So essentially the system collected all the inputs and

483
00:32:49,848 --> 00:32:53,442
later used in updating data

484
00:32:53,496 --> 00:32:57,686
sets. And finally, unrestricted access

485
00:32:57,868 --> 00:33:01,074
can help develop various types of attacks.

486
00:33:01,202 --> 00:33:04,594
It could be absence of any access controls, or absence

487
00:33:04,642 --> 00:33:08,890
of user accounts at all, or just unlimited model queries

488
00:33:09,230 --> 00:33:12,940
or unrestricted data submissions, and many other things.

489
00:33:14,350 --> 00:33:18,090
So what could we do here? You obviously should check

490
00:33:18,160 --> 00:33:21,626
model authenticity. Depending on the system pipeline

491
00:33:21,658 --> 00:33:25,658
configuration. This could be a verification of model artifacts

492
00:33:25,754 --> 00:33:28,730
from the model store or code hosting.

493
00:33:28,890 --> 00:33:31,962
Or it could be a wider model governance approach

494
00:33:32,026 --> 00:33:36,946
where you track the entire model lifecycle in

495
00:33:36,968 --> 00:33:40,994
a single platform. And on top of this you should control who

496
00:33:41,032 --> 00:33:44,786
can roll out models to production to

497
00:33:44,808 --> 00:33:48,566
protect from attacks against AI algorithms. You should keep in

498
00:33:48,588 --> 00:33:52,450
mind your threat model, because it's impossible

499
00:33:52,530 --> 00:33:55,810
to have a model that is fully secure

500
00:33:55,890 --> 00:33:59,818
against every existing attack, but it's possible

501
00:33:59,904 --> 00:34:03,414
to harden the model against concrete threat

502
00:34:03,462 --> 00:34:07,274
scenarios. There are three main directions you can go

503
00:34:07,312 --> 00:34:11,358
here secure training in the first place

504
00:34:11,524 --> 00:34:15,310
with adversa data sets or differential privacy,

505
00:34:16,050 --> 00:34:19,322
then runtime defenses like input preprocessing

506
00:34:19,386 --> 00:34:23,390
or output postprocessing, and also what I call operational

507
00:34:23,470 --> 00:34:26,690
defenses, things like rate limiting.

508
00:34:27,990 --> 00:34:32,130
And the last point is about secure communications and access controls.

509
00:34:32,470 --> 00:34:36,482
The first piece of advice is to control authorized

510
00:34:36,546 --> 00:34:40,178
usage based on the intended model usage,

511
00:34:40,274 --> 00:34:43,160
so you can introduce some rate limits by design,

512
00:34:43,930 --> 00:34:47,886
then mutual authentication of requests and responses

513
00:34:48,018 --> 00:34:51,590
so you can check the requests come from eligible users,

514
00:34:51,670 --> 00:34:55,082
and also you can protect users from man in the middle

515
00:34:55,136 --> 00:34:58,570
attacks and encryption of model

516
00:34:58,640 --> 00:35:02,702
inputs and outputs. Intransit should also

517
00:35:02,756 --> 00:35:06,430
protect both parties from eavesdropping,

518
00:35:07,250 --> 00:35:11,006
and the final stage is model monitoring. So after the

519
00:35:11,028 --> 00:35:14,126
model started producing predictions,

520
00:35:14,238 --> 00:35:17,522
you should carefully monitor it. And not just because every

521
00:35:17,576 --> 00:35:20,914
serious system should be monitored, but also because of

522
00:35:21,032 --> 00:35:24,050
non deterministic nature of AI systems.

523
00:35:24,470 --> 00:35:28,102
Basically, it can change its behavior over time or

524
00:35:28,156 --> 00:35:31,240
with unexpected inputs to the model.

525
00:35:31,770 --> 00:35:35,046
That's why the model performance monitoring should

526
00:35:35,068 --> 00:35:38,774
be a good start. The common use cases include tracking

527
00:35:38,822 --> 00:35:42,758
prediction accuracy and detecting model drift. That essentially

528
00:35:42,854 --> 00:35:46,646
indicates that the real world data is now different from the data we trained

529
00:35:46,678 --> 00:35:50,622
on. Then anomaly detection in

530
00:35:50,676 --> 00:35:53,998
model inputs and outputs, or in some general

531
00:35:54,084 --> 00:35:57,658
request patterns can be another indicator

532
00:35:57,754 --> 00:36:01,086
of model health. And all

533
00:36:01,108 --> 00:36:04,734
of this should be looped back to AI development teams

534
00:36:04,782 --> 00:36:08,530
for continuous improvement. So the outcome of this entire

535
00:36:08,600 --> 00:36:12,322
stage is kind of peace of mind when you know

536
00:36:12,456 --> 00:36:16,566
that things are working as expected. So the

537
00:36:16,588 --> 00:36:20,034
most common problems here are lack of monitoring

538
00:36:20,082 --> 00:36:24,066
in the first place. And I'm talking not just about basic logs,

539
00:36:24,098 --> 00:36:28,070
but also about usable monitoring infrastructure.

540
00:36:28,430 --> 00:36:32,490
There is no way you can do something useful with logs that are only

541
00:36:32,560 --> 00:36:36,310
written in API or proxy log files

542
00:36:36,390 --> 00:36:38,650
or saved in container storage.

543
00:36:39,710 --> 00:36:43,046
Another problem is analytical capability.

544
00:36:43,238 --> 00:36:46,782
So when you have all those logs, do you really know what to look for,

545
00:36:46,916 --> 00:36:50,266
how to group or correlate incidents, and what is normal

546
00:36:50,298 --> 00:36:53,474
and what is not? And finally, with all

547
00:36:53,512 --> 00:36:57,442
those monitoring capabilities, are you passively watching or

548
00:36:57,496 --> 00:37:00,020
actively protecting your a system?

549
00:37:01,190 --> 00:37:04,574
So what you should start with is activity

550
00:37:04,622 --> 00:37:08,422
login with some usable and

551
00:37:08,476 --> 00:37:12,386
actionable dashboards that includes monitoring,

552
00:37:12,418 --> 00:37:16,370
model performance and metrics, metadata from requests,

553
00:37:16,530 --> 00:37:20,154
input and output errors, and you also should

554
00:37:20,192 --> 00:37:24,202
monitor requests at the user level for access control and

555
00:37:24,336 --> 00:37:27,786
behavior anomalies. Then when

556
00:37:27,808 --> 00:37:31,834
you have those monitoring capabilities, you need to add an analytics layer

557
00:37:31,882 --> 00:37:35,774
so you could act on this data. And the basic advice is

558
00:37:35,812 --> 00:37:39,418
to have event becoming so you could filter events

559
00:37:39,514 --> 00:37:43,514
by importance and prioritize investigation. Then it's useful

560
00:37:43,562 --> 00:37:47,022
to have error types so you could identify and group events

561
00:37:47,086 --> 00:37:51,362
for correlations. And ideally you should have more

562
00:37:51,416 --> 00:37:54,562
advanced stuff for incident forensics and

563
00:37:54,616 --> 00:37:57,170
for traceability like sessions,

564
00:37:57,330 --> 00:38:00,946
request hashes and other types of profiling

565
00:38:00,978 --> 00:38:03,160
behaviors inside your AI application.

566
00:38:04,410 --> 00:38:07,614
And finally, the most advanced step is automated

567
00:38:07,682 --> 00:38:11,450
detection and one possible automated response.

568
00:38:11,950 --> 00:38:15,530
So for regular performance incidents it could be as simple

569
00:38:15,600 --> 00:38:19,494
as alerting. And for some input trash unexpected

570
00:38:19,542 --> 00:38:23,150
outputs, like with the recent prompt injection attacks,

571
00:38:23,570 --> 00:38:28,190
it could have some safety filters or custom

572
00:38:28,260 --> 00:38:32,346
responses with custom errors. And for some classic adversarial

573
00:38:32,378 --> 00:38:36,478
attacks it could be either activation of some defenses

574
00:38:36,574 --> 00:38:39,966
or custom responses, or even account blocking.

575
00:38:40,158 --> 00:38:44,014
And of course for all these activities it's important to have feedback

576
00:38:44,062 --> 00:38:47,334
loops. So not only the AI system

577
00:38:47,452 --> 00:38:51,862
issues immediately addressed, but also best practices and

578
00:38:51,996 --> 00:38:55,750
processes updated and some additional trainings conducted.

579
00:38:57,210 --> 00:39:01,446
So now you can see the whole picture of how Mlcikov's

580
00:39:01,478 --> 00:39:05,194
pipeline should look like. And of course I've simplified some

581
00:39:05,232 --> 00:39:08,490
things for introducing this topic the very first

582
00:39:08,560 --> 00:39:11,994
time. And it's also worth mentioning that I didn't

583
00:39:12,042 --> 00:39:15,614
cover things related to traditional cybersecurity that

584
00:39:15,652 --> 00:39:19,514
are obviously also important. For instance,

585
00:39:19,562 --> 00:39:23,358
during work with the data and models,

586
00:39:23,454 --> 00:39:27,422
which happens on data scientist machines, it's important to secure

587
00:39:27,486 --> 00:39:31,422
workspaces. They often have collaboration

588
00:39:31,486 --> 00:39:35,398
tools like Google Collab or Jupyter notebooks that have

589
00:39:35,484 --> 00:39:39,654
remote shared access and could expose access to the

590
00:39:39,772 --> 00:39:43,570
operating system and some other critical system functions,

591
00:39:43,650 --> 00:39:46,840
or provide direct access to data.

592
00:39:47,550 --> 00:39:51,622
And there are known ransomware attacks against Jupyter

593
00:39:51,686 --> 00:39:55,190
setups. And also it's a well known cybersecurity

594
00:39:55,270 --> 00:39:59,830
problem that entire data storages could be exposed at

595
00:39:59,920 --> 00:40:02,510
Amazon or elastic instances,

596
00:40:03,410 --> 00:40:06,766
and then the security of the pipeline itself is

597
00:40:06,788 --> 00:40:09,882
an important topic. What you should pay attention

598
00:40:09,946 --> 00:40:14,082
to is access control for who can commit code,

599
00:40:14,216 --> 00:40:18,334
publish models and data sets, pull artifacts,

600
00:40:18,462 --> 00:40:22,354
approve releases, promote models for deployment and

601
00:40:22,392 --> 00:40:26,566
so on. And of course, secure configuration of

602
00:40:26,748 --> 00:40:31,122
machine learning infrastructure is also important, like the code hosting

603
00:40:31,186 --> 00:40:33,960
feature stores, experiment trackers and so on.

604
00:40:35,850 --> 00:40:39,354
So after this presentation, I want you to remember a few key

605
00:40:39,392 --> 00:40:42,650
ideas. First is every AI system is

606
00:40:42,720 --> 00:40:45,894
vulnerable by design and it expands the attack surface

607
00:40:45,942 --> 00:40:49,494
of software. There are real attacks against AI

608
00:40:49,542 --> 00:40:52,678
systems already in almost every industry and

609
00:40:52,704 --> 00:40:56,094
every application and every data type, so you

610
00:40:56,132 --> 00:41:00,094
should deliberately work on protecting from these

611
00:41:00,132 --> 00:41:03,906
threats. Then. Traditional security

612
00:41:04,008 --> 00:41:07,778
solutions cannot protect AI systems, so don't expect

613
00:41:07,864 --> 00:41:11,118
your firewalls or vulnerability scanners to solve

614
00:41:11,134 --> 00:41:14,910
the problem because the problem of AI systems is

615
00:41:15,000 --> 00:41:18,534
very unique. And last, you should

616
00:41:18,572 --> 00:41:21,862
think about securing the entire AI system,

617
00:41:21,996 --> 00:41:25,378
not just an AI algorithm as often discussed

618
00:41:25,474 --> 00:41:28,630
in the context of adversa attacks.

619
00:41:28,790 --> 00:41:32,250
And also remember that defenses are often more

620
00:41:32,320 --> 00:41:36,218
operational than algorithmic. So start building this

621
00:41:36,304 --> 00:41:39,610
internal infrastructure and best practices sooner.

622
00:41:40,930 --> 00:41:44,382
And if you work in devsecops and you have

623
00:41:44,436 --> 00:41:47,710
AI in your product or in any other product

624
00:41:47,780 --> 00:41:51,758
teams, you should definitely share your security concerns with

625
00:41:51,924 --> 00:41:55,662
AI developers because they rarely know about the real

626
00:41:55,716 --> 00:41:58,922
threats landscape. And if you find this presentation

627
00:41:58,986 --> 00:42:02,878
useful, I ask you to share it with your colleagues and I hope

628
00:42:02,964 --> 00:42:06,486
it will be useful for them too. So this

629
00:42:06,508 --> 00:42:10,598
is it. I appreciate your attention. If you're interested

630
00:42:10,684 --> 00:42:14,870
in this topic or in any form of collaboration on Mlsecops,

631
00:42:15,210 --> 00:42:18,918
search my name on LinkedIn or Twitter and make sure you drop me a

632
00:42:18,924 --> 00:42:20,100
message. Thank you.

