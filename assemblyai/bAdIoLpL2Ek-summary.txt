
 The speakers discuss their team's development of an experimental feature using LLMs in just six weeks. They recognize that failures with LLMs are inevitable and the focus should be on detecting issues quickly. The team captures rich instrumentation data to provide visibility into user interactions and system performance. The instrumentation captures a full trace of each user interaction, including metadata about the LLM request and response. Two other companies, Duolingo and Intercom, are also discussed who use similar observability practices to understand their LLM-powered features. Duolingo finds the bulk of latency comes from their own code rather than the LLM itself. Intercom captures extensive metadata to track how changes to their LLM implementation impact the user experience.