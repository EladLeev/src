1
00:00:36,130 --> 00:00:39,330
Hi everyone, this is Uma Mukara,

2
00:00:39,410 --> 00:00:42,840
co founder CEO of my data.

3
00:00:43,290 --> 00:00:48,002
Today I'm here at the SRE 2020 show organized

4
00:00:48,066 --> 00:00:51,840
by Con 42 Dot folks us to talk about

5
00:00:52,370 --> 00:00:55,470
increasing Kubernetes resilience.

6
00:00:57,010 --> 00:01:00,618
This should be a topic of interest for sres.

7
00:01:00,794 --> 00:01:04,450
Whoever is already planning to practice

8
00:01:05,030 --> 00:01:09,410
has engineering or already practicing chaos engineering.

9
00:01:09,990 --> 00:01:13,170
We're going to talk this topic,

10
00:01:13,750 --> 00:01:16,550
especially when it applies to kubernetes.

11
00:01:18,090 --> 00:01:21,202
So before we delve deep into this topic,

12
00:01:21,266 --> 00:01:25,174
let's look at what we

13
00:01:25,212 --> 00:01:28,790
do at my data. At Myadata

14
00:01:30,590 --> 00:01:34,954
we sponsor two open source projects, open EBS and

15
00:01:34,992 --> 00:01:39,078
Litmus openebs for cloud native data management.

16
00:01:39,254 --> 00:01:42,634
Litmus for cloud native chaos Engineering.

17
00:01:42,762 --> 00:01:46,542
We also have the commercial SaaS solution for

18
00:01:46,596 --> 00:01:49,870
cloud native data management called Kubera.

19
00:01:50,290 --> 00:01:54,510
So with Kubera, sres can turn kubernetes

20
00:01:54,590 --> 00:01:59,090
into a data plane and get the complete solution

21
00:01:59,750 --> 00:02:02,050
around cloud native data management.

22
00:02:03,350 --> 00:02:06,534
So in this talk we sre going to talk

23
00:02:06,572 --> 00:02:10,598
about the importance of resilience and how do

24
00:02:10,604 --> 00:02:16,822
you get resilience on kubernetes and an

25
00:02:16,876 --> 00:02:21,274
introduction to litmus chaos, how it works and

26
00:02:21,392 --> 00:02:26,042
how automate chaos using

27
00:02:26,096 --> 00:02:29,900
litmus chaos and obviously how

28
00:02:30,770 --> 00:02:34,240
you end up getting higher resilience in that process.

29
00:02:34,850 --> 00:02:38,880
We'll also do a brief demo of

30
00:02:39,250 --> 00:02:42,834
Bitmas, how it

31
00:02:42,872 --> 00:02:47,250
works when you try to use this automation tools,

32
00:02:47,750 --> 00:02:51,762
et cetera. So what

33
00:02:51,816 --> 00:02:54,500
is the state of kubernetes today?

34
00:02:54,970 --> 00:02:58,374
Kubernetes is starting to have

35
00:02:58,412 --> 00:03:02,760
greater and greater adoption. It is believed that

36
00:03:03,130 --> 00:03:07,190
in terms of adoption it has crossed the chasm

37
00:03:07,950 --> 00:03:12,406
and most of the IT organizations

38
00:03:12,598 --> 00:03:16,250
are either planning to adopt

39
00:03:16,590 --> 00:03:20,090
or already have adopted.

40
00:03:20,610 --> 00:03:24,670
So what we need right now is more

41
00:03:24,740 --> 00:03:28,394
complete solutions around kubernetes so that the choice

42
00:03:28,442 --> 00:03:32,474
of adopting kubernetes becomes

43
00:03:32,602 --> 00:03:36,770
a valuable one when you have the complete solutions around kubernetes.

44
00:03:38,310 --> 00:03:42,914
So one of the more important tool

45
00:03:43,112 --> 00:03:47,026
is a tool that helps

46
00:03:47,058 --> 00:03:49,640
you keep the resilience high.

47
00:03:50,490 --> 00:03:54,354
You obviously want to keep your kubernetes

48
00:03:54,402 --> 00:03:57,786
clusters and applications on that running all the

49
00:03:57,808 --> 00:04:01,210
time and achieve your or meet your slas.

50
00:04:02,190 --> 00:04:06,634
So for that you need to have this one

51
00:04:06,672 --> 00:04:10,782
tool or an infrastructure set of tools or practices or

52
00:04:10,836 --> 00:04:14,510
processes to keep this resilience

53
00:04:15,090 --> 00:04:19,390
higher than what for slas demand.

54
00:04:19,970 --> 00:04:24,990
So what is resilience? And resilience

55
00:04:25,070 --> 00:04:29,170
is system's ability to adopt

56
00:04:30,550 --> 00:04:33,170
to a chaos load.

57
00:04:34,470 --> 00:04:37,906
Whenever there is a fault happens, it recovers

58
00:04:37,938 --> 00:04:42,200
automatically without affecting any service

59
00:04:43,130 --> 00:04:46,726
user facing services. So what are some of

60
00:04:46,908 --> 00:04:50,986
the examples of this resilience? Or when you

61
00:04:51,008 --> 00:04:54,170
don't have a resilience you call have a weakness?

62
00:04:54,990 --> 00:04:58,874
Some examples that are common in Kubernetes is pods are

63
00:04:58,912 --> 00:05:02,254
evicted for various reasons. If your

64
00:05:02,292 --> 00:05:05,466
system is resilient, your implementation is resilient,

65
00:05:05,658 --> 00:05:09,790
your pods are automatically rescheduled, your services are not affected.

66
00:05:10,370 --> 00:05:12,742
That is a sign of resilience.

67
00:05:12,906 --> 00:05:16,740
And on the other hand, if your services are

68
00:05:17,350 --> 00:05:20,690
becoming slower than certain

69
00:05:20,760 --> 00:05:24,850
threshold, or they're completely down, then that's not healthy.

70
00:05:25,610 --> 00:05:29,974
That means there is a weakness that you need to fix.

71
00:05:30,172 --> 00:05:33,254
Some other examples are nodes going to not

72
00:05:33,292 --> 00:05:37,560
ready state, which is a little more common on

73
00:05:38,430 --> 00:05:42,060
large infrastructure setups on

74
00:05:42,590 --> 00:05:46,794
cloud service providers. And when

75
00:05:46,912 --> 00:05:51,214
nodes go to not ready state depends on

76
00:05:51,412 --> 00:05:55,582
the application that you're using. The blast radius can be very high

77
00:05:55,716 --> 00:06:00,302
and that's not healthy. So you

78
00:06:00,356 --> 00:06:04,034
have to implement your services in such a way that when

79
00:06:04,072 --> 00:06:07,442
these nodes go down or they go into

80
00:06:07,496 --> 00:06:10,898
not ready state, you have to be able to continue

81
00:06:10,984 --> 00:06:14,386
to survive that situation. That's the

82
00:06:14,408 --> 00:06:17,410
resilience that you would want. And similarly,

83
00:06:17,570 --> 00:06:20,770
as you have hundreds and thousands of containers,

84
00:06:20,850 --> 00:06:24,662
it's possible that some of them are not behaving exactly

85
00:06:24,796 --> 00:06:27,986
the way you expect them. In some cases,

86
00:06:28,098 --> 00:06:31,914
some memory leaks are also some

87
00:06:31,952 --> 00:06:36,794
of the common herd examples in

88
00:06:36,832 --> 00:06:40,702
large scale deployments. So these are some

89
00:06:40,756 --> 00:06:44,046
examples. Of course faults, you always expect them not

90
00:06:44,068 --> 00:06:47,374
to happen. But you also know that

91
00:06:47,492 --> 00:06:51,534
a fault can happen anytime respective of

92
00:06:51,572 --> 00:06:55,742
how much careful you are. So it is inevitable

93
00:06:55,806 --> 00:06:59,234
that some fault will happen at some point

94
00:06:59,272 --> 00:07:02,510
of time. So you have to stay afloat

95
00:07:02,590 --> 00:07:06,694
for that. And that's resilience. And the

96
00:07:06,732 --> 00:07:10,246
resilience is more important now in the

97
00:07:10,268 --> 00:07:14,354
Kubernetes environment because there is this promise

98
00:07:14,402 --> 00:07:18,300
of common API, everyone has adopted it.

99
00:07:19,310 --> 00:07:23,260
If you see CNCF landscape then

100
00:07:24,190 --> 00:07:27,914
you realize how many all the

101
00:07:27,952 --> 00:07:31,600
vendors users across the spectrum SRE

102
00:07:32,130 --> 00:07:34,686
adopting kubernetes in some form,

103
00:07:34,788 --> 00:07:38,654
delivering solutions or building solutions based on the

104
00:07:38,692 --> 00:07:42,366
services available. And what they are trying

105
00:07:42,388 --> 00:07:46,546
to do is because of the

106
00:07:46,568 --> 00:07:50,414
new solutions available, they themselves are building stronger

107
00:07:50,462 --> 00:07:53,986
CI CD to meet this demand of

108
00:07:54,088 --> 00:07:58,214
agility, right? So together with so much

109
00:07:58,252 --> 00:08:02,054
adoption and stronger CI practices that

110
00:08:02,092 --> 00:08:05,270
these users or vendors SRE following,

111
00:08:05,770 --> 00:08:09,722
you will see that the applications on

112
00:08:09,776 --> 00:08:13,402
Kubernetes are changing pretty fast. It's a good sign

113
00:08:13,456 --> 00:08:17,690
you don't need to wait for many months before a fix is arriving.

114
00:08:18,990 --> 00:08:22,286
So it is a good news that the system

115
00:08:22,388 --> 00:08:26,906
is more dynamic and they're coming multifold

116
00:08:26,938 --> 00:08:29,760
times faster than earlier times.

117
00:08:30,770 --> 00:08:35,890
For example earlier the

118
00:08:35,960 --> 00:08:39,810
databases. Upgrades to the databases may happen once

119
00:08:39,880 --> 00:08:43,330
in six months or a year, but now

120
00:08:43,480 --> 00:08:46,934
it may be every quarter because they have

121
00:08:46,972 --> 00:08:49,560
moved on to microservices model, right?

122
00:08:50,810 --> 00:08:54,562
And the resilience also depends

123
00:08:54,626 --> 00:08:59,254
on various other infrastructure

124
00:08:59,302 --> 00:09:02,714
items or on services in your

125
00:09:02,752 --> 00:09:06,742
Kubernetes environment. Let's look at the resilience

126
00:09:06,806 --> 00:09:10,254
dependency stack. So at the bottom you

127
00:09:10,292 --> 00:09:13,838
have platform service. You can

128
00:09:14,004 --> 00:09:18,014
expect that some faults can go in that

129
00:09:18,052 --> 00:09:21,822
area. For example, node going into

130
00:09:21,956 --> 00:09:26,094
not ready state is a famous example. And Kubernetes

131
00:09:26,142 --> 00:09:29,954
services itself can be fragile sometimes

132
00:09:30,072 --> 00:09:33,922
if you are not implemented properly. And other

133
00:09:33,976 --> 00:09:37,770
cloud native services that surround kubernetes like DNS,

134
00:09:37,950 --> 00:09:42,022
Prometheus Envoy and other cloud

135
00:09:42,076 --> 00:09:45,654
native databases, they all can get

136
00:09:45,692 --> 00:09:48,298
into some kind of unexpected state.

137
00:09:48,464 --> 00:09:52,826
And on top of all these things is your app.

138
00:09:53,008 --> 00:09:56,714
So if you're looking at slas at

139
00:09:56,752 --> 00:10:00,880
your application level, the resilience of your application,

140
00:10:01,490 --> 00:10:07,502
it really depends on a lot of other services

141
00:10:07,636 --> 00:10:10,602
in your environment. So to summarize,

142
00:10:10,746 --> 00:10:14,882
90% of resilience of your application really

143
00:10:14,936 --> 00:10:18,770
depends on the components that you are not developing or

144
00:10:18,840 --> 00:10:22,930
you don't own. So this is something very

145
00:10:23,000 --> 00:10:27,506
important together with dependency

146
00:10:27,618 --> 00:10:30,694
of other application and they sre coming much

147
00:10:30,732 --> 00:10:34,854
faster. Something that you really need to do

148
00:10:35,052 --> 00:10:38,310
to keep your resilience high is to keep checking

149
00:10:38,470 --> 00:10:43,290
how is my system's resilience? You have to constantly

150
00:10:44,510 --> 00:10:47,660
validate that fact.

151
00:10:48,190 --> 00:10:51,920
Do I have a weakness or my system is resilient enough or not?

152
00:10:52,530 --> 00:10:54,670
So how do you check resilience?

153
00:10:56,050 --> 00:10:59,210
Well, you has to practice has engineering,

154
00:10:59,290 --> 00:11:02,606
that's the topic here, right? So how do

155
00:11:02,628 --> 00:11:06,194
you do that? Typically is you have to know what is your

156
00:11:06,232 --> 00:11:09,620
steady state, either a service or an application,

157
00:11:10,790 --> 00:11:13,902
and then you yourself induce a fault.

158
00:11:13,966 --> 00:11:17,554
Don't wait for a fault to happen. You introduce a fault

159
00:11:17,682 --> 00:11:21,398
and then verify whether steady state condition has

160
00:11:21,484 --> 00:11:25,474
been achieved. We talked about some examples of resilience.

161
00:11:25,522 --> 00:11:28,882
The same thing applies here, right? If the system is resilient,

162
00:11:28,946 --> 00:11:32,346
you're good, if not, you found a weakness. But that's as

163
00:11:32,368 --> 00:11:35,260
simple as that. So in other words,

164
00:11:35,630 --> 00:11:39,402
for the two reasons, the dynamism in the services and

165
00:11:39,456 --> 00:11:43,614
the dependency of those services for your application,

166
00:11:43,812 --> 00:11:47,626
you need to practice chaos engineering in order to achieve

167
00:11:47,738 --> 00:11:50,110
the required levels of resilience.

168
00:11:51,650 --> 00:11:54,978
That's good. And let's also talk about

169
00:11:55,144 --> 00:11:58,942
how do I do that? Chaos engineering.

170
00:11:59,086 --> 00:12:02,546
And before we talk about that, it's important to

171
00:12:02,568 --> 00:12:05,922
know how the configuration or operations

172
00:12:05,986 --> 00:12:11,126
are managed in the cloud native world. Today it

173
00:12:11,148 --> 00:12:13,430
is by using Git ops.

174
00:12:14,090 --> 00:12:17,834
The git functionality is helpful in managing your

175
00:12:17,872 --> 00:12:21,318
configurations as well. The version configuration

176
00:12:21,494 --> 00:12:25,398
and that concept has led

177
00:12:25,574 --> 00:12:29,526
to using the operations

178
00:12:29,638 --> 00:12:33,806
declarative yamls also to automate your

179
00:12:33,908 --> 00:12:37,390
config changes. So this is your cloud

180
00:12:37,460 --> 00:12:40,506
native way of managing your configuration

181
00:12:40,618 --> 00:12:44,162
of your application. So this

182
00:12:44,216 --> 00:12:47,954
is being applied across the spectrum now to

183
00:12:47,992 --> 00:12:51,346
manage Kubernetes services, various applications on

184
00:12:51,368 --> 00:12:56,590
it, various resource management strategies

185
00:12:56,670 --> 00:13:00,610
and policy management. Everything is being done

186
00:13:00,680 --> 00:13:04,358
through Githubs. That is the new

187
00:13:04,444 --> 00:13:08,470
way of doing things for DevOps. And you can

188
00:13:08,540 --> 00:13:11,758
also apply the same Gitops

189
00:13:11,794 --> 00:13:15,034
to resilience checks. You don't need to

190
00:13:15,232 --> 00:13:18,954
start doing something new or new

191
00:13:18,992 --> 00:13:22,734
way of doing things or not

192
00:13:22,772 --> 00:13:26,430
needed for bringing in your resilience checks.

193
00:13:26,930 --> 00:13:31,614
So how would you do that? So in order to get that

194
00:13:31,652 --> 00:13:35,134
resilience check into place, you bring in a has

195
00:13:35,182 --> 00:13:38,718
operator and then some chaos experiments,

196
00:13:38,894 --> 00:13:42,386
and you make changes to the

197
00:13:42,408 --> 00:13:46,520
has experiment and somebody picks up that change

198
00:13:46,890 --> 00:13:50,722
and the chaos experiments are run or resilience

199
00:13:50,786 --> 00:13:53,986
checks are done and then you can observe the results

200
00:13:54,018 --> 00:13:58,662
of that. So this is a simple way of doing chaos

201
00:13:58,726 --> 00:14:02,490
management or chaos execution or fault induction

202
00:14:03,150 --> 00:14:06,810
using Kubernetes style,

203
00:14:07,150 --> 00:14:10,330
using Gitops, right, using operators.

204
00:14:11,170 --> 00:14:15,040
So let's look at what is,

205
00:14:15,490 --> 00:14:18,922
or at least let's summarize what is cloud native

206
00:14:18,986 --> 00:14:23,354
chaos engineering is you want to keep your resilience

207
00:14:23,402 --> 00:14:26,706
high, you want to practice has engineering and you want to

208
00:14:26,728 --> 00:14:30,322
do it the Kubernetes way, cloud native way.

209
00:14:30,456 --> 00:14:34,238
So what does it mean to do it in that fashion

210
00:14:34,334 --> 00:14:39,030
is you pick up an open source infrastructure component

211
00:14:39,610 --> 00:14:42,120
or infrastructure itself,

212
00:14:42,570 --> 00:14:46,680
because it is cooked up in open source, there's more community around it.

213
00:14:47,210 --> 00:14:50,426
You can depend on the survival of

214
00:14:50,448 --> 00:14:53,500
that technology for a long time. Right.

215
00:14:53,870 --> 00:14:58,170
That is very important. And the promise of Kubernetes

216
00:14:58,910 --> 00:15:02,400
has really realized, because it is

217
00:15:02,930 --> 00:15:07,102
neutrally governed, it is open source. Large vendors have come

218
00:15:07,156 --> 00:15:10,702
and depended on that purely because it is

219
00:15:10,756 --> 00:15:13,540
open source and well governed in an open way.

220
00:15:13,990 --> 00:15:17,762
And the next one is you have to have

221
00:15:17,896 --> 00:15:20,210
chaos APIs, crds,

222
00:15:21,990 --> 00:15:25,238
and that's very important to do it the

223
00:15:25,244 --> 00:15:28,518
Kubernetes way. And the third one is

224
00:15:28,684 --> 00:15:32,150
bring your own chaos model. You don't want to be

225
00:15:32,220 --> 00:15:36,306
tied to a particular way of doing chaos.

226
00:15:36,498 --> 00:15:39,978
It can keep improving, you can improve a certain

227
00:15:40,064 --> 00:15:44,410
type of experiments, or you already are

228
00:15:44,480 --> 00:15:47,914
in the process of developing new experiments and you want

229
00:15:47,952 --> 00:15:51,366
to use this framework for those

230
00:15:51,408 --> 00:15:55,790
new experiments as well. So you should be able to plug it in into

231
00:15:55,860 --> 00:15:59,262
that framework and then finally has itself

232
00:15:59,316 --> 00:16:02,506
has to be community oriented, primarily because of the

233
00:16:02,548 --> 00:16:06,382
reason that you will not be able to develop all sorts of chaos

234
00:16:06,446 --> 00:16:09,858
experiment yourself. You have to depend

235
00:16:09,944 --> 00:16:13,438
on the application owners, vendors,

236
00:16:13,614 --> 00:16:16,950
practitioners, whenever they learn a new

237
00:16:17,020 --> 00:16:21,190
way of introducing a fault, they can upstream

238
00:16:21,610 --> 00:16:26,230
that experiment. So that is the fourth principle.

239
00:16:26,890 --> 00:16:31,354
So this is generally putting forward what

240
00:16:31,392 --> 00:16:34,940
is cloud native has engineering is with that,

241
00:16:36,270 --> 00:16:39,526
let me introduce litmus has litmus.

242
00:16:39,718 --> 00:16:43,886
And litmus Chaos is

243
00:16:43,988 --> 00:16:48,154
a complete framework for finding weaknesses in Kubernetes

244
00:16:48,202 --> 00:16:51,562
platform, its implementations and applications

245
00:16:51,626 --> 00:16:54,660
running on kubernetes. So using this,

246
00:16:55,190 --> 00:16:58,974
developers and sres should be able to automate

247
00:16:59,022 --> 00:17:03,074
chaos in a cloud native way, right? So we talked

248
00:17:03,112 --> 00:17:07,826
about what is cloud native way. It's all about declarative

249
00:17:08,018 --> 00:17:11,910
chaos and being able to automate the entire chaos

250
00:17:12,330 --> 00:17:16,354
using Gitops. And Litmus Chaos

251
00:17:16,402 --> 00:17:20,530
is in sandbox. And the mission statement,

252
00:17:20,690 --> 00:17:23,994
as I just described, all about

253
00:17:24,112 --> 00:17:27,926
helping sres and developers bring out weaknesses,

254
00:17:28,038 --> 00:17:32,058
keep the resilience high. And Myadata

255
00:17:32,154 --> 00:17:35,518
is the prime sponsor of the project.

256
00:17:35,684 --> 00:17:39,870
But because of the vendor neutral governance

257
00:17:40,210 --> 00:17:43,182
and being in complete open source,

258
00:17:43,326 --> 00:17:47,074
there are a lot other companies that have

259
00:17:47,112 --> 00:17:51,182
joined as maintainers contributors. Some of them include

260
00:17:51,246 --> 00:17:54,734
Intuit, Amazon, Ringcentral, Container Solutions,

261
00:17:54,782 --> 00:17:58,706
et cetera. And one of the biggest assets of Litmus

262
00:17:58,738 --> 00:18:03,014
Chaos project is the hub itself. And we

263
00:18:03,052 --> 00:18:06,342
expect that more and more community members

264
00:18:06,476 --> 00:18:10,934
will upload or upstream their chaos experiments

265
00:18:10,982 --> 00:18:15,206
onto this hub and the community is really spread

266
00:18:15,238 --> 00:18:18,666
out. At the moment there are about more than 10,000

267
00:18:18,768 --> 00:18:20,910
installations of Litmus.

268
00:18:23,250 --> 00:18:26,762
It is starting to get wider

269
00:18:26,826 --> 00:18:27,710
adoption,

270
00:18:29,650 --> 00:18:32,946
and Litmus chaos as we described it,

271
00:18:32,968 --> 00:18:36,546
is completely cloud native. And the four principles that

272
00:18:36,568 --> 00:18:39,714
we described a little while ago

273
00:18:39,832 --> 00:18:43,682
applies to Litmus chaos. Hub is

274
00:18:43,736 --> 00:18:46,934
the community way of doing has and there sre

275
00:18:46,972 --> 00:18:50,594
some good examples of bringing

276
00:18:50,642 --> 00:18:54,118
their own chaos onto litmus infrastructure and

277
00:18:54,284 --> 00:18:57,914
scaling it up using this infrastructure. It is cloud

278
00:18:57,952 --> 00:19:01,290
native. So to summarize the features,

279
00:19:03,710 --> 00:19:07,210
there is chaos API crds. You can manage

280
00:19:07,280 --> 00:19:11,030
the entire chaos using declarative

281
00:19:11,190 --> 00:19:15,018
manifests, including the Chaos scheduler

282
00:19:15,194 --> 00:19:18,846
and the next is hub itself.

283
00:19:19,028 --> 00:19:23,330
And we expect you will have

284
00:19:23,480 --> 00:19:27,054
more than 50% of your chaos

285
00:19:27,102 --> 00:19:30,322
needs automatically available on the hub already.

286
00:19:30,456 --> 00:19:34,690
And all you need to do is learn how to

287
00:19:34,760 --> 00:19:38,982
use them and then learn how to write

288
00:19:39,036 --> 00:19:42,534
your new experiments that are required which are

289
00:19:42,572 --> 00:19:46,198
specific to your application. So for that you have

290
00:19:46,284 --> 00:19:50,410
chaos SDK available in Go Python and ansible.

291
00:19:50,830 --> 00:19:55,274
And using this SDK you'll be able to bring

292
00:19:55,312 --> 00:19:58,742
up the required skeleton of a chaos

293
00:19:58,806 --> 00:20:03,022
experiment very easily and

294
00:20:03,076 --> 00:20:07,200
then put your chaos logic and your experiment ready.

295
00:20:07,570 --> 00:20:11,630
And the other feature

296
00:20:11,970 --> 00:20:16,062
which is a very important one is has

297
00:20:16,126 --> 00:20:20,242
portal. Litmus portal is very

298
00:20:20,296 --> 00:20:24,594
important because has engineering does

299
00:20:24,632 --> 00:20:27,830
not stop at the introduction of faults.

300
00:20:29,290 --> 00:20:32,786
It's also about monitoring and helping

301
00:20:32,898 --> 00:20:36,518
sres developers take right actions to fix the

302
00:20:36,524 --> 00:20:40,106
weakness. And also you have to

303
00:20:40,128 --> 00:20:43,894
be able to do it at scale. Right. We'll talk about chaos

304
00:20:43,942 --> 00:20:47,578
workflows in a bit, but portal is about

305
00:20:47,664 --> 00:20:51,214
hub is about getting your experiments together

306
00:20:51,332 --> 00:20:55,226
in one place chaos portal is use those experiments,

307
00:20:55,338 --> 00:20:59,118
manage your chaos workflows, execute them,

308
00:20:59,284 --> 00:21:02,650
then monitor them, see what's happening.

309
00:21:02,820 --> 00:21:07,118
So it's about managing your has engineering

310
00:21:07,214 --> 00:21:10,798
end to end this portal. That's chaos portal.

311
00:21:10,814 --> 00:21:14,770
It's under development. Early versions are there

312
00:21:14,840 --> 00:21:18,166
if someone wants to take out, it's not released to the community

313
00:21:18,268 --> 00:21:21,846
formally. Maybe end of the year is what we

314
00:21:21,868 --> 00:21:23,640
are hoping that will happen.

315
00:21:26,490 --> 00:21:29,930
Litmus has many experiments. Right now we have about

316
00:21:30,080 --> 00:21:33,514
30 plus experiments expected to grow as

317
00:21:33,552 --> 00:21:37,574
the community grows. And you sre some stars

318
00:21:37,622 --> 00:21:42,174
here. That is. This really means it

319
00:21:42,372 --> 00:21:45,630
about inducing a fault into the infrastructure.

320
00:21:46,210 --> 00:21:49,514
Disk or node node, cpu node,

321
00:21:49,562 --> 00:21:53,294
memory, all that stuff. The other ones SrE Kubernetes

322
00:21:53,342 --> 00:21:57,874
resources itself. As you can see there

323
00:21:57,912 --> 00:22:00,850
are network duplication, network loss,

324
00:22:01,990 --> 00:22:03,730
Kubelet service skill.

325
00:22:05,930 --> 00:22:09,590
These are some of the important experiments. We have heard

326
00:22:09,660 --> 00:22:13,366
many stories where everything is fine.

327
00:22:13,468 --> 00:22:17,314
Kubernetes itself goes down and blast

328
00:22:17,362 --> 00:22:20,010
radius and very, very high.

329
00:22:20,160 --> 00:22:23,578
So don't wait for that to happen.

330
00:22:23,744 --> 00:22:27,562
Use Kubelet service skill experiment and see

331
00:22:27,616 --> 00:22:30,890
it what is your resilience?

332
00:22:31,050 --> 00:22:35,630
And that's a good one to have. In fact, this really

333
00:22:35,700 --> 00:22:39,290
came from community using litmus

334
00:22:39,370 --> 00:22:43,650
and the team is able to put this experiment

335
00:22:44,390 --> 00:22:48,738
onto. And those are

336
00:22:48,824 --> 00:22:53,214
about generic experiments we call. They're all grouped under kubernetes.

337
00:22:53,342 --> 00:22:57,094
There are also application specific experiments which

338
00:22:57,132 --> 00:23:00,806
are about inducing a fault at

339
00:23:00,828 --> 00:23:04,386
an application level. It could be about causing

340
00:23:04,498 --> 00:23:08,202
a database unavailability or

341
00:23:08,336 --> 00:23:12,330
about bringing down Kafka broker.

342
00:23:13,630 --> 00:23:17,222
It speaks the logic, the chaos logic speaks the language

343
00:23:17,286 --> 00:23:20,578
of an application rather than kubernetes.

344
00:23:20,774 --> 00:23:24,734
And we believe that it is very important to

345
00:23:24,772 --> 00:23:28,398
scale up the deeper faults that you

346
00:23:28,564 --> 00:23:32,400
want to induce. This application

347
00:23:32,710 --> 00:23:35,220
specific experiments will help.

348
00:23:36,870 --> 00:23:41,074
So where all you can use litmus in

349
00:23:41,112 --> 00:23:44,386
DevOps. Of course, in CI pipelines you

350
00:23:44,408 --> 00:23:48,262
can start with very small chaos experiments is

351
00:23:48,316 --> 00:23:52,390
pretty easy. And then chaos

352
00:23:53,130 --> 00:23:56,646
cannot be executed to full extent on pipelines because they are

353
00:23:56,668 --> 00:23:59,946
short lived. So deeper chaos can

354
00:23:59,968 --> 00:24:03,994
be executed on long running test beds, which is where the

355
00:24:04,032 --> 00:24:07,398
code gets moved to after the pipeline typically.

356
00:24:07,574 --> 00:24:11,414
And then there is staging, which is closer to production.

357
00:24:11,462 --> 00:24:14,782
But there are lots of people who are churning the code

358
00:24:14,836 --> 00:24:18,126
out there and you can increase the number of tests there and

359
00:24:18,148 --> 00:24:21,882
then just closer to production. SRe production

360
00:24:22,026 --> 00:24:25,522
can increase the number of tests, but production is

361
00:24:25,576 --> 00:24:29,346
where you want to start. Small, but try to cover

362
00:24:29,528 --> 00:24:33,246
more scenarios. They can be spread

363
00:24:33,278 --> 00:24:36,534
out, but you may want to cover

364
00:24:36,652 --> 00:24:40,246
all the scenarios that are possible in terms of

365
00:24:40,268 --> 00:24:43,750
failure injection so that you stay resilient to those

366
00:24:43,820 --> 00:24:47,442
faults. It all starts small. You need to buy in

367
00:24:47,516 --> 00:24:52,330
from the management has engineering. Many people are scared

368
00:24:52,910 --> 00:24:56,154
to be precise and sres want to do

369
00:24:56,192 --> 00:24:59,002
that. Developers do not want you to do that.

370
00:24:59,136 --> 00:25:03,658
It takes time in any organization to roll out chaos engineering

371
00:25:03,834 --> 00:25:07,326
in a large scale. So you would want

372
00:25:07,508 --> 00:25:10,654
developers themselves. See hey, this is how you can

373
00:25:10,692 --> 00:25:14,542
test yourself and get this automation integration

374
00:25:14,606 --> 00:25:18,082
teams to use chaos and then

375
00:25:18,136 --> 00:25:21,726
sres will be able to convince

376
00:25:21,758 --> 00:25:25,622
them easily. So as the time grows, people will

377
00:25:25,676 --> 00:25:29,234
find it acceptable to run chaos in production

378
00:25:29,282 --> 00:25:33,314
as well, the entire system. So as you increase

379
00:25:33,362 --> 00:25:38,394
the number of chaos tests that you can run in

380
00:25:38,432 --> 00:25:42,246
production, the overall resilience increases,

381
00:25:42,358 --> 00:25:45,450
but it takes a step by step approach typically.

382
00:25:46,430 --> 00:25:50,398
And good news is that it is cloud native. That means

383
00:25:50,484 --> 00:25:54,190
you can start doing automation of kiosks right

384
00:25:54,260 --> 00:25:58,206
into the development lifecycle. So how

385
00:25:58,228 --> 00:26:01,280
do you automate it? It's a very important thing.

386
00:26:01,890 --> 00:26:05,250
Kubernetes kind of one of the promises.

387
00:26:05,590 --> 00:26:09,102
So you would want to generate not scripts

388
00:26:09,166 --> 00:26:12,926
but Yaml manifests. That's the fundamental

389
00:26:12,958 --> 00:26:16,534
block for automation. You put all your

390
00:26:16,652 --> 00:26:20,354
chaos experiment, which is itself is a custom resource YAMl

391
00:26:20,402 --> 00:26:24,614
spec, and you attach that to an

392
00:26:24,652 --> 00:26:27,650
application where the chaos is going to run.

393
00:26:27,820 --> 00:26:31,146
And management of chaos on an

394
00:26:31,168 --> 00:26:35,340
application is also another CR, that's another YAML spec,

395
00:26:35,710 --> 00:26:40,038
that's called chaos engine Yaml and then attach

396
00:26:40,134 --> 00:26:44,926
schedule how often you want to run this. That's also an YAML spec.

397
00:26:45,108 --> 00:26:49,358
Everything, all these three, the experiment, the application

398
00:26:49,524 --> 00:26:52,962
which is taking this

399
00:26:53,016 --> 00:26:56,706
chaos experiment in, and also how often you

400
00:26:56,728 --> 00:27:00,226
want to do it. All this logic is put into a

401
00:27:00,248 --> 00:27:04,254
manifest and we call that as lithmus Yaml

402
00:27:04,302 --> 00:27:07,826
for example. And you want to automate

403
00:27:07,858 --> 00:27:12,280
this, this experiment, put it in git. Use the

404
00:27:12,890 --> 00:27:17,390
deployment tools, auto deployment tools like flux or Argo CD.

405
00:27:17,570 --> 00:27:20,826
And whenever a change to that happens,

406
00:27:21,008 --> 00:27:24,822
when a PR about the change is merged, your has

407
00:27:24,886 --> 00:27:29,020
starts running automatically and litmus gives many

408
00:27:30,210 --> 00:27:33,470
outputs, not just the chaos result,

409
00:27:33,620 --> 00:27:37,086
but it is also about giving has metrics. You can upload to

410
00:27:37,108 --> 00:27:40,606
your prometheus and then automate some of these alerts and

411
00:27:40,708 --> 00:27:44,746
corresponding actions notifications.

412
00:27:44,858 --> 00:27:48,386
And once you get the notification you want to go and see what

413
00:27:48,408 --> 00:27:52,050
exactly happened. You want to debug, you want to fix it. So you get

414
00:27:52,120 --> 00:27:56,054
has events for correlation and taking the right action to fix

415
00:27:56,092 --> 00:27:59,718
the weakness. So how do

416
00:27:59,724 --> 00:28:03,314
you scale this up? You are able to automate this with this fundamental

417
00:28:03,362 --> 00:28:07,910
concept. And how can you automate this automation?

418
00:28:08,510 --> 00:28:11,834
Sorry, how can you scale it? Scale it is being able

419
00:28:11,872 --> 00:28:15,114
to run multiple chaos experiments in

420
00:28:15,152 --> 00:28:18,954
either a sequential manner or in parallel, or a combination of

421
00:28:18,992 --> 00:28:22,266
both. Typical example is there are multiple

422
00:28:22,298 --> 00:28:26,510
namespaces applications are spread out across these namespaces

423
00:28:26,850 --> 00:28:30,862
and you are managing all of them. And end

424
00:28:30,916 --> 00:28:34,722
users are really being

425
00:28:34,776 --> 00:28:38,382
served by services that are spread across these namespaces,

426
00:28:38,446 --> 00:28:41,906
right? So faults can happen anywhere. So you want

427
00:28:41,928 --> 00:28:45,082
to simulate a flow chaos

428
00:28:45,166 --> 00:28:49,014
workflow where introduce two faults into two

429
00:28:49,052 --> 00:28:52,870
different namespaces in parallel, then wait for it, and then

430
00:28:52,940 --> 00:28:56,806
do two more faults in parallel and then kind of drain the

431
00:28:56,828 --> 00:29:00,666
node or multiple combinations of that, right? So this

432
00:29:00,688 --> 00:29:04,074
is one simple chaos workflow. And how do you do

433
00:29:04,112 --> 00:29:07,974
that? Is you develop has experiments

434
00:29:08,022 --> 00:29:11,534
into different Yaml manifests and you

435
00:29:11,572 --> 00:29:15,642
keep them ready. And then you apply a workflow

436
00:29:15,706 --> 00:29:19,290
using tools like Argo. The Argo workflow

437
00:29:19,370 --> 00:29:20,480
we've been using,

438
00:29:23,270 --> 00:29:27,106
it goes very well with the has workflows. So using

439
00:29:27,288 --> 00:29:30,850
Argo workflow, you have your experiments ready,

440
00:29:30,920 --> 00:29:34,180
you embed them into an workflow CR,

441
00:29:34,890 --> 00:29:38,086
and then Argo also has a scheduler. So you

442
00:29:38,108 --> 00:29:42,120
can attach that schedule to that and you

443
00:29:42,490 --> 00:29:46,434
develop a bigger Yaml manifest that manages

444
00:29:46,482 --> 00:29:49,654
these litmus experiments. The same thing will happen.

445
00:29:49,772 --> 00:29:53,558
You go and put them into a git

446
00:29:53,654 --> 00:29:57,500
and use Argo CD or flux to manage this

447
00:29:58,830 --> 00:30:02,974
auto deployment and change management. Then get

448
00:30:03,092 --> 00:30:06,480
multiple of them. You can scale this to

449
00:30:06,930 --> 00:30:10,830
hundreds of experiments in a system where there are

450
00:30:10,980 --> 00:30:14,266
hundreds or even thousands of kubernetes nodes.

451
00:30:14,378 --> 00:30:18,098
So you get an infrastructure to automate your has in

452
00:30:18,104 --> 00:30:21,474
a very natural way, and you can scale it and you can

453
00:30:21,512 --> 00:30:25,234
put it into your existing system of DevOps, which is

454
00:30:25,352 --> 00:30:29,174
Gitops. So with that, let's look

455
00:30:29,212 --> 00:30:32,402
at a very short demo of litmus.

456
00:30:32,546 --> 00:30:36,790
So I have the following setup where

457
00:30:36,860 --> 00:30:41,318
I have two nodes on Amazon eks

458
00:30:41,414 --> 00:30:45,546
cluster and I have deployed the

459
00:30:45,568 --> 00:30:49,114
microservices demo application. You all may

460
00:30:49,152 --> 00:30:53,818
be aware of this, sockshop and install litmus.

461
00:30:53,994 --> 00:30:59,354
A couple of experiments are being run as a workflow onto

462
00:30:59,402 --> 00:31:02,990
this, and litmus is set up in admin mode.

463
00:31:03,510 --> 00:31:06,930
Admin mode is where Litmus can go and inject

464
00:31:07,510 --> 00:31:11,442
chaos into any application because

465
00:31:11,496 --> 00:31:14,980
the service account has the permissions to do that.

466
00:31:15,290 --> 00:31:18,994
And I also have set up a monitoring

467
00:31:19,122 --> 00:31:22,930
infrastructure to receive the chaos metrics

468
00:31:23,090 --> 00:31:26,834
and we can see through Grafana

469
00:31:26,882 --> 00:31:31,100
what's happening. So with that, let's actually

470
00:31:32,430 --> 00:31:35,340
see how.

471
00:31:50,400 --> 00:31:52,540
So I have two nodes.

472
00:31:56,410 --> 00:31:59,960
There's only one that's running on

473
00:32:01,290 --> 00:32:04,360
this cluster. That is the talk shop,

474
00:32:07,210 --> 00:32:09,740
the what is.

475
00:32:15,700 --> 00:32:19,168
So they've been running for quite some time,

476
00:32:19,334 --> 00:32:21,090
about four or five days.

477
00:32:22,580 --> 00:32:25,892
You might see that some of them went down

478
00:32:25,946 --> 00:32:29,140
recently. That's because we've been continuously introducing chaos.

479
00:32:29,480 --> 00:32:33,044
That's a sign of things are

480
00:32:33,082 --> 00:32:35,750
being meddled with.

481
00:32:36,940 --> 00:32:41,320
Somebody is watching the resilience of it continuously.

482
00:32:41,980 --> 00:32:43,770
So let me show,

483
00:32:51,410 --> 00:32:55,362
so we put litmus in admin mode. That means everything

484
00:32:55,416 --> 00:32:59,266
runs within litmus namespace. The other mode is you

485
00:32:59,288 --> 00:33:02,562
can create service accounts in such a way that your

486
00:33:02,616 --> 00:33:06,166
has actually runs within the namespace of

487
00:33:06,188 --> 00:33:10,150
your application. So here you have a chaos operator.

488
00:33:11,130 --> 00:33:15,046
You also have a litmus portal running the

489
00:33:15,068 --> 00:33:18,410
early version of it. You also have

490
00:33:18,480 --> 00:33:22,346
a chaos monitor which is exporting the

491
00:33:22,368 --> 00:33:25,782
metrics into a Prometheus server.

492
00:33:25,926 --> 00:33:30,570
Then there is chaos events being exported

493
00:33:31,570 --> 00:33:34,894
through an event router. And as you can see that there

494
00:33:34,932 --> 00:33:38,320
are some tests being run and they are being run in

495
00:33:40,050 --> 00:33:44,142
it must namespace. There is a cpu memory hog

496
00:33:44,206 --> 00:33:47,458
experiments. So let me also

497
00:33:47,544 --> 00:33:51,998
show monitoring

498
00:33:52,174 --> 00:33:52,900
this,

499
00:33:59,920 --> 00:34:02,610
we have Prometheus and Grafana running.

500
00:34:04,260 --> 00:34:08,050
So let's look at actually how

501
00:34:10,420 --> 00:34:20,700
this is your application that is running workflows.

502
00:34:23,360 --> 00:34:27,560
So we have two chaos workflows

503
00:34:27,720 --> 00:34:30,924
configured within argo workflow

504
00:34:31,052 --> 00:34:34,636
that are running every fifth minute and every 10th

505
00:34:34,668 --> 00:34:39,116
minute. So every five minutes you have cpu chaos pod.

506
00:34:39,148 --> 00:34:42,404
Cpu has pod memory chaos running on two

507
00:34:42,442 --> 00:34:46,116
different applications or containers. So let's look

508
00:34:46,138 --> 00:34:49,350
at one.

509
00:34:50,040 --> 00:34:54,272
So here you have argo workflow

510
00:34:54,336 --> 00:34:57,892
of chaos. We named it and it embeds

511
00:34:58,036 --> 00:35:01,992
the Chaos experiment. This is what we talked about. You have your entire

512
00:35:02,126 --> 00:35:06,288
litmus chaos experiment. You embed that within a workflow.

513
00:35:06,484 --> 00:35:10,920
And the chaos engine calls the has experiments.

514
00:35:11,080 --> 00:35:14,876
For example, this is a chaos experiment CR

515
00:35:14,978 --> 00:35:19,372
on the system, and you can tune through

516
00:35:19,426 --> 00:35:22,960
githubs the

517
00:35:23,110 --> 00:35:28,224
behavior of this chaos experiments. For example, you want to increase the

518
00:35:28,262 --> 00:35:31,724
experiment chaos for a longer

519
00:35:31,782 --> 00:35:35,152
duration, shorter operations, and increase the number of cores,

520
00:35:35,216 --> 00:35:40,260
so everything is possible through Gitops.

521
00:35:40,680 --> 00:35:43,344
And let's look at the other workflow.

522
00:35:43,392 --> 00:35:47,016
Similarly, memory, so it's being run on a

523
00:35:47,038 --> 00:35:49,800
different pod called orders pod,

524
00:35:50,300 --> 00:35:53,732
and the previous one was on catalog

525
00:35:53,796 --> 00:35:57,804
pod. And you're running on the same

526
00:35:57,922 --> 00:36:05,004
namespace. So you're running it in the

527
00:36:05,042 --> 00:36:09,772
application is a sock shop, and you're running memory

528
00:36:09,836 --> 00:36:14,976
chaos on orders application. And this

529
00:36:14,998 --> 00:36:18,336
is what it is. You can specify it declaratively like that.

530
00:36:18,438 --> 00:36:21,708
And it is inside chaos engine Cr.

531
00:36:21,894 --> 00:36:25,716
And the experiment is memory hog. And you

532
00:36:25,738 --> 00:36:29,860
can tune the behavior of your has,

533
00:36:30,600 --> 00:36:34,020
and it's scheduled every fifth minute.

534
00:36:35,240 --> 00:36:36,870
So as you can see,

535
00:36:39,580 --> 00:36:42,820
the workflow itself was executed within 5 seconds.

536
00:36:42,980 --> 00:36:47,164
And you can go and sre

537
00:36:47,202 --> 00:36:49,710
the results of that.

538
00:36:51,360 --> 00:36:55,756
This is argo way of seeing

539
00:36:55,858 --> 00:37:00,508
what is going to happen. The litmus portal that is under development

540
00:37:00,604 --> 00:37:04,352
will have more detailed views for chaos itself.

541
00:37:04,486 --> 00:37:08,716
It uses argo workflow concepts. And let's

542
00:37:08,748 --> 00:37:10,210
look at what's happening.

543
00:37:12,600 --> 00:37:16,788
This is a Grafana dashboard that we put together for

544
00:37:16,874 --> 00:37:21,220
sock shop where this red lines indicate the has induction.

545
00:37:21,640 --> 00:37:25,140
And as you can see, that every fifth minute,

546
00:37:25,480 --> 00:37:30,004
the screen and yellow lines green is catalog CPU

547
00:37:30,052 --> 00:37:33,844
Hog. This one is about memory

548
00:37:33,892 --> 00:37:38,110
hog, and it's being induced into different

549
00:37:39,440 --> 00:37:43,352
containers. So whenever a cpu hog is induced

550
00:37:43,496 --> 00:37:47,192
into catalog,

551
00:37:47,336 --> 00:37:50,896
the performance is going down. You can see that orders are

552
00:37:50,918 --> 00:37:54,530
also going down. Similarly, whenever there is

553
00:37:56,500 --> 00:38:00,672
memory Aug is introduced into this one,

554
00:38:00,806 --> 00:38:04,236
your catalog also is the performance

555
00:38:04,268 --> 00:38:08,420
of catalog pod is going down. So doing

556
00:38:08,490 --> 00:38:11,744
it again and again, of course, you're not going to do, in reality

557
00:38:11,792 --> 00:38:15,270
every five minutes, but let's say every day

558
00:38:15,640 --> 00:38:19,528
some fault is happening. A larger fault can happen randomly in a

559
00:38:19,534 --> 00:38:23,336
week. So all that possible combinations can be

560
00:38:23,438 --> 00:38:25,850
implemented in an automated way.

561
00:38:26,640 --> 00:38:30,780
So that's how your litmus

562
00:38:31,520 --> 00:38:35,500
chaos can help in automating

563
00:38:36,000 --> 00:38:41,260
has engineering, thereby helping you to achieve

564
00:38:41,420 --> 00:38:44,960
higher resilience, right? So a bit of small

565
00:38:45,030 --> 00:38:48,256
introduction to Litmus portal. It's very

566
00:38:48,278 --> 00:38:51,768
much under development. The idea here is you'll

567
00:38:51,804 --> 00:38:55,316
be able to schedule the workflows itself and you

568
00:38:55,338 --> 00:38:59,376
got your own hub. So it's

569
00:38:59,408 --> 00:39:02,788
all about bringing experiments from your hub and

570
00:39:02,874 --> 00:39:07,336
adding more experiments, right? So you have a public

571
00:39:07,438 --> 00:39:12,810
has hub, but you want to share that with your team and

572
00:39:13,420 --> 00:39:17,612
the new experiments. So Litmus portal will help you bring in

573
00:39:17,746 --> 00:39:22,140
your more team members together and develop

574
00:39:22,210 --> 00:39:25,704
the new experiments, create more has workflows,

575
00:39:25,752 --> 00:39:30,940
monitor them. So it provides an end to end infrastructure

576
00:39:31,100 --> 00:39:35,164
or a tool set to practice chaos

577
00:39:35,212 --> 00:39:38,300
engineering and resilience engineering.

578
00:39:38,460 --> 00:39:41,628
That's about the demo.

579
00:39:41,814 --> 00:39:45,536
And in summary, do practice chaos

580
00:39:45,568 --> 00:39:49,200
engineering and do it in cloud native way. And litmus

581
00:39:49,280 --> 00:39:52,932
chaos can help you do that. And overall has. The time

582
00:39:52,986 --> 00:39:56,664
goes by. As you increase more and more chaos tests in

583
00:39:56,702 --> 00:40:00,388
your production, you will end up having higher and higher resilience.

584
00:40:00,564 --> 00:40:04,840
This is definitely a preferred way to increase your resilience on Kubernetes.

585
00:40:05,500 --> 00:40:08,030
So with that, thank you very much.

586
00:40:08,880 --> 00:40:12,910
Do try out litmus. You have very easy

587
00:40:13,680 --> 00:40:17,870
use get started guides available. There is a Litmus demo application

588
00:40:18,720 --> 00:40:22,056
workshop. Whatever the demo that I showed,

589
00:40:22,168 --> 00:40:25,996
you can set it up within a few minutes with that. Thank you

590
00:40:26,018 --> 00:40:29,692
very much, folks. Have a great time

591
00:40:29,746 --> 00:40:32,756
with Litmus has. And do try it out.

592
00:40:32,858 --> 00:40:36,948
Join Litmus has slack on

593
00:40:37,114 --> 00:40:41,376
Slack channel and Kubernetes slack their litmus

594
00:40:41,408 --> 00:40:44,804
channel. So with

595
00:40:44,842 --> 00:40:48,320
that, thank you very much. Thanks for your audience.

