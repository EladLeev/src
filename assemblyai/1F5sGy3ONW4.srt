1
00:00:00,890 --> 00:00:04,714
Hi everybody, thank you so much for coming today. I'm super, super excited

2
00:00:04,762 --> 00:00:07,674
to be here. Today we are going to talk about one of my favorite topics

3
00:00:07,722 --> 00:00:11,486
which is how to build centralized policy management at

4
00:00:11,588 --> 00:00:15,054
scale. But I believe that at least for

5
00:00:15,092 --> 00:00:18,960
some of you this is the first time that we meet. So hello,

6
00:00:19,410 --> 00:00:23,114
my name is Noaa Barki. I've been a developer advocate

7
00:00:23,162 --> 00:00:27,190
at Tree and a full stack developer for about seven years.

8
00:00:27,260 --> 00:00:30,774
Six, seven years. Yeah. I'm also a tech writer and

9
00:00:30,812 --> 00:00:34,162
one of the leaders of GitHub Israel community which is the largest

10
00:00:34,226 --> 00:00:37,882
GitHub community in the whole universe. And I work

11
00:00:37,936 --> 00:00:41,558
at an amazing company called the Tree where we help developers and DevOps

12
00:00:41,574 --> 00:00:45,702
engineers to prevent misconfigurations, Kubernetes misconfigurations

13
00:00:45,766 --> 00:00:49,402
from reaching production. But enough about me,

14
00:00:49,456 --> 00:00:53,198
because today we are going to talk about how to build centralized policy

15
00:00:53,284 --> 00:00:56,942
management at scale. We are going to specifically talk about

16
00:00:57,076 --> 00:01:00,910
policies. What are they? Why do we even care about policy

17
00:01:00,980 --> 00:01:04,974
enforcement? We are going to see some very cool tools that I personally

18
00:01:05,102 --> 00:01:08,994
like like OPa, gatekeeper, conftest, argosd and

19
00:01:09,032 --> 00:01:12,580
my very own those tree open source project.

20
00:01:13,990 --> 00:01:17,720
So without further ado, let's just get it started.

21
00:01:20,650 --> 00:01:24,280
Ladies and gentlemen, close your eyes.

22
00:01:25,130 --> 00:01:29,020
Picture this. You had a long week

23
00:01:29,710 --> 00:01:34,422
and it's Friday now and you're in your bed dreaming peacefully

24
00:01:34,566 --> 00:01:38,970
about a warm wonderful comfortable weekend.

25
00:01:39,790 --> 00:01:43,246
Unfortunately your weekend came a

26
00:01:43,268 --> 00:01:47,006
little sooner than expected when you woke up

27
00:01:47,028 --> 00:01:50,586
from the sound of your phone and you had 15 missing calls

28
00:01:50,618 --> 00:01:54,466
from work. Oh no. You wake up, you wash your

29
00:01:54,488 --> 00:01:59,422
eyes, you go to the Slack channel and oh apparently

30
00:01:59,486 --> 00:02:03,054
somebody forgot to add in one of the deployments memory

31
00:02:03,102 --> 00:02:06,514
limit which caused one of the containers a memory leak

32
00:02:06,562 --> 00:02:10,310
which cases all Kubernetes node to ran out of memory.

33
00:02:13,290 --> 00:02:14,440
Wait a second.

34
00:02:17,710 --> 00:02:21,260
Oh did I?

35
00:02:23,470 --> 00:02:24,620
Oh no,

36
00:02:26,910 --> 00:02:30,640
I think, I think, I think that somebody,

37
00:02:31,970 --> 00:02:34,640
that somebody is you.

38
00:02:37,010 --> 00:02:40,574
Now of course I'm kidding, of course I'm kidding. This is not

39
00:02:40,612 --> 00:02:44,574
you. I'm sure it will never happen to you you. Because first of all you're

40
00:02:44,622 --> 00:02:48,414
very responsible developers. I always forget

41
00:02:48,462 --> 00:02:52,354
that I add this slide anyway. I'm sure it will never happen

42
00:02:52,392 --> 00:02:55,474
to you. You're a very responsible developers, you're here with me

43
00:02:55,512 --> 00:02:59,330
that's for sure. And I know that you know how to use kubernetes,

44
00:02:59,410 --> 00:03:02,950
you definitely know to never forget those memory limit.

45
00:03:03,450 --> 00:03:07,462
So let me ask you a different question. Who's here is

46
00:03:07,516 --> 00:03:10,938
ready to play a game? So the game goes like this.

47
00:03:11,024 --> 00:03:14,362
I'm going to show you two kubernetes manifest each

48
00:03:14,416 --> 00:03:18,102
time I'm going to point to a specific key which is configured

49
00:03:18,166 --> 00:03:22,046
differently on every manifest. You will have

50
00:03:22,068 --> 00:03:25,870
to look very carefully and tell me which one you will deploy,

51
00:03:26,370 --> 00:03:28,880
left or right.

52
00:03:29,890 --> 00:03:33,474
Let's get it started. Okay, so this is a

53
00:03:33,512 --> 00:03:37,102
cron job configuration. Pay attention to the concurrency

54
00:03:37,166 --> 00:03:41,554
policy. Which one you will deploy, left or

55
00:03:41,592 --> 00:03:43,730
right? I'll give you 10 seconds.

56
00:03:45,050 --> 00:03:48,600
Ten, nine. Okay, I'll stop.

57
00:03:51,370 --> 00:03:55,334
And the right answer will be right. You see,

58
00:03:55,372 --> 00:03:58,566
we always want to make sure that we set the concurrency policy to

59
00:03:58,588 --> 00:04:02,522
either forbid or to replace, never to allow. The reason

60
00:04:02,576 --> 00:04:06,534
why is because whenever a cron job will get failed and we set the concurrency

61
00:04:06,582 --> 00:04:10,474
policy to allow, the failed cron job will

62
00:04:10,512 --> 00:04:14,666
never replace the previous one. So you'll end up with a lot of pods

63
00:04:14,698 --> 00:04:18,014
that just will spawn your cluster. And this is actually what

64
00:04:18,052 --> 00:04:21,502
happened to target. They has one failing quant job

65
00:04:21,556 --> 00:04:25,966
that created thousands of pods that were constantly restarting.

66
00:04:26,078 --> 00:04:29,554
And not only that it took their cluster down,

67
00:04:29,672 --> 00:04:33,182
but it also cost them a lot of money because their cluster

68
00:04:33,246 --> 00:04:36,678
accumulated a few hundreds of cpus during

69
00:04:36,764 --> 00:04:41,126
that time. Very sad story. Let's move forward to the next question.

70
00:04:41,308 --> 00:04:44,918
This is another quant job configuration. And once again

71
00:04:45,004 --> 00:04:48,474
pay attention to the concurrency policy. Which one you

72
00:04:48,512 --> 00:04:51,900
will deploy, left or right?

73
00:04:55,390 --> 00:04:58,842
Tan dun dun dun dun dun dun. And those right

74
00:04:58,896 --> 00:05:02,478
answer will be right

75
00:05:02,564 --> 00:05:06,174
again. You see here on the left side,

76
00:05:06,212 --> 00:05:09,678
the concurrency policy isn't part of the cron job

77
00:05:09,764 --> 00:05:13,210
spec. So we end up with a cron job without any limits.

78
00:05:13,290 --> 00:05:17,140
And this is actually what happened to Zalando which is an online

79
00:05:17,510 --> 00:05:21,726
leading fashion company with over 6000 employees.

80
00:05:21,758 --> 00:05:25,574
It's a big company guys. They actually used the

81
00:05:25,612 --> 00:05:29,714
correct configuration, however they placed it incorrectly

82
00:05:29,762 --> 00:05:33,794
in their yaml, which is very sad. And this immediately

83
00:05:33,842 --> 00:05:36,520
took their API server down.

84
00:05:37,870 --> 00:05:41,180
So let's move forward to the next question.

85
00:05:44,110 --> 00:05:46,490
This is pretty simple. Pod,

86
00:05:48,110 --> 00:05:50,880
which one you will deploy? Left.

87
00:05:52,130 --> 00:05:56,640
All right, I'm sure that you know this one and

88
00:05:57,490 --> 00:06:01,454
this is going to be short. The right answer is of course

89
00:06:01,572 --> 00:06:04,594
right again, we want to make sure that

90
00:06:04,632 --> 00:06:08,210
we never forget the memory limit.

91
00:06:08,630 --> 00:06:12,306
And this is actually what happened to Blue Metador. Back then they were a

92
00:06:12,328 --> 00:06:15,986
small startup company with those monitoring software. They pods

93
00:06:16,018 --> 00:06:19,762
hosted a sumo logic third party application whose container

94
00:06:19,826 --> 00:06:23,750
were memory hogs. And because

95
00:06:23,820 --> 00:06:25,910
they forgot to put the memory limit,

96
00:06:27,130 --> 00:06:31,210
nothing. Basically stopped from those pods to take up all the memory in the node

97
00:06:31,550 --> 00:06:34,940
and eventually caused out of memory issues.

98
00:06:35,550 --> 00:06:36,650
Very sad.

99
00:06:39,150 --> 00:06:42,214
But you see target, Zolando,

100
00:06:42,342 --> 00:06:45,546
Blumetor, they aren't the only one who suffered

101
00:06:45,578 --> 00:06:48,814
from the pretty innocent mistakes. I'm talking

102
00:06:48,852 --> 00:06:51,502
about big companies I'm talking about Google,

103
00:06:51,636 --> 00:06:53,950
Spotify, Airbnb, Datadog,

104
00:06:54,030 --> 00:06:57,682
Toyota, Tesla, who's not here. I'm talking

105
00:06:57,736 --> 00:07:01,694
about a lot of other companies who share their own Kubernetes failure

106
00:07:01,742 --> 00:07:04,660
story. Trust me,

107
00:07:05,130 --> 00:07:08,486
nobody is immune to Kubernetes misconfiguration. So first of

108
00:07:08,508 --> 00:07:12,342
all, I highly recommend everybody to read about other

109
00:07:12,396 --> 00:07:16,774
companies failure stories. Not only that you would learn so much

110
00:07:16,892 --> 00:07:20,586
about how Kubernetes works, what to do, what not to do and what are

111
00:07:20,608 --> 00:07:24,122
the best practices, but it will also will make

112
00:07:24,176 --> 00:07:28,074
you ask the ultimate question, which is

113
00:07:28,272 --> 00:07:31,158
how can I make sure it won't happen to me? How can I make sure

114
00:07:31,184 --> 00:07:34,074
that I won't become one of those failure stories?

115
00:07:34,202 --> 00:07:37,534
And this question is very important because it forces you

116
00:07:37,572 --> 00:07:41,454
to think about what are the workloads requirement in

117
00:07:41,492 --> 00:07:44,754
your organization? What is the stability in the security that

118
00:07:44,792 --> 00:07:48,226
you want to achieve for your cluster? How can you make

119
00:07:48,248 --> 00:07:51,810
sure that it won't happen to you? And the answer

120
00:07:51,880 --> 00:07:55,826
will be policy enforcement. I know this because

121
00:07:55,928 --> 00:07:59,462
before we launched the tree for the first time, we wanted to learn

122
00:07:59,516 --> 00:08:02,690
as much as possible about the common misconfigurations

123
00:08:02,770 --> 00:08:06,230
and the most common pitfalls in the Kubernetes ecosystem.

124
00:08:06,570 --> 00:08:10,406
And what we did was to read more than 100 Kubernetes

125
00:08:10,518 --> 00:08:13,978
failure stories. And not only that, we learned that policy

126
00:08:14,064 --> 00:08:17,846
enforcement is the solution to prevent misconfiguration from reaching

127
00:08:17,878 --> 00:08:22,014
production, but it also turned out to be a

128
00:08:22,052 --> 00:08:25,886
key solution to improve the DevOps culture in your

129
00:08:25,908 --> 00:08:26,670
organization.

130
00:08:30,210 --> 00:08:33,934
So great. So how do we start? So first of all,

131
00:08:33,972 --> 00:08:37,986
we want to define the policies and the rules that we want to enforce in

132
00:08:38,008 --> 00:08:42,382
our organization. Maybe we want to make sure that every container

133
00:08:42,446 --> 00:08:46,382
has a memory limit. Maybe we want to make sure that all the containers

134
00:08:46,446 --> 00:08:49,778
has configured readiness or liveness probe.

135
00:08:49,874 --> 00:08:53,522
Maybe it's about quant job, that every quant job has a deadline

136
00:08:53,586 --> 00:08:55,750
or that those scheduler is valid.

137
00:08:57,290 --> 00:09:00,506
It's not really matter. But those policies that you

138
00:09:00,528 --> 00:09:04,570
will define are really dependent on your workloads

139
00:09:04,910 --> 00:09:08,666
requirement. And once you have a set of policies that

140
00:09:08,688 --> 00:09:12,094
you want to enforce, the real question is how will you

141
00:09:12,132 --> 00:09:15,722
integrate, how will you distribute those policies

142
00:09:15,866 --> 00:09:19,006
in your organization, in your pipeline? How will you make

143
00:09:19,028 --> 00:09:22,990
sure that you and your teammate will follow these policies?

144
00:09:25,830 --> 00:09:29,140
So the way I see it, you know what,

145
00:09:30,150 --> 00:09:34,100
let me tell it in a different way. I believe in two things.

146
00:09:34,710 --> 00:09:38,478
I believe in shift left and I believe in Githubs.

147
00:09:38,654 --> 00:09:42,214
I believe that as soon as you find a mistake, the less it might take

148
00:09:42,252 --> 00:09:45,682
your production down. And I believe that every Kubernetes resource,

149
00:09:45,746 --> 00:09:49,306
every config file should be handled exactly the same

150
00:09:49,488 --> 00:09:52,358
as your source code in the CI,

151
00:09:52,454 --> 00:09:56,470
exactly like your source code. So with this mindset,

152
00:09:56,550 --> 00:10:01,310
the way I see it, we should automatically validate our resources

153
00:10:01,810 --> 00:10:05,450
on every code change in the CI.

154
00:10:05,610 --> 00:10:09,662
Furthermore, integrating and validating your

155
00:10:09,716 --> 00:10:13,246
resources in the CI using tools that can be used as a

156
00:10:13,268 --> 00:10:15,070
local testing library.

157
00:10:17,490 --> 00:10:20,830
I'll pause. As a local testing library

158
00:10:21,890 --> 00:10:25,506
can extremely help you nurture the DevOps culture in your organization station.

159
00:10:25,618 --> 00:10:29,446
And the reason why is because local testing library is actually one

160
00:10:29,468 --> 00:10:31,750
of the developers policies.

161
00:10:32,730 --> 00:10:36,566
Developers, they know how to use local testing library. They are used to

162
00:10:36,588 --> 00:10:39,778
it. They are used to write code, test it

163
00:10:39,884 --> 00:10:43,594
locally on their local machine and then submit a pull request. And they

164
00:10:43,632 --> 00:10:47,146
expect those tests, at least those tests to be

165
00:10:47,168 --> 00:10:50,494
ran again in the CI. This is actually part

166
00:10:50,532 --> 00:10:53,726
of the developers policies and allowing the

167
00:10:53,748 --> 00:10:57,022
developers to do the same with infrastructure has code

168
00:10:57,076 --> 00:11:00,762
with Kubernetes resources will allow the DevOps to delegate

169
00:11:00,826 --> 00:11:04,014
more responsibilities to the developers

170
00:11:04,142 --> 00:11:08,498
and therefore to liberate the DevOps from the constant need

171
00:11:08,664 --> 00:11:12,210
to fence every Kubernetes resource from

172
00:11:12,280 --> 00:11:14,370
every possible misconfiguration.

173
00:11:18,820 --> 00:11:23,040
But I hear you. I hear you back there.

174
00:11:23,190 --> 00:11:26,944
I can totally hear you. Here in Israel we have

175
00:11:26,982 --> 00:11:30,348
a thing, it's called let's talk Dugri,

176
00:11:30,444 --> 00:11:33,200
which means show me the real business. Come on Noah.

177
00:11:33,960 --> 00:11:37,520
So let me show you the real business. Let's talk dogri.

178
00:11:37,680 --> 00:11:41,684
Let's talk about how can you start today? Use your

179
00:11:41,722 --> 00:11:44,996
policy. Use policy enforcement in your organization.

180
00:11:45,188 --> 00:11:49,220
And the first tool that I want to talk about is OPA or OPA,

181
00:11:49,300 --> 00:11:53,444
I'm going to call it OPA. So OPA is general purpose

182
00:11:53,492 --> 00:11:57,604
policy engine. You can write all your policies in it and execute

183
00:11:57,652 --> 00:12:00,716
it with a specific input. Check if it violates any one of

184
00:12:00,738 --> 00:12:04,508
those policies. You can practically think about OPA as a super

185
00:12:04,594 --> 00:12:07,870
policy engine. I like to think about it this way.

186
00:12:09,140 --> 00:12:12,864
Now those main idea behind OPA is actually to

187
00:12:12,902 --> 00:12:16,576
decouple all the policy decision making logic from

188
00:12:16,598 --> 00:12:19,804
the policy enforcement usage. You see,

189
00:12:19,862 --> 00:12:23,588
suppose you have microservices architecture and one of the

190
00:12:23,594 --> 00:12:27,300
microservices receives an API request. You probably

191
00:12:27,370 --> 00:12:31,424
need to make some decisions in order to allow or disallow

192
00:12:31,472 --> 00:12:35,364
this request. Right now these decisions,

193
00:12:35,412 --> 00:12:39,428
they are based on rules. They are based on criteria that you want these requests

194
00:12:39,524 --> 00:12:43,828
to meet. These rules, they are called policies.

195
00:12:44,004 --> 00:12:47,630
And what OPA gives you is the ability to

196
00:12:48,400 --> 00:12:51,660
decouple and to offload all those policies into

197
00:12:51,730 --> 00:12:55,630
one dedicated agnostic service

198
00:12:56,000 --> 00:12:59,744
and therefore to provide more control to your

199
00:12:59,782 --> 00:13:02,908
administrators and your ops team in your organization.

200
00:13:03,004 --> 00:13:07,056
To control in a better way over the policy enforcement and the

201
00:13:07,078 --> 00:13:08,880
service at runtime.

202
00:13:13,640 --> 00:13:16,528
So let's talk about how can you use OPA.

203
00:13:16,624 --> 00:13:19,952
So there are actually two ways to use OPA.

204
00:13:20,016 --> 00:13:23,812
If your services are written in go, you can use OPA

205
00:13:23,876 --> 00:13:28,664
as an internal package and embedded it within

206
00:13:28,702 --> 00:13:31,864
your code. The second option is actually to

207
00:13:31,902 --> 00:13:35,324
use OPA as the host level demon and query it with an

208
00:13:35,362 --> 00:13:38,696
HTTP request. You will send the input as JSON

209
00:13:38,888 --> 00:13:42,316
and OPA will evaluate and will send you

210
00:13:42,338 --> 00:13:45,230
the response. Now this is important.

211
00:13:46,880 --> 00:13:50,032
By definition all the policies should be written in a special

212
00:13:50,086 --> 00:13:53,728
language called Rego, which is those official policy

213
00:13:53,814 --> 00:13:57,360
language by OPA. It's very easy to learn in

214
00:13:57,430 --> 00:14:01,380
fun fact actually it's inspired by datalog and

215
00:14:01,450 --> 00:14:05,536
basically that's it. From this moment on, OPA is your centralized

216
00:14:05,648 --> 00:14:09,360
policy service. You will write all the policies in OPa

217
00:14:09,440 --> 00:14:12,744
and you can query all those microservices to ask

218
00:14:12,782 --> 00:14:16,232
OPA what is the policy enforcement. OPA will

219
00:14:16,286 --> 00:14:19,544
evaluate it with the input and will

220
00:14:19,582 --> 00:14:22,490
return a response, valid or not.

221
00:14:24,140 --> 00:14:26,700
Now OPA is great.

222
00:14:26,850 --> 00:14:31,996
OPA is wonderful. I really love OPA, but when

223
00:14:32,018 --> 00:14:35,500
it comes to Kubernetes, not so much

224
00:14:35,650 --> 00:14:38,752
because it still requires a lot of heavy lifting work.

225
00:14:38,886 --> 00:14:42,812
And I do enjoy crossfit training and I do enjoy heavy lifting,

226
00:14:42,876 --> 00:14:46,592
but not when it comes to my Kubernetes cluster. This is where

227
00:14:46,726 --> 00:14:51,248
conftest is coming to picture. So conftest

228
00:14:51,344 --> 00:14:55,204
is an open source utility that allows us to write

229
00:14:55,322 --> 00:14:59,296
tests against any structured file.

230
00:14:59,408 --> 00:15:02,792
And this is number one world about me. When I say any,

231
00:15:02,846 --> 00:15:06,644
I mean any. I'm talking about JSON XML Docker

232
00:15:06,692 --> 00:15:10,792
files and of course YaMl. I'm talking

233
00:15:10,846 --> 00:15:14,516
about our Kubernetes resources. Conftest allows

234
00:15:14,548 --> 00:15:18,872
us to write tests for our Kubernetes resources

235
00:15:18,936 --> 00:15:22,620
with OPA as a policy engine under the hood.

236
00:15:23,120 --> 00:15:26,760
Conftest is specifically designed to be ran in the CI

237
00:15:26,920 --> 00:15:30,720
or just the way I like it as a local testing library.

238
00:15:32,260 --> 00:15:35,696
Now let's talk about how can you use conftest. So first of

239
00:15:35,718 --> 00:15:39,200
all you need to install conftest on your local machine.

240
00:15:40,420 --> 00:15:43,812
Then you need to write all your policies in

241
00:15:43,866 --> 00:15:47,360
Rego. So here for example, you can see two policies.

242
00:15:47,520 --> 00:15:51,332
One that makes sure that verifies that I don't run with

243
00:15:51,386 --> 00:15:54,984
root privileges, and the second that I always use the app

244
00:15:55,022 --> 00:15:56,760
label for my deployments.

245
00:15:58,140 --> 00:16:02,020
And after I wrote all the policies

246
00:16:02,100 --> 00:16:05,636
I put them in a folder. By default the folder

247
00:16:05,668 --> 00:16:09,356
name should be policy and I simply execute conftest with

248
00:16:09,458 --> 00:16:13,036
conftest test with the path of all the files that I

249
00:16:13,058 --> 00:16:17,116
want. Conftest to test conftest will take all the policies, will take

250
00:16:17,138 --> 00:16:20,704
all the files that existed in that path, will send everything back

251
00:16:20,742 --> 00:16:23,650
to OPA and will output those result.

252
00:16:24,100 --> 00:16:27,532
Now I said that you can use conftest in the CI.

253
00:16:27,596 --> 00:16:31,664
So here for example, I used GitHub action and what

254
00:16:31,702 --> 00:16:35,364
I want you to pay attention to is actually the green part over here.

255
00:16:35,482 --> 00:16:38,916
So as you can see I pull conftest and this

256
00:16:38,938 --> 00:16:42,448
is actually an awesome feature by conftest. Conftest allows

257
00:16:42,464 --> 00:16:45,892
us to push and pull our policies to a docker registry,

258
00:16:45,956 --> 00:16:49,496
which is kind of nice. So I pull all my policies and

259
00:16:49,518 --> 00:16:52,952
I simply run a conftest test with those path of all my

260
00:16:53,006 --> 00:16:57,384
resources. Very easy, very fun. I love conftest,

261
00:16:57,432 --> 00:17:00,780
it's super friendly, makes life so much easier.

262
00:17:01,120 --> 00:17:04,190
Highly recommend everybody at least try it.

263
00:17:05,840 --> 00:17:09,964
So with configure in those CI we can safely go back to sleep

264
00:17:10,012 --> 00:17:13,872
now because we validate our resources on every code change.

265
00:17:14,006 --> 00:17:18,080
Let me just put myself here. Oh, this is more comfortable

266
00:17:19,060 --> 00:17:23,376
here we can safely

267
00:17:23,408 --> 00:17:27,044
go back to sleep because we validate our resources on every code

268
00:17:27,082 --> 00:17:30,960
change. And this means that our cluster is truly

269
00:17:31,040 --> 00:17:34,264
project, right? Wrong. This is

270
00:17:34,302 --> 00:17:38,372
not true because what about those criminal users

271
00:17:38,436 --> 00:17:42,920
who can, anytime they want during the day, can simply type

272
00:17:43,070 --> 00:17:46,280
cubectl, apply and do whatever they want with our clusters.

273
00:17:46,360 --> 00:17:49,916
What about those criminal users? And in the

274
00:17:49,938 --> 00:17:52,636
year of 2020, Google,

275
00:17:52,738 --> 00:17:56,830
Styra, Microsoft, Red Hat, they asked themselves

276
00:17:57,140 --> 00:18:00,656
those exact question and their answer was this is

277
00:18:00,678 --> 00:18:03,792
not enough. So this is the story of how they

278
00:18:03,846 --> 00:18:08,092
created gatekeeper and gatekeeper.

279
00:18:08,156 --> 00:18:11,472
Gatekeeper is actually the bridge

280
00:18:11,536 --> 00:18:15,120
between your Kubernetes API server and OPA.

281
00:18:15,200 --> 00:18:18,512
And it allows us to validate our resources,

282
00:18:18,576 --> 00:18:22,464
our Kubernetes resources natively in our

283
00:18:22,602 --> 00:18:26,024
cluster. But before we talk about what

284
00:18:26,062 --> 00:18:29,140
is exactly Gatekeeper and what it does under the hood,

285
00:18:29,220 --> 00:18:33,348
I want us to talk about Kubernetes admission controls and specifically

286
00:18:33,444 --> 00:18:37,404
Kubernetes admission webhooks. You see,

287
00:18:37,522 --> 00:18:41,064
when an API comes into Kubernetes API

288
00:18:41,112 --> 00:18:44,876
server, it passes through a series of steps. First of

289
00:18:44,898 --> 00:18:47,580
all, it's being authenticated and authorized.

290
00:18:48,000 --> 00:18:52,284
Then it passed into the admission controllers

291
00:18:52,332 --> 00:18:56,016
which basically triggers a list of webhooks that

292
00:18:56,038 --> 00:19:01,296
can mutate, validate and that's

293
00:19:01,328 --> 00:19:04,692
it. Mutate and validate your request. And then,

294
00:19:04,746 --> 00:19:08,052
and only then when your request is valid it move

295
00:19:08,106 --> 00:19:12,336
forward and to be persisted and executed to

296
00:19:12,378 --> 00:19:15,860
those AHCD. Now, gatekeeper,

297
00:19:16,020 --> 00:19:19,316
gatekeeper is a customizable admission webhook.

298
00:19:19,348 --> 00:19:22,804
So whenever a resource in the cluster is being created,

299
00:19:22,852 --> 00:19:27,016
updated or deleted, the API server calls the admission

300
00:19:27,048 --> 00:19:30,904
controls, which triggers the gatekeeper Webhook

301
00:19:31,032 --> 00:19:34,952
gatekeeper takes those request along with the resource and predefined

302
00:19:35,016 --> 00:19:38,892
policies, sends everything to OPA. OPA evaluates

303
00:19:38,956 --> 00:19:42,944
everything, and if OPA find any

304
00:19:42,982 --> 00:19:46,604
misconfigurations, any violation, then gatekeeper will project

305
00:19:46,652 --> 00:19:49,840
a request and the user will receive an error.

306
00:19:52,200 --> 00:19:55,428
Now let's talk about how can you use

307
00:19:55,594 --> 00:19:58,944
Gatekeeper. So first of all, you need to install gatekeeper

308
00:19:58,992 --> 00:20:02,276
on your cluster. Then you need to write all

309
00:20:02,298 --> 00:20:06,104
your policies. But this time, since Gatekeeper is

310
00:20:06,142 --> 00:20:09,956
installed on the cluster, you don't write your policies in rego files,

311
00:20:10,068 --> 00:20:13,204
you write them in a constraint template

312
00:20:13,252 --> 00:20:16,836
CRD. So as you can see here, the constraint

313
00:20:16,868 --> 00:20:20,236
template is basically the policy that you want to enforce. You can see

314
00:20:20,258 --> 00:20:23,212
the actual rego in the green part over here.

315
00:20:23,346 --> 00:20:26,556
And this is an example of a policy that receives a

316
00:20:26,578 --> 00:20:29,936
required label and check if that resource, if a

317
00:20:29,958 --> 00:20:33,570
resource actually includes that

318
00:20:34,420 --> 00:20:37,888
label. Then after

319
00:20:37,974 --> 00:20:41,856
you write all your policies, you still need to tell Gatekeeper how

320
00:20:41,878 --> 00:20:45,460
to use that policy on which kind you want this policy

321
00:20:45,530 --> 00:20:49,184
to be applied to, and you do that with those constraint.

322
00:20:49,312 --> 00:20:53,032
So as you can see here, I created a constraint that takes

323
00:20:53,086 --> 00:20:56,904
those Kubernetes required labels, which is the policy that we

324
00:20:56,942 --> 00:21:00,452
just wrote, and applies it on every namespace

325
00:21:00,516 --> 00:21:04,088
with the owner label. So in conclusion, this policy

326
00:21:04,174 --> 00:21:08,104
ensures that every namespace in the cluster has the owner

327
00:21:08,232 --> 00:21:09,100
label.

328
00:21:13,200 --> 00:21:17,604
So with conf test in the CI gatekeeper in the cluster,

329
00:21:17,752 --> 00:21:20,652
now we can go back to sleep safely,

330
00:21:20,716 --> 00:21:24,524
because we gain some very powerful policy enforcement

331
00:21:24,652 --> 00:21:27,564
to our Kubernetes resources,

332
00:21:27,692 --> 00:21:31,684
but does come with a price. There are a couple of challenges that

333
00:21:31,722 --> 00:21:35,030
I want us to talk about. So first of all,

334
00:21:35,480 --> 00:21:39,444
as your organization grows, you would probably want to

335
00:21:39,482 --> 00:21:43,400
modify or to change some of your policies. You would probably

336
00:21:43,470 --> 00:21:46,616
want to add new policy, delete some of the

337
00:21:46,638 --> 00:21:50,664
policies to change one of the policies. And this

338
00:21:50,702 --> 00:21:54,656
can be very challenging tasks when your policies are written

339
00:21:54,788 --> 00:21:58,636
both as rego files and as a constraint template for a

340
00:21:58,658 --> 00:22:02,236
gatekeeper, especially if you have multiple or a

341
00:22:02,258 --> 00:22:05,180
lot of many git repositories.

342
00:22:06,240 --> 00:22:09,804
Now there are a couple of ways to face this challenge.

343
00:22:09,932 --> 00:22:13,900
You can use a dynamic Yaml file that you download

344
00:22:13,980 --> 00:22:17,020
from s three with all your configuration.

345
00:22:17,180 --> 00:22:21,624
You can use eTCD as your centralized management,

346
00:22:21,772 --> 00:22:25,636
state management, and you can pull your configuration from

347
00:22:25,658 --> 00:22:29,344
it. Those are many ways to face this challenge, but it is a challenge

348
00:22:29,392 --> 00:22:31,620
that you should take under consideration.

349
00:22:32,200 --> 00:22:35,764
Another challenge that I wanted to talk about is actually the Rego

350
00:22:35,812 --> 00:22:39,736
language itself. And I know this because I tried to implement some of

351
00:22:39,758 --> 00:22:43,252
those tree policies in Rego language.

352
00:22:43,396 --> 00:22:47,340
So if your policies require some level of complementary

353
00:22:49,920 --> 00:22:54,284
let's reverse it. If your policies don't require any

354
00:22:54,322 --> 00:22:57,820
level of complexity, and they are pretty straightforward,

355
00:22:58,480 --> 00:23:02,464
you shouldn't worry at all about Rego. But if your policies will

356
00:23:02,502 --> 00:23:06,032
require some complexity, such as, I don't know,

357
00:23:06,086 --> 00:23:09,808
like make sure that one of

358
00:23:09,814 --> 00:23:13,636
the keys is within a range of numbers,

359
00:23:13,818 --> 00:23:17,524
make sure that one of the keys, I don't know, uses a specific kind

360
00:23:17,562 --> 00:23:21,104
of input unit, or compare

361
00:23:21,152 --> 00:23:24,932
one of the keys to another key, maybe in a different file.

362
00:23:25,076 --> 00:23:28,824
Then Rego might be a little difficult to

363
00:23:28,862 --> 00:23:31,828
use and a little bit difficult to implement.

364
00:23:32,004 --> 00:23:35,992
So my fair advice to you is

365
00:23:36,046 --> 00:23:39,144
to look for tools that already come

366
00:23:39,182 --> 00:23:42,844
with built in policies, or at least look for

367
00:23:42,882 --> 00:23:46,652
policies that already written in Rego that you can use and send

368
00:23:46,706 --> 00:23:47,580
inspiration.

369
00:23:52,100 --> 00:23:56,144
But another way

370
00:23:56,182 --> 00:23:59,772
to face this challenge, if you think about it, would be

371
00:23:59,846 --> 00:24:04,052
by not using gatekeeper at all and to guarantee that

372
00:24:04,106 --> 00:24:07,376
your git repository is your single source

373
00:24:07,408 --> 00:24:11,232
of truth and by that eliminate the need to fence

374
00:24:11,296 --> 00:24:14,832
or to guard your cluster because users

375
00:24:14,896 --> 00:24:18,090
won't be able to kubectl apply whatever they want

376
00:24:19,580 --> 00:24:23,224
and to destroy our production. Some people might call

377
00:24:23,262 --> 00:24:25,480
it githubs.

378
00:24:27,900 --> 00:24:32,008
Now I know that there are many ways to practice githubs

379
00:24:32,024 --> 00:24:35,116
and I know that there are many tools out there, but the tool that I

380
00:24:35,138 --> 00:24:38,624
want to talk about today is Argo CD because

381
00:24:38,662 --> 00:24:42,400
I really love Argo CD. It makes life so much easier

382
00:24:42,740 --> 00:24:46,640
and it's specifically designed to make

383
00:24:46,710 --> 00:24:50,700
continuous deployment in kubernetes more efficient.

384
00:24:50,860 --> 00:24:54,640
But before we talk about Argucd and why so magical,

385
00:24:54,800 --> 00:24:59,056
I want us to talk about how SCD workflow without archd,

386
00:24:59,168 --> 00:25:03,684
how it actually looks like. So let's

387
00:25:03,732 --> 00:25:07,316
say that I have Kubernetes cluster

388
00:25:07,348 --> 00:25:10,484
in production and I use Jenkins

389
00:25:10,532 --> 00:25:14,296
for Sci CD. So you know the drill. Usually we have

390
00:25:14,318 --> 00:25:17,832
a developer that submit a code to a repository

391
00:25:17,896 --> 00:25:23,470
upstream, maybe new feature, maybe Hotfix and

392
00:25:24,080 --> 00:25:27,984
it triggers Sci pipeline which build

393
00:25:28,022 --> 00:25:31,744
those code, test those code, build a new image of

394
00:25:31,782 --> 00:25:35,856
our application, push that image into a docker registry and

395
00:25:35,878 --> 00:25:39,420
then in CD we update Kubernetes

396
00:25:39,500 --> 00:25:43,412
deployments resource, maybe some more resources to use that

397
00:25:43,546 --> 00:25:47,856
new image of our application and we apply those resources

398
00:25:47,968 --> 00:25:51,636
to our cluster. Now this is

399
00:25:51,658 --> 00:25:54,936
all nice and easy, but there are a

400
00:25:54,958 --> 00:25:59,140
couple of challenges. First of all, in order to Kubectl

401
00:25:59,220 --> 00:26:03,144
to fully function, we first of all need to configure some

402
00:26:03,182 --> 00:26:05,752
access to our Kubernetes cluster.

403
00:26:05,896 --> 00:26:09,212
Additionally we usually use, I don't know,

404
00:26:09,266 --> 00:26:12,892
AWS, Microsoft or Google. So for example I use

405
00:26:12,946 --> 00:26:16,830
eks. So I also need to configure some

406
00:26:17,780 --> 00:26:22,016
my credentials to the AWS account, then not

407
00:26:22,038 --> 00:26:26,416
only this is a configuration challenge, but this is also a

408
00:26:26,438 --> 00:26:29,896
security challenge. Another challenge

409
00:26:29,948 --> 00:26:34,948
that I want us to talk about is once

410
00:26:35,034 --> 00:26:38,372
Jenkins deploys that deployment, we don't have

411
00:26:38,426 --> 00:26:42,456
any visibility over the deployment status. We don't know if

412
00:26:42,478 --> 00:26:46,820
the deployment actually failed or not, and we need to manually

413
00:26:46,980 --> 00:26:50,392
check the logs in our cluster, which can be very difficult

414
00:26:50,526 --> 00:26:53,450
or very uncomfortable for sometimes.

415
00:26:55,120 --> 00:26:59,320
And Argo CD is made just for that, to make continuous deployment

416
00:26:59,400 --> 00:27:03,688
in kubernetes more efficient. And it does it by simply

417
00:27:03,784 --> 00:27:07,920
reversing the flow instead of pushing the changes

418
00:27:07,990 --> 00:27:11,330
to the cluster. Argo pulls those changes

419
00:27:11,700 --> 00:27:15,404
from a git repository. And the real magic

420
00:27:15,452 --> 00:27:18,896
about Argo CD is that it is actually part of the

421
00:27:18,918 --> 00:27:22,992
Kubernetes cluster itself. It's an extension to the Kubernetes

422
00:27:23,056 --> 00:27:26,932
cluster. So you don't need to provide any secrets or any

423
00:27:26,986 --> 00:27:30,932
configuration credentials, or to configure any credentials to

424
00:27:30,986 --> 00:27:34,890
Argo CD, which is really awesome,

425
00:27:35,340 --> 00:27:39,384
but let's talk about it in more details. So first

426
00:27:39,422 --> 00:27:43,348
of all, to use Argo you need to install it on your cluster

427
00:27:43,444 --> 00:27:47,704
and then you need to configure it with a repository so that Argo

428
00:27:47,752 --> 00:27:52,380
will monitor that repository by default every 3 seconds.

429
00:27:52,960 --> 00:27:56,764
So if Argo will detect that new changes were

430
00:27:56,962 --> 00:28:00,684
submitted, I. E the last commit

431
00:28:00,812 --> 00:28:04,400
has is different now, then Argo will pull those changes,

432
00:28:04,470 --> 00:28:07,632
will clone the repository and pull those changes to

433
00:28:07,686 --> 00:28:11,284
the cluster. Now you're probably wondering what will

434
00:28:11,322 --> 00:28:14,996
happen if we will apply Kubectl apply manually to

435
00:28:15,018 --> 00:28:18,288
our cluster. Now, since Argo CD is installed

436
00:28:18,304 --> 00:28:22,592
on the cluster, then in this case Argo controller will detect

437
00:28:22,736 --> 00:28:26,392
that the cluster is out of sync and Argo will

438
00:28:26,526 --> 00:28:30,024
override those changes with the state that exists in the

439
00:28:30,062 --> 00:28:34,072
repository, which will make the repository our single source of

440
00:28:34,126 --> 00:28:34,920
truth.

441
00:28:41,150 --> 00:28:44,630
Now a quick note about repositories,

442
00:28:44,710 --> 00:28:47,690
because it's very important, and I mentioned it a lot,

443
00:28:47,840 --> 00:28:51,326
it's been established as a best practice in

444
00:28:51,348 --> 00:28:55,050
githubs and specifically with ArgosD to separate

445
00:28:55,130 --> 00:28:58,954
between the application source code and the application's configurations

446
00:28:59,082 --> 00:29:02,218
repositories, and to have run a repository

447
00:29:02,314 --> 00:29:06,194
to the application source code and a different repository to

448
00:29:06,232 --> 00:29:09,634
the application configure. The reason why is because you

449
00:29:09,672 --> 00:29:13,102
usually have more than one deployment

450
00:29:13,246 --> 00:29:16,534
file. You usually have services and

451
00:29:16,572 --> 00:29:20,182
ingress files and deployment and

452
00:29:20,236 --> 00:29:23,954
secrets config maps, and you have a lot of other config

453
00:29:24,002 --> 00:29:28,086
file types. You usually manage them

454
00:29:28,268 --> 00:29:31,762
in different environments with tools like has customize

455
00:29:31,826 --> 00:29:35,686
Bazel whatsoever. So you have a lot of configure files

456
00:29:35,718 --> 00:29:39,094
to manage. And once you change one of these files, you don't

457
00:29:39,142 --> 00:29:42,606
want to trigger the application CI pipeline because

458
00:29:42,708 --> 00:29:46,366
nothing really changed in the application source code. And this

459
00:29:46,388 --> 00:29:49,582
is why it's very recommended to

460
00:29:49,636 --> 00:29:53,258
separate between the two and to have one repository for the

461
00:29:53,284 --> 00:29:57,886
application source code and another repository to the application configuration,

462
00:29:57,998 --> 00:30:01,730
which usually called the Gitops repository.

463
00:30:04,230 --> 00:30:08,102
So now let's talk about how a CD workflow looks

464
00:30:08,156 --> 00:30:11,474
like with Argocd and a separated Gitops

465
00:30:11,522 --> 00:30:14,966
repository. So once again we have

466
00:30:14,988 --> 00:30:20,886
a developer that submit new code into GitHub's

467
00:30:20,918 --> 00:30:24,134
repository upstream, which triggers the CI

468
00:30:24,182 --> 00:30:27,018
pipeline, which build the code, test those code,

469
00:30:27,104 --> 00:30:31,214
create a new image of the application, push that

470
00:30:31,252 --> 00:30:35,866
image into docker registry, and then the CI updates

471
00:30:35,978 --> 00:30:39,246
a deployment to use that new image of

472
00:30:39,268 --> 00:30:42,910
the application. But in a separated repository,

473
00:30:43,250 --> 00:30:46,334
a repository, a Githubs repository

474
00:30:46,462 --> 00:30:50,046
that argo monitors. And when Argo

475
00:30:50,078 --> 00:30:53,762
will detect that those new changes were made, Argo will pull

476
00:30:53,816 --> 00:30:58,310
those changes and will apply them to the cluster.

477
00:30:59,610 --> 00:31:03,334
So if you think about it, using conftest for

478
00:31:03,372 --> 00:31:07,110
instance in the CI of your Githubs repository

479
00:31:11,150 --> 00:31:15,018
will provide you a very powerful policy

480
00:31:15,104 --> 00:31:18,778
enforcement because now you can guarantee that your

481
00:31:18,864 --> 00:31:22,394
GitHub's repository is your single source

482
00:31:22,442 --> 00:31:27,520
of truth much.

483
00:31:29,170 --> 00:31:32,880
Now the good news is that

484
00:31:33,750 --> 00:31:37,234
you have very powerful policy enforcement. The bad

485
00:31:37,272 --> 00:31:41,314
news is that this is only the beginning because

486
00:31:41,432 --> 00:31:45,754
once you have policy enforcement, your organization is a continuously

487
00:31:45,822 --> 00:31:49,810
living, breathing cell and policy management policy enforcement

488
00:31:49,890 --> 00:31:53,874
should continuously evolve and continuously updated according

489
00:31:53,922 --> 00:31:56,120
to your organization's needs.

490
00:31:58,250 --> 00:32:02,118
And this is where the centralized policy management comes into the picture.

491
00:32:02,294 --> 00:32:05,978
So to build centralized policy management, first of all you

492
00:32:05,984 --> 00:32:09,926
need to make sure that you have those right environment to dynamically adjust

493
00:32:09,958 --> 00:32:12,874
your policies. Not only implement those policies,

494
00:32:13,002 --> 00:32:17,520
you also need to make sure that you can update your policies and

495
00:32:18,530 --> 00:32:21,802
reconfigure policies and control your policies.

496
00:32:21,946 --> 00:32:25,322
And git. Git isn't the best solution

497
00:32:25,386 --> 00:32:29,266
because Git won't provide you anything that you need. Let's take permissions for

498
00:32:29,288 --> 00:32:33,154
instance. How will you use git to controls over who can

499
00:32:33,272 --> 00:32:36,550
delete or create a new policy, for instance,

500
00:32:37,530 --> 00:32:41,846
another thing that I wanted to talk about is that you

501
00:32:41,868 --> 00:32:46,134
would also need to make sure that your policies are

502
00:32:46,172 --> 00:32:49,338
actually effective. And the thing about policies in

503
00:32:49,344 --> 00:32:52,906
Kubernetes is that those is sort of like a contract between

504
00:32:53,088 --> 00:32:57,446
application owners, cluster admins and security stakeholders.

505
00:32:57,638 --> 00:33:00,786
So in order for the policies to be truly effective,

506
00:33:00,918 --> 00:33:04,350
they not only need to work and to

507
00:33:04,420 --> 00:33:09,226
really enforce what you want them to enforce, they also need to be communicated

508
00:33:09,338 --> 00:33:12,670
properly between everybody in those organization

509
00:33:13,010 --> 00:33:16,862
and to make sure that your policies are actually being communicated

510
00:33:16,926 --> 00:33:20,610
properly. You need to ensure that people actually

511
00:33:20,680 --> 00:33:24,274
know what to do when one of the policies gets failed. You need to

512
00:33:24,312 --> 00:33:28,086
know which policy gets failed the

513
00:33:28,108 --> 00:33:32,086
most. You need to ask yourself,

514
00:33:32,268 --> 00:33:35,334
how will I delegate the knowledge? How will

515
00:33:35,372 --> 00:33:38,726
I provide some guidelines to my people in order for

516
00:33:38,748 --> 00:33:42,102
them to actually solve the policies when they fail?

517
00:33:42,246 --> 00:33:45,562
You can use email for that, and we saw a lot of companies do it,

518
00:33:45,616 --> 00:33:49,366
but obviously this doesn't scale because I'm a developer,

519
00:33:49,478 --> 00:33:52,906
I do code for my life, I'm a feature machine,

520
00:33:53,098 --> 00:33:56,650
I don't know infrastructure, I don't know Kubernetes.

521
00:33:56,810 --> 00:34:00,638
And honestly I can't remember to put the

522
00:34:00,644 --> 00:34:04,014
memory limit or to use that version instead of that version.

523
00:34:04,142 --> 00:34:06,980
Misconfiguration can easily happen here,

524
00:34:07,670 --> 00:34:10,910
so you need to be able to review constantly,

525
00:34:10,990 --> 00:34:14,242
continuously review, monitor and

526
00:34:14,296 --> 00:34:17,506
control your policies. Which policies fail

527
00:34:17,538 --> 00:34:21,030
the most? Which policies are actually being used in practice?

528
00:34:21,370 --> 00:34:24,918
How can you make sure that you make

529
00:34:25,004 --> 00:34:28,450
progress and that you get those improvement that you want

530
00:34:28,540 --> 00:34:32,074
with your policies? And the last

531
00:34:32,112 --> 00:34:35,942
tool that I wanted to talk about is actually my very own the Datree

532
00:34:36,006 --> 00:34:37,770
Cli open source,

533
00:34:38,830 --> 00:34:42,542
which actually combines everything that we just talked about.

534
00:34:42,676 --> 00:34:47,118
So the datree, much like conftest, allows us to validate and

535
00:34:47,284 --> 00:34:50,826
scan all our Kubernetes resources. But unlike

536
00:34:50,858 --> 00:34:55,006
conftest, the tree already comes with built in rules

537
00:34:55,038 --> 00:34:58,690
and policies for Kubernetes and Argo CD.

538
00:34:59,750 --> 00:35:03,694
Now the tree is specially designed to be run in the CI

539
00:35:03,822 --> 00:35:07,106
or as a local testing library or as a

540
00:35:07,128 --> 00:35:10,758
pre commit hook. And the way that those CLI works

541
00:35:10,844 --> 00:35:14,486
is that for every resource and for every file that exists on

542
00:35:14,508 --> 00:35:17,894
a given pet, the datree runs automatic checks to

543
00:35:17,932 --> 00:35:21,914
see if it violates any

544
00:35:21,952 --> 00:35:25,530
one of the policies and the rules, and for

545
00:35:25,600 --> 00:35:29,766
every violation that it finds, for every potential misconfiguration,

546
00:35:29,878 --> 00:35:34,170
the datree displays a full detailed output of the violation

547
00:35:34,330 --> 00:35:38,042
and guidelines of how to actually solve

548
00:35:38,186 --> 00:35:41,450
this failure. Now under the hood,

549
00:35:41,610 --> 00:35:45,294
every automatic check, every one of these automatic checks

550
00:35:45,342 --> 00:35:48,946
include three steps. First of all, to ensure that the

551
00:35:48,968 --> 00:35:52,622
file is actually valid yaml file.

552
00:35:52,766 --> 00:35:56,594
The second step is that those file is

553
00:35:56,712 --> 00:36:00,562
actually a valid Kubernetes file or ArgoCD resource.

554
00:36:00,706 --> 00:36:04,486
And the last step is the policy check to verify that

555
00:36:04,508 --> 00:36:08,194
the resource follows the best practices, the rules

556
00:36:08,242 --> 00:36:12,074
that you customize, that you wrote for your organization and

557
00:36:12,112 --> 00:36:14,380
the tree provided out of the box.

558
00:36:15,470 --> 00:36:18,826
Now to use the Datree, first of all you need to install it

559
00:36:18,848 --> 00:36:22,686
on your machine, on your local machine, and then

560
00:36:22,788 --> 00:36:26,266
simply run it with the tree test and the path

561
00:36:26,298 --> 00:36:28,640
of all the files that you want to test for.

562
00:36:30,050 --> 00:36:33,902
And that's it. It's free, it's open source. And I

563
00:36:33,956 --> 00:36:37,246
highly recommend everybody submit a code, review the

564
00:36:37,268 --> 00:36:41,118
code, submit a pull request. I will be there. Can't wait

565
00:36:41,204 --> 00:36:46,246
to meet you. And last

566
00:36:46,268 --> 00:36:49,730
but not least, I really those that this session inspired

567
00:36:49,810 --> 00:36:53,046
you to start thinking about what are the policies in

568
00:36:53,068 --> 00:36:57,414
your organization, how would be the best way for you to enforce them,

569
00:36:57,612 --> 00:37:01,874
and how will you start to build your own centralized policy management

570
00:37:01,922 --> 00:37:04,040
solution. Thank you very much.

