1
00:01:42,590 --> 00:01:46,226
You. Hey everybody, welcome to my

2
00:01:46,248 --> 00:01:49,554
talk. Who goes there? Actively detecting intruders with

3
00:01:49,592 --> 00:01:53,166
cyber deception tools here at Comp 42. I'm very excited

4
00:01:53,198 --> 00:01:56,386
to be part of the lineup. I hope you enjoy all the amazing content from

5
00:01:56,408 --> 00:02:00,274
all the creators and all the providers out there. Let's go ahead and get started.

6
00:02:00,472 --> 00:02:04,098
So, I'm Dwayne. I live in Chicago, Illinois. I've been a

7
00:02:04,104 --> 00:02:07,562
developer advocate since 2016. You can go out and hear hear me

8
00:02:07,616 --> 00:02:11,434
co host a podcast called the Security Repo podcast. We have some really

9
00:02:11,472 --> 00:02:14,762
awesome hosts, really awesome guests, I should say some awesome hosts too,

10
00:02:14,816 --> 00:02:18,570
but some awesome guests telling the world about great things

11
00:02:18,640 --> 00:02:22,246
in the world of security, from physical security to pen testing to

12
00:02:22,288 --> 00:02:25,646
API security and of course, code security. Hit me up

13
00:02:25,668 --> 00:02:29,770
on the Internet out there, McDwayne, at most places, including GitHub.

14
00:02:29,850 --> 00:02:33,586
And feel free to email me. Dwayne mcdaniel@gitguardian.com

15
00:02:33,688 --> 00:02:37,454
I work for Gitguardian. We are a code security platform focused

16
00:02:37,502 --> 00:02:41,166
on helping companies eliminate the problem of hard coded

17
00:02:41,198 --> 00:02:44,638
credentials, finding where those plaintext credentials appear,

18
00:02:44,734 --> 00:02:48,162
and, well, it's giving you a path to do something about it.

19
00:02:48,296 --> 00:02:51,654
We also make honey tokens, which will come up later. But real

20
00:02:51,692 --> 00:02:55,382
quick, before I go any further, I need to deploy something. So I'm going to

21
00:02:55,516 --> 00:02:58,866
go ahead and copy these credentials and go to GitHub,

22
00:02:59,058 --> 00:03:02,246
and we're going to edit this and paste

23
00:03:02,278 --> 00:03:05,514
those in. And just because I feel like it, I'm going to go ahead and

24
00:03:05,552 --> 00:03:08,714
make those a comment, and we'll commit that

25
00:03:08,752 --> 00:03:12,398
change and we'll come back to

26
00:03:12,404 --> 00:03:15,998
that later. Attackers want your credentials. We know this.

27
00:03:16,084 --> 00:03:20,046
We all know this. We're up against a

28
00:03:20,068 --> 00:03:23,482
lot of threats out there in the world, and if they get those credentials,

29
00:03:23,546 --> 00:03:26,650
some bad things can happen. I'm going to tell a couple of

30
00:03:26,660 --> 00:03:30,114
horror stories and they might be a little disconcerting, a little

31
00:03:30,152 --> 00:03:33,938
scary. And if you get a little scared, and this is true of anytime you

32
00:03:33,944 --> 00:03:37,474
get a little scared, feel free to recite the Benning jesuit litany against

33
00:03:37,512 --> 00:03:40,678
fear. I'm a huge dune fan, and this is one of

34
00:03:40,684 --> 00:03:44,418
the greatest things that came out of that series. I think you will remain.

35
00:03:44,514 --> 00:03:47,480
Only the fear will be gone at the end.

36
00:03:48,170 --> 00:03:51,546
So just take a deep breath. And this is true of, again, anytime you

37
00:03:51,568 --> 00:03:55,526
do leak something or you think that you are being breached.

38
00:03:55,718 --> 00:04:00,270
So uber last year they had attack,

39
00:04:00,420 --> 00:04:04,494
they had a super admin, got phished. Now they

40
00:04:04,532 --> 00:04:07,838
have MFA. So it's not like they weren't taking security

41
00:04:07,924 --> 00:04:11,326
seriously. They think that with

42
00:04:11,348 --> 00:04:14,634
that flood of MFA requests, multi factor authentication

43
00:04:14,682 --> 00:04:18,066
requests to that admin. His thumb slipped, he got

44
00:04:18,088 --> 00:04:21,586
tired, eventually just clicked the wrong button. Once the

45
00:04:21,608 --> 00:04:25,234
attacker was in, finds a bunch of powershell scripts

46
00:04:25,282 --> 00:04:29,826
chock full of credentials to everything else, including their psychotic

47
00:04:29,858 --> 00:04:33,814
pam, which allowed access to hacker one and

48
00:04:33,932 --> 00:04:38,294
Slack and their Google Drive and everything else.

49
00:04:38,412 --> 00:04:41,734
We don't know exactly what this attacker took, but we do know this story because

50
00:04:41,772 --> 00:04:44,730
they didn't take him seriously. They thought it was some prankster,

51
00:04:45,310 --> 00:04:48,426
and the next person talked to was the New York Times. And you can go

52
00:04:48,448 --> 00:04:50,446
read that story from the New York Times.

53
00:04:50,628 --> 00:04:54,206
AstraZeneca here's an interesting one, where a

54
00:04:54,228 --> 00:04:57,870
hard coded credential caused a problem for them.

55
00:04:58,020 --> 00:05:01,502
When a developer pushed a test environment credential out

56
00:05:01,556 --> 00:05:05,218
to public GitHub repo, where it was discovered and used

57
00:05:05,304 --> 00:05:08,754
by outsiders, and you might be thinking, okay, well, what's the big deal?

58
00:05:08,792 --> 00:05:12,174
It's in a test environment. Well, another developer had pushed

59
00:05:12,222 --> 00:05:15,810
actual customer data into that test environment.

60
00:05:16,150 --> 00:05:19,474
Perfect storm, because they don't know exactly what was stolen.

61
00:05:19,522 --> 00:05:23,382
They don't know exactly what all customers were affected over the year

62
00:05:23,436 --> 00:05:26,822
period where this was true. This was in public

63
00:05:26,876 --> 00:05:30,250
GitHub, so it was very easy to detect. And those

64
00:05:30,320 --> 00:05:33,866
credentials were used by, well, they don't know exactly how

65
00:05:33,888 --> 00:05:38,058
many times by who. Circle CI, maybe you lived through this.

66
00:05:38,224 --> 00:05:41,950
They had a remote developer who had

67
00:05:42,020 --> 00:05:45,854
an insecure system. He had a plex server that had never been

68
00:05:45,892 --> 00:05:49,120
patched on his remote working box.

69
00:05:49,730 --> 00:05:53,826
Attacker is just broadly attacking plex that day.

70
00:05:54,008 --> 00:05:57,534
Finds that's the vulnerability into that particular computer realizes,

71
00:05:57,582 --> 00:06:01,406
hey, this computer can access the circleci internal

72
00:06:01,438 --> 00:06:05,098
network, plants some malware, and it starts

73
00:06:05,134 --> 00:06:08,786
stealing credentials anywhere it can find them from heap dumps,

74
00:06:08,818 --> 00:06:12,422
from memory from anywhere it can find them

75
00:06:12,476 --> 00:06:16,214
pasted in plain text anywhere. Uses those credentials to

76
00:06:16,252 --> 00:06:20,454
then get into customer applications and start planning the same malware that

77
00:06:20,492 --> 00:06:24,214
started stealing things the same day that they announced,

78
00:06:24,342 --> 00:06:27,974
hey, customers, we had to rotate all of these API keys.

79
00:06:28,022 --> 00:06:31,774
This was January 3, I believe, of 2023,

80
00:06:31,892 --> 00:06:35,374
same day they announced that. And security researchers said,

81
00:06:35,412 --> 00:06:38,442
hey, all of my honey tokens went off inside Circle CI.

82
00:06:38,586 --> 00:06:42,426
Something's gone wrong. And that's

83
00:06:42,458 --> 00:06:46,750
what we're talking about today. Now, all of these stories involved hard coded credentials

84
00:06:46,830 --> 00:06:50,942
because we know that that is what attackers want. If they're following

85
00:06:51,006 --> 00:06:54,366
the standard attack path, then it's that initial breach.

86
00:06:54,478 --> 00:06:58,274
Live off the land, figure out what's there, laterally expand escalate

87
00:06:58,322 --> 00:07:02,070
privileges, find what data you can, exfiltrate that out,

88
00:07:02,140 --> 00:07:05,334
and, well, do whatever nasty business you're going to do with it.

89
00:07:05,532 --> 00:07:09,186
We know they're acting faster than ever before because they know we're

90
00:07:09,218 --> 00:07:12,774
defending faster than ever before. We know how they behave,

91
00:07:12,822 --> 00:07:16,410
though. We know that path, and we can start using that against them.

92
00:07:16,560 --> 00:07:20,026
That's the important takeaway from all of the data. All of

93
00:07:20,048 --> 00:07:23,754
the Verizon DbIR, the SOFOS reporting, the CISO

94
00:07:23,802 --> 00:07:26,590
reporting, all of the other acronyms out there in security.

95
00:07:26,740 --> 00:07:29,854
All that reporting says they behave generally the same way.

96
00:07:30,052 --> 00:07:32,240
We know exactly what they want, too.

97
00:07:33,410 --> 00:07:37,122
They want your data so they can ransom it or sell it out there on

98
00:07:37,176 --> 00:07:41,326
the Internet. They also want your machine resources to either crypto

99
00:07:41,358 --> 00:07:45,082
mine or to sell access to those machines

100
00:07:45,246 --> 00:07:48,502
to other malicious people to do other things

101
00:07:48,556 --> 00:07:52,134
with, like DDoS attacks or their own crypto mining and

102
00:07:52,172 --> 00:07:55,814
anything that leads back to those

103
00:07:55,852 --> 00:07:58,810
abilities and those data or systems.

104
00:07:59,150 --> 00:08:02,986
We know this is a problem that we

105
00:08:03,008 --> 00:08:06,742
as developers aren't making the attackers lives harder, we're making it easier

106
00:08:06,806 --> 00:08:09,862
because we keep leaving plain text credentials around

107
00:08:10,016 --> 00:08:13,450
again, lateral expansion and escalation

108
00:08:13,530 --> 00:08:17,470
are general themes we see in almost all attacks.

109
00:08:17,890 --> 00:08:21,466
Last year, we found over 10 million hard coded credentials added

110
00:08:21,498 --> 00:08:24,954
to GitHub public repos. Here at Gitguardian,

111
00:08:25,002 --> 00:08:28,466
we look at every single commit that hits GitHub public through the

112
00:08:28,488 --> 00:08:31,854
API. And last year it was over a billion commits.

113
00:08:31,902 --> 00:08:35,426
And out of that we found 10 million hard coded credentials, and we found out

114
00:08:35,448 --> 00:08:38,534
about one out of every ten developers has done

115
00:08:38,572 --> 00:08:42,086
this. You can read the full report to get all the fine details of what

116
00:08:42,108 --> 00:08:46,454
was stolen and what was exposed and what

117
00:08:46,652 --> 00:08:50,250
potential attackers could have stolen and used

118
00:08:50,320 --> 00:08:52,700
to leverage to get into attack.

119
00:08:53,710 --> 00:08:57,514
But point is, we know that this is what they're after.

120
00:08:57,632 --> 00:09:00,926
We know that once they're inside, they're always going to be looking for those hard

121
00:09:00,948 --> 00:09:04,990
coded credentials, and that's what we can use against them. That is

122
00:09:05,060 --> 00:09:08,990
our advantage as blue teamers, as defense.

123
00:09:09,570 --> 00:09:12,766
We have been using cyber deception for a long time. We actually have been

124
00:09:12,788 --> 00:09:16,098
using deception for a long time. Let's look

125
00:09:16,184 --> 00:09:20,030
back through history. Let's start a little bit before the Internet existed

126
00:09:20,190 --> 00:09:24,660
and go back to that first famous story of deception where

127
00:09:25,030 --> 00:09:29,578
the Trojans were fighting the Akkadians.

128
00:09:29,774 --> 00:09:33,446
Homer details all this out. And the

129
00:09:33,468 --> 00:09:36,550
Odyssey or the Iliad, I'm sorry, the Iliad.

130
00:09:38,250 --> 00:09:41,338
Sorry, the trojan horse. We all kind of know this, and we're still living with

131
00:09:41,344 --> 00:09:44,986
trojan horses today. It looks like it's something, but inside is

132
00:09:45,008 --> 00:09:48,490
something else, and it's malicious. Fast forward a little bit

133
00:09:48,560 --> 00:09:52,590
and this becomes a very common tactic in war

134
00:09:52,740 --> 00:09:56,334
that will appear strong where we're weak, and weak, where we're strong and

135
00:09:56,372 --> 00:10:00,062
lure people to lower the defenses when they shouldn't lower

136
00:10:00,116 --> 00:10:03,454
the defenses and attack them that way.

137
00:10:03,652 --> 00:10:05,954
Sun Tzu might or might not have said this, but it is in the art

138
00:10:05,992 --> 00:10:09,166
of war. The ghost army. One of my favorite

139
00:10:09,198 --> 00:10:13,230
stories from World War II. We didn't have enough tanks and bombs

140
00:10:13,310 --> 00:10:16,558
and planes at the beginning of the war. We just didn't.

141
00:10:16,734 --> 00:10:21,106
The US involvement in the war, I should say. We were building them and mobilizing

142
00:10:21,138 --> 00:10:25,014
as fast as we could. So the US military turned to Hollywood and said,

143
00:10:25,052 --> 00:10:28,778
hey, Hollywood, can you build us a bunch of traps that look like

144
00:10:28,864 --> 00:10:32,954
planes and tanks from a distance? Remember in 1942,

145
00:10:32,992 --> 00:10:36,874
reconnaissance relies on binoculars and high

146
00:10:36,912 --> 00:10:40,202
flying planes, and not up close.

147
00:10:40,256 --> 00:10:43,934
We don't have radar, we don't have satellites, we don't have drones. So this looks

148
00:10:43,972 --> 00:10:47,518
good enough from a distance. So Hollywood built us a

149
00:10:47,524 --> 00:10:50,686
bunch of balloons, and that's inflatable tanks and

150
00:10:50,708 --> 00:10:54,078
planes so we could position them and play a lot of

151
00:10:54,084 --> 00:10:57,726
loud noises so it sounded like they were staging in one direction.

152
00:10:57,838 --> 00:11:01,380
Meanwhile, we snuck the actual planes and tanks that we built

153
00:11:01,750 --> 00:11:06,010
around in another direction and, well, eventually won World War II.

154
00:11:06,190 --> 00:11:09,938
Great documentary about that, by the way. Speaking of great documentaries,

155
00:11:10,034 --> 00:11:13,334
one of my favorites I've seen recently is

156
00:11:13,372 --> 00:11:16,742
the KGB. The computer in me. It's a documentary from

157
00:11:16,796 --> 00:11:20,554
1990. That's the actual name of it. That's a screenshot from the

158
00:11:20,592 --> 00:11:24,150
opening credits. It's a Nova special, Nova from PBS,

159
00:11:24,230 --> 00:11:27,494
the public broadcasting station here in our public broadcast

160
00:11:27,542 --> 00:11:29,500
system here in the United States.

161
00:11:31,310 --> 00:11:34,826
It's based on his book the Cuckoo's egg, which is a really good book unto

162
00:11:34,858 --> 00:11:38,062
itself, and I highly recommend checking it out. Long story short,

163
00:11:38,196 --> 00:11:41,754
this is where we get the term honeypot. Cliff stole, still alive,

164
00:11:41,802 --> 00:11:45,566
still awesome dude. He is working at the Lawrence Berkeley

165
00:11:45,598 --> 00:11:49,342
National Laboratory, and he's investigating this missing

166
00:11:49,406 --> 00:11:53,234
$0.75 in billing. It costs $300

167
00:11:53,272 --> 00:11:57,122
an hour to rent these machines from Lawrence Berkeley National Laboratory,

168
00:11:57,186 --> 00:12:00,786
so you can do your research on it. Long story short, he ends

169
00:12:00,818 --> 00:12:04,226
up finding that it's somebody in eastern Europe,

170
00:12:04,418 --> 00:12:08,018
eastern Germany, I should say, who is stealing

171
00:12:08,114 --> 00:12:11,690
any data they can find on open networks, us government

172
00:12:11,760 --> 00:12:15,334
networks, military networks, and in this case, a university

173
00:12:15,382 --> 00:12:18,762
network. He won't stay on the line long enough for them

174
00:12:18,816 --> 00:12:22,506
to get a good traps on exactly who this person is.

175
00:12:22,608 --> 00:12:25,754
So his girlfriend, Cliff Stole's girlfriend at the time, suggests,

176
00:12:25,882 --> 00:12:29,534
hey, what if we put a bunch of fake data on the system and

177
00:12:29,572 --> 00:12:33,678
lure them in? Cliff Stole said, that's a great idea. He does this,

178
00:12:33,764 --> 00:12:36,270
calls it a honeypot because it's sticky.

179
00:12:37,270 --> 00:12:40,110
Download speeds in 1985 are very slow.

180
00:12:40,190 --> 00:12:43,138
So by the time this person figures out, hey, this is all just a bunch

181
00:12:43,144 --> 00:12:46,370
of junk, they've already been caught. Won't spoil the ending of this,

182
00:12:46,440 --> 00:12:49,730
but go watch that documentary. It's absolutely amazing and fascinating.

183
00:12:49,810 --> 00:12:53,078
And his book's pretty good, too. Fast forward a little bit in time,

184
00:12:53,164 --> 00:12:56,726
and honey tokens kind of takes off. As a concept. We get to

185
00:12:56,748 --> 00:13:00,426
Fred Cohen deception toolkit 91, which is the

186
00:13:00,448 --> 00:13:03,706
first description of how to

187
00:13:03,728 --> 00:13:07,482
build a honeypot system inside

188
00:13:07,536 --> 00:13:11,878
of your network. You can go refine

189
00:13:11,894 --> 00:13:15,406
this documentation today. Basically, the idea is, if it's a

190
00:13:15,428 --> 00:13:19,038
system that's not in use, let's turn it on and

191
00:13:19,124 --> 00:13:22,366
wait for people to try to access it. And we'll catch those people and they

192
00:13:22,388 --> 00:13:26,514
won't know what they're supposed to be getting into and what they're not really

193
00:13:26,712 --> 00:13:30,020
big step in the history of computer security.

194
00:13:30,710 --> 00:13:34,062
Fast forward a little bit further and this idea keeps

195
00:13:34,126 --> 00:13:37,910
catching on and people keep reinventing the wheel. But then someone finally

196
00:13:37,980 --> 00:13:41,400
says, hey, here's a commercial version of this

197
00:13:42,090 --> 00:13:45,414
enterprise. It honey pots are a great

198
00:13:45,452 --> 00:13:49,206
proven idea. Here's one off the shelf. And I think this marks

199
00:13:49,238 --> 00:13:52,842
a really important point in the history of

200
00:13:52,896 --> 00:13:56,950
hacking, because he says something. Alfred Uger

201
00:13:57,030 --> 00:14:00,554
says hackers aren't kids on a digital joyride. It's clear

202
00:14:00,592 --> 00:14:03,838
their motives, financial gain. That's as true today as it

203
00:14:03,844 --> 00:14:08,298
was when he said it. But it marks this turning point. The term hacking

204
00:14:08,394 --> 00:14:12,394
comes from MIT. It was originally meant as engineering

205
00:14:12,442 --> 00:14:15,502
students who played elaborate pranks,

206
00:14:15,646 --> 00:14:19,234
like mostly harmless. They built a car on top

207
00:14:19,272 --> 00:14:22,626
of the roof of this building, and nobody to this day knows exactly how they

208
00:14:22,648 --> 00:14:26,658
did it. Very clever. They hooked a fire hydrant

209
00:14:26,754 --> 00:14:28,550
up to a drinking fountain.

210
00:14:29,690 --> 00:14:30,950
Hilariously,

211
00:14:33,130 --> 00:14:35,618
just little fun pranks hijinks.

212
00:14:35,794 --> 00:14:39,274
Well, this is the point where we've gone from

213
00:14:39,392 --> 00:14:42,694
phone freaks and people as kind of victimless crimes

214
00:14:42,742 --> 00:14:46,506
to, hey, they're starting to steal our stuff for real.

215
00:14:46,608 --> 00:14:50,334
They're not free riders. They're not getting a free phone call long

216
00:14:50,372 --> 00:14:54,240
distance. They are actually stealing data. They're stealing money.

217
00:14:54,690 --> 00:14:57,882
Fast forward a little bit more. And honey

218
00:14:57,946 --> 00:15:01,870
pots have become a mainstream conversation in computer

219
00:15:01,940 --> 00:15:06,034
security. Augusto Destabaros in 2003

220
00:15:06,072 --> 00:15:10,146
writes inside of a message board. This is

221
00:15:10,168 --> 00:15:13,646
the exact message. But he says he's more playing with this idea called honey tokens.

222
00:15:13,758 --> 00:15:16,530
So instead of an entire system a honey pot,

223
00:15:16,680 --> 00:15:20,406
it's just information that shouldn't be flowing over

224
00:15:20,428 --> 00:15:23,618
the network. It's a piece of data that shouldn't move, in other words. And that's

225
00:15:23,634 --> 00:15:27,560
where honey token comes from. And it changes

226
00:15:27,930 --> 00:15:31,030
that part of the conversation of like, honey pot. Now it's a subpart

227
00:15:31,530 --> 00:15:35,610
of that to a token that shouldn't be touched. Fast forward

228
00:15:35,680 --> 00:15:39,126
a little bit further. And I think we've now reached the modern

229
00:15:39,238 --> 00:15:42,398
definition, which I'll properly define a little bit later

230
00:15:42,564 --> 00:15:46,586
in the slide deck. But Finkst,

231
00:15:46,618 --> 00:15:50,254
a company out of South Africa, builds the

232
00:15:50,292 --> 00:15:54,194
system called canary Tokens, and in 2016 they

233
00:15:54,232 --> 00:15:58,082
add AWS tokens into

234
00:15:58,136 --> 00:16:01,826
their system. And I think this is a very definitive moment in the

235
00:16:01,848 --> 00:16:05,302
history of what we're talking about, where a token goes from

236
00:16:05,356 --> 00:16:09,000
this idea of a piece of data that shouldn't move to really

237
00:16:09,610 --> 00:16:13,106
combining with tokens like JWTs or bearer

238
00:16:13,138 --> 00:16:15,670
tokens, in this case,

239
00:16:15,740 --> 00:16:20,138
AWS token, to really be something

240
00:16:20,224 --> 00:16:24,106
for someone's trying to get in using this

241
00:16:24,288 --> 00:16:28,154
and set off an alert. Fast forward to 2023 at

242
00:16:28,192 --> 00:16:31,418
RSA. I was very fortunate enough to see this talk, see Kevin

243
00:16:31,434 --> 00:16:35,662
Mandia talk about second

244
00:16:35,716 --> 00:16:39,086
line defense. Whole pointer's presentation is we

245
00:16:39,108 --> 00:16:43,150
can build elaborate walls, we can build these elaborate defenses and wafts,

246
00:16:43,490 --> 00:16:46,894
but they're going to get in. We just know this. We have to

247
00:16:46,932 --> 00:16:50,290
assume that we can be breached and assume that breach is happening all the time.

248
00:16:50,360 --> 00:16:53,906
So we need early warning signs. And you can see it at the

249
00:16:53,928 --> 00:16:57,794
very bottom of this picture. He says, honey tokens are your early warning

250
00:16:57,842 --> 00:17:01,250
signs. We have now reached this is mainstream.

251
00:17:01,330 --> 00:17:05,254
This is Google Cloud saying, this is how you protect yourself.

252
00:17:05,452 --> 00:17:08,166
And that gets us to where we're at today. And what I'm going to talk

253
00:17:08,188 --> 00:17:11,990
about for the rest of this session. What exactly is a honey token?

254
00:17:12,070 --> 00:17:15,740
Well, we talked about the original definition from Gusto and

255
00:17:16,110 --> 00:17:19,046
way we watched that merge with other tokens.

256
00:17:19,158 --> 00:17:22,318
And here's where I think we are today. This is the definition we use

257
00:17:22,404 --> 00:17:25,994
internally at Gitguardian. Honey token is a decoy credential

258
00:17:26,122 --> 00:17:28,160
that doesn't allow any real access.

259
00:17:29,170 --> 00:17:32,754
Importantly, it looks identical to a real

260
00:17:32,792 --> 00:17:35,780
credential, to an attacker or to anybody else.

261
00:17:36,470 --> 00:17:40,686
If it's used, it exposes that it's

262
00:17:40,718 --> 00:17:44,146
being used through an alert and giving

263
00:17:44,168 --> 00:17:47,346
you at least the ip address of the person trying to

264
00:17:47,368 --> 00:17:51,254
use it. This is how you build them. This is one way to build

265
00:17:51,292 --> 00:17:54,678
them. This is an approach. This is a GG canary. This is an

266
00:17:54,684 --> 00:17:58,806
open source repository that Gitguardian built that

267
00:17:58,828 --> 00:18:02,086
uses terraform and AWS. But the concept is very straightforward.

268
00:18:02,198 --> 00:18:05,306
Let's create users in the system that have no

269
00:18:05,328 --> 00:18:09,590
rights whatsoever. If you are going to use this, I would advise building

270
00:18:09,680 --> 00:18:13,310
this in a different entire region than your

271
00:18:13,380 --> 00:18:16,878
other tools that you're using or other

272
00:18:16,964 --> 00:18:20,286
deployments on AWS just for safety. Really isolate it as

273
00:18:20,308 --> 00:18:23,826
much as possible. But you want a list of

274
00:18:23,848 --> 00:18:25,970
users who have no credentials.

275
00:18:26,470 --> 00:18:30,206
Let's take those users and build a lambda

276
00:18:30,238 --> 00:18:34,260
function that uses cloud trail to watch for

277
00:18:34,730 --> 00:18:38,694
those credentials trying to be used. Create that event

278
00:18:38,732 --> 00:18:42,866
in an s three bucket or from the logging throw that s three bucket

279
00:18:43,058 --> 00:18:46,630
lambda. Does the triangulation of

280
00:18:46,700 --> 00:18:50,346
does that name on the list match one of

281
00:18:50,368 --> 00:18:53,786
our honey token credentials from the list? If it

282
00:18:53,808 --> 00:18:57,734
does, send either a slack

283
00:18:57,782 --> 00:19:02,326
message or an email using SES or sendgrid.

284
00:19:02,518 --> 00:19:05,886
This is the product GG Canary out of the box open source this is

285
00:19:05,908 --> 00:19:10,058
exactly how it works. Quick note, you will not find this exact diagram

286
00:19:10,154 --> 00:19:13,326
inside the repository. This comes from a blog post. If you

287
00:19:13,348 --> 00:19:16,586
just google Gitguardian GG Canary, it's the first

288
00:19:16,628 --> 00:19:20,466
blog post that pops up about it from the Gitguardian blog. But this

289
00:19:20,488 --> 00:19:23,954
is the idea. Is this the only way to build them? Absolutely not.

290
00:19:23,992 --> 00:19:28,020
This is how we built this one. And that gets me to my next point.

291
00:19:28,390 --> 00:19:31,718
Honey tokens can be built by hand. There's a

292
00:19:31,724 --> 00:19:35,094
lot of open source uses, open source repos we're going to talk about,

293
00:19:35,212 --> 00:19:38,454
and then there's a lot of stuff off the shelf just

294
00:19:38,492 --> 00:19:42,154
really quickly. There are a lot more open source options than what I'm talking,

295
00:19:42,272 --> 00:19:45,786
than what I'm showing here. But these are the main ones that I

296
00:19:45,808 --> 00:19:49,114
drew inspiration from for this talk. But before that,

297
00:19:49,152 --> 00:19:52,426
there's the idea you can just build these now that

298
00:19:52,448 --> 00:19:56,126
you have the concept in your head. Yeah, there's a lot of ways

299
00:19:56,148 --> 00:19:59,566
to approach this. If you can have some kind of a logging and

300
00:19:59,588 --> 00:20:03,230
some kind of alert system to tell you someone's trying to use it,

301
00:20:03,380 --> 00:20:07,282
you can build it to your imagination. We showed you the

302
00:20:07,416 --> 00:20:10,626
diagram we used for GG Shield, and you can go see the code

303
00:20:10,648 --> 00:20:13,890
for GG Shield or not GG Shield. I'm sorry, GG Canary.

304
00:20:14,710 --> 00:20:18,278
Look at GG Canary and tear it apart and see how it works. And if

305
00:20:18,284 --> 00:20:21,640
you like terraform and AWS, maybe that's the right one for you.

306
00:20:22,090 --> 00:20:25,734
Space Siren is another one that's very interesting history.

307
00:20:25,932 --> 00:20:29,862
It's forked off of something called space crab. Very interesting project.

308
00:20:29,916 --> 00:20:32,890
I'd highly recommend going out and checking that history for fun.

309
00:20:32,960 --> 00:20:36,426
Just if you like researching security histories but turned into

310
00:20:36,448 --> 00:20:40,154
Space Siren. That's the modern, still maintained thing

311
00:20:40,192 --> 00:20:43,682
today. It uses AWS directly.

312
00:20:43,766 --> 00:20:47,214
If you have a little bit of AWS know how, you'll do fine with it,

313
00:20:47,332 --> 00:20:51,230
but it's a jumping off point, I would say. And then thanks Canary tokens.

314
00:20:51,810 --> 00:20:55,246
If you can deploy it in Docker, and if you want to maintain your own

315
00:20:55,268 --> 00:20:58,866
infrastructure and run this yourself, that's a

316
00:20:58,888 --> 00:21:02,498
good one too. They all work. It's just a matter of

317
00:21:02,584 --> 00:21:04,994
what do you want to support and how do you want to build it.

318
00:21:05,112 --> 00:21:07,858
And that how do you want to support it is a very important question,

319
00:21:07,944 --> 00:21:10,438
because if the answer is I don't want to support this, I just want to

320
00:21:10,444 --> 00:21:13,794
use it, well, then you're going to start using the commercial

321
00:21:13,842 --> 00:21:17,766
options, and there's a lot of them out there. The free one that I

322
00:21:17,788 --> 00:21:21,610
think everybody should start with. If you're new to this idea and you've never seen

323
00:21:21,680 --> 00:21:23,590
a honey token in action,

324
00:21:23,750 --> 00:21:26,886
canarytokens.org, go make a honey token,

325
00:21:26,918 --> 00:21:29,260
a one off honey token, and see how it works.

326
00:21:29,970 --> 00:21:33,274
You'll produce not just an AWS credential,

327
00:21:33,322 --> 00:21:36,846
but you can make a fake credit card, a fake SQLite server or

328
00:21:36,868 --> 00:21:41,280
SQL lite file, a fake PDF, a fake email,

329
00:21:41,730 --> 00:21:45,470
and they're not real. But if someone tries it to use it for any reason,

330
00:21:45,540 --> 00:21:48,914
you get an alert and you can see what that alert looks like through their

331
00:21:48,952 --> 00:21:51,938
system. It's really cool, but it's a one off.

332
00:21:52,024 --> 00:21:54,718
And if you're working like one or two projects or one or two places you'd

333
00:21:54,734 --> 00:21:57,778
ever want to put a canary token, it's a really good free option. If you

334
00:21:57,784 --> 00:22:00,934
want to do that at scale because you're an enterprise, they sell that. It's called

335
00:22:00,972 --> 00:22:04,726
canary tools. You can go to that website and thanks to will

336
00:22:04,748 --> 00:22:08,070
gladly sell you commercial version of this at scale.

337
00:22:08,410 --> 00:22:12,230
If you're a gitguardian customer or you're planning to be a gitguardian customer,

338
00:22:12,300 --> 00:22:15,702
or it sounds like a good idea to you and you want to use GitGuardian,

339
00:22:15,766 --> 00:22:19,206
we make one too. It's called honey token module. The GitGuardian

340
00:22:19,238 --> 00:22:23,194
honey token module. We have a platform play, so module is

341
00:22:23,312 --> 00:22:26,430
the add on. It does require you to have a GitGuardian account

342
00:22:26,500 --> 00:22:29,406
which is free for individual users, teams up to 25,

343
00:22:29,508 --> 00:22:33,006
and for open source use. But this isn't a good fit for open source use,

344
00:22:33,028 --> 00:22:35,810
and I'll talk about why here in a few slides.

345
00:22:36,950 --> 00:22:40,846
If you're a Microsoft fan, they have this built into Sentinel.

346
00:22:40,878 --> 00:22:43,938
If you're using Azure, I don't know a lot about it

347
00:22:44,024 --> 00:22:47,710
other than there are documents for it, but your mileage may vary.

348
00:22:47,790 --> 00:22:51,238
Go dig through the documents and talk to your rep. If it sounds interesting

349
00:22:51,324 --> 00:22:54,998
and you're already using Sentinel. I wouldn't say go use Sentinel just for this yet,

350
00:22:55,084 --> 00:22:58,598
though. There's a lot of great reasons to use Sentinel, though. If you're a

351
00:22:58,604 --> 00:23:02,554
crowdstrike customer, they got one too. Go talk to your reps. I have no

352
00:23:02,592 --> 00:23:05,926
idea what it actually looks like, and I don't know what it's called internally,

353
00:23:06,038 --> 00:23:09,610
but I do know there was a blog post about them having honey tokens

354
00:23:10,030 --> 00:23:12,894
proofpoint, which I have talked to. Really interesting company,

355
00:23:13,012 --> 00:23:16,814
very broad play that they have. But one of their many,

356
00:23:16,852 --> 00:23:20,090
many tools is identity threat defense shadow,

357
00:23:20,250 --> 00:23:23,498
which is a honey token play. It's more of a honey

358
00:23:23,514 --> 00:23:26,814
pot play, but you can use it as a honey token system as well.

359
00:23:27,012 --> 00:23:30,018
But like I said, there's tons of options. If you're already a customer of any

360
00:23:30,024 --> 00:23:33,154
of these companies, go talk to your rep. It might even be a free

361
00:23:33,192 --> 00:23:36,514
add on up to a limit, but your mileage may vary on

362
00:23:36,552 --> 00:23:40,038
all that. So now we've talked about how to get them, how to build them,

363
00:23:40,124 --> 00:23:43,318
generally what they are, how to architect them. How do

364
00:23:43,324 --> 00:23:47,640
you use these things? Well, we think there are some best practices around this.

365
00:23:48,030 --> 00:23:50,598
Put honey tokens in private environments.

366
00:23:50,774 --> 00:23:54,838
Anywhere where someone outside of your organization shouldn't

367
00:23:54,854 --> 00:23:57,980
have access, or anyone outside of a team shouldn't have access.

368
00:23:58,590 --> 00:24:01,770
That's a good place. So your private code repositories,

369
00:24:02,430 --> 00:24:05,390
then you know if someone gets in, you have a breach on your hands,

370
00:24:05,460 --> 00:24:09,422
and that's not good. Same thing with your CI environments. Like we saw the real

371
00:24:09,476 --> 00:24:13,230
world example of Circle CI. That's a real tweet.

372
00:24:13,650 --> 00:24:17,634
You can go and find that. A third party researcher says,

373
00:24:17,672 --> 00:24:21,262
hey, all my honey tokens went off. Something's going on with Circle CI,

374
00:24:21,406 --> 00:24:25,326
and he knew before the announcement came

375
00:24:25,368 --> 00:24:28,562
out, your messaging systems,

376
00:24:28,626 --> 00:24:31,922
your project management systems, anywhere internal,

377
00:24:31,986 --> 00:24:35,702
there's no legitimate use for these things. So if someone internally does find one

378
00:24:35,756 --> 00:24:40,054
and just uses it just to use it, that's a whole different conversation

379
00:24:40,102 --> 00:24:43,562
than a breach. But it's still a security concern. Like, why is this person doing

380
00:24:43,616 --> 00:24:46,780
that? It's an educational moment at best,

381
00:24:47,790 --> 00:24:51,066
and it's a breach at worst. Put them

382
00:24:51,088 --> 00:24:54,526
in your vault systems, because if someone breaches your

383
00:24:54,548 --> 00:24:57,614
vault, that's a very bad day. That means they have access to

384
00:24:57,652 --> 00:25:02,030
literally everything, and you don't want that. Back to my point earlier on

385
00:25:02,100 --> 00:25:05,346
open source use, you don't want to put these in public places. And the whole

386
00:25:05,368 --> 00:25:09,202
nature of open source is it's public. The main reason.

387
00:25:09,256 --> 00:25:12,514
Why is that all? Not just these

388
00:25:12,552 --> 00:25:16,260
platforms, but there's a lot of platforms in the world, good and bad.

389
00:25:16,870 --> 00:25:20,178
And a lot of bots out there that are constantly scanning the Internet,

390
00:25:20,274 --> 00:25:24,022
trying to find hard coded credentials that they can

391
00:25:24,076 --> 00:25:27,494
harvest, and they're also looking for other things. But that's a big

392
00:25:27,532 --> 00:25:31,170
thing that they're doing now is let's find and validate these credentials.

393
00:25:31,330 --> 00:25:34,586
And if you put them in a private repository and

394
00:25:34,608 --> 00:25:37,526
all of a sudden a public scanner hits it, you know you got a leak

395
00:25:37,558 --> 00:25:40,159
on your hands. If you put it in a private environment and all of a

396
00:25:40,659 --> 00:25:44,058
sudden a public scanner hits it, something's gone horribly wrong.

397
00:25:44,144 --> 00:25:47,310
And you know, you need to deal with that right now and respond very quickly.

398
00:25:47,380 --> 00:25:50,606
And that's the whole point of this, is we can respond faster and cut

399
00:25:50,628 --> 00:25:54,834
those dwell times or those breach times and leak times down

400
00:25:54,872 --> 00:25:58,914
as much as possible and mitigate the situation as

401
00:25:58,952 --> 00:26:01,902
best we can as fast as we can, use a one to one ratio.

402
00:26:01,966 --> 00:26:05,106
This is another huge time saver you might be

403
00:26:05,128 --> 00:26:08,258
tempted to. I have this one honey token, I'm going to put it everywhere.

404
00:26:08,434 --> 00:26:11,906
Then if it goes off, you don't know what triggered

405
00:26:11,938 --> 00:26:15,478
it or what specific repo or environment has been

406
00:26:15,644 --> 00:26:19,666
infiltrated or leaked. So if you have 100 repositories

407
00:26:19,778 --> 00:26:23,594
and a Jira instance, and you put the same honey token everywhere and it goes

408
00:26:23,632 --> 00:26:27,494
off, now you have to triangulate which of those caused

409
00:26:27,542 --> 00:26:31,758
that. Keep it real simple, put it in one place and

410
00:26:31,844 --> 00:26:35,070
then create a new one to put somewhere else. Pretty straightforward.

411
00:26:35,570 --> 00:26:38,750
Do think in terms of scaling this with automation,

412
00:26:39,170 --> 00:26:42,718
doing this is a one off exercise. Again, if you

413
00:26:42,724 --> 00:26:46,114
have only a couple small places to put them and

414
00:26:46,152 --> 00:26:49,634
you're done, good. But if you're thinking

415
00:26:49,752 --> 00:26:53,438
I have a whole enterprise secure, I have hundreds of repos and I have thousands

416
00:26:53,454 --> 00:26:57,394
of developers, and I have way too many internal systems to

417
00:26:57,432 --> 00:27:01,586
count, then you're going to start thinking like, how do I spin

418
00:27:01,618 --> 00:27:05,094
these up and put them somewhere? That example down below,

419
00:27:05,292 --> 00:27:08,950
not sure how useful it is. I keep meaning to document it better,

420
00:27:09,020 --> 00:27:12,218
but it's just a simple script that shows, hey, here's a tool to create a

421
00:27:12,224 --> 00:27:15,706
honey token, and here's some logic to insert it into

422
00:27:15,808 --> 00:27:19,146
a git repo. That's all it is, but just a jumping off point

423
00:27:19,168 --> 00:27:21,930
to like, okay, that's how we can think of automation.

424
00:27:22,530 --> 00:27:25,710
You are probably on the blue team if you're watching this, or you might be

425
00:27:25,780 --> 00:27:28,080
someone who's just interested generally in security.

426
00:27:28,850 --> 00:27:31,600
Unless you are specifically a law enforcement agent,

427
00:27:32,130 --> 00:27:35,586
don't go after these people. When these go off, you'll get an

428
00:27:35,608 --> 00:27:39,026
IP address and I'll show these going off here in a little

429
00:27:39,048 --> 00:27:43,570
bit, but know that your job is really to protect

430
00:27:43,640 --> 00:27:47,106
your stuff. So think in terms of I got to get

431
00:27:47,128 --> 00:27:50,326
this IP address out of here, I got to make sure that the breach is

432
00:27:50,348 --> 00:27:54,338
stopped, I got to make sure that anything someone got into is secured,

433
00:27:54,434 --> 00:27:58,134
and any credentials that get leaked get rotated. Think in those terms,

434
00:27:58,252 --> 00:28:01,500
not I'm going to go hunt these people down and stop them.

435
00:28:02,110 --> 00:28:05,350
Because the truth is, that's not your job, unless specifically

436
00:28:05,430 --> 00:28:09,158
you are tasked with doing that. Then good luck.

437
00:28:09,334 --> 00:28:12,860
And like with everything else in computer science, like with every other technology,

438
00:28:13,310 --> 00:28:16,474
this is a journey. It's not a one off exercise.

439
00:28:16,522 --> 00:28:19,674
If you treat it like a one off exercise, you'll get some return on investment,

440
00:28:19,722 --> 00:28:22,910
but you'll burn yourself out trying to do it all at once,

441
00:28:22,980 --> 00:28:26,618
or you will do a couple of places and just never think about this again.

442
00:28:26,724 --> 00:28:30,226
So start thinking about honey tokens if you're going to deploy them,

443
00:28:30,248 --> 00:28:33,346
if you're going to embrace this as a strategy, which I highly encourage you

444
00:28:33,368 --> 00:28:36,830
to do, think long term and think how do we

445
00:28:36,840 --> 00:28:40,518
do this at a regular pace? Is this a

446
00:28:40,524 --> 00:28:43,574
once a week things? Is it once a quarter thing? Is once a sprint thing?

447
00:28:43,692 --> 00:28:47,462
If you added one new honey token per

448
00:28:47,516 --> 00:28:50,998
sprint, and your average sprint is three weeks, then you're

449
00:28:51,014 --> 00:28:54,074
going to get a lot of coverage in a year's time. And hopefully they never

450
00:28:54,112 --> 00:28:58,102
go off. Once they're set. They're set until that AWS

451
00:28:58,166 --> 00:29:01,500
instance completely vanishes, until you take them down.

452
00:29:01,950 --> 00:29:04,606
It's a good time to go back and check and see what happened to that

453
00:29:04,628 --> 00:29:08,014
honey token, because that's what I did earlier. I didn't explain myself, but what I

454
00:29:08,052 --> 00:29:11,914
did is I took a honey token I created through the gitguardian

455
00:29:11,962 --> 00:29:14,894
honey token module, which is an AWS token,

456
00:29:15,022 --> 00:29:18,926
AWS key id and access key pair.

457
00:29:19,118 --> 00:29:23,106
And I added it as a comment to a public repo that

458
00:29:23,128 --> 00:29:26,146
I originally created back in July. I'm recycling an old repo, I'll be honest with

459
00:29:26,168 --> 00:29:29,654
you. So over here in my email, I know it's a little hard

460
00:29:29,692 --> 00:29:33,206
to see, so I'll make it a little bit bigger. See that? Hey, I have

461
00:29:33,228 --> 00:29:36,466
these alerts that comp 42 was triggered.

462
00:29:36,578 --> 00:29:40,054
My honey token for comp 42 that it created. That's literally

463
00:29:40,102 --> 00:29:43,194
what I deployed that got triggered 26

464
00:29:43,232 --> 00:29:46,634
minutes ago. I deployed this 26 minutes ago.

465
00:29:46,832 --> 00:29:50,138
Let's go look at the honey token itself. And I can see

466
00:29:50,304 --> 00:29:53,980
all of these twelve so far

467
00:29:54,670 --> 00:29:58,414
system scans have triggered this. If we can look at the

468
00:29:58,452 --> 00:30:01,760
first one. The first one was AWS itself.

469
00:30:02,210 --> 00:30:05,518
AWS knows this is a problem. They are constantly scanning

470
00:30:05,534 --> 00:30:08,690
the Internet trying to see has anyone leaked one of our credentials?

471
00:30:09,030 --> 00:30:12,654
And that has fired off some internal stuff at AWS.

472
00:30:12,702 --> 00:30:16,274
I might get an email about that. An email will

473
00:30:16,312 --> 00:30:20,054
have been sent to someone about that. Well, that's us,

474
00:30:20,092 --> 00:30:23,686
that's gitguardian. We are constantly scanning as well because we put this on GitHub public

475
00:30:23,868 --> 00:30:28,650
and then we see, oh, here's two people in the US that

476
00:30:28,720 --> 00:30:32,534
use truffle hog as the user agent, and there's

477
00:30:32,662 --> 00:30:36,678
the same person in India did it. They scanned

478
00:30:36,854 --> 00:30:39,930
and validated. So just the act of reading

479
00:30:40,270 --> 00:30:43,214
isn't going to trigger this. They tried to validate, they tried to use it.

480
00:30:43,252 --> 00:30:47,742
They were getting caller identity, which is a very common

481
00:30:47,796 --> 00:30:51,790
call to make. Just to who am I? Just to make sure

482
00:30:51,940 --> 00:30:55,946
to see if this worked. Don't worry, these all came back invalid,

483
00:30:56,138 --> 00:30:59,438
but they were read from here immediately.

484
00:30:59,614 --> 00:31:02,274
So that's what that looks like. So what do I do about that? Well,

485
00:31:02,312 --> 00:31:05,746
in this case, what I'm going to do about it is I'm simply going to

486
00:31:05,768 --> 00:31:09,854
revoke the honey token. Revoke the honey token? It's been triggered,

487
00:31:09,902 --> 00:31:13,366
so it's not that useful anymore. People are going to mark it

488
00:31:13,388 --> 00:31:16,518
as invalid if they're trying to create it on a list, and then I'm going

489
00:31:16,524 --> 00:31:18,966
to clean up after myself and just get rid of it here. But this is

490
00:31:18,988 --> 00:31:22,426
what I'm doing. I could rewrite my history here

491
00:31:22,528 --> 00:31:25,946
and remove all instances of that, but it was

492
00:31:25,968 --> 00:31:29,386
never good anyway. So there's no harm in leaving it in my code base,

493
00:31:29,488 --> 00:31:32,458
especially my code history. If I was a real credential, I would have cleaned it

494
00:31:32,464 --> 00:31:36,238
out of my code base, my history, my git history. But again, this is

495
00:31:36,244 --> 00:31:40,334
a honey token, it doesn't really matter. So in conclusion, wrapping things up,

496
00:31:40,532 --> 00:31:43,794
if we just think in terms of defense, it's really hard

497
00:31:43,832 --> 00:31:47,426
to win if you only play defense. What we need to

498
00:31:47,448 --> 00:31:51,790
do is start actively kicking people out faster,

499
00:31:51,870 --> 00:31:55,614
cut the dwell time, put the attacker on their heels,

500
00:31:55,742 --> 00:31:59,266
make them think, wow, I don't know what to do next because the second we

501
00:31:59,288 --> 00:32:02,678
do that, we win. Because as soon as they don't know what

502
00:32:02,684 --> 00:32:05,906
to do, as soon as it no longer matches the playbook they downloaded that they're

503
00:32:05,938 --> 00:32:09,546
using for an attack, they're going to run out of ideas and

504
00:32:09,568 --> 00:32:12,060
go somewhere else where that playbook will work.

505
00:32:12,430 --> 00:32:17,046
So we need to act quicker and kick

506
00:32:17,078 --> 00:32:20,282
them off a network and make them think, I don't know how to

507
00:32:20,416 --> 00:32:25,034
further engage if it's an advanced, persistent threat like

508
00:32:25,232 --> 00:32:29,086
a Lazarus group or North Korea is attacking you. I don't know if this

509
00:32:29,108 --> 00:32:32,606
will work, but I'd say the vast majority of attacks, this is

510
00:32:32,628 --> 00:32:35,882
going to work great. It's figuring out who they are and boot them out,

511
00:32:36,036 --> 00:32:39,666
honey. Tokens are decoys. They look real because they are real, but they just

512
00:32:39,688 --> 00:32:42,834
don't give any access. But they give you an alert. You should use

513
00:32:42,872 --> 00:32:46,418
them wherever you got private data and wherever you

514
00:32:46,424 --> 00:32:49,878
got private code. There's a lot of options.

515
00:32:49,964 --> 00:32:53,158
You can build them yourself. There's tons of great articles and

516
00:32:53,164 --> 00:32:56,546
resources out there if you're going to go the DIY route. And of course there's

517
00:32:56,578 --> 00:32:59,846
commercial options. The ones I showed are just the tip of the iceberg. There's a

518
00:32:59,868 --> 00:33:03,786
lot more solutions out there. I highly am personally kind

519
00:33:03,808 --> 00:33:07,514
of skewed toward Gitguardian because I work here. But there's a lot of good

520
00:33:07,552 --> 00:33:11,466
options and don't put them publicly unless you are specifically trying to

521
00:33:11,488 --> 00:33:14,986
just gather ip addresses from people scanning you. And there's

522
00:33:15,018 --> 00:33:18,286
not a lot of use to that in all reality, because again, your job should

523
00:33:18,308 --> 00:33:21,738
be to defend, not to actively go after people that are scraping

524
00:33:21,754 --> 00:33:24,650
the Internet. Because there's a lot of people scraping the Internet.

525
00:33:24,810 --> 00:33:28,654
Anyway, I've been Dwayne, I live in Chicago. I've been a developer advocate

526
00:33:28,702 --> 00:33:32,722
since 2016. Check out the security repo podcast. Highly recommend it.

527
00:33:32,776 --> 00:33:36,174
We have a lot of great guests and talk about a very wide

528
00:33:36,222 --> 00:33:39,726
range of security subjects. Hit me up on the Internet, McDwayne.

529
00:33:39,758 --> 00:33:43,126
Pretty much everywhere, including GitHub. And if you ever want

530
00:33:43,128 --> 00:33:47,578
to hit me up about anything, you can reach out to me at dwayne mcDaniel@getguardian.com.

531
00:33:47,664 --> 00:33:51,130
Thank you so much for listening. Had a blast making this and

532
00:33:51,200 --> 00:33:54,106
I hope that everybody has a great rest of your day and enjoys all the

533
00:33:54,128 --> 00:33:57,930
amazing content here at comp 42.

534
00:33:58,000 --> 00:33:58,280
Thank you.

