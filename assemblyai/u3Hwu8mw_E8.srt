1
00:00:00,250 --> 00:00:04,830
Are you an SRe? A developer,

2
00:00:06,610 --> 00:00:10,014
a quality engineer who wants to tackle the challenge of

3
00:00:10,052 --> 00:00:14,026
improving reliability in your DevOps? You can enable your DevOps

4
00:00:14,058 --> 00:00:17,614
for reliability with chaos native. Create your

5
00:00:17,652 --> 00:01:17,046
free account at Chaos native litmus cloud hello,

6
00:01:17,148 --> 00:01:21,394
my name is Ajuna Kyaruzi. I'm a technical evangelist at Datadog

7
00:01:21,522 --> 00:01:24,694
and I'll be talking to you about sustainable instant management for

8
00:01:24,732 --> 00:01:28,294
happy SRE teams today. So I know the topic sounds

9
00:01:28,332 --> 00:01:31,574
a little vague, but hopefully at the end of this you'll be thinking about instant

10
00:01:31,622 --> 00:01:34,954
management in a way that makes your instant responders or

11
00:01:34,992 --> 00:01:38,410
your SRE team specifically as

12
00:01:38,480 --> 00:01:41,902
fulfilled as possible so that they're not burnt out and are able to do

13
00:01:41,956 --> 00:01:44,830
instant management and instant response sustainably.

14
00:01:45,250 --> 00:01:49,006
So let's start thinking about this from something that's relatable to

15
00:01:49,028 --> 00:01:52,766
all of us. Your pager goes off. It can

16
00:01:52,788 --> 00:01:55,882
mean a lot of different things, but the first thing is, oh my gosh,

17
00:01:55,946 --> 00:01:59,054
what's going on? How can I resolve this? Let me find out what's

18
00:01:59,102 --> 00:02:02,338
happening. You could have been interrupted in lots of different ways.

19
00:02:02,504 --> 00:02:06,366
Maybe you were just doing your work. Maybe you were at your kid's birthday

20
00:02:06,398 --> 00:02:09,750
party and now have to step away or you were having brunch with friends.

21
00:02:09,900 --> 00:02:13,446
Either way, your immediate response is, I got to figure out what's going on and

22
00:02:13,468 --> 00:02:18,614
solve it as quickly as possible. The process that you take to

23
00:02:18,652 --> 00:02:22,054
solve what's going on and to figure out what's happening is what we call

24
00:02:22,092 --> 00:02:26,090
instant management. We can think about it from the process

25
00:02:26,160 --> 00:02:29,242
of being paged now that we know things are going wrong,

26
00:02:29,376 --> 00:02:32,000
looking at all the alerts that triggered what happened,

27
00:02:33,090 --> 00:02:35,310
looking for the root cause of the incident,

28
00:02:35,650 --> 00:02:38,670
mitigating it to reduce the customer impact,

29
00:02:39,010 --> 00:02:42,686
launching a new change or rolling back the reason that the

30
00:02:42,788 --> 00:02:46,322
incident happened and then reviewing it,

31
00:02:46,376 --> 00:02:50,082
making sure that things are back to normal, writing a post

32
00:02:50,136 --> 00:02:54,194
mortem and trying to make sure that that incident doesn't happen again.

33
00:02:54,392 --> 00:02:58,106
All in all, we call this process incident management.

34
00:02:58,238 --> 00:03:01,430
But what do I mean by making this sustainable?

35
00:03:01,850 --> 00:03:05,414
For something to be sustainable, let's look at the definition of what that

36
00:03:05,452 --> 00:03:09,194
means. And generally it's a process that can be reused or done again and

37
00:03:09,232 --> 00:03:13,210
again without depleting the natural resources.

38
00:03:13,550 --> 00:03:16,906
The resources that we're discussing right now are the

39
00:03:16,928 --> 00:03:20,186
people who are responders to your incidents. How do we make sure

40
00:03:20,208 --> 00:03:23,040
that they're able to continue the work that they're doing?

41
00:03:23,810 --> 00:03:27,840
Despite the fact that it is a lot of stress to be on call,

42
00:03:28,450 --> 00:03:32,398
it's hard to really track and measure what

43
00:03:32,484 --> 00:03:36,762
the stress or the happiness of someone who is an instant responders.

44
00:03:36,906 --> 00:03:40,386
Some of our other fields do a much better job of this. But how do

45
00:03:40,408 --> 00:03:43,474
we know, other than whether people are leaving the team, how do we know whether

46
00:03:43,512 --> 00:03:46,834
or not they're happy? There are lots of different things that we can think about

47
00:03:46,872 --> 00:03:49,766
and the values that we want to make sure that our team are able to

48
00:03:49,788 --> 00:03:53,670
do. But in general, we can look at the pain points that come up with

49
00:03:53,740 --> 00:03:57,414
instant management and look at ways that we can reduce them or make

50
00:03:57,452 --> 00:04:01,174
them as small as possible so that this

51
00:04:01,212 --> 00:04:05,450
process of instant management is sustainable. So first, let's just

52
00:04:05,600 --> 00:04:09,498
think about being on call and the general incident management process.

53
00:04:09,664 --> 00:04:13,710
When your pager goes off, usually you've become the incident commander.

54
00:04:15,810 --> 00:04:19,454
You've declared an incident, you're in charge of it. Usually it's the lead

55
00:04:19,492 --> 00:04:23,198
person involved in the incident. So when you're on call,

56
00:04:23,364 --> 00:04:26,834
you're working through the incident. You might pull in a few other folks to work

57
00:04:26,872 --> 00:04:30,846
with you, these people, or they might become the incident commander

58
00:04:30,878 --> 00:04:34,706
if you need to be the one responsible for resolving it. But these people you

59
00:04:34,728 --> 00:04:38,574
are working together, coming up with, following the instant command

60
00:04:38,622 --> 00:04:41,766
hierarchy or system that

61
00:04:41,788 --> 00:04:44,934
a lot of these companies follow, can be really helpful in making sure that

62
00:04:44,972 --> 00:04:48,534
communication is working freely so that the folks who need to work

63
00:04:48,572 --> 00:04:51,974
on resolving the instant can focus on that, while the people handling

64
00:04:52,022 --> 00:04:56,234
the process and administration and coordination around

65
00:04:56,272 --> 00:04:59,754
the instant, aka the instant commander, can focus on that and

66
00:04:59,792 --> 00:05:03,506
be the main communication for the stakeholders

67
00:05:03,558 --> 00:05:06,622
involved in this process. A point where

68
00:05:06,676 --> 00:05:10,222
things can get really hard is just the process of being on call.

69
00:05:10,356 --> 00:05:14,042
I know that we're all on call at some point, especially on SRE

70
00:05:14,106 --> 00:05:16,580
teams, but what does that look like?

71
00:05:17,750 --> 00:05:21,474
Especially on SRE teams where you're on call for a service that maybe you

72
00:05:21,512 --> 00:05:25,314
didn't write or just you're on call for multiple different services, finding ways

73
00:05:25,352 --> 00:05:29,110
to generalize this process so that it's as smooth as possible

74
00:05:29,180 --> 00:05:33,000
can be great. Having the same technology that

75
00:05:33,530 --> 00:05:36,774
reports on issues that are happening to make it easier to find out

76
00:05:36,812 --> 00:05:39,660
what went wrong, or finding out,

77
00:05:40,350 --> 00:05:43,914
having a general production system for launching new

78
00:05:43,952 --> 00:05:47,734
features so that rollbacks can happen make this process a lot easier

79
00:05:47,782 --> 00:05:51,802
to do. But in general, when you're on call, finding out ways

80
00:05:51,856 --> 00:05:55,374
to make that process as unintrusive to life can make it

81
00:05:55,412 --> 00:05:58,446
more sustainable. A lot of teams have 24 hours,

82
00:05:58,548 --> 00:06:02,094
24/7 oncall cycles where one person's on call

83
00:06:02,132 --> 00:06:05,534
for a whole week and then they sort of swap off. Sometimes you're on

84
00:06:05,572 --> 00:06:08,978
call by yourself for a while, especially if you don't have a lot

85
00:06:08,984 --> 00:06:12,386
of folks on your team finding out ways to sort of cut down

86
00:06:12,408 --> 00:06:15,346
the time that you're on call so it's not interfering in the rest of your

87
00:06:15,368 --> 00:06:19,010
life can make it more sustainable for the instant responders.

88
00:06:19,510 --> 00:06:23,234
Twelve hour on call cycles sre the most ideal, but that might be easier

89
00:06:23,282 --> 00:06:26,454
if you have a team that's in another time zone where you can sort of

90
00:06:26,492 --> 00:06:30,034
swap off when the sun goes down, for example. But generally

91
00:06:30,082 --> 00:06:33,866
finding ways where you can have maybe multiple tiers of responders so

92
00:06:33,888 --> 00:06:37,510
folks can lean on each other if they need to go on the subway

93
00:06:37,590 --> 00:06:40,986
or take a walk because they've just been tied to their laptop all day.

94
00:06:41,088 --> 00:06:44,174
Make it a lot easier for folks to not feel like they're on call by

95
00:06:44,212 --> 00:06:48,494
themselves and are the sole person responsible for a system in

96
00:06:48,532 --> 00:06:51,920
general, just having these tiers can make it a lot easier if

97
00:06:52,450 --> 00:06:56,194
people are unable to respond because of an emergency. So the next person on

98
00:06:56,232 --> 00:06:59,982
call, the secondary or the tertiary on call, can take over temporarily,

99
00:07:00,046 --> 00:07:03,214
and then all these people can be working together to be responsible

100
00:07:03,262 --> 00:07:06,990
for resolving can incident. Something else that we can think about to

101
00:07:07,000 --> 00:07:10,002
make things sustainable is the idea of instant severities.

102
00:07:10,146 --> 00:07:13,462
Lots of different companies have an idea of instant severities where

103
00:07:13,516 --> 00:07:16,310
maybe a sev five is something that's pretty minor.

104
00:07:17,450 --> 00:07:21,186
Your app's just running a little slower than normal. Maybe you're getting more traffic,

105
00:07:21,218 --> 00:07:24,826
but it's not anything noticeable to your end users all

106
00:07:24,848 --> 00:07:27,674
the way up to a step two or step one where you might actively be

107
00:07:27,712 --> 00:07:31,146
losing money or losing customers because they're unable to access your

108
00:07:31,168 --> 00:07:35,066
service. The benefit of something like instant severity

109
00:07:35,178 --> 00:07:38,286
and how it helps things become a little more

110
00:07:38,308 --> 00:07:41,758
sustainable is it shows you a lot of things that are going on.

111
00:07:41,924 --> 00:07:45,806
One, something that's more severe of an incident means it just needs more

112
00:07:45,828 --> 00:07:49,426
resources. It's a clear indication to pull in folks who might have

113
00:07:49,448 --> 00:07:52,546
more expertise. A lot of companies have where if

114
00:07:52,568 --> 00:07:55,854
you have a Sev two or a SeV one, you immediately

115
00:07:55,902 --> 00:07:59,526
pull teams that are much better at handling the larger scale of

116
00:07:59,548 --> 00:08:03,510
the incident, from the communication that need to be involved with other

117
00:08:03,580 --> 00:08:07,394
end users and stakeholders, to just the larger scale

118
00:08:07,442 --> 00:08:10,826
coordination that happens to all the different responders that need

119
00:08:10,848 --> 00:08:14,650
to be pulled in for the larger scale of the incident.

120
00:08:15,550 --> 00:08:19,066
Another reason why severities can be very

121
00:08:19,088 --> 00:08:22,874
helpful is just the process of escalation means that

122
00:08:23,072 --> 00:08:26,622
you are asking core people for help. Even if it's another

123
00:08:26,676 --> 00:08:30,046
person on your team, it makes it a lot easier for you to

124
00:08:30,068 --> 00:08:33,666
know that, hey, this has become a larger task and I need to split it

125
00:08:33,688 --> 00:08:37,426
up a little bit. Escalating and having

126
00:08:37,448 --> 00:08:40,818
can environment where it's easy to escalate makes a process

127
00:08:40,904 --> 00:08:45,170
where the team feels supported to know what's going on and

128
00:08:45,320 --> 00:08:49,206
how much they can help. And even just increasing the severity might mean someone from

129
00:08:49,228 --> 00:08:52,374
a different team knows to reach out to you if they think

130
00:08:52,412 --> 00:08:56,006
something that's related involves them. I'd also like to talk

131
00:08:56,028 --> 00:08:59,734
about the ramp up process when you're joining a new team or when

132
00:08:59,772 --> 00:09:03,642
you're becoming on call. For a team, the onboarding of being a

133
00:09:03,696 --> 00:09:07,306
new instant responder can be an area of a lot of stress because all of

134
00:09:07,328 --> 00:09:10,810
a sudden you're now one of the people responsible for a system,

135
00:09:10,880 --> 00:09:14,526
and maybe if you're new, you don't know anything about it. Trying to

136
00:09:14,548 --> 00:09:18,746
make this process as easy as possible for folks can really help increase the sustainability

137
00:09:18,858 --> 00:09:22,318
and help them see themselves grow within the system. So a lot

138
00:09:22,324 --> 00:09:26,026
of different ways to think about ramping up folks is of course shadowing

139
00:09:26,058 --> 00:09:29,666
and reverse shadowing the person who's actually on call. Getting an

140
00:09:29,688 --> 00:09:33,262
opportunity to practice being on call without just the sole responsibility

141
00:09:33,326 --> 00:09:36,386
of it can be a great relief to a lot of people, and even the

142
00:09:36,408 --> 00:09:39,590
person on call has an opportunity to collaborate with someone new.

143
00:09:39,740 --> 00:09:43,366
Finding ways to make the onboarding processes as easy as possible means that

144
00:09:43,388 --> 00:09:46,886
generally the incident management process gets easier, whether it's the

145
00:09:46,908 --> 00:09:50,458
training on incident management. So everyone knows that they're on the same page when it

146
00:09:50,464 --> 00:09:54,458
comes to the terminology that they're using and how to split tasks and

147
00:09:54,464 --> 00:09:57,594
what to do when you are on call or helping out someone who

148
00:09:57,632 --> 00:10:01,274
is, to different ways that you can

149
00:10:01,472 --> 00:10:04,850
handle common incidents like having runbooks

150
00:10:04,870 --> 00:10:09,066
or playbooks, for things like what to do when you run out of quota.

151
00:10:09,178 --> 00:10:12,462
Hopefully you have as much automation as you can for some of these things,

152
00:10:12,516 --> 00:10:16,250
but other ones you do need human interaction to find out

153
00:10:16,340 --> 00:10:20,078
what made you run out of quota. But when you do, these are the steps

154
00:10:20,094 --> 00:10:23,666
that you have to take every time. So having these opportunities for

155
00:10:23,688 --> 00:10:27,086
folks to kind of get quick wins

156
00:10:27,118 --> 00:10:30,854
and also have their opportunity to fix something, because that's one of the exciting things

157
00:10:30,892 --> 00:10:34,758
about being on call, being the person that fixed the thing that went wrong,

158
00:10:34,924 --> 00:10:38,242
making it as easy as possible to get to that level of achievement

159
00:10:38,306 --> 00:10:42,202
makes it a lot easier for folks to feel fulfilled from instant management and

160
00:10:42,256 --> 00:10:45,466
continue on on the process. Another thing

161
00:10:45,488 --> 00:10:48,998
that you can think about is just the idea of practicing, especially for newer

162
00:10:49,014 --> 00:10:52,782
folks, but literally for everyone on the team. Learning about

163
00:10:52,836 --> 00:10:56,602
the instant command system I talked about earlier and having common terminology

164
00:10:56,666 --> 00:10:59,450
on how to resolve incidents can be really helpful.

165
00:10:59,610 --> 00:11:03,950
So just doing an instant response training together, or various

166
00:11:04,030 --> 00:11:08,254
things like game days or disaster and recovery

167
00:11:08,302 --> 00:11:12,162
training like what Google does, or just role playing different

168
00:11:12,216 --> 00:11:16,274
incidents and figuring out what went wrong can be really helpful. Something that

169
00:11:16,312 --> 00:11:19,458
can really assist here is just reading old postmortems,

170
00:11:19,474 --> 00:11:23,186
which I'll talk about a little more later. Another area that's

171
00:11:23,218 --> 00:11:27,270
a huge pain point for people is just communication. During incidents.

172
00:11:27,850 --> 00:11:31,046
We want to think about how to make it as easy as possible to find

173
00:11:31,068 --> 00:11:34,906
out what's going on in can incident when you're ramping up new folks who are

174
00:11:34,928 --> 00:11:38,154
going to be joining the incident, and maybe even when you're handing the pager over

175
00:11:38,192 --> 00:11:41,834
to the next incident commander, what do you need to do to

176
00:11:41,952 --> 00:11:44,846
get up to date information on what's going on?

177
00:11:45,028 --> 00:11:48,634
Having a unified channel of information makes this a lot easier.

178
00:11:48,682 --> 00:11:52,126
Whether it's a Slack channel, an IRC channel,

179
00:11:52,228 --> 00:11:55,994
whatever you want to use, where folks who want to get updates on the instant

180
00:11:56,042 --> 00:11:59,346
can just go without interrupting anyone who's resolving it to

181
00:11:59,368 --> 00:12:02,306
find out up to date details on what's been done and what needs to be

182
00:12:02,328 --> 00:12:06,130
done on what's left to do makes it a lot easier for folks.

183
00:12:06,710 --> 00:12:10,054
Alternatively, when you're later looking back on everything that

184
00:12:10,092 --> 00:12:13,238
happened for a review, it's all in one place where you

185
00:12:13,244 --> 00:12:17,046
can find everything and the finer details that happened. If you have

186
00:12:17,068 --> 00:12:21,082
a larger scale instant, maybe it makes sense to even have multiple channels, but still

187
00:12:21,136 --> 00:12:24,730
making it as narrow as possible where folks who

188
00:12:24,800 --> 00:12:28,422
are stakeholders and want communications

189
00:12:28,486 --> 00:12:32,266
that are going out even to the end. Customers can be all in one place

190
00:12:32,368 --> 00:12:35,982
and a channel for the people who are responding and answering each other what's happening

191
00:12:36,036 --> 00:12:39,694
in another, but generally making sure that all of this in a place where

192
00:12:39,732 --> 00:12:43,326
it's not in a direct message to

193
00:12:43,348 --> 00:12:47,166
someone else, so that everyone who's working on the instant gets an update

194
00:12:47,198 --> 00:12:49,570
on what's going on makes it a lot easier.

195
00:12:49,910 --> 00:12:54,366
Lastly, an area that's really important for instant management is automation,

196
00:12:54,558 --> 00:12:58,930
especially for the postmortems process, the review or

197
00:12:59,000 --> 00:13:02,230
the document that you write at the end of the incident to find out everything

198
00:13:02,300 --> 00:13:05,686
that happened. Automating this process makes it a lot easier for

199
00:13:05,708 --> 00:13:09,174
the incident responders so they don't have to go back and remember all

200
00:13:09,212 --> 00:13:12,370
the details that happened in the timeline. For the different instance.

201
00:13:12,530 --> 00:13:16,802
Using a tool like Datadog, we sre able to automatically

202
00:13:16,866 --> 00:13:20,640
create a draft postmortem for you when

203
00:13:22,210 --> 00:13:25,482
your incident is resolved. By pulling in all the different metrics,

204
00:13:25,546 --> 00:13:29,214
logs, dashboards and even linking to

205
00:13:29,252 --> 00:13:32,798
your different slack channels where you were communicating with folks to

206
00:13:32,884 --> 00:13:36,174
get can accurate incident timeline where you can then instead

207
00:13:36,212 --> 00:13:39,938
of having to pull that information in yourself, just edit it so that it looks

208
00:13:40,024 --> 00:13:43,842
great and matches what happened. You can even include the tasks that people

209
00:13:43,896 --> 00:13:47,654
did so that you know what has already been done and what maybe needs

210
00:13:47,692 --> 00:13:51,366
to be pushed to production rather than

211
00:13:51,388 --> 00:13:54,966
just a quick fix or future tasks I need to do for remediation of the

212
00:13:54,988 --> 00:13:58,486
incident automating as much as you can of this process can

213
00:13:58,508 --> 00:14:02,018
really alleviate a lot of the pain point of writing a post mortem,

214
00:14:02,114 --> 00:14:05,338
because for a lot of incident responders, it's yet another thing they have

215
00:14:05,344 --> 00:14:08,826
to do after the incident is over and they want to just go back to

216
00:14:08,848 --> 00:14:12,874
what they were doing before they were interrupted. Automating this process can make

217
00:14:12,912 --> 00:14:16,570
life a lot easier where you can pull and create an incident timeline,

218
00:14:16,650 --> 00:14:19,882
figure out exactly when wrong, and also include all the remediation

219
00:14:19,946 --> 00:14:23,758
tasks afterwards that need to be done to solidify all the work that

220
00:14:23,764 --> 00:14:26,400
you've done so that the incident doesn't happen again.

221
00:14:26,930 --> 00:14:29,742
So these are the few pain points that I wanted to talk to you about

222
00:14:29,796 --> 00:14:33,374
about making incident management more sustainable. If you think of a few more,

223
00:14:33,412 --> 00:14:36,774
please reach out to me. I'd love to chat more about this and answer any

224
00:14:36,812 --> 00:14:40,310
questions, and also just continue the conversation about

225
00:14:40,380 --> 00:14:43,140
making instant management more sustainable. Thank you.

