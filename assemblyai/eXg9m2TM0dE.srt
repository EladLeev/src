1
00:00:26,130 --> 00:00:28,758
Hello everybody, and welcome to this talk in which we're going to learn how to

2
00:00:28,764 --> 00:00:32,626
build a realtime realtime realtime realtime realtime realtime realtime analytics

3
00:00:32,658 --> 00:00:36,502
Dashboard streamlit Apache Pinot let's first start by defining real

4
00:00:36,556 --> 00:00:40,054
time analytics. So Gartner has

5
00:00:40,092 --> 00:00:44,130
this quite nice definition where they say that real time analytics

6
00:00:44,210 --> 00:00:48,102
is the discipline that applies logic and mathematics to data to provide insights for making

7
00:00:48,156 --> 00:00:51,358
better decisions quickly. And so I've highlighted the

8
00:00:51,364 --> 00:00:54,574
key bit here, which is that we want to make decisions quickly. That's the idea.

9
00:00:54,612 --> 00:00:57,646
The data is coming in and we want to quickly be able to make some

10
00:00:57,668 --> 00:01:01,406
sort of actionable insight based on that data. They further divide it down

11
00:01:01,428 --> 00:01:05,102
into on demand and continuous analytics, where on demand is

12
00:01:05,236 --> 00:01:08,946
where the user makes a query and gets some data back and continuous is where

13
00:01:08,968 --> 00:01:12,226
maybe the data gets pushed to you. So we're going to be focusing on the

14
00:01:12,248 --> 00:01:16,046
on demand part of that. So that's real time analytics.

15
00:01:16,078 --> 00:01:19,566
But what about real time user facing analytics? So the extra bit here

16
00:01:19,608 --> 00:01:23,346
is the idea that maybe we've always had these analytical queries

17
00:01:23,378 --> 00:01:26,294
capabilities, but we want to give them to the end users rather than just having

18
00:01:26,332 --> 00:01:28,230
them for our internal users.

19
00:01:29,850 --> 00:01:33,334
We want to give our end users analytical querying capabilities on fresh data.

20
00:01:33,372 --> 00:01:36,074
So we want to get the data in quickly and be able to query it.

21
00:01:36,192 --> 00:01:39,222
And in the bottom right hand side of the slide, you can see some examples

22
00:01:39,366 --> 00:01:42,526
of the types of applications that we might use where we might

23
00:01:42,548 --> 00:01:45,966
want to be able to do this. We can further break

24
00:01:45,988 --> 00:01:50,126
it down like this. So the real time part

25
00:01:50,148 --> 00:01:53,662
of this definition means the data should be available for querying as soon as possible.

26
00:01:53,716 --> 00:01:56,718
So the data comes in often from a real time stream. We want to be

27
00:01:56,724 --> 00:01:59,834
able to query it as quickly as we can. So we want very low latency

28
00:01:59,882 --> 00:02:03,418
between for the data ingestion. And then on the user

29
00:02:03,434 --> 00:02:07,942
facing side, we want to be able to write queries that are similar in

30
00:02:08,076 --> 00:02:11,350
query response time to what we'd get with an OLAP database. So I e.

31
00:02:11,420 --> 00:02:15,046
It should be in milliseconds, like how quickly the queries come back. And then in

32
00:02:15,068 --> 00:02:18,546
addition to that, lots of users are going to be executing those queries

33
00:02:18,578 --> 00:02:20,934
at the same time. So we need to be able to handle high throughput,

34
00:02:20,982 --> 00:02:24,060
low latency queries on very fresh data.

35
00:02:24,750 --> 00:02:28,426
And let's have a look at some examples of places where those type of

36
00:02:28,448 --> 00:02:32,214
dashboards have been built. So weve got one example here. So this is the Uber

37
00:02:32,262 --> 00:02:35,774
eats. So imagine we're running a restaurant on Uber eats. So in the middle

38
00:02:35,812 --> 00:02:39,182
of the screen we can see there are some examples of the data

39
00:02:39,236 --> 00:02:42,558
of what's happened in the last twelve weeks, like how much money have

40
00:02:42,564 --> 00:02:45,946
we been making? Can we see changes in what's

41
00:02:45,978 --> 00:02:48,946
happened over the last week? And then on the right hand side are some things

42
00:02:48,968 --> 00:02:51,774
that might need our attention. So we can see there are some missed orders,

43
00:02:51,822 --> 00:02:55,186
there's some inaccurate orders, there's some downtime, and those are like places where we

44
00:02:55,208 --> 00:02:57,666
might want to be able to go and do something like, and do it now.

45
00:02:57,768 --> 00:03:01,366
So it's allowing us to see in real time there's something that

46
00:03:01,388 --> 00:03:04,040
you need to do that might be able to fix a problem for somebody.

47
00:03:05,130 --> 00:03:08,726
LinkedIn is another good example of where these

48
00:03:08,748 --> 00:03:11,766
types of dashboards have been built. So we've got three different ones on here.

49
00:03:11,788 --> 00:03:14,858
So on the left hand side we've got a user facing one. So that might

50
00:03:14,864 --> 00:03:18,618
be for you and me using LinkedIn. So for example, we might want

51
00:03:18,624 --> 00:03:21,498
to see who's viewed our profile. So it sort of shows the traffic to our

52
00:03:21,504 --> 00:03:25,182
profile over the last few months. And then if you

53
00:03:25,236 --> 00:03:28,286
remember this page, we can also see who's been viewing it. And where do they

54
00:03:28,308 --> 00:03:32,234
work for? Over in the middle we've got a recruiter

55
00:03:32,282 --> 00:03:36,046
dashboard. So recruiters are trying to find people to fill the jobs that they have

56
00:03:36,068 --> 00:03:38,866
available. And so this is kind of giving them like a view of all the

57
00:03:38,888 --> 00:03:41,822
data that's there to try and help them work out where they might best target

58
00:03:41,886 --> 00:03:45,566
those jobs. And then finally, on the right hand side is more of an internal

59
00:03:45,598 --> 00:03:48,714
product or bi tool. So this is capturing metrics

60
00:03:48,862 --> 00:03:51,894
and we want to be able to see in real time what's happening. So maybe

61
00:03:51,932 --> 00:03:55,750
this is a tool that's being used by a product manager.

62
00:03:56,650 --> 00:03:59,986
Okay, so that's the overview of what a realtime, realtime, realtime, realtime,

63
00:04:00,018 --> 00:04:03,658
realtime analytics dashboard, streamlit, Apache, Pinot, try and build our own one. And we're going

64
00:04:03,664 --> 00:04:06,934
to be using three tools to do this. So we're going to be using Streamlit,

65
00:04:06,982 --> 00:04:10,714
Pinot and Kafka. So let's have a quick overview of what

66
00:04:10,752 --> 00:04:13,866
each of those tools are. So, Streamlit, I first came across it a couple of

67
00:04:13,888 --> 00:04:17,454
years ago, so it's sort of beginning of 2020. So it's a Python based web

68
00:04:17,492 --> 00:04:21,040
application framework, and the idea is that it should be used to build

69
00:04:21,890 --> 00:04:25,694
data apps, often machine learning apps, and it integrates

70
00:04:25,742 --> 00:04:28,926
with lots of different Python libraries. So plotly, Pytorch, Tensorflow,

71
00:04:28,958 --> 00:04:32,766
Scikitlearn. If you have a Python client driver for your database,

72
00:04:32,798 --> 00:04:36,318
it'll probably integrate with that as well. And if you want to share your apps

73
00:04:36,334 --> 00:04:40,130
with anyone else they have a thing called the Streamlit

74
00:04:40,210 --> 00:04:43,458
cloud that you can use to do that. Apache Pinot

75
00:04:43,634 --> 00:04:46,898
is next up. So that's a columnar OLAP database,

76
00:04:46,914 --> 00:04:51,126
but it also has additional indexing indexes that you can put on top of the

77
00:04:51,148 --> 00:04:54,886
default columns and it's all queryable by SQL.

78
00:04:54,918 --> 00:04:59,062
And the idea here is that it's used for real time low latency analytics

79
00:04:59,206 --> 00:05:03,022
and these finally we've got Apache Kafka, a distributed event

80
00:05:03,076 --> 00:05:06,510
streaming platform. It uses a publish and subscribe

81
00:05:06,930 --> 00:05:10,830
sort of setup for two streams of records

82
00:05:11,890 --> 00:05:15,310
and those records are then stored in a fault tolerant way and we can process

83
00:05:15,380 --> 00:05:18,706
them as they occur. So how are we going to

84
00:05:18,728 --> 00:05:22,034
glue all these components together? So were going to start in the top left hand

85
00:05:22,072 --> 00:05:25,234
corner. So were going to have some sort of streamlit API. So some events coming

86
00:05:25,272 --> 00:05:29,394
in from a data source, we'll have a Python application processing

87
00:05:29,442 --> 00:05:32,978
those. We'll then load them into Kafka. Pinot will listen to the Kafka

88
00:05:32,994 --> 00:05:36,614
topic and process those events and ingest them into Pinot. And then finally

89
00:05:36,732 --> 00:05:40,454
our streamlit application will query Pinot to

90
00:05:40,492 --> 00:05:43,626
generate some tables or some charts so that we can see what's actually going on

91
00:05:43,648 --> 00:05:47,354
with our data. So the data set that we're going to use is

92
00:05:47,472 --> 00:05:51,246
called the Wikimedia recent changes feed. So this is a continuous streamlit of all

93
00:05:51,268 --> 00:05:55,086
the changes that are being made to various Wikimedia properties. So like for

94
00:05:55,108 --> 00:05:58,746
example Wikipedia and it publishes

95
00:05:58,778 --> 00:06:02,506
the events over HTTP using the server side events protocol.

96
00:06:02,538 --> 00:06:05,566
And you can see in the bottom of the slide an example of what a

97
00:06:05,588 --> 00:06:07,758
message looks like. So you can see weve got an event, it says it's a

98
00:06:07,764 --> 00:06:10,202
message, weve got an id and we've got a bunch of data. So it indicates

99
00:06:10,266 --> 00:06:13,530
like the title of the page, it's got the Uri of these page, the user,

100
00:06:13,610 --> 00:06:16,920
what type of change they made when these made it and so on.

101
00:06:17,690 --> 00:06:20,982
So what we're going to do now is we're going to move onto our demo.

102
00:06:21,036 --> 00:06:24,166
So we'll go into visual studio here and what you can see

103
00:06:24,188 --> 00:06:27,398
on the screen at the moment is a Docker compose file that we're going to

104
00:06:27,404 --> 00:06:31,158
use to spin up all those components on our machine. So first up we've

105
00:06:31,174 --> 00:06:34,714
got Zookeeper. So this is used by both Pinot and Kafka to manage their

106
00:06:34,752 --> 00:06:37,914
metadata. Next one down is

107
00:06:37,952 --> 00:06:41,754
of course Kafka. So that's where the stream of data is

108
00:06:41,792 --> 00:06:44,886
going to be it connects to zookeeper and then it also

109
00:06:44,928 --> 00:06:48,446
indicates a couple of ports where we can access it from Pinot and then from

110
00:06:48,468 --> 00:06:51,726
our Python script. And then finally on the last three bits we've got

111
00:06:51,748 --> 00:06:55,562
the various Pinot components. So we've got the Pinot controller.

112
00:06:55,626 --> 00:06:59,266
That's what manages the pinot cluster and sort of

113
00:06:59,288 --> 00:07:02,846
takes care of all the metadata for us. We've got the pinot broker.

114
00:07:02,878 --> 00:07:06,066
So that's where the queries get sent to. And then it then sends them out

115
00:07:06,088 --> 00:07:08,914
to the different servers. In this case we've only got one server, but in a

116
00:07:08,952 --> 00:07:12,466
production setup you'd have many more and it would get those results

117
00:07:12,498 --> 00:07:15,906
back and then return them to the client. And then finally we've got the Pinot

118
00:07:15,938 --> 00:07:19,786
servers themselves. So this is who stores the data

119
00:07:19,888 --> 00:07:23,926
and processes the queries. So I'm just going to open up a terminal window

120
00:07:23,958 --> 00:07:27,162
here and we'll run the

121
00:07:27,216 --> 00:07:30,746
docker compose script, so docker compose up and that

122
00:07:30,768 --> 00:07:35,930
will then spin up all of these containers

123
00:07:36,090 --> 00:07:39,946
on our machine. So we'll just minimize that for the moment. And we're

124
00:07:39,978 --> 00:07:43,614
going to navigate over to this wiki Py. So this is the

125
00:07:43,812 --> 00:07:47,922
script that we're going to use to get the data, the event streamlit from

126
00:07:47,976 --> 00:07:51,470
Wikimedia. So you can see were on lines ten to 13.

127
00:07:51,550 --> 00:07:55,042
We've got the Uri with the recent changes. We're then

128
00:07:55,096 --> 00:07:57,858
going to call it with the request library and then we're going to wrap it

129
00:07:57,864 --> 00:08:00,946
in this SSE client. So you can see here, this is a server side events

130
00:08:00,978 --> 00:08:04,550
client for Python and it then gives us back this events function.

131
00:08:04,620 --> 00:08:07,846
It just has a stream of all these events. So if we open up the

132
00:08:07,868 --> 00:08:12,066
terminal again and we'll just open a new tab and if we call Python

133
00:08:12,258 --> 00:08:16,086
wiki py, we'll see we get like a stream of all those events.

134
00:08:16,118 --> 00:08:19,546
You can see loads and loads of events coming through. We'll just quickly stop that.

135
00:08:19,568 --> 00:08:21,574
And if you have a look at one of the events, if we just highlight

136
00:08:21,622 --> 00:08:24,874
one down here, you can see it has exactly, very,

137
00:08:24,912 --> 00:08:27,674
very similar to what we saw on the slides. Weve got the schema, it indicates

138
00:08:27,722 --> 00:08:31,406
if it's a bot. We've got the timestamp, we've got the title of the

139
00:08:31,428 --> 00:08:35,246
page and a bunch of other metadata as well. Okay, so that

140
00:08:35,268 --> 00:08:37,938
was the top, that was the top left of our diagram. So we've got the

141
00:08:37,944 --> 00:08:41,294
data from our event stream into our python

142
00:08:41,342 --> 00:08:44,500
application. Now we need to get it across into Kafka. So that's our

143
00:08:44,870 --> 00:08:48,306
next bit. So that's Wiki to Kafka. So the

144
00:08:48,328 --> 00:08:51,702
first bits are the same. So we've got the same code

145
00:08:51,756 --> 00:08:55,206
getting the data from the source. We've now got a

146
00:08:55,228 --> 00:08:58,226
while true loop. So this handles like if we lose the connection to the recent

147
00:08:58,258 --> 00:09:01,554
changes, it will just reconnect and then carry on processing

148
00:09:01,602 --> 00:09:05,206
events. The extra bit we've got is here on line 39, where we're adding

149
00:09:05,238 --> 00:09:09,274
an event to Kafka. And so producer is defined up here on line 25.

150
00:09:09,312 --> 00:09:12,630
And that's a Kafka producer for putting data onto a

151
00:09:12,640 --> 00:09:16,858
Kafka topic. In this case, the topic is Wikipedia events.

152
00:09:17,034 --> 00:09:20,206
And then the last interesting thing is on line 41 down to

153
00:09:20,228 --> 00:09:24,830
44. Every hundred messages, we're going to flush those messages into Kafka.

154
00:09:25,330 --> 00:09:27,780
So let's just get our terminal back again.

155
00:09:28,230 --> 00:09:31,890
And instead of calling that wiki one, we'll call Wiki to Kafka.

156
00:09:32,710 --> 00:09:35,566
And so if we run that, the output will just be every hundred messages.

157
00:09:35,598 --> 00:09:38,958
It's going to say I've flushed hundred of messages. I've flushed another hundred messages.

158
00:09:38,974 --> 00:09:41,974
So you can kind of see those are going in nicely. If we want to

159
00:09:42,012 --> 00:09:45,334
check that they're making their way into Kafka, we can

160
00:09:45,372 --> 00:09:48,946
run this command here. So this is going to run call the Kafka console

161
00:09:48,978 --> 00:09:53,002
consumer, connect to the Kafka running on localhost 1992,

162
00:09:53,056 --> 00:09:56,502
this topic, and then get all the messages starting from the beginning.

163
00:09:56,646 --> 00:09:59,978
So if we open ourselves up another tab here, let's just have a

164
00:09:59,984 --> 00:10:03,178
quick check that the data is making its way into Kafka. So these we go,

165
00:10:03,184 --> 00:10:05,998
you can see all the messages flying through there. And if we kill it,

166
00:10:06,004 --> 00:10:09,550
it says, hey, I've processed 922 messages. And that was

167
00:10:09,620 --> 00:10:12,974
because we killed it. There are probably more of them in there now. And again

168
00:10:13,012 --> 00:10:16,494
we can see it's a JSOn message. It's exactly the same message

169
00:10:16,612 --> 00:10:19,966
that we had before. We've just put it straight onto Kafka. Okay, so so far

170
00:10:19,988 --> 00:10:22,526
we've got to the middle of our architecture diagram. So we took the data from

171
00:10:22,548 --> 00:10:25,798
the source, we got it into our python application. Originally we printed it to the

172
00:10:25,804 --> 00:10:29,126
screen, but now weve put it into Kafka. So all the events are going into

173
00:10:29,148 --> 00:10:32,226
Kafka. So our next thing is, can we get it into Pinot?

174
00:10:32,258 --> 00:10:35,126
So let's just minimize this for the moment. What we need to do now is

175
00:10:35,148 --> 00:10:38,646
we need to create a pinot table. So that's what this code here will

176
00:10:38,668 --> 00:10:42,086
do. And a pinot table also needs to attach to a schema.

177
00:10:42,118 --> 00:10:46,278
So let's start with that. So what's a schema? So a schema defines

178
00:10:46,454 --> 00:10:49,866
the columns that are going to exist in a table. So our

179
00:10:49,888 --> 00:10:53,846
table is going to be called, our schema is called Wikipedia. We've got some dimension

180
00:10:53,878 --> 00:10:57,706
fields, so these are like the descriptive fields for a table.

181
00:10:57,738 --> 00:11:00,878
So we've got id, we've got wiki, weve got user. These are sort of

182
00:11:01,044 --> 00:11:04,080
all the fields that come out of the JSON message.

183
00:11:05,730 --> 00:11:09,058
We could have metrics, optionally you can have metric fields. So for example, if there

184
00:11:09,064 --> 00:11:12,578
was a count in there, we could have a field that we use for that.

185
00:11:12,584 --> 00:11:15,954
But in this case we don't actually have that, but we do have

186
00:11:15,992 --> 00:11:18,726
a date time field. So you need to specify those separately. So in this case

187
00:11:18,748 --> 00:11:22,034
we've got a timestamp. So we specify that. So that's a schema.

188
00:11:22,162 --> 00:11:25,414
The table, a table is where we store the

189
00:11:25,532 --> 00:11:29,238
records. And columns like that basically store the

190
00:11:29,244 --> 00:11:32,742
data that's going into pinots. They're stored in segments.

191
00:11:32,806 --> 00:11:36,380
So that's why you'll see the word segments being used.

192
00:11:36,750 --> 00:11:39,946
And so we first need to specify the segment config. So were going to say,

193
00:11:39,968 --> 00:11:43,846
okay, where's the schema? If you had a replication

194
00:11:43,878 --> 00:11:46,670
factor, you could specify it here, although we've just set it to one.

195
00:11:46,820 --> 00:11:50,686
And then you need to indicate the time column name if

196
00:11:50,708 --> 00:11:54,058
you're doing a real time table. So real time table basically means I'm

197
00:11:54,074 --> 00:11:56,366
going to be connecting to some sort of real time stream and the data is

198
00:11:56,388 --> 00:11:59,906
going to be coming in all the time. And I need you to process that.

199
00:12:00,008 --> 00:12:03,662
We then need to say, well, where is that data coming from? So we specified

200
00:12:03,726 --> 00:12:07,314
the stream config. So in this case it's a Kafka streamlit coming

201
00:12:07,352 --> 00:12:10,646
from Wikipedia events. And then we need to tell it, where is

202
00:12:10,668 --> 00:12:13,654
our Kafka broker? So it's over here on port 1990,

203
00:12:13,692 --> 00:12:16,934
these. And then we can just say like, well, how should I process those

204
00:12:16,972 --> 00:12:20,834
messages that are coming from Kafka? And then finally the segments

205
00:12:20,962 --> 00:12:23,594
that store the data, how big should they be? So in this case, we've said

206
00:12:23,632 --> 00:12:27,418
they're going to be 1000 records per se. That's obviously

207
00:12:27,584 --> 00:12:30,726
way smaller than what we'd have in a real production setup.

208
00:12:30,758 --> 00:12:33,886
But for the purpose of this demo, it works quite well.

209
00:12:34,068 --> 00:12:37,406
So let's copy this command, let's get our terminal back again,

210
00:12:37,588 --> 00:12:41,454
and we'll create our pinot table. So that

211
00:12:41,492 --> 00:12:44,106
command is going to run, and you can see down here it's been successful.

212
00:12:44,138 --> 00:12:47,538
So the pinot table is ready to go. So now we're going

213
00:12:47,544 --> 00:12:50,980
to go into our web browser so that

214
00:12:51,510 --> 00:12:55,460
we can see what's going on. So we'll just load that up. There we go.

215
00:12:55,990 --> 00:12:59,362
And so this is called the Pinot data Explorer. So it's like a web basic

216
00:12:59,416 --> 00:13:02,406
tool for seeing what's going on in your Pinot cluster. So you can see here

217
00:13:02,428 --> 00:13:06,566
we've got one controller, one broker, one server. Normally you'd have more than

218
00:13:06,588 --> 00:13:08,998
one of those, but since we're just running it locally, we just have one.

219
00:13:09,084 --> 00:13:11,482
And then on the left hand side over here you can see we've got the

220
00:13:11,536 --> 00:13:14,794
query console and we can navigate into this

221
00:13:14,832 --> 00:13:18,986
table. We can write a query so we can say, hey, show me how

222
00:13:19,008 --> 00:13:21,594
many documents or rows you have. And you see each time you run it,

223
00:13:21,632 --> 00:13:25,306
the number is going up and up and up. We can also go back into

224
00:13:25,328 --> 00:13:28,446
here and we can see, we could navigate to the table. And so you can

225
00:13:28,468 --> 00:13:32,154
see that, hey, there's a table. It's got a schema. We could navigate

226
00:13:32,202 --> 00:13:35,806
into the table. The schema is defined in more detail there you

227
00:13:35,828 --> 00:13:39,454
can edit the table config if you want to, and down here it indicates

228
00:13:39,582 --> 00:13:43,198
what segments are in there. So you can see this is like the first segment

229
00:13:43,294 --> 00:13:46,418
that was created. What we're going to do now is we're going

230
00:13:46,424 --> 00:13:49,558
to go and have a look at how we can build a streamlit application on

231
00:13:49,564 --> 00:13:54,166
top of this. So we'll go back into visual studio again and

232
00:13:54,188 --> 00:13:57,542
we've got our app Py class.

233
00:13:57,596 --> 00:14:00,998
So this is a streamlit script. It's basically just a normal python bit of

234
00:14:01,004 --> 00:14:04,198
code, except we've imported the streamlit library at the top and then we've

235
00:14:04,214 --> 00:14:07,546
got a bunch of other things that we're going to use and then there's just

236
00:14:07,568 --> 00:14:11,014
some python code. And whenever you want to render

237
00:14:11,062 --> 00:14:14,774
something to the screen there's like a streamlit something.

238
00:14:14,832 --> 00:14:17,898
So streamlit markdown, streamlit header,

239
00:14:17,994 --> 00:14:21,258
whatever it is. And that will then put the data on the screen. So we'll

240
00:14:21,274 --> 00:14:23,134
come back and look at this in a minute. But let's just have a look

241
00:14:23,172 --> 00:14:26,578
what a streamlit application actually looks like. So if we

242
00:14:26,584 --> 00:14:29,170
do streamlit, run app py,

243
00:14:29,590 --> 00:14:32,946
that will launch a streamlit application on port eight five one.

244
00:14:32,968 --> 00:14:36,606
So we'll just copy that. Let's navigate back to our web browser.

245
00:14:36,638 --> 00:14:40,086
So we'll open a new tab on here. Just paste that

246
00:14:40,108 --> 00:14:44,274
in and you can see here we've got a streamlit

247
00:14:44,322 --> 00:14:47,846
application running. So this is actually refreshing every 5 seconds. You can see

248
00:14:47,868 --> 00:14:51,242
here the last time that it updated. So I've got it like running on

249
00:14:51,296 --> 00:14:55,146
a little loop that refreshes it every 5 seconds and you can

250
00:14:55,168 --> 00:14:58,794
see that these number of records is changing every time it

251
00:14:58,832 --> 00:15:02,074
refreshes. The last 1 minute there's been 2000 changes by

252
00:15:02,112 --> 00:15:05,454
398 endusers on 61 different domains. The last

253
00:15:05,492 --> 00:15:09,086
five minutes is a bit more, last ten minutes, obviously a bit more

254
00:15:09,108 --> 00:15:12,334
than that. And then actually the last 1 hour in all time

255
00:15:12,372 --> 00:15:15,854
are exactly the same because we haven't actually

256
00:15:15,892 --> 00:15:18,846
been running it for that long. We can then zoom in. So on our navigation

257
00:15:18,878 --> 00:15:22,098
on the left hand side, that's an overview of what's happened. We can

258
00:15:22,104 --> 00:15:24,862
then see who's been making those changes. So we can see is it bots,

259
00:15:24,926 --> 00:15:28,818
is it not bots? So at the moment it says only at least I'm

260
00:15:28,824 --> 00:15:31,942
not sure exactly how it defines what a bot is, but 27% of the changes

261
00:15:31,996 --> 00:15:35,734
have been made by what they define as bots and 73%

262
00:15:35,932 --> 00:15:39,474
not by bots. And every 5 seconds this updates. So these percentages

263
00:15:39,522 --> 00:15:42,914
will be adjusting as we go. We can then see which users

264
00:15:43,042 --> 00:15:46,762
was it, who were the top bots and who were the top number.

265
00:15:46,816 --> 00:15:49,226
So you can see these people are making a lot of changes. Like this one

266
00:15:49,248 --> 00:15:53,274
here has made 548 changes in the seven

267
00:15:53,312 --> 00:15:56,766
minutes or so since we started running it. We can also see where

268
00:15:56,788 --> 00:15:59,934
the changes are being made. So if we click onto this next tab here were

269
00:15:59,972 --> 00:16:03,178
are the changes being made like which properties which wikimedia?

270
00:16:03,194 --> 00:16:06,738
So it's mostly on commonswikimedia.org, which is surprising to me.

271
00:16:06,824 --> 00:16:10,946
I'd expected most of it to be done on Wikipedia, but it's actually not.

272
00:16:11,128 --> 00:16:15,058
It's mostly on Wikidata and Commons Wikimedia. And then we

273
00:16:15,064 --> 00:16:18,466
can see the types of changes. So weve got edit, categorize, log and then new

274
00:16:18,488 --> 00:16:21,894
pages and then interestingly conf 42 I'm not entirely sure what that is.

275
00:16:22,092 --> 00:16:25,878
We can also do drill down. So when you're building analytics dashboards that's a

276
00:16:25,884 --> 00:16:29,138
pretty common thing. So you might have like these overview pages,

277
00:16:29,154 --> 00:16:31,158
but then you want to do a drill down. So like take me into one

278
00:16:31,164 --> 00:16:34,598
of them. So maybe it's show me what's being done by a particular user.

279
00:16:34,614 --> 00:16:37,722
So in this case we can pick a user, see where they've been making changes

280
00:16:37,776 --> 00:16:41,018
and what type of changes they've been making. So this

281
00:16:41,104 --> 00:16:44,158
user here, I'm not going to attempt to pronounce that, but they've made like a

282
00:16:44,164 --> 00:16:47,694
lot of changes on wikidata.org and mostly editing stuff not

283
00:16:47,732 --> 00:16:50,986
really any categorizing, if we pick like a different user,

284
00:16:51,018 --> 00:16:56,026
so say the KR, but let's just pick one changes

285
00:16:56,058 --> 00:16:58,818
back again. But if we were to pick another one, we could sort of see

286
00:16:58,984 --> 00:17:02,338
what they've been up to. So this is a sort

287
00:17:02,344 --> 00:17:06,558
of dashboard that we can build. So now let's

288
00:17:06,574 --> 00:17:09,734
go back and just have a quick look at how we went about

289
00:17:09,772 --> 00:17:12,918
building that. So we might be able to build one for ourselves. So we'll

290
00:17:12,924 --> 00:17:17,158
just minimize this here. So you can see we can

291
00:17:17,324 --> 00:17:20,994
ingest rather any python libraries

292
00:17:21,042 --> 00:17:24,826
that we want to use. This is where were setting the refresh. So this is

293
00:17:24,848 --> 00:17:28,106
refreshing the screen every 5 seconds. You don't necessarily have to have that.

294
00:17:28,128 --> 00:17:30,426
If you wanted to just have a manual button to refresh it, you could have

295
00:17:30,448 --> 00:17:33,918
that. Instead. We can define the title. So this was the title that we had

296
00:17:33,924 --> 00:17:37,546
on the top of the page. We've also got. This is how we're

297
00:17:37,738 --> 00:17:40,480
printing like the last update that was made.

298
00:17:41,730 --> 00:17:44,782
I've done a bit of styling myself here on

299
00:17:44,916 --> 00:17:48,338
this overview. So this is the overview tab. So I've actually just explained how that

300
00:17:48,344 --> 00:17:52,142
works. We've got down here, we've got a sidebar title

301
00:17:52,206 --> 00:17:56,194
and these we're building a little map that has a function representing each

302
00:17:56,232 --> 00:17:59,938
of the radio buttons. So we've got overview who's making the changes? Where are the

303
00:17:59,944 --> 00:18:02,806
changes that done and drilled down? You don't have to have this. If you had

304
00:18:02,828 --> 00:18:06,006
just a single page app, you wouldn't need to bother with this. You could just

305
00:18:06,028 --> 00:18:09,506
literally just print everything out straight

306
00:18:09,538 --> 00:18:11,846
in one script and it will show all of it on one page. But I

307
00:18:11,868 --> 00:18:15,386
wanted to be able to break it down and then if we narrow in on

308
00:18:15,408 --> 00:18:18,506
one of them, so say this one here, this is showing the types of

309
00:18:18,528 --> 00:18:22,238
changes being made. We've got our query were. So it's saying select

310
00:18:22,404 --> 00:18:26,222
type, count the number of users, group by the type, and for

311
00:18:26,276 --> 00:18:29,694
a particular user. So where user equals whichever user you

312
00:18:29,732 --> 00:18:32,954
selected and then it puts the results,

313
00:18:33,002 --> 00:18:36,334
it executes the query, puts the results into a data frame here,

314
00:18:36,452 --> 00:18:40,050
and then finally builds a plotly chart or graph

315
00:18:40,550 --> 00:18:43,714
online 303 and prints it out to the screen. And the rest of the code

316
00:18:43,752 --> 00:18:47,622
is pretty similar to that. It's quite procedural code. We're not really doing

317
00:18:47,676 --> 00:18:51,122
anything all that clever. It's just sort of reasonably

318
00:18:51,186 --> 00:18:54,694
basic python code. It's just that streamlit is making it

319
00:18:54,732 --> 00:18:58,546
super easy to render it to the screen. So hopefully

320
00:18:58,578 --> 00:19:01,738
that's given you an idea of what you can do with these tools. So I

321
00:19:01,744 --> 00:19:05,146
just want to conclude the talk by sort of going back to the

322
00:19:05,168 --> 00:19:08,620
slides and just recapping what weve been doing.

323
00:19:09,630 --> 00:19:13,066
So just to remind ourselves, what did we do? So we had this streamlit

324
00:19:13,098 --> 00:19:16,186
API on Wikipedia events.

325
00:19:16,218 --> 00:19:19,790
We used a python application using a couple of python libraries to the SSC

326
00:19:20,290 --> 00:19:24,202
client that processed those events. We then connected to Kafka

327
00:19:24,266 --> 00:19:28,130
using the Kafka Python driver. We put the events into Kafka.

328
00:19:28,550 --> 00:19:32,014
We then had Pinot connected up to that. So we created, wrote some Yaml,

329
00:19:32,062 --> 00:19:36,158
I guess not only Python, but a bit of yaml, connected that to the Kafka

330
00:19:36,174 --> 00:19:40,482
topic and ingested that data into Pinot. And then we were using a pinot Python

331
00:19:40,546 --> 00:19:44,054
client to then get the data out. And then finally we

332
00:19:44,092 --> 00:19:47,718
used the streamlit Python library to get the data in here. And so we've used

333
00:19:47,804 --> 00:19:51,434
lots of different python tools. So we used other tools outside of

334
00:19:51,472 --> 00:19:55,066
Python, but we were able to glue them all together really nicely and build a

335
00:19:55,088 --> 00:19:58,618
web app really quickly. That looks actually pretty good.

336
00:19:58,784 --> 00:20:02,122
It's pretty nice, it's pretty interactive, and it's all done using

337
00:20:02,176 --> 00:20:05,918
Python. And finally, if you want to play around with any of

338
00:20:05,924 --> 00:20:08,698
these tools, this is where they live. So we've got streamlit, we've got Apache,

339
00:20:08,714 --> 00:20:12,398
Pinot, Apache, Kafka and the code used in

340
00:20:12,404 --> 00:20:15,850
the demo is all available on this GitHub

341
00:20:15,930 --> 00:20:19,422
repository. If you want to ask me any questions of

342
00:20:19,476 --> 00:20:22,926
anything doesn't make any sense, you can contact me here. So I've put my

343
00:20:22,948 --> 00:20:25,940
Twitter handle, but otherwise, thank you for listening to the talk.

