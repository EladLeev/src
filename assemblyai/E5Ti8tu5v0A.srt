1
00:00:23,930 --> 00:00:27,240
Hello, and welcome to our talk, chaos in the cloud.

2
00:00:28,250 --> 00:00:31,574
I'm going to start by telling you a little bit about what we are going

3
00:00:31,612 --> 00:00:35,206
to do in the next 45 minutes. We are

4
00:00:35,228 --> 00:00:38,610
going to start with a recap on what is chaos engineering.

5
00:00:38,770 --> 00:00:41,960
Then we are going to talk about chaos engineering in the cloud.

6
00:00:42,730 --> 00:00:46,006
Taking an aside into what the cloud is and what

7
00:00:46,028 --> 00:00:49,526
its resiliency features are, we're going to talk about the

8
00:00:49,548 --> 00:00:53,146
AWS fault injection simulator, which is the AWS

9
00:00:53,258 --> 00:00:56,846
fully managed chaos engineering service. Then we're going

10
00:00:56,868 --> 00:01:00,346
to have a demonstration of how that looks in practice.

11
00:01:00,538 --> 00:01:04,142
Then we're going to finally share some best practices on working

12
00:01:04,196 --> 00:01:08,286
with the fault injection simulator and architecting

13
00:01:08,318 --> 00:01:10,740
for resiliency in general.

14
00:01:11,510 --> 00:01:15,394
What is chaos engineering and why do we do it?

15
00:01:15,592 --> 00:01:19,522
I like to start this talk with a quote from

16
00:01:19,576 --> 00:01:23,254
Werner Fogeld. He's the CDO of Amazon.com,

17
00:01:23,372 --> 00:01:27,126
and he said, failures are a given, and everything will

18
00:01:27,148 --> 00:01:31,290
eventually fail over time. And that's so because

19
00:01:31,360 --> 00:01:35,046
with the rise of microservices and distributed architectures,

20
00:01:35,238 --> 00:01:39,030
the web and our applications have grown increasingly complex.

21
00:01:39,110 --> 00:01:42,586
And as a result of that, random failures,

22
00:01:42,778 --> 00:01:46,030
random failures have grown difficult to predict.

23
00:01:46,530 --> 00:01:50,142
At the same time, our dependence on those systems has only

24
00:01:50,196 --> 00:01:54,718
ever increased. And so as we moved away from monolithic architectures

25
00:01:54,814 --> 00:01:58,638
towards microservices to become more agile,

26
00:01:58,734 --> 00:02:02,382
build faster, and be able to scale, we naturally adds

27
00:02:02,446 --> 00:02:05,410
some complexity between those microservices.

28
00:02:07,610 --> 00:02:11,142
And chaos engineering is experimenting on those

29
00:02:11,196 --> 00:02:14,534
distributed systems to build confidence in those systems and

30
00:02:14,572 --> 00:02:16,630
make sure they can survive failure,

31
00:02:18,270 --> 00:02:22,214
and to get a better understanding of their failure conditions

32
00:02:22,262 --> 00:02:27,142
and of the performance envelopes. And that's all contained

33
00:02:27,206 --> 00:02:31,290
in the quote I have on the slide from principlesofchaos.org.

34
00:02:32,130 --> 00:02:35,854
So, diving a little bit deeper into chaos engineering, what is

35
00:02:35,892 --> 00:02:39,002
it? We generally talk about kind of a loop,

36
00:02:39,066 --> 00:02:42,814
stressing a system, observing how it reacts, and then improving

37
00:02:42,862 --> 00:02:46,142
the system. And we do that to improve

38
00:02:46,206 --> 00:02:50,702
resiliency and performance, uncover hidden issues in our architectures,

39
00:02:50,846 --> 00:02:54,574
and expose blind spots, for example, in monitoring observability

40
00:02:54,702 --> 00:02:59,122
and alarming. And usually we also achieve

41
00:02:59,186 --> 00:03:03,410
a degree of improvement in recovery times, improving our operational

42
00:03:03,490 --> 00:03:06,934
skills with the system, and a culture in

43
00:03:06,972 --> 00:03:11,450
our technical. Sorry. And a culture in our tech.org.

44
00:03:13,790 --> 00:03:17,322
And I talked about that being a loop and going

45
00:03:17,376 --> 00:03:21,534
deeper into the loop. We see a loop of setting an

46
00:03:21,572 --> 00:03:24,734
objective. We want to achieve something with our system.

47
00:03:24,932 --> 00:03:28,314
We want to make it resilient against single ac failure,

48
00:03:28,362 --> 00:03:32,458
for example. We set that objective, then we

49
00:03:32,644 --> 00:03:34,740
design and implement our system.

50
00:03:35,750 --> 00:03:39,426
We design an experiment to test

51
00:03:39,608 --> 00:03:43,106
if our objective is actually achieved. We run the

52
00:03:43,128 --> 00:03:46,990
test, we operate the system, and from the experiment

53
00:03:47,070 --> 00:03:50,566
and the system operation we learn about the

54
00:03:50,588 --> 00:03:54,822
system. We can respond to failure conditions, improve the system and

55
00:03:54,876 --> 00:03:58,220
set new objectives and get better over time.

56
00:03:58,910 --> 00:04:02,378
So chaos engineering locally on

57
00:04:02,384 --> 00:04:05,914
your local machine, on your server, versus in the cloud. I'm going to

58
00:04:05,952 --> 00:04:10,002
start by talking about chaos engineering in Java,

59
00:04:10,166 --> 00:04:13,966
and I just choose Java because it's one

60
00:04:13,988 --> 00:04:18,030
of the most widely used languages and it has awesome

61
00:04:18,100 --> 00:04:21,834
support for chaos testing, both via libraries and via

62
00:04:21,882 --> 00:04:25,170
tooling. What you generally see here

63
00:04:25,320 --> 00:04:28,962
is that a lot of those tests are either JVM based,

64
00:04:29,016 --> 00:04:32,706
they run inside the local JVM, they might be

65
00:04:32,728 --> 00:04:35,858
agent based, they run as a sidecar next to the JVM on

66
00:04:35,864 --> 00:04:39,350
the same machine, or they might be moved into the service

67
00:04:39,420 --> 00:04:43,190
mesh connecting many systems together. And that's really an awesome

68
00:04:43,260 --> 00:04:47,122
way to test your application code and to make sure it's resilient

69
00:04:47,266 --> 00:04:50,474
and to be able to control those experiments from

70
00:04:50,512 --> 00:04:54,426
your application code. And if

71
00:04:54,448 --> 00:04:58,074
that is so awesome and it already works quite well, it's a great

72
00:04:58,112 --> 00:05:01,406
improvement and everybody should do more of it. Why would we

73
00:05:01,428 --> 00:05:04,480
want to move chaos engineering into the cloud?

74
00:05:05,730 --> 00:05:09,982
We believe that using cloud services we

75
00:05:10,036 --> 00:05:13,634
gain a more holistic monitoring view of how

76
00:05:13,672 --> 00:05:17,970
our system works. We can identify cloud

77
00:05:18,040 --> 00:05:22,050
based challenges, for example around limits, around scaling,

78
00:05:22,790 --> 00:05:26,280
around interacting with other accounts. For example,

79
00:05:27,210 --> 00:05:30,614
we can run dedicated scaling experiments to

80
00:05:30,652 --> 00:05:33,926
learn where the failure points of our application is if

81
00:05:33,948 --> 00:05:37,618
we scale to a very large degree or

82
00:05:37,644 --> 00:05:41,162
very fast. And finally, we can do

83
00:05:41,216 --> 00:05:45,206
validation of disaster recovery strategies where we gain

84
00:05:45,238 --> 00:05:48,710
the ability to run chaos experiments

85
00:05:48,870 --> 00:05:52,654
on a system and see if it, for example, can be switched over to

86
00:05:52,692 --> 00:05:56,350
another region without

87
00:05:56,420 --> 00:05:59,630
an time or in a specified timeframe.

88
00:06:00,930 --> 00:06:04,820
And this is the point where I'm going to take an insight into

89
00:06:05,750 --> 00:06:09,090
resiliency features of the cloud of AWS

90
00:06:09,590 --> 00:06:12,370
and how they interact with applications.

91
00:06:13,110 --> 00:06:17,714
What we see on the slide is the region design of AWS.

92
00:06:17,842 --> 00:06:21,990
And we see a region is made up of multiple availability zones

93
00:06:22,330 --> 00:06:24,950
here, abbreviated as AZ.

94
00:06:25,690 --> 00:06:29,482
And each AZ is made up of one or more data

95
00:06:29,536 --> 00:06:33,322
centers. And that's the only time I'm actually going to talk about data

96
00:06:33,376 --> 00:06:36,790
centers. The fundamental

97
00:06:36,870 --> 00:06:40,242
logical part of resiliency in AZ

98
00:06:40,326 --> 00:06:44,030
you should think about is an availability zone, because those

99
00:06:44,100 --> 00:06:47,838
availability zones are independent from each other.

100
00:06:48,004 --> 00:06:51,674
And what that means is they have independent network

101
00:06:51,722 --> 00:06:55,170
providers, they have independent power connections,

102
00:06:55,670 --> 00:06:59,262
and if there is a geographic feature

103
00:06:59,406 --> 00:07:03,026
in the city where the region is located, the AZ will be

104
00:07:03,048 --> 00:07:07,246
located while being mindful of that geographic

105
00:07:07,278 --> 00:07:10,342
feature. So for example, if there is a river flowing through the city,

106
00:07:10,476 --> 00:07:14,166
not all of the AWS will be close to the river so

107
00:07:14,188 --> 00:07:17,766
that we can be sure that in case of a flooding, not all of

108
00:07:17,788 --> 00:07:21,462
the AWS go down. And so as you distribute

109
00:07:21,526 --> 00:07:23,930
an application across AWS,

110
00:07:25,230 --> 00:07:29,210
it will become resilient against single AZ failure

111
00:07:29,550 --> 00:07:33,230
and gain a degree of resilience.

112
00:07:33,890 --> 00:07:37,694
It will become resilient against single

113
00:07:37,812 --> 00:07:41,854
availability zone failure, even though those failures are

114
00:07:41,892 --> 00:07:43,570
quite unlikely in practice.

115
00:07:47,030 --> 00:07:50,500
What does it actually look like? How do we distribute an application

116
00:07:51,350 --> 00:07:53,890
across multiple availability zones?

117
00:07:54,710 --> 00:07:58,434
What we see here is a simplified architecture

118
00:07:58,482 --> 00:08:01,410
of a web application. We see cloud balancing,

119
00:08:01,570 --> 00:08:05,622
distributing traffic to different instances, and that means EC two

120
00:08:05,676 --> 00:08:09,066
instances for us and those instances act as application

121
00:08:09,168 --> 00:08:12,486
servers. And then underneath

122
00:08:12,518 --> 00:08:15,734
that we see a primary and a standby DB

123
00:08:15,782 --> 00:08:19,466
instance, and we see that those are also distributed across

124
00:08:19,568 --> 00:08:23,134
availability zones. And so in the end, like the event that,

125
00:08:23,172 --> 00:08:27,146
for example, availability zone r goes down, the load

126
00:08:27,178 --> 00:08:30,240
balancer would distribute traffic to b and C,

127
00:08:30,690 --> 00:08:34,510
and there would be an automatic DB switch over to make the

128
00:08:34,660 --> 00:08:37,694
current standby instance and c the primary instance,

129
00:08:37,742 --> 00:08:40,900
and your application would continue to work.

130
00:08:43,830 --> 00:08:47,110
Now that we have a certain understanding of resiliency in the cloud

131
00:08:47,180 --> 00:08:50,374
and why we would want to actually test it, let me

132
00:08:50,412 --> 00:08:54,294
go deeper into the AWS fault injection simulator and how

133
00:08:54,332 --> 00:08:57,910
it supports you in running chaos experiments.

134
00:08:59,390 --> 00:09:02,762
The AWS fault injection simulator is our

135
00:09:02,816 --> 00:09:05,740
fully managed chaos engineering service.

136
00:09:06,110 --> 00:09:10,294
It's an easy way to get started and it lets you reproduce

137
00:09:10,342 --> 00:09:13,646
real world failures, whether they are very simple like stopping an

138
00:09:13,668 --> 00:09:17,150
instance, or more complex like swapping APIs.

139
00:09:19,010 --> 00:09:22,962
And finally, the service fully embraces the idea

140
00:09:23,016 --> 00:09:26,914
of safeguards. So you can make sure that a

141
00:09:26,952 --> 00:09:31,330
running chaos experiment does not impact your productive deployment.

142
00:09:31,910 --> 00:09:36,174
I'm going to go into detail about all of those three features

143
00:09:36,222 --> 00:09:40,200
that I just mentioned. It's easy to get started

144
00:09:40,970 --> 00:09:44,230
because we spent actually a lot of time making it easy to get started,

145
00:09:44,300 --> 00:09:47,394
because when we talk to customers about chaos engineering,

146
00:09:47,522 --> 00:09:51,174
what customers repeatedly told us is that it is a little bit hard

147
00:09:51,212 --> 00:09:54,346
to get started with chaos engineering. And so we want to make that as

148
00:09:54,368 --> 00:09:57,866
easy as possible. You can use the console to

149
00:09:57,888 --> 00:10:01,210
get familiar with the service and actually try things out,

150
00:10:01,360 --> 00:10:05,214
and then you can use the CLI to take advantage of the templates and

151
00:10:05,252 --> 00:10:08,938
integrate the service with your CI CD pipelines.

152
00:10:09,114 --> 00:10:12,946
And those templates are JSon or YAML files that you can share

153
00:10:12,968 --> 00:10:16,514
with your team. You can version control them and use all of the benefits

154
00:10:16,632 --> 00:10:20,210
and best practices associated with version control,

155
00:10:20,280 --> 00:10:21,620
like code review.

156
00:10:24,630 --> 00:10:28,294
You can run experiments in sequence. You can

157
00:10:28,332 --> 00:10:32,630
run impact of gradual degradation like a secret.

158
00:10:33,050 --> 00:10:36,950
Gradual degradation experiments are to test

159
00:10:37,020 --> 00:10:40,730
the impact of multiple concurrent issues, which is actually

160
00:10:40,800 --> 00:10:44,554
how a lot of real world outages happen. You don't see outages because of

161
00:10:44,592 --> 00:10:48,470
a single failure, but because of a chain of single failures

162
00:10:48,630 --> 00:10:51,390
leading up to a real world outage.

163
00:10:51,810 --> 00:10:55,050
It currently supports services like EC two rds,

164
00:10:55,130 --> 00:10:58,650
ecs and eks. So virtual instances,

165
00:10:58,730 --> 00:11:02,678
databases, container runtimes and managed

166
00:11:02,714 --> 00:11:06,642
kubernetes. And we are working all the time to

167
00:11:06,696 --> 00:11:10,466
provide support for more service, for more services,

168
00:11:10,568 --> 00:11:13,570
sorry. And for more complex conditions.

169
00:11:14,150 --> 00:11:17,474
And just to hit a nail on the head here, those faults,

170
00:11:17,522 --> 00:11:20,966
they are really happening at the service control plane level.

171
00:11:21,148 --> 00:11:24,854
So an instance might actually be terminated, memory is actually being

172
00:11:24,892 --> 00:11:28,110
utilized, APIs are actually being throttled.

173
00:11:28,290 --> 00:11:31,690
It's not faking something with metric manipulation,

174
00:11:32,350 --> 00:11:36,250
but it's actually impacting how things

175
00:11:36,320 --> 00:11:39,354
work at the control plane level. So you will

176
00:11:39,392 --> 00:11:43,054
have to take extra or use extra caution when using the

177
00:11:43,092 --> 00:11:46,734
service. And to enable you to do that, we have

178
00:11:46,772 --> 00:11:50,382
safeguards. Safeguards act as the automated stop

179
00:11:50,436 --> 00:11:54,594
button. Avoid to monitor the blast radius of your experiments and

180
00:11:54,632 --> 00:11:58,862
make sure that it's contained and that failures created with the experiment

181
00:11:59,006 --> 00:12:02,546
are rolled backed if alarms go off. And that's kind

182
00:12:02,568 --> 00:12:06,210
of runtime controls what happens during an experiment.

183
00:12:06,370 --> 00:12:10,242
And the service of course integrates with identity

184
00:12:10,306 --> 00:12:13,862
and access management. IAM and Im controls can

185
00:12:13,916 --> 00:12:17,254
be used to control what fault types can be used in an

186
00:12:17,292 --> 00:12:21,078
experiment and what resource can be affected. And that's

187
00:12:21,094 --> 00:12:23,946
of course working with stack based policies. So for example,

188
00:12:24,048 --> 00:12:27,398
only EC two instances with a tag environment

189
00:12:27,494 --> 00:12:31,150
equals test can be affected. That's one of many

190
00:12:31,220 --> 00:12:34,350
possible safeguards you can implement.

191
00:12:37,250 --> 00:12:40,350
What kind of targets and actions are supported?

192
00:12:41,010 --> 00:12:44,558
There's a host of targets

193
00:12:44,574 --> 00:12:47,454
and actions supported across the categories of compute,

194
00:12:47,502 --> 00:12:51,460
storage, networking databases and management service

195
00:12:51,910 --> 00:12:54,180
services. Sorry. And management services.

196
00:12:56,310 --> 00:13:00,226
And I will dive somewhat deeper into the architecture

197
00:13:00,258 --> 00:13:04,546
of default injection service and how it interacts

198
00:13:04,578 --> 00:13:08,220
with the different components of the AWS cloud.

199
00:13:09,230 --> 00:13:12,586
And we see here a diagram of how the

200
00:13:12,608 --> 00:13:16,582
service works. At a high level, we start with an experiment

201
00:13:16,646 --> 00:13:20,090
template which will comprise,

202
00:13:20,610 --> 00:13:23,918
sorry. At a high level we will start with an

203
00:13:24,004 --> 00:13:27,818
experiments template which is or which contains

204
00:13:27,914 --> 00:13:31,690
different fault injection actions, targets that will be affected

205
00:13:31,770 --> 00:13:36,094
and safeguards to be run during the experiment. And you can see that here slightly

206
00:13:36,142 --> 00:13:40,414
to the left of center called experiment template

207
00:13:40,462 --> 00:13:41,300
and white.

208
00:13:43,430 --> 00:13:47,314
And then when we start an experiments, default injection

209
00:13:47,362 --> 00:13:50,854
simulator performs the actions. So it

210
00:13:50,892 --> 00:13:55,974
injects faults into supported resources that

211
00:13:56,012 --> 00:13:59,274
are specified as the targets, and then

212
00:13:59,312 --> 00:14:02,634
those faults interact with your

213
00:14:02,672 --> 00:14:06,026
resources and that will change what happens in

214
00:14:06,048 --> 00:14:10,026
monitoring, in monitoring in Amazon Cloudwatch for example, or in

215
00:14:10,048 --> 00:14:13,982
your third party monitoring solution and then you can take

216
00:14:14,036 --> 00:14:18,106
action based on those observability metrics

217
00:14:18,138 --> 00:14:21,662
you have there on those alarms you have there on those logs you have there

218
00:14:21,796 --> 00:14:26,206
by using Amazon eventbridge to, for example, stop an experiment

219
00:14:26,318 --> 00:14:29,922
if the wrong alarm is triggered, or to start

220
00:14:29,976 --> 00:14:32,900
a second experiment at that point in time.

221
00:14:33,830 --> 00:14:37,726
And so now we have an understanding of what chaos engineering

222
00:14:37,758 --> 00:14:41,974
is, why we want to do it in the cloud, and how the

223
00:14:42,012 --> 00:14:45,778
AWS fault injection simulator works. And it's the point where I'm

224
00:14:45,794 --> 00:14:49,578
going to hand over to bent to tell you about

225
00:14:49,664 --> 00:14:53,174
some exciting new scenarios we saw from reinvent,

226
00:14:53,222 --> 00:14:56,886
and to show you a demonstration of how the fault injection

227
00:14:56,918 --> 00:14:58,730
simulator works in practice.

228
00:15:00,510 --> 00:15:04,206
All right, thank you very much, Oliver. We're now going to

229
00:15:04,228 --> 00:15:07,546
take a closer look at those two new scenarios

230
00:15:07,578 --> 00:15:11,054
we launched at reinvent. So the first one is

231
00:15:11,092 --> 00:15:14,574
about cross region connectivity disruptions.

232
00:15:14,702 --> 00:15:18,222
So we have customers who have the requirement of operating

233
00:15:18,286 --> 00:15:22,430
their application at the highest possible availability

234
00:15:22,510 --> 00:15:25,926
rates, and those customers typically tend to

235
00:15:26,108 --> 00:15:29,830
architect their applications to span two regions.

236
00:15:30,170 --> 00:15:33,382
And those customers also asked us that we could

237
00:15:33,436 --> 00:15:37,094
maybe help them to make chaos testing even easier for

238
00:15:37,132 --> 00:15:41,062
them to test whether their applications can really withstand

239
00:15:41,126 --> 00:15:44,918
a connectivity disruption between two regions. So for instance,

240
00:15:45,014 --> 00:15:49,030
think of an active active or active passive

241
00:15:49,110 --> 00:15:52,910
kind of setup between two regions where all of a sudden your

242
00:15:52,980 --> 00:15:56,990
database, like a dynamodb, won't replicate new data.

243
00:15:57,140 --> 00:16:01,310
This is now possible with the new cross region

244
00:16:01,650 --> 00:16:05,694
connectivity disruption scenario that is available with the fault injection

245
00:16:05,742 --> 00:16:09,502
simulator. Another new scenario that we also launch

246
00:16:09,566 --> 00:16:12,798
is around availability zone power interruptions.

247
00:16:12,894 --> 00:16:16,162
So we already mentioned that those kind of scenarios

248
00:16:16,226 --> 00:16:19,590
have a really low likelihood of happening. However,

249
00:16:19,660 --> 00:16:23,894
there are customers who still want to experience how

250
00:16:23,932 --> 00:16:27,666
their application would behave in such an event. So in the event

251
00:16:27,708 --> 00:16:31,082
of a power interruption, you would see, for instance, scenarios like

252
00:16:31,216 --> 00:16:34,874
EC two instances or containers stopping out of

253
00:16:34,912 --> 00:16:38,314
nowhere. And with the new scenario of AC

254
00:16:38,432 --> 00:16:41,806
availability power disruptions, you will basically be able to

255
00:16:41,828 --> 00:16:45,310
test how your application behaves under those kind of events.

256
00:16:46,370 --> 00:16:49,710
But now we basically want to have a look

257
00:16:49,780 --> 00:16:53,106
at our own demo. So we will take you

258
00:16:53,208 --> 00:16:56,900
through a journey of testing an application

259
00:16:57,270 --> 00:17:01,150
and improving its resiliency when it comes to availability

260
00:17:01,230 --> 00:17:04,514
zones failures. So what we brought for

261
00:17:04,552 --> 00:17:08,838
today here is a simple workload that is currently running

262
00:17:08,924 --> 00:17:12,274
in a single container. So we have a container

263
00:17:12,322 --> 00:17:16,374
that is basically a simple API that responds with a pong to

264
00:17:16,412 --> 00:17:20,138
every request to send to the container. And this is currently

265
00:17:20,224 --> 00:17:23,878
running on eks. It's a single pod

266
00:17:23,974 --> 00:17:27,226
hosted on a single node that we have in our cluster, which is

267
00:17:27,248 --> 00:17:30,574
currently running in the availability zone. A and I

268
00:17:30,612 --> 00:17:33,818
personally am not the most Kubernetes experienced

269
00:17:33,914 --> 00:17:37,790
guy. So I'm not really sure how Kubernetes behaves

270
00:17:39,170 --> 00:17:42,682
in the case of an availability zone disruption.

271
00:17:42,826 --> 00:17:46,862
And I personally want to have a really low

272
00:17:46,916 --> 00:17:50,802
cost with my application. So I just want to check whether

273
00:17:50,936 --> 00:17:54,366
running a single pod is enough to tolerate the failure

274
00:17:54,398 --> 00:17:59,042
of an AC or if for instance, Kubernetes will automatically

275
00:17:59,106 --> 00:18:02,214
for me schedule this pod on a new

276
00:18:02,252 --> 00:18:05,030
node that is hosted in a different availability zone.

277
00:18:05,610 --> 00:18:09,420
But before we jump into the console, I already want to show you

278
00:18:10,350 --> 00:18:13,942
the result of the experiment.

279
00:18:14,086 --> 00:18:17,990
So of course this is not going to happen. We won't see an automatic

280
00:18:18,070 --> 00:18:21,502
pod reassignment to a different node hosted in a different

281
00:18:21,556 --> 00:18:24,942
availability zone. This is not how it actually

282
00:18:24,996 --> 00:18:28,606
works. So instead what we would want to do here to make

283
00:18:28,628 --> 00:18:32,894
the system more resilient is basically by updating

284
00:18:32,942 --> 00:18:38,274
our deployment to at least run in two

285
00:18:38,312 --> 00:18:41,650
different availability zones. And this is actually

286
00:18:41,720 --> 00:18:45,326
what we are going to take a look at now in the demo. Now it's

287
00:18:45,358 --> 00:18:48,798
time to look at the fault injection simulator in action. For this

288
00:18:48,824 --> 00:18:52,466
we are going to first take a look at the application that we deployed

289
00:18:52,498 --> 00:18:56,710
in Kubernetes. Then we are going to set up a load testing tool to

290
00:18:56,780 --> 00:19:00,154
send frequent requests against our application to

291
00:19:00,192 --> 00:19:04,074
inspect some metrics like the availability and also the

292
00:19:04,112 --> 00:19:06,570
response time of our request.

293
00:19:07,550 --> 00:19:10,902
Then we are setting up the fizz

294
00:19:10,966 --> 00:19:14,320
to introduce some chaos in our application.

295
00:19:14,690 --> 00:19:18,606
And then we are revisiting our load testing tool to see the impact of

296
00:19:18,628 --> 00:19:22,366
our chaos experiment. So let's start with taking a look at

297
00:19:22,388 --> 00:19:27,138
the Kubernetes manifest so

298
00:19:27,224 --> 00:19:31,310
we can quickly see here with our deployment. This is basically the Kubernetes

299
00:19:31,390 --> 00:19:34,818
resource that you need to deploy containers in

300
00:19:34,824 --> 00:19:38,402
your cluster. We can see here that we have one container

301
00:19:38,466 --> 00:19:42,086
that we are deploying on our cluster. This is

302
00:19:42,108 --> 00:19:45,480
basically pulling the container image from my application

303
00:19:46,250 --> 00:19:49,740
from a elastic container registry of my account.

304
00:19:50,990 --> 00:19:54,166
Then I'm also using a node

305
00:19:54,198 --> 00:19:57,510
selector here to ensure that this container

306
00:19:57,670 --> 00:20:01,114
will basically be scheduled on a node

307
00:20:01,162 --> 00:20:05,178
that is running in the US east one, a availability zone.

308
00:20:05,274 --> 00:20:09,134
Besides our deployment we also have a service

309
00:20:09,252 --> 00:20:12,618
and an ingress resource. Those are required

310
00:20:12,714 --> 00:20:16,194
to expose our deployment in the Internet and

311
00:20:16,232 --> 00:20:19,954
we need that in order to run our load testing here

312
00:20:20,072 --> 00:20:23,950
for this use case. One thing to highlight here is the ingress

313
00:20:24,030 --> 00:20:28,114
section. So we are using the AWS load balancer

314
00:20:28,162 --> 00:20:31,746
controller to create an application load balancer

315
00:20:31,778 --> 00:20:35,490
in the cloud from this ingress resource that we just deployed.

316
00:20:35,650 --> 00:20:39,242
So both the ingress and the deployment yaml files are already

317
00:20:39,296 --> 00:20:42,250
deployed. So let's have a look if that was successful.

318
00:20:42,910 --> 00:20:46,822
So I'm jumping in the console. Now I'm

319
00:20:46,966 --> 00:20:49,690
running Kubectl get pod.

320
00:20:51,230 --> 00:20:54,606
This should now return one pod. This looks good.

321
00:20:54,788 --> 00:20:58,714
And now we want to basically double check whether this node

322
00:20:58,762 --> 00:21:02,170
here is really running in the correct availability

323
00:21:02,250 --> 00:21:05,700
zone. And this looks good. So the node is

324
00:21:06,070 --> 00:21:08,990
running in the US east one a availability zone.

325
00:21:09,070 --> 00:21:13,230
So that basically means that the container is running in the availability

326
00:21:13,310 --> 00:21:17,080
zone that we want to run a chaos experiment on.

327
00:21:17,610 --> 00:21:22,214
Now let's get the URL of

328
00:21:22,252 --> 00:21:25,586
the ingress controller or to be more precise

329
00:21:25,618 --> 00:21:29,798
of the load balancer. So let's say Kubectl get ingresses.

330
00:21:29,894 --> 00:21:33,034
That looks good and

331
00:21:33,072 --> 00:21:36,902
here's the address. Let's send a curl request

332
00:21:36,966 --> 00:21:40,174
here and we are getting

333
00:21:40,212 --> 00:21:44,622
a response pong. This looks good. So now let's copy this

334
00:21:44,756 --> 00:21:48,554
URL and let's set up the cloud testing

335
00:21:48,602 --> 00:21:52,474
tool. So for this case I'm using locost.

336
00:21:52,602 --> 00:21:56,498
Locost is basically running on my local machine and will send a

337
00:21:56,504 --> 00:21:59,620
good amount of requests per second to my application.

338
00:21:59,990 --> 00:22:03,874
Let's start this here on the top

339
00:22:03,912 --> 00:22:07,446
right corner we can see the requests per second that are sent against

340
00:22:07,628 --> 00:22:11,170
our application. And here we can see the failures.

341
00:22:11,250 --> 00:22:15,430
So failures would indicate a status code 400 ish.

342
00:22:15,770 --> 00:22:18,966
Now we can take a look at the chart and we can see here that

343
00:22:19,148 --> 00:22:22,970
we slowly ramped up on requests per seconds. And now we are

344
00:22:23,040 --> 00:22:26,394
around sending around 240 requests per

345
00:22:26,432 --> 00:22:30,162
second. And this looks good. We have no big failures.

346
00:22:30,326 --> 00:22:34,270
Our response time is quite static and

347
00:22:34,340 --> 00:22:38,240
with 120 milliseconds this looks good.

348
00:22:39,170 --> 00:22:42,526
So I would say this is a successful deployment of

349
00:22:42,548 --> 00:22:46,542
our application. So now let's check how the resiliency

350
00:22:46,606 --> 00:22:50,126
of our application really looks like. And let's

351
00:22:50,158 --> 00:22:53,986
see the impact of a chaos experiment. For this I'm using to

352
00:22:54,008 --> 00:22:57,478
the AWS console. And here I'm basically

353
00:22:57,644 --> 00:23:00,310
searching for the service called AWS.

354
00:23:00,970 --> 00:23:02,790
AWS fizz.

355
00:23:03,930 --> 00:23:07,942
I'm going to open up this one here.

356
00:23:07,996 --> 00:23:12,410
I'm basically taking a look at the experiment templates

357
00:23:13,150 --> 00:23:17,174
here we can see one template that I already created. This is called disrupt

358
00:23:17,222 --> 00:23:20,506
Aza and this is doing exactly that. Let's have a look at

359
00:23:20,528 --> 00:23:24,462
it. So the name is disrupt Aza and this is

360
00:23:24,516 --> 00:23:28,206
exactly what happens here. Let's quickly take

361
00:23:28,228 --> 00:23:31,886
a look at the update wizard. I think this is really good for the

362
00:23:31,908 --> 00:23:35,714
visualization what's happening here. So we can see that

363
00:23:35,752 --> 00:23:39,634
this template is quite small. So we have one action which

364
00:23:39,672 --> 00:23:42,946
is called disrupt aza. Let's have a look

365
00:23:42,968 --> 00:23:47,750
at it first. So here we basically specify

366
00:23:48,170 --> 00:23:52,402
the action type. This is disrupt connectivity.

367
00:23:52,546 --> 00:23:56,470
Disrupt connectivity will prevent packets

368
00:23:57,050 --> 00:24:02,170
from leaving and entering the given subnet.

369
00:24:03,230 --> 00:24:06,826
And down here we can see the duration. In this case we

370
00:24:06,848 --> 00:24:10,806
have two minutes configured and we have configured the target.

371
00:24:10,918 --> 00:24:14,826
So the target are the subnets that are impacted by this event. Now let's

372
00:24:14,858 --> 00:24:18,650
have a look at the targets. So I'm quickly opening up the target

373
00:24:18,730 --> 00:24:21,962
here we can see the subnet

374
00:24:22,026 --> 00:24:25,502
target one. We are using a resource

375
00:24:25,566 --> 00:24:29,486
filter here to ensure that not every single subnet

376
00:24:29,518 --> 00:24:34,878
is targeted, but instead we are only taking subnets

377
00:24:34,974 --> 00:24:38,454
in the US east one a availability zone in

378
00:24:38,492 --> 00:24:42,006
scope of this experiments. So a subnet in

379
00:24:42,028 --> 00:24:45,670
AWS is a zonal resource. So that means that a

380
00:24:45,740 --> 00:24:49,022
subnet is deployed in one availability zone.

381
00:24:49,106 --> 00:24:53,402
And this configuration really just makes sure that no

382
00:24:53,456 --> 00:24:56,982
network traffic is leaving or entering every single subnet

383
00:24:57,046 --> 00:25:00,134
in the availability zone. Us east one a so I would say

384
00:25:00,192 --> 00:25:03,886
let's go back here and start this experiment to take a

385
00:25:03,908 --> 00:25:07,690
look at what's going on. So I'm

386
00:25:07,770 --> 00:25:11,920
clicking start here and

387
00:25:12,290 --> 00:25:16,130
it takes some time until the experiment is actually

388
00:25:16,200 --> 00:25:20,206
executed so you can take a look at timeline to see what's

389
00:25:20,238 --> 00:25:23,330
going on. So currently this one is pending.

390
00:25:23,750 --> 00:25:27,586
This is just taking a couple of seconds to deploy

391
00:25:27,618 --> 00:25:31,686
the disruption. So let's wait. There we go. Now it's actually

392
00:25:31,788 --> 00:25:35,366
running. So if we go back to the locals page.

393
00:25:35,548 --> 00:25:38,970
Yeah, there we go. We should see a drop in availability.

394
00:25:39,950 --> 00:25:43,114
So you can see that straight away we

395
00:25:43,152 --> 00:25:47,260
have a really reduced amount of

396
00:25:47,870 --> 00:25:52,350
requests per seconds and now we can see that there's no request

397
00:25:53,090 --> 00:25:57,790
being successful anymore. So that's basically

398
00:25:57,860 --> 00:26:00,480
showing us that there's not a single request going through.

399
00:26:00,850 --> 00:26:04,306
What's interesting, if you can go back here we can see that

400
00:26:04,408 --> 00:26:07,950
those requests right now are somehow dangling.

401
00:26:08,030 --> 00:26:11,650
So there is no bad response code

402
00:26:11,720 --> 00:26:15,338
like a HTTP 400 ish,

403
00:26:15,454 --> 00:26:19,154
but the connection is just not completely opened and closed.

404
00:26:19,202 --> 00:26:23,186
So this is really the impact of our availability zone

405
00:26:23,218 --> 00:26:26,614
outage here. So let's wait

406
00:26:26,812 --> 00:26:31,610
for some time until this experiments is finished.

407
00:26:35,550 --> 00:26:38,746
Here we go. The experiment is

408
00:26:38,768 --> 00:26:42,186
now finished. Let's go back to the console and we can see straight

409
00:26:42,218 --> 00:26:46,590
away once the experiment was finished

410
00:26:46,930 --> 00:26:51,386
our network is available again and our application continues

411
00:26:51,418 --> 00:26:55,378
to serve traffic. So here we can see that our

412
00:26:55,464 --> 00:26:58,866
current architecture with one container being

413
00:26:58,888 --> 00:27:02,386
deployed in one availability zone is not really able to

414
00:27:02,488 --> 00:27:06,034
handle this situation well. So let's see how we can improve

415
00:27:06,082 --> 00:27:10,854
the availability. I will first stop

416
00:27:10,892 --> 00:27:14,470
the load test here and we are now jumping back into

417
00:27:14,540 --> 00:27:18,234
the editor because I already prepared an

418
00:27:18,272 --> 00:27:22,582
updated deployment. So here we have the updated deployment.

419
00:27:22,726 --> 00:27:25,354
It's pretty similar to the previous one,

420
00:27:25,472 --> 00:27:29,062
besides two major updates.

421
00:27:29,126 --> 00:27:32,430
So first of all, we have two replicas. So this basically

422
00:27:32,500 --> 00:27:36,030
means that we deploy two containers. And then what's even more important

423
00:27:36,100 --> 00:27:39,594
is we have an affinity rule created. So this affinity rule

424
00:27:39,642 --> 00:27:43,634
will basically ensure that those containers will

425
00:27:43,672 --> 00:27:48,078
be spread across the nodes and specifically

426
00:27:48,174 --> 00:27:52,254
spread across the different availability zones. So now let's

427
00:27:52,302 --> 00:27:53,730
go back to the terminal.

428
00:27:55,370 --> 00:27:59,510
And now let's delete the old deployment.

429
00:28:01,850 --> 00:28:07,314
And now

430
00:28:07,452 --> 00:28:11,690
let's apply the updated deployment.

431
00:28:13,470 --> 00:28:17,500
So this will take just a couple of seconds to deploy our

432
00:28:18,110 --> 00:28:21,920
pods. Let's have a look at them.

433
00:28:23,170 --> 00:28:26,986
There they are running on two different nodes.

434
00:28:27,018 --> 00:28:31,034
And if we now again open up the nodes

435
00:28:31,082 --> 00:28:35,042
here, we will see that those

436
00:28:35,096 --> 00:28:38,750
two nodes are actually running in two different availability

437
00:28:38,830 --> 00:28:42,002
zones. So now we can test again.

438
00:28:42,056 --> 00:28:46,040
So let's go back to the load testing tool. Let's do a new test.

439
00:28:46,410 --> 00:28:50,070
Let's configure the same amount of users being and

440
00:28:50,140 --> 00:28:51,960
run the load test.

441
00:28:53,210 --> 00:28:57,614
Give it some seconds here to create seconds.

442
00:28:57,762 --> 00:29:01,900
So here we go.

443
00:29:02,590 --> 00:29:06,250
This looks good. Now let's go back into

444
00:29:06,320 --> 00:29:09,354
the AWS console to rerun the

445
00:29:09,392 --> 00:29:12,974
experiment. So we

446
00:29:13,012 --> 00:29:14,830
are going to experiment,

447
00:29:16,450 --> 00:29:19,726
going to the templates, opening up the

448
00:29:19,748 --> 00:29:23,598
template. We're now starting another experiments

449
00:29:23,694 --> 00:29:25,940
here. Let's start this one.

450
00:29:28,230 --> 00:29:32,338
So this again

451
00:29:32,424 --> 00:29:34,020
just takes some time.

452
00:29:37,370 --> 00:29:40,760
Still pending. Let's see. Now it's running.

453
00:29:41,450 --> 00:29:44,680
So let's also wait for.

454
00:29:49,150 --> 00:29:52,874
So we can see quite a different result now. So what we

455
00:29:52,912 --> 00:29:57,386
see here is a very, very short time

456
00:29:57,488 --> 00:30:01,166
period where our service wasn't available. And this

457
00:30:01,188 --> 00:30:05,438
is basically because of the load balancer health check

458
00:30:05,604 --> 00:30:09,790
that checks the availability of its targets

459
00:30:10,130 --> 00:30:13,154
every 5 seconds. And this is just

460
00:30:13,192 --> 00:30:16,674
the small time gap where the load balancer thought

461
00:30:16,792 --> 00:30:20,578
that the container is available. And then

462
00:30:20,664 --> 00:30:24,014
after the next evaluation figured out, no, I cannot

463
00:30:24,062 --> 00:30:27,654
send any further request to the target. And as we can see

464
00:30:27,692 --> 00:30:30,934
here with this update, our application now

465
00:30:31,132 --> 00:30:35,078
reacted way better to the outage. And this

466
00:30:35,164 --> 00:30:37,842
would be a. Yeah,

467
00:30:37,996 --> 00:30:41,478
would basically show you the full lifecycle of running an experiment.

468
00:30:41,574 --> 00:30:45,450
So we made an assumption, figured out that this is not correct,

469
00:30:45,600 --> 00:30:49,466
updated our application to

470
00:30:49,488 --> 00:30:53,198
see an improvement in the resiliency. And this basically

471
00:30:53,284 --> 00:30:56,640
here concludes the demo of today.

472
00:30:57,090 --> 00:31:00,846
I now also want to share some best practices to

473
00:31:00,868 --> 00:31:04,510
get started with chaos engineering for your own applications.

474
00:31:04,670 --> 00:31:08,306
So I would recommend you to start with very

475
00:31:08,408 --> 00:31:11,330
small templates in the beginning that maybe only,

476
00:31:11,400 --> 00:31:14,542
as in our demo, only include one single action,

477
00:31:14,606 --> 00:31:18,214
because this allows you to very quickly understand the impact of

478
00:31:18,252 --> 00:31:21,702
a certain template action. The second

479
00:31:21,756 --> 00:31:25,206
tip that I have for you here is testing close to your

480
00:31:25,228 --> 00:31:29,018
production environment. So let's say you have a containerized workload that is

481
00:31:29,104 --> 00:31:32,854
in your staging environment, running on Docker compose, for instance,

482
00:31:32,982 --> 00:31:36,746
and on your production environment, those containers run

483
00:31:36,768 --> 00:31:39,210
on a fully fledged eks cluster.

484
00:31:39,870 --> 00:31:43,694
Here I would recommend you to basically maybe add

485
00:31:43,732 --> 00:31:47,502
a new test environment that is in the architecture more

486
00:31:47,556 --> 00:31:50,942
closer to what you have in production, because else

487
00:31:50,996 --> 00:31:54,590
you won't be able to basically catch

488
00:31:54,670 --> 00:31:58,498
AWS in your staging application architecture that you

489
00:31:58,504 --> 00:32:01,826
can basically apply in production to increase the resiliency of your

490
00:32:01,848 --> 00:32:05,342
application. So always try to test as close

491
00:32:05,416 --> 00:32:08,950
as possible to production as possible, maybe even in production.

492
00:32:10,010 --> 00:32:14,246
The next one is about minimizing the blast radios. So we

493
00:32:14,268 --> 00:32:17,474
mentioned that with the port injection simulator on AWS,

494
00:32:17,522 --> 00:32:21,070
it's possible basically to minimize the blast radios

495
00:32:21,090 --> 00:32:25,066
to two ways. So the first one being with limiting the access that the

496
00:32:25,088 --> 00:32:27,542
service has to your resources. So for instance,

497
00:32:27,606 --> 00:32:31,650
with principle of least privilege in your IAM policies,

498
00:32:31,750 --> 00:32:35,246
you can basically limit the access to the resources that

499
00:32:35,268 --> 00:32:39,470
the fault injection simulator has by, for instance, making sure that only your

500
00:32:39,540 --> 00:32:43,086
application servers and your databases of the staging

501
00:32:43,118 --> 00:32:45,860
environment are accessible by the service.

502
00:32:46,630 --> 00:32:50,322
And also we would recommend you to basically use

503
00:32:50,376 --> 00:32:54,638
the health check capabilities, the emergency brake stops,

504
00:32:54,734 --> 00:32:58,134
to stop an experiment when you

505
00:32:58,172 --> 00:33:01,926
see that you really have degraded when you, for instance, test in

506
00:33:01,948 --> 00:33:05,398
production. To get started

507
00:33:05,484 --> 00:33:09,098
with collecting your first hands on experience, I can recommend this

508
00:33:09,184 --> 00:33:12,940
workshop that we have for you here. With this workshop you basically

509
00:33:13,630 --> 00:33:17,274
have a guided step by step experience where you

510
00:33:17,312 --> 00:33:21,306
basically will learn and understand those different functionalities of

511
00:33:21,328 --> 00:33:24,558
the fault injection service firsthand. And I

512
00:33:24,564 --> 00:33:27,786
would recommend you to check it out either by scanning the QR

513
00:33:27,818 --> 00:33:31,546
code or visiting the URL on the screen. And this concludes

514
00:33:31,578 --> 00:33:34,960
the session. So you see another QR code on the screen.

515
00:33:35,490 --> 00:33:38,666
This is really important. So if you scan this QR

516
00:33:38,698 --> 00:33:42,734
code or visit the URL, you can give us feedback. And we really, really need

517
00:33:42,772 --> 00:33:46,854
your feedback. We want to understand if you like this session and

518
00:33:47,052 --> 00:33:50,198
what we could improve next time. So please take a

519
00:33:50,204 --> 00:33:54,134
minute and fill out the form. It would really mean a lot

520
00:33:54,172 --> 00:33:57,654
to us and we thank you really much and we wish you a great

521
00:33:57,772 --> 00:34:01,330
day ahead and also fun with all the other interesting sessions.

