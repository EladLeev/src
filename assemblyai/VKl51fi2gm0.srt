1
00:00:24,010 --> 00:00:27,506
Everyone, I hope you having a great time in the Kube native conference.

2
00:00:27,698 --> 00:00:31,414
I am Twinkll Sisodia, software engineer at Red Hat and I

3
00:00:31,452 --> 00:00:34,950
work with red Hat partners to build their robust cloud native architectures.

4
00:00:35,530 --> 00:00:39,014
So today we'll be looking into why monitoring is so important.

5
00:00:39,212 --> 00:00:42,882
We'll look closely into what Prometheus is, its usage

6
00:00:43,026 --> 00:00:46,326
and its components. Then we will look into

7
00:00:46,428 --> 00:00:50,040
Grafana, how it can be used to visualize metric data,

8
00:00:50,370 --> 00:00:53,454
and lastly in the demo, I'll walk you through how an

9
00:00:53,492 --> 00:00:57,386
application is deployed on Openshift and how it can be monitored

10
00:00:57,418 --> 00:01:01,130
efficiently. We'll also be using an observability

11
00:01:01,210 --> 00:01:05,460
operator which will deploy Prometheus and alert manager instances for us.

12
00:01:06,470 --> 00:01:09,954
So as we all know how cctv cameras are used for

13
00:01:09,992 --> 00:01:13,074
safety and security purposes. Similar to that,

14
00:01:13,112 --> 00:01:17,346
we have few tools like Prometheus Grafana which is like cctvs

15
00:01:17,378 --> 00:01:20,802
for our system, say your cpu,

16
00:01:20,866 --> 00:01:24,210
your memory utilization reached critical limits,

17
00:01:24,370 --> 00:01:28,350
or your Kubernetes resources like pods deployments

18
00:01:28,450 --> 00:01:31,754
failed. So in this case monitoring can

19
00:01:31,792 --> 00:01:35,862
help and will minimize the risk of server down or unavailability

20
00:01:35,926 --> 00:01:39,654
of resources. And with that it will also help in proactive

21
00:01:39,702 --> 00:01:43,374
management of clusters. So for monitoring, we have few

22
00:01:43,412 --> 00:01:46,350
open source tools and one of them is Prometheus.

23
00:01:46,850 --> 00:01:51,038
Prometheus collects and stores its metrics as time series data,

24
00:01:51,204 --> 00:01:55,042
and it was designed to monitoring highly dynamic container environments like

25
00:01:55,176 --> 00:01:57,330
kubernetes, Docker, swarm,

26
00:01:58,950 --> 00:02:02,450
say there are many servers on which containers are running

27
00:02:02,600 --> 00:02:05,654
and they are all interconnected. Now,

28
00:02:05,692 --> 00:02:09,366
maintaining such complex systems becomes really very challenging and to make

29
00:02:09,388 --> 00:02:12,310
sure that everything runs smoothly without downtimes.

30
00:02:13,610 --> 00:02:17,394
Now imagine having multiple such infrastructures and you have no

31
00:02:17,452 --> 00:02:21,660
idea what's going inside it, either in the hardware level or in the application level,

32
00:02:22,190 --> 00:02:24,550
like errors, response latencies,

33
00:02:24,630 --> 00:02:28,454
overloaded, hardware down, or maybe running out of resources,

34
00:02:28,502 --> 00:02:32,234
et cetera. So this complexity would be minimized

35
00:02:32,282 --> 00:02:35,850
if you have a tool which constantly monitors your resources

36
00:02:36,010 --> 00:02:40,154
and activities, which is happening inside the cluster and alerts

37
00:02:40,202 --> 00:02:44,462
whenever something critical happens. So all this automated

38
00:02:44,526 --> 00:02:47,986
monitoring and alerting is what Prometheus offers as part

39
00:02:48,008 --> 00:02:51,794
of modern DevOps workflow. Now, for us to

40
00:02:51,832 --> 00:02:56,050
enable monitoring, we would need few Prometheus components,

41
00:02:56,210 --> 00:02:58,786
and I'll start with service monitors.

42
00:02:58,978 --> 00:03:03,190
Service monitors specify which services should be monitored.

43
00:03:03,610 --> 00:03:06,898
In place of service monitor. You can also use pod monitors.

44
00:03:07,074 --> 00:03:10,714
The difference is it specify which pods the Prometheus should

45
00:03:10,752 --> 00:03:15,046
monitor. Next we would need Prometheus rules.

46
00:03:15,238 --> 00:03:18,614
Now, Prometheus rule defines recording and alerting

47
00:03:18,662 --> 00:03:21,906
rules. Recording rule allow you to pre compute

48
00:03:21,958 --> 00:03:25,854
frequently used data and alerting rules specify when

49
00:03:25,892 --> 00:03:29,310
should we get the alerts like setting up the thresholds.

50
00:03:30,850 --> 00:03:35,006
Next we'll need alert manager config which specifies

51
00:03:35,118 --> 00:03:38,830
config for the alerts and custom receivers like Slack,

52
00:03:38,910 --> 00:03:42,546
pagerduty, etc. So this is

53
00:03:42,568 --> 00:03:46,614
a short glance of service monitors in

54
00:03:46,652 --> 00:03:50,246
this namespace selector has all the namespaces it will

55
00:03:50,268 --> 00:03:53,670
monitoring. The selector has the label for the app

56
00:03:53,740 --> 00:03:57,486
blue which it will match. And lastly the endpoint

57
00:03:57,538 --> 00:04:01,626
is HTTP port. Next is

58
00:04:01,648 --> 00:04:05,690
the Prometheus rule. It contains alerting and recording rule.

59
00:04:07,150 --> 00:04:10,426
In this example the app request per minute

60
00:04:10,458 --> 00:04:13,774
is greater than 20, so it will send low

61
00:04:13,812 --> 00:04:17,920
load alert and so on, so forth medium high.

62
00:04:19,650 --> 00:04:23,006
Next is the alert manager config secret. It has

63
00:04:23,028 --> 00:04:27,010
the API URL for the slack workspace and it has the channel

64
00:04:27,080 --> 00:04:30,100
name to which all the notifications will be sent.

65
00:04:31,110 --> 00:04:34,706
So so far we have seen how Prometheus works and how

66
00:04:34,728 --> 00:04:38,006
it collects and stores its metrics as time series data.

67
00:04:38,188 --> 00:04:41,890
Now let's see how we can visualize those data effectively on Grafana.

68
00:04:41,970 --> 00:04:45,606
And what's Grafana? Grafana is

69
00:04:45,628 --> 00:04:48,550
an open source software which enables us to query,

70
00:04:48,630 --> 00:04:52,054
visualize, alert on, and also explore metrics,

71
00:04:52,102 --> 00:04:54,730
logs, traces, wherever they are stored.

72
00:04:56,350 --> 00:04:59,578
Grafana provides us with tools to turn

73
00:04:59,664 --> 00:05:03,018
the time series database into insightful graphs

74
00:05:03,034 --> 00:05:06,906
and visualizations. Now these are the Grafana

75
00:05:06,938 --> 00:05:10,462
operator components we would need. So on the Grafana side

76
00:05:10,516 --> 00:05:13,838
we would need Grafana data source and Grafana

77
00:05:13,854 --> 00:05:17,554
dashboard. This is a short glimpse of how

78
00:05:17,752 --> 00:05:21,538
the data source manifest looks like it takes the

79
00:05:21,544 --> 00:05:25,794
Prometheus service URL, it takes the type the database

80
00:05:25,842 --> 00:05:29,446
type as Prometheus. So now this is an

81
00:05:29,468 --> 00:05:33,462
architecture diagram I'll be implementing in the demo how

82
00:05:33,516 --> 00:05:37,582
I monitored an application and got metrics out of it and visualized

83
00:05:37,666 --> 00:05:40,954
on Grafana. So here you can

84
00:05:40,992 --> 00:05:44,870
see we will deploy on an openshift dedicated cluster.

85
00:05:45,030 --> 00:05:48,166
We'll have a blue application in the blue namespace,

86
00:05:48,358 --> 00:05:52,122
an observability operator in the monitor namespace which is responsible

87
00:05:52,186 --> 00:05:55,950
for creating like instances of Prometheus and alert manager.

88
00:05:56,850 --> 00:06:00,766
Here Prometheus will scrape the metrics from the blue app and it

89
00:06:00,788 --> 00:06:04,626
will send alerts to alert manager, which will then

90
00:06:04,728 --> 00:06:08,974
send the alerts to slack as notifications. And lastly,

91
00:06:09,022 --> 00:06:12,658
the Grafana dashboard will visualize metric information in the

92
00:06:12,664 --> 00:06:16,014
form of graphs. So now let's

93
00:06:16,062 --> 00:06:19,654
move on to our demo. On the right hand side you can see the red

94
00:06:19,692 --> 00:06:23,634
hat openshift dedicated cluster, and on the left bottom corner

95
00:06:23,682 --> 00:06:27,266
you can see the slack workspace where all the notifications and alerts

96
00:06:27,298 --> 00:06:31,274
will be coming. So on the openshift dedicated cluster we have

97
00:06:31,312 --> 00:06:35,370
two namespaces, one for the blue application which is deployed already,

98
00:06:35,520 --> 00:06:38,758
and the other for the observability operator and its instance

99
00:06:38,854 --> 00:06:42,666
which is up and running already. Next I'm

100
00:06:42,698 --> 00:06:46,698
going to create the Prometheus components like service monitors, Prometheus rules,

101
00:06:46,714 --> 00:06:51,818
and alert manager. So I'll

102
00:06:51,994 --> 00:06:53,970
create the service monitors.

103
00:06:56,790 --> 00:07:00,574
Service monitor is up. I'll create the Prometheus

104
00:07:00,622 --> 00:07:05,794
rules. After that I'll create alert

105
00:07:05,842 --> 00:07:10,214
manager secret okay

106
00:07:10,252 --> 00:07:13,986
so the Prometheus components are in place. Next I'll

107
00:07:14,018 --> 00:07:17,994
create the cluster role and cluster role bindings so that

108
00:07:18,032 --> 00:07:22,006
the monitor namespace will have the permission to scrape

109
00:07:22,038 --> 00:07:24,010
the metrics from the blue namespace.

110
00:07:31,650 --> 00:07:35,550
The cluster role blue view is created. Next I'll

111
00:07:36,130 --> 00:07:43,766
create the cluster role binding so

112
00:07:43,788 --> 00:07:46,410
the role binding is now created.

113
00:07:46,910 --> 00:07:50,054
I'll port forward the Prometheus pod

114
00:07:50,182 --> 00:07:53,500
and let's see how the Prometheus dashboard looks like.

115
00:08:01,730 --> 00:08:05,722
So this is the Prometheus dashboard. If I navigate to alerts

116
00:08:05,866 --> 00:08:10,054
I can see all the lets like high medium, low. If I navigate

117
00:08:10,122 --> 00:08:13,486
to rules I can see the recording rules

118
00:08:13,518 --> 00:08:17,506
and the alerting rules. And lastly if

119
00:08:17,528 --> 00:08:20,834
I go to targets I can see the blue application which we

120
00:08:20,872 --> 00:08:25,266
have deployed recently up. Now let's trigger

121
00:08:25,298 --> 00:08:29,110
this blue application and see how we get the alerts on slack.

122
00:08:33,050 --> 00:08:40,150
So I'll created

123
00:08:41,630 --> 00:08:45,466
and curl it for at

124
00:08:45,488 --> 00:08:46,860
least 25 times.

125
00:08:59,530 --> 00:09:02,954
Youll once the threshold is met we can see the

126
00:09:02,992 --> 00:09:05,920
alerts popping up on the Slack channel.

127
00:09:07,250 --> 00:09:11,310
This shouldn't take time, should be like 25 to 30 seconds.

128
00:09:38,740 --> 00:09:42,604
So you can clearly see that the alerts are getting triggered low load,

129
00:09:42,652 --> 00:09:46,916
medium load. So on expanding one of these alerts we

130
00:09:46,938 --> 00:09:50,916
can see the metadata like where this alert is coming from. Like the

131
00:09:50,938 --> 00:09:53,988
alert name, the container name, endpoint IP address,

132
00:09:54,154 --> 00:09:58,230
the namespace path et how.

133
00:09:58,780 --> 00:10:01,940
So this is a small use case of how an organization

134
00:10:02,020 --> 00:10:05,236
can use all these monitoring tools. Like Prometheus,

135
00:10:05,348 --> 00:10:09,412
we can use slack alert manager to enhances

136
00:10:09,476 --> 00:10:12,968
the workflow and this is how one can minimize the risk

137
00:10:12,984 --> 00:10:16,812
of downtime. So far we have seen how we used Prometheus and

138
00:10:16,866 --> 00:10:20,496
alert manager to send alerts to slack. Now let's see how

139
00:10:20,518 --> 00:10:24,096
we use that data to turn it into insightful graphs and

140
00:10:24,118 --> 00:10:27,644
visualizations using Grafana. Let's move to operator

141
00:10:27,692 --> 00:10:30,560
hub and install Grafana operator.

142
00:10:34,040 --> 00:10:40,574
I'll install it into the monitor namespace and

143
00:10:40,612 --> 00:10:43,726
once the Grafana operator is installed I'll go forward and

144
00:10:43,748 --> 00:10:47,154
create its instance and data source. And then we'll port forward

145
00:10:47,192 --> 00:10:50,820
the Grafana pod to look how the dashboard looks like.

146
00:11:02,740 --> 00:11:05,120
So the Grafana operator is installed,

147
00:11:06,980 --> 00:11:11,140
I'll go forward and create its instance.

148
00:11:18,270 --> 00:11:22,630
The instance is created. Next I'll create the Grafana

149
00:11:22,790 --> 00:11:30,644
data source and

150
00:11:30,682 --> 00:11:34,772
the data source is created. Now I'll see if the pods are

151
00:11:34,826 --> 00:11:37,910
up and running or not. It is not.

152
00:11:51,170 --> 00:11:52,960
Okay, now it's up and running.

153
00:12:00,150 --> 00:12:04,914
So I'll port forward the Grafana pod at

154
00:12:04,952 --> 00:12:14,114
port nine. At port 3000 it's

155
00:12:14,162 --> 00:12:17,846
put forwarded. Let's put forward to

156
00:12:17,868 --> 00:12:21,366
3000 and sign in with

157
00:12:21,388 --> 00:12:25,898
the same username and password I provided in

158
00:12:25,904 --> 00:12:27,370
the Grafana instance.

159
00:12:31,470 --> 00:12:34,842
Now before proceeding, I'll just quickly confirm if my data

160
00:12:34,896 --> 00:12:38,334
source is working. See when test

161
00:12:38,372 --> 00:12:42,586
my data source is working fine. I'll navigate

162
00:12:42,618 --> 00:12:46,186
to import and quickly import my sample dashboard

163
00:12:46,218 --> 00:12:49,600
which I created. Now you can create your own or

164
00:12:49,990 --> 00:12:52,660
just import it from the Grafana website.

165
00:12:53,990 --> 00:12:59,074
I'll rename it to blue dashboard and import it here.

166
00:12:59,112 --> 00:13:02,766
You can see we are getting different metrics. Starting with alerts.

167
00:13:02,798 --> 00:13:06,450
We can see which alerts are being triggered recently.

168
00:13:06,530 --> 00:13:10,546
So high load, low load and medium load are alerts

169
00:13:10,578 --> 00:13:14,466
which are triggered. What was the alert state, which container

170
00:13:14,498 --> 00:13:16,810
it was, what was the endpoint, et cetera.

171
00:13:17,710 --> 00:13:21,046
Next we see that blue request per minute

172
00:13:21,078 --> 00:13:24,534
metrics. So this metrics show that how many requests

173
00:13:24,582 --> 00:13:28,426
were there per minute for the blue application. Apart from that

174
00:13:28,448 --> 00:13:30,750
we can see response status, process,

175
00:13:30,820 --> 00:13:34,906
cpu, and lastly the up metrics. The up metrics

176
00:13:34,938 --> 00:13:38,574
show that how many containers are up currently. So there are

177
00:13:38,692 --> 00:13:42,594
one out manager, one for blue application and one for

178
00:13:42,632 --> 00:13:46,226
Prometheus which are up and running. So this

179
00:13:46,248 --> 00:13:49,666
is how you can use a data source like

180
00:13:49,688 --> 00:13:54,150
Prometheus and convert the data into insightful graphs and visualizations.

181
00:13:54,570 --> 00:13:57,874
This will help can sre to be mindful

182
00:13:57,922 --> 00:14:01,586
of all the resources and all the costs involved.

183
00:14:01,778 --> 00:14:04,994
And with that it will also help organizations to minimize

184
00:14:05,042 --> 00:14:06,090
their downtime.

185
00:14:07,870 --> 00:14:10,890
And that concludes my presentation.

186
00:14:11,710 --> 00:14:15,034
Just to summarize what we have discussed so far.

187
00:14:15,232 --> 00:14:18,794
So we have talked about the importance of monitoring.

188
00:14:18,922 --> 00:14:23,230
We have discussed about Prometheus Grafana components involved

189
00:14:23,810 --> 00:14:27,870
in the demo. We deployed an app, we deployed observability operator

190
00:14:28,210 --> 00:14:31,950
which installed Prometheus and alert manager. And finally

191
00:14:32,020 --> 00:14:36,138
we sent alerts to slack. Then we deployed

192
00:14:36,234 --> 00:14:39,534
Grafana operator and its components. And lastly we

193
00:14:39,572 --> 00:14:43,320
imported custom dashboards to see insightful graphs us.

194
00:14:46,120 --> 00:14:49,396
So here I would like to thank everyone. I hope

195
00:14:49,418 --> 00:14:52,304
you all enjoyed it. If you want to get connected,

196
00:14:52,352 --> 00:14:55,524
I'm there on LinkedIn and if

197
00:14:55,562 --> 00:14:58,836
anyone wants to do like a hands on you can visit my

198
00:14:58,858 --> 00:15:03,152
GitHub repository. It has all the in depth details.

199
00:15:03,296 --> 00:15:06,690
Read me for that. So yeah, thanks everyone.

