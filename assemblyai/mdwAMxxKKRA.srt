1
00:02:01,460 --> 00:02:05,452
Hi, my name is Benjamin. I'm the co founder and CEO of Steadybit

2
00:02:05,516 --> 00:02:09,040
and welcome to my talk reliability in the face of uncertainty.

3
00:02:09,620 --> 00:02:12,804
If you would like to reach out to after the talk, please feel free via

4
00:02:12,852 --> 00:02:17,016
Twitter or LinkedIn. I'm happy to answer all your questions and hopefully maybe you

5
00:02:17,038 --> 00:02:20,410
will provide me some feedback. So let's get started.

6
00:02:23,580 --> 00:02:26,190
Let's start the talk with important questions.

7
00:02:27,520 --> 00:02:31,004
Question what do we strive for? And what goals does each of

8
00:02:31,042 --> 00:02:33,820
us pursue every day? Or in other words,

9
00:02:33,970 --> 00:02:37,084
what's the mission of software development? Because I assume that

10
00:02:37,122 --> 00:02:40,672
we are all part of the software development process.

11
00:02:40,806 --> 00:02:44,352
We are maybe developers, sres, people who are running

12
00:02:44,406 --> 00:02:48,176
the systems, operating the systems. So we are all part of it and we

13
00:02:48,198 --> 00:02:51,796
are all on the mission. And the

14
00:02:51,818 --> 00:02:55,952
mission is continuously improve and deliver a software solution

15
00:02:56,016 --> 00:02:58,996
that reliably delivers value to its customers.

16
00:02:59,178 --> 00:03:02,328
And that's the important part. All of us here have a

17
00:03:02,334 --> 00:03:06,148
common goal and we work on it every day. Happy and satisfied

18
00:03:06,244 --> 00:03:10,008
customers who use our software. Our customers

19
00:03:10,094 --> 00:03:13,460
have high expectations from the software they use and therefore

20
00:03:13,540 --> 00:03:17,100
from us. And from the point of view of our customers,

21
00:03:17,250 --> 00:03:20,764
the software must always work and they are not interested

22
00:03:20,882 --> 00:03:24,312
in the technical complexity that this requires

23
00:03:24,376 --> 00:03:26,300
or even in outages.

24
00:03:30,340 --> 00:03:31,760
What is reliable?

25
00:03:32,980 --> 00:03:36,684
One very important characteristic is reliability

26
00:03:36,812 --> 00:03:40,244
in our system. And yeah, let's therefore take

27
00:03:40,282 --> 00:03:44,096
a look at this characteristic and see what's

28
00:03:44,128 --> 00:03:49,636
the definition. What's behind a

29
00:03:49,658 --> 00:03:53,512
software system is gliable if it's consistently good

30
00:03:53,566 --> 00:03:55,480
in quality and performance.

31
00:03:56,300 --> 00:04:00,024
So it's important to note that this characteristic must

32
00:04:00,062 --> 00:04:03,816
exist from the end user point of view. We all know that our systems

33
00:04:03,848 --> 00:04:07,900
are in a constantly changing environment,

34
00:04:09,040 --> 00:04:12,924
and as a customer, I expect a result from the

35
00:04:12,962 --> 00:04:16,750
system that fits my request in a reasonable time.

36
00:04:17,060 --> 00:04:20,224
And if the system does not give me an answer,

37
00:04:20,342 --> 00:04:23,872
gives me an error, or makes me events wait too long,

38
00:04:24,006 --> 00:04:27,616
it does not fulfill these characteristics and

39
00:04:27,798 --> 00:04:31,044
I as a customer will continue, will move on to

40
00:04:31,082 --> 00:04:33,030
another vendor, maybe.

41
00:04:37,000 --> 00:04:40,512
But if the system is able to answer me in a reasonable

42
00:04:40,576 --> 00:04:43,704
time and with a suitable result, I as

43
00:04:43,742 --> 00:04:49,784
a customer, yeah, I build up trust and

44
00:04:49,822 --> 00:04:54,080
I use this system, let's say gladly and repeatedly,

45
00:04:54,260 --> 00:04:56,270
because I'm able to trust the system.

46
00:04:58,000 --> 00:05:01,788
So in a summary, we as customers,

47
00:05:01,954 --> 00:05:05,180
we trust the system when it's consistently good in quality and

48
00:05:05,250 --> 00:05:06,220
performance.

49
00:05:09,540 --> 00:05:13,216
Today's systems what are the systems we need to

50
00:05:13,238 --> 00:05:17,680
build today to meet our customers expectations?

51
00:05:20,280 --> 00:05:25,104
Let's take a look at today's system. I have brought you two illustrations

52
00:05:25,152 --> 00:05:29,524
to diagrams of a complex system where

53
00:05:29,562 --> 00:05:33,156
it's easy to see how many dependencies exist in

54
00:05:33,178 --> 00:05:36,336
a microservice architecture. And from this quickly. Yeah,

55
00:05:36,378 --> 00:05:39,992
comes the question, why is it so complex? Why do we need to build

56
00:05:40,046 --> 00:05:43,756
it so complex? The answer is

57
00:05:43,778 --> 00:05:47,148
yeah, we need to meet different

58
00:05:47,234 --> 00:05:51,244
requirements. Like we need to handle high load, the system

59
00:05:51,362 --> 00:05:54,430
and the services within,

60
00:05:55,280 --> 00:05:59,788
they need to be resilient and the load needs to be distributed.

61
00:05:59,884 --> 00:06:04,160
We need to be able to do rolling updates under load

62
00:06:04,500 --> 00:06:08,112
and maybe doing load peaks. Yeah, we also

63
00:06:08,166 --> 00:06:12,368
need to create new instances. They must be added automatically

64
00:06:12,464 --> 00:06:16,080
and then cleared up afterwards.

65
00:06:16,240 --> 00:06:19,430
If not, your cloud provider is getting happy but not you.

66
00:06:20,940 --> 00:06:24,996
This resulting dynamic must be considered

67
00:06:25,028 --> 00:06:28,580
during development and appropriate

68
00:06:28,740 --> 00:06:32,824
precautions must be taken. So however, the two

69
00:06:32,862 --> 00:06:36,124
diagrams also only show a part

70
00:06:36,162 --> 00:06:39,592
of it and obscure the underlying layers.

71
00:06:39,736 --> 00:06:43,212
We add with like kubernetes or cloud

72
00:06:43,266 --> 00:06:47,212
providers or maybe with third party providers

73
00:06:47,276 --> 00:06:50,130
for let's say authentication or other things.

74
00:06:50,500 --> 00:06:54,320
That's something we have to handle and

75
00:06:54,390 --> 00:06:58,364
take care. I really assume that everyone

76
00:06:58,502 --> 00:07:01,968
here can share one or two or maybe more positive

77
00:07:02,064 --> 00:07:06,400
and also negative experience with today's

78
00:07:06,480 --> 00:07:07,940
complex systems.

79
00:07:11,240 --> 00:07:14,776
So with this in mind, with this knowledge, this experience we

80
00:07:14,798 --> 00:07:17,672
all have, I really like the quote from Dr.

81
00:07:17,726 --> 00:07:20,680
Richard Cook, which is very applicable.

82
00:07:21,260 --> 00:07:25,660
It's not surprising that your system sometimes fails.

83
00:07:26,640 --> 00:07:30,110
What is surprising that it ever works at all.

84
00:07:33,600 --> 00:07:37,120
And yeah, we haven't solved

85
00:07:38,740 --> 00:07:42,524
the requirements of our customers and the associated

86
00:07:42,572 --> 00:07:46,176
expectations. They make it very difficult for us to

87
00:07:46,198 --> 00:07:50,416
meet them. And why I'm claiming

88
00:07:50,448 --> 00:07:54,084
it such hard. Yeah, let's take a look.

89
00:07:54,282 --> 00:07:57,824
Let's say look at the incidents, the incidents we are all dealing

90
00:07:57,872 --> 00:08:02,168
with every day, maybe. And there was a report done by

91
00:08:02,334 --> 00:08:06,660
the people at fire hydrants. They have analyzed about fifty k of incidents.

92
00:08:06,820 --> 00:08:10,760
And you can see even in small organizations

93
00:08:11,340 --> 00:08:14,956
there are ten incidents each month they

94
00:08:14,978 --> 00:08:19,212
have to deal with because of something is

95
00:08:19,266 --> 00:08:22,632
not working normal if the organization

96
00:08:22,696 --> 00:08:26,770
is growing. Also the complexity and the system

97
00:08:27,620 --> 00:08:31,184
is increasing and therefore the

98
00:08:31,222 --> 00:08:34,736
number of incidents as well. And you can see that the

99
00:08:34,758 --> 00:08:38,896
numbers are getting up on bigger

100
00:08:38,928 --> 00:08:42,736
companies as well. And let's take a look at the time we are spending

101
00:08:42,768 --> 00:08:46,784
on each incident. So it's

102
00:08:46,912 --> 00:08:50,532
about 24 hours in average for each

103
00:08:50,586 --> 00:08:53,988
incident. So we are spending 24 hours from creating an incident,

104
00:08:54,084 --> 00:08:57,716
to analyze the incident, to find a fix, to deploy

105
00:08:57,748 --> 00:09:01,720
the fix, and to close the incident. And that's just the average

106
00:09:02,060 --> 00:09:05,692
time we are spending on. And it does not mean that

107
00:09:05,746 --> 00:09:09,564
our customer is not able to work with our product for 25

108
00:09:09,602 --> 00:09:13,452
hours. No, it's the time we are spending from

109
00:09:13,506 --> 00:09:15,740
opening to closing the incident.

110
00:09:19,440 --> 00:09:22,584
Yeah, famous sentence

111
00:09:22,632 --> 00:09:26,588
by Vana Fugas. Everything fails all the time and that's

112
00:09:26,604 --> 00:09:30,630
something we all have realized in our daily life.

113
00:09:33,640 --> 00:09:37,952
So we should ask, or we need to ask ourselves, what's normal.

114
00:09:38,096 --> 00:09:41,880
And under these conditions I described earlier,

115
00:09:43,580 --> 00:09:47,268
they are providing the impression of everything is being chaotic,

116
00:09:47,364 --> 00:09:50,340
it's unpredictable, and we must ourselves,

117
00:09:50,500 --> 00:09:52,200
is this really normal?

118
00:09:54,640 --> 00:09:58,444
And failures are normal. Failures are normal and are part of our

119
00:09:58,482 --> 00:10:02,510
daily life. But errors or failures are much more.

120
00:10:02,880 --> 00:10:06,072
They contain valuable information from which we can

121
00:10:06,146 --> 00:10:09,744
derive knowledge and bring

122
00:10:09,782 --> 00:10:13,196
up improvements in our systems. We can improve our systems

123
00:10:13,228 --> 00:10:17,296
based on failures. But I would

124
00:10:17,398 --> 00:10:20,720
go one step further and interpret

125
00:10:20,880 --> 00:10:24,852
failures as the attempt. How our

126
00:10:24,906 --> 00:10:28,868
systems want to get in touch with us. They would like to tell us something.

127
00:10:29,034 --> 00:10:32,384
Our systems want to point

128
00:10:32,442 --> 00:10:35,944
us to problems where we should optimize, where we should

129
00:10:35,982 --> 00:10:40,010
fix something. So it's like a communication channel from our systems

130
00:10:40,460 --> 00:10:44,188
and the system is calling for help. Please support me.

131
00:10:44,274 --> 00:10:47,230
Here's something not working. That's something we can do better.

132
00:10:50,080 --> 00:10:53,256
And yeah, we are working under chaotic conditions.

133
00:10:53,448 --> 00:10:57,760
And why chaotic? Because we

134
00:10:57,830 --> 00:11:01,024
are not able to know when

135
00:11:01,062 --> 00:11:05,040
the next failure occurs. And under such

136
00:11:05,110 --> 00:11:09,232
chaotic conditions, chaos engineering is necessary

137
00:11:09,376 --> 00:11:13,300
to improve our systems and train them to deal with failures

138
00:11:15,720 --> 00:11:19,440
and failures. Yeah, failures are the foundation failures.

139
00:11:19,600 --> 00:11:23,128
We know about our starting point and

140
00:11:23,214 --> 00:11:26,504
they help us to develop the

141
00:11:26,542 --> 00:11:30,772
appropriate test cases. So we can use a failure and transform

142
00:11:30,836 --> 00:11:33,800
a failure into a test case in an experiment.

143
00:11:37,280 --> 00:11:38,990
What can we do exactly?

144
00:11:41,520 --> 00:11:45,832
We need to do it proactive. We need to proactively improve

145
00:11:45,896 --> 00:11:47,970
the reliability of our system.

146
00:11:49,140 --> 00:11:53,632
And we need to ask specific questions before

147
00:11:53,686 --> 00:11:56,976
we are in production. And we need to know the answers before we are going

148
00:11:56,998 --> 00:12:00,500
into production. For example, here are three examples.

149
00:12:01,560 --> 00:12:05,510
The first question maybe is can our users continue

150
00:12:06,040 --> 00:12:09,924
hoping while we are doing a routing update? That's something

151
00:12:09,962 --> 00:12:12,648
you should figure out before you're in production, if you are able to do a

152
00:12:12,654 --> 00:12:16,170
routing update under load or not. The second example,

153
00:12:16,700 --> 00:12:20,212
will our service start when the service dependencies

154
00:12:20,276 --> 00:12:26,136
are unreachable? That's something we

155
00:12:26,158 --> 00:12:29,644
can check and we can validate if the

156
00:12:29,682 --> 00:12:32,988
service is restarting or not. Or do we need to take it in a different

157
00:12:33,074 --> 00:12:35,790
order? If so, it's not a good place,

158
00:12:36,100 --> 00:12:38,850
not a good choice. Or the last example,

159
00:12:39,860 --> 00:12:44,000
can our users buy products in the event of

160
00:12:44,070 --> 00:12:47,968
cloud sound outage? Maybe you are running on a

161
00:12:47,974 --> 00:12:51,488
specific cloud provider and sometimes there are sound outages.

162
00:12:51,664 --> 00:12:55,184
Are you able to shift the load in a different zone?

163
00:12:55,232 --> 00:12:58,644
Is there something done automatically? Have you checked it?

164
00:12:58,682 --> 00:13:01,430
Have you tested? So take care.

165
00:13:04,860 --> 00:13:08,744
Now, let's take an

166
00:13:08,782 --> 00:13:12,040
example. Let's take a look at this showcase. It's a

167
00:13:12,110 --> 00:13:16,296
kubernetes based example. There's a microservice

168
00:13:16,328 --> 00:13:19,848
architecture inside, there are multiple services connected

169
00:13:19,864 --> 00:13:24,012
to each other. And now let's identify our

170
00:13:24,066 --> 00:13:28,096
key services. So we need to know, what are our key players, what are

171
00:13:28,118 --> 00:13:32,224
our services we have to

172
00:13:32,422 --> 00:13:35,904
deal with. We have to take care. You can see there

173
00:13:35,942 --> 00:13:39,590
are some lines in between, there are some connections. So let's check

174
00:13:40,600 --> 00:13:43,844
on a deeper level. You can see the entry service is

175
00:13:43,882 --> 00:13:47,350
a gateway. The gateway is connected to four internal services.

176
00:13:50,040 --> 00:13:53,584
That's, of course, one of our key services because it's the entry point

177
00:13:53,642 --> 00:13:57,236
in front of the gateway. There's a load balancer, and the load balancer is handling

178
00:13:57,268 --> 00:14:01,080
the load. On all the instances from the gateway, in this example,

179
00:14:01,150 --> 00:14:04,924
just two. But what's going on

180
00:14:04,962 --> 00:14:07,868
inside after the gateway was called?

181
00:14:08,034 --> 00:14:11,880
So there is hot deals, fashion and toys,

182
00:14:12,040 --> 00:14:14,940
and they are all connected to one specific service,

183
00:14:15,010 --> 00:14:18,416
the inventory service. So that's a key services,

184
00:14:18,518 --> 00:14:22,364
because if the inventory is not working or maybe responding

185
00:14:22,412 --> 00:14:27,040
slow, there's an impact, maybe a high impact

186
00:14:27,780 --> 00:14:30,808
for the hot deals, toys and bestseller.

187
00:14:30,924 --> 00:14:34,950
And that's something we need to know. It's not something. Let's say,

188
00:14:35,800 --> 00:14:39,396
let's hope everything is fine. No, hope is not a strategy. So we have

189
00:14:39,418 --> 00:14:43,336
to check it upfront. How can we

190
00:14:43,358 --> 00:14:44,760
do this proactive?

191
00:14:47,660 --> 00:14:51,064
We need to test under real conditions, early as possible.

192
00:14:51,182 --> 00:14:55,150
And to be honest, happy pass testing is not enough.

193
00:14:56,080 --> 00:14:59,612
We need to improve the confidence in our

194
00:14:59,666 --> 00:15:03,880
systems. So we need to test under real conditions,

195
00:15:03,960 --> 00:15:07,520
like we are able to see in production, because production is not a happy place.

196
00:15:07,590 --> 00:15:10,336
It's not everything is fine in production. No.

197
00:15:10,518 --> 00:15:13,568
And this is where experimentation comes in.

198
00:15:13,654 --> 00:15:17,324
And I believe this is the true reason why chaos engineering exists,

199
00:15:17,452 --> 00:15:20,864
to check what's going on under normal

200
00:15:20,912 --> 00:15:22,340
conditions in production.

201
00:15:25,640 --> 00:15:27,620
Let's talk about the reconditions.

202
00:15:29,240 --> 00:15:33,016
Let's go one step deeper, and let's use a

203
00:15:33,038 --> 00:15:37,464
very technical example. Based on the already known showcase from

204
00:15:37,582 --> 00:15:38,810
some slides ago.

205
00:15:43,660 --> 00:15:46,750
We know that the inventory service is one of our key services,

206
00:15:47,440 --> 00:15:50,844
and therefore we need to know, how is it called

207
00:15:50,882 --> 00:15:54,652
in advanced, what effects a non normal behavior of the

208
00:15:54,786 --> 00:15:56,690
inventory service will have.

209
00:15:57,940 --> 00:16:01,356
We know from our monitoring, and also from our load

210
00:16:01,388 --> 00:16:04,972
tests that the average response time of the inventory

211
00:16:05,036 --> 00:16:09,144
service is about 25 milliseconds. Under these conditions,

212
00:16:09,212 --> 00:16:12,390
we have a reliable behavior in the whole system.

213
00:16:12,840 --> 00:16:16,160
That's what we already know from production,

214
00:16:16,240 --> 00:16:20,020
what we are able to see in production and in our monitoring solution,

215
00:16:25,040 --> 00:16:28,124
but also from our monitoring, we,

216
00:16:28,162 --> 00:16:32,316
or I, have also learned that there

217
00:16:32,338 --> 00:16:35,916
were some spikes in the response time of

218
00:16:35,938 --> 00:16:39,936
the inventory service. And this data tells us, okay, we had some

219
00:16:39,958 --> 00:16:42,720
response times up to 500 milliseconds,

220
00:16:43,220 --> 00:16:47,136
and that's knowledge, or that's data we

221
00:16:47,158 --> 00:16:51,536
can use. So let's use this knowledge and create an experiment to simulate

222
00:16:51,568 --> 00:16:55,024
the high response time of the inventory service by injecting

223
00:16:55,072 --> 00:16:58,020
latency with up to 500 milliseconds.

224
00:16:59,160 --> 00:17:02,756
What we don't know is what impact

225
00:17:02,788 --> 00:17:07,400
the high response times have and whether the service that

226
00:17:07,470 --> 00:17:11,464
depend on them can handle it. That is

227
00:17:11,502 --> 00:17:15,256
exactly what we are now testing. And we will run

228
00:17:15,278 --> 00:17:19,484
an experiment in a preproduction environment and proactively test

229
00:17:19,682 --> 00:17:23,100
how our system behaves under such conditions.

230
00:17:25,840 --> 00:17:29,920
Let's take a look at the experiment,

231
00:17:30,500 --> 00:17:33,650
what experiment we are running. So,

232
00:17:35,060 --> 00:17:38,176
it's a complex failure scenario, and we

233
00:17:38,198 --> 00:17:42,004
can recreate such scenarios with the right

234
00:17:42,042 --> 00:17:45,780
tools, and we can generate knowledge

235
00:17:46,760 --> 00:17:49,952
about how our system is reacting.

236
00:17:50,016 --> 00:17:53,304
So you can see there's a gray element, it's just a wait step. And then

237
00:17:53,342 --> 00:17:56,628
the blue line is a load test, so we can reuse,

238
00:17:56,644 --> 00:18:00,372
and we should reuse our existing load tests

239
00:18:00,516 --> 00:18:03,864
to execute them. But in combination with

240
00:18:03,902 --> 00:18:07,416
some bad behavior which has been injected

241
00:18:07,448 --> 00:18:10,812
during the execution, and that's no longer happy pass

242
00:18:10,866 --> 00:18:13,820
testing, it's like testing under real conditions.

243
00:18:14,560 --> 00:18:18,448
The yellow ones are verification steps. So, for example,

244
00:18:18,614 --> 00:18:21,728
check if every instance is up and running, if a

245
00:18:21,734 --> 00:18:24,210
specific endpoint is responding or not.

246
00:18:25,620 --> 00:18:27,410
And there's a special one.

247
00:18:29,560 --> 00:18:32,836
We need to define the expected behavior of

248
00:18:32,858 --> 00:18:36,400
the system. And if it's not given, the experiment

249
00:18:36,480 --> 00:18:40,084
fails, because the reliability is not given.

250
00:18:40,122 --> 00:18:43,624
So the system is not working as it should.

251
00:18:43,822 --> 00:18:47,944
And that's something we can test. We can write

252
00:18:47,982 --> 00:18:51,770
a test case for it, and the experiment will fail if this check

253
00:18:52,940 --> 00:18:54,330
is not true.

254
00:18:57,280 --> 00:19:01,016
Let's take a look at the experimentation run. So, the experimentation,

255
00:19:01,128 --> 00:19:03,224
the experiment was successful,

256
00:19:03,272 --> 00:19:07,256
successfully, holy. The experiment was successfully

257
00:19:07,368 --> 00:19:10,816
executed, and our system proved that

258
00:19:10,838 --> 00:19:14,684
it can handle short term, increased response times by the inventory

259
00:19:14,732 --> 00:19:15,330
service.

260
00:19:18,660 --> 00:19:22,048
If we now encounter these conditions in production,

261
00:19:22,144 --> 00:19:25,940
we know that our system can handle them, so we are now more

262
00:19:26,010 --> 00:19:29,792
safe. We have tested a specific, complex scenario

263
00:19:29,856 --> 00:19:32,932
from the past in a pre proactive environment,

264
00:19:33,076 --> 00:19:36,810
and the system is able to survive, which is good.

265
00:19:40,380 --> 00:19:41,450
Next example.

266
00:19:44,400 --> 00:19:48,190
Now, the recondition is something

267
00:19:48,800 --> 00:19:52,556
we also found in the past data. So in the failure data of

268
00:19:52,578 --> 00:19:56,232
the past, another failure appears in our monitoring solution.

269
00:19:56,376 --> 00:19:59,890
And that hit our system quite hard.

270
00:20:00,420 --> 00:20:04,828
Now we need to transform this failure into an experiment to validate

271
00:20:04,924 --> 00:20:08,770
if we can handle the situation or not. And the error was

272
00:20:09,140 --> 00:20:13,604
this time for the hotdeal service. So the hot yield service was

273
00:20:13,642 --> 00:20:17,344
not responding in a specific time, so there was a delay.

274
00:20:17,392 --> 00:20:21,284
And this delay was limited in two zones from our

275
00:20:21,322 --> 00:20:24,456
cloud provider followed by a DNS outage in

276
00:20:24,478 --> 00:20:28,440
our Kubernetes cluster. That was the failure scenario.

277
00:20:30,300 --> 00:20:33,412
Now recreate this failure scenario,

278
00:20:33,476 --> 00:20:37,064
let's rebuild it and we will inject the failure

279
00:20:37,112 --> 00:20:40,316
events that occurred. And yeah, we will also check if

280
00:20:40,338 --> 00:20:43,576
the Lisa had behavior is provided

281
00:20:43,608 --> 00:20:46,956
or not. Again, the yellow one is

282
00:20:46,978 --> 00:20:50,528
a check verification. A specific endpoint is called

283
00:20:50,694 --> 00:20:54,060
and it needs to be reachable

284
00:20:54,140 --> 00:20:57,596
and needs to respond with a 200 and HTTP status

285
00:20:57,628 --> 00:21:02,660
code. If not there's

286
00:21:03,320 --> 00:21:06,868
a success rate we can tweak and we would like to

287
00:21:06,874 --> 00:21:10,368
get 100%. Again, the gray

288
00:21:10,384 --> 00:21:14,312
one is a wait step and then in the second line there is a green

289
00:21:14,366 --> 00:21:18,296
one where we will inject latency in

290
00:21:18,318 --> 00:21:22,090
two specific zones for the hot yield service only,

291
00:21:22,620 --> 00:21:25,956
followed by in the third line by an

292
00:21:25,998 --> 00:21:29,464
DNS outage for our Kubernetes cluster. So no DNS

293
00:21:29,512 --> 00:21:33,230
communication inside of this Kubernetes cluster was possible.

294
00:21:34,080 --> 00:21:37,660
That is what the data was telling us from the monitoring.

295
00:21:40,980 --> 00:21:44,210
Now let's execute. And you can see, okay,

296
00:21:45,940 --> 00:21:49,776
a lot of red elements, so something went wrong and you can see in

297
00:21:49,798 --> 00:21:53,316
the top corner the experiment failed. So it's clear that our system is

298
00:21:53,338 --> 00:21:56,756
once again unable to handle this failure and the

299
00:21:56,778 --> 00:22:00,628
behavior we expect is not given and our customers are not able

300
00:22:00,794 --> 00:22:04,616
to use our service. Now it's up to

301
00:22:04,638 --> 00:22:08,760
us to adapt our service to deal with the failure,

302
00:22:09,580 --> 00:22:12,520
but we need to mitigate the impact.

303
00:22:13,500 --> 00:22:17,052
And as we said in the beginning, failures are normal and will always

304
00:22:17,106 --> 00:22:20,716
occur, but we have it

305
00:22:20,738 --> 00:22:24,040
in our hands to control the efforts. And failures

306
00:22:24,120 --> 00:22:28,000
are of often failure and our system

307
00:22:28,070 --> 00:22:30,050
can learn to deal with it.

308
00:22:32,980 --> 00:22:38,576
When should we run these experiments as

309
00:22:38,598 --> 00:22:41,892
an integral part of our software development process, is the answer. And this

310
00:22:41,946 --> 00:22:45,632
question when should we run these tests?

311
00:22:45,776 --> 00:22:49,904
Is not new and used to arise with the unit test integration

312
00:22:49,952 --> 00:22:53,012
tests as well. But over time we

313
00:22:53,146 --> 00:22:56,296
and the industry have started to run these best before we

314
00:22:56,318 --> 00:22:59,940
check in code or in the case of integration tests

315
00:23:00,100 --> 00:23:03,770
before we merge or cut a new version of our system.

316
00:23:04,480 --> 00:23:08,510
So I look at an experimentation as

317
00:23:08,880 --> 00:23:13,064
let's say real end to end integration test based on production

318
00:23:13,112 --> 00:23:16,884
situations, conditions that we want to be resilient

319
00:23:16,952 --> 00:23:26,144
to, that we would like to be able to handle or mitigate with

320
00:23:26,182 --> 00:23:29,756
this described approach of creating experiments

321
00:23:29,788 --> 00:23:34,100
and running them in a preproduction environment. We test the

322
00:23:34,250 --> 00:23:37,856
reliability of our systems and this results

323
00:23:37,888 --> 00:23:42,420
in a list of experiments. So we are creating continuously new experiments

324
00:23:43,160 --> 00:23:46,824
that we can integrate into our deployment process and run

325
00:23:46,862 --> 00:23:49,992
them continuously after a new deployment to

326
00:23:50,046 --> 00:23:53,368
check how big is the risk we are taking? And is the

327
00:23:53,374 --> 00:23:57,340
system still able to survive this past incident or not?

328
00:23:57,490 --> 00:24:00,540
Or is it able to survive a specific scenario?

329
00:24:01,440 --> 00:24:05,340
And yeah, to make it an integral part of your development process,

330
00:24:05,410 --> 00:24:09,948
you can trigger the experiment execution via API or via

331
00:24:10,124 --> 00:24:13,244
like a GitHub action that automatically starts

332
00:24:13,292 --> 00:24:16,784
one or more experiments after running a new deployment in a pre

333
00:24:16,822 --> 00:24:20,208
production stage. And after the execution, you will get a

334
00:24:20,214 --> 00:24:24,368
list of, let's say ten experiments. They are executed,

335
00:24:24,464 --> 00:24:27,712
you will get a result back. Five of them are successful,

336
00:24:27,856 --> 00:24:31,996
maybe five of them have failed, and now you can handle the risk

337
00:24:32,128 --> 00:24:33,960
before you go into production.

338
00:24:36,860 --> 00:24:39,370
Let's recap. Let's summarize once again.

339
00:24:41,180 --> 00:24:44,360
Failures are normal and they will always occur.

340
00:24:45,020 --> 00:24:49,404
It's how we handle them and how we build and operate our systems that

341
00:24:49,602 --> 00:24:53,004
makes the difference. Just sitting there

342
00:24:53,042 --> 00:24:56,796
and waiting until the next error or failure occurs in production is not

343
00:24:56,818 --> 00:25:00,560
a strategy. We need to

344
00:25:00,630 --> 00:25:04,096
proactive, deal with possible failures and

345
00:25:04,118 --> 00:25:08,048
test what impacts they have on our system and of course

346
00:25:08,134 --> 00:25:11,636
on our customers. So embrace failures and turn them

347
00:25:11,658 --> 00:25:15,604
into experiments to understand how your system is

348
00:25:15,642 --> 00:25:17,780
reacting under such conditions.

349
00:25:19,240 --> 00:25:24,164
And as said, chaos engineering is necessary. And with

350
00:25:24,362 --> 00:25:27,620
our tool steadybit, there's a very easy

351
00:25:27,690 --> 00:25:30,976
entry how you can get started with chaos engineering to improve

352
00:25:31,008 --> 00:25:34,636
your system. And if you have any questions, please reach out to me.

353
00:25:34,778 --> 00:25:38,760
And yeah, thanks a lot for having me and enjoy the conference.

