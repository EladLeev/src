1
00:00:21,930 --> 00:00:25,842
Hi everyone. Welcome to my presentation of the vector search. And in Weaviate,

2
00:00:25,906 --> 00:00:30,146
I'm Laura. I am a community solution engineer at semi technologies,

3
00:00:30,258 --> 00:00:33,334
and in this presentation I will introduce you to our

4
00:00:33,372 --> 00:00:36,230
open source factorsearch engine, Weaviate.

5
00:00:36,810 --> 00:00:40,410
First, let's take a look at data, and particularly about

6
00:00:40,480 --> 00:00:43,706
unstructured data. Unstructured data are forms of data that

7
00:00:43,728 --> 00:00:47,114
are not organized in a predefined manner. Take for

8
00:00:47,152 --> 00:00:50,566
example, big pieces of text. We learned

9
00:00:50,598 --> 00:00:54,622
that 93% of your data stays unused and

10
00:00:54,676 --> 00:00:57,866
is unstructured, and that 80% of businesses don't

11
00:00:57,898 --> 00:01:01,438
know how to use their unstructured data in favor of their business.

12
00:01:01,604 --> 00:01:05,086
Why is this so difficult? What's so difficult about unstructured

13
00:01:05,118 --> 00:01:09,586
data? One thing that is difficult is searching through unstructured data.

14
00:01:09,768 --> 00:01:13,266
For example, to answer business questions. Let me give you

15
00:01:13,288 --> 00:01:17,362
an example, a simple example of searching through unstructured data.

16
00:01:17,496 --> 00:01:20,486
If you want to find information from unstructured text,

17
00:01:20,588 --> 00:01:23,880
you will need to use exact matching of keywords to find an answer.

18
00:01:24,730 --> 00:01:28,578
If you look, for example, for a wine that fits with your seafood dinner,

19
00:01:28,674 --> 00:01:31,866
while wine in your database only tells you that it

20
00:01:31,888 --> 00:01:35,354
is good with fish, you will most likely not find this

21
00:01:35,392 --> 00:01:38,922
wine. If you instead use a vector search

22
00:01:38,976 --> 00:01:42,986
engine like VVA, you can find information in unstructured

23
00:01:43,018 --> 00:01:46,654
data based on semantics. Compare this with Google Search.

24
00:01:46,772 --> 00:01:50,094
If you ask Google a very abstract question, it might

25
00:01:50,132 --> 00:01:54,286
find an answer. The question here, what color of wine is Chardonnay?

26
00:01:54,398 --> 00:01:58,098
Is very abstract. Still, Google search finds exactly

27
00:01:58,184 --> 00:02:01,758
this answer from a particular data node.

28
00:02:01,854 --> 00:02:05,714
So the question is, how does Google find exactly this answer from

29
00:02:05,752 --> 00:02:09,474
exactly this data node? And how can we predict the relation

30
00:02:09,522 --> 00:02:13,062
between this answer and the question that we asked?

31
00:02:13,196 --> 00:02:16,406
And in addition, how can we do this so fast? So the

32
00:02:16,428 --> 00:02:19,240
main question here is, yeah,

33
00:02:20,170 --> 00:02:23,754
what if you could do the same with your own data in a simple

34
00:02:23,792 --> 00:02:27,290
and secure way? So the answer that we case up with is

35
00:02:27,360 --> 00:02:30,986
Weaviate. Weaviate is a database that uses machine learning to

36
00:02:31,008 --> 00:02:34,502
understand the data that is in it. Weaviate is a cloud native,

37
00:02:34,646 --> 00:02:38,254
modular, real time vector search engine that is built

38
00:02:38,292 --> 00:02:41,134
to scale your machine learning models. So as I said,

39
00:02:41,172 --> 00:02:44,894
weaviate is a vector search engine. So first, let's dive in what

40
00:02:44,932 --> 00:02:48,526
vector search actually is. Weaviate stores data as vectors,

41
00:02:48,638 --> 00:02:51,806
which are placed in a space in relation to other data objects.

42
00:02:51,918 --> 00:02:55,570
Machine learning models are used to compute a vector for each data object

43
00:02:55,640 --> 00:02:59,206
and also for each semantic query. So VV eight really

44
00:02:59,228 --> 00:03:02,758
takes to understand your data and your queries in more detail.

45
00:03:02,844 --> 00:03:06,210
This is how weaviate works with a text factorization module.

46
00:03:06,370 --> 00:03:10,598
A pretrained model, for example, a fast text or bird transformer

47
00:03:10,614 --> 00:03:14,730
model can compute vectors from known concepts. This is, for example,

48
00:03:14,800 --> 00:03:18,586
our daily human language. You can add your own data

49
00:03:18,688 --> 00:03:22,366
to a weaviate instance and all this data will be

50
00:03:22,388 --> 00:03:26,174
vectorized using these machine learning models and be placed as

51
00:03:26,212 --> 00:03:30,510
vectors and with their own data object into weaviate.

52
00:03:31,410 --> 00:03:34,526
The data object will then be indexed by the machine

53
00:03:34,558 --> 00:03:37,860
learning models and be placed in the high dimensional vector space.

54
00:03:38,790 --> 00:03:43,074
Then you can perform, for example, a search query which

55
00:03:43,112 --> 00:03:47,358
will also be vectorized using machine learning models of WeAV eight.

56
00:03:47,544 --> 00:03:50,774
For example, let's find a wine that fits with

57
00:03:50,892 --> 00:03:53,990
seafood. Weaviate computes

58
00:03:54,570 --> 00:03:58,674
does its nearest similarity search and find the objects that lies nearest

59
00:03:58,722 --> 00:04:02,586
to your search vector. So the answer that lies closest to the vector of

60
00:04:02,608 --> 00:04:06,086
this question will be returned. With weaviate, you can do the following

61
00:04:06,118 --> 00:04:09,546
tasks with unstructured data. You can search through data,

62
00:04:09,648 --> 00:04:13,646
you can discover answers to your specific questions.

63
00:04:13,828 --> 00:04:17,182
You can classify and label your data

64
00:04:17,236 --> 00:04:20,538
automatically with machine learning models and Weaviate

65
00:04:20,554 --> 00:04:24,350
can predict relations in your database.

66
00:04:24,710 --> 00:04:27,726
The vector database Weaviate has full crud

67
00:04:27,758 --> 00:04:31,662
support for both data and vectors and you can combine vector

68
00:04:31,726 --> 00:04:35,746
search and scalar filters, which means you can combine semantic

69
00:04:35,778 --> 00:04:39,122
search with traditionally search. It has a graphQL and rEsTful

70
00:04:39,186 --> 00:04:42,514
API and weaviate supports multiple

71
00:04:42,642 --> 00:04:45,510
data types like text but also images.

72
00:04:46,090 --> 00:04:49,394
And this is all possible through Weaviate modules.

73
00:04:49,522 --> 00:04:53,366
So modules can be attached to

74
00:04:53,388 --> 00:04:57,366
the Weaviate core vector database to enable the features

75
00:04:57,398 --> 00:05:00,490
I just described in the previous slide. For example,

76
00:05:00,560 --> 00:05:04,330
you can choose to use an image vectorization module

77
00:05:04,410 --> 00:05:08,318
to index images and search through these images, but you

78
00:05:08,324 --> 00:05:12,366
can also attach a question answering module. You can also attach any

79
00:05:12,468 --> 00:05:16,030
transformer NLP model or you can even attach

80
00:05:16,110 --> 00:05:19,474
your own machine learning or NLP models. So this

81
00:05:19,512 --> 00:05:23,746
allows you to use Weaviate really for scaling your own machine learning models to

82
00:05:23,768 --> 00:05:27,720
a production scale as well. So now let's move on to a demo.

83
00:05:29,770 --> 00:05:33,030
I will use a data set of news articles for this demo

84
00:05:33,100 --> 00:05:37,062
and you can also find this demo data set running

85
00:05:37,116 --> 00:05:40,362
on our website. You can go here

86
00:05:40,416 --> 00:05:43,660
from any code example, a query example.

87
00:05:44,270 --> 00:05:48,266
So over here there's a really simple question

88
00:05:48,448 --> 00:05:51,882
or query to first just see

89
00:05:51,936 --> 00:05:55,966
what kind of articles we have in this data set. So I can perform a

90
00:05:56,068 --> 00:05:59,534
get query to get all the articles. And here

91
00:05:59,572 --> 00:06:03,860
I just want to see their titles, their URLs and their word count.

92
00:06:04,230 --> 00:06:08,082
And here I get a list in random order of

93
00:06:08,136 --> 00:06:12,100
all the articles. So now of course,

94
00:06:12,470 --> 00:06:15,940
nothing special or nothing magic is happening here.

95
00:06:16,470 --> 00:06:20,760
And just to show you that it is a vectors database, you can also

96
00:06:21,450 --> 00:06:24,520
query the whole factor of a data object.

97
00:06:25,050 --> 00:06:27,670
So you get the long list of vectors.

98
00:06:28,590 --> 00:06:32,586
So now let's only show the title. And as

99
00:06:32,608 --> 00:06:35,642
I said, this is just scalar search.

100
00:06:35,776 --> 00:06:38,842
So I don't do any machine learning magic here.

101
00:06:38,976 --> 00:06:42,218
But now let's take a semantic

102
00:06:42,314 --> 00:06:45,566
filter. So for example, let's see if

103
00:06:45,588 --> 00:06:49,450
the data set has any articles regarding housing prices.

104
00:06:49,610 --> 00:06:53,662
I can perform a near text query and this filter

105
00:06:53,726 --> 00:06:57,518
is added by a specific text factorization module.

106
00:06:57,694 --> 00:07:01,534
So let's see if there are articles about housing

107
00:07:01,582 --> 00:07:05,560
prices. And you can see this is very abstract question.

108
00:07:06,490 --> 00:07:10,630
In here, the list of articles is ordered to the relevancy,

109
00:07:12,730 --> 00:07:16,360
to the search query. So we have for example

110
00:07:17,550 --> 00:07:21,286
something about housing becoming

111
00:07:21,318 --> 00:07:25,610
the biggest asset class, something else about housing prices,

112
00:07:26,110 --> 00:07:31,310
expensive housing, et cetera. And note that the

113
00:07:31,380 --> 00:07:34,814
query using prices is not

114
00:07:34,852 --> 00:07:38,142
an exact match of any of the words here

115
00:07:38,276 --> 00:07:41,850
in the title. So here you can see that weaviate

116
00:07:41,930 --> 00:07:45,822
uses semantics and the context rather than exact matching

117
00:07:45,886 --> 00:07:50,610
keywords. We can also ask for

118
00:07:50,760 --> 00:07:54,290
how certain we feel is to show certain results.

119
00:07:54,710 --> 00:07:58,760
This is called certainty. So here you see that the first result is

120
00:07:59,290 --> 00:08:02,870
around 87% certain

121
00:08:02,940 --> 00:08:06,326
that it is matching the search query. And then we

122
00:08:06,348 --> 00:08:10,170
can also make a filter based on this. So now

123
00:08:10,240 --> 00:08:14,134
I will show only results that are above

124
00:08:14,182 --> 00:08:17,914
80% certain. Now you can see that this is

125
00:08:17,952 --> 00:08:22,090
a very abstract query and we can also make this a bit more concrete.

126
00:08:22,250 --> 00:08:26,014
For example, to see the prices of

127
00:08:26,052 --> 00:08:27,470
houses in Greece,

128
00:08:31,010 --> 00:08:34,770
this is a bit more concrete. And you can see that there's only one result

129
00:08:34,920 --> 00:08:38,194
returned now because we made the query more concrete and

130
00:08:38,232 --> 00:08:41,166
this is about Ethene.

131
00:08:41,358 --> 00:08:45,542
So yeah, we can see that weaviate matches Greece here

132
00:08:45,596 --> 00:08:49,094
with its capital without saying anything about

133
00:08:49,132 --> 00:08:52,486
Greece. So as I said, with VV you

134
00:08:52,508 --> 00:08:56,038
cannot only store factors, but also it stores the

135
00:08:56,044 --> 00:08:59,354
whole data object. So this means you can combine these kind

136
00:08:59,392 --> 00:09:03,260
of factor searches with traditional scalar search.

137
00:09:03,630 --> 00:09:07,226
And yeah, for example, I will show you

138
00:09:07,408 --> 00:09:11,274
this, I will add some properties

139
00:09:11,402 --> 00:09:14,762
first. So we have for example each article

140
00:09:14,826 --> 00:09:16,750
appearing in a publication.

141
00:09:18,130 --> 00:09:21,614
So we can see that this article for example appeared in the

142
00:09:21,652 --> 00:09:25,906
Financial Times. This is a graph relation in

143
00:09:25,928 --> 00:09:29,780
the database. And now I can takes a

144
00:09:31,030 --> 00:09:34,740
scalar filter combining with this already

145
00:09:35,530 --> 00:09:39,126
existing factor query. So this

146
00:09:39,148 --> 00:09:40,440
is a rare search.

147
00:10:21,430 --> 00:10:25,366
So now we can see I'm querying for using prices again and I want

148
00:10:25,388 --> 00:10:29,510
the result to appear in this publication, the Economist.

149
00:10:29,850 --> 00:10:33,942
So let's see now. So this first

150
00:10:33,996 --> 00:10:38,346
result, it's 87% sure this

151
00:10:38,368 --> 00:10:43,546
is the title. And we can see that Pearson economist we

152
00:10:43,568 --> 00:10:46,906
can also ask questions to weavy eight or

153
00:10:46,928 --> 00:10:50,682
to the data in Weaviate if we have a question answering module

154
00:10:50,826 --> 00:10:54,526
available or enabled. I can

155
00:10:54,548 --> 00:11:02,094
also show this so I will remove the previous filters so

156
00:11:02,132 --> 00:11:05,810
I can ask a question also in a filter.

157
00:11:06,950 --> 00:11:07,940
For example,

158
00:11:11,190 --> 00:11:14,894
what was the monkey doing in the

159
00:11:15,032 --> 00:11:18,360
neura link video?

160
00:11:20,570 --> 00:11:23,606
And I will not query the certainty here, but I

161
00:11:23,628 --> 00:11:25,080
want to see the answer.

162
00:11:26,730 --> 00:11:30,986
And the answer here is he

163
00:11:31,008 --> 00:11:34,266
was playing mind pong. So if we limit the

164
00:11:34,288 --> 00:11:37,482
result to one and we

165
00:11:37,536 --> 00:11:41,406
also ask for the summary, this is the

166
00:11:41,428 --> 00:11:45,006
summary of the article. So the

167
00:11:45,028 --> 00:11:48,606
result is that monkey was playing Mindpong and the

168
00:11:48,628 --> 00:11:51,630
answer was found somewhere in this whole summary,

169
00:11:52,390 --> 00:11:56,180
which is of course the bit of unstructured text that we have here.

170
00:11:57,830 --> 00:12:00,610
Okay, so now let's go back to the presentation.

171
00:12:01,830 --> 00:12:05,526
So you can use weaviate for a big variety of

172
00:12:05,548 --> 00:12:09,560
use cases due to the flexibility of choosing your own machine learning model

173
00:12:10,410 --> 00:12:14,454
or also keeping it very general. So this was my

174
00:12:14,492 --> 00:12:18,120
presentation. Thank you for listening and watching.

175
00:12:18,650 --> 00:12:22,870
In this presentation you learned that with the open source search

176
00:12:22,940 --> 00:12:26,502
factor engine vv eight you can search

177
00:12:26,556 --> 00:12:30,398
through unstructured data and in addition you can use to

178
00:12:30,404 --> 00:12:34,094
bring your own machine learning models to production skill. Thank you

179
00:12:34,132 --> 00:12:34,620
and see you.

