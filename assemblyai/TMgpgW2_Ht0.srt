1
00:00:34,450 --> 00:00:37,602
Hello everyone, thanks for joining my session.

2
00:00:37,666 --> 00:00:41,494
My name is Samuel Baruffi. I am a solutions architect with

3
00:00:41,532 --> 00:00:44,754
AWS and today we are going to be discussing

4
00:00:44,802 --> 00:00:48,214
about Carpenter. The name of my presentation is just in time.

5
00:00:48,252 --> 00:00:52,474
Nodes for any EKS cluster auto scaling

6
00:00:52,522 --> 00:00:56,190
with Carpenter. So let's just

7
00:00:56,260 --> 00:00:59,726
get started with a quick agenda. We're going

8
00:00:59,748 --> 00:01:03,646
to, at a very high level, discuss about EKS.

9
00:01:03,758 --> 00:01:07,390
What is EKS elastic Kubernetes services on AWS

10
00:01:07,550 --> 00:01:11,794
and we just want to set the tune and set the page, set the

11
00:01:11,832 --> 00:01:15,682
understanding of what is EKS because Carpenter actually works

12
00:01:15,736 --> 00:01:19,574
on top of eks. If you're not familiar with EKS, you might need a little

13
00:01:19,612 --> 00:01:23,686
bit of understanding of kubernetes, but hopefully the

14
00:01:23,708 --> 00:01:26,658
quick overview will be able to provide that guidance.

15
00:01:26,834 --> 00:01:29,942
After that we're going to talk about kubernetes auto scaling.

16
00:01:30,006 --> 00:01:33,194
What are the mechanisms that both on cloud

17
00:01:33,232 --> 00:01:36,874
native Kubernetes native are available for us, but also

18
00:01:36,992 --> 00:01:40,734
what are the currently implementations for

19
00:01:40,772 --> 00:01:44,238
cloud. After that we're going to talk about some customer

20
00:01:44,324 --> 00:01:48,510
challenges based on those implementations, current implementations,

21
00:01:49,170 --> 00:01:53,106
and then we're going to talk about Carpenter and how Carpenter solves some of

22
00:01:53,128 --> 00:01:57,726
the challenges that we've heard from customers trying to do autoscaling

23
00:01:57,758 --> 00:02:02,482
on kubernetes. And in the end we spend probably

24
00:02:02,536 --> 00:02:06,422
1015 minutes doing a demo, installing Karpenter and actually

25
00:02:06,556 --> 00:02:10,498
showcasing how carpenter can help you with a lot of flexibility

26
00:02:10,674 --> 00:02:15,030
and speed to scale up and scale down your clusters,

27
00:02:15,850 --> 00:02:18,490
your specific nodes within your clusters.

28
00:02:21,150 --> 00:02:24,630
So moving forward, let's do an overview of EKS.

29
00:02:24,790 --> 00:02:28,694
So EKS is short for elastic Kubernetes

30
00:02:28,742 --> 00:02:32,270
service. It's a managed service on AWS.

31
00:02:32,690 --> 00:02:36,266
EKS actually runs on vanilla upstream Kubernetes.

32
00:02:36,378 --> 00:02:40,762
It's also certified Kubernetes conformant for specific Kubernetes

33
00:02:40,826 --> 00:02:44,514
versions at any given time. EKS currently supports four

34
00:02:44,552 --> 00:02:48,050
versions of your Kubernetes, which gives you as a customer

35
00:02:48,200 --> 00:02:51,246
time to test and roll out upgrades.

36
00:02:51,358 --> 00:02:54,706
Having a lifecycle management of upgrades on

37
00:02:54,728 --> 00:02:58,274
your Kubernetes clusters is really important and AWS

38
00:02:58,322 --> 00:03:03,106
helps you with that because it's a managed service. EKS provides

39
00:03:03,218 --> 00:03:06,578
an experience for reliability,

40
00:03:06,754 --> 00:03:09,994
security, availability and performance on top

41
00:03:10,032 --> 00:03:14,186
of eks on eks. On the next slide you see how you

42
00:03:14,208 --> 00:03:17,994
have data plane and control plane that can be managed for you on

43
00:03:18,032 --> 00:03:22,782
both sides. And the whole idea is by

44
00:03:22,836 --> 00:03:26,778
using EKS you don't need to do a lot of the operations

45
00:03:26,874 --> 00:03:30,574
and what we call undifferentiated heavlifting for

46
00:03:30,612 --> 00:03:34,254
managing your Kubernetes clusters, you can just rely on a managed

47
00:03:34,382 --> 00:03:38,882
service like EKS to take care of those

48
00:03:38,936 --> 00:03:42,494
tasks like upgrades, lifecycle management,

49
00:03:42,622 --> 00:03:47,010
security and so forth. Of course, it's always a shared responsibility

50
00:03:47,170 --> 00:03:50,358
that some of the things will be taken care by AWS. And some

51
00:03:50,364 --> 00:03:53,442
of the things it's your responsibility to proper configure,

52
00:03:53,506 --> 00:03:56,150
giving you the proper flexibility.

53
00:03:57,770 --> 00:04:01,322
So when we look at a high level overview of

54
00:04:01,376 --> 00:04:04,700
what EKS is, you have two boxes here.

55
00:04:05,390 --> 00:04:08,246
The first box that we're going to talk is the control plane.

56
00:04:08,278 --> 00:04:11,914
So when you look at the box on the right which says AWS

57
00:04:11,962 --> 00:04:16,474
cloud, it means that it's running behind the scenes by AWS.

58
00:04:16,602 --> 00:04:20,346
And here on the top you can see that the control plane,

59
00:04:20,378 --> 00:04:24,122
which is a fully managed single tenant,

60
00:04:24,186 --> 00:04:28,078
kubernetes control plane per cluster. So once you create your EKS

61
00:04:28,174 --> 00:04:32,062
cluster behind the scenes, AWS is going to create a single tenant

62
00:04:32,126 --> 00:04:36,022
only for you control plane. And you're only going to get the

63
00:04:36,076 --> 00:04:41,266
specific endpoint. You can create private or public endpoints,

64
00:04:41,378 --> 00:04:44,806
we know we call them cluster endpoints. And behind the

65
00:04:44,828 --> 00:04:48,018
scenes, if you're familiar with Kubernetes architecture,

66
00:04:48,114 --> 00:04:52,134
you have the ETCD database, you have the API,

67
00:04:52,182 --> 00:04:54,726
you have the schedulers and you have the controller.

68
00:04:54,918 --> 00:04:58,122
AWS is going to manage the control plane for you and not only

69
00:04:58,176 --> 00:05:01,642
manage, but scale as needed. So you don't need to worry about

70
00:05:01,696 --> 00:05:04,766
that. It's all taken care on the control plane side

71
00:05:04,868 --> 00:05:08,398
by AWS. Then when you look at the left box,

72
00:05:08,484 --> 00:05:11,822
you see that the customer VPC. So that's the virtual private cloud

73
00:05:11,876 --> 00:05:15,086
that you have on your AWS account and that's where you

74
00:05:15,108 --> 00:05:18,434
can deploy your data plane. So the data plane means that

75
00:05:18,552 --> 00:05:21,618
those are the nodes where your containers, your pods are going to

76
00:05:21,624 --> 00:05:25,934
be running on. You have kind of two types of node

77
00:05:25,982 --> 00:05:29,686
groups that you can create it. You can have a self managed nodes group and

78
00:05:29,708 --> 00:05:33,302
a managed node group. With self managed node group you're actually

79
00:05:33,356 --> 00:05:37,186
responsible for all the configurations for your altiscaling

80
00:05:37,218 --> 00:05:41,100
group, for managing AMI and everything else.

81
00:05:42,510 --> 00:05:46,394
With manage no groups you have a managed experience for

82
00:05:46,432 --> 00:05:50,118
your data plane as well. So for doing know

83
00:05:50,144 --> 00:05:53,486
lifecycle management, scaling, those are actually going to

84
00:05:53,508 --> 00:05:56,574
be responsible to take

85
00:05:56,612 --> 00:05:59,854
those actions for you behind the scenes. You can

86
00:05:59,892 --> 00:06:03,486
actually also use forgate. Forgate, it's a

87
00:06:03,508 --> 00:06:07,540
serverless container offering that does not require any

88
00:06:08,150 --> 00:06:11,326
specific nodes group in the sense that you don't need any EC

89
00:06:11,358 --> 00:06:14,786
two, neither a self managed node group or a managed node group.

90
00:06:14,888 --> 00:06:18,194
But with forgate you pay per pod

91
00:06:18,242 --> 00:06:21,718
and that specific

92
00:06:21,804 --> 00:06:25,240
running pod behind the scenes is running on the AWS account.

93
00:06:25,690 --> 00:06:29,926
So you can see here that the way it works, it creates an Eni

94
00:06:30,118 --> 00:06:33,558
within your VPC that links back to the Fargate

95
00:06:33,654 --> 00:06:37,562
micro VM that is running on the AWS cloud. Fargate also

96
00:06:37,616 --> 00:06:40,778
works on ecs but has integration with eks like you

97
00:06:40,784 --> 00:06:44,398
can see here for this talk. We're not going

98
00:06:44,404 --> 00:06:48,142
to focus on too much on EKS data

99
00:06:48,196 --> 00:06:52,046
plane or control plane. We are going to talk about EKS auto scaling and

100
00:06:52,068 --> 00:06:55,746
Kubernetes auto scaling. So with that said, let's move to

101
00:06:55,768 --> 00:06:59,426
the next section when we look at different

102
00:06:59,608 --> 00:07:03,490
so what you as a customer or a user of kubernetes,

103
00:07:04,870 --> 00:07:09,314
what are the available resources and configuration that you can fine tune

104
00:07:09,442 --> 00:07:12,934
for autoscaling? So you're going to start at the application level

105
00:07:13,052 --> 00:07:16,962
so you can separate autoscaling and kubernetes

106
00:07:17,026 --> 00:07:20,666
at two different categories. One is the application itself and the

107
00:07:20,688 --> 00:07:23,990
other one is the nodes and the infrastructure.

108
00:07:24,150 --> 00:07:28,086
So the first two items are more focused on the applications

109
00:07:28,118 --> 00:07:32,394
that are running. The first one is called horizontal pod outscaling.

110
00:07:32,522 --> 00:07:35,440
HPA is the short version of that.

111
00:07:35,890 --> 00:07:39,278
The whole idea of HPA is you do a

112
00:07:39,284 --> 00:07:43,418
deployment on your cluster and you decide

113
00:07:43,514 --> 00:07:46,802
how many replicas of that specific deployment you want to have.

114
00:07:46,856 --> 00:07:50,194
Let's say I want to have an Nginx server and I want to have

115
00:07:50,232 --> 00:07:54,398
three replicas of that specific Nginx pod

116
00:07:54,494 --> 00:07:58,482
to be deployed across my specific environment. You can configure

117
00:07:58,546 --> 00:08:02,806
HPA on top of that deployment, and you can specify specific

118
00:08:02,908 --> 00:08:06,726
metrics, for example cpu, memory, or even have your own

119
00:08:06,748 --> 00:08:10,818
custom metrics. And once you do know,

120
00:08:10,924 --> 00:08:14,698
HPA is going to look for that metric. And if a specific

121
00:08:14,784 --> 00:08:18,934
threshold that you have configured goes above. So let's say you configure

122
00:08:18,982 --> 00:08:22,602
that if at any given time the cpu

123
00:08:22,746 --> 00:08:26,462
aggregation of your deployment goes above 80%, it wants

124
00:08:26,516 --> 00:08:29,774
to increase to another pod, another replica within

125
00:08:29,812 --> 00:08:33,918
your deployment. So HPA is going to take that job

126
00:08:34,004 --> 00:08:37,674
for you and it's going to just add horizontally more nodes

127
00:08:37,802 --> 00:08:40,994
for as much as you have configured and for

128
00:08:41,032 --> 00:08:44,842
whatever metric you have configured. So that's what is called HPA.

129
00:08:45,006 --> 00:08:49,270
But you also have another option which is called the vertical pod outscaling,

130
00:08:50,490 --> 00:08:53,478
which is VPA is short for.

131
00:08:53,644 --> 00:08:56,790
VPA is less common in a sense,

132
00:08:56,860 --> 00:09:00,998
because Kubernetes is really good at distributed systems horizontally.

133
00:09:01,094 --> 00:09:04,810
But you also have an ability to actually change

134
00:09:04,880 --> 00:09:08,806
a pod that is running, for example, with 2gb

135
00:09:08,838 --> 00:09:12,026
of memory. But if a specific threshold has

136
00:09:12,048 --> 00:09:15,886
been achieved, you want to create a new pod with 4gb of

137
00:09:15,908 --> 00:09:19,114
memory. So the same things that works on HPA.

138
00:09:19,162 --> 00:09:22,458
Now, instead of adding new replicas on your deployment, it's just

139
00:09:22,484 --> 00:09:25,730
going to recreate a new pod with more memory available

140
00:09:25,800 --> 00:09:28,882
for that specific pod. But those are always

141
00:09:28,936 --> 00:09:30,340
looking at your application.

142
00:09:31,750 --> 00:09:35,542
Those two configurations, both HPA and VPA, don't actually

143
00:09:35,596 --> 00:09:39,574
look at the cluster itself or add more infrastructure nodes. You only

144
00:09:39,612 --> 00:09:43,606
do at the application level. So you actually need

145
00:09:43,628 --> 00:09:47,666
to. If you want to run a flexible and

146
00:09:47,708 --> 00:09:51,258
elastic Kubernetes cluster, you also need something

147
00:09:51,344 --> 00:09:54,380
that is responsible for creating more nodes for you.

148
00:09:54,910 --> 00:09:58,006
With that said, that's where cluster altoscaler

149
00:09:58,038 --> 00:10:01,294
comes in. So with cluster outscaling, if let's say

150
00:10:01,332 --> 00:10:05,306
you have two nodes on your data plane

151
00:10:05,498 --> 00:10:08,430
and you try to schedule in this example,

152
00:10:08,500 --> 00:10:12,394
four more pods, but there are no resources

153
00:10:12,442 --> 00:10:16,586
available within those existing nodes on your node

154
00:10:16,618 --> 00:10:19,826
group. A cluster out scaler once you install and you

155
00:10:19,848 --> 00:10:22,866
configure and integrate it with your provider, let's say in

156
00:10:22,888 --> 00:10:26,558
this case AWS cluster outscaler

157
00:10:26,574 --> 00:10:29,862
will look for penning pods and we say, wow, I don't really have

158
00:10:29,916 --> 00:10:33,894
resources currently available for me to deploy those four

159
00:10:33,932 --> 00:10:37,382
penning nodes. So the cluster outscaler will go and we talk

160
00:10:37,436 --> 00:10:41,066
to the altiscaling group as part of your node group, either a

161
00:10:41,088 --> 00:10:44,666
self managed node group or a managed node group.

162
00:10:44,768 --> 00:10:48,714
So the cluster outscale itself will go and you talk to the API of your

163
00:10:48,752 --> 00:10:52,174
altiscaling group and you say please spin up a new

164
00:10:52,372 --> 00:10:56,142
nodes or a new EC two for me within that

165
00:10:56,196 --> 00:11:01,534
specific outscaling group. So then I can go

166
00:11:01,572 --> 00:11:05,066
and actually schedule and run all my four nodes that were penning.

167
00:11:05,178 --> 00:11:08,562
So behind the scenes, each outscaling group will

168
00:11:08,616 --> 00:11:12,430
increment the size based on the recommendation of the penning pods.

169
00:11:12,590 --> 00:11:15,922
This works fine for most applications and workload.

170
00:11:16,066 --> 00:11:19,154
However, as kubernetes and eks have grained,

171
00:11:19,202 --> 00:11:22,722
broader adoption customers are moving for a variety

172
00:11:22,786 --> 00:11:26,326
of different workloads. And as you can see in this example,

173
00:11:26,428 --> 00:11:30,182
it's actually just creating a new instance of the same instance type

174
00:11:30,236 --> 00:11:33,686
within the same auto scaling group. And that's

175
00:11:33,718 --> 00:11:37,302
where some challenges come into the picture.

176
00:11:37,446 --> 00:11:41,946
So what we've heard, we've heard some customers bringing

177
00:11:41,978 --> 00:11:46,010
some feedback and saying why potentially

178
00:11:46,090 --> 00:11:49,950
cluster autoscaler doesn't work every single time,

179
00:11:50,020 --> 00:11:53,540
or there is potentially improvement that should be made.

180
00:11:54,710 --> 00:11:57,890
So nearly half of AWS Kubernetes customers

181
00:11:58,040 --> 00:12:03,534
have told AWS that configure cluster autoscaler

182
00:12:03,582 --> 00:12:06,774
is challenging, and we are going to now just go

183
00:12:06,812 --> 00:12:09,974
through some of those challenges to set a scene on

184
00:12:10,012 --> 00:12:13,110
why it was important for us to create carpenter.

185
00:12:13,850 --> 00:12:18,330
So first of all, no groups and autoscaling group sprawl

186
00:12:18,990 --> 00:12:23,206
different workloads will need different compute resources.

187
00:12:23,398 --> 00:12:26,954
So AI ML workload, we have

188
00:12:26,992 --> 00:12:30,606
a very different requirement than for example your web application or

189
00:12:30,628 --> 00:12:34,714
your batch applications, right? Unfortunately, with cluster

190
00:12:34,762 --> 00:12:38,490
auto scaler, the only thing that the cluster outscaler

191
00:12:38,570 --> 00:12:42,174
is able to do is to add new instances of

192
00:12:42,212 --> 00:12:46,206
the same type on your existing manage nodes

193
00:12:46,238 --> 00:12:50,142
group. You can create multiple manage node groups with different instance

194
00:12:50,206 --> 00:12:53,570
types, but that adds a lot of complexity

195
00:12:54,310 --> 00:12:57,494
in managing those, right? So what customers have

196
00:12:57,532 --> 00:13:01,480
told you that not all workloads needs to be isolated on specific

197
00:13:02,170 --> 00:13:05,878
nodes groups, and balancing the needs of specific

198
00:13:05,964 --> 00:13:09,638
workloads adds a lot of complexity because now you need to manage

199
00:13:09,724 --> 00:13:13,238
multiple outscaling groups and multiple managed node

200
00:13:13,254 --> 00:13:16,794
groups and it becomes just cumbersome and it's really

201
00:13:16,832 --> 00:13:21,098
hard to achieve proper performance for cost

202
00:13:21,184 --> 00:13:24,686
and also for availability. As an

203
00:13:24,708 --> 00:13:27,914
example, if you need spot, you can't mix

204
00:13:27,962 --> 00:13:31,486
and match in a specific managed node group is spot and on

205
00:13:31,508 --> 00:13:35,006
demand. You need to have multiple managed nodes groups

206
00:13:35,038 --> 00:13:38,546
that behind the scenes each one of them have an auto scaling group and

207
00:13:38,568 --> 00:13:42,606
it becomes really challenge how you actually provide availability

208
00:13:42,718 --> 00:13:46,642
for spot interruption or best practice for for example,

209
00:13:46,776 --> 00:13:50,310
spreading workloads across the z while always thinking

210
00:13:50,380 --> 00:13:53,974
about cost and trying to improve cost and

211
00:13:54,012 --> 00:13:58,134
performance for those workloads. Another challenge

212
00:13:58,182 --> 00:14:02,538
that we've heard from customers is cluster outscaler actually sometimes

213
00:14:02,624 --> 00:14:05,994
can be very slow to respond for capacity needs

214
00:14:06,032 --> 00:14:10,126
and spike workloads. So if you think

215
00:14:10,148 --> 00:14:13,882
about ETL jobs or GPU training jobs or ML

216
00:14:13,946 --> 00:14:17,502
workloads, the speed that it's required for

217
00:14:17,556 --> 00:14:21,546
those workloads, like big data and AML workloads

218
00:14:21,738 --> 00:14:25,294
to be spun up is critical. Delay in providing

219
00:14:25,342 --> 00:14:29,442
those capacities for these workloads can

220
00:14:29,496 --> 00:14:33,662
slow down innovation and potentially decrease

221
00:14:33,806 --> 00:14:36,930
the satisfaction of your data science and engineers.

222
00:14:37,590 --> 00:14:42,482
This job typically spin up several nodes of expensive accelerated

223
00:14:42,626 --> 00:14:45,590
EC, two instances, for example, GPU,

224
00:14:45,930 --> 00:14:49,926
very expensive gpus. So you want those to be very quickly spun

225
00:14:49,958 --> 00:14:52,380
up, but also very quickly spun down.

226
00:14:53,550 --> 00:14:56,742
And a slow scale down means waste resources,

227
00:14:56,806 --> 00:14:59,580
which you don't want to be in the business of.

228
00:15:01,410 --> 00:15:04,922
Another challenge. It is very hard to balance utilization,

229
00:15:04,986 --> 00:15:08,874
availability and cost. So typically with cluster

230
00:15:08,922 --> 00:15:12,394
altoscaler is hard to get high cluster utilization and

231
00:15:12,452 --> 00:15:16,494
efficiency operation while not over provisioning

232
00:15:16,542 --> 00:15:20,594
resources to ensure a consistent user experience. So what

233
00:15:20,632 --> 00:15:24,626
this can result is in a low utilization and lead to

234
00:15:24,648 --> 00:15:28,690
waste resources that impact, which can be significant,

235
00:15:28,770 --> 00:15:32,726
which the impact can be significant. So as

236
00:15:32,748 --> 00:15:35,878
an example, let's say you want to make sure your application is running

237
00:15:35,964 --> 00:15:38,650
across multiple availability zones,

238
00:15:39,230 --> 00:15:42,582
but have a different resource requirement.

239
00:15:42,726 --> 00:15:46,486
Then you potentially need to have multiple auto scaling groups.

240
00:15:46,598 --> 00:15:49,914
And that adds just a lot of challenge managing those

241
00:15:49,952 --> 00:15:53,566
auto scaling groups that are across AZ and you want to make

242
00:15:53,588 --> 00:15:57,214
sure that they are fully utilized, that becomes very

243
00:15:57,252 --> 00:16:00,654
challenging, sometimes potentially impossible to

244
00:16:00,692 --> 00:16:04,882
not have wasted resources. So with

245
00:16:04,936 --> 00:16:08,450
all those three challenges we so far have discussed,

246
00:16:09,030 --> 00:16:13,170
we have come up with Carpenter. But what is actually carpenter.

247
00:16:13,830 --> 00:16:17,926
So carpenter, it's a open resources, a flexible and

248
00:16:17,948 --> 00:16:21,270
high performance Kubernetes cluster altiscaler.

249
00:16:21,930 --> 00:16:25,362
So instead of actually deploying cluster altiscaler,

250
00:16:25,426 --> 00:16:28,950
you can actually deploy Karpenter on your eks.

251
00:16:29,310 --> 00:16:31,930
It is open source and Kubernetes native.

252
00:16:32,910 --> 00:16:36,266
It doesn't have any concept of group. So it's what we call

253
00:16:36,288 --> 00:16:39,802
a groupless approach. And we are going to talk about

254
00:16:39,856 --> 00:16:43,674
in a moment why it's called groupless, but it's pretty much automatic node

255
00:16:43,722 --> 00:16:48,442
sizing. So instead of having a specific requirement

256
00:16:48,586 --> 00:16:52,382
or a specific, I guess, blocker of just being able

257
00:16:52,436 --> 00:16:56,686
to launch instance of the same type that you have on your altiscaling

258
00:16:56,718 --> 00:17:00,546
group with carpenter, it can look at the specific requirements for

259
00:17:00,568 --> 00:17:04,606
the painting jobs and choose the best performance

260
00:17:04,798 --> 00:17:08,134
and costs for that specific need at any

261
00:17:08,172 --> 00:17:12,294
given time. And it's also much more performant at scale because it

262
00:17:12,332 --> 00:17:15,814
has some changes on the way it behaviors compared to

263
00:17:15,852 --> 00:17:19,070
cluster autoscaler. The way APIs and the way it's

264
00:17:19,090 --> 00:17:22,138
actually looking for pending pods on your cluster is a

265
00:17:22,144 --> 00:17:26,202
little bit different. The goal is to launch 1000

266
00:17:26,256 --> 00:17:30,246
pods within 30 seconds. That's the goal that carpenter has set

267
00:17:30,288 --> 00:17:33,882
in mind. And it can, depending on your environment,

268
00:17:34,026 --> 00:17:37,934
actually achieve that. So let's look at

269
00:17:37,972 --> 00:17:42,266
how Karpenter works. Very similar to cluster outscaler

270
00:17:42,298 --> 00:17:45,826
when you have penning nodes. Karpenter, we're always going to look

271
00:17:45,848 --> 00:17:50,062
at the schedule on Kubernetes because he works integrated into Kubernetes

272
00:17:50,126 --> 00:17:53,762
native ecosystem. Look for penning pods and

273
00:17:53,816 --> 00:17:57,314
those panning pods looks at existing capacity

274
00:17:57,362 --> 00:18:00,614
in this case and see, well, I can't actually deploy more

275
00:18:00,652 --> 00:18:04,790
pods because it's full here. So penning pods becomes

276
00:18:05,450 --> 00:18:09,782
unschedulable nodes and then that's where capital comes

277
00:18:09,836 --> 00:18:12,958
in. So this would actually replace your cluster out scalar.

278
00:18:12,994 --> 00:18:16,746
You're not going to have in this case cluster outscaler. Here you

279
00:18:16,768 --> 00:18:20,042
have carpenter deploy and Karpenter. We actually go

280
00:18:20,096 --> 00:18:24,106
and instead of talking to an is because there

281
00:18:24,128 --> 00:18:28,170
is no groups, we go and we talk to the EC two fleet API.

282
00:18:28,330 --> 00:18:31,520
And the EC two fleet API provides a bunch of benefits.

283
00:18:31,890 --> 00:18:35,586
And behind the scenes, what carpenter does, it looks at

284
00:18:35,608 --> 00:18:39,102
the specific requirements for those unscheduleable

285
00:18:39,166 --> 00:18:42,750
nodes and will find just in time capacity

286
00:18:42,910 --> 00:18:46,386
that is perfect for what you need. So you

287
00:18:46,408 --> 00:18:49,766
can configure and it's very flexible what type of configuration allows you

288
00:18:49,788 --> 00:18:53,174
to do. But if you don't provide any specific limitations of the

289
00:18:53,212 --> 00:18:56,360
instances that it can choose, you'll find a specific

290
00:18:56,730 --> 00:19:00,138
instance that can fit your requirement while

291
00:19:00,224 --> 00:19:03,386
also improving for cost and performance, and you make that

292
00:19:03,408 --> 00:19:08,902
decision for you. So it's also deeply

293
00:19:09,046 --> 00:19:12,730
integrated with Kubernetes. So you look for watch APIs,

294
00:19:12,810 --> 00:19:16,494
you have a lot of labels and finalizers, and like

295
00:19:16,532 --> 00:19:20,122
I said, it does a lot of the automated instance

296
00:19:20,186 --> 00:19:24,222
selection. So you match a specific workload to a specific instance

297
00:19:24,286 --> 00:19:24,900
type.

298
00:19:27,350 --> 00:19:31,054
Carpenter also works really well with spot.

299
00:19:31,182 --> 00:19:35,154
So if you configure a specific provision, and we're going to talk

300
00:19:35,192 --> 00:19:39,110
in a moment, what is the provision? But if your provisioner has support

301
00:19:39,180 --> 00:19:42,534
for spot and you can mix and match spot and on demand, you can

302
00:19:42,572 --> 00:19:46,182
have both being supported. He can actually

303
00:19:46,236 --> 00:19:49,986
look for what cheapest and most performance

304
00:19:50,018 --> 00:19:54,306
spot is available on this currently specific availability zone

305
00:19:54,338 --> 00:19:57,846
that he wants to deploy and actually pick and choose that and deploy

306
00:19:57,878 --> 00:20:01,486
those for you, and it can also hand interruption for you.

307
00:20:01,588 --> 00:20:06,270
You actually have integration with the two minute interruption

308
00:20:07,410 --> 00:20:10,894
system that spot has in place to allow your

309
00:20:10,932 --> 00:20:14,814
applications to potentially be rescheduled before the node interruption

310
00:20:14,862 --> 00:20:18,242
is in place. Another really good

311
00:20:18,296 --> 00:20:22,274
thing that carpenter has actually done is

312
00:20:22,312 --> 00:20:26,802
the ability to consolidate. And consolidation

313
00:20:26,866 --> 00:20:30,866
is a feature that actually looks for opportunity to improve

314
00:20:30,898 --> 00:20:34,870
your cluster utilization over time. So carpenter does not

315
00:20:34,940 --> 00:20:38,394
also works on scaling up and down, but also look

316
00:20:38,432 --> 00:20:43,354
at your cluster high level and look at which

317
00:20:43,392 --> 00:20:46,906
current nodes you have in place. And if there is potentially an

318
00:20:46,928 --> 00:20:50,634
opportunity to maybe remove some of those

319
00:20:50,672 --> 00:20:54,078
nodes and bring up other nodes that are more performance and

320
00:20:54,164 --> 00:20:57,822
a price optimized for you. So you can reschedule running

321
00:20:57,876 --> 00:21:01,802
pods onto existing clusters that are underutilized

322
00:21:01,866 --> 00:21:05,010
at the cluster capacity, but you can also launch new

323
00:21:05,080 --> 00:21:08,654
more cost efficiency nodes within the cluster

324
00:21:08,782 --> 00:21:12,734
and replace potentially nodes that were much more expensive. So let's

325
00:21:12,782 --> 00:21:15,974
say in this case here

326
00:21:16,092 --> 00:21:20,230
you have three nodes

327
00:21:20,890 --> 00:21:24,486
that are potentially m five large.

328
00:21:24,588 --> 00:21:28,394
Let's just give an example. Carpenter will look at that saying, well, I can

329
00:21:28,432 --> 00:21:31,642
potentially spin up an m five two x large and

330
00:21:31,696 --> 00:21:34,906
maybe then an m five

331
00:21:35,088 --> 00:21:38,410
medium and actually bring

332
00:21:38,480 --> 00:21:42,026
all those capacity to those two instances that are

333
00:21:42,048 --> 00:21:45,198
going to be more performance and better for your price.

334
00:21:45,364 --> 00:21:49,086
So that consolidation is a feature that you can enable. If you don't want

335
00:21:49,188 --> 00:21:53,074
the feature, it's okay because it's actually going to reschedule the

336
00:21:53,112 --> 00:21:56,734
pods and you want to make sure your application is highly

337
00:21:56,782 --> 00:22:00,242
distributed within your cluster. So consolidation doesn't ever

338
00:22:00,296 --> 00:22:04,238
bring your application down, but that actually optimize capacity

339
00:22:04,334 --> 00:22:08,134
quite a lot. So when

340
00:22:08,172 --> 00:22:12,306
we look at how Karpenter works, how carpenter provisions

341
00:22:12,338 --> 00:22:15,954
a node on AWS. So let's first look at how cluster

342
00:22:16,002 --> 00:22:19,594
autoscaler does so. Cluster autoscaler, we look at,

343
00:22:19,712 --> 00:22:23,190
let's say you have an application scheduler or HPA

344
00:22:23,270 --> 00:22:26,262
is triggering a specific pod to be created.

345
00:22:26,326 --> 00:22:30,246
It gets into a pending pod scenario, then the cluster

346
00:22:30,278 --> 00:22:33,886
altiscaler will actually look at those pending pods, will talk

347
00:22:33,908 --> 00:22:37,294
to altiscaling group and then outscaling group will talk to EC two

348
00:22:37,332 --> 00:22:41,166
API to increase or decrease whatever in this case increase because

349
00:22:41,188 --> 00:22:45,060
you have pending nodes, increase the number of nodes that you have on your

350
00:22:45,590 --> 00:22:48,658
node group. Now the way

351
00:22:48,824 --> 00:22:52,114
carpenter works is instead of having to

352
00:22:52,152 --> 00:22:55,842
talk to closer outscaler and

353
00:22:55,976 --> 00:22:59,494
the specific outscaling group, penning pods will actually talk

354
00:22:59,532 --> 00:23:02,370
directly. A carpenter will watch for those penning pods.

355
00:23:02,450 --> 00:23:05,922
Those penning pods will then actually set an action on Karpenter.

356
00:23:05,986 --> 00:23:08,890
And instead of Karpenter talking to EC two API,

357
00:23:09,390 --> 00:23:13,718
carpenter talks to EC two fleet instance, which is much more performance

358
00:23:13,814 --> 00:23:17,754
when you're trying to grab what is the capability and

359
00:23:17,792 --> 00:23:21,402
possibilities that carpenter can deploy on a specific availability

360
00:23:21,466 --> 00:23:25,274
zone in a region. EC two fleet is the one responsible on the AWS

361
00:23:25,322 --> 00:23:28,878
side to make those decisions and

362
00:23:28,964 --> 00:23:32,222
consolidation instance orchestration responsibility within a single

363
00:23:32,276 --> 00:23:34,740
system. It's what Carpenter does,

364
00:23:36,150 --> 00:23:40,242
and we've talked about groupless provisioning. So what

365
00:23:40,296 --> 00:23:43,442
actually carpenter does, it's an application first

366
00:23:43,496 --> 00:23:47,406
approach because it's always looking for the pending pods and grouping

367
00:23:47,438 --> 00:23:51,730
those pending pods, which is called beam packing. So every time there are pending pods,

368
00:23:51,810 --> 00:23:55,318
you're actually going to beam pack all those panning pods and look at

369
00:23:55,404 --> 00:23:58,898
what is the simplest node

370
00:23:59,074 --> 00:24:02,586
provisioning that will make sense for your cluster. And the way you

371
00:24:02,608 --> 00:24:05,914
configure, compute, you'll see on the demo. It's really simple. You just have

372
00:24:05,952 --> 00:24:09,654
two objects. One is the provisioning

373
00:24:09,702 --> 00:24:13,126
and the other one is the node template for your specific provider,

374
00:24:13,158 --> 00:24:16,906
in this case AWS. And it reduced a lot of the cloud provider

375
00:24:16,938 --> 00:24:20,026
API load because now it's going directly to the EC two fleet

376
00:24:20,058 --> 00:24:23,614
API. It doesn't have the limitations that how

377
00:24:23,652 --> 00:24:27,450
many times you can call out scaling groups

378
00:24:27,530 --> 00:24:30,926
which the cluster altiscaler had and it reduced

379
00:24:30,958 --> 00:24:34,686
the latency significantly. So you choose the instance type from the pod

380
00:24:34,718 --> 00:24:38,498
resource request. So when you're

381
00:24:38,514 --> 00:24:42,790
doing deployments, you always want to make sure you set the requests of memory

382
00:24:43,290 --> 00:24:47,494
and CPU. And that's what carpenter we actually use to

383
00:24:47,532 --> 00:24:51,434
look at how much memory and cpu requires. Then you choose the

384
00:24:51,472 --> 00:24:54,794
specific node as per pod scheduling constraints. So if you have

385
00:24:54,832 --> 00:24:58,506
constraints of specific availability zones that you want that pod to be

386
00:24:58,528 --> 00:25:03,242
deployed, or potentially some specific instance

387
00:25:03,306 --> 00:25:06,734
types or gpus, it's going to look at the

388
00:25:06,772 --> 00:25:09,982
pod deployment like labels. And then

389
00:25:10,036 --> 00:25:13,290
carpenter read that and make a decision based on those constraints.

390
00:25:13,370 --> 00:25:16,938
And that capacity is directly done on EC two instance fleet.

391
00:25:17,034 --> 00:25:21,346
And then you track the nodes using native Kubernetes labels and

392
00:25:21,368 --> 00:25:24,546
you also bind, this is a specific one.

393
00:25:24,648 --> 00:25:28,434
It binds the pod early to the nodes

394
00:25:28,482 --> 00:25:32,038
because it doesn't need to wait for the

395
00:25:32,044 --> 00:25:35,160
cluster altoscaler to make any decision like it was before.

396
00:25:35,770 --> 00:25:39,210
While it's actually creating the node behind the scenes,

397
00:25:40,590 --> 00:25:44,266
the cube scheduler is already kind of downloading everything that

398
00:25:44,288 --> 00:25:46,780
it needs to do and it becomes ready.

399
00:25:47,310 --> 00:25:51,158
The schedule for the node becomes ready. It can start preparing the

400
00:25:51,184 --> 00:25:55,386
node immediately. It doesn't need to wait much how the cluster autoscaler

401
00:25:55,418 --> 00:25:59,594
needs, including the pre pulling of the image. And this can actually shave

402
00:25:59,642 --> 00:26:03,578
seconds off nodes per node startup latency.

403
00:26:03,674 --> 00:26:07,314
So it just is a very nice feature that

404
00:26:07,352 --> 00:26:11,262
helps carpenter be more reliable

405
00:26:11,326 --> 00:26:15,090
and fast when actually doing those scaling activities.

406
00:26:16,250 --> 00:26:19,798
So let's just quickly look at how carpenter can scale up.

407
00:26:19,884 --> 00:26:24,194
Let's see, we have specific panning

408
00:26:24,242 --> 00:26:27,318
pods here on the top. Karpenter will look at

409
00:26:27,324 --> 00:26:30,298
those panning pods and you create a new node, right?

410
00:26:30,384 --> 00:26:33,866
And assuming you have targets here because you

411
00:26:33,888 --> 00:26:38,954
have requests set on your application. So he knows how much at

412
00:26:38,992 --> 00:26:41,674
both at a node level but also at a cluster level,

413
00:26:41,792 --> 00:26:45,966
what is the utilization and the target that he wants to set for

414
00:26:45,988 --> 00:26:49,470
a specific one. In this case you can set up provisioners

415
00:26:50,050 --> 00:26:53,870
by default. It has all instances, types able

416
00:26:53,940 --> 00:26:57,346
on that that are included that carpenter can pick and choose.

417
00:26:57,448 --> 00:27:00,738
But you can specify, and we have some examples that I'm going

418
00:27:00,744 --> 00:27:04,180
to show of specific instance types that you want to make available.

419
00:27:05,670 --> 00:27:09,000
And then when we have scaling, we have different options.

420
00:27:10,490 --> 00:27:14,374
We talked about consolidation and we're going to show an example before,

421
00:27:14,572 --> 00:27:18,082
but you have two options. You can either use the TTL

422
00:27:18,146 --> 00:27:21,202
seconds after empty or you can use the consolidation.

423
00:27:21,266 --> 00:27:24,010
They are mucco exclusive.

424
00:27:24,670 --> 00:27:28,710
Consolidation is more like a newer feature, but before consolidation existed

425
00:27:28,790 --> 00:27:32,154
you have these settings set PTL seconds

426
00:27:32,202 --> 00:27:35,614
after empty. In this case, in this example I'm showing you

427
00:27:35,652 --> 00:27:39,280
is set as 10 seconds. So what this feature will do,

428
00:27:42,130 --> 00:27:45,506
it will look for nodes that are empty. In this case I

429
00:27:45,528 --> 00:27:48,930
just removed some pods from my nodes and 10

430
00:27:49,000 --> 00:27:52,574
seconds after, if the node is still empty,

431
00:27:52,702 --> 00:27:55,220
you're actually just going to remove the nodes completely.

432
00:27:55,750 --> 00:27:59,830
And one thing that I just want to mention

433
00:27:59,980 --> 00:28:03,506
is it doesn't actually care about the demon sets because demon

434
00:28:03,538 --> 00:28:07,094
sets are looking at every single node. You just look for

435
00:28:07,132 --> 00:28:11,366
nodes. So it is smart enough to realize if there are three demon sets

436
00:28:11,398 --> 00:28:14,646
running at that cluster, sorry, at that node, it doesn't

437
00:28:14,678 --> 00:28:18,746
really cares about that because it knows the demon sets actually running across

438
00:28:18,928 --> 00:28:22,060
all nodes. So remove that.

439
00:28:22,590 --> 00:28:26,014
I talked about bing pack. The cool thing is it

440
00:28:26,052 --> 00:28:29,386
combines all those specific penning pods requirements

441
00:28:29,498 --> 00:28:33,882
and has well known labels that you can define on your specific deployment

442
00:28:33,946 --> 00:28:37,186
that are actually configured at the node as well. So let's say you

443
00:28:37,208 --> 00:28:41,038
want to run this specific application on arm

444
00:28:41,134 --> 00:28:45,210
graviton tube processor. You can actually define those specific labels.

445
00:28:45,310 --> 00:28:48,854
And when carpeting, doing the beam packing is going to make a

446
00:28:48,892 --> 00:28:52,998
consolidation decision on how they can organize all

447
00:28:53,084 --> 00:28:56,230
the panning pods you have on the queue.

448
00:28:57,130 --> 00:29:00,586
And then consolidation, which I recommend

449
00:29:00,688 --> 00:29:03,754
rather than using. There are potentially reasons why you want to use

450
00:29:03,792 --> 00:29:06,630
CTL seconds after expire,

451
00:29:06,790 --> 00:29:11,058
but potentially consolidation is a much more broader and feature rich

452
00:29:11,254 --> 00:29:14,446
solution that allows you, if you enable here on

453
00:29:14,468 --> 00:29:17,242
your provision AWS, you see consolidation enable.

454
00:29:17,306 --> 00:29:20,750
True, let's say in this example you had

455
00:29:20,900 --> 00:29:24,990
five nodes within this node here on the right.

456
00:29:25,140 --> 00:29:28,706
What you can actually do once it goes back to Chi, you can see that

457
00:29:28,728 --> 00:29:31,490
you have a lot of underutilized resource.

458
00:29:31,910 --> 00:29:35,358
Carpenter will look at that. If you have the consolidation enabled

459
00:29:35,374 --> 00:29:39,078
and you say, you know what, I can actually run those chew pods in a

460
00:29:39,084 --> 00:29:42,566
much cheaper node. So it's going to

461
00:29:42,668 --> 00:29:44,440
spin up the node for you,

462
00:29:45,930 --> 00:29:49,382
it's going to spin up the node for you, then it's going to move those

463
00:29:49,436 --> 00:29:53,082
pods into the new node and finally it's going to remove the old

464
00:29:53,136 --> 00:29:56,410
node. So in that environment it actually allows you

465
00:29:56,480 --> 00:30:00,454
to delete a nodes when pods can run free on capacity

466
00:30:00,502 --> 00:30:03,866
that other exists in the cluster, but it can also delete

467
00:30:03,898 --> 00:30:08,414
a node when you

468
00:30:08,452 --> 00:30:11,198
don't have a lot of requirement for that big node that you have. And it

469
00:30:11,204 --> 00:30:14,820
can just create smaller ones like the one you saw here.

470
00:30:15,270 --> 00:30:19,490
That is just a replacing of a nodes, in this case a

471
00:30:19,560 --> 00:30:22,626
specific. So continue the information.

472
00:30:22,808 --> 00:30:25,460
The example here, you had four nodes on this one,

473
00:30:26,150 --> 00:30:29,678
the third node from the top. Now you only have one pod.

474
00:30:29,774 --> 00:30:32,326
What it's actually going to do is going to remove from here, it's going to

475
00:30:32,348 --> 00:30:35,366
move from the one in the bottom and it's just going to remove. So it

476
00:30:35,388 --> 00:30:39,420
keeps an idea of cost optimization, which is really important to

477
00:30:40,030 --> 00:30:42,330
scale your Kubernetes solutions.

478
00:30:44,190 --> 00:30:49,174
So here is we are just going to spend. A few examples

479
00:30:49,222 --> 00:30:53,082
here how you can configure your provisioning. So your provisioner

480
00:30:53,146 --> 00:30:56,666
is the Kubernetes object that once you deploy karpenter

481
00:30:56,698 --> 00:30:59,802
and you see on the demo you can provide specific configuration,

482
00:30:59,866 --> 00:31:03,454
how your provisioner can behave and you can create multiple

483
00:31:03,502 --> 00:31:06,994
provisioning with different weights or you

484
00:31:07,032 --> 00:31:10,862
want to match your specific pod to a specific provisioner.

485
00:31:10,926 --> 00:31:14,194
There are actually labels that you can actually mix and match.

486
00:31:14,392 --> 00:31:18,002
So one example for the flexibility

487
00:31:18,066 --> 00:31:21,846
of provisioning is the ability to select purchase options so you

488
00:31:21,868 --> 00:31:26,002
can select capacity type. In this case you have requirements

489
00:31:26,066 --> 00:31:29,814
capacity type. You actually are choosing spot

490
00:31:29,862 --> 00:31:33,446
and on demand. When you have spot and on demand configured

491
00:31:33,478 --> 00:31:35,862
at the same time on a specific provisioner,

492
00:31:36,006 --> 00:31:39,446
carpenter will always favor spot and it's

493
00:31:39,478 --> 00:31:42,878
only going to pick on demand if there are

494
00:31:42,964 --> 00:31:47,582
spot constraints. So if potentially the

495
00:31:47,716 --> 00:31:51,406
good options for spot to launch an UEC two instance for

496
00:31:51,428 --> 00:31:55,220
you are not available at that time then it's going to default back

497
00:31:55,590 --> 00:31:58,930
to on demand. But you can also select different

498
00:31:59,000 --> 00:32:02,286
architecture types. You can have provisioners that can deploy

499
00:32:02,318 --> 00:32:05,606
both arm 64 graviton two processors and

500
00:32:05,628 --> 00:32:09,000
AMD 64 processors type with x 86.

501
00:32:09,450 --> 00:32:12,886
So that means that that provisioner will look for

502
00:32:12,908 --> 00:32:16,566
the specific architecture type that your

503
00:32:16,588 --> 00:32:20,406
panning pod needs. And if it needs an arm and there

504
00:32:20,428 --> 00:32:23,914
is no capacity available on your cluster, it's actually just going to go and

505
00:32:23,952 --> 00:32:27,574
deploy a new arm gravitant tube ec

506
00:32:27,622 --> 00:32:31,200
two node. But it can also do that for AMD 64.

507
00:32:32,850 --> 00:32:36,654
Another capability is you can

508
00:32:36,772 --> 00:32:40,890
restrict instance selection

509
00:32:41,050 --> 00:32:44,494
by diversification across different configurations.

510
00:32:44,622 --> 00:32:47,650
So you can define the size,

511
00:32:47,720 --> 00:32:51,700
the family, the generation and the cpus. So in this example

512
00:32:52,310 --> 00:32:55,940
you don't want carpenter to spin up

513
00:32:56,250 --> 00:32:59,654
instances that are nano tiny small and

514
00:32:59,692 --> 00:33:03,014
large. You only want medium x large, two x large

515
00:33:03,052 --> 00:33:07,442
for x large. So you can create this specific requirement on your provisioner

516
00:33:07,506 --> 00:33:11,686
and then carpet will always look at those. And if you can have multiple provisions,

517
00:33:11,718 --> 00:33:15,366
but if whatever specific configuration have a pending pod

518
00:33:15,398 --> 00:33:19,498
that has gone to that specific provisioner then you

519
00:33:19,584 --> 00:33:23,406
just use the configuration you have in place. But you can also have

520
00:33:23,428 --> 00:33:27,710
availability zone. You can say well this provisioner should only deploy

521
00:33:28,050 --> 00:33:31,806
new nodes into us West QA and

522
00:33:31,828 --> 00:33:35,594
us two b availability nodes. So you can restrict

523
00:33:35,642 --> 00:33:39,202
for availability zones. If you have a requirement that you want to make sure your

524
00:33:39,256 --> 00:33:42,546
applications are only run or a set of your applications can run and

525
00:33:42,568 --> 00:33:46,614
run on this environment. Another thing

526
00:33:46,652 --> 00:33:50,566
you can actually do this is just

527
00:33:50,748 --> 00:33:54,562
a new specific provisioner.

528
00:33:54,706 --> 00:33:58,226
You can create different provisioners. In this case it's not a default

529
00:33:58,258 --> 00:34:02,042
provisioner, it's called west zones. And you can say well west zones can only

530
00:34:02,096 --> 00:34:06,394
deploy within these three availability zones on us west two

531
00:34:06,592 --> 00:34:09,830
region and it can do either spot and on demand.

532
00:34:09,910 --> 00:34:13,586
So between this is a very simple provisioner,

533
00:34:13,718 --> 00:34:17,342
you just pick whatever instance type is the more performant and available

534
00:34:17,396 --> 00:34:20,766
at a time. It's very like it's going to be a spot instance if

535
00:34:20,788 --> 00:34:22,160
it's available for you.

536
00:34:24,390 --> 00:34:28,142
And you can also isolate expensive hardware.

537
00:34:28,206 --> 00:34:31,906
So if you have needs, for example for applications that

538
00:34:31,928 --> 00:34:36,562
need a gpu, you can specify which instances

539
00:34:36,626 --> 00:34:40,006
you want this specific provision to deploy. So in this

540
00:34:40,028 --> 00:34:44,054
case GPU, you just want p three x eight x large

541
00:34:44,172 --> 00:34:47,766
or p 316 x large. But then what

542
00:34:47,788 --> 00:34:50,774
you do is you create a tent on those nodes.

543
00:34:50,822 --> 00:34:54,778
And if you're familiar with tent and toleration it means that only

544
00:34:54,944 --> 00:34:58,698
pods that have a toleration to support this specific

545
00:34:58,784 --> 00:35:02,142
tent will actually go on a go and be able to

546
00:35:02,276 --> 00:35:05,774
request and provision those resources within these

547
00:35:05,812 --> 00:35:09,642
nodes. So if you don't specify on your pods or deployments

548
00:35:09,706 --> 00:35:13,230
a toleration to support this tent, this is not going to be selected.

549
00:35:13,310 --> 00:35:17,314
But that gives you an ability to have different provisioners to fit

550
00:35:17,352 --> 00:35:21,086
your specific use case. And this is all declarative

551
00:35:21,198 --> 00:35:25,330
using kubernetes, custom resources definitions using crds.

552
00:35:27,610 --> 00:35:32,454
So hopefully I provided a little bit of information

553
00:35:32,572 --> 00:35:36,482
on carpenter before we do the demo, but there are some takeaways

554
00:35:36,546 --> 00:35:40,374
that if you're looking to implement Karpenter, you should be familiar

555
00:35:40,422 --> 00:35:45,146
with and or at least evaluate. The first one is if

556
00:35:45,168 --> 00:35:49,146
your application can support disruptions in the sense if

557
00:35:49,168 --> 00:35:52,334
you have distributed your applications across multiple nodes and

558
00:35:52,372 --> 00:35:55,934
availability zones, please use ECG spot instances to

559
00:35:55,972 --> 00:35:59,294
optimize for cost because Karpenter actually looks for

560
00:35:59,332 --> 00:36:02,634
those node interruptions for spot and reschedule

561
00:36:02,682 --> 00:36:06,618
automatically your pods into a new instance that will configure.

562
00:36:06,794 --> 00:36:10,194
But of course you don't want to be in that business. If you only

563
00:36:10,232 --> 00:36:13,746
have one pod, for whatever reason you have a stateful application that can only run

564
00:36:13,768 --> 00:36:17,126
in one pod, you probably want to avoid spot. So it needs to be a

565
00:36:17,148 --> 00:36:21,666
case by case. But most of the times if you're using stateless applications

566
00:36:21,858 --> 00:36:26,018
on kubernetes you should be using pod and then you're

567
00:36:26,034 --> 00:36:30,486
going to use provisioners to ensure that your scaling nodes and spots

568
00:36:30,678 --> 00:36:34,010
are actually implemented with best practice by default.

569
00:36:34,430 --> 00:36:38,298
Like I said, you can have multiple provisioners, but you

570
00:36:38,304 --> 00:36:41,866
should have a default provisioner with a very diverse instance type and

571
00:36:41,888 --> 00:36:45,374
availability zone. So if you don't have specific needs like

572
00:36:45,412 --> 00:36:48,558
GPU in that example, you can just let carpenter choose what

573
00:36:48,564 --> 00:36:52,286
is the best for you given a wide variety that you have configured on

574
00:36:52,308 --> 00:36:56,206
default provisioning. But then you configure additional provisioning for different compute

575
00:36:56,238 --> 00:36:59,586
constraints like you have a GPU or you have jobs that you

576
00:36:59,608 --> 00:37:03,438
want to make sure it runs on specific instance types because of performance

577
00:37:03,614 --> 00:37:07,042
or architecture. Then you create those additional provisioners

578
00:37:07,106 --> 00:37:10,418
and you link your deployments to those additional provisioners.

579
00:37:10,594 --> 00:37:14,274
And of course you want to control your scheduling using Kubernetes

580
00:37:14,322 --> 00:37:17,042
native solutions like node selector, topology,

581
00:37:17,106 --> 00:37:20,358
spread constraints, taints, tolerations and provisioners.

582
00:37:20,454 --> 00:37:23,674
You actually integrate that within your scheduling of your

583
00:37:23,712 --> 00:37:26,954
application. And of course you should use

584
00:37:26,992 --> 00:37:30,178
horizontal pod outscaler in conjunction with Carpenter.

585
00:37:30,214 --> 00:37:34,154
So you have HPA focusing on the application scaling

586
00:37:34,202 --> 00:37:38,090
and you have carpenter focusing on the cluster scaling,

587
00:37:38,170 --> 00:37:42,510
spinning up instances as needed for specific requirements.

588
00:37:45,110 --> 00:37:48,834
And then before we go into the demo, please look

589
00:37:48,872 --> 00:37:52,674
at these resources. I'm just going to go quick through

590
00:37:52,712 --> 00:37:55,694
them. The first one you have Karpenter webpage,

591
00:37:55,742 --> 00:37:59,718
Karpenter Sh, you have all the documentation and everything available

592
00:37:59,804 --> 00:38:03,046
there. You have a lot of examples and you go into a lot of

593
00:38:03,068 --> 00:38:06,706
details on how carpenter works. Because carpenter is open source,

594
00:38:06,738 --> 00:38:10,358
you can just look at the carpenter specific GitHub. If you

595
00:38:10,364 --> 00:38:14,186
have an issue, feel free to just create an issue on GitHub. Or if you

596
00:38:14,208 --> 00:38:17,398
need some help, the community is always there for helping.

597
00:38:17,574 --> 00:38:21,406
You have a workshop if you want to play around on

598
00:38:21,428 --> 00:38:24,798
your own with carpenter, you have two workshops here.

599
00:38:24,884 --> 00:38:28,126
The first one the carpenter workshops with the

600
00:38:28,148 --> 00:38:31,966
ecgspotworkshop.com. It goes in

601
00:38:31,988 --> 00:38:35,306
depth on carpenter. So it's a really good workshop.

602
00:38:35,418 --> 00:38:38,846
And if you just want a more high level, you can do the eks workshop

603
00:38:38,878 --> 00:38:42,098
and go to the carpenter selection and play around with those.

604
00:38:42,184 --> 00:38:45,506
And there is a really good 50 minutes video. If you

605
00:38:45,528 --> 00:38:48,962
just want to hear from other SMEs on AWS

606
00:38:49,026 --> 00:38:52,262
talking about Carpenter, you can just click on that button.

607
00:38:52,396 --> 00:38:55,800
And before I go on the demo, the only thing I want to mention is

608
00:38:56,410 --> 00:38:59,926
carpenter currently only supports AWS as a provider,

609
00:38:59,958 --> 00:39:03,610
but because carpenter is open source, we do expect in the future

610
00:39:03,680 --> 00:39:07,354
that potentially other providers can adopt carpenter and also

611
00:39:07,392 --> 00:39:10,986
make available for their users to utilize

612
00:39:11,018 --> 00:39:14,970
this flexible way of auto scaling

613
00:39:15,050 --> 00:39:19,134
on kubernetes. So we'll see you in a moment on the

614
00:39:19,172 --> 00:39:22,622
demo. Okay, so let's jump in

615
00:39:22,676 --> 00:39:27,858
into the demo. So I have created before

616
00:39:28,024 --> 00:39:31,250
a eks cluster. So if you just want to go

617
00:39:31,320 --> 00:39:34,706
and take a look at the nodes that I have on my cluster. So I

618
00:39:34,728 --> 00:39:38,326
have actually two nodes already created. Those are actually nodes that

619
00:39:38,348 --> 00:39:42,418
are managed by a managed node group on eks, those are not managed

620
00:39:42,434 --> 00:39:46,134
by carpenter. I just want to start from a

621
00:39:46,172 --> 00:39:49,766
clean slate. So you need a place where carpenter will work.

622
00:39:49,868 --> 00:39:53,462
So you can actually have Karpenter being deployed on a managed node group.

623
00:39:53,516 --> 00:39:56,650
But that managed node group doesn't need shisk or anything like that.

624
00:39:56,720 --> 00:40:01,482
So if I go on the console and I just show you I have one

625
00:40:01,536 --> 00:40:04,926
managed node group, which two desired instance which are

626
00:40:04,948 --> 00:40:08,366
the ones I showed and they are up and running. And if

627
00:40:08,388 --> 00:40:11,754
you look at the nodes that I currently have available on this nodes

628
00:40:11,802 --> 00:40:15,430
here, nothing fancy, I just have this cube apps

629
00:40:15,450 --> 00:40:18,942
view which is a application that I can look at the stats

630
00:40:19,006 --> 00:40:22,494
and a nice visualization of my nodes, AWS nodes,

631
00:40:22,542 --> 00:40:25,714
each of the specific nodes to talk to.

632
00:40:25,752 --> 00:40:29,894
AWS core DNS cube proxy and you

633
00:40:29,932 --> 00:40:33,266
know, if I want to use HPA I need metric server.

634
00:40:33,298 --> 00:40:36,822
So it's deployed behind here. It's a nice tool.

635
00:40:36,876 --> 00:40:39,954
It's called eks node reviewer. It's open resources.

636
00:40:40,002 --> 00:40:44,074
You can just Google eks node viewer. This actually shows in

637
00:40:44,112 --> 00:40:47,590
real time what is the currently state of my eks cluster.

638
00:40:47,670 --> 00:40:51,066
So the one on the top here is the cluster aggregation. You can see

639
00:40:51,088 --> 00:40:54,622
the price per hour and the price per month. And below here it's per

640
00:40:54,676 --> 00:40:58,174
node which instance type how many nodes are

641
00:40:58,212 --> 00:41:01,726
running each of them, instance type, the price if they are

642
00:41:01,748 --> 00:41:05,966
on demand and they're ready. You see as I go through and install carpenter,

643
00:41:05,998 --> 00:41:09,554
and once carpenter will actually go and deploy things for me,

644
00:41:09,672 --> 00:41:12,686
you see that this will keep changing.

645
00:41:12,718 --> 00:41:16,226
So that's why I'm sharing with you. So I have

646
00:41:16,248 --> 00:41:20,594
everything already set up. I just want to install Karpenter. And so carpenter

647
00:41:20,642 --> 00:41:23,954
is available AWS, a helm chart. I have this command

648
00:41:24,002 --> 00:41:26,694
here that I'm just going to deploy. What is this actually doing?

649
00:41:26,812 --> 00:41:30,634
It's creating the carpenter installation for me.

650
00:41:30,832 --> 00:41:34,458
I have already some environmental variables and some pre configuration that

651
00:41:34,464 --> 00:41:38,102
I've done. Actually if you want to deploy carpenter

652
00:41:38,246 --> 00:41:41,414
with integration with EC two spot, there are a few pre configuration

653
00:41:41,462 --> 00:41:45,262
you got to do like creating, you know,

654
00:41:45,316 --> 00:41:49,470
making sure you're creating rules for that sqs queue for an event bridge

655
00:41:49,810 --> 00:41:53,198
and so forth. Those were already created for me. So I

656
00:41:53,204 --> 00:41:56,946
have deployed carpenter and now if I go and I look, so let's look

657
00:41:56,968 --> 00:42:00,894
at the pods that carpenter have. So carpenter is deployed

658
00:42:00,942 --> 00:42:04,738
within its specific namespace called Carpenter. So if I

659
00:42:04,744 --> 00:42:08,534
look at the nodes that carpenter has actually deployed, I can

660
00:42:08,572 --> 00:42:12,006
see that I have two. So karpenter doesn't work as

661
00:42:12,028 --> 00:42:15,574
a demon set. Carpenter is just a deployment. And if you look here,

662
00:42:15,612 --> 00:42:19,306
Kubectl get deployment on

663
00:42:19,328 --> 00:42:23,066
the carpenter namespace you're actually going to see that it's just a

664
00:42:23,088 --> 00:42:27,142
deployment and Karpenter works in an active standby

665
00:42:27,206 --> 00:42:30,730
approach. So there are two pods and they are going to be running

666
00:42:30,800 --> 00:42:34,062
across different nodes but only one pod at a given

667
00:42:34,116 --> 00:42:37,214
time is responsible for making those

668
00:42:37,252 --> 00:42:40,974
decisions and making the scaling actions for me. So the other one

669
00:42:41,012 --> 00:42:44,402
will actually take place as the lead if something happens with the first

670
00:42:44,456 --> 00:42:48,734
one. So it's important to understand that it's a high availability scenario.

671
00:42:48,782 --> 00:42:52,690
It's not using demo set, it's just a deployment. They have both enabled.

672
00:42:53,030 --> 00:42:56,694
So what we're going to do now we

673
00:42:56,732 --> 00:43:00,374
have created Karpenter but I haven't created my

674
00:43:00,412 --> 00:43:04,322
provisioner. And provisioner is what actually is responsible

675
00:43:04,386 --> 00:43:07,746
for making those decisions. You saw on this slide before is how

676
00:43:07,788 --> 00:43:12,534
you tell which decisions you want to make when there is a specific scaling

677
00:43:12,582 --> 00:43:16,490
activity. And then there is also the AWS node template

678
00:43:16,830 --> 00:43:20,566
object that is responsible to telling Carpenter

679
00:43:20,598 --> 00:43:23,774
how he actually goes and talks to the cloud to scale up

680
00:43:23,812 --> 00:43:27,646
and scale down applications on easy to instances on your

681
00:43:27,668 --> 00:43:30,974
AWS account. So I have pre configured a very

682
00:43:31,012 --> 00:43:34,078
simple example here. Just gonna paste it and I'm going to

683
00:43:34,084 --> 00:43:38,222
show it to you. So what I've done so far, I've created, I installed Carpenter

684
00:43:38,286 --> 00:43:41,326
and I have deployed a provisioner and I've deployed an AWS

685
00:43:41,358 --> 00:43:45,554
template. So let's just quickly look at the provisioner and see what this provisioner

686
00:43:45,602 --> 00:43:49,910
tells us. So if I go kubectl get provisioner

687
00:43:50,970 --> 00:43:54,614
default OEM that is the name of my

688
00:43:54,652 --> 00:43:58,600
provisioner. So what this provisioner is telling me

689
00:43:58,970 --> 00:44:01,420
is telling me that every single,

690
00:44:04,190 --> 00:44:08,918
this provisioner has a label of intent apps and we'll

691
00:44:08,934 --> 00:44:12,622
see in a moment why that is important. You can also create some

692
00:44:12,676 --> 00:44:16,010
limits on your provisioner. So the provisioner

693
00:44:16,090 --> 00:44:19,834
will keep an account of how much cpu

694
00:44:19,882 --> 00:44:23,342
and memory it has controlling and you can define how much

695
00:44:23,396 --> 00:44:27,022
memory you want to give memory and cpu aggregated

696
00:44:27,086 --> 00:44:30,100
on all the instances that that provisioner will create.

697
00:44:30,790 --> 00:44:34,142
What is the limit? So this provisioner will never go above

698
00:44:34,206 --> 00:44:37,334
1000 cpu and eight terabyte of

699
00:44:37,372 --> 00:44:41,366
memory. Then I provide a specific name for my

700
00:44:41,388 --> 00:44:44,914
provisioner. This is the default provisioner and here I provide some requirements.

701
00:44:44,962 --> 00:44:48,694
So I'm saying that for my capacity type I just

702
00:44:48,732 --> 00:44:52,570
want to do spot. So this is only going to do spot.

703
00:44:53,230 --> 00:44:56,922
And then I'm saying for my instance types I don't want to be

704
00:44:56,976 --> 00:45:00,454
nanomic small, medium large. I only want instance to actually

705
00:45:00,512 --> 00:45:04,174
be two x large and above. And then for

706
00:45:04,292 --> 00:45:08,266
operating systems I only want carpenter to actually deploy Linux

707
00:45:08,458 --> 00:45:12,042
and for my app architecture I only want Karpenter

708
00:45:12,106 --> 00:45:15,574
to actually deploy AMD 64 instances

709
00:45:15,722 --> 00:45:19,394
and the instance category are only CM and

710
00:45:19,432 --> 00:45:22,706
R. I know there is a lot here, you don't need

711
00:45:22,728 --> 00:45:26,422
to do that. If you just leave all empty on the requirements carpenter will figure

712
00:45:26,476 --> 00:45:30,262
out by itself. But I'm just showcasing how

713
00:45:30,316 --> 00:45:33,974
flexible and customizable a provisioner can be.

714
00:45:34,172 --> 00:45:37,862
And I'm saying that instant generation can only be greater than

715
00:45:37,916 --> 00:45:41,574
two. So you won't be able to deploy like an m two.

716
00:45:41,772 --> 00:45:44,906
I don't even know if those are still available, but it won't be able to

717
00:45:44,928 --> 00:45:48,554
deploy an M two instance if that was available. And then I'm not

718
00:45:48,592 --> 00:45:52,246
using consolidation this specific example, I'm just using

719
00:45:52,288 --> 00:45:56,142
the TTL seconds after empty. So 30 seconds after

720
00:45:56,196 --> 00:45:59,726
the nodes is empty is going to remove and then this is an

721
00:45:59,748 --> 00:46:03,306
interesting one that it says Ttl seconds until expire.

722
00:46:03,418 --> 00:46:07,102
You can create expiration dates on your nodes which helps you

723
00:46:07,156 --> 00:46:10,814
with lifecycle managements. And if you want to do new AMI

724
00:46:10,862 --> 00:46:13,982
updates, this is a good selection. You can specify

725
00:46:14,046 --> 00:46:17,990
a specific number here and after that number has expired

726
00:46:18,810 --> 00:46:22,326
Karpenter automatically move, create a new instance with

727
00:46:22,348 --> 00:46:25,814
the new AMI if you have a new AMI in place

728
00:46:25,932 --> 00:46:29,814
or use the old AMi if you don't. And then

729
00:46:29,932 --> 00:46:33,594
we'll start deploying those nodes from this

730
00:46:33,632 --> 00:46:37,542
old instance to the new instance. So we always keep those instance fresh.

731
00:46:37,686 --> 00:46:41,294
The other object has deployed is called

732
00:46:41,332 --> 00:46:43,550
the AWS node template.

733
00:46:44,850 --> 00:46:47,840
And if we go just show Yaml here,

734
00:46:49,170 --> 00:46:52,994
you can see that what this does, it is

735
00:46:53,032 --> 00:46:56,420
responsible for. So this is the important piece here.

736
00:46:57,430 --> 00:47:01,118
So what this does, this is responsible for telling

737
00:47:01,214 --> 00:47:04,942
a carpenter how do you actually deploy new instances

738
00:47:05,006 --> 00:47:08,866
like which security group do you use when you are deploying those instances,

739
00:47:08,978 --> 00:47:12,134
which is the subnet that you use when you deploy these new

740
00:47:12,172 --> 00:47:15,654
instances. And potentially you

741
00:47:15,692 --> 00:47:19,170
want to select some tags on those

742
00:47:19,260 --> 00:47:22,794
instances as well, right? So you can provide this

743
00:47:22,832 --> 00:47:26,026
here, there are many more other configurations. You can specify an

744
00:47:26,048 --> 00:47:29,642
AMI, a specific AMI here, you can do much more

745
00:47:29,696 --> 00:47:33,674
configurations and you can check that out on the carpenter documentation.

746
00:47:33,802 --> 00:47:37,086
But let's go and try to

747
00:47:37,108 --> 00:47:40,794
do a deployment where carpenter can go and spin up new instances

748
00:47:40,842 --> 00:47:44,286
for me. So let's just going to go here and

749
00:47:44,308 --> 00:47:48,094
let's just going to create a specific deployment using inflate.

750
00:47:48,222 --> 00:47:51,570
So I have deployed, so here I'm deploying

751
00:47:52,150 --> 00:47:55,720
this object called deployment zero replicas right now.

752
00:47:56,170 --> 00:47:59,926
And I'm telling here on

753
00:47:59,948 --> 00:48:03,142
this deployment that I want to select nodes with

754
00:48:03,196 --> 00:48:07,266
intent apps. And if you remember carpenter once the nodes

755
00:48:07,298 --> 00:48:10,954
are created will have intent apps. So I'm just telling this is just

756
00:48:10,992 --> 00:48:15,050
to say please don't deploy this application on the existing two nodes.

757
00:48:15,790 --> 00:48:19,818
Deploy with nodes that have this label intent apps and then

758
00:48:19,984 --> 00:48:23,534
carpenter will actually spin up those nodes with this label and

759
00:48:23,572 --> 00:48:27,306
just doing a pause container and I'm giving one cpu

760
00:48:27,418 --> 00:48:31,482
for each pod and 1.5gb

761
00:48:31,546 --> 00:48:34,770
per each pod when I deploy. But because of course

762
00:48:34,840 --> 00:48:38,974
if we do kubectl get deployment I have zero replicas so it haven't created

763
00:48:39,022 --> 00:48:42,642
anything for me. Right so let's just

764
00:48:42,696 --> 00:48:46,354
see 1 second deployment. So you

765
00:48:46,392 --> 00:48:49,846
see here that I have inflate it

766
00:48:49,868 --> 00:48:53,622
hasn't done anything. So what I want to do here let's just go and

767
00:48:53,676 --> 00:48:56,774
actually scale this up for us. So let's just create one

768
00:48:56,812 --> 00:49:00,410
replica to start with. So let's just go create one replica.

769
00:49:02,270 --> 00:49:05,754
So you see now in a moment down below, keep an eye down below

770
00:49:05,872 --> 00:49:09,526
you see there you go. Karpenter looked that there was spanning

771
00:49:09,558 --> 00:49:14,174
pods because there wasn't any node that

772
00:49:14,212 --> 00:49:18,154
was able to sustain all the requirements that those pods

773
00:49:18,202 --> 00:49:21,360
had. So now it's creating an L four x large

774
00:49:21,890 --> 00:49:25,866
that is its pod because remember my provisioning only said spot

775
00:49:25,898 --> 00:49:29,186
it was not supporting on demand. It tells me the price and

776
00:49:29,208 --> 00:49:32,274
it's an L four x large so you can see now that

777
00:49:32,312 --> 00:49:35,634
it's actually spinning up the node. You take maybe a minute or so

778
00:49:35,672 --> 00:49:39,234
to spin up the nodes. Once the node is up and running you see

779
00:49:39,272 --> 00:49:41,926
and we can keep an eye actually let's look at the logs so we can

780
00:49:41,948 --> 00:49:46,038
look at the logs for let's just do one thing here.

781
00:49:46,124 --> 00:49:49,770
Let's look at the logs for carpenter so

782
00:49:49,840 --> 00:49:54,166
let's look at the logs for Karpenter and see what logs

783
00:49:54,198 --> 00:49:58,010
are telling us. So if you do kubectl logs and carpenter

784
00:50:00,450 --> 00:50:02,720
if you see here, let's just wait a moment.

785
00:50:04,850 --> 00:50:08,206
You can see yeah so you can see

786
00:50:08,228 --> 00:50:11,854
that it started. Let's see where it actually says oh

787
00:50:11,892 --> 00:50:15,586
sorry this is actually not so remember I

788
00:50:15,608 --> 00:50:18,786
said that carpenter has two pods. I select the pod that is

789
00:50:18,808 --> 00:50:22,660
not the lead is the standby one. So let's just select this 1

790
00:50:25,370 --> 00:50:27,800
second let's just select this one.

791
00:50:30,890 --> 00:50:33,814
There you go. Kubectl logs M.

792
00:50:33,852 --> 00:50:37,078
Carpenter and let's look at the logs so you

793
00:50:37,084 --> 00:50:40,442
can see that. Found provisionable pods. Oh sorry yeah,

794
00:50:40,496 --> 00:50:44,554
found provisionable pods. Compute new nodes to fit pods. So he

795
00:50:44,592 --> 00:50:47,702
found a pending pod, then it launched a new node

796
00:50:47,766 --> 00:50:51,286
here and then it discovered the security group for my node.

797
00:50:51,318 --> 00:50:54,778
It discovered the specific Kubernetes version discovered the AMI create a

798
00:50:54,784 --> 00:50:58,270
launch template and launch the instance which is this instance that you see here.

799
00:50:58,340 --> 00:51:01,758
And if you now look at the Kubectl get nodes, you see that

800
00:51:01,764 --> 00:51:04,946
I have a new node which is the one down below here that ends with

801
00:51:04,968 --> 00:51:09,122
145, that has one pod. So if you see here kubectl get

802
00:51:09,176 --> 00:51:12,706
pods o wide and

803
00:51:12,728 --> 00:51:16,078
we see that this pod for inflate

804
00:51:16,174 --> 00:51:20,022
is running on that 145 instance. But what

805
00:51:20,076 --> 00:51:23,878
happened? If I want to scale this specific, let's say

806
00:51:23,884 --> 00:51:27,526
I want to scale this deployment a little bit more, let's say I want

807
00:51:27,548 --> 00:51:30,730
you scale this deployment, you have ten replicas, what will actually

808
00:51:30,800 --> 00:51:33,834
happen? So if I go and I do a deployment, okay, I need

809
00:51:33,872 --> 00:51:37,354
ten replicas, right? So you

810
00:51:37,392 --> 00:51:42,174
see that it has a schedule five nodes here and

811
00:51:42,212 --> 00:51:45,662
now he said well ten replicas won't fit in this

812
00:51:45,716 --> 00:51:48,878
r four x large, it just won't, right?

813
00:51:48,964 --> 00:51:52,622
So what in this case will happen, Karpenter will say

814
00:51:52,676 --> 00:51:56,426
okay, I need a new instance and it looks for whatever capacity

815
00:51:56,458 --> 00:52:00,146
he had available that would fit the requirements. So always remember it

816
00:52:00,168 --> 00:52:04,834
looks for performance and cost and it has a spin up c

817
00:52:04,872 --> 00:52:08,002
xlarge so it is spinning up and we can actually see

818
00:52:08,056 --> 00:52:11,734
Kubectl get pods, we can see the status of those

819
00:52:11,772 --> 00:52:15,160
pods. So I have one, two,

820
00:52:15,850 --> 00:52:19,162
three nodes that were running that actually fitted here.

821
00:52:19,296 --> 00:52:22,682
The other two pods that are running on this

822
00:52:22,736 --> 00:52:26,406
specific node are demon sets, the AWS node

823
00:52:26,438 --> 00:52:30,106
and cube proxy that you need to deploy. Kubernetes will

824
00:52:30,128 --> 00:52:33,898
deploy automatically but that needs to be deployed on those instance,

825
00:52:33,994 --> 00:52:37,406
on every single instance. So now it's ready and we

826
00:52:37,428 --> 00:52:40,974
can see if we do get nodes again off

827
00:52:41,012 --> 00:52:45,620
then are running and I have nine nodes running right again

828
00:52:47,110 --> 00:52:50,338
it has all the seven pods for inflate and

829
00:52:50,344 --> 00:52:53,714
the two demon sets that is required for my pod to run.

830
00:52:53,912 --> 00:52:57,634
But if now if I go and I finally let's scale this to zero.

831
00:52:57,832 --> 00:53:01,330
So now I want to see how capitol behaves to remove.

832
00:53:01,410 --> 00:53:05,170
So remember I don't have consolidation here enabled

833
00:53:05,250 --> 00:53:09,266
but after 30 seconds that my nodes

834
00:53:09,298 --> 00:53:12,438
are empty and you see there are two pods but those two pods

835
00:53:12,454 --> 00:53:15,882
are demon sets. And you can see here that if we do Kubectl get

836
00:53:15,936 --> 00:53:19,626
pods a and we look for all the pods for example that

837
00:53:19,648 --> 00:53:22,746
are on 145. So let's

838
00:53:22,778 --> 00:53:26,400
just do apologies, let's just bring this up again

839
00:53:27,170 --> 00:53:30,926
and if we do show white, if you look for all the pods that are

840
00:53:30,948 --> 00:53:34,126
running, for example the one 9145,

841
00:53:34,228 --> 00:53:37,966
I mean they were gone, you see now they are gone right? Like carpenter

842
00:53:37,998 --> 00:53:42,034
already removed. But if you look before 9145

843
00:53:42,232 --> 00:53:46,834
you see here that this was qproxy and 9145

844
00:53:46,952 --> 00:53:50,086
this was AWs node which is just demon sets that

845
00:53:50,108 --> 00:53:53,206
are running. But you saw how carpenter can works.

846
00:53:53,388 --> 00:53:57,046
So just because I'm running out of time here, one thing I actually want

847
00:53:57,068 --> 00:53:59,942
to do is I want to enable consolidation.

848
00:54:00,006 --> 00:54:03,770
So I'm going to replace my default provisioner.

849
00:54:04,190 --> 00:54:07,306
So I'm going to replace my default provisioner. So what you

850
00:54:07,328 --> 00:54:11,230
see on my default provisioning here now I'm telling consolidation is true.

851
00:54:11,300 --> 00:54:13,600
So you can see here consolidation true.

852
00:54:14,930 --> 00:54:18,718
And then 1 second just my computer is

853
00:54:18,724 --> 00:54:22,298
a little bit slow. I'm saying that I only want on demand

854
00:54:22,394 --> 00:54:26,114
now I don't want spot and I don't want these specific

855
00:54:26,232 --> 00:54:29,874
instance sizes and that's pretty much it. And remember once

856
00:54:29,912 --> 00:54:34,530
you have consolidation you need to remove the TTL seconds

857
00:54:35,050 --> 00:54:38,402
until seconds after empty.

858
00:54:38,466 --> 00:54:43,186
So I don't need that. So what I want to do now let's

859
00:54:43,218 --> 00:54:46,440
deploy three replicas of my inflate application here.

860
00:54:47,530 --> 00:54:51,514
So remember now I have consolidation enabled. Let's see

861
00:54:51,552 --> 00:54:55,958
what is the difference. Right. Okay, I have said okay, spin up three more replicas.

862
00:54:56,054 --> 00:54:59,238
Karpenter said okay, in order to spin up three more replicas,

863
00:54:59,334 --> 00:55:02,506
now it's on demand because my provisioner said it was on demand and

864
00:55:02,528 --> 00:55:06,394
not using spot anymore. He found that this specific instance

865
00:55:06,442 --> 00:55:09,966
is the most performance and cost optimized for. What I'm trying to

866
00:55:09,988 --> 00:55:13,474
do is now spinning up those. So we're just going to wait for those

867
00:55:13,512 --> 00:55:14,660
to spin up.

868
00:55:16,470 --> 00:55:17,220
Okay.

869
00:55:19,270 --> 00:55:23,006
Remember the idea of actually having beam packing

870
00:55:23,038 --> 00:55:27,202
and fast rescaling cluster scaling is going to be much slower than this.

871
00:55:27,336 --> 00:55:31,014
You could see that it's actually pre pulling the image and making

872
00:55:31,052 --> 00:55:34,390
those cached available just when the node is up and running.

873
00:55:34,460 --> 00:55:38,134
So it's doing already some work before even the node is ready on

874
00:55:38,172 --> 00:55:41,754
kubernetes. And you can see those details here. And off

875
00:55:41,792 --> 00:55:45,862
he goes. They are actually available. So if we quickly do kubectl

876
00:55:45,926 --> 00:55:49,580
get pods, I can see that I have three pods. And if you do,

877
00:55:52,030 --> 00:55:55,246
you can see that I have three pods. Let's just

878
00:55:55,268 --> 00:55:59,146
wait a second. You can see that I have three nodes that are schedule

879
00:55:59,258 --> 00:56:03,242
132 which is the new one. So now finally let's deploy,

880
00:56:03,306 --> 00:56:07,486
let's actually go and do the same. Let's do, okay, I want ten replicas.

881
00:56:07,678 --> 00:56:11,074
Okay, now I want ten replicas. Let's see what carpenter does. Right, okay.

882
00:56:11,112 --> 00:56:14,910
Carpenter said I cannot fit all the remaining

883
00:56:14,990 --> 00:56:18,514
seven nodes that you want me to deploy on the existing

884
00:56:18,642 --> 00:56:22,006
infrastructure. So now I am deploying another c, two a.

885
00:56:22,108 --> 00:56:25,190
They are two x large, also on demand.

886
00:56:26,170 --> 00:56:29,260
So let's wait and see what happens there.

887
00:56:30,830 --> 00:56:34,890
It's taking a few minutes to actually a minute or so to scale

888
00:56:36,270 --> 00:56:39,978
to create a new instance and then deploy those containers on

889
00:56:39,984 --> 00:56:44,000
that new instance. But the goal I'm trying to do, you can see that

890
00:56:45,090 --> 00:56:48,574
before it's even ready on the Kubernetes, Carpenter is already making

891
00:56:48,612 --> 00:56:52,046
all the work behind the scenes. And you can see

892
00:56:52,068 --> 00:56:55,346
all the logs here. If you look at the logs. Let's see if we get

893
00:56:55,368 --> 00:56:56,660
the logs here.

894
00:56:58,550 --> 00:57:00,820
It's just going to 1 second here.

895
00:57:04,470 --> 00:57:07,298
So you can see the logs that it has deployed. A new instance and they

896
00:57:07,304 --> 00:57:12,120
are all available. So if you do Kubectl get pause a

897
00:57:12,570 --> 00:57:15,974
and we see all the pods and you see all the inflates. Now, I have

898
00:57:16,012 --> 00:57:19,750
pods on 106, but I also have pods on 132,

899
00:57:19,820 --> 00:57:23,434
which is 106 and 32, the one I have deployed. So what

900
00:57:23,472 --> 00:57:27,114
should happen if I actually now scale my application down to

901
00:57:27,152 --> 00:57:30,474
six? Remember what? Consolidation does not

902
00:57:30,512 --> 00:57:34,374
only remove empty nodes, but also try to make good decisions

903
00:57:34,422 --> 00:57:38,302
if we can do it. So I'm going to spin up to six

904
00:57:38,356 --> 00:57:41,886
replicas now instead of

905
00:57:41,908 --> 00:57:45,746
Santa one, six replicas. So you see that, okay, it has

906
00:57:45,928 --> 00:57:50,210
now removed to 40% utilization.

907
00:57:52,310 --> 00:57:56,162
So let's see if Carpenter will make any decision here.

908
00:57:56,216 --> 00:58:00,086
It might make a decision here or it might not. So let's just wait

909
00:58:00,188 --> 00:58:02,806
a few minutes. So he made a decision. He said,

910
00:58:02,908 --> 00:58:06,358
okay, the remaining pods that you had,

911
00:58:06,524 --> 00:58:10,534
the six replicas that you had, could actually all be

912
00:58:10,572 --> 00:58:14,650
filled within the chew xlarge. So we remove my

913
00:58:14,800 --> 00:58:18,218
x large instance before removed, of course,

914
00:58:18,304 --> 00:58:21,414
moved all the pods into the existing node

915
00:58:21,462 --> 00:58:25,038
here. Because he saw like, well, you shouldn't have two. You don't need to

916
00:58:25,044 --> 00:58:28,714
have two instance. You can only have one instance, keep the bigger instance,

917
00:58:28,762 --> 00:58:32,922
move the nodes to the bigger instance, and then remove the smaller instance.

918
00:58:33,066 --> 00:58:36,834
So we see that. But what happens now if we go a

919
00:58:36,872 --> 00:58:40,370
step above, we have this two x large.

920
00:58:40,520 --> 00:58:43,246
What happens if you only need now three replicas?

921
00:58:43,358 --> 00:58:46,806
Right? So you only need three replicas. And you can

922
00:58:46,828 --> 00:58:50,822
see now that only 40% of the two

923
00:58:50,876 --> 00:58:53,554
x large is being utilized.

924
00:58:53,682 --> 00:58:57,618
Carpenter should actually go and look for a cheaper instance

925
00:58:57,714 --> 00:59:02,060
that can fit those spots. So let's just wait.

926
00:59:03,790 --> 00:59:07,180
Oh, actually what this is going to happen here is

927
00:59:07,790 --> 00:59:11,706
because. Yeah, there you go. Because you

928
00:59:11,728 --> 00:59:15,082
can see that it's now cordon. It's waiting for a new instance.

929
00:59:15,146 --> 00:59:18,750
That's xlarge to actually to be available because

930
00:59:18,820 --> 00:59:22,254
much cheaper than two xlarge. Right? So it's waiting for a new

931
00:59:22,292 --> 00:59:26,078
instance to be up and running. Once that instance is rampant running, it's going

932
00:59:26,084 --> 00:59:29,954
to move the pods from the bigger instance to the smaller instance. And once

933
00:59:29,992 --> 00:59:33,058
those pods are moved and ready and running, it's going

934
00:59:33,064 --> 00:59:36,802
to remove the bigger instance. So long story

935
00:59:36,856 --> 00:59:40,034
short, carpenter will always be looking for

936
00:59:40,072 --> 00:59:43,366
you for the best performance and cost optimized way

937
00:59:43,548 --> 00:59:47,526
and you can create many different things, multiple provisioning that will

938
00:59:47,548 --> 00:59:50,938
fit specific needs for your application. But hopefully I

939
00:59:50,944 --> 00:59:54,794
was able to demonstrate once this finished. So you see this is ready

940
00:59:54,992 --> 00:59:58,540
and it actually hasn't removed the two xlarge. And if I look at

941
00:59:58,990 --> 01:00:02,060
the pods running here and we look

942
01:00:02,670 --> 01:00:06,386
once, just a few seconds, when my page refreshes

943
01:00:06,438 --> 01:00:10,494
here, you see that all the inflate pods are

944
01:00:10,532 --> 01:00:15,022
actually now running on my specific 123

945
01:00:15,076 --> 01:00:18,574
instance. So you can see 123 instances they are up

946
01:00:18,612 --> 01:00:21,806
and running. So I just want to say thank you so

947
01:00:21,828 --> 01:00:25,286
much. Hopefully the demo was useful. Please reach me

948
01:00:25,308 --> 01:00:28,806
out on Twitter and LinkedIn if you have any questions. Go ahead and

949
01:00:28,828 --> 01:00:32,226
test carpenter. I would highly recommend running carpenter on eks.

950
01:00:32,338 --> 01:00:35,654
It can make your life much easier, more flexible and

951
01:00:35,692 --> 01:00:39,686
more cost optimized. So hope you had fun. Thanks for

952
01:00:39,708 --> 01:00:42,820
tuning in and have a great rest of your conference. Bye bye everyone.

