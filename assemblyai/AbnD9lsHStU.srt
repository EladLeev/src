1
00:00:38,930 --> 00:00:42,374
Hello everyone, my name is Brunfre and

2
00:00:42,492 --> 00:00:46,370
in this talk I will tell you about optimizing observability

3
00:00:46,530 --> 00:00:49,914
using the Opentelemetry collector for budget friendly

4
00:00:50,002 --> 00:00:54,126
Insights. I'm currently a DevOps engineer at Theydo and

5
00:00:54,148 --> 00:00:57,338
they do is a platform for customer centric collaboration.

6
00:00:57,514 --> 00:01:00,442
The good news is that we are hiring for different roles,

7
00:01:00,506 --> 00:01:04,000
so feel free to check out our product and careers page.

8
00:01:04,450 --> 00:01:08,050
I think most of us agree that 2023 was not

9
00:01:08,120 --> 00:01:12,114
economically great. We had a rising inflation, lots of tech

10
00:01:12,152 --> 00:01:15,614
companies doing massive layoffs, and hiring freezes throughout

11
00:01:15,662 --> 00:01:20,346
the year. And yeah, taking these contexts

12
00:01:20,478 --> 00:01:24,406
into account, the last thing you want is to have a

13
00:01:24,428 --> 00:01:28,198
$65 million bill for your observability

14
00:01:28,294 --> 00:01:31,994
system as we read on the news during last year for

15
00:01:32,032 --> 00:01:35,642
a famous cryptocurrency company. In this talk

16
00:01:35,696 --> 00:01:39,970
I will share our experience at theydo adopting opentelemetry,

17
00:01:40,150 --> 00:01:43,770
and hopefully you can get some useful tips on how to avoid

18
00:01:43,850 --> 00:01:47,294
this kind of spending when setting up your

19
00:01:47,412 --> 00:01:51,054
observability system. Last year we

20
00:01:51,092 --> 00:01:54,302
did a major effort at Deidu to adopt opentelemetry

21
00:01:54,366 --> 00:01:56,770
as our observability framework,

22
00:01:57,510 --> 00:02:00,834
and depending on the platform that you are using

23
00:02:00,872 --> 00:02:04,062
to store and process your open telemetry signals,

24
00:02:04,206 --> 00:02:08,070
either metrics, traces and logs or logs,

25
00:02:08,410 --> 00:02:11,986
you might be either charged by the amount of ingested

26
00:02:12,018 --> 00:02:16,930
data or in some vendors you will be eventually throttled.

27
00:02:17,090 --> 00:02:20,922
If you are ingesting more data than you should

28
00:02:20,976 --> 00:02:24,540
or it's included in your plan.

29
00:02:26,430 --> 00:02:29,754
And that was our main issue during the first few

30
00:02:29,792 --> 00:02:32,586
weeks after adopting opentelemetry.

31
00:02:32,778 --> 00:02:36,190
As you can see here on the image on the right,

32
00:02:36,340 --> 00:02:39,806
our usage was low, so we

33
00:02:39,828 --> 00:02:42,726
were below the threshold during weekends,

34
00:02:42,778 --> 00:02:47,074
but during weekdays we were above the

35
00:02:47,112 --> 00:02:50,770
threshold, so we were above the daily target.

36
00:02:51,110 --> 00:02:54,818
And as we had more load on our product

37
00:02:54,984 --> 00:02:57,910
and we were ingesting more data than we should,

38
00:02:58,060 --> 00:03:01,346
eventually we would be throttled and events

39
00:03:01,378 --> 00:03:04,680
would be rejected. So we had to do something about this.

40
00:03:05,370 --> 00:03:08,906
So the first question we asked was ourselves was like,

41
00:03:09,008 --> 00:03:12,378
do we really need all this data? And the answer

42
00:03:12,464 --> 00:03:16,922
was, well, probably not. So our

43
00:03:16,976 --> 00:03:20,494
first action was, okay, then let's pick the data

44
00:03:20,612 --> 00:03:23,520
that is actually useful for us. But how?

45
00:03:25,490 --> 00:03:29,120
The first thing was to really think

46
00:03:31,010 --> 00:03:34,466
how we were using the auto instrumentation. And the first tip I can give you

47
00:03:34,488 --> 00:03:38,370
is to be careful with the auto instrumentation. If you're using

48
00:03:38,440 --> 00:03:41,726
one of the popular libraries to have auto

49
00:03:41,758 --> 00:03:45,086
instrumentation for node JS or Python, it's really

50
00:03:45,128 --> 00:03:48,466
common that the default configurations will be just sending

51
00:03:48,498 --> 00:03:51,240
too many spans that you won't need.

52
00:03:53,930 --> 00:03:57,778
So auto instrumentation is really useful to have your initial signals

53
00:03:57,874 --> 00:04:01,786
sent to your observability back end, but at

54
00:04:01,808 --> 00:04:04,506
some point you will need to optimize it.

55
00:04:04,688 --> 00:04:07,958
Here's two examples that caused us some trouble.

56
00:04:08,134 --> 00:04:12,154
So the auto instrumentation for the AWS SDK

57
00:04:12,202 --> 00:04:15,680
as you can see on line five and six.

58
00:04:16,290 --> 00:04:19,486
So by default it is

59
00:04:19,508 --> 00:04:23,362
not suppressing any internal instrumentation. And this means

60
00:04:23,416 --> 00:04:26,962
that any call that you do to s three,

61
00:04:27,096 --> 00:04:30,974
for example, you will have at least four more spans.

62
00:04:31,102 --> 00:04:35,186
So you will have the parent spend

63
00:04:35,368 --> 00:04:38,546
for the s three action. In this case, as you can see,

64
00:04:38,568 --> 00:04:42,198
is a put object, and then you will have four

65
00:04:42,364 --> 00:04:45,942
more spends. You will have a put

66
00:04:46,076 --> 00:04:49,338
for the HTTP action, and then you will have another

67
00:04:49,424 --> 00:04:53,014
one for the DNS lookup, another one for the TLS

68
00:04:53,062 --> 00:04:57,082
connection, and another one for the TCP connection. Of course

69
00:04:57,136 --> 00:05:00,958
this might be useful in case there's any DNS issue

70
00:05:01,044 --> 00:05:04,526
or something, so you will see it right away

71
00:05:04,628 --> 00:05:08,446
on the trace, but that can also be enabled later

72
00:05:08,628 --> 00:05:12,878
when needed if there's any weird behavior that detected

73
00:05:12,894 --> 00:05:17,410
on s three operations that you suspect that might be related with

74
00:05:17,560 --> 00:05:20,020
some DNS problem.

75
00:05:21,030 --> 00:05:24,690
But most of the times you probably won't need

76
00:05:24,760 --> 00:05:27,590
all these internal spans on every trace.

77
00:05:28,010 --> 00:05:31,654
A similar situation happened on the auto instrumentation for

78
00:05:31,692 --> 00:05:35,026
COA. So by default

79
00:05:35,218 --> 00:05:38,682
it will create a new span for any type

80
00:05:38,736 --> 00:05:42,582
of middleware you have on your API.

81
00:05:42,726 --> 00:05:46,346
And most of the times you won't need all

82
00:05:46,368 --> 00:05:49,738
of them. So you can probably ignore it

83
00:05:49,904 --> 00:05:53,438
and enable it later. Or if you have any suspicion that

84
00:05:53,524 --> 00:05:56,846
one of the middlewares can be the

85
00:05:56,868 --> 00:06:00,494
root cause of a LATC issue, for example, or you can

86
00:06:00,612 --> 00:06:04,642
go for manual instrumentation and apply the needed

87
00:06:04,696 --> 00:06:07,620
instrumentation inside the middleware logic itself.

88
00:06:09,510 --> 00:06:13,358
So in this case, as you can see, we had lots of spans

89
00:06:13,454 --> 00:06:17,490
on every trace that were automatically

90
00:06:17,570 --> 00:06:21,234
sent by the auto instrumentation for COA.

91
00:06:21,362 --> 00:06:25,814
And how we solved this was to ignore the

92
00:06:25,852 --> 00:06:29,930
layer type and ignore these middleware spends,

93
00:06:30,670 --> 00:06:34,230
but this was not enough. So another essential technique

94
00:06:34,310 --> 00:06:37,786
we used to filter out the needed data was to

95
00:06:37,808 --> 00:06:41,600
do tail based sampling. Tailbased sampling is basically

96
00:06:43,250 --> 00:06:46,942
where a decision to sample a trace happens after

97
00:06:46,996 --> 00:06:49,870
all the spans in a request have been completed,

98
00:06:51,250 --> 00:06:55,266
and to execute tail based sampling. The most popular tool is an

99
00:06:55,288 --> 00:06:58,754
open telemetry collector. For this we have

100
00:06:58,792 --> 00:07:02,434
multiple options. Here's some of them. So the first

101
00:07:02,472 --> 00:07:06,610
one that we tried was the AWS distro for Opentelemetry,

102
00:07:07,270 --> 00:07:10,934
a dot, and it's an

103
00:07:10,972 --> 00:07:14,930
AWS supported version of the upstream Opentelemetry collector

104
00:07:15,090 --> 00:07:18,166
and is distributed by Amazon. It supports the selected

105
00:07:18,198 --> 00:07:22,134
components from the open telemetry community and it's fully compatible

106
00:07:22,182 --> 00:07:26,150
with AWS computing platforms such as ACS

107
00:07:26,230 --> 00:07:27,690
or EKS.

108
00:07:29,070 --> 00:07:32,714
It has some niceties like being able

109
00:07:32,752 --> 00:07:36,110
to load a configuration from an s three file,

110
00:07:37,250 --> 00:07:40,622
so you don't need to bake the configuration for the

111
00:07:40,676 --> 00:07:43,954
opentelemetry collector in the docker image and

112
00:07:43,992 --> 00:07:46,500
you can just retrieve it from an s three.

113
00:07:47,750 --> 00:07:51,598
But we had some issues that we couldn't

114
00:07:51,694 --> 00:07:55,006
go for this solution, and the first issue that we

115
00:07:55,048 --> 00:07:58,870
found is that it didn't support all the processors that we need,

116
00:07:59,020 --> 00:08:02,706
especially the transform processor. And I actually opened

117
00:08:02,738 --> 00:08:05,878
an issue for this, so currently

118
00:08:06,044 --> 00:08:09,130
it's not included on this distribution.

119
00:08:11,470 --> 00:08:14,842
And the other problem was

120
00:08:14,896 --> 00:08:18,506
that it didn't support logs at that time. Now it

121
00:08:18,528 --> 00:08:21,806
does. It was announced a few weeks later, but at

122
00:08:21,828 --> 00:08:25,018
the time it didn't support logs

123
00:08:25,034 --> 00:08:28,334
and we needed it because we also wanted to enrich these

124
00:08:28,372 --> 00:08:32,202
logs with some extra attributes and we couldn't

125
00:08:32,266 --> 00:08:35,266
use the opentelemetry collector for this.

126
00:08:35,448 --> 00:08:39,570
So yeah, be aware of these things and

127
00:08:39,640 --> 00:08:43,746
the documentation on the repository is pretty good, and you

128
00:08:43,768 --> 00:08:47,442
will have a list there of all the processors available on the distro,

129
00:08:47,506 --> 00:08:51,126
so you can see beforehand if

130
00:08:51,148 --> 00:08:54,854
it's the right tool for you or not. So taking this into

131
00:08:54,892 --> 00:08:58,178
account, these limitations

132
00:08:58,274 --> 00:09:01,958
so we had to go for the official upstream open telemetry

133
00:09:02,054 --> 00:09:06,198
distribution, and here you have two options. You have the open telemetry core

134
00:09:06,294 --> 00:09:10,186
and you have the open telemetry contrib, which was the one that

135
00:09:10,208 --> 00:09:14,590
we used. The core is a limited docker image and it includes

136
00:09:14,930 --> 00:09:18,794
components that are maintained by the core open telemetry

137
00:09:18,842 --> 00:09:23,030
collector team, and it includes the most commonly used components

138
00:09:23,210 --> 00:09:27,218
like filter and attribute processors and some common

139
00:09:27,304 --> 00:09:30,462
exporters for Yager and Zipkin.

140
00:09:30,606 --> 00:09:34,334
And the contrib version includes almost every processor

141
00:09:34,462 --> 00:09:39,302
component available, with some exceptions where

142
00:09:39,356 --> 00:09:42,806
the components are still in development. So if you want

143
00:09:42,828 --> 00:09:46,886
to create a slimmer image, because if

144
00:09:46,908 --> 00:09:50,634
you find that the open telemetry contribute has too

145
00:09:50,672 --> 00:09:54,294
many things that you don't need, there's this recent blog

146
00:09:54,342 --> 00:09:57,766
post from Martin on how to build a secure open telemetry

147
00:09:57,798 --> 00:10:01,870
collector so you can create a slimmer image with just

148
00:10:02,020 --> 00:10:05,360
the processors that you need, with just the components that you need.

149
00:10:05,890 --> 00:10:08,878
And it's also a good idea in terms of security,

150
00:10:08,964 --> 00:10:14,818
of course, because you

151
00:10:14,824 --> 00:10:18,366
won't include any processors that you don't

152
00:10:18,398 --> 00:10:23,506
use, so you reduce the

153
00:10:23,528 --> 00:10:27,138
surface area of attack and it's

154
00:10:27,154 --> 00:10:31,234
not that hard to build. As the blog explains,

155
00:10:31,282 --> 00:10:34,934
it's not that hard to build an open telemetry collector from

156
00:10:34,972 --> 00:10:38,214
scratch. And finally, you have also some

157
00:10:38,252 --> 00:10:41,574
vendors like Anacom that provide their own solution for tail

158
00:10:41,622 --> 00:10:45,402
based sampling. So you have refinery in their

159
00:10:45,456 --> 00:10:49,046
case, which is like a complete different project from the opentelemetry

160
00:10:49,078 --> 00:10:53,070
collector. And it's also a sampling proxy

161
00:10:53,490 --> 00:10:56,890
that examines the whole traces and then intelligently

162
00:10:56,970 --> 00:11:00,602
applies sampling decisions to each trace.

163
00:11:00,746 --> 00:11:05,170
These decisions determine whether to keep or drop the trace data

164
00:11:05,320 --> 00:11:08,450
in the sampled data forwarded to

165
00:11:08,520 --> 00:11:09,490
Anikom.

166
00:11:11,030 --> 00:11:14,900
So our current architecture for

167
00:11:15,210 --> 00:11:18,306
the opentelemetry collector usage,

168
00:11:18,498 --> 00:11:21,842
it looks like this. So we run the opentelemetry collector

169
00:11:21,906 --> 00:11:25,542
as a sidecar and our app

170
00:11:25,596 --> 00:11:28,860
container forwards the spans to it.

171
00:11:29,390 --> 00:11:33,286
Then the collector also calls the metrics

172
00:11:33,318 --> 00:11:38,506
endpoint on the app here to

173
00:11:38,528 --> 00:11:42,670
fetch some metrics from the Prometheus client running

174
00:11:42,740 --> 00:11:46,794
on the application. And regarding logs, they are being tailed

175
00:11:46,842 --> 00:11:51,086
by fluent bit sidecar, another sidecar that we have

176
00:11:51,268 --> 00:11:55,262
that then forwards the logs to the opentelemetry collector

177
00:11:55,326 --> 00:11:59,470
container. And then the opentelemetry collector filters

178
00:11:59,550 --> 00:12:03,726
the spans and enriches also the metrics,

179
00:12:03,758 --> 00:12:08,194
spans and logs with new attributes like the identification

180
00:12:08,322 --> 00:12:12,102
of the task that is running and other

181
00:12:12,156 --> 00:12:15,960
attributes that are useful for us. And then

182
00:12:16,330 --> 00:12:20,806
it's responsible to send the

183
00:12:20,828 --> 00:12:24,506
metrics, tracing and logs to one of

184
00:12:24,528 --> 00:12:28,166
these backends. It can send like to honeycomb, to Grafana,

185
00:12:28,198 --> 00:12:32,362
to datadog any vendor that supports the OTLP

186
00:12:32,506 --> 00:12:33,390
protocol,

187
00:12:35,970 --> 00:12:39,360
and then you can visualize your data there.

188
00:12:41,090 --> 00:12:44,130
Regarding the collector configuration,

189
00:12:45,590 --> 00:12:49,522
you will have to do that configuration on a YAml file and

190
00:12:49,576 --> 00:12:53,570
here we can see a visual representation of that configuration.

191
00:12:54,890 --> 00:12:58,760
For logs, metrics and traces we used

192
00:12:59,370 --> 00:13:02,882
the image was generated on the hotelbean IO,

193
00:13:02,946 --> 00:13:06,870
which is also a really great tool to visualize your configuration

194
00:13:09,870 --> 00:13:14,234
for the open telemetry collector. And on the left

195
00:13:14,352 --> 00:13:18,298
you can see for logs, metrics and traces you can see the different

196
00:13:18,384 --> 00:13:22,414
receivers, so OTLP or

197
00:13:22,612 --> 00:13:23,790
Prometheus.

198
00:13:25,570 --> 00:13:29,470
And then after the data is received,

199
00:13:30,130 --> 00:13:33,826
you will have the different processors that will

200
00:13:33,848 --> 00:13:36,994
process the data filter, enrich with more

201
00:13:37,032 --> 00:13:40,260
attributes, and then in the end

202
00:13:41,030 --> 00:13:44,274
you will see the destination of those of the

203
00:13:44,312 --> 00:13:48,210
open telemetry signals, which in this case it's also OTLP,

204
00:13:48,370 --> 00:13:52,818
so it is sent to a backend to then visualize

205
00:13:52,994 --> 00:13:54,950
the open telemetry signals.

206
00:13:56,650 --> 00:14:00,182
In this case, the type of signal that was generating

207
00:14:00,246 --> 00:14:03,530
more data for us was the traces

208
00:14:04,110 --> 00:14:06,460
and that's when we needed to act.

209
00:14:08,430 --> 00:14:11,674
So the first processor, so let's

210
00:14:11,722 --> 00:14:15,022
focus on the pipeline for the traces. The first

211
00:14:15,076 --> 00:14:18,474
processor on the pipeline configured on the collector is the batch

212
00:14:18,522 --> 00:14:22,422
processor, and it's really simple. It accepts

213
00:14:22,586 --> 00:14:25,982
the spans and places them into batches. And batching

214
00:14:26,046 --> 00:14:29,374
basically helps to better compress

215
00:14:29,422 --> 00:14:33,086
the data and reduce the number of outgoing connections required

216
00:14:33,118 --> 00:14:36,920
to transmit the data, so it's a recommended processor to use.

217
00:14:38,650 --> 00:14:42,514
After that, the data is then handled by the next processor,

218
00:14:42,562 --> 00:14:46,114
which is a tail sampler that we call default.

219
00:14:46,162 --> 00:14:50,762
As I will explain later here,

220
00:14:50,816 --> 00:14:54,026
the trace will be analyzed and it

221
00:14:54,048 --> 00:14:57,462
is not dropped. If it is not dropped,

222
00:14:57,526 --> 00:15:00,210
it goes for the next processor.

223
00:15:00,390 --> 00:15:03,822
The next processor is another tail sampler in our

224
00:15:03,876 --> 00:15:07,646
case, and here if the

225
00:15:07,668 --> 00:15:11,200
trace is dropped, so all the data

226
00:15:11,730 --> 00:15:15,026
on the trace can be dropped depending on the

227
00:15:15,048 --> 00:15:18,782
configuration. So let's see how these two are configured

228
00:15:18,846 --> 00:15:22,174
in our specific case. The first tail

229
00:15:22,222 --> 00:15:25,702
sampler named default has three different

230
00:15:25,756 --> 00:15:29,206
policies. So the first policy is

231
00:15:29,308 --> 00:15:33,094
the errors policy that will send or

232
00:15:33,132 --> 00:15:36,774
that will sample any trace that constraints a span with

233
00:15:36,812 --> 00:15:40,330
an error status. So we assume that

234
00:15:40,400 --> 00:15:43,734
if it has an error, it will be an important signal

235
00:15:43,782 --> 00:15:47,366
that we can then analyze and get to the root

236
00:15:47,398 --> 00:15:50,894
cause of it. The next policy is

237
00:15:50,932 --> 00:15:55,086
the latency policy where we check if

238
00:15:55,108 --> 00:15:59,226
the trace took more than 100 milliseconds

239
00:15:59,258 --> 00:16:02,778
to be processed, or in this case like if the

240
00:16:02,804 --> 00:16:06,734
request took more than 100 milliseconds to be processed.

241
00:16:06,862 --> 00:16:10,098
We also sample the complete trace and the

242
00:16:10,104 --> 00:16:13,682
main idea is the same as before. So we

243
00:16:13,736 --> 00:16:17,638
sample slow operations to then analyze it

244
00:16:17,724 --> 00:16:21,734
and get to the root cause of it. These two

245
00:16:21,772 --> 00:16:25,494
policies will already filter out most of the

246
00:16:25,692 --> 00:16:29,686
simple operations that you might have, like status or health checks

247
00:16:29,878 --> 00:16:33,274
calls that you might have on your API. But you could

248
00:16:33,312 --> 00:16:36,554
also filter those kind of calls explicitly by

249
00:16:36,592 --> 00:16:40,070
using the HTTP path or attribute, for example.

250
00:16:40,240 --> 00:16:43,760
That would be another way to do it. And finally,

251
00:16:44,690 --> 00:16:49,130
the last policy is to sample any trace

252
00:16:49,290 --> 00:16:52,670
that contains a span

253
00:16:53,430 --> 00:16:57,282
with a specific attribute. In this case we

254
00:16:57,336 --> 00:17:01,298
sample every trace that contains a

255
00:17:01,384 --> 00:17:04,782
span with the graphQL operation

256
00:17:04,846 --> 00:17:08,386
name Resync project. So in

257
00:17:08,408 --> 00:17:12,182
this case this might be an important operation that we will always

258
00:17:12,236 --> 00:17:15,686
want to sample. For example, it can be like a

259
00:17:15,708 --> 00:17:18,342
new feature that you want to check its usage,

260
00:17:18,486 --> 00:17:22,374
and we will always want all the traces related

261
00:17:22,422 --> 00:17:25,946
with that operation. An important thing

262
00:17:25,968 --> 00:17:29,858
to notice here that you maybe already noticed

263
00:17:29,974 --> 00:17:33,150
is that these policies have an or relationship

264
00:17:33,300 --> 00:17:37,614
between them, so the trace will be sampled. If any of these

265
00:17:37,812 --> 00:17:41,440
conditions is true, you can have

266
00:17:42,050 --> 00:17:45,602
multiple tail samplers. In this case we have

267
00:17:45,656 --> 00:17:49,122
two of them. So the next one we called

268
00:17:49,176 --> 00:17:52,542
it synthetics. And this exists

269
00:17:52,606 --> 00:17:56,562
basically because we have synthetic monitors to check our API

270
00:17:56,626 --> 00:17:59,250
every minute from different regions,

271
00:17:59,410 --> 00:18:03,190
and on each of these calls it will generate

272
00:18:03,770 --> 00:18:07,494
multiple spans that are not interesting at all if they

273
00:18:07,532 --> 00:18:11,386
run successfully. So therefore for this processor we

274
00:18:11,408 --> 00:18:15,094
configured the same way. So we configured

275
00:18:15,142 --> 00:18:18,586
an error policy and the latency in

276
00:18:18,608 --> 00:18:21,262
this case, the latency is bigger than 1 second.

277
00:18:21,396 --> 00:18:25,166
So if one of these synthetic monitors throws an

278
00:18:25,188 --> 00:18:28,494
error, or it took

279
00:18:28,532 --> 00:18:31,934
more than 1 second to complete,

280
00:18:32,052 --> 00:18:36,862
then we sample the data, because that's

281
00:18:37,006 --> 00:18:39,140
an interesting event, right?

282
00:18:40,710 --> 00:18:43,938
Then we have two extra policies in this case.

283
00:18:44,104 --> 00:18:47,478
So the first one is to sample only

284
00:18:47,564 --> 00:18:51,270
1% of the synthetic requests that are successful.

285
00:18:51,770 --> 00:18:55,846
And this might be useful, like to have an idea of

286
00:18:55,948 --> 00:19:00,666
what's the average latency, for example on

287
00:19:00,688 --> 00:19:03,370
the synthetic requests.

288
00:19:04,670 --> 00:19:06,460
And as you can see here,

289
00:19:08,590 --> 00:19:12,110
we can create a policy with two sub policies,

290
00:19:13,170 --> 00:19:17,166
and it will evaluate them using an and instead of

291
00:19:17,188 --> 00:19:18,560
the default r.

292
00:19:20,290 --> 00:19:24,130
In the end, the last policy will serve as a failover

293
00:19:24,710 --> 00:19:28,002
to basically sample all the other traces that do not

294
00:19:28,056 --> 00:19:32,306
have the cloud watch user agent. So in

295
00:19:32,328 --> 00:19:35,622
this case it's the opposite of

296
00:19:35,756 --> 00:19:39,334
the previous one. So we check for all

297
00:19:39,372 --> 00:19:43,042
the traces that so we have invert match equals

298
00:19:43,106 --> 00:19:46,370
true. So we sample

299
00:19:46,450 --> 00:19:49,870
100% of the non synthetic requests. So it's

300
00:19:49,890 --> 00:19:53,226
basically a failover. So we sample all the other

301
00:19:53,248 --> 00:19:56,970
traces that do not have the Cloudwatch user agent on the attributes.

302
00:19:57,630 --> 00:20:01,134
And this is important because without this failover this

303
00:20:01,172 --> 00:20:05,130
processor would discard all the other traces because they wouldn't

304
00:20:05,210 --> 00:20:09,150
be evaluated as true by any of the other policies.

305
00:20:12,050 --> 00:20:16,338
The last processor we have that also filters some data

306
00:20:16,504 --> 00:20:20,206
is this filter that excludes any span

307
00:20:20,318 --> 00:20:23,170
taking into account an attribute.

308
00:20:23,910 --> 00:20:27,560
And in this case, the main difference here

309
00:20:28,330 --> 00:20:31,714
with the tail sampling is that the tail sampler

310
00:20:31,762 --> 00:20:35,618
filters complete traces, while this one is filtering

311
00:20:35,714 --> 00:20:39,194
just specific spans. And because of that we

312
00:20:39,232 --> 00:20:41,820
need to be really careful when doing this,

313
00:20:42,750 --> 00:20:46,710
because we need to be careful with the data being dropped

314
00:20:46,870 --> 00:20:50,418
because dropping a span may lead to orphaned

315
00:20:50,534 --> 00:20:54,170
spans if the dropped span is apparent. So ideally

316
00:20:54,250 --> 00:20:57,614
you would create rules here to guarantee that

317
00:20:57,652 --> 00:21:01,198
you drop only childless spans, which in

318
00:21:01,204 --> 00:21:06,306
this case is true because as you can see in

319
00:21:06,328 --> 00:21:11,250
this case, like we are dropping only trivial

320
00:21:11,750 --> 00:21:15,654
graphql attributes, so we are looking

321
00:21:15,692 --> 00:21:19,766
at the field type and we are dropping only

322
00:21:19,868 --> 00:21:23,814
the ones that we know for sure that

323
00:21:23,852 --> 00:21:27,750
were trivial and they were childless spend.

324
00:21:27,820 --> 00:21:32,346
So they didn't have any child, so it

325
00:21:32,368 --> 00:21:34,700
was safe to drop them.

326
00:21:35,150 --> 00:21:39,066
And most of the times they were not interesting because they would complete in just

327
00:21:39,088 --> 00:21:43,054
a couple of milliseconds. So it was just information that

328
00:21:43,092 --> 00:21:46,654
we didn't need to keep. So in

329
00:21:46,692 --> 00:21:50,490
resume, the logic behind the configurations I previously

330
00:21:50,570 --> 00:21:54,234
mentioned are represented pretty well on this image

331
00:21:54,282 --> 00:21:58,574
by Riz Lee and posted on the official opentelemetry

332
00:21:58,622 --> 00:22:02,594
blog. So on the configurations for your tail sampler running

333
00:22:02,632 --> 00:22:06,534
on the collector, you will want first the traces with

334
00:22:06,572 --> 00:22:09,910
high latency and errors,

335
00:22:10,250 --> 00:22:14,258
and then sample. Also traces with specific attributes,

336
00:22:14,354 --> 00:22:17,974
as we did, for example, for the resync operation

337
00:22:18,022 --> 00:22:23,274
that we had there. The others 99%

338
00:22:23,312 --> 00:22:26,106
of the time won't be interesting for you,

339
00:22:26,208 --> 00:22:29,954
and you can either discard them or execute some random

340
00:22:30,022 --> 00:22:33,514
sampling on them if you prefer, and have the budget

341
00:22:33,562 --> 00:22:36,734
for it. And this way you can

342
00:22:36,772 --> 00:22:40,058
have an efficient

343
00:22:40,154 --> 00:22:43,230
and economic observability framework.

344
00:22:43,970 --> 00:22:48,074
And that's it. Thank you. And hopefully

345
00:22:48,202 --> 00:22:51,342
you got some useful information on how to use the open

346
00:22:51,396 --> 00:22:55,026
telemetry collector to keep your wallet safe.

347
00:22:55,178 --> 00:22:58,214
And if you have any feedback and

348
00:22:58,252 --> 00:23:01,734
questions about what I said here, you can find me

349
00:23:01,772 --> 00:23:04,630
on LinkedIn or on Twitter.

350
00:23:05,450 --> 00:23:09,090
So, yeah, that's it. Thank you. Thanks for watching, and bye.

