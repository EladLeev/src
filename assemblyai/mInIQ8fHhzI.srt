1
00:00:34,610 --> 00:00:38,390
Hello everyone, welcome to scaling Kubernetes clusters without

2
00:00:38,460 --> 00:00:42,118
losing your mind or your money. In the next 30 minutes we

3
00:00:42,124 --> 00:00:45,714
are going to talk about the challenge of making efficient

4
00:00:45,762 --> 00:00:50,026
use of the ever changing options of compute type software

5
00:00:50,098 --> 00:00:53,534
by cloud providers. You can find today on

6
00:00:53,572 --> 00:00:55,930
AWS different options of powerful,

7
00:00:56,010 --> 00:00:59,002
sustainable, cost effective infrastructure.

8
00:00:59,146 --> 00:01:02,974
And there is a question. How do you tailor that to meet your Kubernetes

9
00:01:03,022 --> 00:01:06,542
workload needs while implementing automations to scale

10
00:01:06,606 --> 00:01:10,510
based on your business demand? The answer is carpenter.

11
00:01:10,670 --> 00:01:14,642
In this talk, we are going to review how carpenter simplifies and

12
00:01:14,696 --> 00:01:18,710
accelerates provisioning of cost effective infrastructure.

13
00:01:19,050 --> 00:01:22,722
My name is Yale and I'm a options architect at AWS

14
00:01:22,866 --> 00:01:25,942
focusing on compute services. In the past

15
00:01:25,996 --> 00:01:29,146
few years I've been working with clusters to help them make

16
00:01:29,248 --> 00:01:32,554
optimized and efficient selection of

17
00:01:32,592 --> 00:01:36,582
compute infrastructure for different kinds of workloads with offerings

18
00:01:36,646 --> 00:01:40,602
like EC two and Graviton, as well as specialized

19
00:01:40,666 --> 00:01:44,222
hardware based on their requirements while keeping their

20
00:01:44,276 --> 00:01:48,094
operational efforts to the minimum. In this talk

21
00:01:48,132 --> 00:01:51,834
we will dive into Karpenter, an autoscaling

22
00:01:51,882 --> 00:01:55,070
solution that helps scale efficiently the Kubernetes

23
00:01:55,150 --> 00:01:58,786
infrastructure. We will touch on the technical aspects of

24
00:01:58,808 --> 00:02:02,242
the implementation and the integration with other cost

25
00:02:02,296 --> 00:02:06,470
optimization techniques like easy to spot and graviton.

26
00:02:08,010 --> 00:02:11,554
So let's start with what we want to achieve

27
00:02:11,602 --> 00:02:13,766
with auto scaling. In other words,

28
00:02:13,868 --> 00:02:17,554
efficiency. We start from talking about scale,

29
00:02:17,602 --> 00:02:21,226
which is the obvious part. Pay only for what you use,

30
00:02:21,408 --> 00:02:25,162
provision just the right amount of resources that you actually need

31
00:02:25,216 --> 00:02:27,450
based on your business requirements.

32
00:02:28,110 --> 00:02:30,778
The next part is density,

33
00:02:30,874 --> 00:02:34,254
and by saying density, I mean being able to

34
00:02:34,292 --> 00:02:37,614
select the right compute option and bin pack the

35
00:02:37,652 --> 00:02:41,054
containers intelligently into shared resources to

36
00:02:41,092 --> 00:02:44,546
maximize efficiency. And this is the main part of

37
00:02:44,568 --> 00:02:47,566
the advantages that you can get by using kubernetes,

38
00:02:47,678 --> 00:02:50,740
but it's still not an easy task to do.

39
00:02:51,350 --> 00:02:53,650
The next part is flexibility.

40
00:02:54,550 --> 00:02:58,134
Flexibility is a requirement that is related to

41
00:02:58,172 --> 00:03:03,014
being able to take advantage of cost effective compute options you

42
00:03:03,052 --> 00:03:06,818
can find today. Different types of instances on AWS

43
00:03:06,994 --> 00:03:10,442
and usually more than one instance type

44
00:03:10,496 --> 00:03:14,742
can power your application needs, and there might be more costeffective

45
00:03:14,806 --> 00:03:18,394
solutions than others. The most effective way to

46
00:03:18,432 --> 00:03:21,582
get high amount of resources very economically is easy

47
00:03:21,636 --> 00:03:25,230
to spot the spare compute capacity of AWS.

48
00:03:25,570 --> 00:03:29,162
Another way to get higher performance

49
00:03:29,226 --> 00:03:32,822
with lower cost is the latest graviton processors

50
00:03:32,906 --> 00:03:36,050
that are based on the arm architecture.

51
00:03:36,390 --> 00:03:40,274
Being flexible between different instance types and options will

52
00:03:40,312 --> 00:03:43,374
allow taking advantage of spot and graviton

53
00:03:43,502 --> 00:03:47,880
in a way that will let you get more and pay less.

54
00:03:48,650 --> 00:03:52,166
Now you can notice that these three requirements are obviously

55
00:03:52,268 --> 00:03:55,986
related, being able to scale automatically

56
00:03:56,098 --> 00:04:00,150
when scaling choose the right instances that maximize

57
00:04:00,230 --> 00:04:03,446
efficiency, and that's what we want to accomplish.

58
00:04:03,638 --> 00:04:07,878
And in addition to all of these, we also want to minimize

59
00:04:07,974 --> 00:04:11,386
the operational overhead that you invest in

60
00:04:11,408 --> 00:04:15,466
order to be able to get to this goal. And if you're in this session,

61
00:04:15,498 --> 00:04:19,146
you're probably a DevOps or a platform engineer. You're in charge

62
00:04:19,178 --> 00:04:23,074
of operating production, test dev environments, and you

63
00:04:23,112 --> 00:04:26,738
have ton of tasks to do.

64
00:04:26,904 --> 00:04:30,830
When choosing a solution for auto scaling, one of the main requirements

65
00:04:30,910 --> 00:04:34,574
would be that it will be easy to implement

66
00:04:34,702 --> 00:04:37,720
and will require minimal effort from your side.

67
00:04:39,850 --> 00:04:43,522
So I talked about how efficiency translates into scale

68
00:04:43,586 --> 00:04:47,762
density and flexibility, which are, in other words,

69
00:04:47,916 --> 00:04:52,070
scaling the right opensource, using the most cost optimal resources

70
00:04:52,150 --> 00:04:56,166
as possible based on your container requirements.

71
00:04:56,358 --> 00:04:59,210
So let's talk about container requirements.

72
00:04:59,890 --> 00:05:03,434
They start from defining cpu and memory

73
00:05:03,562 --> 00:05:06,974
that should be defined as resource requests in

74
00:05:07,012 --> 00:05:10,030
your pod or deployment manifest.

75
00:05:11,810 --> 00:05:15,970
They also might need storage network, sometimes gpus.

76
00:05:16,870 --> 00:05:20,402
On the other hand, we'll have EC two instances that

77
00:05:20,456 --> 00:05:24,018
provide a set of opensource that will support the needs

78
00:05:24,184 --> 00:05:25,890
of your applications.

79
00:05:27,430 --> 00:05:31,126
The EC two naming convention represents the amount of

80
00:05:31,148 --> 00:05:34,722
resources that you get from each instance. The instance

81
00:05:34,786 --> 00:05:38,246
size, what we see here is extra large, represents the

82
00:05:38,268 --> 00:05:41,894
amount of cpu that the instance is providing, and the instance

83
00:05:41,942 --> 00:05:46,246
family represents the cpu to memory ratio and therefore defining

84
00:05:46,278 --> 00:05:50,382
how much memory we get. You can also find attributes that

85
00:05:50,436 --> 00:05:53,866
talk about how much additional resources

86
00:05:53,898 --> 00:05:57,360
you're getting, for example, disks or

87
00:05:57,890 --> 00:06:00,270
increased networking throughput.

88
00:06:00,850 --> 00:06:04,282
In this case, the G represents the graviton processor.

89
00:06:04,346 --> 00:06:08,242
You can also identify what processor you're working within each

90
00:06:08,296 --> 00:06:11,742
instance. Type now, in a Kubernetes

91
00:06:11,806 --> 00:06:15,134
cluster, the Kubernetes scheduler is in charge of

92
00:06:15,192 --> 00:06:19,890
matching pods to nodes and initiating

93
00:06:19,970 --> 00:06:23,458
preemption when required. But it's

94
00:06:23,474 --> 00:06:27,058
not in charge of node creation, pod creation,

95
00:06:27,234 --> 00:06:30,982
rescheduling, scheduling, or rebalancing pods

96
00:06:31,046 --> 00:06:35,110
between nodes. And here comes a need for an external solution

97
00:06:35,190 --> 00:06:38,922
that will perform the task of node management that

98
00:06:38,976 --> 00:06:43,118
complies to the same pattern that the Kubernetes scheduler has,

99
00:06:43,284 --> 00:06:46,922
as well as be aware of the cloud elasticity,

100
00:06:46,986 --> 00:06:50,254
the pricing model, and the infrastructure options in

101
00:06:50,292 --> 00:06:53,620
order to maximize the value getting from it.

102
00:06:55,510 --> 00:06:58,814
The question is, in fact, how do we scale

103
00:06:58,862 --> 00:07:02,322
EC two instances to support our application

104
00:07:02,456 --> 00:07:06,134
needs? Now, when working with

105
00:07:06,252 --> 00:07:10,022
Kubernetes, a common practice to scale nodes is using

106
00:07:10,076 --> 00:07:14,482
the clusters autoscaler. Cluster autoscaler

107
00:07:14,546 --> 00:07:18,178
is a very popular open source solution that is in

108
00:07:18,204 --> 00:07:22,074
charge of ensuring that your cluster has enough nodes to schedule your

109
00:07:22,112 --> 00:07:24,970
pods without wasting opensource.

110
00:07:25,870 --> 00:07:29,558
It runs AWS a deployment inside your eks cluster and it's aware

111
00:07:29,574 --> 00:07:33,166
of pod scheduling decisions. So essentially one

112
00:07:33,188 --> 00:07:37,018
of the goals is to bridge between the Kubernetes abstraction

113
00:07:37,114 --> 00:07:40,506
into the cloud abstraction, the auto scaling

114
00:07:40,538 --> 00:07:44,814
groups that are the entity that supports

115
00:07:44,942 --> 00:07:47,940
provisioning nodes for the application needs.

116
00:07:48,550 --> 00:07:52,350
Now let's take a look on a process of the scale up activity

117
00:07:52,430 --> 00:07:56,206
presented in this slide. It starts from a pod

118
00:07:56,238 --> 00:07:59,830
that is in a pending state due to insufficient resources.

119
00:08:00,170 --> 00:08:03,926
This is a good thing because we always want to run just

120
00:08:03,948 --> 00:08:07,878
the amount of resources that we need, and when

121
00:08:08,044 --> 00:08:12,022
we'll have more applications that need to run more containers

122
00:08:12,086 --> 00:08:15,980
that are being created, they will go into a pending state.

123
00:08:16,510 --> 00:08:20,386
Then the scaling solution, the Kubernetes cluster autoscaler,

124
00:08:20,438 --> 00:08:23,902
identifies that event and it's in charge of

125
00:08:23,956 --> 00:08:27,754
scaling up the underlying resources to support the requirements

126
00:08:27,802 --> 00:08:31,662
of the pod. How it works is that

127
00:08:31,716 --> 00:08:35,098
it's in charge of selecting the right auto

128
00:08:35,114 --> 00:08:38,654
scaling group that can support the needs of the spending pod.

129
00:08:38,782 --> 00:08:42,574
It increases the amount of requested

130
00:08:42,622 --> 00:08:46,166
instances in this auto scaling group and waits to

131
00:08:46,188 --> 00:08:50,386
get them back. When those instances are provisioned,

132
00:08:50,498 --> 00:08:53,190
they will run the bootstrap scripts,

133
00:08:54,010 --> 00:08:57,506
join the eks clusters, and then the

134
00:08:57,548 --> 00:09:01,414
pod can be scheduled. Now let's dive deeper

135
00:09:01,462 --> 00:09:05,178
into phase number two and three. When the

136
00:09:05,184 --> 00:09:09,286
Kubernetes cluster autoscaler reaches out to the auto scaling

137
00:09:09,318 --> 00:09:12,654
groups API, it needs to know how much

138
00:09:12,692 --> 00:09:16,362
resources it will get back. It works by simulating

139
00:09:16,426 --> 00:09:20,174
the amount of resources it expects to get from each one of

140
00:09:20,212 --> 00:09:22,800
the auto scaling groups it works with.

141
00:09:23,270 --> 00:09:27,042
So that enforces a requirement that

142
00:09:27,096 --> 00:09:31,250
each auto scaling group should run instances that have similar

143
00:09:31,400 --> 00:09:34,478
resources between the different instance types.

144
00:09:34,654 --> 00:09:39,250
So you need to run each auto scaling group with homogeneous

145
00:09:39,330 --> 00:09:42,706
instance types. That means that in your auto scaling

146
00:09:42,738 --> 00:09:45,990
group you can combine instances like c five

147
00:09:46,060 --> 00:09:49,346
to Excel, c six to Excel. You can

148
00:09:49,388 --> 00:09:52,794
put there the older generation of four.

149
00:09:52,992 --> 00:09:56,682
If you don't mind how much memory you're getting, you only

150
00:09:56,736 --> 00:10:00,286
care about the amount of cpu you're getting. You can combine C

151
00:10:00,308 --> 00:10:04,318
instances together with M instances or R

152
00:10:04,404 --> 00:10:07,454
instances, but you can't, for example,

153
00:10:07,572 --> 00:10:11,214
combine two Excel instances with four Excel because the

154
00:10:11,252 --> 00:10:14,846
cluster autoscaler would not be able to know in advance

155
00:10:14,958 --> 00:10:18,802
how much cpu opensource it's getting from each

156
00:10:18,856 --> 00:10:22,494
instance. The solution of cluster

157
00:10:22,542 --> 00:10:26,454
autoscaler to this problem is to

158
00:10:26,572 --> 00:10:30,434
replicate and run multiple auto scaling groups

159
00:10:30,482 --> 00:10:33,814
in your environment. So if you know that

160
00:10:33,852 --> 00:10:37,774
you have applications that require two excel instances

161
00:10:37,842 --> 00:10:41,638
and others require twelve Excel instances,

162
00:10:41,814 --> 00:10:45,642
simply run a lot of auto scaling groups. And every

163
00:10:45,696 --> 00:10:49,222
time that the cluster auto scaler will have a pending pod

164
00:10:49,286 --> 00:10:53,002
that needs a big instance, it will provision resources

165
00:10:53,066 --> 00:10:56,462
from the big auto scaling group. When it needs a small

166
00:10:56,516 --> 00:10:59,978
instance it will provision resources from the small auto

167
00:10:59,994 --> 00:11:03,642
scaling group. But this does bring

168
00:11:03,716 --> 00:11:06,898
a lot of challenges. So for one,

169
00:11:07,064 --> 00:11:10,530
managing a lot of auto scaling groups is tough because

170
00:11:10,600 --> 00:11:14,558
you need to update the AMIs and roll the instances

171
00:11:14,734 --> 00:11:18,290
and make sure you are maintaining every configuration

172
00:11:18,370 --> 00:11:21,858
there. There are also other challenges

173
00:11:21,954 --> 00:11:26,166
related to running application in multiav fashion for

174
00:11:26,348 --> 00:11:29,514
high availability applications that do

175
00:11:29,552 --> 00:11:33,274
have flexibility between different instance types and they

176
00:11:33,312 --> 00:11:37,354
just want to choose the most optimal one for them

177
00:11:37,552 --> 00:11:41,218
and being able to use spot capacity spot

178
00:11:41,334 --> 00:11:44,654
is the spare capacity of AWS and one of the

179
00:11:44,692 --> 00:11:48,254
main best practices in order for customers to be able

180
00:11:48,292 --> 00:11:51,422
to take advantage of spot is be able to

181
00:11:51,476 --> 00:11:54,994
diversify their instance selection AWS much as

182
00:11:55,032 --> 00:11:58,642
possible. One of the best practices there when

183
00:11:58,696 --> 00:12:02,702
working with spot capacity is be able to diversify

184
00:12:02,846 --> 00:12:05,678
between different sizes of instances.

185
00:12:05,854 --> 00:12:09,862
So for example be able to use four Excel and

186
00:12:09,916 --> 00:12:13,798
eight Excel. If you can pack your application to

187
00:12:13,964 --> 00:12:17,474
one eight excel instance, you can also pack it to

188
00:12:17,532 --> 00:12:21,580
two for Excel instances and so on.

189
00:12:22,110 --> 00:12:25,974
So this is something that we would like our autoscaling

190
00:12:26,022 --> 00:12:29,718
solution to be aware of and implement

191
00:12:29,814 --> 00:12:33,454
in an easy way. This brings me to talk

192
00:12:33,492 --> 00:12:37,230
about carpenter because Carpenter was designed to overcome these

193
00:12:37,300 --> 00:12:41,034
challenges. So similar to cluster

194
00:12:41,082 --> 00:12:45,198
autoscaler, Karpenter is an open source scaling solution

195
00:12:45,374 --> 00:12:48,766
that automatically provisions new nodes in response

196
00:12:48,798 --> 00:12:52,126
to unschedulable pod events. It provisions EC

197
00:12:52,158 --> 00:12:55,650
two capacity directly based on the application requirements

198
00:12:55,730 --> 00:12:58,470
you put in your pod manifest file,

199
00:12:59,050 --> 00:13:02,482
so you can take advantage of all the AC two instance

200
00:13:02,546 --> 00:13:06,662
options available and reduce much of the overhead that

201
00:13:06,716 --> 00:13:10,566
cluster autoscaler had. Carpenter has lots

202
00:13:10,598 --> 00:13:14,934
of cool features, but I'm going to dive specifically

203
00:13:14,982 --> 00:13:19,450
into the features that are related to managing the underlying compute.

204
00:13:21,490 --> 00:13:25,434
So carpenter is implemented as a groupless

205
00:13:25,482 --> 00:13:28,714
auto scaling, meaning it directly scales resources

206
00:13:28,762 --> 00:13:32,590
based on the requirements without the middleware of node groups.

207
00:13:33,510 --> 00:13:36,690
This provides simplification of the configuration

208
00:13:37,110 --> 00:13:41,058
and it allows you to improve the efficiency because

209
00:13:41,144 --> 00:13:44,850
different kinds of applications can run in shared infrastructure.

210
00:13:45,450 --> 00:13:49,186
It also improves performance because scaling decisions

211
00:13:49,218 --> 00:13:52,006
are made in seconds when demand changes.

212
00:13:52,188 --> 00:13:54,870
Even in the largest Kubernetes clusters,

213
00:13:56,970 --> 00:14:00,330
carpenter will perform an EC two

214
00:14:00,400 --> 00:14:04,330
instant fleet request based on the resource requirements.

215
00:14:04,830 --> 00:14:08,346
So if we recap for a second how

216
00:14:08,368 --> 00:14:12,190
we saw that the clusters autoscaler works, we first

217
00:14:12,260 --> 00:14:15,674
have some entity that creates

218
00:14:15,802 --> 00:14:19,258
more pods. They enter into pending state due

219
00:14:19,274 --> 00:14:22,990
to insufficient capacity. Cluster autoscaler will

220
00:14:23,060 --> 00:14:27,114
identify this event and will perform an API

221
00:14:27,162 --> 00:14:30,802
call to the autoscaling group that was already created by

222
00:14:30,856 --> 00:14:35,314
the administrator and the administrator already had to

223
00:14:35,512 --> 00:14:39,714
define what instance stack requirements you should include

224
00:14:39,762 --> 00:14:42,914
inside your auto scaling group and manage multiple

225
00:14:42,962 --> 00:14:46,790
groups in order to support multiple pod requirements.

226
00:14:47,390 --> 00:14:50,780
With carpenter this changes.

227
00:14:51,550 --> 00:14:55,238
You have carpenter right here consolidating

228
00:14:55,414 --> 00:14:59,290
the two phases that we had with cluster oil scalar and

229
00:14:59,360 --> 00:15:02,714
carpenter simply identifies depending pods and

230
00:15:02,752 --> 00:15:06,682
creates an API call to EC two fleet. This API

231
00:15:06,746 --> 00:15:10,490
call is custom made based on the requirements

232
00:15:10,570 --> 00:15:14,206
we have right now from our pending pods. So there

233
00:15:14,228 --> 00:15:17,886
is no need to prepare in advanced list of instance types

234
00:15:17,918 --> 00:15:21,682
that support this pod requirements and

235
00:15:21,736 --> 00:15:26,094
it simplifies the process a lot. So carpenter

236
00:15:26,142 --> 00:15:29,554
is implemented in Kubernetes AWS, a custom resource

237
00:15:29,602 --> 00:15:32,822
definition which is really cool if you think about

238
00:15:32,876 --> 00:15:36,390
it because it's Kubernetes native and you don't need to manage

239
00:15:36,460 --> 00:15:41,050
any resources that are external to your Kubernetes microcosmos.

240
00:15:41,630 --> 00:15:45,638
So the provisioner CRD holds all the configurations

241
00:15:45,734 --> 00:15:49,130
related to the compute that you want to work within the cluster.

242
00:15:49,550 --> 00:15:52,814
By default you can just leave it

243
00:15:52,932 --> 00:15:56,414
as is and allow the provisioner to take

244
00:15:56,452 --> 00:15:59,754
advantage of all the instance types available by EC

245
00:15:59,802 --> 00:16:03,986
two, which are more than 600 today. But if you want

246
00:16:04,008 --> 00:16:07,486
to customize that and you want to include

247
00:16:07,518 --> 00:16:11,262
or exclude something from your instance specification,

248
00:16:11,326 --> 00:16:12,820
you can also do that.

249
00:16:14,550 --> 00:16:18,722
The provision also allows defining other configurations

250
00:16:18,866 --> 00:16:22,706
like limiting the amount of resources provisioned

251
00:16:22,738 --> 00:16:25,926
by a workload in case you want to control a

252
00:16:25,948 --> 00:16:29,834
budget for a team for example, or define when all

253
00:16:29,872 --> 00:16:33,660
nodes will be replaced by putting a time to leave

254
00:16:34,430 --> 00:16:37,530
setting inside the provisioner.

255
00:16:39,070 --> 00:16:43,102
Now let's see how it actually works for different common use

256
00:16:43,156 --> 00:16:46,814
cases. So inside your

257
00:16:46,852 --> 00:16:52,270
Kubernetes microcosmos you might have containers coming

258
00:16:52,340 --> 00:16:56,018
with different requirements. These requirements will usually be

259
00:16:56,104 --> 00:17:00,478
managed by resource requests, node selectors,

260
00:17:00,574 --> 00:17:03,954
affinity topology spread. Carpenter will

261
00:17:03,992 --> 00:17:07,846
eventually select the instances to provision for the

262
00:17:07,868 --> 00:17:11,490
pods based on the combination of all these requirements,

263
00:17:11,570 --> 00:17:15,206
so it reads directly all these constraints that

264
00:17:15,228 --> 00:17:18,310
you can put inside your pod.

265
00:17:18,470 --> 00:17:21,882
Yamls you have different

266
00:17:21,936 --> 00:17:25,418
types of topologies that you can build with

267
00:17:25,504 --> 00:17:29,050
karpenter. So let's start from a single

268
00:17:29,120 --> 00:17:32,746
provisioner. A single provisioner can run multiple types

269
00:17:32,778 --> 00:17:35,946
of workloads where each workload or container

270
00:17:35,978 --> 00:17:39,454
can ask for what it needs, but it has the option to

271
00:17:39,492 --> 00:17:43,386
share resources with other applications as much as possible to

272
00:17:43,428 --> 00:17:46,722
maximize efficiency. On the other hand,

273
00:17:46,776 --> 00:17:49,934
if I want to separate workloads and I want to enforce

274
00:17:49,982 --> 00:17:54,254
them to run on separated nodes, I can do that with multiple

275
00:17:54,302 --> 00:17:58,098
provisioners and each provisioner can define

276
00:17:58,274 --> 00:18:02,322
different compute requirements. For example, I can have my default

277
00:18:02,386 --> 00:18:05,894
provisioning to use spot and on demand and

278
00:18:05,932 --> 00:18:08,970
use all the instance types available. And I

279
00:18:09,040 --> 00:18:13,126
can have another provisioner supporting only GPU

280
00:18:13,238 --> 00:18:17,222
instances for containers that require GPU

281
00:18:17,286 --> 00:18:20,734
and I don't want to share these instances because they

282
00:18:20,772 --> 00:18:24,670
are expensive. Another option

283
00:18:24,820 --> 00:18:27,914
is building prioritized or weighted

284
00:18:27,962 --> 00:18:32,030
provisioners if I want to use different

285
00:18:32,100 --> 00:18:35,460
configurations, but don't really separate it

286
00:18:37,030 --> 00:18:40,754
between the two configurations, but allow for example

287
00:18:40,872 --> 00:18:44,670
running 30% of my deployment

288
00:18:44,750 --> 00:18:48,438
on graviton instances and run all the rest

289
00:18:48,524 --> 00:18:52,040
on x 86 instances. I can do that

290
00:18:52,810 --> 00:18:56,434
with prioritized provisioners and implement

291
00:18:56,562 --> 00:18:58,070
kind of waiting.

292
00:19:00,590 --> 00:19:04,214
So inside a single provisioner, the point is to be as flexible

293
00:19:04,262 --> 00:19:08,154
as possible between the resources that can be consumed by

294
00:19:08,192 --> 00:19:11,854
the containers, so that carpenter will be the one that will make

295
00:19:11,892 --> 00:19:15,326
the intelligent choice of the right instance type to support the

296
00:19:15,348 --> 00:19:18,894
application needs. So what we see here is that we can have

297
00:19:18,932 --> 00:19:22,842
inside a single provisioning, use multiple instance

298
00:19:22,906 --> 00:19:27,322
types and multiple Aws, and I can have my deployment

299
00:19:27,466 --> 00:19:31,102
opensource topology spread between availability zones

300
00:19:31,246 --> 00:19:34,386
so that each replica is required to

301
00:19:34,408 --> 00:19:37,330
run in a different availability zone.

302
00:19:37,490 --> 00:19:40,774
Carpenter will be aware of this

303
00:19:40,812 --> 00:19:45,270
requirement and carpenter will be able to provision

304
00:19:45,930 --> 00:19:49,818
a node an instance for each replica in a

305
00:19:49,824 --> 00:19:53,354
different availability zone. Or carpenter will be

306
00:19:53,392 --> 00:19:56,922
able to run instances to

307
00:19:56,976 --> 00:20:00,314
run containers that require different instance

308
00:20:00,362 --> 00:20:03,614
types. For example, one container can

309
00:20:03,652 --> 00:20:06,970
request a memory optimized instance

310
00:20:07,050 --> 00:20:11,200
while all the rest can just run on whatever is available for it.

311
00:20:13,670 --> 00:20:16,846
One of the major ways that you can save on compute

312
00:20:16,878 --> 00:20:20,770
infrastructure is by using spot instances, and I already

313
00:20:20,840 --> 00:20:24,986
touched about it a little bit. So AWS offers

314
00:20:25,038 --> 00:20:28,406
different pricing models to allow you to choose the best option

315
00:20:28,508 --> 00:20:32,002
for your specific needs. Spot instances

316
00:20:32,066 --> 00:20:35,458
are the AWS pair unused capacity

317
00:20:35,634 --> 00:20:39,142
and it's offered in the same infrastructure

318
00:20:39,206 --> 00:20:42,858
as the other models at a lower price without

319
00:20:42,944 --> 00:20:46,570
any commitment. The only caveat is

320
00:20:46,720 --> 00:20:49,930
that whenever EC two needs that

321
00:20:50,000 --> 00:20:53,534
instance back, it will be able to interrupt it with

322
00:20:53,572 --> 00:20:56,030
a two minute notification warning.

323
00:20:56,930 --> 00:21:01,370
Now spot is a very effective way to get a large amount of capacity

324
00:21:01,450 --> 00:21:05,060
very economically. As long AWS, your application

325
00:21:05,670 --> 00:21:09,582
is aware of these interruption events

326
00:21:09,726 --> 00:21:13,582
and is capable of moving from one instance

327
00:21:13,646 --> 00:21:17,906
to a different one. So let's

328
00:21:17,938 --> 00:21:21,542
talk about containers. Containers are usually very

329
00:21:21,596 --> 00:21:25,014
flexible. If you modernize them, you went through

330
00:21:25,052 --> 00:21:28,662
the process of building them in a fault tolerant

331
00:21:28,726 --> 00:21:32,806
way. They are usually stateless. We have kubernetes

332
00:21:32,918 --> 00:21:36,662
and carpenter that can binpack our containers

333
00:21:36,806 --> 00:21:40,338
into shared resources. So we can use different sizes

334
00:21:40,374 --> 00:21:44,186
of instance types and different families of instance types

335
00:21:44,218 --> 00:21:48,618
or availability zones. And so containers

336
00:21:48,714 --> 00:21:52,190
fit really well to use spot instances.

337
00:21:52,770 --> 00:21:57,278
What's unique about carpenter is that it implements all the spot best practices

338
00:21:57,374 --> 00:22:01,550
which are listed in this slide. It's simplifies

339
00:22:01,630 --> 00:22:05,338
flexibility because by default it allows us to use all the EC

340
00:22:05,374 --> 00:22:09,160
two instance types that are available on the EC two platform.

341
00:22:09,690 --> 00:22:13,410
It uses allocation strategy of price capacity optimized

342
00:22:13,490 --> 00:22:17,518
that helps improve the workload stability and reduce interruption

343
00:22:17,554 --> 00:22:21,434
rates by always choosing the EC two instance that is from

344
00:22:21,472 --> 00:22:25,414
the deepest capacity pool. And carpenter

345
00:22:25,462 --> 00:22:30,178
also manages the spot lifecycle which includes identifying

346
00:22:30,214 --> 00:22:34,222
the interruption events, moving your

347
00:22:34,276 --> 00:22:38,478
containers from the interrupted instance to a different one,

348
00:22:38,644 --> 00:22:42,910
and making sure that we always choose the cheapest

349
00:22:43,350 --> 00:22:45,380
instance to work with.

350
00:22:47,110 --> 00:22:51,102
So this helps us get to the understanding

351
00:22:51,166 --> 00:22:54,370
that spot could be a very good fit for containers

352
00:22:55,050 --> 00:22:58,582
when working with Karpenter and you can tap into

353
00:22:58,636 --> 00:23:02,390
spot capacity and gain up to 90% discount.

354
00:23:03,770 --> 00:23:06,950
The next way to save on your compute is by

355
00:23:07,100 --> 00:23:11,050
using the Amazon develop chips Graviton

356
00:23:11,710 --> 00:23:14,454
I won't dive too much into graviton,

357
00:23:14,582 --> 00:23:18,678
but in two sentences graviton

358
00:23:18,854 --> 00:23:22,270
can provide you up to 40% better price

359
00:23:22,340 --> 00:23:25,918
performance. The list price is usually around

360
00:23:26,004 --> 00:23:30,030
15 or 20% less than the equivalent x

361
00:23:30,100 --> 00:23:33,918
86 intel instances and you can gain

362
00:23:34,014 --> 00:23:37,886
up to a lot more of the performance benefit depending

363
00:23:37,918 --> 00:23:41,214
on the use case. Graviton processors

364
00:23:41,342 --> 00:23:45,170
also provide improved sustainability,

365
00:23:45,530 --> 00:23:50,182
up to 60% more than more

366
00:23:50,236 --> 00:23:53,702
energy efficiency than the comparable x 86

367
00:23:53,756 --> 00:23:55,910
instance processors.

368
00:23:57,550 --> 00:24:01,322
So why carpenter is a great

369
00:24:01,376 --> 00:24:04,874
system, a great orchestration system

370
00:24:04,992 --> 00:24:09,594
to work with graviton processors if

371
00:24:09,632 --> 00:24:12,858
you went through the process of building multi

372
00:24:12,874 --> 00:24:16,174
architecture container images, which means that

373
00:24:16,212 --> 00:24:19,438
you want to allow your applications to use both

374
00:24:19,604 --> 00:24:23,150
graviton as well as x 86 processors,

375
00:24:24,470 --> 00:24:28,594
Karpenter is able to combine graviton, intel and

376
00:24:28,632 --> 00:24:32,286
AMD together in a single cluster just by adding

377
00:24:32,318 --> 00:24:37,030
the support of the different processors in your provisioner.

378
00:24:38,170 --> 00:24:42,006
And then when carpenter will scale up an instance, it will be

379
00:24:42,028 --> 00:24:45,720
able to choose whatever is available in the lowest price.

380
00:24:46,090 --> 00:24:48,730
Let's say that you got a graviton instance,

381
00:24:49,470 --> 00:24:53,414
then your multi architecture container

382
00:24:53,462 --> 00:24:57,894
manifests will be able to pull the graviton container

383
00:24:57,942 --> 00:25:01,342
image and run on Graviton. On the other hand,

384
00:25:01,396 --> 00:25:04,190
if you got an intel based instance,

385
00:25:05,090 --> 00:25:08,190
the multi architecture container manifest will pull

386
00:25:08,260 --> 00:25:12,270
the container image that is suitable for x 86

387
00:25:12,340 --> 00:25:16,626
processors. So carpenter really simplifies the

388
00:25:16,648 --> 00:25:21,506
combination and the usage of different processors inside

389
00:25:21,608 --> 00:25:23,490
worker nodes in kubernetes.

390
00:25:25,750 --> 00:25:29,494
So I'm going to summarize now what I've been

391
00:25:29,532 --> 00:25:33,046
talking about. Remember we defined in the beginning what

392
00:25:33,068 --> 00:25:36,546
is efficiency and what we want to achieve with auto scaling.

393
00:25:36,738 --> 00:25:40,346
We want to be able to provision just the amount

394
00:25:40,448 --> 00:25:44,950
of resources that our applications need. We want to densify

395
00:25:45,030 --> 00:25:48,682
them and be able to choose the right instance sizes that will

396
00:25:48,736 --> 00:25:51,966
allow for the highest bin packing, and we want

397
00:25:51,988 --> 00:25:55,354
to be able to be flexible with different purchase

398
00:25:55,402 --> 00:25:59,054
options. Instance types and instance families so that

399
00:25:59,092 --> 00:26:02,974
we can use the best price performance instances

400
00:26:03,022 --> 00:26:06,606
for our applications. Carpenter essentially provides

401
00:26:06,638 --> 00:26:10,926
the ability to accomplish all of those. It's compatible

402
00:26:11,038 --> 00:26:15,070
with native Kubernetes scheduling. It offers flexibility

403
00:26:15,150 --> 00:26:19,110
and cost optimization using spot and gravitron instances.

404
00:26:19,610 --> 00:26:24,310
And because all the configurations are built in with carpenter,

405
00:26:24,890 --> 00:26:28,250
you know you are scaling with the best practices

406
00:26:28,910 --> 00:26:33,846
so that you can gain the most out of your carpenter

407
00:26:33,878 --> 00:26:37,562
deployment. Last but not least,

408
00:26:37,696 --> 00:26:41,486
carpenter is a project in huge development right

409
00:26:41,508 --> 00:26:45,070
now and new features are going out

410
00:26:45,140 --> 00:26:48,734
all the time. Carpenter is an open source and

411
00:26:48,772 --> 00:26:52,698
you can follow code and roadmap on GitHub,

412
00:26:52,874 --> 00:26:56,206
and you can open issues directly to the development team to

413
00:26:56,228 --> 00:26:59,870
get quick feedback. So I really recommend taking a look on the

414
00:26:59,940 --> 00:27:02,350
Carpenter project on GitHub.

415
00:27:03,810 --> 00:27:07,430
Thanks so much for are listening to me and enjoy

416
00:27:07,500 --> 00:27:08,450
the rest of the conference.

