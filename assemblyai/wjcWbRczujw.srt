1
00:00:25,410 --> 00:00:28,280
You. Hi everyone,

2
00:00:28,730 --> 00:00:32,280
thanks for joining. We'll be talking

3
00:00:33,210 --> 00:00:36,566
on building a data platform

4
00:00:36,668 --> 00:00:40,962
to process over 50 billion in card transactions.

5
00:00:41,106 --> 00:00:45,410
I'm Sandeep, I'm engineering lead data platforms. I'm from Dojo.

6
00:00:45,570 --> 00:00:47,480
If you don't know about us,

7
00:00:48,970 --> 00:00:52,202
we are the fastest growing fintech in Europe by net revenue.

8
00:00:52,266 --> 00:00:56,174
We power face to face payments for around 15% of

9
00:00:56,212 --> 00:00:59,950
card transaction in the UK every day. We are mainly focused on

10
00:01:00,100 --> 00:01:03,546
experience economy of bars, pubs,

11
00:01:03,578 --> 00:01:07,586
restaurants, even your local corner shop or farmers market. We also

12
00:01:07,608 --> 00:01:10,866
have a consumer facing app that allows you to join the queue for

13
00:01:10,888 --> 00:01:14,526
a high street restaurants for up to around 2 miles

14
00:01:14,718 --> 00:01:18,694
radius. We have an amazing line

15
00:01:18,732 --> 00:01:21,782
of payment products. We have tap to pay pocket pay.

16
00:01:21,836 --> 00:01:25,302
We also, sorry, provide the small

17
00:01:25,356 --> 00:01:29,306
business funding. And we are also building many, many products

18
00:01:29,488 --> 00:01:32,940
to change how the payment industry works.

19
00:01:33,470 --> 00:01:37,466
We are going international soon. Hopefully you will see us in

20
00:01:37,488 --> 00:01:41,070
every corner of Europe first and then in the world.

21
00:01:41,220 --> 00:01:44,702
What makes us different,

22
00:01:44,836 --> 00:01:48,062
I guess, and also makes all of this possible is the fact

23
00:01:48,116 --> 00:01:52,106
that we are highly, highly data driven and value the insights

24
00:01:52,138 --> 00:01:55,858
in our decision making. When you go

25
00:01:55,864 --> 00:01:59,646
to a shop and when you tap your card and within a blink

26
00:01:59,678 --> 00:02:03,042
of a second your transaction happens there and

27
00:02:03,096 --> 00:02:07,054
you get the goods and the customer, the merchant,

28
00:02:07,102 --> 00:02:10,566
basically for us it's the customer, but the merchant who is selling you the

29
00:02:10,588 --> 00:02:13,240
goods or selling you the services get the money.

30
00:02:14,330 --> 00:02:17,622
All of this happens within a blink of a second.

31
00:02:17,676 --> 00:02:20,978
As we just talked about, when you tap a card,

32
00:02:21,084 --> 00:02:24,570
the first thing happens is the card machine takes the details from the card

33
00:02:24,640 --> 00:02:28,438
and securely sends these to the authorization gateway. At Dojo,

34
00:02:28,534 --> 00:02:32,170
we use point to point encryption between the card machine and our authorization

35
00:02:32,250 --> 00:02:36,650
gateway with hardware security modules in multiple facilities

36
00:02:36,730 --> 00:02:39,466
with direct pairing to multiple cloud providers.

37
00:02:39,658 --> 00:02:43,442
It's highly, highly secure and highly, highly scalable. We can't let

38
00:02:43,496 --> 00:02:47,730
our authorization gateway down because that's the main point

39
00:02:47,800 --> 00:02:51,342
from where we take the transactions. The authorization

40
00:02:51,406 --> 00:02:54,638
gateway then contacts the card network such as Visa,

41
00:02:54,734 --> 00:02:58,326
Mastercard, Amex, and then forwards this request to

42
00:02:58,348 --> 00:03:01,894
the issuing bank for approval. This will be

43
00:03:02,092 --> 00:03:05,062
someone like your bank in UK Barclays card,

44
00:03:05,116 --> 00:03:09,178
Monzo, Tesco, who will then freeze the funds on the customer's account

45
00:03:09,344 --> 00:03:13,754
in support. It's very important to note that the

46
00:03:13,792 --> 00:03:17,914
money, the actual money has

47
00:03:17,952 --> 00:03:21,734
not exchanged yet, and it is just a promise

48
00:03:21,782 --> 00:03:24,080
for a payment that can be reversed as well.

49
00:03:25,010 --> 00:03:28,794
The card network then sends this approval back to the authorization gateway,

50
00:03:28,842 --> 00:03:32,346
which is then displayed to the customer as approved or declined.

51
00:03:32,378 --> 00:03:36,050
And this is how your card transactions actually

52
00:03:36,200 --> 00:03:39,986
works. And this whole process generates a

53
00:03:40,008 --> 00:03:42,180
lot of data from a payment point of view,

54
00:03:43,750 --> 00:03:47,974
because we are a payment company. So we

55
00:03:48,012 --> 00:03:53,494
have a couple of regulatory challenges where we

56
00:03:53,532 --> 00:03:57,858
own the end to end payments experience. So we operate under e money license

57
00:03:57,954 --> 00:04:01,510
from the FCA, which is a financial conduct authority.

58
00:04:01,670 --> 00:04:04,870
With this comes strict regulatory requirements,

59
00:04:05,030 --> 00:04:08,554
most notably the fact that we have to ensure the whole time

60
00:04:08,592 --> 00:04:12,890
that customer funds are safeguarded in case the business becomes irrelevant

61
00:04:13,050 --> 00:04:14,590
or insolvent.

62
00:04:16,210 --> 00:04:20,080
Then you have PCI DSS level one

63
00:04:21,570 --> 00:04:25,314
compliance, which is around safeguarding or not

64
00:04:25,352 --> 00:04:29,246
safeguarding, storing of full card numbers. So owning

65
00:04:29,278 --> 00:04:32,366
the whole payment stacks with full card numbers. We also have to comply

66
00:04:32,398 --> 00:04:35,862
with PCI DSS level one. And we also are

67
00:04:35,916 --> 00:04:39,286
independently audited every year to ensure that we

68
00:04:39,308 --> 00:04:43,218
actually follow all the files and guidelines provided

69
00:04:43,234 --> 00:04:47,822
by PCI. Then we have other complexities

70
00:04:47,906 --> 00:04:51,434
around schemas. So we process,

71
00:04:51,552 --> 00:04:54,746
I guess more than 1000 plus data contracts or

72
00:04:54,768 --> 00:04:58,282
schemas. And this talk, sorry, I completely

73
00:04:58,336 --> 00:05:01,706
forgot to mention this talk will be around file

74
00:05:01,738 --> 00:05:04,974
processing. So I'm going to just talk about how we

75
00:05:05,012 --> 00:05:08,542
built the processing of these

76
00:05:08,596 --> 00:05:12,094
files coming from different, different sources and all these

77
00:05:12,132 --> 00:05:15,554
schemes, Visa, Mastercard, and all the payment

78
00:05:15,752 --> 00:05:19,060
boxes. We have so

79
00:05:19,830 --> 00:05:23,566
1000 schemas, as I said. And then you have multiple file sizes

80
00:05:23,598 --> 00:05:27,074
and multiple formats. So it's just not

81
00:05:27,112 --> 00:05:30,866
like you have standard file size. Okay, we're going to have a file

82
00:05:30,978 --> 00:05:34,486
size range from five mb to ten mb. It doesn't work like that.

83
00:05:34,508 --> 00:05:37,666
We have files which are two kb, ten kb,

84
00:05:37,698 --> 00:05:41,370
50 kb, I don't know. And then it goes up to gigabytes,

85
00:05:41,790 --> 00:05:45,770
and then we also get like zipped files as well. Then you

86
00:05:45,840 --> 00:05:49,610
have a scalability, you can have unpredictable demand.

87
00:05:50,030 --> 00:05:53,406
Sometimes we have, I don't know, 500,000 of

88
00:05:53,428 --> 00:05:57,086
files coming right now because of

89
00:05:57,268 --> 00:06:00,638
slas between multiple sources, they overlap and all of

90
00:06:00,644 --> 00:06:03,850
the file comes at the same time. And then all of

91
00:06:03,860 --> 00:06:07,186
those files sometimes are business critical. So we have to process all of them at

92
00:06:07,208 --> 00:06:10,690
the same time. So for example,

93
00:06:10,760 --> 00:06:14,318
if you see this is a snapshot

94
00:06:14,334 --> 00:06:17,702
of internal reconciliation process performed by our

95
00:06:17,756 --> 00:06:21,410
payments analytical engineering team, just to ensure

96
00:06:21,490 --> 00:06:24,934
that a merchant's net settlement tallies with card transactions that

97
00:06:24,972 --> 00:06:28,650
have been actually authorized on the tills in their shops.

98
00:06:28,990 --> 00:06:32,730
As you can see, there's like CSVs XLS XML

99
00:06:33,550 --> 00:06:36,780
that many files required to get just that done.

100
00:06:38,750 --> 00:06:44,686
And also another bit like all

101
00:06:44,708 --> 00:06:48,506
the files coming in doesn't just have XLS or XML,

102
00:06:48,618 --> 00:06:51,040
they have all the files formats possible.

103
00:06:52,950 --> 00:06:56,286
We have JSON files, we even have Avro

104
00:06:56,318 --> 00:06:59,966
files. Now we have parquet files

105
00:06:59,998 --> 00:07:03,474
as well. We have fix

106
00:07:03,512 --> 00:07:07,170
with files. We have done this proprietary formats

107
00:07:07,330 --> 00:07:10,818
created by this scheme,

108
00:07:10,994 --> 00:07:14,806
companies like Visa, Mastercard. For that you had to write

109
00:07:14,988 --> 00:07:18,450
very heavy custom parsers to actually make sense out of

110
00:07:18,540 --> 00:07:22,234
those files and put them into your transactional kind

111
00:07:22,272 --> 00:07:23,530
of data warehouse.

112
00:07:25,710 --> 00:07:29,906
And the goal

113
00:07:30,038 --> 00:07:34,314
which we came up, or the goal

114
00:07:34,362 --> 00:07:38,494
which we thought would be good for us to actually take

115
00:07:38,532 --> 00:07:41,966
all of these file formats, process them into

116
00:07:42,068 --> 00:07:45,854
a final single format which can be then later utilized

117
00:07:45,902 --> 00:07:49,330
by processes downstreams to stream that into

118
00:07:49,400 --> 00:07:54,814
warehouse, or stream that into Kafka, or stream that into snowflake,

119
00:07:54,862 --> 00:07:57,494
or stream that into anywhere else, wherever we want.

120
00:07:57,612 --> 00:08:01,058
And we chose Avro because of most of the powers

121
00:08:01,074 --> 00:08:06,130
around scheme evolution and mainly

122
00:08:06,210 --> 00:08:09,634
that I guess. But there now there are multiple different formats

123
00:08:09,682 --> 00:08:12,940
which you can choose. But we chose Avro at that point.

124
00:08:14,430 --> 00:08:17,786
And it's not just payments. There are a lot of other business areas in the

125
00:08:17,808 --> 00:08:21,914
company which are important. They produce a lot of events data and

126
00:08:21,952 --> 00:08:25,486
also generate a lot of files data. And that

127
00:08:25,508 --> 00:08:29,534
can be coming from APIs or that can be coming from external tools or

128
00:08:29,572 --> 00:08:32,590
that can be generated by their own microservices.

129
00:08:33,330 --> 00:08:36,830
So to support the processing of files with all the schema evolution,

130
00:08:36,910 --> 00:08:40,674
with the scale and with all these challenges in mind, it was very

131
00:08:40,712 --> 00:08:44,254
important for us to design a data platform which is scalable

132
00:08:44,302 --> 00:08:47,720
and self serve. And now before deep diving into

133
00:08:48,490 --> 00:08:51,862
the modern data infracessing and how

134
00:08:51,916 --> 00:08:55,286
we have done it, I would like to take us back into the

135
00:08:55,308 --> 00:08:56,950
history of data processing.

136
00:08:58,430 --> 00:09:01,862
There were three generation of data processing.

137
00:09:01,926 --> 00:09:05,366
First generation was the generation of enterprise data warehouse

138
00:09:05,398 --> 00:09:09,562
solutions. Big giants like Oracle, IBM, Microsoft were

139
00:09:09,616 --> 00:09:13,966
building those big big warehouse which only few people

140
00:09:14,068 --> 00:09:17,546
know how to use. Then came the era

141
00:09:17,578 --> 00:09:20,858
of big data ecosystem, the era of Hadoop, the era

142
00:09:20,874 --> 00:09:24,350
of pig hide spark came into the picture.

143
00:09:24,510 --> 00:09:27,662
We built those monolith big, big centralized,

144
00:09:27,726 --> 00:09:31,860
beefy hadoops, big data platforms which was again

145
00:09:32,790 --> 00:09:36,534
only utilized or monitored or operated by

146
00:09:36,572 --> 00:09:40,246
few people. And huge bottleneck and huge

147
00:09:40,348 --> 00:09:44,422
number of skill shortage to actually make the

148
00:09:44,476 --> 00:09:48,082
huge, to make the use of it, I would say.

149
00:09:48,236 --> 00:09:52,122
Then we talk about current generation, mostly centralized data

150
00:09:52,176 --> 00:09:56,522
platforms. Mix of batch and real time processing based on tools like

151
00:09:56,656 --> 00:09:59,980
Kafka, Apache Beam also gives you the flavor of

152
00:10:01,730 --> 00:10:05,402
the mix. Real time and batch processing, then pubsub,

153
00:10:05,466 --> 00:10:09,374
then red panda, then all sort of cloud

154
00:10:09,412 --> 00:10:12,942
managed services like AWS, GCP, other cloud

155
00:10:12,996 --> 00:10:16,366
providers like confluent, Avon, they are giving

156
00:10:16,388 --> 00:10:20,002
their own managed services on top. Well, this centralized model

157
00:10:20,056 --> 00:10:23,266
can work for organization that have a simpler domain with

158
00:10:23,288 --> 00:10:27,286
a smaller number of diverse conception cases. It files for us

159
00:10:27,468 --> 00:10:31,746
because we have rich domains, a large number of sources,

160
00:10:31,858 --> 00:10:34,230
a large number of consumers,

161
00:10:37,930 --> 00:10:41,274
and most of the companies who are going

162
00:10:41,312 --> 00:10:44,506
through such a growth, they also have this problem and

163
00:10:44,528 --> 00:10:48,502
this centralized data platform really doesn't work. And there are other challenges

164
00:10:48,566 --> 00:10:52,478
with it. The challenge that the centralized data platform

165
00:10:52,564 --> 00:10:56,720
is mainly owned by a centralized data team which

166
00:10:57,410 --> 00:11:01,054
is focused on building, maintaining data

167
00:11:01,092 --> 00:11:04,898
processing pipelines, then building data contracts working hand in

168
00:11:04,904 --> 00:11:08,466
hand with stakeholders. But at the end of the day, there's no clear ownership on

169
00:11:08,488 --> 00:11:12,834
that. Then issues support for

170
00:11:12,872 --> 00:11:16,386
all the domains without actually having the domain

171
00:11:16,418 --> 00:11:20,280
knowledge. Then you have silos, then specialized data team

172
00:11:21,450 --> 00:11:25,240
which will keep on adding features if they get some time away from

173
00:11:26,010 --> 00:11:29,610
the support issues or everyday change requests.

174
00:11:30,030 --> 00:11:33,322
Then another issue is data

175
00:11:33,376 --> 00:11:37,114
quality, accountability, democratization of data.

176
00:11:37,232 --> 00:11:40,702
It's very difficult to enhance or put measures for data quality

177
00:11:40,756 --> 00:11:44,574
if there is no clear ownership on the data. The fact that

178
00:11:44,612 --> 00:11:48,446
data team manages data access, it makes it a bottleneck when

179
00:11:48,468 --> 00:11:53,190
it comes to access request. Then scalability,

180
00:11:53,370 --> 00:11:57,250
adding new data sources, increased data volumes, data contract changes,

181
00:11:57,320 --> 00:12:01,166
et cetera, can be delayed due to a huge data team backlog

182
00:12:01,278 --> 00:12:05,010
because they are the one who are actually

183
00:12:05,080 --> 00:12:09,046
managing and maintaining the data pipelines. And the byproduct of

184
00:12:09,068 --> 00:12:13,542
this also is not such a great relationship between

185
00:12:13,596 --> 00:12:16,806
data team and the other teams, and also a lot of blame game when this

186
00:12:16,828 --> 00:12:20,106
goes wrong. And over the last decade or

187
00:12:20,128 --> 00:12:23,354
so, we have successfully applied the domain driven design into

188
00:12:23,392 --> 00:12:26,380
our engineering or operational side.

189
00:12:27,150 --> 00:12:31,206
But as a whole data community, we completely, or as a whole engineering

190
00:12:31,238 --> 00:12:34,654
community, we completely forgot to put that into the data side.

191
00:12:34,852 --> 00:12:38,202
Now, what should we do in this kind of scenario?

192
00:12:38,266 --> 00:12:42,366
Right? How should we go on? And what is the right way of building

193
00:12:42,468 --> 00:12:47,074
that file processing platform which we built at dojo, or any

194
00:12:47,112 --> 00:12:51,010
kind of data platform which you might want to build it or anybody

195
00:12:51,080 --> 00:12:54,354
wants to build it. Now imagine this,

196
00:12:54,392 --> 00:12:57,942
right? What if the centralized data team will

197
00:12:57,996 --> 00:13:01,734
only focus on creating a generic data infrastructure, building self

198
00:13:01,772 --> 00:13:06,018
served data platforms by abstracting away all the technical complexities

199
00:13:06,194 --> 00:13:09,834
and enabling other teams to easily process their own data.

200
00:13:10,032 --> 00:13:13,994
Apart from that, they will also provide a global governance model and

201
00:13:14,032 --> 00:13:17,830
policies to have better access management, better naming conventions,

202
00:13:17,910 --> 00:13:20,410
better cis, better security policies,

203
00:13:20,930 --> 00:13:24,030
and at the same time, domain ownership moves,

204
00:13:24,450 --> 00:13:28,094
sorry, data ownership moves from centralized team

205
00:13:28,132 --> 00:13:31,246
to the domains. I have said domains many times in this

206
00:13:31,268 --> 00:13:34,882
chat so far, but by domains I mean teams that are working

207
00:13:34,936 --> 00:13:38,174
on a certain business area, for example, payment, marketing,

208
00:13:38,222 --> 00:13:41,250
customer, et cetera, et cetera.

209
00:13:42,070 --> 00:13:45,182
Now, this whole approach brings a set of benefits.

210
00:13:45,336 --> 00:13:48,678
Finally, the data is owned by people who understand

211
00:13:48,764 --> 00:13:52,182
it better company wise. You reach a better

212
00:13:52,236 --> 00:13:56,274
scalability by distributing data processing across the domains.

213
00:13:56,402 --> 00:14:00,166
Domains are now independent and they are able to add new data sources,

214
00:14:00,278 --> 00:14:03,180
create new data pipelines, fix the issues by themselves.

215
00:14:03,550 --> 00:14:06,794
Domains can put better data quality and reliability measures than

216
00:14:06,832 --> 00:14:09,980
the centralized data team because they understand it better.

217
00:14:10,510 --> 00:14:14,462
Domains have the flexibility of using only what they need from

218
00:14:14,516 --> 00:14:18,410
the generic data infrastructure and self serve data platform features,

219
00:14:18,490 --> 00:14:21,950
which reduces the complexity blast radius if things goes

220
00:14:22,020 --> 00:14:25,890
bad and things always goes bad. And having

221
00:14:25,960 --> 00:14:29,922
said that, the centralized data team should

222
00:14:29,976 --> 00:14:33,854
make sure that there is a good monitoring

223
00:14:33,902 --> 00:14:37,174
alerting in place to monitor all

224
00:14:37,212 --> 00:14:41,142
the flavors or features of data platform across

225
00:14:41,196 --> 00:14:44,422
the domains. Now this approach will also

226
00:14:44,476 --> 00:14:48,450
enforce well documented data contracts data API,

227
00:14:48,610 --> 00:14:51,962
which will allow data to flow seamlessly between one system and the another

228
00:14:52,016 --> 00:14:55,370
system. It can be internal or external as well.

229
00:14:55,440 --> 00:14:59,466
And having domains owning their data infrastructure brings more visibility on

230
00:14:59,488 --> 00:15:03,246
resource allocation and utilization, which could

231
00:15:03,268 --> 00:15:06,510
lead into cost efficiency.

232
00:15:06,850 --> 00:15:09,710
Now this whole concept is, my friend is data mesh.

233
00:15:11,730 --> 00:15:15,262
Now data Mesh is, I know I'm talking

234
00:15:15,316 --> 00:15:18,434
a lot of theory right now, but I will come back how

235
00:15:18,472 --> 00:15:22,094
we build that dojo. But this is very important. So data mesh

236
00:15:22,142 --> 00:15:25,074
is like domain ownership. It's one of the best,

237
00:15:25,192 --> 00:15:28,790
or sorry, not one of the best. It's one of the important pillar.

238
00:15:30,090 --> 00:15:33,366
It's your data, you own it,

239
00:15:33,548 --> 00:15:37,320
you manage it, and if there is a problem with it, you fix it.

240
00:15:39,210 --> 00:15:42,742
Data as a product, data is not a byproduct.

241
00:15:42,806 --> 00:15:45,878
Treat your data as a product, whether it's

242
00:15:45,894 --> 00:15:49,386
a file, whether it's an event, whether it's a warehouse, or whether

243
00:15:49,408 --> 00:15:52,880
it's a beefy table, all of that treat it as a product.

244
00:15:53,650 --> 00:15:56,974
Federated computational governance each domain can set

245
00:15:57,012 --> 00:16:00,510
their own policies or rules over data. For example,

246
00:16:00,580 --> 00:16:04,686
payment domain sftps encryption standards on payment data or marketing

247
00:16:04,718 --> 00:16:08,418
domain sftps retention on the customer data to 28 days to

248
00:16:08,424 --> 00:16:11,986
adhere to GDPR standards. Then the

249
00:16:12,008 --> 00:16:15,170
final and my favorite bit is self serve data platform.

250
00:16:15,320 --> 00:16:19,542
This is the key in all of this. If you successfully build

251
00:16:19,676 --> 00:16:22,710
this, which means self serve data platform,

252
00:16:22,860 --> 00:16:26,470
you've got 80% things right. This will enable teams to own

253
00:16:26,540 --> 00:16:30,138
and manage their data and data processing. But the big question

254
00:16:30,224 --> 00:16:33,882
is how you're going to build it. It looks

255
00:16:33,936 --> 00:16:37,402
easy now, but when you have so many choices and

256
00:16:37,456 --> 00:16:40,862
so many people pulling you in conferences that their data platform is the best,

257
00:16:40,916 --> 00:16:45,310
they have the one stop solution. It can be quite overwhelming,

258
00:16:46,770 --> 00:16:50,702
but I guess based on your use case you will do certain

259
00:16:50,756 --> 00:16:53,826
PoCs. You would try to try to

260
00:16:53,848 --> 00:16:57,806
look at probably open source solutions

261
00:16:57,838 --> 00:17:01,394
available before buying already, I don't know,

262
00:17:01,432 --> 00:17:04,974
committing to thousands and thousands of pounds or thousands and thousands of dollars

263
00:17:05,032 --> 00:17:08,806
to some managed providers claiming that they can

264
00:17:08,828 --> 00:17:12,566
solve all your problems. When we

265
00:17:12,588 --> 00:17:16,486
were building this file processing platform, we were quite sure that

266
00:17:16,668 --> 00:17:20,098
the platform, as a service or self serve platform offering was the

267
00:17:20,124 --> 00:17:23,706
only way to move forward. Now on

268
00:17:23,728 --> 00:17:27,146
a high level, four main features in our platform provides this end

269
00:17:27,168 --> 00:17:30,330
to end solution for file processing, which we just talked about before,

270
00:17:30,400 --> 00:17:34,366
that we have 20 plus different types of

271
00:17:34,388 --> 00:17:38,206
files coming in, and then we have around 500k

272
00:17:38,388 --> 00:17:41,934
files every day coming in, which we have to process varies in different size,

273
00:17:42,052 --> 00:17:45,442
and then they come in in certain hours and we have to scale the system

274
00:17:45,496 --> 00:17:48,846
accordingly as well. Now, four components.

275
00:17:48,878 --> 00:17:52,466
First component is source connectors, which is ingesting data from

276
00:17:52,488 --> 00:17:55,686
external providers in a consistent way. So we

277
00:17:55,708 --> 00:17:58,962
did build those connectors to bring the data. Then you have PCI

278
00:17:59,026 --> 00:18:02,470
processing platform, which actually makes sure

279
00:18:02,620 --> 00:18:05,954
that a clear credit card information is stored,

280
00:18:06,082 --> 00:18:09,254
masked and processed successfully and very,

281
00:18:09,292 --> 00:18:12,918
very securely, and then being sent to non PCI platform,

282
00:18:13,084 --> 00:18:16,860
and the non PCI platform which takes all the non PCI data

283
00:18:17,470 --> 00:18:20,826
and also the data coming from all these other domains and

284
00:18:20,848 --> 00:18:25,226
everywhere and perform schema

285
00:18:25,258 --> 00:18:29,182
evaluation. I always struggle with this word and data

286
00:18:29,236 --> 00:18:32,834
validation and generate outputs into that final format which

287
00:18:32,872 --> 00:18:36,606
we agreed before Avro and basically chunked Avro

288
00:18:36,798 --> 00:18:41,890
because the file size has been different,

289
00:18:41,960 --> 00:18:45,854
the source file size. So we make sure that the final avro files

290
00:18:45,902 --> 00:18:49,254
are chunked into somewhere around roughly to the same

291
00:18:49,292 --> 00:18:52,786
size. So we don't end up having like one avro file which is two gig,

292
00:18:52,818 --> 00:18:55,190
and one avro file which is like few kb's.

293
00:18:56,970 --> 00:19:00,102
Sorry. Then target

294
00:19:00,166 --> 00:19:04,678
connectors. Streaming the data generated

295
00:19:04,854 --> 00:19:07,926
avro files, let's say from this lake

296
00:19:07,958 --> 00:19:11,930
house kind of, or distributed data lakes into the data warehouse

297
00:19:12,090 --> 00:19:16,560
or into any other streaming system or into

298
00:19:17,410 --> 00:19:20,954
any other external kind of like warehouse

299
00:19:21,002 --> 00:19:24,754
if Snowflake, or loading that

300
00:19:24,792 --> 00:19:27,794
into mlops platform and things like that.

301
00:19:27,912 --> 00:19:31,090
Now let's deep dive into all of these components.

302
00:19:33,830 --> 00:19:37,506
Connectors, source connectors. We have a number of

303
00:19:37,528 --> 00:19:41,026
different data sources. We have storage buckets, we have external

304
00:19:41,058 --> 00:19:44,610
APIs, we have webhooks, we have Gmail attachments.

305
00:19:44,690 --> 00:19:48,326
Trust me, we still have processes where we have to get the data from attachments.

306
00:19:48,438 --> 00:19:52,730
Then we have external sftps servers. So mainly we use

307
00:19:52,880 --> 00:19:56,282
Arclone to move most

308
00:19:56,336 --> 00:19:59,830
of the data which is coming in files. It's an amazing

309
00:19:59,920 --> 00:20:03,754
open source utility, which comes very handy

310
00:20:03,802 --> 00:20:07,454
when you want to move files between two storage systems. You can move

311
00:20:07,492 --> 00:20:11,722
from s three to gcs

312
00:20:11,786 --> 00:20:14,830
or SFTP to any kind of storage bucket,

313
00:20:14,990 --> 00:20:22,306
and it works like a charm. But you

314
00:20:22,328 --> 00:20:25,220
have to spend some time on the configuration part of it.

315
00:20:25,990 --> 00:20:29,222
Then you have Webex. So if the data

316
00:20:29,276 --> 00:20:32,902
was not in files, for example in case of webhooks we batch those events into

317
00:20:32,956 --> 00:20:37,110
files to keep things consistent. And we

318
00:20:37,180 --> 00:20:40,330
did have some strict lsas slas but we

319
00:20:40,400 --> 00:20:44,954
did not have slas to process this information in real time. So in

320
00:20:44,992 --> 00:20:48,442
those cases where we don't have adhere kind of like

321
00:20:48,576 --> 00:20:51,754
slas, okay, we need to process this information straight away.

322
00:20:51,952 --> 00:20:56,122
We also use webhook kind of connectors

323
00:20:56,186 --> 00:20:59,082
which batch those events and send them into the files.

324
00:20:59,226 --> 00:21:03,086
I guess we are moving that completely into the overstreaming side now. I guess that

325
00:21:03,108 --> 00:21:06,254
was the historic decision which we took. Then we have serverless

326
00:21:06,302 --> 00:21:10,034
functions which allows the data to be received automatically by

327
00:21:10,072 --> 00:21:13,982
email, including email body. I just talked about the email attachments

328
00:21:14,046 --> 00:21:17,762
up there. We have one provider that does not attach a file

329
00:21:17,826 --> 00:21:21,414
but provides a system configuration message in the body of the email.

330
00:21:21,532 --> 00:21:24,280
So we had to write a parser for that as well.

331
00:21:25,370 --> 00:21:28,934
I know they always have use cases like that

332
00:21:29,052 --> 00:21:32,726
and then we use these connectors to land the data in the PCI and non

333
00:21:32,758 --> 00:21:36,362
PCI platforms like depending on the sensitivity of the data.

334
00:21:36,416 --> 00:21:39,718
So if we know that these files

335
00:21:39,814 --> 00:21:43,338
are encrypted and they are supposed to

336
00:21:43,344 --> 00:21:46,702
be processed by PCI to get the credit card

337
00:21:46,756 --> 00:21:50,366
information masked, they directly go to the PCI platform and if

338
00:21:50,388 --> 00:21:53,698
they are not they then bypass that process and directly go to

339
00:21:53,704 --> 00:21:57,090
the non PCI platform. Before jumping into

340
00:21:57,160 --> 00:22:00,434
the PCI platform, just few

341
00:22:00,472 --> 00:22:03,250
lines on what is PCI compliance.

342
00:22:03,750 --> 00:22:07,606
So all the listeners would understand how

343
00:22:07,628 --> 00:22:10,918
much complexity is or how much things you have

344
00:22:10,924 --> 00:22:14,726
to consider while building a PCI data platform kind

345
00:22:14,748 --> 00:22:17,750
of environment.

346
00:22:18,330 --> 00:22:21,734
Adhering to PCI standard is one of our prime

347
00:22:21,782 --> 00:22:25,654
concerns given we own the end to end payment stack

348
00:22:25,782 --> 00:22:29,782
and these standards are the set of security requirements established by the PCI

349
00:22:29,926 --> 00:22:33,470
SSE to ensure that credit card information

350
00:22:33,540 --> 00:22:37,070
is process successfully within any organization. Some of the key points

351
00:22:37,220 --> 00:22:40,446
from this compliance are at a high level. They are like all

352
00:22:40,468 --> 00:22:44,846
the credit card data has to be transmitted through secure encrypted

353
00:22:44,878 --> 00:22:48,738
channels and then clear card

354
00:22:48,824 --> 00:22:51,986
numbers cannot be

355
00:22:52,008 --> 00:22:55,630
stored anywhere unless they are anonymized or encrypted.

356
00:22:55,790 --> 00:22:59,202
Then you have the data platform that the platform which deals with the PCI

357
00:22:59,266 --> 00:23:02,454
data has to be audited for

358
00:23:02,492 --> 00:23:04,600
any security concerns every year.

359
00:23:05,210 --> 00:23:08,654
So when we have PCI, so this is our PCI

360
00:23:08,722 --> 00:23:10,860
management process within the PCI platform,

361
00:23:11,870 --> 00:23:15,226
we have those files coming in, we don't know the

362
00:23:15,248 --> 00:23:18,694
size of the files, they are zipped, they are encrypted.

363
00:23:18,742 --> 00:23:22,590
So we have to decrypt the source file in memory using

364
00:23:22,660 --> 00:23:26,382
confidential computing nodes. Then we encrypt with our

365
00:23:26,436 --> 00:23:30,362
own key and archive the files for future purposes and for safeguarding.

366
00:23:30,506 --> 00:23:34,266
Then if it's a zip file, we unzip on the disk. If the file

367
00:23:34,298 --> 00:23:38,066
contains pans, we open the file, we mask the pans, then send the

368
00:23:38,088 --> 00:23:41,746
masked version of the file to the non PCI platform. Because now,

369
00:23:41,848 --> 00:23:45,300
because it's masked, it doesn't fall into the

370
00:23:46,410 --> 00:23:50,214
PCI category. And this

371
00:23:50,252 --> 00:23:54,230
is how the overall processing looks like.

372
00:23:54,300 --> 00:23:57,350
So as you can see, the files start arriving.

373
00:23:58,750 --> 00:24:01,722
It's a bit smaller for me. Yeah.

374
00:24:01,856 --> 00:24:06,090
So the file starts arriving into the PCI storage bucket

375
00:24:07,310 --> 00:24:10,938
and this is all running in GKE.

376
00:24:11,034 --> 00:24:14,286
In Kubernetes, Rclone is running as

377
00:24:14,308 --> 00:24:17,806
a cron job in Kubernetes. For object storage we are

378
00:24:17,828 --> 00:24:21,306
using GCP, GCP's gcs

379
00:24:21,418 --> 00:24:24,500
and for queues we are using pubsub at the moment.

380
00:24:24,870 --> 00:24:28,900
So just so you know that we are completely GCP based,

381
00:24:29,670 --> 00:24:33,780
but most of our workloads and most of our processing power

382
00:24:34,230 --> 00:24:37,478
runs on GKE. So it's a

383
00:24:37,484 --> 00:24:41,746
good mixture of being cloud native as well as cloud agnostic

384
00:24:41,778 --> 00:24:45,702
at the same time. So these connectors are running, they are pulling the file from

385
00:24:45,756 --> 00:24:49,702
SFTP or storage or s three, and then these files arrives into

386
00:24:49,756 --> 00:24:53,206
object storage. The moment the file arrives in object storage,

387
00:24:53,238 --> 00:24:56,506
a file event has been created that okay, the file is

388
00:24:56,528 --> 00:24:59,226
created, the files is created, the file is created, and that goes into a pub

389
00:24:59,248 --> 00:25:02,814
sub queue. And then based on the number

390
00:25:02,852 --> 00:25:06,766
of messages in the queue, the HPA will scale the workload. We will

391
00:25:06,788 --> 00:25:10,110
talk about scaling in detail in the later part of the presentation.

392
00:25:12,450 --> 00:25:16,286
We typically have around 300 pods running at peak

393
00:25:16,318 --> 00:25:20,530
hours across 40 nodes to process around 300

394
00:25:20,600 --> 00:25:23,300
or 200k files within like 30 minutes.

395
00:25:24,070 --> 00:25:27,826
Pods will then fetch the file information from these events.

396
00:25:28,018 --> 00:25:31,910
In the pub subtopic process, the files mask the content if required.

397
00:25:32,250 --> 00:25:35,718
We have very strict slas and scaling of the platform

398
00:25:35,804 --> 00:25:39,574
to meet those slas is a critical part of our architecture.

399
00:25:39,702 --> 00:25:43,386
We are using horizontal pod scaling and cluster auto scaling together to

400
00:25:43,408 --> 00:25:44,380
scale the platform.

401
00:25:47,150 --> 00:25:50,602
A bit more about auto scaling now. Auto scaling is a crucial

402
00:25:50,666 --> 00:25:52,080
part of our platform.

403
00:25:54,210 --> 00:25:59,246
We have to process these files as soon as they arrive so

404
00:25:59,268 --> 00:26:03,350
we can perform the settlement and billing operations and also pay our merchants

405
00:26:03,530 --> 00:26:07,122
and do the reconciliation of the money,

406
00:26:07,256 --> 00:26:11,842
which is very crucial to our business also.

407
00:26:11,896 --> 00:26:14,974
On the other hand, we also wanted to make sure that our infrastructure is cost

408
00:26:15,032 --> 00:26:18,534
effective and we are not running workloads when they are not

409
00:26:18,572 --> 00:26:22,038
needed to be aligned with bit more like a

410
00:26:22,124 --> 00:26:25,718
phenopsy culture. There are

411
00:26:25,724 --> 00:26:28,966
a few challenges we faced when we implemented horizontal port scaling,

412
00:26:28,998 --> 00:26:32,198
setting up resources like on the pods,

413
00:26:32,294 --> 00:26:37,660
that was a bit difficult

414
00:26:38,670 --> 00:26:42,218
to decide what should be the starting request and what should be

415
00:26:42,224 --> 00:26:46,266
the limit. I know there has been a lot of talks

416
00:26:46,298 --> 00:26:49,726
in kubernetes that we don't need to put limits and stuff like that, but in

417
00:26:49,748 --> 00:26:53,006
our case we had to do it because we are scaling so many pods and

418
00:26:53,028 --> 00:26:56,242
pods can consume like a lot of memory and resources. So we

419
00:26:56,296 --> 00:26:59,634
tried a lot of different options when we were

420
00:26:59,672 --> 00:27:03,330
trying this in production. We started hating pagerduty

421
00:27:04,230 --> 00:27:09,720
but eventually leveraging worked out for us and

422
00:27:11,850 --> 00:27:16,150
it's now scaling quite nicely. And there are two types of scaling

423
00:27:17,050 --> 00:27:20,798
available, right? High level two types. We have horizontal

424
00:27:20,834 --> 00:27:24,566
auto scaling which updates a workload with the aim of scaling

425
00:27:24,598 --> 00:27:28,086
the workload to match the demand. It actually means that the response

426
00:27:28,118 --> 00:27:31,386
to increase the load is deploy more pods and if

427
00:27:31,408 --> 00:27:35,146
the load decreases, scale back down the deployment. By removing the scaled

428
00:27:35,178 --> 00:27:39,006
pods. Then you have vertical auto scaling means assigning more

429
00:27:39,028 --> 00:27:42,302
resources, for example memory or cpu, to the pods that are already

430
00:27:42,356 --> 00:27:46,146
running in

431
00:27:46,168 --> 00:27:50,142
the deployment. You can trigger.

432
00:27:50,286 --> 00:27:54,094
There are multiple ways to trigger this auto scaling you can trigger based on resource

433
00:27:54,142 --> 00:27:58,146
usage. For example when a pods given memory or cpu exceeds

434
00:27:58,178 --> 00:28:01,286
a threshold, you can add more pods if you

435
00:28:01,308 --> 00:28:05,074
want to. Then metrics within Kubernetes

436
00:28:05,122 --> 00:28:08,666
any metrics reported by Kubernetes object with the cluster, such as I

437
00:28:08,688 --> 00:28:12,186
don't know, input output rate or

438
00:28:12,208 --> 00:28:16,022
things like that. Then also like metrics coming from external sources

439
00:28:16,086 --> 00:28:20,154
like pub sub. For example you can create an external metrics based

440
00:28:20,352 --> 00:28:24,174
on the size of the queue. Configure the horizontal pod scaler to

441
00:28:24,212 --> 00:28:27,534
automatically increase the number of pods when

442
00:28:27,572 --> 00:28:31,166
the queue size reaches a given threshold and to reduce the

443
00:28:31,188 --> 00:28:34,626
number of pods when the queue size shrinks. This is

444
00:28:34,648 --> 00:28:38,194
exactly what we did and this is exactly we

445
00:28:38,232 --> 00:28:42,466
did and used to scale our platform we

446
00:28:42,488 --> 00:28:46,520
are talking about so far. We talked about HPA and adding more number of

447
00:28:46,890 --> 00:28:49,270
pods to scale the deployment.

448
00:28:50,890 --> 00:28:53,954
But we also need to remember that Kubernetes cluster

449
00:28:54,002 --> 00:28:57,786
also need to increase its capacity. How do we

450
00:28:57,808 --> 00:29:00,966
do it? How do we fit all these scaling

451
00:29:00,998 --> 00:29:05,066
pods into kubernetes? For that we

452
00:29:05,088 --> 00:29:09,110
need to add more nodes and we use Kubernetes cluster

453
00:29:09,190 --> 00:29:10,410
autoscaler.

454
00:29:12,290 --> 00:29:16,074
Kubernetes cluster or autoscaler is a tool that automatically adjusts

455
00:29:16,122 --> 00:29:19,966
the size of kubernetes cluster by scaling up or down by adding or

456
00:29:19,988 --> 00:29:23,470
removing nodes. When one of the following condition is true,

457
00:29:23,620 --> 00:29:26,974
there are pods in the pending state in the cluster

458
00:29:27,022 --> 00:29:30,302
due to insufficient resources. And there are nodes in the cluster

459
00:29:30,366 --> 00:29:34,834
that have been underutilized for an extended period of time and

460
00:29:34,872 --> 00:29:38,710
their pods can be placed on other existing nodes.

461
00:29:39,770 --> 00:29:43,734
One very thing, very important thing to remember is that if your pods have

462
00:29:43,772 --> 00:29:47,880
requested too few resources when it first started,

463
00:29:48,270 --> 00:29:52,634
and after some point your

464
00:29:52,672 --> 00:29:56,314
pod wants more cpu or more memory, but your

465
00:29:56,352 --> 00:29:59,658
node in which your pod is actually running

466
00:29:59,824 --> 00:30:02,350
is experiencing resource shortages.

467
00:30:02,930 --> 00:30:06,462
In this case, cluster autoscaler won't do anything for you.

468
00:30:06,596 --> 00:30:09,834
We actually did not read the documentation properly

469
00:30:09,962 --> 00:30:13,694
and we believe the other way around and we lost good couple of days trying

470
00:30:13,732 --> 00:30:16,690
to figure it out why the processing is very slow.

471
00:30:17,590 --> 00:30:21,380
You'll have to go back and revisit the resources for the ports all the time.

472
00:30:22,630 --> 00:30:26,086
Now we know the HP and CA works together to scale the

473
00:30:26,108 --> 00:30:30,054
platform and

474
00:30:30,092 --> 00:30:33,942
it works hand in hand and our

475
00:30:33,996 --> 00:30:38,074
files processing can become very

476
00:30:38,112 --> 00:30:42,262
scalable all of a sudden because we adopted kubernetes

477
00:30:42,326 --> 00:30:46,300
and all these flavors of scalability with it.

478
00:30:49,870 --> 00:30:53,354
And this is how it actually looks like right now.

479
00:30:53,392 --> 00:30:56,846
If you can see, I'm going to take you back again to the

480
00:30:56,868 --> 00:31:00,526
processing. You have object storage. All the files are landing and landing and

481
00:31:00,548 --> 00:31:03,962
then the queue is becoming, having those events of file creation,

482
00:31:04,026 --> 00:31:07,666
right, and the queue is becoming big and big and big. Now how

483
00:31:07,688 --> 00:31:10,734
do we scale it? We said unact messages meant.

484
00:31:10,782 --> 00:31:14,820
That means this is a pub sub terminology, but that means

485
00:31:15,370 --> 00:31:19,014
not processed events divided by four is equal to number

486
00:31:19,052 --> 00:31:21,350
of pods or number of workers required.

487
00:31:24,810 --> 00:31:28,450
But we still have a maximum

488
00:31:28,530 --> 00:31:32,342
limit as well. For example, if I have 500k files

489
00:31:32,406 --> 00:31:36,540
still needs to be process, I cannot be running, I don't know,

490
00:31:36,990 --> 00:31:40,666
125k pods. So we can

491
00:31:40,688 --> 00:31:44,446
still say okay, maximum 300 or maximum 400 pods can

492
00:31:44,548 --> 00:31:48,106
be running at a certain point in time on this particular cluster.

493
00:31:48,138 --> 00:31:52,394
And that actually works because the file processing is very fast. So within few

494
00:31:52,452 --> 00:31:55,986
seconds it process one file or within a second or

495
00:31:56,008 --> 00:31:56,580
so.

496
00:32:00,070 --> 00:32:04,594
And this is how the non PCI platform looks

497
00:32:04,632 --> 00:32:08,678
like. And sometimes this is the platform where all the

498
00:32:08,844 --> 00:32:13,426
validation, all the schema

499
00:32:13,458 --> 00:32:16,934
contracts, all the monitoring events and everything

500
00:32:17,132 --> 00:32:21,274
kind of is encapsulated into this

501
00:32:21,312 --> 00:32:25,574
group of microservices or collection of tools or infrastructure

502
00:32:25,622 --> 00:32:28,730
as you can say. So all the collector,

503
00:32:29,230 --> 00:32:33,214
it works exactly the same as PCI, just a bit more that it

504
00:32:33,252 --> 00:32:38,026
has some extra flavors of schema history and file

505
00:32:38,058 --> 00:32:41,434
stores and things like that. So all the connectors

506
00:32:41,482 --> 00:32:43,570
send files into the source bucket,

507
00:32:44,310 --> 00:32:48,450
which is non PCI bucket. Then file creation events generate

508
00:32:48,950 --> 00:32:52,386
that file creation event into the topic. Then HPA comes into

509
00:32:52,408 --> 00:32:56,562
the picture it scales the deployment of

510
00:32:56,616 --> 00:33:00,454
these translate parts. We call them translate because they're doing the translation based

511
00:33:00,492 --> 00:33:04,258
on the config provided. And then the CA cluster auto scaler

512
00:33:04,274 --> 00:33:08,274
kicks in to scale the cluster, and then the translate pods actually

513
00:33:08,332 --> 00:33:11,478
parse and translate every single file, every single source file

514
00:33:11,494 --> 00:33:15,082
into chunked, every files. Now translate process completely works

515
00:33:15,136 --> 00:33:18,746
based on what's inside the schema of

516
00:33:18,768 --> 00:33:22,362
the file. And this is where it becomes more like a self

517
00:33:22,416 --> 00:33:27,610
serve data platform. So every single pipeline

518
00:33:27,690 --> 00:33:31,006
belongs to a single domain. Payments have its

519
00:33:31,028 --> 00:33:34,446
own pipeline here, marketing have its own pipeline here.

520
00:33:34,548 --> 00:33:40,162
And every single schema history, if you see there is

521
00:33:40,216 --> 00:33:43,326
like one schema history at the end of the day and there is a UI

522
00:33:43,358 --> 00:33:46,738
on top of it from where users can log in,

523
00:33:46,824 --> 00:33:49,894
go and challenges the schema, they can say okay,

524
00:33:50,012 --> 00:33:53,526
now this file contains an extra column, and I want

525
00:33:53,548 --> 00:33:56,758
to process this extra column, but I don't want to go

526
00:33:56,764 --> 00:34:01,180
to the data team and raise a request and ask them to process this.

527
00:34:01,950 --> 00:34:03,660
I would like to do it by myself.

528
00:34:05,070 --> 00:34:08,618
And this is how it happens. So they go

529
00:34:08,784 --> 00:34:12,362
to this web interface and there's schema version,

530
00:34:12,426 --> 00:34:16,590
schema name, source, blah blah blah, lot of information there.

531
00:34:16,660 --> 00:34:20,080
We are actually trying to make it a bit more nicer now.

532
00:34:20,770 --> 00:34:23,646
We are actually taking away all the configuration out.

533
00:34:23,828 --> 00:34:26,994
Now we have Argo CD workloads and things like that.

534
00:34:27,032 --> 00:34:30,498
So we're taking that all out and we're just leaving the schema bit there.

535
00:34:30,584 --> 00:34:34,594
But the gist here is like the users can

536
00:34:34,632 --> 00:34:38,066
actually, or the domain owners or domain, those teams

537
00:34:38,178 --> 00:34:41,880
can actually manage their own pipelines by themselves. We provide

538
00:34:44,410 --> 00:34:47,922
the data catalog by using data hub, they can discover

539
00:34:48,066 --> 00:34:51,526
everything, what they need to do. That is also an ongoing

540
00:34:51,558 --> 00:34:55,114
project at the moment, but it's very interesting. Maybe someday we'll talk about this more

541
00:34:55,232 --> 00:34:59,158
then we provide monitoring on top of it. The schema registry

542
00:34:59,334 --> 00:35:02,942
also have a component where you can say, you know what,

543
00:35:03,076 --> 00:35:06,606
I want to know when my file arrives and

544
00:35:06,708 --> 00:35:10,362
when my file lands into the bigquery or my data warehouse.

545
00:35:10,426 --> 00:35:13,938
And if that doesn't happen by 09:00 in the morning, I want to

546
00:35:13,944 --> 00:35:17,602
get alerted because my processes are going to fail and

547
00:35:17,656 --> 00:35:20,478
I need to notify downstream stakeholders.

548
00:35:20,654 --> 00:35:24,226
Any other reason? So that's how

549
00:35:24,328 --> 00:35:27,558
the schema registry plays a very key role. And at the end of the day

550
00:35:27,564 --> 00:35:31,206
it's a data contract between the source and the processing and

551
00:35:31,228 --> 00:35:35,174
the target. When we

552
00:35:35,212 --> 00:35:38,378
process files from PCI to non PCI environment with the

553
00:35:38,384 --> 00:35:42,854
help of schema SD, the files moves from many different stages throughout

554
00:35:42,902 --> 00:35:46,374
the whole processing journey and the state management

555
00:35:46,422 --> 00:35:50,442
of the file becomes very important because you

556
00:35:50,496 --> 00:35:53,678
want the file processing to be fault tolerant. You want to

557
00:35:53,684 --> 00:35:57,760
handle errors when the error happens.

558
00:35:58,290 --> 00:36:01,806
You also want to support kind of live monitoring. You also want to

559
00:36:01,828 --> 00:36:05,314
prevent duplicate processing, because queues can

560
00:36:05,352 --> 00:36:10,082
have duplicate events. And you

561
00:36:10,136 --> 00:36:13,170
want to make sure that once the file is processed, is processed.

562
00:36:14,150 --> 00:36:17,590
Now let's see how the flow works. So object file is created,

563
00:36:18,250 --> 00:36:22,054
it goes into the file event, subscription topic, for example.

564
00:36:22,252 --> 00:36:25,750
So then it's kind of in a to do state. And after that

565
00:36:25,820 --> 00:36:29,862
the HPA is listening to that topic,

566
00:36:30,006 --> 00:36:34,410
and then it say, okay, let's scale everything and

567
00:36:34,480 --> 00:36:38,246
all the pod starts consuming from this topic, and they consume

568
00:36:38,278 --> 00:36:41,690
from this topic. They goes first to the store,

569
00:36:41,760 --> 00:36:45,598
file store, or you can call it a data store, a state store,

570
00:36:45,684 --> 00:36:49,198
to check whether the file is already being processed by some other pod or not.

571
00:36:49,284 --> 00:36:53,326
If that's the case, then they skip it. If not,

572
00:36:53,428 --> 00:36:57,106
then they put that status into in progress, and then they start processing it.

573
00:36:57,128 --> 00:37:00,882
And if everything is succeeded, they said, okay, translate is

574
00:37:01,016 --> 00:37:04,594
completed, or translate was started before translate completed without any

575
00:37:04,632 --> 00:37:07,906
error, everybody's happy. And if

576
00:37:07,928 --> 00:37:11,762
that doesn't happen, if there's an error, then they say, okay, there was an error.

577
00:37:11,906 --> 00:37:15,106
And for some transient errors, we can actually resend

578
00:37:15,218 --> 00:37:18,786
the files to be processed again. So they put the status

579
00:37:18,818 --> 00:37:21,466
of the file to to do again so that some other pod can pick it

580
00:37:21,488 --> 00:37:22,060
up.

581
00:37:24,830 --> 00:37:28,790
This is also very useful for monitoring.

582
00:37:28,870 --> 00:37:32,226
So all these events are being sent into metricstore.

583
00:37:32,278 --> 00:37:36,046
And then we have a file monitoring service which actually talk

584
00:37:36,068 --> 00:37:39,854
to the schema registrar, and based on the config provided in those

585
00:37:39,972 --> 00:37:43,890
schemas or feeds, what we call it,

586
00:37:43,960 --> 00:37:48,290
actually aggregate this information and start generating

587
00:37:48,950 --> 00:37:52,834
metrics, report or alerts, or send

588
00:37:52,952 --> 00:37:58,822
more aggregated information to Grafana and

589
00:37:58,876 --> 00:38:01,640
for infrastructure observability. So most,

590
00:38:02,090 --> 00:38:06,006
as I said before, everything what we do and

591
00:38:06,028 --> 00:38:09,946
everything what we run mostly is on kubernetes. So we

592
00:38:09,968 --> 00:38:13,946
are running Prometheus integrations at the moment, getting all the important metrics, such as

593
00:38:14,048 --> 00:38:17,354
resource usage pods, health nodes, health pub sub

594
00:38:17,392 --> 00:38:20,700
queue metrics, et cetera, whatever,

595
00:38:21,150 --> 00:38:24,720
literally to Grafana. And then we have live dashboards running,

596
00:38:25,090 --> 00:38:27,520
which actually reflects the status of the platform.

597
00:38:28,050 --> 00:38:31,178
And the users can actually go and see their own feeds,

598
00:38:31,354 --> 00:38:34,942
their own pipelines, and see if anything

599
00:38:34,996 --> 00:38:39,326
is down or not. And they also get alerted. We also get alerted

600
00:38:39,518 --> 00:38:43,106
because the centralized team own the infrastructure, so they actually come as

601
00:38:43,128 --> 00:38:46,600
a second line support to fix if the issues are happening there.

602
00:38:48,410 --> 00:38:51,590
I'm not going to touch much into the analytics platform,

603
00:38:51,740 --> 00:38:55,750
but analytics platform is made basically

604
00:38:55,900 --> 00:38:59,418
to analyze all the raw data

605
00:38:59,504 --> 00:39:03,206
which is coming into bigquery and then run some DBT

606
00:39:03,238 --> 00:39:06,794
models and then create those drive tables and

607
00:39:06,832 --> 00:39:09,740
then the insights out of it.

608
00:39:10,910 --> 00:39:14,254
Leveraging is based on Kubernetes and then this can be

609
00:39:14,292 --> 00:39:18,154
also not. This can be. This is really this whole deployment

610
00:39:18,202 --> 00:39:21,738
of analytics platform is owned by every single domain. So payments

611
00:39:21,754 --> 00:39:25,026
have its own, marketing have its own, customer has its

612
00:39:25,048 --> 00:39:30,386
own, everybody have their own kind

613
00:39:30,408 --> 00:39:33,650
of analytics platform. This is my

614
00:39:33,800 --> 00:39:38,238
kind of like a showcase end to end file monitoring.

615
00:39:38,334 --> 00:39:41,554
And this is actually the slack message. Look like if you see that stage

616
00:39:41,602 --> 00:39:45,314
one, stage two, stage three, stage four, stage five and see the stage

617
00:39:45,362 --> 00:39:49,318
four is failing and the user can just click on it which file

618
00:39:49,334 --> 00:39:53,034
is failing and then from there we have playbooks and then

619
00:39:53,072 --> 00:39:56,566
we have ways to identify the errors

620
00:39:56,598 --> 00:40:00,726
and ways to fix them as well. This is how the

621
00:40:00,768 --> 00:40:04,702
overall ecosystem of data platform looks like. I just talked

622
00:40:04,756 --> 00:40:08,302
about today the PCI platform and data file processing platform and only

623
00:40:08,356 --> 00:40:13,346
touched the analytics platform, but we have done a

624
00:40:13,368 --> 00:40:16,830
lot of work in streaming side, we have done a lot of work in discovery,

625
00:40:16,910 --> 00:40:19,170
observability, quality, governance,

626
00:40:20,310 --> 00:40:23,794
developer experience and we are still doing a lot

627
00:40:23,832 --> 00:40:27,302
more. And we still have to go a long way to

628
00:40:27,356 --> 00:40:31,670
completely embrace this self serve data platform or data mesh.

629
00:40:32,410 --> 00:40:36,098
And if anybody is interested please join.

630
00:40:36,194 --> 00:40:40,600
Go to this dojo career page, not just data team. We are hiding across

631
00:40:42,090 --> 00:40:45,174
I guess all the functions. And of course thank you for your

632
00:40:45,212 --> 00:40:48,626
time. And if you want to dm me directly

633
00:40:48,658 --> 00:40:51,914
or connect me on LinkedIn, this is my, this is my profile.

634
00:40:52,042 --> 00:40:54,380
Thank you so much, have a good day.

