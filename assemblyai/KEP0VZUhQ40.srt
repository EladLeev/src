1
00:00:00,410 --> 00:00:06,126
Jamaica make up real

2
00:00:06,148 --> 00:00:09,530
time feedback into the behavior of your distributed systems

3
00:00:09,610 --> 00:00:13,374
and observing changes exceptions errors in

4
00:00:13,412 --> 00:00:16,666
real time allows you to not only experiment with confidence,

5
00:00:16,778 --> 00:00:20,480
but respond instantly to get things working again.

6
00:00:24,610 --> 00:01:04,770
Cse hi,

7
00:01:04,840 --> 00:01:08,190
I'm Narmatha Balasundram. I'm a software engineering manager

8
00:01:08,270 --> 00:01:11,694
with a commercial software engineering team at Microsoft.

9
00:01:11,822 --> 00:01:15,422
In today's talk, we are going to see how to increase confidence

10
00:01:15,486 --> 00:01:18,950
as you're doing your chaos testing, and how having

11
00:01:19,020 --> 00:01:22,198
good observability can help take the chaos out of

12
00:01:22,204 --> 00:01:26,114
the chaos testing. So what is chaos engineering? Chaos engineering

13
00:01:26,162 --> 00:01:29,954
is the art of intentionally breaking the system with the sole purpose

14
00:01:30,002 --> 00:01:33,766
of making the system more stable. So in a typical

15
00:01:33,798 --> 00:01:37,002
chaos testing environment, we start with a steady state,

16
00:01:37,136 --> 00:01:40,218
understand how the system behaves during steady state,

17
00:01:40,304 --> 00:01:44,718
and come up with a hypothesis. So if we say the system during

18
00:01:44,804 --> 00:01:48,330
high traffic is supposed to scale, then we do best

19
00:01:48,410 --> 00:01:52,218
to see if the system behaves as we intend it to behave.

20
00:01:52,314 --> 00:01:55,566
And when we find that the system does not behave as we

21
00:01:55,588 --> 00:01:59,490
intended to, then we make changes and then do the testing again.

22
00:01:59,560 --> 00:02:03,070
And this is a very iterative process. So what is observability?

23
00:02:03,230 --> 00:02:07,074
Observability is the ability to answer new questions with existing

24
00:02:07,122 --> 00:02:10,770
data. So having observability while doing chaos testing

25
00:02:10,850 --> 00:02:14,646
helps us understand what the base state is. How does normal look for

26
00:02:14,668 --> 00:02:18,606
the system, and what the deviated state is when there is an unusual

27
00:02:18,658 --> 00:02:22,474
activity or unusual events or response in the system, how does

28
00:02:22,512 --> 00:02:26,726
that change things? The chaos engineering without doing observability

29
00:02:26,838 --> 00:02:29,626
will just lead to chaos. In the talk today,

30
00:02:29,728 --> 00:02:33,530
we'll see how we can take that chaos out of testing observability.

31
00:02:33,690 --> 00:02:37,022
So what is the true observability in the system and

32
00:02:37,076 --> 00:02:40,286
what are the different attributes. Secondly, we'll see what are the

33
00:02:40,308 --> 00:02:44,462
golden signals of monitoring and how we can help have actionable

34
00:02:44,526 --> 00:02:48,482
failures. Number three, we'll see what is service

35
00:02:48,536 --> 00:02:51,246
level agreements and service level objectives,

36
00:02:51,358 --> 00:02:54,974
and how can we use this in our chaos testing to

37
00:02:55,032 --> 00:02:58,866
understand how the system is behaving. And lastly,

38
00:02:58,978 --> 00:03:02,582
monitoring and alerting, identifying based on the service

39
00:03:02,636 --> 00:03:05,862
level objectives, we will see

40
00:03:05,916 --> 00:03:09,058
how monitoring can help during chaos testing

41
00:03:09,154 --> 00:03:12,410
and even before we start the chaos testing process.

42
00:03:12,560 --> 00:03:15,882
So the true observability of the system are the things are

43
00:03:15,936 --> 00:03:19,866
made up of the things, what the different attributes of

44
00:03:19,888 --> 00:03:23,258
the application look like. So we talked about a

45
00:03:23,264 --> 00:03:26,734
little earlier, we talked about what chaos testing is, and it is about

46
00:03:26,852 --> 00:03:30,894
deviating what the normal looks like. And when the testing is run,

47
00:03:30,932 --> 00:03:35,130
what does the deviated state look like. So for us to understand normal,

48
00:03:35,210 --> 00:03:38,706
we need to understand what does the health of the components of the

49
00:03:38,728 --> 00:03:42,258
system look like when there are requests going against a service

50
00:03:42,344 --> 00:03:45,970
and it gives a 200 okay, and that means the service

51
00:03:46,040 --> 00:03:48,950
health of the API is doing well.

52
00:03:49,100 --> 00:03:53,238
And when there are additional stress that is going on in the system,

53
00:03:53,404 --> 00:03:56,962
the resource health of the system could also be constrained.

54
00:03:57,106 --> 00:04:00,938
Things like CPU or disk, iOS or memory for

55
00:04:00,944 --> 00:04:04,038
that instance. That could be a constraint. And additionally,

56
00:04:04,134 --> 00:04:07,974
with these two looking at what the business transaction

57
00:04:08,022 --> 00:04:11,210
KPIs are. So let's say if we are looking

58
00:04:11,280 --> 00:04:15,086
for number of logins per second, or if we are looking for the number of

59
00:04:15,108 --> 00:04:19,182
searches per second, then these are the key business

60
00:04:19,316 --> 00:04:22,618
indicators that we should look for as we are

61
00:04:22,644 --> 00:04:26,434
looking at these data across the different components. Now, all these

62
00:04:26,472 --> 00:04:30,062
data collected by itself, service health, then resource

63
00:04:30,126 --> 00:04:33,918
health and the business transaction KPIs separately,

64
00:04:34,094 --> 00:04:37,542
does not help in giving the holistic view of the system.

65
00:04:37,676 --> 00:04:41,522
Creating a dashboard and having the dashboard represent these values

66
00:04:41,666 --> 00:04:45,782
that the different stakeholders of the business are looking for is

67
00:04:45,836 --> 00:04:49,574
what makes it cohesive. And let's say at the time of

68
00:04:49,612 --> 00:04:52,906
chaos testing. For an SRE engineer coming in,

69
00:04:53,008 --> 00:04:56,506
what are the key metrics that they should be aware of and they need

70
00:04:56,528 --> 00:04:59,610
to look at? Creating a dashboard for that particular

71
00:04:59,680 --> 00:05:02,974
stakeholder or for the SRE teams here makes much

72
00:05:03,012 --> 00:05:06,394
more sense than just throwing everything into a combined dashboard.

73
00:05:06,522 --> 00:05:10,266
Alerts. Alerts are the end result of a dashboard.

74
00:05:10,378 --> 00:05:14,082
So let's say we see a change in the

75
00:05:14,216 --> 00:05:18,414
current state from into an abnormal state, then creating

76
00:05:18,462 --> 00:05:21,854
alerts and identifying the parties that needed to be alerted

77
00:05:21,902 --> 00:05:26,238
on each of these scenarios are very important. We talked about what

78
00:05:26,264 --> 00:05:30,322
these different attributes are, but how do we ensure that these attributes

79
00:05:30,386 --> 00:05:34,198
are what is getting measured? So Google's SRE team

80
00:05:34,284 --> 00:05:38,242
came up with the four golden signals, namely traffic latency

81
00:05:38,306 --> 00:05:41,754
errors and saturation. So let's take the example of a

82
00:05:41,792 --> 00:05:44,634
scenario where there is a high stress on the system.

83
00:05:44,752 --> 00:05:48,682
It could either be because of increased traffic, or it could be because

84
00:05:48,736 --> 00:05:51,994
of the vms being down. So we

85
00:05:52,032 --> 00:05:55,770
start off with a normal tech, normal. The traffic, for instance,

86
00:05:55,930 --> 00:05:59,742
in a normal scenario looks like 200 seconds per request per second,

87
00:05:59,876 --> 00:06:03,034
increasing the traffic during the chaos,

88
00:06:03,082 --> 00:06:06,354
testing to, let's say, 500 requests per second, or even

89
00:06:06,392 --> 00:06:09,570
600 requests per second. How does that affect the latency?

90
00:06:09,990 --> 00:06:13,438
Latency during a normal state looks like 500 milliseconds

91
00:06:13,454 --> 00:06:15,970
per request. With an increased traffic,

92
00:06:16,550 --> 00:06:19,686
how does that deviate from what the normal state looks like?

93
00:06:19,788 --> 00:06:23,174
And how does traffic and latency play

94
00:06:23,212 --> 00:06:27,110
a role in errors? Are you seeing more timeout errors? And because

95
00:06:27,180 --> 00:06:30,070
of the high traffic that's coming in into the system,

96
00:06:30,220 --> 00:06:33,894
are the resources like the cpu, the memory and the disk IO,

97
00:06:33,942 --> 00:06:37,882
are they constrained? So these are the key things to watch out for

98
00:06:38,016 --> 00:06:41,514
as you're looking at the signals that's coming in, in the system.

99
00:06:41,632 --> 00:06:45,054
So looking at the, we talked about the attributes and we talked

100
00:06:45,092 --> 00:06:49,086
about what the golden signals are. How do we identify what these

101
00:06:49,108 --> 00:06:52,698
actionable failures are and what makes a good actionable failure?

102
00:06:52,794 --> 00:06:56,914
The actionable failure is something where the key to

103
00:06:57,032 --> 00:07:00,078
the recovery time is very minimum.

104
00:07:00,174 --> 00:07:03,282
So from the time you identify a problem to the time it gets

105
00:07:03,336 --> 00:07:06,790
recovered needs to be minimum. Meaning any

106
00:07:06,860 --> 00:07:10,610
logs that we collect in the system that contain

107
00:07:10,690 --> 00:07:14,630
information should have enough contextual information in it,

108
00:07:14,700 --> 00:07:17,894
so we get to the problem area faster. Sometimes when

109
00:07:17,932 --> 00:07:21,498
logs are built, and this reminds me of one of the scenarios that I had

110
00:07:21,664 --> 00:07:25,274
in my previous experience, where just building

111
00:07:25,312 --> 00:07:29,114
can observable system meant creating logs, right? We've all done

112
00:07:29,152 --> 00:07:33,578
that, where we just go in and log, and we were feeling pretty confident

113
00:07:33,674 --> 00:07:37,194
that we did all the good things. We had good logs,

114
00:07:37,322 --> 00:07:40,714
we had alert system in place, things seemed

115
00:07:40,762 --> 00:07:44,494
fine. And then we realized, as we started looking at

116
00:07:44,612 --> 00:07:47,706
the production scenarios and production troubleshooting,

117
00:07:47,818 --> 00:07:51,390
that the logs are very atomic and with no correlation

118
00:07:51,470 --> 00:07:54,610
between the logs from different components or having

119
00:07:54,680 --> 00:07:58,502
no contextual data, it took longer for us to identify what

120
00:07:58,556 --> 00:08:02,834
may have caused the issue. And a point for you to remember is logs.

121
00:08:02,962 --> 00:08:06,486
There's a lot of logs, and it's just huge volume of

122
00:08:06,588 --> 00:08:10,262
logs. And as you're thinking about what your observability looks like,

123
00:08:10,316 --> 00:08:14,074
just make sure that these logs are ingested well. And there is

124
00:08:14,112 --> 00:08:17,754
good analytical engines at the back end that can actually help crunch through these

125
00:08:17,792 --> 00:08:21,662
logs and give the data that you're looking for. Next, we look at service level

126
00:08:21,716 --> 00:08:25,066
agreements and objectives. So slas

127
00:08:25,178 --> 00:08:29,246
are service level agreements. They are typically agreements between

128
00:08:29,348 --> 00:08:32,926
two parties about what the services are going to

129
00:08:32,948 --> 00:08:36,754
be and what is the uptime and the response time between the

130
00:08:36,792 --> 00:08:40,062
services look like. For instance, let's say an agreements

131
00:08:40,126 --> 00:08:43,426
between a mapping provider and

132
00:08:43,448 --> 00:08:46,822
let's say a right sharing application. So the

133
00:08:46,876 --> 00:08:50,482
kind of agreement that they would get into would be that the mapping

134
00:08:50,546 --> 00:08:54,854
provider can come up with the agreements that they

135
00:08:54,972 --> 00:08:58,614
say that the maps would be available about

136
00:08:58,812 --> 00:09:02,234
99.99% of the time. And when they make an

137
00:09:02,272 --> 00:09:05,546
API changes, it could be something like they give

138
00:09:05,568 --> 00:09:08,810
a two week notice to the right sharing company.

139
00:09:08,960 --> 00:09:13,360
And this is what is called as an slas. And so when

140
00:09:14,050 --> 00:09:16,814
the mapping provider company takes it back,

141
00:09:16,932 --> 00:09:20,046
they need to understand what do they need

142
00:09:20,068 --> 00:09:23,758
to do to be able to meet the slas. Now, this is

143
00:09:23,764 --> 00:09:26,946
the slO, which is a service level objective. So what

144
00:09:26,968 --> 00:09:30,946
are the objectives that a team must hit to meet the agreement that they

145
00:09:30,968 --> 00:09:33,490
just made with their client?

146
00:09:34,230 --> 00:09:38,166
So this boils down to what are the things that they need to monitor to

147
00:09:38,188 --> 00:09:41,846
be able to meet the agreement that they just had. So we

148
00:09:41,868 --> 00:09:45,458
cannot talk about slas and slos and not talk about error

149
00:09:45,474 --> 00:09:49,290
budgets. So error budget is that maximum amount of time

150
00:09:49,360 --> 00:09:53,226
that a technical system can fail without the

151
00:09:53,328 --> 00:09:57,158
consequences of having any contractual obligations. So let's

152
00:09:57,174 --> 00:10:00,314
say if the agreement is about 99.99%,

153
00:10:00,432 --> 00:10:03,934
the error budget for the applying provider companies is

154
00:10:03,972 --> 00:10:07,406
about 52 minutes per year. So that is

155
00:10:07,428 --> 00:10:10,746
the maximum amount of time that the technical system can fail.

156
00:10:10,858 --> 00:10:14,274
So we'll look at how knowing what these

157
00:10:14,312 --> 00:10:18,062
slas and slos are can help us with our chaos

158
00:10:18,126 --> 00:10:21,666
testing. So number one, it identifies before the

159
00:10:21,688 --> 00:10:24,770
chaos testing even starts. It helps to understand

160
00:10:24,920 --> 00:10:28,934
what does critical issues for the user experience look

161
00:10:28,972 --> 00:10:32,374
like. Let's say if it's a streaming providing company,

162
00:10:32,572 --> 00:10:36,178
provider company, and then they look at there's

163
00:10:36,194 --> 00:10:39,738
a little bit of buffering as the users view the content.

164
00:10:39,904 --> 00:10:43,414
Is this something that needs to be identified as a critical

165
00:10:43,462 --> 00:10:46,886
issue, or is this something that can be, that resolves

166
00:10:46,918 --> 00:10:50,570
by itself? So, knowing what the criticality of the issue

167
00:10:50,640 --> 00:10:54,494
is helps for us to be able to make a decision on

168
00:10:54,532 --> 00:10:58,110
identifying to fix it or not. Let's say if it's an issue where it

169
00:10:58,180 --> 00:11:01,706
is very intermittent and then the reload of the page fixes

170
00:11:01,738 --> 00:11:05,714
it, or it's very short lived. Right? So these are the

171
00:11:05,832 --> 00:11:09,890
scenarios that helps with chaos testing. So during the chaos testing,

172
00:11:12,070 --> 00:11:15,242
we want to be careful when to do the chaos testing.

173
00:11:15,326 --> 00:11:18,966
We do not want to be introducing more uncertainty in

174
00:11:18,988 --> 00:11:22,674
the network when the user experience is deteriorating

175
00:11:22,802 --> 00:11:26,294
or when there's a system performance and things are

176
00:11:26,332 --> 00:11:29,650
being slow, we want to be very informed

177
00:11:29,730 --> 00:11:33,146
about when we want to start doing the chaos testing. And once the

178
00:11:33,168 --> 00:11:37,354
chaos testing starts, we measure how the system is doing with

179
00:11:37,392 --> 00:11:41,562
the chaos and without the chaos and what the difference is. And this helps us

180
00:11:41,616 --> 00:11:45,262
to increase the load in the system, because we are seeing

181
00:11:45,316 --> 00:11:48,702
a real time feedback on how the system is doing

182
00:11:48,756 --> 00:11:52,666
by looking at the signals that we are monitoring using our golden

183
00:11:52,698 --> 00:11:56,354
signals. And then we figure out, is it good to

184
00:11:56,392 --> 00:12:00,178
tune up the traffic or are we hurting the system? And should we

185
00:12:00,184 --> 00:12:03,346
be turning back the traffic? Then comes our monitoring and

186
00:12:03,368 --> 00:12:07,206
alerts. So monitoring and alerts are a

187
00:12:07,228 --> 00:12:10,854
great place to get an overall view of how

188
00:12:10,892 --> 00:12:14,546
we are doing. How are the attributes doing with respect to the golden

189
00:12:14,578 --> 00:12:18,300
signals? And also, when things do go bad,

190
00:12:18,990 --> 00:12:22,550
what do we do with those? So while we are doing the chaos testing,

191
00:12:22,630 --> 00:12:26,794
when the system is bound to

192
00:12:26,832 --> 00:12:30,454
break, bound to deteriorate, we evaluate what are the missing

193
00:12:30,502 --> 00:12:34,158
alarms? Are the alarms even in the right place?

194
00:12:34,324 --> 00:12:38,522
Are we looking at the right things or are we just looking at symptoms

195
00:12:38,666 --> 00:12:42,366
and they are not truly the causes? Are we measuring the right things?

196
00:12:42,468 --> 00:12:45,342
Are we looking at the right latency,

197
00:12:45,486 --> 00:12:49,186
or are we looking at the right error numbers,

198
00:12:49,288 --> 00:12:52,706
for instance? And then once we do that,

199
00:12:52,808 --> 00:12:56,226
then we take a step back and we look at the thresholds

200
00:12:56,258 --> 00:12:59,830
of the alerts. And this is a very key component because

201
00:12:59,980 --> 00:13:03,110
sometimes the alert fatigue,

202
00:13:03,450 --> 00:13:07,074
if the threshold is too low, then that may result in an alert

203
00:13:07,122 --> 00:13:10,790
fatigue. Too many alerts, folks that are responsible

204
00:13:10,870 --> 00:13:14,746
for fixing the alerts may become immune to the

205
00:13:14,768 --> 00:13:18,518
fact that there is an alert. So the threshold of the alerts

206
00:13:18,614 --> 00:13:22,150
are key. And lastly, the alerts need to be

207
00:13:22,160 --> 00:13:26,046
sent to the right team. So we need to identify who owns or who is

208
00:13:26,068 --> 00:13:29,786
responsible for fixing the alerts for each of the segment.

209
00:13:29,898 --> 00:13:34,190
So doing this practice while doing chaos testing

210
00:13:34,350 --> 00:13:38,274
helps us to make sure that all these different things are aligned when we

211
00:13:38,312 --> 00:13:41,650
start seeing things going bad in production.

212
00:13:42,390 --> 00:13:46,342
We've already done this, we've already tested this. We know when

213
00:13:46,396 --> 00:13:49,894
things go bad. How do we identify things

214
00:13:49,932 --> 00:13:53,894
and how do we fix those things? And are these

215
00:13:53,932 --> 00:13:57,720
going to the right folks? As we wrap this up,

216
00:13:58,350 --> 00:14:02,246
here are a few closing thoughts with observability.

217
00:14:02,438 --> 00:14:06,074
I would suggest, depending on where you are

218
00:14:06,272 --> 00:14:10,380
in the observability track, I would say always start small.

219
00:14:11,010 --> 00:14:14,606
Start with auto instrumentation that's available out

220
00:14:14,628 --> 00:14:18,526
of the tool that you're going to use, and start small

221
00:14:18,628 --> 00:14:22,486
and keep adding information on top of it. And in our distributed

222
00:14:22,538 --> 00:14:25,938
environment, like how our tech stack is built,

223
00:14:26,104 --> 00:14:29,490
always have distributed logs that have

224
00:14:29,560 --> 00:14:33,202
information correlated with each other, and there are

225
00:14:33,256 --> 00:14:36,934
sufficient traces so you could track how things are

226
00:14:37,052 --> 00:14:40,486
moving along. And there is enough context for the

227
00:14:40,508 --> 00:14:45,218
logs as you're logging them. And secondly, iterate instrumentation.

228
00:14:45,394 --> 00:14:49,154
It's a rinse and repeat process. And there are things you discover

229
00:14:49,282 --> 00:14:52,280
that needs to be added. And that is all right.

230
00:14:53,530 --> 00:14:57,202
As long as we do these as an iterative

231
00:14:57,266 --> 00:15:00,606
process, we learn stuff. And as we learn stuff, go and make

232
00:15:00,628 --> 00:15:03,834
it better. And lastly, celebrate learnings.

233
00:15:03,882 --> 00:15:07,246
So once you figure out something doesn't work, it's quite all

234
00:15:07,268 --> 00:15:10,510
right. Go back in and fix it again with that.

235
00:15:10,580 --> 00:15:14,506
I just want to say thank you for listening through this presentation

236
00:15:14,618 --> 00:15:18,270
and I'm very happy to be a part of this. Thank you.

237
00:15:18,340 --> 00:15:18,490
Bye.

