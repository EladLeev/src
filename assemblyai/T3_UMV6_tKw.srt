1
00:00:40,930 --> 00:00:44,726
Hi, my name is Dhiraj Hegde and I'm from Salesforce. Today I'll be

2
00:00:44,748 --> 00:00:48,022
talking about how we manage hbase in public cloud.

3
00:00:48,156 --> 00:00:51,406
Hbase is a distribute at key value store that is

4
00:00:51,428 --> 00:00:55,294
horizontally scalable and it runs on top of Hadoop file system. For many

5
00:00:55,332 --> 00:00:59,386
years, Salesforce has been running HBS in its own private data centers.

6
00:00:59,498 --> 00:01:03,614
It's been running a very large footprint of clusters with thousands of machines,

7
00:01:03,742 --> 00:01:06,942
petabytes of data storage and billions

8
00:01:07,006 --> 00:01:10,766
of queries per day. But it had been using very traditional mechanisms

9
00:01:10,798 --> 00:01:14,414
of managing these clusters on bare metal hosts

10
00:01:14,542 --> 00:01:17,550
using open source tools like puppet and ambari.

11
00:01:17,630 --> 00:01:20,598
When we started moving some of our clusters into public cloud,

12
00:01:20,684 --> 00:01:25,186
we decided to take a very different approach, using kubernetes to manage these clusters.

13
00:01:25,298 --> 00:01:28,882
We'll explain in this talk why we chose kubernetes, what challenges

14
00:01:28,946 --> 00:01:32,886
we ran into with our choice, and how we overcame those challenges.

15
00:01:33,078 --> 00:01:36,710
So why did we pick kubernetes? There are a number of reasons.

16
00:01:36,790 --> 00:01:40,534
One of the main problems that we ran into in an old deployment

17
00:01:40,582 --> 00:01:43,318
mechanism was that it was in place in a hosts,

18
00:01:43,414 --> 00:01:46,810
meaning you would go into a host that already had software

19
00:01:46,890 --> 00:01:50,126
installed and try to modify it. The problem with that is that when

20
00:01:50,148 --> 00:01:53,386
you are trying to deploying on thousands of hosts, some of those hosts,

21
00:01:53,418 --> 00:01:56,526
the installation could fail, leaving it in an uncertain

22
00:01:56,558 --> 00:01:59,986
stateful, with some binaries present or some configs present and

23
00:02:00,008 --> 00:02:03,406
others missing. And if you happen to miss these failures, those hosts

24
00:02:03,438 --> 00:02:07,446
would remain in this inconsistent stateful for a very long time. The other thing

25
00:02:07,468 --> 00:02:10,866
we have noticed is there's a temptation when you're dealing

26
00:02:10,898 --> 00:02:14,502
with emergency issues, maybe a site issue that people

27
00:02:14,556 --> 00:02:18,962
just go and log into. Hosts go in and modify these configurations locally

28
00:02:19,026 --> 00:02:22,954
and completely forget that they did that. And the problem there again

29
00:02:22,992 --> 00:02:26,634
is that once the config is forgotten, it looks a little different on that one

30
00:02:26,672 --> 00:02:30,586
host compared to many of the other hosts. With containers. A lot of

31
00:02:30,608 --> 00:02:33,946
these problems go away because they have this thing called immutability,

32
00:02:34,058 --> 00:02:37,774
meaning with a container, it is actually created from

33
00:02:37,812 --> 00:02:41,466
an image, and that image contains all the configuration,

34
00:02:41,578 --> 00:02:45,006
all the binaries already present. And as that

35
00:02:45,028 --> 00:02:48,658
container comes up, it is an exact replica of what that image has.

36
00:02:48,824 --> 00:02:52,706
And if you even attempt to make a change after it's running, what happens is

37
00:02:52,728 --> 00:02:56,814
that the next time the container is restarted could be a crash or a deliberate

38
00:02:56,862 --> 00:03:00,310
restart. It would once again start from the image. So whatever

39
00:03:00,380 --> 00:03:03,814
you made, changes locally would be totally lost. So in that

40
00:03:03,852 --> 00:03:07,670
kind of immutable environment, it is pretty easy to make sure

41
00:03:07,740 --> 00:03:11,510
that over a period of time, the images are all consistent,

42
00:03:11,590 --> 00:03:14,982
no matter what people try to do to it or what failures

43
00:03:15,046 --> 00:03:18,502
happen with it. The second reason why we felt Kubernetes

44
00:03:18,566 --> 00:03:22,626
and containerization was a good thing was availability and scalability.

45
00:03:22,758 --> 00:03:26,222
With Kubernetes, it's really good about making sure

46
00:03:26,276 --> 00:03:30,094
that if you are running your containers on a particular host or

47
00:03:30,132 --> 00:03:34,382
set of hosts, and if something happens to those hosts, kubernetes can monitor

48
00:03:34,446 --> 00:03:37,934
and cache those issues and immediately restart those containers

49
00:03:37,982 --> 00:03:41,300
on a different hosts. By the way, just to be clear here

50
00:03:41,990 --> 00:03:45,026
in Kubernetes, containers are managed by

51
00:03:45,048 --> 00:03:48,642
a construct called pods. But in this talk we are going to treat

52
00:03:48,696 --> 00:03:52,006
the term containers and pods as one and these same to

53
00:03:52,028 --> 00:03:54,806
keep it simple. They are not exactly the same, but we are not going to

54
00:03:54,828 --> 00:03:58,306
get into it. And for the purposes of this talk, it doesn't

55
00:03:58,338 --> 00:04:02,070
really matter as much. So coming back to the question of availability,

56
00:04:02,230 --> 00:04:05,894
it makes sure that when pods fail

57
00:04:05,942 --> 00:04:09,466
on a particular host, it will make sure that

58
00:04:09,488 --> 00:04:12,734
if it can find another healthy host out there, it'll move these pods to those

59
00:04:12,772 --> 00:04:16,234
other hosts. Similarly, when you need to scale

60
00:04:16,282 --> 00:04:19,406
up your application, it's very easy in

61
00:04:19,428 --> 00:04:22,802
Kubernetes to specify a higher number of pods to meet

62
00:04:22,856 --> 00:04:26,930
to the requirements of that particular traffic. And once the traffic

63
00:04:27,270 --> 00:04:31,294
spike is gone, you can also equally

64
00:04:31,342 --> 00:04:34,914
quickly reduce the number of pods that are run to

65
00:04:34,952 --> 00:04:38,354
serve the traffic. So this ability to scale up and down

66
00:04:38,392 --> 00:04:41,430
is very valuable to us, especially when you are moving to the cloud.

67
00:04:41,500 --> 00:04:45,698
And one of the big claims of cloud is that you got this elasticity.

68
00:04:45,794 --> 00:04:48,630
So how can you take advantage of this elasticity?

69
00:04:48,970 --> 00:04:52,554
Using Kubernetes and its elastic management of

70
00:04:52,592 --> 00:04:55,642
pods is one great way of achieving it. The third

71
00:04:55,696 --> 00:04:59,226
reason is actually one of the most important reasons as to why we went with

72
00:04:59,248 --> 00:05:03,054
Kubernetes. We had this desire or goal to make

73
00:05:03,092 --> 00:05:07,226
sure that whatever we built, and we started by these way with AWS,

74
00:05:07,338 --> 00:05:11,182
which is Amazon's cloud offering. But our plan was to try and

75
00:05:11,316 --> 00:05:14,494
build something there that would apply to other clouds,

76
00:05:14,542 --> 00:05:18,482
because it is very likely that we would be running a software in other

77
00:05:18,536 --> 00:05:22,366
clouds. Example of other clouds are things like Azure from Microsoft

78
00:05:22,478 --> 00:05:25,300
or GCP from Google.

79
00:05:25,670 --> 00:05:29,414
So we wanted to be able to run our software in

80
00:05:29,452 --> 00:05:33,094
all those different environments. But the problem there is when

81
00:05:33,132 --> 00:05:37,282
you're building your software deployment processes for one cloud, you are so intimately

82
00:05:37,346 --> 00:05:41,350
tied to the APIs and the way they manage compute, storage network

83
00:05:41,510 --> 00:05:44,970
that whatever you build there doesn't naturally apply to

84
00:05:45,040 --> 00:05:48,602
these other clouds. But fortunately with Kubernetes, things turn

85
00:05:48,656 --> 00:05:52,702
upside down. Kubernetes is actually very opinionated. It actually

86
00:05:52,756 --> 00:05:55,578
specifies exactly how compute should be managed,

87
00:05:55,674 --> 00:05:58,926
exactly how storage should be managed, and how network should be

88
00:05:58,948 --> 00:06:02,758
managed. At that point, it becomes incumbent upon

89
00:06:02,794 --> 00:06:07,214
the other cloud providers to make sure that they manage storage,

90
00:06:07,342 --> 00:06:10,590
network and compute in the way Kubernetes

91
00:06:10,670 --> 00:06:14,462
expects it to be managed. So when you build your software

92
00:06:14,526 --> 00:06:18,022
deployment processes around kubernetes on any one of those cloud

93
00:06:18,076 --> 00:06:22,034
providers, you automatically get the same deployment

94
00:06:22,082 --> 00:06:25,974
processes working in all these other different clouds, because it sort of

95
00:06:26,012 --> 00:06:29,426
enforces that sort of behavior. So for those three reasons,

96
00:06:29,458 --> 00:06:32,730
we found Kubernetes to be very interesting to us. Okay,

97
00:06:32,800 --> 00:06:36,042
now we'll get into a little bit about some of the challenges that we had

98
00:06:36,096 --> 00:06:39,626
with Kubernetes once we started using it. To understand it a little

99
00:06:39,648 --> 00:06:43,834
better, you would need to know the difference between stateful and stateless applications.

100
00:06:43,962 --> 00:06:46,750
Here we show you a typical stateless application,

101
00:06:46,900 --> 00:06:50,414
basically HTTP servers that run in a website. Each one of these

102
00:06:50,452 --> 00:06:54,046
server instances has nothing that is unique in them. They kind

103
00:06:54,068 --> 00:06:57,682
of serve the same content. If you ever try to make a request to them

104
00:06:57,736 --> 00:07:01,374
that changes the content, it usually goes to some back end database

105
00:07:01,422 --> 00:07:04,594
that is shared across all those instances, or maybe even a

106
00:07:04,632 --> 00:07:08,534
shared file system across all these instances. So essentially each

107
00:07:08,572 --> 00:07:12,162
one of those running servers in Kubernetes

108
00:07:12,226 --> 00:07:15,186
world, these would be pods. They're all stateless.

109
00:07:15,298 --> 00:07:18,870
And when a client is trying to access the service

110
00:07:18,940 --> 00:07:22,202
from them, they typically go to a load balancer, as you can see here,

111
00:07:22,336 --> 00:07:25,462
and the load balancer then forwards those requests,

112
00:07:25,526 --> 00:07:29,690
each or any one of those HTTP servers in the back end. Now the interesting

113
00:07:29,760 --> 00:07:33,454
thing to note here is that the client only needs to know the hostname and

114
00:07:33,492 --> 00:07:36,926
IP address of the load balancer. It doesn't even need to know the

115
00:07:36,948 --> 00:07:40,142
host names of the HTTP servers running behind it,

116
00:07:40,196 --> 00:07:44,014
because they are just getting traffic forwarded to them by the cloud balancer.

117
00:07:44,062 --> 00:07:47,442
Kubernetes usually manages these kinds of environments by

118
00:07:47,496 --> 00:07:51,006
specifying a manifest, which is really a document

119
00:07:51,198 --> 00:07:55,026
describing how many instances of these HTTP servers to

120
00:07:55,048 --> 00:07:58,786
run as pods. It can also describe the configuration of

121
00:07:58,808 --> 00:08:02,646
the cloud balancer, which is called a service in Kubernetes, but again it

122
00:08:02,668 --> 00:08:06,326
is specified using a document called a manifest. And with that you can

123
00:08:06,348 --> 00:08:10,342
nicely set up all of this. And in the early years, this is what Kubernetes

124
00:08:10,406 --> 00:08:14,154
was really known for, the ability to very quickly spin up

125
00:08:14,272 --> 00:08:17,866
a set of stateless applications and provide some

126
00:08:17,888 --> 00:08:22,374
sort of service. But when you look at something like Hadoop

127
00:08:22,422 --> 00:08:25,998
hbase, which is a very stateful application, it's a

128
00:08:26,004 --> 00:08:29,562
database. Typically the way it behaves is that you got a client

129
00:08:29,626 --> 00:08:33,418
there. And when it needs to access data for reasons

130
00:08:33,434 --> 00:08:36,542
of reading, for querying, that is for modification,

131
00:08:36,686 --> 00:08:40,002
for deletion, so on and so forth, it needs access

132
00:08:40,056 --> 00:08:42,660
to the data servers which you can see at the bottom,

133
00:08:43,430 --> 00:08:46,734
lined up at the bottom, a number of these data servers.

134
00:08:46,862 --> 00:08:50,166
And each one of those data servers has very different data in

135
00:08:50,188 --> 00:08:53,814
them. It typically does not have the same data across all of them

136
00:08:53,852 --> 00:08:57,458
because then every data server would have to have huge amount of storage.

137
00:08:57,554 --> 00:09:00,822
Instead you break it up into pieces and you spread it across a large

138
00:09:00,876 --> 00:09:04,282
number of data servers. And that's how you scale as you need to store

139
00:09:04,336 --> 00:09:07,866
more data. You just add more data servers. And on the left hand side you

140
00:09:07,888 --> 00:09:11,702
typically have something called a metadata server which is responsible

141
00:09:11,766 --> 00:09:15,550
for knowing where all this data is present. It basically knows the

142
00:09:15,620 --> 00:09:18,974
geography of things. So a client, when it is

143
00:09:19,012 --> 00:09:22,570
trying to access data, it goes to the metadata server

144
00:09:22,650 --> 00:09:26,606
with a key saying that I want to access this. The metadata

145
00:09:26,638 --> 00:09:29,986
server then gives a location of where these can find the data.

146
00:09:30,168 --> 00:09:33,794
And then the client directly goes to the data server based

147
00:09:33,832 --> 00:09:37,646
on these information and accesses the data. Now in all of this, what you will

148
00:09:37,688 --> 00:09:41,170
notice is that the client actually needs to know the identity,

149
00:09:41,250 --> 00:09:44,598
the hostname of all the elements. I mean it needs to

150
00:09:44,604 --> 00:09:48,634
know the host names of the metadata server that can provide this

151
00:09:48,672 --> 00:09:52,570
information. And once a metadata server provides the hostname information

152
00:09:52,640 --> 00:09:55,866
of the data server, it needs to directly deal with those

153
00:09:55,968 --> 00:10:00,038
particular data servers. There's no magical load balancer

154
00:10:00,134 --> 00:10:03,738
hiding things from you, which is why the DNS that you see on

155
00:10:03,744 --> 00:10:07,150
the right hand top corner is very important because it has to have all these

156
00:10:07,220 --> 00:10:09,950
information of the host names and IP addresses.

157
00:10:10,290 --> 00:10:13,790
And by the way, all the clients usually have a cache

158
00:10:13,870 --> 00:10:17,614
because once you discover this information, having to rediscover

159
00:10:17,662 --> 00:10:20,926
this information every time for a request is very inefficient.

160
00:10:21,118 --> 00:10:25,394
So you hbase to kind of cache this information and

161
00:10:25,592 --> 00:10:29,254
hopefully the stuff that you cache doesn't change too often because that is

162
00:10:29,292 --> 00:10:31,926
a little bit of a disturbance that the client had to deal with. So you

163
00:10:31,948 --> 00:10:35,606
try to minimize this disturbance. So all of this makes it

164
00:10:35,628 --> 00:10:39,194
very important that the identity of

165
00:10:39,232 --> 00:10:42,906
the servers that you're accessing tend to be

166
00:10:42,928 --> 00:10:46,694
stable. They do not change very much. And given a host

167
00:10:46,742 --> 00:10:50,154
name, the location of what it contains, it becomes very

168
00:10:50,192 --> 00:10:53,726
important. Took so that association between the name of the

169
00:10:53,748 --> 00:10:57,374
server and its content, the state it contains is very,

170
00:10:57,412 --> 00:11:01,454
very important to the client for good performance. And this

171
00:11:01,492 --> 00:11:05,106
is a very good example of what a stateful application is, and you can

172
00:11:05,128 --> 00:11:08,306
see how it's a little different from stateless applications. So the

173
00:11:08,328 --> 00:11:11,522
Kubernetes community, when it decided to support use

174
00:11:11,576 --> 00:11:14,942
cases like Hadoop, hbase or Cassandra,

175
00:11:15,086 --> 00:11:18,414
they introduced a feature called stateful.

176
00:11:18,542 --> 00:11:22,022
What stateful provides is these same ability to create

177
00:11:22,076 --> 00:11:25,910
pods, but in this case, when it creates pods, it gives it

178
00:11:25,980 --> 00:11:29,770
unique names. And the names are kind of easy to guess.

179
00:11:29,840 --> 00:11:32,954
They usually begin with the same prefix. In this case

180
00:11:32,992 --> 00:11:36,266
it was pod. As an example I took. It can

181
00:11:36,288 --> 00:11:39,514
even be HTTP or hadoop or whatever,

182
00:11:39,712 --> 00:11:43,226
but then it would associate with each instance of the pod

183
00:11:43,338 --> 00:11:46,654
a unique number, like zero, one, two,

184
00:11:46,772 --> 00:11:50,126
depending upon how many of these you want. So each pod would

185
00:11:50,148 --> 00:11:53,802
have a unique name. And also you can see in the right hand top corner

186
00:11:53,866 --> 00:11:57,426
that the DNS is also modified to give these pods a

187
00:11:57,448 --> 00:12:01,026
proper hostname and IP address. So you got everything with

188
00:12:01,048 --> 00:12:04,526
it. You got unique names, hostnames and IP address associated

189
00:12:04,558 --> 00:12:08,520
with each one of these pods. In addition, since this is a stateful application,

190
00:12:08,970 --> 00:12:13,474
you could also define how much storage you want to associate

191
00:12:13,602 --> 00:12:16,662
with each one of these pods, the size of it,

192
00:12:16,716 --> 00:12:21,210
also the class, whether you want SSD or HDD. All of this can be specified

193
00:12:21,550 --> 00:12:25,382
using a construct called a persistent volume claim.

194
00:12:25,526 --> 00:12:29,286
It's basically a claim for storage. It's not the actual storage you are requesting

195
00:12:29,318 --> 00:12:33,578
storage. And this is embedded in each one of these pod definitions

196
00:12:33,754 --> 00:12:37,530
where a pv claim is specified. When this is defined,

197
00:12:37,610 --> 00:12:41,454
what happens is that the providers or cloud providers who run

198
00:12:41,492 --> 00:12:45,018
Kubernetes like AWS, Azure or GCP,

199
00:12:45,194 --> 00:12:49,458
they will notice this claim and these immediately carve out a disk in the cloud

200
00:12:49,624 --> 00:12:53,150
which has these size and the class of storage that is being requested,

201
00:12:53,230 --> 00:12:57,490
and then it is made available to Kubernetes. Kubernetes then mounts that

202
00:12:57,640 --> 00:13:01,794
disk in each one of these pods so that it becomes a locally available storage

203
00:13:01,842 --> 00:13:05,254
in each one of those pods. And at that point

204
00:13:05,292 --> 00:13:08,578
you've got a unique name which is well defined in DNS

205
00:13:08,754 --> 00:13:12,486
associated with storage. And this is a one to one mapping

206
00:13:12,518 --> 00:13:16,426
between the two. Now what's interesting to note here is that

207
00:13:16,528 --> 00:13:20,586
let's take an example of any one of these pods which is

208
00:13:20,608 --> 00:13:24,154
running on host a right now. It has a claim and it is accessing pv

209
00:13:24,202 --> 00:13:28,202
zero which is mounted as a disk in it. Let's say for some reason host

210
00:13:28,266 --> 00:13:31,002
a has some problem, it goes up in smoke.

211
00:13:31,146 --> 00:13:34,414
Kubernetes would notice that. And it

212
00:13:34,452 --> 00:13:38,034
will then say that okay, this pod is gone, it'll remove it from

213
00:13:38,072 --> 00:13:41,634
its system and you'll notice in the right hand top corner that the DNS also

214
00:13:41,672 --> 00:13:45,490
is modified to remove any DNS entries related to it.

215
00:13:45,640 --> 00:13:49,586
You can still see the storage is present because claim that created

216
00:13:49,618 --> 00:13:53,190
the storage is still present. These pod is gone but the claim is still there.

217
00:13:53,340 --> 00:13:56,582
Eventually what Kubernetes will do is it will find another free

218
00:13:56,636 --> 00:14:00,566
host like host d in this example and recreate

219
00:14:00,598 --> 00:14:04,202
the same pod so it has the same hostname. Pod zero,

220
00:14:04,336 --> 00:14:07,850
the one that got destroyed, is recreated with the same hostname.

221
00:14:08,670 --> 00:14:12,558
And because it has the same claim embedded inside of it,

222
00:14:12,644 --> 00:14:16,350
these same storage is again associated with it. And even

223
00:14:16,420 --> 00:14:19,918
DNS is updated to have the DNS record.

224
00:14:20,084 --> 00:14:23,694
One thing that is different here is that when the DNS record is

225
00:14:23,732 --> 00:14:26,830
recreated, it did not get the same IP address.

226
00:14:26,900 --> 00:14:30,434
So it had the same hostname, but the IP address had to change then.

227
00:14:30,472 --> 00:14:34,670
That's just the nature of networking. When you move from one compute

228
00:14:34,830 --> 00:14:38,134
unit to another, the IP addresses that

229
00:14:38,172 --> 00:14:41,638
you associate with that compute unit has to change.

230
00:14:41,724 --> 00:14:45,538
It's just how network is managed in kubernetes, but otherwise

231
00:14:45,634 --> 00:14:49,254
you basically achieve something quite interesting, which is a

232
00:14:49,292 --> 00:14:52,742
given host name is always associated with the same volume

233
00:14:52,806 --> 00:14:57,222
no matter where your compute goes, moves around inside the Kubernetes

234
00:14:57,286 --> 00:14:59,898
clusters a very interesting and useful property.

235
00:15:00,064 --> 00:15:03,338
That stickiness between hostname and the

236
00:15:03,504 --> 00:15:07,562
volume that you use. You also notice that the IP addresses

237
00:15:07,706 --> 00:15:11,294
can change even if the hostname doesn't change. And this is kind of important because

238
00:15:11,332 --> 00:15:14,402
we'll get into some of the issues we have because of this a few minutes

239
00:15:14,456 --> 00:15:18,386
from now. So using stateful set we were able

240
00:15:18,408 --> 00:15:22,226
to deploy Hadoop hbase and going back to

241
00:15:22,248 --> 00:15:25,714
the same slide that I showed you a while back, you can see that each

242
00:15:25,752 --> 00:15:29,382
one of the data servers is being deployed as a stateful set.

243
00:15:29,436 --> 00:15:34,774
And every one of them has a unique name, like DS 123-4123

244
00:15:34,812 --> 00:15:38,214
rather. And similarly the metadata server, which is also

245
00:15:38,252 --> 00:15:42,166
stateful, has a unique name and disks associated

246
00:15:42,198 --> 00:15:45,702
with it. So we were able to model and deploy a software

247
00:15:45,766 --> 00:15:49,046
using stateful set pretty well. Stateful sets are managed

248
00:15:49,078 --> 00:15:52,842
by a controller called as the stateful set controller or the STS

249
00:15:52,906 --> 00:15:56,350
controller in Kubernetes. And while we found

250
00:15:56,420 --> 00:16:00,138
many of its features around managing compute storage

251
00:16:00,234 --> 00:16:03,790
failover et centers very useful, we also

252
00:16:03,860 --> 00:16:07,026
had some challenges with it. One area where there

253
00:16:07,048 --> 00:16:10,926
was a problem was with its rolling upgrade process where you're trying to upgrade

254
00:16:10,958 --> 00:16:14,626
the software. And the way it does upgrade is it starts with the

255
00:16:14,648 --> 00:16:17,990
pod with the highest number and goes one by one,

256
00:16:18,060 --> 00:16:21,430
upgrading each one of them in

257
00:16:21,500 --> 00:16:25,254
strict order all the way down to zero. And while this is a very

258
00:16:25,292 --> 00:16:27,990
nice and careful way of upgrading software.

259
00:16:28,910 --> 00:16:32,918
It is also very slow. You can imagine

260
00:16:33,014 --> 00:16:35,910
in the world of Hadoop hbase,

261
00:16:36,070 --> 00:16:39,498
you got hundreds of these pods, and each one of

262
00:16:39,504 --> 00:16:43,006
them is a heavy server that takes around five minutes to boot up and

263
00:16:43,028 --> 00:16:45,802
initialize and set up its security credentials,

264
00:16:45,866 --> 00:16:49,262
cordless kerberos, key tabs, et cetera. So going

265
00:16:49,316 --> 00:16:53,682
through it one by one would take a very long time and

266
00:16:53,736 --> 00:16:57,470
almost make it impractical for us to use such a valuable feature.

267
00:16:57,630 --> 00:17:00,750
Fortunately, Kubernetes is also very extensible,

268
00:17:00,830 --> 00:17:04,718
so you can kind of go in and modify behavior

269
00:17:04,814 --> 00:17:08,630
or introduce new behavior by providing your own controllers in certain

270
00:17:08,700 --> 00:17:12,294
areas. And in this particular case, we were able to

271
00:17:12,332 --> 00:17:15,670
build a new controller, which we call the custom controller,

272
00:17:16,010 --> 00:17:19,370
which actually works in communication with the

273
00:17:19,440 --> 00:17:23,274
default stateful set controller. So the stateful set

274
00:17:23,312 --> 00:17:27,222
controller would continue to create pods and create storage

275
00:17:27,286 --> 00:17:30,746
and coordinate the mounting and all of

276
00:17:30,768 --> 00:17:34,462
that, whereas the custom controller that we built would be

277
00:17:34,516 --> 00:17:37,806
in charge of deciding which pods would be

278
00:17:37,828 --> 00:17:41,354
deleted next in order to be replaced. So the deletion

279
00:17:41,402 --> 00:17:45,006
would be the custom controller's job and rest of

280
00:17:45,028 --> 00:17:48,834
it would be these existing stateful set controller's job. So once

281
00:17:48,872 --> 00:17:52,594
we had this ability, and this is enabled by a flag called on

282
00:17:52,632 --> 00:17:56,180
delete strategy in stateful, if you're interested in looking it up,

283
00:17:57,030 --> 00:18:00,486
basically, these custom controller would then enable batching where

284
00:18:00,508 --> 00:18:04,070
it would go after a batch of pods, delete them first,

285
00:18:04,220 --> 00:18:07,506
and then the stateful controller would notice that these pods

286
00:18:07,538 --> 00:18:10,982
are missing and would recreate them with the new configuration,

287
00:18:11,046 --> 00:18:14,614
though. Similarly, the custom controller would then move to the next batch

288
00:18:14,662 --> 00:18:18,566
of three, in this case, delete them, and stateful

289
00:18:18,598 --> 00:18:21,994
would do the remaining part of bringing up these new ones.

290
00:18:22,112 --> 00:18:25,882
So in this manner, by coordinating with these existing behavior,

291
00:18:25,946 --> 00:18:29,658
we were able to get batch upgrades enabled in Kubernetes,

292
00:18:29,834 --> 00:18:32,960
which is a very big problem when we initially faced it.

293
00:18:33,890 --> 00:18:37,626
Another limitation that we had was that in Kubernetes,

294
00:18:37,818 --> 00:18:41,426
when you are deploying your services, you also define what is called

295
00:18:41,448 --> 00:18:45,282
as the pod disruption budget. This is important to make sure that

296
00:18:45,336 --> 00:18:48,806
whatever operations you do in your cluster, you don't let the number of

297
00:18:48,828 --> 00:18:52,374
unhealthy pods or disrupted pods. To use

298
00:18:52,412 --> 00:18:55,986
that terminology, you make sure that you put a limit on how many pods

299
00:18:56,018 --> 00:18:59,206
are disrupted. In this case, for example, let's consider that

300
00:18:59,228 --> 00:19:02,346
the pod disruption budget is one. What you're saying is that at

301
00:19:02,368 --> 00:19:06,182
any given time in your cluster, you'd at most disrupt one pod

302
00:19:06,326 --> 00:19:09,894
and not more than that when you're doing any of your administrative tasks.

303
00:19:09,942 --> 00:19:13,466
Now, the problem here is that if more than one of your pods

304
00:19:13,498 --> 00:19:17,358
is unhealthy, in this case, pod three and pod one are in an unhealthy state

305
00:19:17,444 --> 00:19:20,542
because of some issue with them, and you are trying to

306
00:19:20,596 --> 00:19:24,110
upgrade that particular stateful set,

307
00:19:24,260 --> 00:19:27,714
maybe because you want to fix the issue by deploying new code.

308
00:19:27,832 --> 00:19:31,010
Unfortunately, since it always starts with the highest number,

309
00:19:31,080 --> 00:19:32,980
pod five in this case,

310
00:19:33,910 --> 00:19:37,026
when it tries to upgrade, Kubernetes will prevent it

311
00:19:37,048 --> 00:19:40,214
from being upgraded because it would increase the number

312
00:19:40,252 --> 00:19:44,118
of unhealthy pods, because you hbase to destroy a healthy pod to create a

313
00:19:44,124 --> 00:19:47,382
new one, and it bold increase the number of unhealthy pods as a result.

314
00:19:47,516 --> 00:19:50,950
So in this case, again, a custom controller is really useful.

315
00:19:51,030 --> 00:19:54,666
What it did was it went after the unhealthy pods first while

316
00:19:54,688 --> 00:19:58,374
doing upgrades, instead of just being according to a strict ordering,

317
00:19:58,502 --> 00:20:01,706
delete the first unhealthy pod and replace it with

318
00:20:01,728 --> 00:20:05,058
the healthy pod as a result, and then go after the next unhealthy pod,

319
00:20:05,094 --> 00:20:08,346
replace it with these new one, and then finally go to the healthy

320
00:20:08,378 --> 00:20:11,854
pods which can now be replaced because there are no unhealthy pods left.

321
00:20:11,892 --> 00:20:15,454
So in this way, we were able to overcome any blockage

322
00:20:15,502 --> 00:20:19,540
due to pod disruption budget and move the rolling upgrade forward.

323
00:20:20,230 --> 00:20:24,020
Another interesting problem we had, which is kind of unique to

324
00:20:24,790 --> 00:20:28,658
stateful applications, I guess, especially things like Zookeeper

325
00:20:28,754 --> 00:20:32,326
and many other such services. You have a number of

326
00:20:32,348 --> 00:20:35,750
instances, but one of them is elected a leader

327
00:20:36,410 --> 00:20:39,686
and it's the leader of the group, and it has certain responsibilities as

328
00:20:39,708 --> 00:20:43,146
a result. And to create a leader, you have to go through an

329
00:20:43,168 --> 00:20:46,726
election process. So there is some activity and delay involved

330
00:20:46,758 --> 00:20:50,650
in doing some of these things. Unfortunately, Kubernetes at its level

331
00:20:50,720 --> 00:20:54,366
knows nothing about these leader business. So the

332
00:20:54,388 --> 00:20:57,774
controller would typically just go after the highest number of

333
00:20:57,812 --> 00:21:01,086
pod, and if that pod is disrupted and a new one

334
00:21:01,108 --> 00:21:05,194
is created, the leader might be reelected into one of the older pods.

335
00:21:05,322 --> 00:21:08,786
So the next upgrade would hit that leader again, and once again you would have

336
00:21:08,808 --> 00:21:12,306
election. And if you're really unlucky, the third pod also would be from

337
00:21:12,328 --> 00:21:15,598
the older set of pods. So you end up disrupting the

338
00:21:15,624 --> 00:21:19,750
leader these times in this case. But you can imagine in a real cluster,

339
00:21:20,090 --> 00:21:23,698
this repeated leader election bold be very disruptive

340
00:21:23,794 --> 00:21:27,254
to the cluster. So to avoid this,

341
00:21:27,292 --> 00:21:30,406
once again, the custom controller came to a rescue.

342
00:21:30,598 --> 00:21:34,646
We built sidecar containers. These are basically logic

343
00:21:34,678 --> 00:21:38,678
that runs inside each one of these pods, which checks

344
00:21:38,694 --> 00:21:42,922
to see if it's a leader, and it makes that information available through

345
00:21:42,976 --> 00:21:47,066
labels in the pod. And the custom controller is basically monitoring

346
00:21:47,098 --> 00:21:50,398
all these pods to see which one of them has this leader label on it.

347
00:21:50,484 --> 00:21:53,834
And it would then avoid that particular leader and update

348
00:21:53,882 --> 00:21:57,406
all the other pods first, then finally go and update the leader.

349
00:21:57,438 --> 00:22:00,930
So you end up disrupting the leader pod only once

350
00:22:01,000 --> 00:22:04,514
throughout this process, which was a nice capability that we could

351
00:22:04,552 --> 00:22:08,582
have thanks to this custom controller. So another area

352
00:22:08,636 --> 00:22:12,070
of problems that we experienced was around DNS.

353
00:22:12,890 --> 00:22:16,162
As you can imagine in kubernetes,

354
00:22:16,226 --> 00:22:19,814
it's a very dynamic world. As pods move from one hosts

355
00:22:19,862 --> 00:22:23,526
to another, even though they keep the same hosts names, the IP

356
00:22:23,558 --> 00:22:26,730
addresses keep changing. And I kind of went over that earlier.

357
00:22:27,310 --> 00:22:30,538
This creates a strange problem because

358
00:22:30,704 --> 00:22:33,818
traditional software like hbase, Hadoop file

359
00:22:33,834 --> 00:22:37,534
system, et cetera, they were largely developed in an environment where

360
00:22:37,572 --> 00:22:40,830
DNS did not change so much. So as a result,

361
00:22:40,900 --> 00:22:44,466
there was a lot of bugs in this code base where

362
00:22:44,648 --> 00:22:48,146
it would resolve DNS hostname to IP address and

363
00:22:48,168 --> 00:22:52,082
cache that information for literally forever in its

364
00:22:52,136 --> 00:22:55,300
code. So you can imagine if you had that kind of code,

365
00:22:56,170 --> 00:23:00,214
you would have invalid information in the software pretty quickly.

366
00:23:00,412 --> 00:23:04,466
And in particular, what we noticed is that if the metadata

367
00:23:04,498 --> 00:23:08,310
servers had these IP addresses changing

368
00:23:08,730 --> 00:23:12,266
and if a large number of data servers sort of had to

369
00:23:12,288 --> 00:23:15,926
talk to these metadata servers, they were kind of losing connection

370
00:23:15,958 --> 00:23:19,078
to this metadata server as its ip address changed.

371
00:23:19,254 --> 00:23:22,814
Now obviously the fix to this kind of problem is to go into

372
00:23:22,852 --> 00:23:25,854
the open source code, find where all these bugs are.

373
00:23:25,892 --> 00:23:29,902
These it is holding on to these addresses and fix

374
00:23:29,956 --> 00:23:33,726
those bugs. But with a large code base like

375
00:23:33,748 --> 00:23:36,898
Hadoop file system and hbase, it's kind

376
00:23:36,904 --> 00:23:40,802
of challenging to find all the places that this issue exists. And especially

377
00:23:40,856 --> 00:23:45,106
when we had to get our software out and very sort

378
00:23:45,128 --> 00:23:48,710
of depending upon our eyeballing capabilities to find

379
00:23:48,860 --> 00:23:52,486
all these issues or a testing test matrix to

380
00:23:52,508 --> 00:23:55,842
find all these issues seemed a little risky.

381
00:23:55,986 --> 00:23:59,606
So what we ended up doing was that even as we went about fixing

382
00:23:59,638 --> 00:24:03,562
these bugs, we came up with a solution where for

383
00:24:03,616 --> 00:24:07,062
each one of our pods we put a load balancer

384
00:24:07,126 --> 00:24:11,014
and it's called a service in Kubernetes. And it's actually not a physical

385
00:24:11,062 --> 00:24:14,282
load balancer, it's a virtual one which works using network

386
00:24:14,346 --> 00:24:18,254
magic, really there's no physical load balancer involved. So we

387
00:24:18,292 --> 00:24:21,694
created this virtual load balancer in front of each one of

388
00:24:21,732 --> 00:24:25,054
these metadata server instances. So now

389
00:24:25,092 --> 00:24:28,226
what that does is that when you create a load balancer, not only does it

390
00:24:28,248 --> 00:24:32,114
get a host name but also an IP address. And that IP address is

391
00:24:32,152 --> 00:24:35,538
very static in nature. It doesn't change as long as you don't delete the

392
00:24:35,544 --> 00:24:39,286
load balancer. So even though your pods may be changing the

393
00:24:39,308 --> 00:24:43,174
IP addresses, the load balancer does not. So when

394
00:24:43,212 --> 00:24:46,306
the client is trying to contact these pod, it would first go to the load

395
00:24:46,338 --> 00:24:50,006
balancer and then the load balancer would forward the request to the pod.

396
00:24:50,118 --> 00:24:54,694
So we sort of recreated that stateless applications methodology,

397
00:24:54,822 --> 00:24:58,586
at least for metadata servers, so that we

398
00:24:58,608 --> 00:25:02,126
can kind of protect ourselves from IP address related issues.

399
00:25:02,308 --> 00:25:05,786
And in the meantime we also went about finding

400
00:25:05,818 --> 00:25:09,594
all these bugs using various testing mechanisms and eliminating

401
00:25:09,642 --> 00:25:13,618
it. But it gave us some breathing time. Another interesting

402
00:25:13,704 --> 00:25:17,218
issue that you have with kubernetes is how DNS actually

403
00:25:17,304 --> 00:25:21,374
works inside it. There's actually a dedicated DNS server

404
00:25:21,422 --> 00:25:24,974
that is providing all this support for the changing IP

405
00:25:25,022 --> 00:25:28,678
addresses. It's called core DNS. It actually runs inside the

406
00:25:28,684 --> 00:25:32,194
Kubernetes cluster. And as you create pods of various

407
00:25:32,242 --> 00:25:35,414
type and delete it, this core DNS is the one

408
00:25:35,452 --> 00:25:39,026
which keeps track of when to create a DNS record and when to delete

409
00:25:39,058 --> 00:25:42,762
it. The problem with this approach is that while it all works

410
00:25:42,816 --> 00:25:46,426
great on the server side, there's no guarantee that your clients are

411
00:25:46,448 --> 00:25:50,042
actually running in the same Kubernetes cluster. Really in the real

412
00:25:50,096 --> 00:25:53,674
world your client is typically outside of a Kubernetes cluster.

413
00:25:53,722 --> 00:25:57,482
It's probably running in some external host or VM,

414
00:25:57,626 --> 00:26:00,734
but not necessarily inside your Kubernetes cluster. And that

415
00:26:00,772 --> 00:26:04,366
client is actually depending upon typically a different DNS server,

416
00:26:04,398 --> 00:26:08,786
which is the global DNS server that is visible across a

417
00:26:08,808 --> 00:26:12,226
large number of environments and not the core DNS, which is

418
00:26:12,248 --> 00:26:15,954
visible only inside the Kubernetes cluster. So to deal

419
00:26:15,992 --> 00:26:19,766
with this issue, what you have to do is find a way of getting your

420
00:26:19,788 --> 00:26:22,850
records from the core DNS into the global DNS.

421
00:26:23,010 --> 00:26:26,294
Otherwise your client would not know how to contact all

422
00:26:26,332 --> 00:26:29,894
your services. So in our case we use an open source

423
00:26:29,942 --> 00:26:33,718
tool called external DNS. It's something that is open source

424
00:26:33,734 --> 00:26:37,498
and most people use it when they're trying to deal with

425
00:26:37,504 --> 00:26:40,826
this particular scenario. And what external DNS does is

426
00:26:40,848 --> 00:26:45,162
that it transfers these DNS records that are within the Kubernetes cluster

427
00:26:45,306 --> 00:26:49,386
into this global DNS server. I've simplified the picture here by showing

428
00:26:49,418 --> 00:26:52,746
that it's actually moving data from core DNS to global

429
00:26:52,778 --> 00:26:56,034
DNS. In reality that's not exactly how it does it,

430
00:26:56,072 --> 00:26:59,790
but in effect it has the same impact.

431
00:26:59,870 --> 00:27:03,774
It makes sure that those DNS records are available in global

432
00:27:03,822 --> 00:27:07,446
DNS. Once they are available in global DNS, the client is able

433
00:27:07,468 --> 00:27:10,802
to then contact your data servers

434
00:27:10,866 --> 00:27:14,246
and communicate with them effectively now one challenge with

435
00:27:14,268 --> 00:27:17,250
this approach is that external DNS only runs periodically,

436
00:27:17,330 --> 00:27:20,934
every minute or so. So your DNS

437
00:27:20,982 --> 00:27:24,102
records are not immediately available in global DNS.

438
00:27:24,166 --> 00:27:27,766
So for example, if data server four here is just booting

439
00:27:27,798 --> 00:27:31,914
up, it should not go online until it's absolutely

440
00:27:32,032 --> 00:27:35,658
certain that its DNS records are available in global DNS.

441
00:27:35,754 --> 00:27:39,866
So we have to actually build logic to make sure that it can validate

442
00:27:39,978 --> 00:27:43,370
that global DNS has actually got its DNS records.

443
00:27:43,530 --> 00:27:46,758
Once it's confirmed, only then would the data server declare

444
00:27:46,794 --> 00:27:50,286
itself as available for traffic. So this is one of those steps

445
00:27:50,318 --> 00:27:53,266
you kind of might have to deal with in the real world when you're trying

446
00:27:53,288 --> 00:27:56,966
to use Kubernetes and stateful applications in

447
00:27:56,988 --> 00:28:00,134
general. Now finally, I want to talk

448
00:28:00,172 --> 00:28:03,574
a little bit about scalability architecture in

449
00:28:03,612 --> 00:28:07,014
public cloud. Typically you deploy software in a

450
00:28:07,052 --> 00:28:10,594
certain region. You can deploy it across multiple

451
00:28:10,642 --> 00:28:14,010
regions, but if you are doing a high performance software that needs

452
00:28:14,080 --> 00:28:17,782
very low latency, you deploy that software

453
00:28:17,846 --> 00:28:21,614
in a particular region, which is really a geographical region like

454
00:28:21,652 --> 00:28:23,390
us east or US west.

455
00:28:24,530 --> 00:28:28,410
And within that region you can also spread your software

456
00:28:28,490 --> 00:28:32,254
across different availability zones. Availability zones can

457
00:28:32,292 --> 00:28:36,078
mean different things for different cloud providers, but typically

458
00:28:36,174 --> 00:28:40,014
it is either a separate building, a separate

459
00:28:40,062 --> 00:28:43,346
campus even, but very close to each other, so that

460
00:28:43,368 --> 00:28:46,406
the network latency between the different availability zones is

461
00:28:46,428 --> 00:28:50,342
not too high. So you can actually spread your software across it without

462
00:28:50,396 --> 00:28:53,240
experiencing too much of a performance issue.

463
00:28:53,610 --> 00:28:57,414
So I'll be calling availability zones AZ for

464
00:28:57,452 --> 00:29:00,554
short here. So the goal is typically for you

465
00:29:00,592 --> 00:29:03,834
to take a few AZ. In our case

466
00:29:03,872 --> 00:29:07,654
we took three AZ approach and make sure that your software

467
00:29:07,702 --> 00:29:11,386
is spread across the instances of your software are spread across each

468
00:29:11,408 --> 00:29:14,302
one of these AZ to achieve this.

469
00:29:14,356 --> 00:29:17,514
Fortunately in kubernetes there has been significant

470
00:29:17,562 --> 00:29:21,114
effort to make sure that you can support this kind of deployment.

471
00:29:21,242 --> 00:29:25,374
So they have got something called affinity and anti affinity rules

472
00:29:25,422 --> 00:29:29,166
where you can tell the Kubernetes scheduler

473
00:29:29,278 --> 00:29:32,558
to spread the pods across different AZ.

474
00:29:32,654 --> 00:29:36,430
And the way they do it is that the hosts that run in each AZ

475
00:29:36,510 --> 00:29:39,926
have a certain label indicating what AZ that hosts is

476
00:29:39,948 --> 00:29:43,030
running in. And then you can tell Kubernetes that,

477
00:29:43,100 --> 00:29:46,742
make sure that when you deploy these pods, they run on

478
00:29:46,876 --> 00:29:50,620
hosts that have different label values as much as possible.

479
00:29:51,630 --> 00:29:55,162
Obviously you will have more than three pods, so you're not

480
00:29:55,216 --> 00:29:58,714
going to be able to spread these all on different AZ, but you do your

481
00:29:58,752 --> 00:30:02,250
best effort to equally balance it across different AZ.

482
00:30:02,690 --> 00:30:06,314
Now that takes care of making sure that your software

483
00:30:06,362 --> 00:30:10,334
is running on different azs, but what about the data inside that

484
00:30:10,372 --> 00:30:13,930
software. A good example of it is Hadoop file system,

485
00:30:14,100 --> 00:30:18,190
which keeps three copies of data for high availability reasons.

486
00:30:18,270 --> 00:30:21,634
Now you want to make sure that that copy each one of those

487
00:30:21,672 --> 00:30:25,690
copies is running in different AZ for safety reasons.

488
00:30:25,870 --> 00:30:29,346
So fortunately in Hadoop itself, when they designed

489
00:30:29,378 --> 00:30:33,446
it, they introduced this concept called rack topology, which is sort

490
00:30:33,468 --> 00:30:37,302
of the traditional data center terminology where you tell

491
00:30:37,356 --> 00:30:41,450
Hadoop fails system. In particular it's metadata server.

492
00:30:42,430 --> 00:30:45,718
What is the topology of your servers?

493
00:30:45,814 --> 00:30:49,370
In which racks do they run in? And Hadoop will make sure that

494
00:30:49,440 --> 00:30:53,102
these replicas are kind of distributed on

495
00:30:53,156 --> 00:30:56,494
different racks, so that if one whole rack goes down,

496
00:30:56,692 --> 00:30:59,918
you still got other racks that can serve the data.

497
00:31:00,084 --> 00:31:04,018
We were then able to convince Hadoop through using its

498
00:31:04,184 --> 00:31:08,302
script based interface that each AZ is a different rack.

499
00:31:08,366 --> 00:31:12,382
And thereby Hadoop was able to spread these replicas

500
00:31:12,446 --> 00:31:16,626
across different azs using that mapping. The metadata

501
00:31:16,658 --> 00:31:20,086
server itself had multiple instances which are

502
00:31:20,108 --> 00:31:23,586
again spread across different AZ

503
00:31:23,778 --> 00:31:27,522
using the same affinity anti affinity rules that Kubernetes

504
00:31:27,586 --> 00:31:31,174
supports. So what you achieve with all these spread is

505
00:31:31,212 --> 00:31:34,902
that if an entire AZ goes away due to a power

506
00:31:34,956 --> 00:31:38,614
outage or a network outage, you still have the software and

507
00:31:38,652 --> 00:31:42,638
its data available in the other AZ and still

508
00:31:42,724 --> 00:31:46,554
serving traffic in spite of this outage. So it's really useful

509
00:31:46,602 --> 00:31:49,754
for resilience in general. So that's

510
00:31:49,802 --> 00:31:53,694
pretty much all I had for today's talk. Thank you

511
00:31:53,732 --> 00:31:54,570
so much for listening.

