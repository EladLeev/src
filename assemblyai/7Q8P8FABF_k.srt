1
00:00:00,250 --> 00:00:01,630
Are you an SRE?

2
00:00:03,570 --> 00:00:07,358
A developer, a quality

3
00:00:07,444 --> 00:00:11,406
engineer who wants to tackle the challenge of improving reliability in

4
00:00:11,428 --> 00:00:15,294
your DevOps? You can enable your DevOps for reliability with

5
00:00:15,332 --> 00:00:19,114
chaos native. Create your free account at Chaos

6
00:00:19,162 --> 00:01:17,254
native Litmus cloud hi

7
00:01:17,292 --> 00:01:20,902
everyone, Chaos Engineering has proved to be a great to have

8
00:01:20,956 --> 00:01:24,694
option in the SRE and SDE toolbox, but the

9
00:01:24,732 --> 00:01:28,242
transition into more complex system is accelerating.

10
00:01:28,386 --> 00:01:31,894
My name is Guillauna George and I am a developer advocate at Amazon

11
00:01:31,942 --> 00:01:36,022
Web Services. And in this session we'll look at how automated Chaos

12
00:01:36,086 --> 00:01:40,086
engineering experiments can help us cover a more extensive set

13
00:01:40,128 --> 00:01:43,662
of experiments than what we can cover manually, and how

14
00:01:43,716 --> 00:01:47,326
it allows us to verify our assumptions over time

15
00:01:47,428 --> 00:01:50,000
as unknown parts of the system change.

16
00:01:51,170 --> 00:01:54,114
Chaos engineering, as most of you know,

17
00:01:54,232 --> 00:01:57,490
is the process of stressing an application in

18
00:01:57,560 --> 00:02:01,406
testing or production environment by creating these disruptive

19
00:02:01,438 --> 00:02:05,354
events, such as server outages or API throttling,

20
00:02:05,502 --> 00:02:09,266
and then observing how the system responds and implementing

21
00:02:09,298 --> 00:02:13,570
our improvements. And we do that to prove or disprove our assumptions

22
00:02:13,650 --> 00:02:16,950
about our system's capability to handle these

23
00:02:17,020 --> 00:02:21,034
disruptive events. But rather than to let them happen

24
00:02:21,232 --> 00:02:24,762
in the middle of the night or during the weekend, we can create

25
00:02:24,816 --> 00:02:28,154
them in a controlled environment and during working hours.

26
00:02:28,352 --> 00:02:32,350
And it's important to note that chaos engineering is not just about

27
00:02:32,420 --> 00:02:37,162
improving the resilience of your application, but also its performance.

28
00:02:37,306 --> 00:02:40,314
Uncover hidden issues, expose monitoring,

29
00:02:40,362 --> 00:02:43,794
observability and alarm blind spots, and more like

30
00:02:43,912 --> 00:02:47,646
improving the recovery time, the operational skills,

31
00:02:47,758 --> 00:02:49,700
culture and so on.

32
00:02:52,390 --> 00:02:55,442
And chaos engineering is but breaking things,

33
00:02:55,496 --> 00:02:59,154
but in a controlled environment. So we create these well planned

34
00:02:59,202 --> 00:03:02,760
experiments in order to build confidence in our application

35
00:03:03,210 --> 00:03:07,790
and in the tools we're using to withstand these turbulent conditions.

36
00:03:07,890 --> 00:03:11,830
And to do that, we follow this well defined scientific

37
00:03:11,910 --> 00:03:15,814
method that takes us from understanding the steady

38
00:03:15,862 --> 00:03:19,382
state of the system we're dealing with, to articulating

39
00:03:19,446 --> 00:03:22,410
a hypothesis, running an experiment,

40
00:03:22,490 --> 00:03:26,794
often using fault injection, verifying the results,

41
00:03:26,922 --> 00:03:30,494
and finally learning from the experiment in order

42
00:03:30,532 --> 00:03:34,430
to improve the system improvements such AWS resilience to failure,

43
00:03:34,510 --> 00:03:37,726
its performance, the monitoring, the alarms,

44
00:03:37,838 --> 00:03:40,980
the operations, well, the overall system.

45
00:03:41,990 --> 00:03:45,762
And today we're seeing customers using chaos engineering

46
00:03:45,826 --> 00:03:49,798
quite a lot. And that usage is really growing. And we've seen

47
00:03:49,884 --> 00:03:53,270
two clear use cases have emerged.

48
00:03:54,330 --> 00:03:57,842
The perhaps most common way of doing chaos engineering experiments

49
00:03:57,906 --> 00:04:01,562
is creating the one off experiment. And this is when you create

50
00:04:01,616 --> 00:04:05,510
can experiment by, for instance, looking at a previous outage

51
00:04:05,590 --> 00:04:08,106
or different events for your system,

52
00:04:08,288 --> 00:04:12,046
or you can also identify the services that have

53
00:04:12,068 --> 00:04:15,262
the biggest impact on your end users or

54
00:04:15,316 --> 00:04:19,178
customers if they go down or don't function properly.

55
00:04:19,274 --> 00:04:23,134
And then you create experiments for those, or maybe you've built

56
00:04:23,172 --> 00:04:27,234
a new feature, added a new service, or just made changes to the

57
00:04:27,272 --> 00:04:31,154
code or the architecture, and you create an experiments to

58
00:04:31,192 --> 00:04:34,370
verify that the system works as intended.

59
00:04:35,050 --> 00:04:39,394
And companies are doing this in different ways. Some have dedicated

60
00:04:39,522 --> 00:04:43,618
chaos engineers creating and running the experiments.

61
00:04:43,714 --> 00:04:47,122
For others, it's part of the SRES responsibilities,

62
00:04:47,266 --> 00:04:51,002
or as we partly do at AWS, chaos engineering is done

63
00:04:51,056 --> 00:04:54,460
by the engineering teams themselves on their services.

64
00:04:56,030 --> 00:04:59,974
The other very common use case is to use chaos engineering

65
00:05:00,022 --> 00:05:03,166
as part of your game days. And a game day.

66
00:05:03,268 --> 00:05:06,542
It is the process of rehearsing ahead of an event by

67
00:05:06,596 --> 00:05:09,982
creating the anticipated conditions and then

68
00:05:10,036 --> 00:05:14,342
observing how effectively the team and the system responds.

69
00:05:14,506 --> 00:05:18,574
And an event, well, that could be an unusually high traffic

70
00:05:18,622 --> 00:05:22,370
day, a new launch, a failure, or something else.

71
00:05:22,520 --> 00:05:26,102
And you can use chaos engineering experiment to run

72
00:05:26,156 --> 00:05:30,018
a game day by creating the event conditions

73
00:05:30,114 --> 00:05:33,558
and monitoring the performance of your team and

74
00:05:33,644 --> 00:05:37,734
for your system. So doing these one

75
00:05:37,772 --> 00:05:41,306
off experiments and perhaps the occasional game day now and then,

76
00:05:41,408 --> 00:05:45,462
it gets us very far on the road to improving resilience

77
00:05:45,526 --> 00:05:49,514
of our system. So isn't this enough? Well, it definitely can

78
00:05:49,552 --> 00:05:53,200
be, and it is for many, but let's look at an example.

79
00:05:54,050 --> 00:05:57,134
So this is a use case example of an

80
00:05:57,172 --> 00:05:58,880
ecommerce web application.

81
00:06:00,610 --> 00:06:04,978
So this is our application. It's a simple ecommerce site

82
00:06:05,144 --> 00:06:08,494
where we have tons and tons of end users buying

83
00:06:08,542 --> 00:06:11,090
things off the site continuously.

84
00:06:11,990 --> 00:06:16,450
And we've built this using well architected principles.

85
00:06:16,530 --> 00:06:20,742
So were set it up using multiple instances running

86
00:06:20,796 --> 00:06:25,026
in auto scaling groups spread over multiple availability

87
00:06:25,138 --> 00:06:28,214
zones. We have our database instances

88
00:06:28,342 --> 00:06:33,158
using read replicas and replication across availability

89
00:06:33,254 --> 00:06:37,290
zones as well. So we're trying to build for resilience and

90
00:06:37,360 --> 00:06:40,298
reliability. And next then?

91
00:06:40,384 --> 00:06:44,078
Well, this is just one part of the system. Of course,

92
00:06:44,164 --> 00:06:47,674
this is the product service, but we've added

93
00:06:47,722 --> 00:06:50,958
chaos engineering AWS, a practice, to our example.

94
00:06:51,124 --> 00:06:54,626
In this case, we use it to verify the

95
00:06:54,648 --> 00:06:58,882
resilience of our service and to learn and gain confidence in

96
00:06:58,936 --> 00:07:02,654
the application. And we also, of course, have introduced

97
00:07:02,702 --> 00:07:06,370
CI CD practices. So continuous integration

98
00:07:06,450 --> 00:07:10,070
and continuous delivery has not only made it possible,

99
00:07:10,220 --> 00:07:13,826
but it even encourages frequent deployments. And that's

100
00:07:13,858 --> 00:07:17,814
what we do in our use case example, as we

101
00:07:17,852 --> 00:07:21,226
know, frequent deployments, they are less likely to break

102
00:07:21,328 --> 00:07:24,714
and it's more likely that we'll be able to catch any bugs or

103
00:07:24,752 --> 00:07:28,630
gaps early on. But frequent deployments,

104
00:07:28,710 --> 00:07:32,382
when done perhaps daily, multiple times a day,

105
00:07:32,436 --> 00:07:35,882
or perhaps even by the hour in some cases,

106
00:07:36,026 --> 00:07:39,294
they are really hard to cover manually with

107
00:07:39,332 --> 00:07:42,962
chaos engineering experiments, it's just hard to keep

108
00:07:43,016 --> 00:07:44,370
up with the pace.

109
00:07:45,910 --> 00:07:50,978
And next, we also

110
00:07:51,064 --> 00:07:55,254
have of course, multiple services within our

111
00:07:55,292 --> 00:07:59,110
application. So we have different

112
00:07:59,180 --> 00:08:03,106
services that do different things within our application. So besides

113
00:08:03,138 --> 00:08:06,918
the product, we have order, we have a user service.

114
00:08:07,084 --> 00:08:10,630
We of course need a cart service and a recommendation

115
00:08:10,710 --> 00:08:14,570
service and a search service, all built

116
00:08:14,640 --> 00:08:18,602
using slight different architectures. They have different code

117
00:08:18,656 --> 00:08:22,014
bases and perhaps even different teams building and

118
00:08:22,052 --> 00:08:24,400
running these parts of our application.

119
00:08:24,930 --> 00:08:27,600
And next, then, well,

120
00:08:28,050 --> 00:08:31,866
there are dependencies between these services and different parts

121
00:08:31,898 --> 00:08:36,094
of our system. So the CaRt service is of course dependent

122
00:08:36,142 --> 00:08:39,522
on the user service. We have the product service,

123
00:08:39,656 --> 00:08:43,698
and that's also used by the cart service and the order

124
00:08:43,784 --> 00:08:48,390
service, and the cart service works together, and the search

125
00:08:48,460 --> 00:08:52,770
service needs to be able to search our products, and the recommendation

126
00:08:52,850 --> 00:08:56,630
engine also uses our products, for instance. So we create

127
00:08:56,700 --> 00:09:00,346
these dependencies between our microservices or services

128
00:09:00,448 --> 00:09:04,380
within the application. And that's also hard because

129
00:09:04,910 --> 00:09:08,714
teams operate differently, they might make change to

130
00:09:08,752 --> 00:09:12,222
the application at different times, and you

131
00:09:12,276 --> 00:09:15,280
depend on that other service to be there.

132
00:09:15,650 --> 00:09:19,326
So creating experiments that is able to cover those

133
00:09:19,428 --> 00:09:23,938
changes, that's also quite hard. So based

134
00:09:24,024 --> 00:09:27,858
on what we just looked at, some learnings from this

135
00:09:28,024 --> 00:09:31,666
very simple use case is that frequent deployments are

136
00:09:31,688 --> 00:09:35,358
hard to cover manually with chaos engineering experiments,

137
00:09:35,534 --> 00:09:39,506
just because we do them often, it's hard to create experiments

138
00:09:39,538 --> 00:09:43,830
and run them as frequently in a manual fashion and

139
00:09:43,900 --> 00:09:46,738
to cover a more extensive set of experiments.

140
00:09:46,834 --> 00:09:50,234
It is time consuming. And then even

141
00:09:50,272 --> 00:09:54,314
though you might have full control of the service or microservice that

142
00:09:54,352 --> 00:09:58,426
you're working on, unknown parts of the system might change

143
00:09:58,528 --> 00:10:01,120
because the other teams are making changes.

144
00:10:01,490 --> 00:10:04,506
And also finally, systems are becoming

145
00:10:04,538 --> 00:10:08,430
more complex. It's hard for anyone to keep and create

146
00:10:08,500 --> 00:10:12,254
a mental model of how the systems works, let alone to

147
00:10:12,292 --> 00:10:15,682
keep documentation up to date. And that,

148
00:10:15,816 --> 00:10:19,070
well, it brings us to automated experiments.

149
00:10:19,230 --> 00:10:22,562
Automation helps us cover a larger set of experiments than

150
00:10:22,616 --> 00:10:26,930
what we can cover manually. Automated experiments verifies

151
00:10:27,010 --> 00:10:30,534
our assumptions over time as unknown parts of the

152
00:10:30,572 --> 00:10:34,242
system are changed and doing automated

153
00:10:34,306 --> 00:10:37,994
experiments. It really goes back to the scientific part of

154
00:10:38,032 --> 00:10:41,510
chaos engineering in that repeating experiments,

155
00:10:41,590 --> 00:10:45,478
it's standard scientific practice for most fields,

156
00:10:45,574 --> 00:10:49,414
and repeating an experiment more than once, it helps us determine

157
00:10:49,462 --> 00:10:52,922
if that data was just a fluke or if it represents

158
00:10:52,986 --> 00:10:56,346
the normal case. It helps us guard against jumping

159
00:10:56,378 --> 00:10:59,550
to conclusions without enough evidence.

160
00:11:00,450 --> 00:11:04,402
So let's take a look at three different ways that we can automate our

161
00:11:04,456 --> 00:11:07,970
chaos engineering experiments. First off,

162
00:11:08,120 --> 00:11:11,842
let's think about how our system evolves. I mentioned it

163
00:11:11,896 --> 00:11:15,362
in the use case example, so that even

164
00:11:15,416 --> 00:11:18,550
when we might have full control over our service,

165
00:11:18,620 --> 00:11:21,430
our microservice, the one we're working on,

166
00:11:21,580 --> 00:11:25,414
other teams or even third parties, they are making changes, they are

167
00:11:25,452 --> 00:11:29,690
delivering new code, they are releasing new versions of their service,

168
00:11:29,840 --> 00:11:33,210
and those might be services that you depend

169
00:11:33,280 --> 00:11:36,682
on. So the verification you got from doing

170
00:11:36,736 --> 00:11:40,278
a one of chaos experiment a week ago or a month ago,

171
00:11:40,384 --> 00:11:43,950
it might quickly become obsolete because of these other

172
00:11:44,020 --> 00:11:48,154
changes. So by scheduling experiments to run on a recurring

173
00:11:48,202 --> 00:11:51,902
schedule, you can get that verification over and

174
00:11:51,956 --> 00:11:55,794
over again as unknown parts of the system change.

175
00:11:55,992 --> 00:12:00,050
So let's take a look at how this can quite

176
00:12:00,120 --> 00:12:01,810
easily be achieved.

177
00:12:02,550 --> 00:12:05,782
So I'm building

178
00:12:05,916 --> 00:12:09,650
a simple scheduling service for my chaos

179
00:12:09,730 --> 00:12:13,366
engineering experiments, and I'm doing this using

180
00:12:13,468 --> 00:12:17,830
a simple serverless application, setting up a schedule

181
00:12:18,510 --> 00:12:22,314
cloudwatch event rule. And this is based on the

182
00:12:22,352 --> 00:12:26,058
schedule that I define and basically a chrome job

183
00:12:26,144 --> 00:12:27,980
for when this should run.

184
00:12:29,150 --> 00:12:32,554
And this is a simple AWS lambda

185
00:12:32,602 --> 00:12:36,574
function that will then take the experiment template that

186
00:12:36,612 --> 00:12:40,154
I define and run that on that schedule.

187
00:12:40,282 --> 00:12:43,502
And in this case I'm using AWS fault

188
00:12:43,566 --> 00:12:47,422
injection simulator, AWS FIS. But the same principle

189
00:12:47,486 --> 00:12:51,314
works no matter if you're using another system, if you're using your

190
00:12:51,352 --> 00:12:54,770
own scripts for doing chaos engineering experiments.

191
00:12:55,130 --> 00:12:59,730
So this is my application, the instances

192
00:12:59,810 --> 00:13:03,720
running, let's say the product service that we looked at before,

193
00:13:04,090 --> 00:13:07,990
multiple instances running in different availability zones.

194
00:13:08,910 --> 00:13:12,550
So I've created a simple experiments

195
00:13:12,630 --> 00:13:16,442
for that. Let's just create that template. This is

196
00:13:16,496 --> 00:13:20,410
using cpu and memory stress. On our instances,

197
00:13:20,930 --> 00:13:25,150
I can now take this simple experiment template

198
00:13:26,290 --> 00:13:29,898
and using my scheduler,

199
00:13:30,074 --> 00:13:33,666
my lambda function, I can deploy it

200
00:13:33,768 --> 00:13:37,970
and define which experiment should run on the schedule.

201
00:13:38,390 --> 00:13:41,906
So just pasting in my experiments template id,

202
00:13:42,088 --> 00:13:46,500
setting up the schedule in this case once per day,

203
00:13:46,950 --> 00:13:50,646
simple crone syntax and

204
00:13:50,748 --> 00:13:59,174
deploying that deploying

205
00:13:59,222 --> 00:14:02,694
has started. Let's switch to AWS

206
00:14:02,742 --> 00:14:04,010
lambda console.

207
00:14:06,750 --> 00:14:09,260
All right, so the deploying is done,

208
00:14:10,370 --> 00:14:13,994
switching back. This is Amazon

209
00:14:14,042 --> 00:14:17,566
eventbridge, our event bus, so we can

210
00:14:17,588 --> 00:14:22,160
just have a look at what actually got deployed. So this is

211
00:14:22,950 --> 00:14:26,770
our schedule. So I'm copying a sample

212
00:14:27,190 --> 00:14:30,820
scheduling event and let's try this out

213
00:14:32,150 --> 00:14:36,134
back to AWS lambda, pasting in that sample event,

214
00:14:36,252 --> 00:14:37,960
and now we can test it.

215
00:14:39,210 --> 00:14:42,934
So this is as if my schedule were to

216
00:14:42,972 --> 00:14:46,502
run right now. So the lambda function

217
00:14:46,636 --> 00:14:50,940
kicks off and that should then

218
00:14:52,350 --> 00:14:55,946
start an experiment. Yes. So in AWS FISC we can

219
00:14:55,968 --> 00:14:58,060
see that an experiment is running.

220
00:15:00,030 --> 00:15:04,010
This is a very simple example using cpu stress.

221
00:15:04,170 --> 00:15:06,958
This is one of the instances I'm logged into.

222
00:15:07,124 --> 00:15:10,990
And if we watch the cpu levels, we can now see

223
00:15:11,060 --> 00:15:14,386
that that instance is being stressed by

224
00:15:14,488 --> 00:15:16,690
my chaos experiment,

225
00:15:20,140 --> 00:15:23,480
and it's also using up memory on the instance.

226
00:15:24,780 --> 00:15:28,300
And in this case my experiments

227
00:15:28,640 --> 00:15:32,232
is doing the steps that I defined in my template.

228
00:15:32,376 --> 00:15:36,392
But since this is an automated experiment,

229
00:15:36,456 --> 00:15:39,936
it is a recurring experiments. It will then run

230
00:15:40,038 --> 00:15:44,224
over and over again and verify the same set

231
00:15:44,262 --> 00:15:47,312
of conditions for me. And since

232
00:15:47,366 --> 00:15:50,704
it is automated, we need to have stop conditions in place,

233
00:15:50,822 --> 00:15:54,790
meaning that we have alarms that will then stop the

234
00:15:55,800 --> 00:15:59,892
experiment if anything goes wrong. So that was our

235
00:15:59,946 --> 00:16:03,540
first example. So now let's look at the second

236
00:16:03,610 --> 00:16:07,300
example. And the second approach to automation

237
00:16:07,380 --> 00:16:10,952
is to run chaos experiments. Automation based on

238
00:16:11,006 --> 00:16:14,904
events and an event, well that's basically

239
00:16:15,022 --> 00:16:18,744
anything that happens within your system. It could be an event related

240
00:16:18,792 --> 00:16:22,252
to the tech stack for instance, that latency is

241
00:16:22,306 --> 00:16:25,276
added whenever there's an auto scaling event.

242
00:16:25,378 --> 00:16:29,388
New instances are started for instance. Or maybe it's a business

243
00:16:29,474 --> 00:16:32,956
related event like can API being throttled when items

244
00:16:32,988 --> 00:16:37,072
are added to the cart. So building automation around these

245
00:16:37,126 --> 00:16:40,384
types of experiments, it can help you answer those quite hard

246
00:16:40,422 --> 00:16:44,244
to test questions. What if this when that

247
00:16:44,282 --> 00:16:48,452
is happening, and even when that is an event that's in

248
00:16:48,506 --> 00:16:51,510
a totally different parts of the system.

249
00:16:52,200 --> 00:16:55,050
So let's look at an example of that as well.

250
00:16:56,140 --> 00:17:00,424
So once again, simple automation set

251
00:17:00,462 --> 00:17:03,290
up using serverless application.

252
00:17:05,020 --> 00:17:08,316
In this case it is an

253
00:17:08,338 --> 00:17:12,124
event triggered experiments. So setting up

254
00:17:12,162 --> 00:17:15,852
an event based on cloud watch

255
00:17:15,906 --> 00:17:19,584
event or an event in Eventbridge. And in this case

256
00:17:19,622 --> 00:17:22,364
the pattern is AWS auto scaling.

257
00:17:22,492 --> 00:17:25,916
Whenever an EC two instance is launched,

258
00:17:26,108 --> 00:17:29,596
what will happen is that it will kick off a lambda

259
00:17:29,628 --> 00:17:32,724
function, and that lambda function is pretty much

260
00:17:32,762 --> 00:17:36,484
the same as in the previous example, meaning that it will start an

261
00:17:36,522 --> 00:17:40,692
experiments. So if we look in Eventbridge, we can see

262
00:17:40,746 --> 00:17:44,852
that besides easy to auto scaling events,

263
00:17:44,916 --> 00:17:48,052
we can create these types of event patterns

264
00:17:48,196 --> 00:17:51,640
for a whole bunch of different

265
00:17:51,710 --> 00:17:55,210
AWS services. Or we can create

266
00:17:55,740 --> 00:17:59,384
our custom patterns, meaning that it could be a pattern

267
00:17:59,432 --> 00:18:03,868
based on a business metric, something that happens,

268
00:18:04,034 --> 00:18:07,896
as I mentioned before, items added to cart or a third party

269
00:18:07,928 --> 00:18:10,736
service as well. All right,

270
00:18:10,838 --> 00:18:13,730
so we have that in place.

271
00:18:14,740 --> 00:18:18,304
We just need to define which auto scaling group it

272
00:18:18,342 --> 00:18:21,824
should base this on. And of course I have

273
00:18:21,862 --> 00:18:25,968
stop conditions in place. Once again, it's automated experiments.

274
00:18:26,064 --> 00:18:29,556
We won't watch them manually every time, so we need to

275
00:18:29,578 --> 00:18:33,124
make sure that stop conditions are in place to stop the

276
00:18:33,162 --> 00:18:36,280
experiment if an alarm is triggered.

277
00:18:37,100 --> 00:18:40,760
So creating this new experiment template,

278
00:18:43,500 --> 00:18:48,670
we go now to deploy my

279
00:18:49,680 --> 00:18:53,020
event triggered experiment automation.

280
00:18:55,120 --> 00:18:58,316
And we have that in the

281
00:18:58,338 --> 00:19:00,240
AWS lambda console.

282
00:19:04,410 --> 00:19:08,342
Here we go. So, switching to EC two and let's just

283
00:19:08,396 --> 00:19:11,730
make a change to one of our auto scaling groups.

284
00:19:11,890 --> 00:19:15,782
Let's change the desired capacity from two instances

285
00:19:15,846 --> 00:19:19,606
to three instances, which then is an auto scaling

286
00:19:19,638 --> 00:19:23,430
event. And that gets picked up by eventbridge,

287
00:19:23,510 --> 00:19:27,306
which triggers our AWS lambda function, which in turn

288
00:19:27,408 --> 00:19:30,490
triggers our AWS FisC experiment.

289
00:19:30,650 --> 00:19:39,178
So we can see that one experiment is running and

290
00:19:39,264 --> 00:19:42,238
looking at a logged in instance. Once again,

291
00:19:42,404 --> 00:19:46,446
we can see that the instance is

292
00:19:46,548 --> 00:19:50,782
using cpu and memory, meaning that our

293
00:19:50,836 --> 00:19:53,546
experiment is successful.

294
00:19:53,738 --> 00:19:57,682
And with the stop conditions in place, we don't need to

295
00:19:57,736 --> 00:20:01,666
watch the experiment manually. This can happen over and over again.

296
00:20:01,768 --> 00:20:05,310
And if an alarm is triggered, it will automatically stop.

297
00:20:05,400 --> 00:20:09,202
So our customers end users aren't affected

298
00:20:09,266 --> 00:20:13,078
by the experiment. And then

299
00:20:13,164 --> 00:20:16,614
that gets us to our third example. And the third way

300
00:20:16,652 --> 00:20:20,634
of doing automated experiments is perhaps the most popular one

301
00:20:20,672 --> 00:20:24,186
so far, and the one I'm definitely getting the most questions

302
00:20:24,288 --> 00:20:28,650
around. So, continuous integration and continuous delivery,

303
00:20:29,310 --> 00:20:33,710
as I said before, it encourages frequent deployments,

304
00:20:34,290 --> 00:20:37,546
and this means that the application is less likely

305
00:20:37,578 --> 00:20:41,726
to break. So we have this problem that

306
00:20:41,748 --> 00:20:45,774
we showed in the previous use case that we have frequent

307
00:20:45,822 --> 00:20:49,726
deployments, but aren't perhaps able to do chaos engineering

308
00:20:49,758 --> 00:20:53,486
experiments as frequent. So by adding chaos engineering

309
00:20:53,518 --> 00:20:57,030
experiments as part of our delivery pipelines were able to

310
00:20:57,100 --> 00:21:01,094
continuously verify the output or behavior of

311
00:21:01,132 --> 00:21:04,454
our system. So let's look at an example of that AWS.

312
00:21:04,492 --> 00:21:05,080
Well,

313
00:21:09,520 --> 00:21:11,980
so this is our pipeline.

314
00:21:12,560 --> 00:21:16,376
It is simply deploying to staging and deploying

315
00:21:16,408 --> 00:21:19,900
to production, demo purpose pipeline.

316
00:21:21,200 --> 00:21:25,648
This is built using infrastructure as code, of course. So we have our

317
00:21:25,734 --> 00:21:29,100
pipeline, we have the stages, fetching the source,

318
00:21:29,180 --> 00:21:32,640
deploying to staging and deploying to production.

319
00:21:33,300 --> 00:21:37,412
Now we're adding an experiments stage for

320
00:21:37,466 --> 00:21:41,940
the staging environment. So after deploying to staging,

321
00:21:42,520 --> 00:21:46,740
let's just kick off the deployment of this updated

322
00:21:47,580 --> 00:21:51,748
template. So after deploying to staging,

323
00:21:51,844 --> 00:21:55,640
it will run an experiment on the staging environment.

324
00:21:56,460 --> 00:22:00,476
And what it does, well, it's simply a

325
00:22:00,658 --> 00:22:04,172
state machine using AWS step functions that will

326
00:22:04,226 --> 00:22:08,044
start the experiment and then monitor that experiment to make

327
00:22:08,082 --> 00:22:11,064
sure that it either succeeds,

328
00:22:11,192 --> 00:22:15,200
or if it fails, it will then stop that experiment.

329
00:22:16,340 --> 00:22:20,080
So back to the pipeline,

330
00:22:21,620 --> 00:22:25,540
and now we have this new stage, the experiment stage

331
00:22:26,040 --> 00:22:29,812
in place. So let's give it a try. And as

332
00:22:29,866 --> 00:22:34,132
one does for a demo, let's just edit straight in

333
00:22:34,186 --> 00:22:37,930
GitHub, make a small change to our code base,

334
00:22:39,260 --> 00:22:46,814
and commit. That straightaway

335
00:22:46,862 --> 00:22:50,126
kicks off the pipeline, fetching the source,

336
00:22:50,318 --> 00:22:54,598
deploying to staging, which is a quick process

337
00:22:54,684 --> 00:22:58,470
in our demo environment. And then it gets

338
00:22:58,540 --> 00:23:02,434
to the experiments stage where it will now initiate

339
00:23:02,482 --> 00:23:06,650
our AWs step function workflow,

340
00:23:07,550 --> 00:23:11,590
which in turn then parts our AWs fis

341
00:23:11,670 --> 00:23:16,038
experiments. So that is running as we can see.

342
00:23:16,224 --> 00:23:19,806
And for purpose of this demo, this is a

343
00:23:19,828 --> 00:23:23,182
really quick experiment, so it will

344
00:23:23,316 --> 00:23:27,294
quickly finish and complete so we can see what

345
00:23:27,332 --> 00:23:30,290
happens. All right, so it's already completed.

346
00:23:30,870 --> 00:23:33,570
Let's switch back to the pipeline.

347
00:23:35,590 --> 00:23:39,602
Succeeded, and then it moves on to the next stage, which is

348
00:23:39,656 --> 00:23:43,430
to deploying to production. So, very simple example

349
00:23:43,500 --> 00:23:47,970
of how we can add our chaos engineering experiments to a pipeline.

350
00:23:48,130 --> 00:23:51,862
So let's do another one. What if that

351
00:23:51,916 --> 00:23:55,994
experiment fails? What if an alarm is triggered and it

352
00:23:56,192 --> 00:24:00,394
doesn't work as intended? So let's release a new change,

353
00:24:00,592 --> 00:24:03,420
fetching the source from GitHub once again,

354
00:24:04,990 --> 00:24:07,950
parts to deploying to our staging environment.

355
00:24:09,410 --> 00:24:13,370
Soon as that is done, it will kick off our experiment

356
00:24:13,450 --> 00:24:16,320
once again. There we go.

357
00:24:17,970 --> 00:24:22,818
It's in progress. Let's check

358
00:24:22,904 --> 00:24:27,060
AWS fis. The experiments is running.

359
00:24:27,910 --> 00:24:31,638
So what I can do now is use

360
00:24:31,724 --> 00:24:35,522
the AWS CLI and just set the alarm

361
00:24:35,666 --> 00:24:39,142
to be triggered. So I'm setting the alarm state for our

362
00:24:39,196 --> 00:24:40,310
stop condition.

363
00:24:42,730 --> 00:24:44,380
Let's try that.

364
00:24:47,150 --> 00:24:50,694
And this means that it will act as if an alarm

365
00:24:50,742 --> 00:24:54,246
was triggered, and fisk straightaway stops

366
00:24:54,278 --> 00:24:55,370
our experiments.

367
00:24:57,490 --> 00:25:00,990
Switching back to the pipeline.

368
00:25:01,650 --> 00:25:03,710
We can see that it failed.

369
00:25:09,130 --> 00:25:12,858
And the failed experiment in this case, well, it means

370
00:25:12,944 --> 00:25:16,346
that it won't proceed to the next step. And the

371
00:25:16,368 --> 00:25:19,866
next step would be to deploy to production. But for some reason,

372
00:25:19,968 --> 00:25:23,126
our experiment failed. Might be that something is wrong

373
00:25:23,168 --> 00:25:27,246
with the code, something doesn't work, we have more latency, or whatever it

374
00:25:27,268 --> 00:25:30,910
is we're testing with our experiment. And in this case,

375
00:25:30,980 --> 00:25:33,790
we won't move that into production.

376
00:25:35,490 --> 00:25:39,182
We can build on this AWS. Well, of course, by adding

377
00:25:39,246 --> 00:25:43,410
experiment stage after deploying to production, as well as

378
00:25:43,480 --> 00:25:46,818
an extra way of testing and making sure that everything

379
00:25:46,904 --> 00:25:50,230
works as intended. So this was an example

380
00:25:50,300 --> 00:25:54,070
of how to add experiments to your pipelines. First,

381
00:25:54,140 --> 00:25:57,426
by showing what happens when it works, it just proceeds

382
00:25:57,458 --> 00:26:01,114
to the next step, and when it fails, it stops the

383
00:26:01,152 --> 00:26:05,174
pipeline. And that shows the value of having stop conditions

384
00:26:05,222 --> 00:26:09,846
in place. Stop condition that watches your application behavior

385
00:26:09,958 --> 00:26:13,262
and then stops an experiment if an alarm is

386
00:26:13,316 --> 00:26:17,150
triggered. And what kind of stop conditions you'll use,

387
00:26:17,220 --> 00:26:20,558
that's very much up to you. And the use case,

388
00:26:20,644 --> 00:26:24,170
the traditional it depends answer. But for instance,

389
00:26:24,250 --> 00:26:27,458
it might be that you're seeing less

390
00:26:27,544 --> 00:26:31,026
users adding items to cart, or it might be

391
00:26:31,048 --> 00:26:34,974
a very technical metric, for instance, that you're seeing cpu levels

392
00:26:35,022 --> 00:26:38,840
above a certain threshold or things like that.

393
00:26:39,530 --> 00:26:43,286
So with these

394
00:26:43,388 --> 00:26:47,746
three options, the recurring scheduled experiments, the event triggered

395
00:26:47,778 --> 00:26:51,402
experiments, and the continuous delivery experiments, we have three different ways

396
00:26:51,456 --> 00:26:54,438
to automated our chaos engineering experiments.

397
00:26:54,534 --> 00:26:57,834
So should you then stop doing one off experiments and

398
00:26:57,872 --> 00:27:01,370
the periodic game day? Well, no, you shouldn't.

399
00:27:01,870 --> 00:27:05,226
They should still be at the core of your chaos engineering

400
00:27:05,258 --> 00:27:08,814
practice. They are a super important source for learning,

401
00:27:08,932 --> 00:27:12,090
and it helps your organization build confidence.

402
00:27:12,250 --> 00:27:16,394
But now you have yet another tool to help you improve the resilience

403
00:27:16,442 --> 00:27:19,790
of your system, the automated chaos experiments.

404
00:27:19,870 --> 00:27:23,554
So one way to think, but it is that experiments you start

405
00:27:23,592 --> 00:27:27,298
off by creating as one offs or as part of your game days,

406
00:27:27,384 --> 00:27:30,962
they can then turn into experiments that you run automated

407
00:27:31,106 --> 00:27:34,726
after doing the experiment manually to start with, they can

408
00:27:34,748 --> 00:27:38,630
be set to run every day, every hour, or on every code

409
00:27:38,700 --> 00:27:42,426
deploy. And that brings us to

410
00:27:42,448 --> 00:27:45,414
a summary with a recap of some takeaways.

411
00:27:45,542 --> 00:27:49,434
So automation helps us cover a larger set of experiments than

412
00:27:49,472 --> 00:27:52,874
what we can cover manually and automated experiments

413
00:27:52,922 --> 00:27:56,538
verifies our assumptions over time as unknown parts

414
00:27:56,554 --> 00:28:00,730
of the system are changed and safeguards and stop conditions

415
00:28:00,810 --> 00:28:04,822
are key to safe automation. And introducing

416
00:28:04,906 --> 00:28:08,242
automated chaos engineering experiments does not

417
00:28:08,296 --> 00:28:11,890
mean that you should stop doing manual experiments

418
00:28:13,350 --> 00:28:17,406
if you just can't get enough of chaos engineering to improve resilience.

419
00:28:17,518 --> 00:28:20,894
I've gathered some code samples, the ones used in the demos,

420
00:28:20,942 --> 00:28:24,338
and some additional resources for you in the link shown on the

421
00:28:24,344 --> 00:28:28,194
screen. Now just scan the QR code or Gunnar

422
00:28:28,312 --> 00:28:31,930
grosch link chaos.

423
00:28:33,150 --> 00:28:36,714
And with that, I want to thank you all for watching. We've looked at

424
00:28:36,752 --> 00:28:41,286
how to improve resilience with automated chaos engineering.

425
00:28:41,478 --> 00:28:45,514
As ever, if you have any questions or comments, do reach out on

426
00:28:45,552 --> 00:28:49,322
Twitter at Gunagarosh as shown on screen or

427
00:28:49,376 --> 00:28:52,826
connect on LinkedIn. I am happy to connect. Thank you

428
00:28:52,848 --> 00:28:53,400
all for watching.

