1
00:00:25,410 --> 00:00:28,262
You. Hello.

2
00:00:28,396 --> 00:00:31,894
It's great to be with you all here today at Conf 42 talking

3
00:00:31,932 --> 00:00:35,094
about chaos Engineering with some of my friends. Today I'm going to talk about

4
00:00:35,132 --> 00:00:38,354
normalizing chaos a cloud architectures for embracing

5
00:00:38,402 --> 00:00:42,034
failure. My name is Ryan Guest. I'm a software architect

6
00:00:42,082 --> 00:00:45,510
at Salesforce. If you'd like to chat on email,

7
00:00:45,580 --> 00:00:48,742
my email is included. Or at social media,

8
00:00:48,876 --> 00:00:52,554
my Twitter hashtag is at Ryan Guest and I'd

9
00:00:52,602 --> 00:00:55,966
love to follow up after this conference and talk about some

10
00:00:55,988 --> 00:00:59,534
of those topics here. So to begin,

11
00:00:59,572 --> 00:01:03,290
I'm going to start with a premise, and that is what if we could optimize

12
00:01:03,370 --> 00:01:06,720
for getting paged at 02:00 p.m. Instead of 02:00 a.m.

13
00:01:07,410 --> 00:01:11,010
I think it's kind of funny that every seasoned engineer I talk to,

14
00:01:11,080 --> 00:01:14,514
they have a getting paged at 02:00 a.m. Or 03:00 a.m. In the morning story.

15
00:01:14,712 --> 00:01:18,066
And it seems like it's almost become sort of a ritual from

16
00:01:18,168 --> 00:01:22,182
going from junior engineer to senior engineers. You have to have a situation like that.

17
00:01:22,316 --> 00:01:25,814
But I'd like to throw out there, it doesn't have to be this way.

18
00:01:25,932 --> 00:01:29,914
And then as we look at designing systems for reliability and

19
00:01:30,032 --> 00:01:33,674
other aspects, one of the things we can look at is potentially could we

20
00:01:33,712 --> 00:01:37,770
optimized for getting paged or equivalent to system

21
00:01:37,840 --> 00:01:41,274
errors happens during normal business hours and not

22
00:01:41,312 --> 00:01:44,458
in the evening or not in the middle of the night. Taking a look at

23
00:01:44,464 --> 00:01:48,014
some of the principles from chaos engineering, could we use some of those and

24
00:01:48,052 --> 00:01:51,566
apply them to the architectures we built? And that's exactly what we

25
00:01:51,588 --> 00:01:54,766
did. And I want to share one reference architecture that we used at

26
00:01:54,788 --> 00:01:58,462
Salesforce and were pretty successful with to sort of optimize for failures,

27
00:01:58,526 --> 00:02:01,794
not happens at off hours in the middle of the night, but actually happening during

28
00:02:01,832 --> 00:02:04,974
normal business hours and having full cognitive

29
00:02:05,022 --> 00:02:08,638
ability and the full team on staff to respond to

30
00:02:08,664 --> 00:02:12,214
them and make changes. Now before I go into

31
00:02:12,252 --> 00:02:15,414
that, I think it's important to give you a little background on myself.

32
00:02:15,612 --> 00:02:18,566
Like I said, my name is Ryan. I'm a software architect and I've been at

33
00:02:18,588 --> 00:02:21,914
Salesforce for 13 years. Over those 13 years,

34
00:02:21,952 --> 00:02:25,562
I've spent most of my time on the core platform. Working in data

35
00:02:25,616 --> 00:02:28,940
privacy and security are probably the two areas that interest me the most.

36
00:02:29,470 --> 00:02:33,882
And Salesforce are an interesting time right now when we talk about infrastructure and reliability

37
00:02:34,026 --> 00:02:37,806
in that we just completed a complete rearchitecture of our most

38
00:02:37,988 --> 00:02:41,326
important infrastructure. Here it goes by the

39
00:02:41,348 --> 00:02:44,958
name Hyperforce. And with hyperforce we had a couple of key values that

40
00:02:44,964 --> 00:02:48,226
we focused on, one is local data storage. So for

41
00:02:48,248 --> 00:02:51,666
our customers or tenants, they can know where their data

42
00:02:51,688 --> 00:02:55,026
is stored. Those next is performance at both a, b to

43
00:02:55,048 --> 00:02:58,738
b and b to c scale. And each of those have different sorts

44
00:02:58,754 --> 00:03:02,086
of requirements. We also want to have built in trust. We know

45
00:03:02,108 --> 00:03:05,974
that being a cloud provider, trust is key to our success as

46
00:03:06,012 --> 00:03:09,322
well as backwards compatibility. So at Salesforce, we've been running

47
00:03:09,376 --> 00:03:12,858
cloud systems for 20 plus years, and so

48
00:03:12,944 --> 00:03:16,294
we wanted to interoperate with the latest and greatest,

49
00:03:16,342 --> 00:03:19,334
but also we had legacy systems that are critical,

50
00:03:19,382 --> 00:03:22,700
mission critical for our business and want to keep those running.

51
00:03:23,070 --> 00:03:26,478
Another important value of hyperforce is we wanted to

52
00:03:26,484 --> 00:03:29,566
be multisubstrate. So whatever cloud provider that we

53
00:03:29,588 --> 00:03:33,450
want to use under the covers to provide infrastructure, we want to easily be able

54
00:03:33,460 --> 00:03:36,786
to migrate and let customers choose which ones to

55
00:03:36,808 --> 00:03:40,590
use. Given those challenges in running a large scale

56
00:03:40,750 --> 00:03:44,514
cloud operation, I think it's easy for engineers to fall

57
00:03:44,552 --> 00:03:47,966
into the firefighter mentality. And like I was talking about earlier,

58
00:03:48,078 --> 00:03:51,666
all season, engineers having those story, or a lot

59
00:03:51,688 --> 00:03:54,598
of them, multiple stories, about getting woken up in the middle of night by a

60
00:03:54,604 --> 00:03:57,880
page or a phone call or a text and having to respond to an issue.

61
00:03:59,290 --> 00:04:02,470
We feel good about being able to jump in and save the day.

62
00:04:02,620 --> 00:04:06,330
There's a certain, like I said, badge of honor that folks wear and

63
00:04:06,400 --> 00:04:08,826
they can recall times they've done this, or I was the only one that could

64
00:04:08,848 --> 00:04:13,210
solve the problem, or I had the magic keystrokes to bring things back to normal.

65
00:04:13,630 --> 00:04:16,958
So I want to acknowledge that it does feel good, but on the

66
00:04:16,964 --> 00:04:20,286
other hand, it can lead to sort of firefighter mentality where we spend a lot

67
00:04:20,308 --> 00:04:23,614
of our old time fighting fires and not enough time saying,

68
00:04:23,732 --> 00:04:26,100
how can we prevent them in the first place?

69
00:04:28,390 --> 00:04:31,374
This is not something you can do forever, so you don't want to be constantly

70
00:04:31,502 --> 00:04:35,042
fighting fires. Doing that is one of those obvious ones

71
00:04:35,096 --> 00:04:38,166
that leads to burnout if you don't handle it

72
00:04:38,188 --> 00:04:41,462
well. But also, it can take away from innovation, right. If you're always

73
00:04:41,596 --> 00:04:44,870
trying to keep things up and fighting fires,

74
00:04:45,210 --> 00:04:48,726
it can sometimes hurt sort of innovation and product growth. So you

75
00:04:48,748 --> 00:04:52,738
have to have a balance between the two. There's also at

76
00:04:52,764 --> 00:04:56,618
some organizations have, this is compounded by organizational problems,

77
00:04:56,784 --> 00:05:00,026
where we end up rewarding those people that save the day. So if

78
00:05:00,048 --> 00:05:03,260
you have two engineers, one engineer builds a system

79
00:05:03,870 --> 00:05:07,534
where the system is reliable and just by the design of the system,

80
00:05:07,652 --> 00:05:11,006
you don't have to log in and ssh into production at 03:00 a.m.

81
00:05:11,028 --> 00:05:14,730
And run a magic script to fix things because a

82
00:05:14,740 --> 00:05:18,434
disk log is overflowing. And those other one who

83
00:05:18,472 --> 00:05:22,146
does that is always on call and always available. And this

84
00:05:22,168 --> 00:05:25,522
is a bit facetious, but we need to think what's better

85
00:05:25,576 --> 00:05:29,026
for your career. I think there's a

86
00:05:29,048 --> 00:05:32,626
natural tendency for the spot bonuses and the rewards go the person that's buying

87
00:05:32,658 --> 00:05:36,422
the fire, because all that stuff is more visible. But it's also important

88
00:05:36,476 --> 00:05:40,840
to recognize that folks that are building maintainable, reliable systems, that works important too.

89
00:05:41,230 --> 00:05:45,254
Long term, that's better for the company. So I think we need to balance

90
00:05:45,302 --> 00:05:48,854
both. And although it is okay to appreciate folks

91
00:05:48,902 --> 00:05:52,138
when they do fight fires, we need to think in a bigger context, what can

92
00:05:52,144 --> 00:05:55,406
we do to prevent that in the first place? And so I want to show

93
00:05:55,428 --> 00:05:59,434
you how our architecture evolved to get there right, and so we can minimize

94
00:05:59,482 --> 00:06:02,880
the amount of fires that do happen. And when fires do happen,

95
00:06:03,810 --> 00:06:07,986
can we choose where they go? So when you think about failover, I think most

96
00:06:08,008 --> 00:06:11,966
services start like this. With a traditional failover, this is pseudocode,

97
00:06:11,998 --> 00:06:14,738
but try and connect to server a. If that doesn't work,

98
00:06:14,824 --> 00:06:18,626
we'll try and connect to server b. And this is like circuit

99
00:06:18,658 --> 00:06:22,546
2001 Failover logic here and a very basic

100
00:06:22,578 --> 00:06:26,802
example, but this is calls over the place and it's used in production

101
00:06:26,866 --> 00:06:30,486
in a bunch of companies and pretty straightforward. And for

102
00:06:30,508 --> 00:06:33,500
a lot of places, for simple architectures, this works really well.

103
00:06:34,510 --> 00:06:37,706
But when we start to apply some of those chaos engineering tools, we can

104
00:06:37,728 --> 00:06:41,466
see that this architecture can be exploited. And as things go bigger and bigger

105
00:06:41,498 --> 00:06:45,200
and get more complex, it can be hard to keep up with this.

106
00:06:46,050 --> 00:06:49,786
Now, looking at this, one of my all time favorite distributed

107
00:06:49,818 --> 00:06:53,226
systems papers is called analysis of production failures

108
00:06:53,258 --> 00:06:56,498
in distributed data intensive systems. Comes out of the

109
00:06:56,504 --> 00:06:59,570
University of Toronto a couple years ago. And what this paper

110
00:06:59,640 --> 00:07:02,862
really did is they dug into popular

111
00:07:02,926 --> 00:07:06,798
open source systems, they looked at Cassandra, Hadoop, redis and a few others,

112
00:07:06,984 --> 00:07:10,566
and said, where are failures coming from? And they found the

113
00:07:10,588 --> 00:07:14,294
majority of catastrophic failures could easily have been prevented by

114
00:07:14,332 --> 00:07:18,120
performing simple testing on error handling code, right? And so

115
00:07:18,810 --> 00:07:22,722
they did the research and found that error handling, that's where

116
00:07:22,876 --> 00:07:26,346
a lot of the big bugs occur. And we think about it, we think back

117
00:07:26,368 --> 00:07:29,974
to the previous architecture I was talking about. We only really get to the error

118
00:07:30,022 --> 00:07:33,582
handling scenarios if things have already gone bad in the first place,

119
00:07:33,636 --> 00:07:37,118
right? If the happy path or the normal server I connect to is down,

120
00:07:37,284 --> 00:07:41,470
then we started to start to excise all this exceptional logic.

121
00:07:42,130 --> 00:07:46,094
And their conclusion was that simple testing can prevent most critical failures.

122
00:07:46,222 --> 00:07:50,738
And it begs the question, why don't we test these systems more?

123
00:07:50,904 --> 00:07:54,914
And so I would say think about that. How can we test the exceptional use

124
00:07:54,952 --> 00:07:59,090
cases more? How have we focused on that? As mentioned earlier,

125
00:07:59,910 --> 00:08:03,014
like I said, this paper is one of my favorite. They dug into all these

126
00:08:03,052 --> 00:08:06,246
sort of open source systems and looking at failure modes. What they did is they

127
00:08:06,268 --> 00:08:09,686
went through a lot of the bugs, they sampled the bugs, the highest severity

128
00:08:09,718 --> 00:08:13,674
bugs from the public bug trackers and

129
00:08:13,712 --> 00:08:17,610
did some great research. Also what I found interesting too is that

130
00:08:17,680 --> 00:08:21,386
you were scrapping the code for looking for an error handling code

131
00:08:21,488 --> 00:08:25,754
for to do or fix me. Two comments

132
00:08:25,802 --> 00:08:28,894
that developers mostly leave when they know it's an exceptional case.

133
00:08:28,932 --> 00:08:31,502
They know they probably need to do more, but they're just focused on the happy

134
00:08:31,556 --> 00:08:35,054
path. That was a great way to sort of identify, okay, these are the key

135
00:08:35,092 --> 00:08:37,970
places that a failure happens. It's going to be an issue.

136
00:08:38,120 --> 00:08:42,226
So looking at that research and then thinking about how

137
00:08:42,248 --> 00:08:45,602
come I always get paged at 02:00 a.m. And not PM, I started

138
00:08:45,656 --> 00:08:49,590
thinking, what if we just failed over all the time, right? So there wasn't no

139
00:08:49,740 --> 00:08:53,606
exceptional case. What if all the time we were just in failover mode and

140
00:08:53,628 --> 00:08:56,918
just operate like that? You could say it's a

141
00:08:56,924 --> 00:09:00,890
sort of chaos engineering mode. But really, what if that's our standard?

142
00:09:00,960 --> 00:09:04,380
What if we just normalized it and say, hey, this is how it works,

143
00:09:04,910 --> 00:09:09,270
so that the happy path and the non happy path, they don't diverge.

144
00:09:09,430 --> 00:09:13,790
And so here's our very rudimentary versions of more pseudocode.

145
00:09:14,130 --> 00:09:16,670
But we just randomly pick between two servers,

146
00:09:17,730 --> 00:09:20,718
flip a coin. If it's heads, go to server one, if it's tails, go to

147
00:09:20,724 --> 00:09:23,966
server two and just bounce back and forth all

148
00:09:23,988 --> 00:09:27,826
the time. If server one goes down, then we just keep going to

149
00:09:27,848 --> 00:09:32,130
server two. And this works great because if

150
00:09:32,200 --> 00:09:37,842
one server goes down, it's not the end of the world. And if

151
00:09:37,896 --> 00:09:41,458
one server goes down, we can check the logs or we can look at alerts

152
00:09:41,474 --> 00:09:44,930
or warnings, however you've instrumented or whatever type of observability

153
00:09:45,010 --> 00:09:48,166
you have, and do that during normal business hours and

154
00:09:48,188 --> 00:09:52,166
say, okay, these are the things that went wrong, or these are the errors we

155
00:09:52,188 --> 00:09:55,706
saw. And to the end user, things are still working. But we

156
00:09:55,728 --> 00:09:58,746
know we have to fix these things, we have to change this design or

157
00:09:58,768 --> 00:10:02,738
even just bring the server up. And I would much rather analyze

158
00:10:02,774 --> 00:10:05,998
those and think about those in the afternoon than in the middle of

159
00:10:06,004 --> 00:10:09,440
the night. And so expanding on this,

160
00:10:10,130 --> 00:10:13,614
we can do other things. So if we think of our

161
00:10:13,652 --> 00:10:17,406
main service and the servers we talk to are all in one cloud provider.

162
00:10:17,598 --> 00:10:21,758
We can split them between two different cloud providers to offer more reliability.

163
00:10:21,934 --> 00:10:25,266
And so this type of thinking sort of changes our mindset, right? So we're no

164
00:10:25,288 --> 00:10:28,610
longer thinking, okay, this is the main server, this is a failover,

165
00:10:28,690 --> 00:10:32,354
this is the primary. We're now thinking that, okay, there's a pool of servers,

166
00:10:32,402 --> 00:10:35,958
and there's no difference between if we failed over or it's a

167
00:10:35,964 --> 00:10:39,418
normal happy path. Both the client and the

168
00:10:39,424 --> 00:10:43,180
server don't care and can just behave as we do

169
00:10:44,270 --> 00:10:47,354
doing those thing. There's also some issues that cropped up. So some

170
00:10:47,392 --> 00:10:51,206
pragmatic things that we did, we ended up signing

171
00:10:51,238 --> 00:10:54,558
requests. So you could say, this is

172
00:10:54,564 --> 00:10:58,970
the type of data I'm looking for. You can get into different situations

173
00:10:59,050 --> 00:11:02,558
and you have to manage this to your environment where different services

174
00:11:02,644 --> 00:11:05,602
may be out of date or need a certain version, and you want to account

175
00:11:05,656 --> 00:11:09,166
for that. But this is expanding on our architecture.

176
00:11:09,198 --> 00:11:12,690
We events went a step further and let me dig into that.

177
00:11:12,840 --> 00:11:16,662
So when we talk about fill over everywhere, we realized that 50

178
00:11:16,716 --> 00:11:20,486
50 is kind of a naive approach and we could do

179
00:11:20,508 --> 00:11:24,102
better. And so we did some probabilistic modeling here.

180
00:11:24,156 --> 00:11:27,766
And essentially the difference here is that the choice of what

181
00:11:27,788 --> 00:11:31,206
service you go to becomes a function. And in that function we can

182
00:11:31,228 --> 00:11:34,874
have logic and decide what to do. And so you may,

183
00:11:34,992 --> 00:11:38,634
90% of the time you do want to go the closest server, because in

184
00:11:38,672 --> 00:11:41,626
some cases, you're limited by the speed of light. How fast can I get around

185
00:11:41,648 --> 00:11:44,620
the world or bounce between data centers or whatever?

186
00:11:45,330 --> 00:11:47,982
So you have a pool of outside servers, and you may want to hit them

187
00:11:48,036 --> 00:11:51,630
a lesser percent of the time. This is just a fictional example,

188
00:11:51,700 --> 00:11:54,366
but it's similar to what we're doing right now, is we have one where we

189
00:11:54,388 --> 00:11:58,062
favor server pool servers and then secondary

190
00:11:58,126 --> 00:12:01,506
servers, but during the course of the day of normal operations, we do

191
00:12:01,528 --> 00:12:04,626
send requests to them and we do ping and see how

192
00:12:04,648 --> 00:12:08,478
things are going. Now, we came up with a sort of formalized model for

193
00:12:08,504 --> 00:12:12,274
this. And really what it's important is that all these probabilities

194
00:12:12,322 --> 00:12:15,766
add up to one, like I said, so you can have individual functions for

195
00:12:15,788 --> 00:12:20,246
each service and say, okay, in this 90%

196
00:12:20,268 --> 00:12:23,594
of the time I want to go to my local service, and then this

197
00:12:23,632 --> 00:12:27,718
2% of the time I want to go to a service on another cloud provider

198
00:12:27,894 --> 00:12:31,146
just to make sure that that failover case works. And then maybe this 2%

199
00:12:31,168 --> 00:12:33,878
of the time I want to go to a service, another geographic region.

200
00:12:34,054 --> 00:12:37,678
If it's a service that is built like that. And this 3% of

201
00:12:37,684 --> 00:12:40,910
the time I want to go to another service. But those service

202
00:12:40,980 --> 00:12:44,410
maybe is an on premise service. And so I mentioned earlier,

203
00:12:44,570 --> 00:12:48,482
we are in our feature architecture and what we're doing right now is expanding to

204
00:12:48,616 --> 00:12:52,226
a multi substrate surface. One of the substrates may be a colocated or an on

205
00:12:52,248 --> 00:12:54,866
premise substrate. And how do you balance that in.

206
00:12:54,968 --> 00:12:58,626
Now, this architectures is good for some use cases, but there

207
00:12:58,648 --> 00:13:01,798
are major trade offs. And you're probably thinking in your mind, oh, this is good

208
00:13:01,804 --> 00:13:05,446
for this, but not good for that. So one thing is it's not good

209
00:13:05,468 --> 00:13:08,594
for is if your data changes often. So if you're constantly updating

210
00:13:08,642 --> 00:13:12,458
data, if you have a write heavy database, this is not

211
00:13:12,624 --> 00:13:16,906
the failover, the architecture mode for that. You really want something where

212
00:13:17,008 --> 00:13:20,342
changes don't happen that often, because you don't want to have to worry about replication

213
00:13:20,406 --> 00:13:23,580
or keeping data up to date. If you're bouncing around all over the place,

214
00:13:24,110 --> 00:13:27,882
this also isn't good. Similar to that, if you have a very chatty protocol.

215
00:13:28,026 --> 00:13:30,654
So if you're constantly talking back and forth, you don't want to be sending those

216
00:13:30,692 --> 00:13:34,606
messages all around the world. You probably do want to keep that located in

217
00:13:34,628 --> 00:13:38,446
a smaller region, and that makes sense. Similar to that, if you value performance

218
00:13:38,478 --> 00:13:42,162
over reliability. So if could say, hey, I'd rather have this request happen

219
00:13:42,216 --> 00:13:45,970
really quick or fail, then this isn't the protocol for you.

220
00:13:46,120 --> 00:13:49,394
This has worked best in our systems where it's been the opposite. It said,

221
00:13:49,432 --> 00:13:53,446
hey, reliability is the key here. If a request takes ten times

222
00:13:53,468 --> 00:13:56,246
as long, that's fine. I'd rather get an answer than not get an answer at

223
00:13:56,268 --> 00:14:00,234
all. And I can wait a little bit and that's key.

224
00:14:00,352 --> 00:14:04,490
So like I said, if your data is mostly consistent,

225
00:14:04,830 --> 00:14:08,406
if stale data isn't a big deal, or you have updated versioning

226
00:14:08,438 --> 00:14:12,362
system, and you really value reliability

227
00:14:12,426 --> 00:14:15,870
as one of the key tenants over things like performance,

228
00:14:16,850 --> 00:14:19,950
then this is a great architectures to choose. But it's important

229
00:14:20,020 --> 00:14:23,166
that like all engineering decisions, you consider those trade offs and look

230
00:14:23,188 --> 00:14:26,414
at what's the best. Thanks. I really

231
00:14:26,452 --> 00:14:30,314
appreciated talking to y'all. Feel free to hit me up on Twitter

232
00:14:30,362 --> 00:14:33,754
email both if you have questions or could like to talk about this further.

233
00:14:33,802 --> 00:14:37,794
It is an architectures that we're evolved here and I'd love to hear your thoughts

234
00:14:37,922 --> 00:14:42,054
and folks doing other similar things so they can really optimize for the

235
00:14:42,092 --> 00:14:45,414
chaos mindset. And those failover isn't something that just

236
00:14:45,452 --> 00:14:48,806
happens at 02:00 a.m. In exceptional circumstances, it can happen all

237
00:14:48,828 --> 00:14:51,078
the time, and I would love to hear more about that.

