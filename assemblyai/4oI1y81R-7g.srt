1
00:00:27,330 --> 00:00:31,158
Hello, my name is Antoni Ivanov and today

2
00:00:31,244 --> 00:00:35,302
I'm going to talk about applying DevOps practices in spatial data and

3
00:00:35,356 --> 00:00:39,366
data as a service. Before we

4
00:00:39,388 --> 00:00:42,534
dive in, let's talk a little

5
00:00:42,572 --> 00:00:44,870
bit about moneyball.

6
00:00:45,770 --> 00:00:49,398
Have you watched Moneyball? This is a really great movie and a book

7
00:00:49,564 --> 00:00:53,386
about out Oakland athletics baseball team in general manager

8
00:00:53,418 --> 00:00:56,894
Billy teams the story of Money Ball is a real

9
00:00:56,932 --> 00:01:00,926
one starts in the 2000s with Oakland Athletics, a Major League Baseball team

10
00:01:01,028 --> 00:01:04,370
faced with budget constraints that severely limited its ability

11
00:01:05,030 --> 00:01:07,010
to complete for top talent.

12
00:01:08,630 --> 00:01:13,198
So they were in the bottom right. Bean cooperated

13
00:01:13,294 --> 00:01:16,746
with Paul de Podesta, a Harold graduate with bavarian

14
00:01:16,798 --> 00:01:20,390
economics, to apply statistical analysis to player recruitment.

15
00:01:20,970 --> 00:01:24,182
That means that he was looking at players that most

16
00:01:24,236 --> 00:01:27,366
of the other scouts were missing because

17
00:01:27,548 --> 00:01:31,226
he was using statistics instead of intuitions to

18
00:01:31,248 --> 00:01:35,174
make his decisions. Not only that, but during games

19
00:01:35,302 --> 00:01:39,574
he was prioritizing on base percentage

20
00:01:39,622 --> 00:01:43,134
of a batting average, understanding that getting off base

21
00:01:43,172 --> 00:01:45,710
was often more valuable than hitting home runs.

22
00:01:46,290 --> 00:01:50,366
This totally changed the team for just one year in

23
00:01:50,388 --> 00:01:53,650
2003 seasons, Aquaman 20 went to win 20

24
00:01:53,720 --> 00:01:56,980
consecutive games, setting American League record.

25
00:01:57,670 --> 00:02:01,694
This created paradigm shift in the entire baseball

26
00:02:01,822 --> 00:02:05,146
where now analytics and data being actively

27
00:02:05,198 --> 00:02:08,390
used by all successful teams.

28
00:02:08,810 --> 00:02:12,642
This just goes to prove that good decisions, correct decisions,

29
00:02:12,786 --> 00:02:16,482
are generally data driven decisions and intuition

30
00:02:16,546 --> 00:02:19,320
based decisions generally don't make it.

31
00:02:19,690 --> 00:02:23,002
Today we're going to go quickly over agenda. We're going to talk

32
00:02:23,056 --> 00:02:26,474
about data, applications and API for data. What are

33
00:02:26,512 --> 00:02:30,346
seos and seos for data and the data circle for data and

34
00:02:30,368 --> 00:02:33,710
the open source products that I work on with style data kit and how it

35
00:02:33,780 --> 00:02:37,882
tries to solve some of the challenges

36
00:02:37,946 --> 00:02:41,114
for data engineers. So what are applications?

37
00:02:41,162 --> 00:02:44,990
There are many different types of applications, right? We have ecommerce

38
00:02:45,070 --> 00:02:48,420
applications where people want to buy things.

39
00:02:49,030 --> 00:02:52,910
Of course there are lot of mobile applications,

40
00:02:52,990 --> 00:02:56,550
iPhone, Android, different it

41
00:02:56,620 --> 00:03:00,722
systems that are used to track different business processes,

42
00:03:00,786 --> 00:03:04,150
accommodation system. All those application have one thing in common.

43
00:03:04,220 --> 00:03:07,160
Every single application it generates data.

44
00:03:07,550 --> 00:03:11,622
Huge amount of data. Things like databases

45
00:03:11,766 --> 00:03:15,866
which we use to store data about customers,

46
00:03:15,968 --> 00:03:19,754
right? Or log files that

47
00:03:19,792 --> 00:03:23,182
we use to store the world operational data, click streams about

48
00:03:23,236 --> 00:03:27,146
events, about what is happening and how the customer using the product metrics.

49
00:03:27,178 --> 00:03:31,262
Operational metrics help us maintain our

50
00:03:31,316 --> 00:03:35,314
services. One thing that most people don't realize is

51
00:03:35,352 --> 00:03:39,234
this data is actively useful and

52
00:03:39,432 --> 00:03:43,730
actually being used to create data applications.

53
00:03:44,310 --> 00:03:47,160
Data analytics for example.

54
00:03:47,530 --> 00:03:50,806
We use all kinds of usage data to be able

55
00:03:50,828 --> 00:03:54,374
to understand better how our customers interact with

56
00:03:54,412 --> 00:03:56,840
our products in order to make them better.

57
00:03:57,530 --> 00:04:00,902
We use business intelligence to

58
00:04:00,956 --> 00:04:04,586
understand how our product behave as well to

59
00:04:04,608 --> 00:04:08,922
make better data driven decisions data

60
00:04:08,976 --> 00:04:12,766
science teams built machine learning models so that

61
00:04:12,788 --> 00:04:16,906
they can recommend the best features

62
00:04:17,018 --> 00:04:21,054
to the customers that they need. Finance team

63
00:04:21,092 --> 00:04:24,754
also need to do forecasting models and

64
00:04:24,792 --> 00:04:27,954
so on. Those are all things

65
00:04:27,992 --> 00:04:31,570
that rely on this kind of data, let's call it data existence,

66
00:04:32,070 --> 00:04:36,218
that's produced about out of those applications.

67
00:04:36,334 --> 00:04:39,938
A data application is also applications. They're simply

68
00:04:40,034 --> 00:04:43,046
focusing on data structure slightly different. So what

69
00:04:43,068 --> 00:04:46,022
is the usual data journey I'm going to use? For example,

70
00:04:46,076 --> 00:04:49,366
building ecommerce app. The primary function of

71
00:04:49,388 --> 00:04:52,890
ecommerce app is to be able to

72
00:04:52,960 --> 00:04:56,730
enable customers to buy and sell by products,

73
00:04:56,800 --> 00:05:00,714
right? They have product catalog, shopping cart, process transactions.

74
00:05:00,842 --> 00:05:04,734
And this kind of data is saved in

75
00:05:04,772 --> 00:05:08,094
systems like events are usually saved by

76
00:05:08,212 --> 00:05:11,934
Kafka or other message queue in the databases and

77
00:05:11,972 --> 00:05:16,170
walks. So if I want to create data applications,

78
00:05:16,330 --> 00:05:19,730
I need to ingest the data. That's what we call data ingestion.

79
00:05:20,150 --> 00:05:23,426
All these kind of data sources needs to be ingested in kind

80
00:05:23,448 --> 00:05:27,240
of analytics system so they can be aggregated, joined to each other.

81
00:05:27,850 --> 00:05:31,186
After the data is ingested, we need to ensure

82
00:05:31,218 --> 00:05:34,838
that it's accurate, consistent, updated. That's why we need to transform it in a

83
00:05:34,844 --> 00:05:38,810
format that other teams now can start use.

84
00:05:38,960 --> 00:05:42,666
So our data applications is going to be also create

85
00:05:42,848 --> 00:05:46,890
another data applications. So we need a good model,

86
00:05:47,040 --> 00:05:50,394
sort of like stable model with quality,

87
00:05:50,512 --> 00:05:54,446
test it and then maybe if

88
00:05:54,468 --> 00:05:58,554
you want to work in ML training models, we need to do feature engineering

89
00:05:58,602 --> 00:06:02,014
data enabling data partitioning with Anatabo data automating to create

90
00:06:02,052 --> 00:06:05,186
our ML object. And this is used by all

91
00:06:05,208 --> 00:06:08,802
kinds of other applications. This could be bi applications to

92
00:06:08,856 --> 00:06:12,980
make data driven decision reports or data driven on other data driven products.

93
00:06:13,510 --> 00:06:16,774
Five recommendation service this

94
00:06:16,892 --> 00:06:20,280
of course relies on a lot of data infrastructure that needs to be built.

95
00:06:20,810 --> 00:06:25,302
And all of that we call data application whole

96
00:06:25,356 --> 00:06:29,902
thing. That's the data journey. The data teams are the one that general responsibility

97
00:06:29,986 --> 00:06:33,034
data applications and some kind of, let's call it infra operations teams,

98
00:06:33,072 --> 00:06:36,906
for instance, is the one that sort of provides infrastructure to the

99
00:06:36,928 --> 00:06:41,290
data teams. But how do multiple applications

100
00:06:41,710 --> 00:06:45,462
communicate between each other? Regardless of talking about data

101
00:06:45,616 --> 00:06:49,134
application API API is the

102
00:06:49,172 --> 00:06:52,480
standard between which any application

103
00:06:53,330 --> 00:06:55,380
communicates between each other.

104
00:06:57,030 --> 00:07:01,086
What are the API components? First, we have interface

105
00:07:01,118 --> 00:07:04,750
and contract that includes employee definitions

106
00:07:04,830 --> 00:07:08,580
and the request and response format. Whatever it is.

107
00:07:10,550 --> 00:07:14,086
This sets how the API is going to be used, right? Of course we need

108
00:07:14,108 --> 00:07:17,122
to make sure that the API is secure authentication mechanism,

109
00:07:17,186 --> 00:07:21,606
so only people who have right to access endpoints

110
00:07:21,638 --> 00:07:24,714
have access. It's important. The API is usable. It's easy,

111
00:07:24,752 --> 00:07:29,034
intuitive, it provides quant libraries and

112
00:07:29,072 --> 00:07:33,534
the features are easy to use, integrate in other applications and

113
00:07:33,572 --> 00:07:37,610
of course it needs to be monitored and operated.

114
00:07:37,770 --> 00:07:41,326
Logging, monitoring track usage, manage traffic, ensure smooth operations are

115
00:07:41,348 --> 00:07:45,074
critical. The same thing goes for API for data,

116
00:07:45,192 --> 00:07:48,734
but one of the main changes is what's

117
00:07:48,782 --> 00:07:52,606
really the usual interface in coderack for an API

118
00:07:52,638 --> 00:07:56,340
for data. Continuing with our ecommerce app example.

119
00:07:56,710 --> 00:08:00,406
So we have two data sources, iotpdbs three service

120
00:08:00,508 --> 00:08:04,086
which contains different types of product information. We need to

121
00:08:04,108 --> 00:08:08,200
ingest this data to generate copy it

122
00:08:08,650 --> 00:08:11,210
into our data lake.

123
00:08:11,630 --> 00:08:15,638
This often lately means databases like Snowflake,

124
00:08:15,734 --> 00:08:19,290
Redshift and so on, not necessarily just

125
00:08:19,440 --> 00:08:23,600
bob storage. And now

126
00:08:23,970 --> 00:08:27,374
we still want to expose more of a process

127
00:08:27,492 --> 00:08:31,982
all combined product view that

128
00:08:32,036 --> 00:08:35,766
we guaranteed contains all the necessary

129
00:08:35,818 --> 00:08:39,010
product data. So let's call it our product data module,

130
00:08:39,350 --> 00:08:42,706
which in this case what is our two data sets? But you can

131
00:08:42,728 --> 00:08:47,394
imagine it could get much more complex. So where

132
00:08:47,432 --> 00:08:51,574
is the API here? Well, with API is communicating between two

133
00:08:51,772 --> 00:08:55,234
applications. We have of course API

134
00:08:55,282 --> 00:08:58,634
between the source applications and our data application and we

135
00:08:58,672 --> 00:09:01,958
need an API between our data application and downstream

136
00:09:02,054 --> 00:09:06,620
applications like BI or other

137
00:09:07,310 --> 00:09:10,782
software applications and services. Let's focus

138
00:09:10,836 --> 00:09:14,350
on the right part. We don't have actually

139
00:09:14,420 --> 00:09:18,846
good API for data between the sources and the raw data.

140
00:09:19,028 --> 00:09:21,854
And how can we build those?

141
00:09:21,972 --> 00:09:25,202
I'll give an example using the right part, but our product

142
00:09:25,256 --> 00:09:29,362
data model, but the same things goes for the

143
00:09:29,416 --> 00:09:31,780
API for data needing on the left part.

144
00:09:32,230 --> 00:09:35,686
So let's say that we want to create this

145
00:09:35,708 --> 00:09:39,590
kind of table or entity of products which contains information

146
00:09:39,740 --> 00:09:44,040
about this.

147
00:09:44,410 --> 00:09:48,454
So what do we consider API of

148
00:09:48,572 --> 00:09:52,710
this kind of product data set? Well, there's schema

149
00:09:52,790 --> 00:09:56,170
very important. For example in this case this could be

150
00:09:56,240 --> 00:09:59,574
the different column names and the types of each column

151
00:09:59,622 --> 00:10:04,030
name are part of the data based on the data schema definitions.

152
00:10:05,730 --> 00:10:08,590
What else? Data semantics.

153
00:10:09,170 --> 00:10:12,670
Each column name has certain semantics like

154
00:10:12,740 --> 00:10:15,874
name being non named, string representing latest official name.

155
00:10:15,992 --> 00:10:18,900
We can notice we need to be as specific as possible.

156
00:10:21,590 --> 00:10:25,134
And finally we need a way to

157
00:10:25,192 --> 00:10:30,406
access it. So data access it

158
00:10:30,428 --> 00:10:34,022
could be tables in database using SQL. We could access

159
00:10:34,076 --> 00:10:37,726
it by using Python enterprise that is setting parquet or arrow

160
00:10:37,778 --> 00:10:41,418
data formats. So if you go

161
00:10:41,504 --> 00:10:45,766
back to our data journey, where do we need API

162
00:10:45,798 --> 00:10:49,110
for data? Where do applications communicate?

163
00:10:49,270 --> 00:10:52,542
So as we establish between the data sources in our main data

164
00:10:52,596 --> 00:10:56,282
application, between our data application output,

165
00:10:56,426 --> 00:10:59,978
be the data module or the module object, and the BI tools

166
00:10:59,994 --> 00:11:03,658
and data driven programs that want to bring any value and insights

167
00:11:03,674 --> 00:11:07,214
out of this data. But we can also of course think

168
00:11:07,252 --> 00:11:10,802
about data application. It's a huge thing and we

169
00:11:10,856 --> 00:11:14,674
probably want to make it into subcomponents. So probably

170
00:11:14,712 --> 00:11:18,086
we need even internal APIs within different components of our

171
00:11:18,108 --> 00:11:21,462
data applications, like between the data

172
00:11:21,516 --> 00:11:24,786
that's being ingested in raw data lake and the dimension module,

173
00:11:24,818 --> 00:11:28,246
or maybe dimension module and the module object that's

174
00:11:28,278 --> 00:11:31,180
being created by training the data.

175
00:11:32,190 --> 00:11:35,558
And this currently is missing. There are no tools,

176
00:11:35,654 --> 00:11:40,018
there are no customs for creating API

177
00:11:40,054 --> 00:11:43,866
for data. And that's one of the biggest gaps

178
00:11:43,978 --> 00:11:48,046
that we have in the data engineering that currently

179
00:11:48,148 --> 00:11:52,026
do not exist generally in the software engineering.

180
00:11:52,218 --> 00:11:55,794
The call for here is that the data sources generally in

181
00:11:55,832 --> 00:11:59,506
control of the application developer. That's if

182
00:11:59,528 --> 00:12:02,962
you are developing any application service, mobile app,

183
00:12:03,016 --> 00:12:06,454
ecommerce app, whatever, you are outputting this kind of data into

184
00:12:06,492 --> 00:12:09,810
different database entries

185
00:12:09,890 --> 00:12:13,366
in your IOTP database, and those things are going to

186
00:12:13,388 --> 00:12:15,960
be used in a data application.

187
00:12:16,810 --> 00:12:20,438
So we need to actually put them under API.

188
00:12:20,534 --> 00:12:23,706
Consider them also can API. What else do we

189
00:12:23,728 --> 00:12:27,306
need for this API? Well, we need to

190
00:12:27,328 --> 00:12:31,210
have service level objectives, right? Metrics which guarantee

191
00:12:31,810 --> 00:12:35,982
certain processes to the API users. And that's where

192
00:12:36,116 --> 00:12:40,622
service agreements also come. Generally those

193
00:12:40,756 --> 00:12:44,178
come from the data semantics, a large point. Once we know the

194
00:12:44,184 --> 00:12:48,274
data semantics, we can create rules and

195
00:12:48,392 --> 00:12:52,050
those rules would calculate our data accuracy seos

196
00:12:52,550 --> 00:12:55,954
then we have other more standard seos in the data

197
00:12:56,072 --> 00:12:58,840
and like availability seos, right.

198
00:12:59,210 --> 00:13:01,430
How often the data is quarrable.

199
00:13:02,730 --> 00:13:06,230
We also of course have freshness sels which is very important

200
00:13:06,380 --> 00:13:09,706
that the data needs to be available in

201
00:13:09,728 --> 00:13:13,418
the analytics system within 1 hour, for example.

202
00:13:13,584 --> 00:13:17,660
It depends. It's important that

203
00:13:19,150 --> 00:13:22,814
our data precious co is the correct one for some

204
00:13:22,932 --> 00:13:26,926
hours is okay, for other seconds is

205
00:13:27,028 --> 00:13:30,830
needed. This APIs for data

206
00:13:30,900 --> 00:13:34,602
SEO for data could be also thought about as

207
00:13:34,676 --> 00:13:38,574
creating a data contract. There is a lot that's

208
00:13:38,622 --> 00:13:42,782
being talked about data contracts currently in the industry. I encourage

209
00:13:42,846 --> 00:13:47,206
everyone to go check out both post by Chad Sanderson on

210
00:13:47,228 --> 00:13:50,486
this link to learn much more about it.

211
00:13:50,668 --> 00:13:54,166
They're pretty great. He dives deeply into

212
00:13:54,188 --> 00:13:57,706
the team and the third part we want to discuss is

213
00:13:57,728 --> 00:14:00,970
DevOps cycle for data. With simplicity,

214
00:14:01,630 --> 00:14:05,322
let's sort of flatten out the DevOps cycle. We know

215
00:14:05,376 --> 00:14:08,954
the standard DevOps cycle is one God, right? The first

216
00:14:08,992 --> 00:14:12,110
path, build, test, release, deploy, operating monitor.

217
00:14:12,450 --> 00:14:15,806
So how do we map the data journey to

218
00:14:15,828 --> 00:14:19,002
the DevOps cycle? It's fairly intuitive,

219
00:14:19,066 --> 00:14:22,046
right? The first thing you need to do is to plan.

220
00:14:22,148 --> 00:14:25,506
So you need to go and discover the data. You need

221
00:14:25,528 --> 00:14:29,602
to go and explore the data, find where it is,

222
00:14:29,656 --> 00:14:32,946
find for example where the product info is kept. Is it applying s three

223
00:14:32,968 --> 00:14:36,182
is it capping database and try to access it.

224
00:14:36,236 --> 00:14:40,470
Then you need to ingest it, actually core the ingestions,

225
00:14:41,290 --> 00:14:44,822
build the data ingestion pipelines and

226
00:14:44,876 --> 00:14:48,886
probably transform it. Transformation log this is another type of core

227
00:14:48,908 --> 00:14:52,402
information log needs to be encoded, usually using SQL

228
00:14:52,466 --> 00:14:56,198
or Python or something like this and built in data pipeline.

229
00:14:56,294 --> 00:15:00,858
Those roughly go into the blank code in build stages

230
00:15:01,034 --> 00:15:04,510
or generally tasks that you do during those stages.

231
00:15:05,010 --> 00:15:08,510
Once the data has been stored, ingest and transform.

232
00:15:09,010 --> 00:15:12,494
It's ready to be deployed for using reports and

233
00:15:12,532 --> 00:15:15,962
it data visible machine learning modules or other applications,

234
00:15:16,106 --> 00:15:19,938
be it data applications or normal software applications. So we

235
00:15:19,944 --> 00:15:23,714
had the whole depth wall. We need to test

236
00:15:23,752 --> 00:15:27,878
the data or release it and

237
00:15:28,044 --> 00:15:33,350
deploy it similarly as we do in the data DevOps

238
00:15:33,850 --> 00:15:37,442
cycle. And of course we need to maintain it.

239
00:15:37,596 --> 00:15:40,774
It must be consistently managed to ensure it stays accurate,

240
00:15:40,822 --> 00:15:44,374
secure and available, similar to how applications

241
00:15:44,422 --> 00:15:47,546
operate. Monitoring DevOps environment let's call

242
00:15:47,568 --> 00:15:51,726
this managed data. Now that we sort of covered all

243
00:15:51,748 --> 00:15:55,402
the aspects of how do we apply DevOps for data API,

244
00:15:55,466 --> 00:15:58,798
SE World and DevOps cycle, I want

245
00:15:58,804 --> 00:16:02,382
to introduce wastal data kit. It's an open

246
00:16:02,436 --> 00:16:06,670
source framework which provides solution

247
00:16:07,330 --> 00:16:10,734
for users to have self service environment for data engineers

248
00:16:10,782 --> 00:16:13,474
to create end to end data part points using this kind of code,

249
00:16:13,512 --> 00:16:16,760
first decentralized, fully automated way.

250
00:16:17,210 --> 00:16:21,782
Its focus is more on the DevOps part and

251
00:16:21,836 --> 00:16:25,266
the data journey part. But let's

252
00:16:25,298 --> 00:16:29,238
see. So it provides really two things. One is the

253
00:16:29,324 --> 00:16:33,500
SDK or a framework almost. You can figure the spring for data

254
00:16:33,950 --> 00:16:37,750
which you can allow you to develop data jobs using iterpython

255
00:16:37,830 --> 00:16:41,690
so you have some methods to extract raw data or secure.

256
00:16:42,050 --> 00:16:46,350
You can do things like parameterized transformations

257
00:16:47,090 --> 00:16:51,022
and it allows us to deploy monitor using

258
00:16:51,076 --> 00:16:53,490
control service and operations UI.

259
00:16:54,870 --> 00:16:58,740
This data and data applications that you're building,

260
00:16:59,670 --> 00:17:03,902
we call them here data jobs. Looking at the data journey

261
00:17:04,046 --> 00:17:07,762
versatile data kit ultimately fits the ingestion and transform

262
00:17:07,826 --> 00:17:12,054
part. It can be used also to train data modules and

263
00:17:12,092 --> 00:17:14,920
generally you can use to export data in itself.

264
00:17:15,610 --> 00:17:19,530
Constell data kit is a framework, so it allows you to write

265
00:17:19,600 --> 00:17:22,602
the code that you need to ingest, transform, train,

266
00:17:22,656 --> 00:17:26,374
export the data and integrates

267
00:17:26,422 --> 00:17:30,022
with different databases which can

268
00:17:30,096 --> 00:17:33,402
like Snowflake, empower, postgres,

269
00:17:33,466 --> 00:17:37,098
MySQL or different compute

270
00:17:37,114 --> 00:17:41,674
engines. You can use like Sparklink

271
00:17:41,802 --> 00:17:45,438
to actually make the

272
00:17:45,524 --> 00:17:48,782
heavy lift computation a water focus

273
00:17:48,836 --> 00:17:52,046
has been meant to simplify the heights complexity of data infrastructure

274
00:17:52,078 --> 00:17:55,960
and generate DevOps for the data team as much as possible.

275
00:17:56,490 --> 00:18:00,360
So it allows you to basically plug in

276
00:18:00,890 --> 00:18:04,322
between data infrastructure and the data applications,

277
00:18:04,466 --> 00:18:08,122
allowing a lot of control both over the DevOps cycle and over

278
00:18:08,176 --> 00:18:11,994
the monitoring aspects and the infrastructure to

279
00:18:12,032 --> 00:18:15,498
the sort of infra operations team. So the data teams can focus

280
00:18:15,584 --> 00:18:18,806
only on data and the overall watch mount

281
00:18:18,838 --> 00:18:22,286
about the software part. So for example, let's show a

282
00:18:22,308 --> 00:18:25,886
quick demo of what I mean. Let's say that we have

283
00:18:25,908 --> 00:18:29,262
this kind of databases and the data teams are developing their data

284
00:18:29,316 --> 00:18:32,970
application, running SQL queries and maybe they're

285
00:18:33,050 --> 00:18:36,210
running such big SQL queries that are breaking the database.

286
00:18:36,870 --> 00:18:39,060
That's it. So what do we do?

287
00:18:41,430 --> 00:18:44,980
We have to either block the data team or ask them to stop.

288
00:18:45,590 --> 00:18:49,202
Would be much nicer if instead of finding out when those

289
00:18:49,256 --> 00:18:52,566
queries land production, we find that you can give up a time as

290
00:18:52,588 --> 00:18:57,118
early as possible. And that's what you can do with Versaille data git.

291
00:18:57,314 --> 00:19:01,254
Let's say the data team is here developing

292
00:19:01,302 --> 00:19:05,850
their job, running this kind of SQL queries and accounts from employees

293
00:19:06,270 --> 00:19:09,386
and different, using different, all kinds of

294
00:19:09,408 --> 00:19:13,462
different methods. What completely data teams

295
00:19:13,526 --> 00:19:17,518
without even knowing? Of course you will know. The platform

296
00:19:17,604 --> 00:19:20,858
team or operation team or data center team could evolve

297
00:19:20,874 --> 00:19:24,800
this kind of VDK query validation. It could be as simple as

298
00:19:25,330 --> 00:19:28,706
compare the size of a query or it could be much more complex by doing

299
00:19:28,728 --> 00:19:33,262
some kind of analytics on the query. It doesn't matter because VDK

300
00:19:33,326 --> 00:19:37,026
allows you to intercept both the data and the metadata

301
00:19:37,218 --> 00:19:41,506
in plugins. You can intercept each query statement

302
00:19:41,698 --> 00:19:44,310
and decide what to do with it through plugin.

303
00:19:45,370 --> 00:19:49,074
In this case, let's say that we reject all queries

304
00:19:49,122 --> 00:19:54,780
bigger than 1000 expressions or 1.

305
00:19:55,710 --> 00:19:59,754
While the data user and data team is developing their

306
00:19:59,792 --> 00:20:03,014
job locally, they'll complete the error immediately.

307
00:20:03,142 --> 00:20:06,378
The query wouldn't leave their work

308
00:20:06,464 --> 00:20:09,806
environment. I don't think there's a

309
00:20:09,828 --> 00:20:13,220
huge benefit to be able to do this very quickly.

310
00:20:13,830 --> 00:20:17,102
This type of query

311
00:20:17,166 --> 00:20:20,610
validation can be used to enforce data

312
00:20:20,680 --> 00:20:25,498
quality rules and data APIs.

313
00:20:25,614 --> 00:20:29,142
Basically as early as possible at the data,

314
00:20:29,276 --> 00:20:33,106
at the query, when the query is being developed,

315
00:20:33,298 --> 00:20:37,442
basically the data model is being created and that's pretty

316
00:20:37,516 --> 00:20:41,420
powerful. Now let's look at more of the

317
00:20:42,350 --> 00:20:46,460
operations and monitoring and deployment part,

318
00:20:47,150 --> 00:20:52,626
not just in the developer part. So how can we help with the deployment

319
00:20:52,678 --> 00:20:56,590
part? So yeah, let's go back to our DevOps cycle.

320
00:20:57,090 --> 00:21:01,022
What we want to do is enabling fruit operators or centralized data team

321
00:21:01,156 --> 00:21:05,598
to sort of have this kind of more control over

322
00:21:05,764 --> 00:21:08,946
how the data applications, the data jobs are

323
00:21:08,968 --> 00:21:12,286
being built and even tested, how they are being released and deployed.

324
00:21:12,318 --> 00:21:15,478
Because there are a lot of concerns. You have to think about version, you have

325
00:21:15,484 --> 00:21:19,826
to be containerization, you have to think about adding

326
00:21:19,858 --> 00:21:23,778
metrics, metadata and so on, creating docker images,

327
00:21:23,874 --> 00:21:30,086
creating cron jobs or kubernetes objects.

328
00:21:30,278 --> 00:21:33,894
This all should be completely hidden

329
00:21:33,942 --> 00:21:37,594
out for data engineers and data user data because you

330
00:21:37,632 --> 00:21:41,494
want them to focus on core data modeling and applying

331
00:21:41,542 --> 00:21:45,406
business inside, applying business transformation insights on

332
00:21:45,428 --> 00:21:49,226
the data and not to worry about all these kind of software engineering

333
00:21:49,258 --> 00:21:52,986
concerns, DevOps concerns, and that's

334
00:21:53,178 --> 00:21:56,266
very important for them. So data teams

335
00:21:56,298 --> 00:21:59,426
focus on planning, coding, data application. Of course they need some way

336
00:21:59,448 --> 00:22:02,946
to monitor their data while the let's call

337
00:22:02,968 --> 00:22:07,134
it it team or whatever, this small DevOps team can establish both

338
00:22:07,192 --> 00:22:10,806
policies through this kind of

339
00:22:10,828 --> 00:22:14,598
extensibility mechanism the VDK provides. What policy?

340
00:22:14,764 --> 00:22:18,600
Well, let's look at example, very simple example

341
00:22:19,210 --> 00:22:22,890
the way by default VDK also install

342
00:22:22,960 --> 00:22:26,442
a kind of dependency package, the data team job ready

343
00:22:26,496 --> 00:22:30,070
for automatic executions in the cloud environment.

344
00:22:30,230 --> 00:22:33,886
In VDK case it's using kubernetes. But let's say

345
00:22:33,908 --> 00:22:37,710
that we want to make sure that we add some centralized system tests

346
00:22:38,610 --> 00:22:41,806
for all jobs that verify certain quality or

347
00:22:41,828 --> 00:22:45,790
super API contract or center metrics.

348
00:22:46,210 --> 00:22:49,778
We can do this very simplified everything. The way plugins are built in the

349
00:22:49,784 --> 00:22:53,342
control service are through docker

350
00:22:53,486 --> 00:22:57,006
images. Basically we can extend a style

351
00:22:57,038 --> 00:23:00,566
data into a builder document. Anything you can

352
00:23:00,588 --> 00:23:04,194
run system teams, you can remove execution privileges.

353
00:23:04,242 --> 00:23:07,818
And this script is being run during the

354
00:23:07,824 --> 00:23:10,806
build and test phase. So before anything is released and deployed,

355
00:23:10,998 --> 00:23:13,500
after the job is being built,

356
00:23:13,950 --> 00:23:19,340
they can run this kind of test and change

357
00:23:19,890 --> 00:23:24,238
the job scripts by this way guaranteeing certain

358
00:23:24,324 --> 00:23:27,386
level of quality. So if the system test failed,

359
00:23:27,418 --> 00:23:31,498
the job wouldn't be deployed and security by removing

360
00:23:31,594 --> 00:23:35,422
unnecessary permissions. In summary,

361
00:23:35,486 --> 00:23:39,220
what do we think about by DevOps for data?

362
00:23:39,670 --> 00:23:43,586
Well, one spec is API for data, right? API for

363
00:23:43,608 --> 00:23:47,358
data means we need data enabling, we need data semantics defined.

364
00:23:47,454 --> 00:23:51,094
We need some kind of validations and documentation that people can

365
00:23:51,132 --> 00:23:54,610
explore the data, right? They need to be able to access it easily,

366
00:23:54,770 --> 00:23:58,706
find out where it is, explore it's

367
00:23:58,738 --> 00:24:02,134
best well documented, the same normal API, we need

368
00:24:02,172 --> 00:24:04,220
seos and teams for data.

369
00:24:04,750 --> 00:24:07,980
So we need to be able to collect metrics about the data,

370
00:24:08,510 --> 00:24:12,314
things like freshness, things like unique

371
00:24:12,362 --> 00:24:16,270
values and so on, and what if metrics.

372
00:24:16,850 --> 00:24:20,430
And we need the data to follow the

373
00:24:20,500 --> 00:24:24,580
DevOps cycle. Basically the whole pipeline from

374
00:24:25,190 --> 00:24:29,300
sources to insights should be automated using the

375
00:24:30,070 --> 00:24:33,534
best DevOps practices, which means ability

376
00:24:33,582 --> 00:24:37,526
to plan code, ingestion, transformation logic, ability to

377
00:24:37,548 --> 00:24:41,480
deploy that logic and to manage it to operate it.

378
00:24:41,850 --> 00:24:45,062
Versatile data kit actively helped in a lot of those,

379
00:24:45,196 --> 00:24:48,482
especially in the DevOps data part. It also through plugins

380
00:24:48,546 --> 00:24:50,390
could help, as we discovered,

381
00:24:51,290 --> 00:24:54,120
to create can enforced API for data.

382
00:24:55,870 --> 00:24:59,306
This is really direction that we need a lot of more research and a lot

383
00:24:59,328 --> 00:25:02,510
of more work to do in Versaille data kit.

384
00:25:03,010 --> 00:25:06,702
So I will be really helpful if anything that you find

385
00:25:06,756 --> 00:25:10,986
interesting here. If you'd

386
00:25:11,018 --> 00:25:14,660
like to learn more, just get in contact with us.

387
00:25:15,750 --> 00:25:19,570
Yeah, you can do this by going to the gift of repo and starting

388
00:25:19,640 --> 00:25:24,014
a discussion, contacting our swap

389
00:25:24,062 --> 00:25:28,010
channel, writing an email or contacting

390
00:25:28,030 --> 00:25:32,280
us on LinkedIn. And there are a lot of work

391
00:25:32,890 --> 00:25:36,210
to make sure that data engineering practices

392
00:25:36,290 --> 00:25:39,846
are able to adapt and adopt good staff engineer and

393
00:25:39,868 --> 00:25:43,850
DevOps practices. A lot of this was discussed is what

394
00:25:43,920 --> 00:25:47,562
it would be nice for staff engineer to have already and it's still

395
00:25:47,616 --> 00:25:50,986
missing dollars which were started. We tried to bridge

396
00:25:51,018 --> 00:25:54,720
the gap. We know that we have a lot of work to do

397
00:25:55,170 --> 00:25:59,326
and your input, anybody's input will

398
00:25:59,348 --> 00:26:03,422
be extremely valuable. I hope

399
00:26:03,556 --> 00:26:06,958
you get in contact with us and thank you again

400
00:26:07,044 --> 00:26:11,120
for listening to these 30 minutes of me talking.

401
00:26:11,810 --> 00:26:15,574
And have a nice day or night or

402
00:26:15,612 --> 00:26:17,860
whichever part of your.

