1
00:00:25,490 --> 00:00:29,330
Good day everyone. I'm Ranjan Mohan, a senior software developer at Alice,

2
00:00:29,410 --> 00:00:32,786
and today this session will be about best practices

3
00:00:32,898 --> 00:00:36,406
with respect to python development. A few things before we get into

4
00:00:36,428 --> 00:00:40,134
the session. As such, I'll be using a few metrics to highlight as to

5
00:00:40,172 --> 00:00:43,926
why certain suggestions are better, and those metrics should be taken with a pinch of

6
00:00:43,948 --> 00:00:47,830
salt, simply because metrics like the execution time

7
00:00:47,980 --> 00:00:51,598
haven't excluded the the thread sleep time, as in the

8
00:00:51,604 --> 00:00:55,118
time taken to context switch in the CPU. So that's the

9
00:00:55,124 --> 00:00:58,766
eventual goal, to exclude that in our measurements in a future iteration of

10
00:00:58,788 --> 00:01:02,314
this, but as of now it doesn't. But because of the steep

11
00:01:02,362 --> 00:01:05,678
gap in the time difference, we work under the assumption that it's

12
00:01:05,694 --> 00:01:09,074
pretty evident that the context switch time is not

13
00:01:09,112 --> 00:01:13,214
playing a big role here, simply because I'm not running anything CPU intensive,

14
00:01:13,342 --> 00:01:17,250
and the fact that the time taken to basically run that is

15
00:01:17,320 --> 00:01:21,046
pretty close to the overall thread execution time. The second thing over

16
00:01:21,068 --> 00:01:24,406
here is this is a fragment of my experience, and I

17
00:01:24,428 --> 00:01:28,290
have only covered best practices that I have resonated with closely.

18
00:01:28,370 --> 00:01:31,810
So it's not the whole list altogether, but I've just

19
00:01:31,900 --> 00:01:34,874
cherry picked a few and would like to share it to you over the next

20
00:01:34,912 --> 00:01:38,250
few minutes. So let's get into the session within the general

21
00:01:38,320 --> 00:01:41,974
section. The first suggestion I have is to use built in methods.

22
00:01:42,102 --> 00:01:45,226
Here we have a collection numbers which contains

23
00:01:45,258 --> 00:01:49,294
values from 100 to zero, as in 199, so on

24
00:01:49,412 --> 00:01:52,974
all the way till one, right before zero, because one is zero is

25
00:01:53,012 --> 00:01:57,018
excluded, followed by squared numbers, which basically should store

26
00:01:57,124 --> 00:02:00,466
all the squared values of the numbers over here.

27
00:02:00,568 --> 00:02:03,746
So we could use a for loop for it. But we have chosen to use

28
00:02:03,768 --> 00:02:07,666
a map which is an inbuilt function, and we give it a lambda to say

29
00:02:07,688 --> 00:02:11,346
hey, apply this lambda for all numbers within the numbers collection.

30
00:02:11,458 --> 00:02:14,866
So this lambda takes in a single input, squares it, and returns

31
00:02:14,898 --> 00:02:18,038
that as the output. Similarly, when we want to get the sum of the

32
00:02:18,044 --> 00:02:21,218
squared numbers, we just make use of the sum function, and when we

33
00:02:21,244 --> 00:02:24,426
want to find the product of all the squared numbers, we use a reduce with

34
00:02:24,448 --> 00:02:27,866
a lambda that multiplies the two inputs that it has.

35
00:02:27,968 --> 00:02:31,066
So this lambda on consecutive application to the

36
00:02:31,088 --> 00:02:34,334
collection of squared numbers will continuously reduce it one

37
00:02:34,372 --> 00:02:37,518
by one, and eventually at the end, it'll reduce it to a single value,

38
00:02:37,604 --> 00:02:40,926
which ends up being the product of all the squared numbers.

39
00:02:41,108 --> 00:02:44,954
So like I said before, all these goals

40
00:02:45,002 --> 00:02:48,366
can be achieved using for loops. But there are two key reasons

41
00:02:48,398 --> 00:02:51,950
why using inbuilt functions would be much better for these scenarios.

42
00:02:52,030 --> 00:02:55,426
Number one is basically, most of the times it offers a very good

43
00:02:55,448 --> 00:02:58,958
performance boost, especially when you're dealing with a large collection of

44
00:02:58,984 --> 00:03:02,562
numbers or values. The second thing is it simplifies

45
00:03:02,626 --> 00:03:05,798
the code and makes it more elegant, as opposed to a for loop where you

46
00:03:05,804 --> 00:03:09,046
need to maintain variable names and add other references and so

47
00:03:09,068 --> 00:03:13,206
on. Using a map or a sum or a reduce clearly

48
00:03:13,398 --> 00:03:17,194
simplifies as to what operation you're planning to do. And it

49
00:03:17,232 --> 00:03:20,874
also takes care of efficiency like point number one. So those are the two main

50
00:03:20,912 --> 00:03:24,522
reasons. Number one, efficiency in terms of performance memory and

51
00:03:24,576 --> 00:03:28,410
cpus. Number two is because of the fact that it generates

52
00:03:28,490 --> 00:03:31,646
cleaner code when you use inbuilt functions, as opposed to

53
00:03:31,668 --> 00:03:35,506
trying to reinvent the wheel. Now, the second suggestion over here is to

54
00:03:35,528 --> 00:03:39,246
fail fast. Consider the requirement where I need to convert

55
00:03:39,358 --> 00:03:42,866
years into months and the number of years is the

56
00:03:42,888 --> 00:03:46,098
input over here. One approach I can take is I can check

57
00:03:46,184 --> 00:03:49,670
for the success or the valid criteria to see if it's a valid year.

58
00:03:49,740 --> 00:03:53,586
Go ahead and compute the number of months and return it. Else throw an exception

59
00:03:53,618 --> 00:03:56,658
and fail. That's basically the fail late solution,

60
00:03:56,754 --> 00:03:59,954
because you're checking for the validity and computing.

61
00:04:00,002 --> 00:04:03,834
And if that's not the case, then you're failing. So you're failing later,

62
00:04:03,952 --> 00:04:07,786
as opposed to you checking for the invalid condition first. And if

63
00:04:07,808 --> 00:04:11,706
that's true, you're failing first, else you're basically computing the

64
00:04:11,728 --> 00:04:15,834
necessary output. Let's try and run these methods,

65
00:04:15,882 --> 00:04:19,594
both these methods for your values that are invalid from minus

66
00:04:19,642 --> 00:04:23,070
n to minus one. So it runs from -100 to minus two,

67
00:04:23,220 --> 00:04:26,954
and I'm going to run it for both and over 1000

68
00:04:27,012 --> 00:04:30,514
times. I'm sorry, it's from -1000 to minus one.

69
00:04:30,552 --> 00:04:33,806
So it goes from -1000 to minus two. So I'm

70
00:04:33,838 --> 00:04:37,394
going to basically run it for all those values and then find the average execution

71
00:04:37,442 --> 00:04:40,690
time for it. Let's see what is more efficient,

72
00:04:40,770 --> 00:04:44,054
failing first or failing last. As you can see over

73
00:04:44,092 --> 00:04:47,314
here, failing first is about zero, zero, five milliseconds,

74
00:04:47,362 --> 00:04:50,550
whereas failing late is about 00:13

75
00:04:50,710 --> 00:04:54,490
so there's a clearly more than a 50% cut

76
00:04:54,560 --> 00:04:57,914
in the time taken when you're failing fast. So this is also

77
00:04:57,952 --> 00:05:01,626
under the premise that the condition that you use to check a

78
00:05:01,648 --> 00:05:04,730
failure criteria and a success criteria and a valid criteria

79
00:05:04,810 --> 00:05:08,094
pretty much takes the same time. If the conditions that you use to

80
00:05:08,132 --> 00:05:11,934
evaluate success or a valid criteria takes way more, it's way

81
00:05:11,972 --> 00:05:15,738
less time than the one makes to evaluate a failure criteria,

82
00:05:15,834 --> 00:05:19,454
then you'll obviously see the vice versa. So this is under the assumption

83
00:05:19,502 --> 00:05:23,618
that the conditions to check for a failure, as well as a validity criteria take

84
00:05:23,704 --> 00:05:27,014
the same amount of resources in terms of cpu time and

85
00:05:27,052 --> 00:05:30,566
memory. Now the third suggestion over here

86
00:05:30,668 --> 00:05:33,586
is to import only when necessary.

87
00:05:33,698 --> 00:05:37,414
I have a baseline function here which does absolutely nothing, it just

88
00:05:37,452 --> 00:05:40,546
returns. And I have another function which imports two commonly

89
00:05:40,578 --> 00:05:44,218
used libraries, numpy and URL lib. So what I'm going to do is I'm going

90
00:05:44,224 --> 00:05:47,674
to try calling both those functions, and I'm going to try timing it to see

91
00:05:47,712 --> 00:05:51,030
how much time it takes. As you can see, the baseline

92
00:05:51,110 --> 00:05:54,606
function did practically nothing, and I think the execution time might have been

93
00:05:54,628 --> 00:05:58,766
in microseconds, so it rounded off to zero milliseconds. But on

94
00:05:58,788 --> 00:06:03,178
the other hand, the import call function took about 264 milliseconds.

95
00:06:03,274 --> 00:06:07,010
Now imagine a scenario where you're calling a function which calls another function

96
00:06:07,080 --> 00:06:10,238
which calls another function, another file, and so on. For each file

97
00:06:10,254 --> 00:06:13,474
it accesses, if it has a global import that is

98
00:06:13,512 --> 00:06:16,594
right on top over here that imports things that are not

99
00:06:16,632 --> 00:06:20,578
needed for the function being called, it keeps coding several hundreds of milliseconds

100
00:06:20,594 --> 00:06:24,182
to the runtime, and that can end up being your performance bottleneck. It can add

101
00:06:24,236 --> 00:06:28,262
few seconds to your overall execution time. So one

102
00:06:28,316 --> 00:06:31,862
thing to notice, especially for expensive imports,

103
00:06:32,006 --> 00:06:35,654
please do it only when necessary. I understand there is a trade

104
00:06:35,702 --> 00:06:39,254
off, as in the convenience of managing all imports here is lost,

105
00:06:39,382 --> 00:06:42,438
but with the convenience of ides where you can refactoring

106
00:06:42,534 --> 00:06:46,458
analyze things. And a lot of these tools which you can use to analyze dependencies,

107
00:06:46,554 --> 00:06:49,646
this becomes a bit of a moot point to maintain it on

108
00:06:49,668 --> 00:06:53,134
the top. So for expensive imports, use it only

109
00:06:53,172 --> 00:06:56,610
in the context where it's needed, be it within a method, within a class,

110
00:06:56,680 --> 00:06:59,954
within a code block. That's for you to decide. But that

111
00:06:59,992 --> 00:07:03,746
would end up saving a few seconds of runtime, if not 100

112
00:07:03,768 --> 00:07:07,474
milliseconds of runtime at least. Now the fourth suggestion over

113
00:07:07,512 --> 00:07:11,346
here is to use caches for methods where for a unique

114
00:07:11,378 --> 00:07:15,090
sequence of input, you have a very, very repetitive, specific output,

115
00:07:15,170 --> 00:07:18,278
right? The output is not going to change if the input is the same.

116
00:07:18,364 --> 00:07:21,946
So for such scenarios, it would be wise to use a cache as in

117
00:07:21,968 --> 00:07:25,306
what the cache does is it maps the input that

118
00:07:25,328 --> 00:07:28,826
was previously called to the output it generated. So the next time, if the

119
00:07:28,848 --> 00:07:32,186
same input is supplied and the method is called, it returns the same output from

120
00:07:32,208 --> 00:07:35,710
the cache. It doesn't need to do the computation again. So for this

121
00:07:35,780 --> 00:07:38,830
we're basically trying to compute the sum of the first n

122
00:07:38,900 --> 00:07:42,126
Fibonacci sequence numbers and we're doing it

123
00:07:42,148 --> 00:07:45,674
using recursion over here. And both of the method logics

124
00:07:45,722 --> 00:07:49,006
are the same, except that the first method doesn't use a cache,

125
00:07:49,118 --> 00:07:52,446
whereas the second method uses an LRU cache, and it stores

126
00:07:52,478 --> 00:07:56,318
up to 128 elements, as in 128 unique combinations of inputs

127
00:07:56,334 --> 00:07:59,782
and outputs. So what an LRU stands for is a least

128
00:07:59,836 --> 00:08:03,970
recently used when the cache becomes full, it evics entries

129
00:08:04,050 --> 00:08:07,606
that are not used recently. So as long as the

130
00:08:07,628 --> 00:08:10,826
entry in the cache hasn't been used recently, it evicts it. That is

131
00:08:10,848 --> 00:08:14,474
the strategy of this particular caching technique. So let's try running both

132
00:08:14,512 --> 00:08:18,314
of these and try to see what the performance gain in terms

133
00:08:18,352 --> 00:08:22,406
of execution time is. So as you can see, with the LRU cache,

134
00:08:22,438 --> 00:08:25,866
it's pretty much close to zero. We don't see any noticeable increase

135
00:08:25,898 --> 00:08:28,506
in time as the number of terms to some increase,

136
00:08:28,618 --> 00:08:32,366
whereas for the method without a cache, we see

137
00:08:32,388 --> 00:08:35,822
it exponentially rising. So as you can see, this is a scenario

138
00:08:35,886 --> 00:08:38,658
where using an LRU cache, or any cache for that matter,

139
00:08:38,744 --> 00:08:42,286
would significantly help. But one thing to notice,

140
00:08:42,398 --> 00:08:45,934
in such scenarios where there's for the same input,

141
00:08:45,982 --> 00:08:49,334
you get the same output. I wouldn't recommend using cache only for

142
00:08:49,372 --> 00:08:52,646
one one exception only if the input objects are very,

143
00:08:52,668 --> 00:08:56,230
very expensive memory wise, because in that particular

144
00:08:56,300 --> 00:08:59,494
scenario, or that case, your cache can end

145
00:08:59,532 --> 00:09:02,886
up occupying a lot of your memory and reduce the amount of free memory

146
00:09:02,918 --> 00:09:06,474
available for other parts of the code to execute. So that is the only

147
00:09:06,512 --> 00:09:09,866
caution you need to exercise. But in scenarios like this, where the

148
00:09:09,888 --> 00:09:13,766
method just takes in a measly integer value, you could jolly

149
00:09:13,798 --> 00:09:17,626
well use a cache to improve performance by several manifolds.

150
00:09:17,738 --> 00:09:21,418
Now that we're done with the general category, let's move on to the loops category,

151
00:09:21,514 --> 00:09:25,702
where let's compare the performance between loops, list comprehensions,

152
00:09:25,786 --> 00:09:29,346
and a map. So the test code over here

153
00:09:29,368 --> 00:09:33,122
is basically trying to accept a list of numbers, and trying to return a list

154
00:09:33,176 --> 00:09:36,322
which contains the same numbers, but the square of the numbers,

155
00:09:36,456 --> 00:09:39,846
not the exact same numbers as such, but the square of all the numbers that

156
00:09:39,868 --> 00:09:43,030
have been passed as the input. So when I run this,

157
00:09:43,180 --> 00:09:46,578
I'm basically trying to call all three methods

158
00:09:46,674 --> 00:09:50,646
for values between zero and 1 million with increments of 10,000.

159
00:09:50,748 --> 00:09:53,818
So it's going to take a bit of time for the execution to complete,

160
00:09:53,904 --> 00:09:57,866
but we'll get a very good idea as to how the performance of all

161
00:09:57,888 --> 00:10:01,258
three approaches for the same problem, or the same goal that we

162
00:10:01,264 --> 00:10:04,366
need to achieve is like. So as you

163
00:10:04,388 --> 00:10:08,254
can see here, list comprehension is the clear winner. It uses much

164
00:10:08,292 --> 00:10:11,646
lesser time than any of them, especially when the size of the

165
00:10:11,668 --> 00:10:14,986
input increases dramatically, whereas loop is somewhere

166
00:10:15,018 --> 00:10:17,666
in between and map ends up using most of the time.

167
00:10:17,768 --> 00:10:21,278
So one thing I would strongly recommend here is even if map ends

168
00:10:21,294 --> 00:10:24,894
up using a bit more time in the order of a few tens of milliseconds,

169
00:10:24,942 --> 00:10:28,294
I would still consider using map more than list for

170
00:10:28,332 --> 00:10:31,862
loop, simply because of the fact that it's cleaner code. And over

171
00:10:31,916 --> 00:10:35,702
time, especially for very large inputs, that's when you really start seeing

172
00:10:35,756 --> 00:10:38,978
the efficacy of map as opposed to a for loop.

173
00:10:39,074 --> 00:10:42,682
And over here, list comprehension is the clear winner for simple

174
00:10:42,736 --> 00:10:47,398
scenarios like this. Go ahead and knock yourselves out with list comprehension.

175
00:10:47,494 --> 00:10:51,166
But when you start generating in a much more complicated manner with

176
00:10:51,268 --> 00:10:54,862
several conditions or multiple objects, then reading list

177
00:10:54,916 --> 00:10:58,542
comprehension is not very developer friendly, so such

178
00:10:58,596 --> 00:11:02,394
code can actually end up adding more cognitive complexity

179
00:11:02,442 --> 00:11:05,614
in terms of the developer to take time to understand. So it would make more

180
00:11:05,652 --> 00:11:09,522
sense to add a map or even a loop at some point.

181
00:11:09,656 --> 00:11:12,946
So performance wise, list comprehension is the winner, even though for

182
00:11:12,968 --> 00:11:16,642
loop gives you a better performance overhead, up to 1 million numbers. I would

183
00:11:16,696 --> 00:11:20,386
personally still prefer using map simply because it generates cleaner

184
00:11:20,418 --> 00:11:23,634
code and it generates a much better performance throughput

185
00:11:23,682 --> 00:11:27,314
at a much, much higher scale. Now that we're done with the loop section,

186
00:11:27,362 --> 00:11:29,290
let's move on to the string section.

187
00:11:30,590 --> 00:11:34,474
Any PR where I see string concatenation about 90%

188
00:11:34,512 --> 00:11:37,830
of the time, I see a comment that says use join,

189
00:11:37,910 --> 00:11:40,938
don't use plus. And there's a very, very good reason why.

190
00:11:41,024 --> 00:11:44,666
Let's take a look. There's a method over here which uses plus to concatenate

191
00:11:44,698 --> 00:11:48,250
a given list of strings. There's another method over here which uses

192
00:11:48,330 --> 00:11:52,462
join to concatenate the given list of strings. Let's try running

193
00:11:52,516 --> 00:11:56,434
this across sequence of lists where the first list is

194
00:11:56,472 --> 00:11:59,506
basically about one value, second list is two values, and so on,

195
00:11:59,528 --> 00:12:03,346
up to 1000 values. And see how the performance difference is when we

196
00:12:03,368 --> 00:12:07,150
use concat, as in the plus operator versus join.

197
00:12:07,310 --> 00:12:10,998
So if you see the execution time for using plus is a

198
00:12:11,004 --> 00:12:14,342
bit erratic. It goes up, goes down, goes up, goes down, goes up, goes down,

199
00:12:14,396 --> 00:12:18,454
and as. And when we move across larger number of strings,

200
00:12:18,582 --> 00:12:22,118
the trend of the execution time increases,

201
00:12:22,294 --> 00:12:26,614
whereas while using join, it's almost close to zero, it's not even observable.

202
00:12:26,742 --> 00:12:30,454
So that parallelizes it makes it way more efficient

203
00:12:30,502 --> 00:12:34,346
in terms of performance. And that is the preferred

204
00:12:34,458 --> 00:12:37,438
method of concatenating strings as opposed to plus,

205
00:12:37,604 --> 00:12:40,718
for obvious reasons, as we can see, now that we are done with the

206
00:12:40,724 --> 00:12:43,550
string section, let's move on to the collection section.

207
00:12:43,890 --> 00:12:47,618
So in this case, whenever we talk about maintaining a

208
00:12:47,624 --> 00:12:51,218
dictionary for a particular reason, we might have to initialize values that

209
00:12:51,224 --> 00:12:54,754
are not present inside it. So assuming we want to add a fruit or

210
00:12:54,792 --> 00:12:58,318
keep a dictionary that keeps track of the number of fruits,

211
00:12:58,414 --> 00:13:01,414
and when we say add a fruit and we pass a fruit argument, it should

212
00:13:01,452 --> 00:13:04,630
check if the fruit is there in the dictionary. If it is not there,

213
00:13:04,700 --> 00:13:08,486
you basically initialize it to zero and then increment the value by one.

214
00:13:08,588 --> 00:13:11,494
So this keeps track of fruits that have been added.

215
00:13:11,622 --> 00:13:14,778
This is the primitive approach to go about it.

216
00:13:14,864 --> 00:13:17,994
But one of the more elegant approaches, and not only

217
00:13:18,032 --> 00:13:21,542
elegant, but even a performance gain approach,

218
00:13:21,606 --> 00:13:25,422
is to use a get function within the dictionary. So what it does is

219
00:13:25,556 --> 00:13:29,066
it tries to get the value of fruit. If fruit doesn't exist, it returns

220
00:13:29,098 --> 00:13:32,394
the default value that we are specified, which is zero, and then it increments

221
00:13:32,442 --> 00:13:35,886
it by one. So by doing such a thing, we're not only finishing it

222
00:13:35,908 --> 00:13:39,598
elegantly within a line, we're also gaining performance. So that's what we're

223
00:13:39,614 --> 00:13:42,942
going to test over here. So there are two operations that I'm testing.

224
00:13:43,006 --> 00:13:46,398
Adding a fruit for the first time versus updating an existing fruit.

225
00:13:46,494 --> 00:13:49,294
So let's try testing both of it.

226
00:13:49,432 --> 00:13:52,454
When we try to add a new fruit in a regular way, it takes about

227
00:13:52,492 --> 00:13:55,810
001299 milliseconds,

228
00:13:55,890 --> 00:13:59,094
whereas when we try to add a new fruit using get,

229
00:13:59,212 --> 00:14:02,314
it's zero, zero. There are three terms, seven, nine,

230
00:14:02,352 --> 00:14:05,350
seven. But to be fair, adding a new fruit,

231
00:14:05,430 --> 00:14:08,586
there's not much of a performance overhead. If I run it again, you'll see

232
00:14:08,608 --> 00:14:12,394
the gap shrink or even the gap for the new fruit

233
00:14:12,442 --> 00:14:15,680
being higher. But where you'll see a good

234
00:14:16,370 --> 00:14:19,934
difference in the opposite direction is basically the regular way

235
00:14:19,972 --> 00:14:22,878
versus regular way to update a fruit versus get.

236
00:14:23,044 --> 00:14:26,578
So the regular way makes much lesser time in

237
00:14:26,584 --> 00:14:29,954
this scenario, as opposed to using get, simply because

238
00:14:29,992 --> 00:14:33,346
of the fact that when you use a get function, you're trying to

239
00:14:33,368 --> 00:14:36,850
get it directly and there is an indirection inside that

240
00:14:36,920 --> 00:14:40,166
to basically pass a default value as

241
00:14:40,188 --> 00:14:43,686
well. So that is a bit of an overhead, not much. So let

242
00:14:43,708 --> 00:14:46,806
me try running this again and we'll see what's happening. So in

243
00:14:46,828 --> 00:14:50,106
this scenario, getting the fruit using get is much lesser than

244
00:14:50,128 --> 00:14:53,210
this, as in getting the fruit or getting

245
00:14:53,280 --> 00:14:56,794
the fruit after updating in the regular way,

246
00:14:56,832 --> 00:15:00,410
as opposed to using get. In a nutshell, if you take a look,

247
00:15:00,560 --> 00:15:04,174
using get for updating or adding, the average time

248
00:15:04,212 --> 00:15:07,966
taken for both operations will generally be lesser than the

249
00:15:07,988 --> 00:15:11,274
time taken to update and add in the regular

250
00:15:11,322 --> 00:15:14,622
way. So you take an average of all these operations for the regular way,

251
00:15:14,676 --> 00:15:18,478
and the average of all these operations for the regular way. This average

252
00:15:18,574 --> 00:15:22,418
using get will always be lower than the average in the regular way,

253
00:15:22,504 --> 00:15:26,174
and that's the reason why comparing it individually may not yield meaningful

254
00:15:26,222 --> 00:15:29,622
results. But comparing all the operations that have been done

255
00:15:29,676 --> 00:15:32,886
totally in the regular way versus using get will

256
00:15:32,908 --> 00:15:36,834
make more sense in this scenario. The second suggestion for the collections

257
00:15:36,882 --> 00:15:40,838
category is to use the generators as opposed to collections. When you

258
00:15:40,844 --> 00:15:44,426
want to generate a list of numbers or a collection of numbers. One way is

259
00:15:44,448 --> 00:15:47,210
to use a list comprehension to generate a list,

260
00:15:47,280 --> 00:15:51,130
or even a for loop to generate a list. The other approach is by just

261
00:15:51,200 --> 00:15:55,166
setting up a generator. So the key difference over here is when you use a

262
00:15:55,188 --> 00:15:58,670
list comprehension, it actually generates the list and

263
00:15:58,740 --> 00:16:02,250
how many of the numbers you're generating are generated and unstored in the memory.

264
00:16:02,330 --> 00:16:06,050
Whereas in this scenario you're just storing a generator object

265
00:16:06,120 --> 00:16:09,314
that lazily generates whenever you need that number or

266
00:16:09,352 --> 00:16:13,054
you call that generator object to generate. So in that case, the memory

267
00:16:13,102 --> 00:16:16,322
used for this is significantly lower than the memory needed

268
00:16:16,376 --> 00:16:19,654
for this. So let's try to run this piece of

269
00:16:19,692 --> 00:16:23,570
code and see whether it gives us any performance gain.

270
00:16:23,730 --> 00:16:27,730
So if we look at the size occupied, especially with the increasing number of values,

271
00:16:27,810 --> 00:16:30,970
we see a steady increase when we are generating the numbers

272
00:16:31,040 --> 00:16:34,646
list as opposed to the generator. The memory usage is constant,

273
00:16:34,758 --> 00:16:37,750
very very minimal compared to the list generation,

274
00:16:37,830 --> 00:16:41,162
because it's not generating all the values and storing it is only

275
00:16:41,216 --> 00:16:44,526
generating it when we use a generator to iterate or when

276
00:16:44,548 --> 00:16:48,186
we iterate through the generator. So that is also one of the best practices

277
00:16:48,218 --> 00:16:51,642
that I would like to suggest. Now that we're done with collections,

278
00:16:51,706 --> 00:16:55,518
let's move on to the condition section. Whenever we have

279
00:16:55,604 --> 00:16:58,594
huge list of values, there are two ways we can go about checking it.

280
00:16:58,632 --> 00:17:01,986
One is to use a for loop and then just check if the number is

281
00:17:02,008 --> 00:17:05,218
present within that. The other approach is simply to use the in.

282
00:17:05,304 --> 00:17:09,506
This is not only more elegant, it comes back to the built in methods

283
00:17:09,538 --> 00:17:13,330
and keywords that we have to use. This is way more efficient

284
00:17:13,490 --> 00:17:17,078
in most of the scenarios. So let's take a look to see

285
00:17:17,164 --> 00:17:20,698
how much of a time gain we get by using in,

286
00:17:20,784 --> 00:17:23,626
or by just iterating through it and checking it out.

287
00:17:23,808 --> 00:17:27,466
So as you can see, the time, there are four sections over

288
00:17:27,488 --> 00:17:30,954
here. Primarily. One is basically the red,

289
00:17:31,072 --> 00:17:34,746
which is matching using in, and then we have the orange,

290
00:17:34,858 --> 00:17:37,934
which is basically matching using the for loop. So this is

291
00:17:37,972 --> 00:17:41,374
basically the orange, whereas the red is over here and it's being

292
00:17:41,412 --> 00:17:44,986
overlapped by the green and the other elements,

293
00:17:45,098 --> 00:17:48,098
the colors over here. But in a nutshell, if you take a look,

294
00:17:48,184 --> 00:17:52,030
the red spike over this is the red spike and these are the orange spikes.

295
00:17:52,110 --> 00:17:55,394
So this red spike over here is for in. So there's a momentary spike over

296
00:17:55,432 --> 00:17:59,282
here, whereas that may be because of the context switching in the cpu,

297
00:17:59,426 --> 00:18:02,994
whereas for the for loop we see way more context switching

298
00:18:03,042 --> 00:18:07,154
and way more increase in the time duration as opposed

299
00:18:07,202 --> 00:18:10,998
to using in. So that is one of the key things that

300
00:18:11,164 --> 00:18:15,018
we observe over here and reason why we would recommend using in,

301
00:18:15,104 --> 00:18:18,646
simply because it's cleaner code and there is also a performance gain as opposed

302
00:18:18,678 --> 00:18:22,970
to using the for loop. Now that we're done with condition, we have successfully

303
00:18:23,050 --> 00:18:27,034
concluded the session. So I would be sharing this repository

304
00:18:27,162 --> 00:18:30,826
containing this code, as well as a best practice markdown file

305
00:18:30,858 --> 00:18:33,982
that contains all best practices we have discussed and

306
00:18:34,036 --> 00:18:37,626
more, in fact. So due to a time constraint, I had to restrict

307
00:18:37,658 --> 00:18:40,894
the number of best practices I could discuss with you folks. But I would love

308
00:18:40,932 --> 00:18:44,606
to hear feedback from you and any other best practices that you may recommend,

309
00:18:44,708 --> 00:18:47,618
so I would be glad to include it in my future session.

310
00:18:47,714 --> 00:18:51,126
And any constructive criticism feedback is more

311
00:18:51,148 --> 00:18:54,290
than welcome. Thank you so much for investing your time in this presentation.

312
00:18:54,370 --> 00:18:57,190
I hope to see you in one soon, another one soon.

313
00:18:57,340 --> 00:18:57,700
Thank you.

