1
00:00:25,490 --> 00:00:28,626
Okay. Hi, I'm Matt Harrison and we're going to be talking about idiomatic pandas.

2
00:00:28,658 --> 00:00:31,526
So I'm excited to be here today. I run a company called Metasnake that does

3
00:00:31,548 --> 00:00:34,598
corporate training and consulting in python and data science. Some of the reasons you

4
00:00:34,604 --> 00:00:37,206
might want to listen to me talking about pandas is because I've written a couple

5
00:00:37,228 --> 00:00:40,934
books on pandas. I co wrote the one point x cookbook, also wrote learning

6
00:00:40,972 --> 00:00:44,198
the Pandas library, and I have this machine learning pocket reference that also uses a

7
00:00:44,204 --> 00:00:46,966
lot of pandas as well. In addition, I've taught pandas to thousands of people over

8
00:00:46,988 --> 00:00:50,446
the years and used it for various consulting projects. So ive seen it

9
00:00:50,468 --> 00:00:54,014
basically since it was released and used it from about that time as well.

10
00:00:54,052 --> 00:00:56,798
I've seen a lot of good pandas code and bad pandas code. So what I

11
00:00:56,804 --> 00:00:59,386
want to do today is talk about some ways that you can make your pandas

12
00:00:59,418 --> 00:01:03,066
code better. Let me just share my notebook here and here's the outline

13
00:01:03,098 --> 00:01:06,174
of what we're going to do. We're going to be looking at loading our data

14
00:01:06,212 --> 00:01:09,294
really quickly. Then we'll talk about using the correct types for your data. We'll talk

15
00:01:09,332 --> 00:01:12,526
about chaining, mutation, application or the

16
00:01:12,548 --> 00:01:15,798
apply method, and finally aggregation. Okay, I'm going to do my imports here at the

17
00:01:15,804 --> 00:01:19,266
top. We're going to be using numpy and pandas. We are using pandas version

18
00:01:19,298 --> 00:01:22,326
1.2.3. I'm going to set some options here just so I can see a few

19
00:01:22,348 --> 00:01:24,918
more rows than the default rows, and reload our data set. This data set is

20
00:01:24,924 --> 00:01:28,358
from fuel economy Gov. It's got information about cars that

21
00:01:28,364 --> 00:01:31,586
were sold in the US, I believe from 1984 to 2018

22
00:01:31,618 --> 00:01:35,414
or so. Information about them. So it looks something like this and it's got

23
00:01:35,452 --> 00:01:38,946
quite a bit of data in it. So we've got 40,000 rows and 83 columns

24
00:01:38,978 --> 00:01:41,326
of data about this. So this is a nice little data set to let us

25
00:01:41,348 --> 00:01:44,686
explore various features that has different types in these data sets as well. So if

26
00:01:44,708 --> 00:01:47,166
you look at the columns in here, here's some of the columns we have.

27
00:01:47,188 --> 00:01:50,078
Like how many barrels of gas does a car consume in a year? What's the

28
00:01:50,084 --> 00:01:53,646
city mileage, highway mileage, engine description, the drive?

29
00:01:53,748 --> 00:01:57,022
So a bunch of good information about cars. Okay, so that's the data.

30
00:01:57,076 --> 00:02:00,494
Let's dive in. So the first part here is using the right types. So getting

31
00:02:00,532 --> 00:02:03,214
the right types in pandas can be a challenge. But if you can do that,

32
00:02:03,252 --> 00:02:06,854
it's going to save you memory. It's going to make your code easier to work

33
00:02:06,892 --> 00:02:10,166
with and it can save you computation time as well. I'm just going to

34
00:02:10,188 --> 00:02:13,126
limit what we're going to do here to a few columns. So again, we had

35
00:02:13,148 --> 00:02:16,246
83 columns in that data set. I don't want all 83 columns here. So what

36
00:02:16,268 --> 00:02:18,038
I'm going to do is pull off the columns I want to look at and

37
00:02:18,044 --> 00:02:21,366
look at the dtypes attribute. This is going to give me a series that has

38
00:02:21,388 --> 00:02:24,646
the column names in the index and the values for those attributes on the right

39
00:02:24,668 --> 00:02:26,886
hand side there. So you can see I've got a bunch of Int 64,

40
00:02:26,908 --> 00:02:30,318
some float, 64, some objects, and those are the types. One thing you

41
00:02:30,324 --> 00:02:32,206
might want to do is you might want to look at how much memory this

42
00:02:32,228 --> 00:02:35,934
is using. So here's our memory usage for each column. If we sum that up,

43
00:02:35,972 --> 00:02:38,638
we can see that this is the amount of memory we're using. Just look at

44
00:02:38,644 --> 00:02:41,962
that number. It's around 19, blah, blah, blah. So we'll just say it's

45
00:02:42,026 --> 00:02:45,406
19 right now, 19 megabytes. And we're going to basically try and

46
00:02:45,428 --> 00:02:48,734
lower that if we can. So the first thing is considering what our

47
00:02:48,772 --> 00:02:51,486
integer columns. So again, I can look at some of the integer columns here.

48
00:02:51,508 --> 00:02:54,254
So I'm going two, use the select dtypes, pass an int into it, and then

49
00:02:54,292 --> 00:02:57,606
chain that with the described method here. And here are the integer types that are

50
00:02:57,628 --> 00:03:01,106
currently on this data frame that I have. We've got city mileage, combined mileage,

51
00:03:01,218 --> 00:03:04,662
highway mileage, fuel cost, the range and the year.

52
00:03:04,716 --> 00:03:08,102
So fuel cost is how much you would spend on fuel, range is

53
00:03:08,156 --> 00:03:11,266
how far an electric car will go, and city combined and highway,

54
00:03:11,298 --> 00:03:14,386
that's miles per gallon. How far these cars go, how many miles

55
00:03:14,418 --> 00:03:17,766
they go with a gallon of gas or petroleum. And then year, that's the year

56
00:03:17,788 --> 00:03:21,646
the car was released. Okay? So of all, I probably wouldn't write this

57
00:03:21,668 --> 00:03:24,426
code like this. I would write this in this chain style right here. So you'll

58
00:03:24,458 --> 00:03:27,806
see throughout the course today that I do a lot of this chaining. And I

59
00:03:27,828 --> 00:03:30,238
just think it makes it easier to read. So you can see, first I'm going

60
00:03:30,244 --> 00:03:32,078
to take the autos, then I'm going to pull off the columns and I'm going

61
00:03:32,084 --> 00:03:34,526
to select the dtypes. And then finally I'm going to describe that it's a few

62
00:03:34,548 --> 00:03:37,018
more lines. We're just splitting it into a few more lines. And we're also wrapping

63
00:03:37,034 --> 00:03:40,446
it with parentheses, which allows us to put it on multiple lines. Now, one thing

64
00:03:40,468 --> 00:03:42,318
you want to do when you do this describe is you want to look at

65
00:03:42,324 --> 00:03:45,030
the min and max values because that can indicate what types you can use for

66
00:03:45,060 --> 00:03:48,294
integers. By default, pandas is going to use this lowercase int 64 type.

67
00:03:48,412 --> 00:03:51,366
And you might want to use a different type. So there are other types in

68
00:03:51,388 --> 00:03:54,582
here. Let's just show some of them. For example, this one down here can int

69
00:03:54,636 --> 00:03:58,214
eight. If you use that type, you can go from negative 128 to 127.

70
00:03:58,252 --> 00:04:01,718
So if we look at our values here, for example highway mileage, we might get

71
00:04:01,724 --> 00:04:05,362
away with using this int eight. There's also an int 16 here. An int 16

72
00:04:05,426 --> 00:04:08,534
goes up to 32,000. So it looks like all of our values there would support

73
00:04:08,652 --> 00:04:12,142
being changed to int 16 instead. So what we can do is we can

74
00:04:12,196 --> 00:04:15,866
stick in this as type call here into our chain.

75
00:04:15,978 --> 00:04:19,262
And then in select dtypes we'll just select int and int

76
00:04:19,396 --> 00:04:22,158
eight. And you can see that this is now showing us what is either an

77
00:04:22,164 --> 00:04:25,214
int or an int eight. It's not showing us the int 16 there,

78
00:04:25,252 --> 00:04:28,514
but we did. Okay. And so if you're not familiar with this as

79
00:04:28,552 --> 00:04:32,226
type, what this is doing. Here's the documentation. Basically you can pass in

80
00:04:32,248 --> 00:04:35,518
this dtype here or you can pass in a dictionary of types.

81
00:04:35,534 --> 00:04:38,574
So what we're doing above with this as type is we're passing in a dictionary

82
00:04:38,622 --> 00:04:40,918
with the columns and the types that we want those columns to be. So you

83
00:04:40,924 --> 00:04:44,470
can see that highway. We're casting that to an int eight city and

84
00:04:44,540 --> 00:04:47,814
combined eight. We're casting those or we're changing those to int

85
00:04:47,852 --> 00:04:51,046
16s. That's why city and int city and

86
00:04:51,068 --> 00:04:53,754
combined don't show up in this. Because when we said select dtypes, we did not

87
00:04:53,792 --> 00:04:57,238
select the int 16 dtype. Okay, so here I'm just flushing

88
00:04:57,254 --> 00:05:01,018
this out a little bit more. I'm filling in range and year as well.

89
00:05:01,104 --> 00:05:04,586
Note that I'm saying now select dtypes with the string integer. And if we do

90
00:05:04,608 --> 00:05:07,806
that, this is going to pull off anything that's integer esque. And so it will

91
00:05:07,828 --> 00:05:10,826
get all those types in there. This is looking okay. We haven't lost any precision

92
00:05:10,858 --> 00:05:13,946
by casting our integer columns to these other columns. We still have the same values

93
00:05:13,978 --> 00:05:16,942
in them. But now we should be using a lot less memory to do that.

94
00:05:16,996 --> 00:05:20,266
Let's just look at the memory here. And now we're using 18 instead of 19.

95
00:05:20,298 --> 00:05:22,894
So we've got a little bit of savings in that. Let's look at floating point

96
00:05:22,932 --> 00:05:26,086
numbers. So I'm going to say, let's describe what's a float and

97
00:05:26,108 --> 00:05:29,318
we've got cylinders and displacement in there. So I'm

98
00:05:29,324 --> 00:05:31,798
going to jump into cylinders. One thing that strikes out at me as I look

99
00:05:31,804 --> 00:05:34,738
at this is that cylinders actually looks integer. Like, if you look at the values

100
00:05:34,754 --> 00:05:37,446
that came up there, those look very integer esque. What I'm going to do is

101
00:05:37,468 --> 00:05:40,950
just stick a describe on here to look at cylinders. And this is confirming my

102
00:05:41,020 --> 00:05:44,854
suspicion that we have the mins and the quartiles there that are all whole numbers.

103
00:05:44,972 --> 00:05:48,038
So why is that being represented as can float instead of an

104
00:05:48,044 --> 00:05:50,254
integer? That might be the question that pops up in your mind, hopefully, is,

105
00:05:50,292 --> 00:05:52,686
well, one thing you can do is you can use this value counts call.

106
00:05:52,788 --> 00:05:55,886
And if I do that and say drop Na is equal to false, we can

107
00:05:55,908 --> 00:05:59,294
see why this is represented as a float. And the reason why is because in

108
00:05:59,332 --> 00:06:02,478
pandas, the lowercase int 64 type does not have support for missing numbers. And you

109
00:06:02,484 --> 00:06:05,818
can see right here, there is 206 entries here where the cylinders

110
00:06:05,834 --> 00:06:08,894
are missing. These are cars that do not have cylinders in them. So you might

111
00:06:08,932 --> 00:06:11,806
want to dive into that and figure out why those cylinders are missing. So this

112
00:06:11,828 --> 00:06:14,286
code right here will just say, let's query at. And I want to query where

113
00:06:14,308 --> 00:06:17,906
cylinders is Na. And so these are the cars where cylinders are missing.

114
00:06:17,938 --> 00:06:21,446
And it looks like these are mostly, if you look in this model column right

115
00:06:21,468 --> 00:06:24,486
here, it looks like these are mostly electric cars, which makes sense. I'm not a

116
00:06:24,508 --> 00:06:27,686
super car buff, but apparently electric motors do not have cylinders in

117
00:06:27,708 --> 00:06:30,166
them. So at this point we need to figure out what we want to do

118
00:06:30,188 --> 00:06:32,438
with that cylinders column. So what I'm going to do is I'm going to make

119
00:06:32,444 --> 00:06:35,238
an executive decision. Rather than having missing numbers in there, I'm just going to say

120
00:06:35,244 --> 00:06:38,118
fill an a with this little operation here. And then with that I should be

121
00:06:38,124 --> 00:06:41,166
able to cast it to an int eight. Also with the, this displacement here,

122
00:06:41,188 --> 00:06:43,726
I'm going to do a similar thing. I'm going to cast that or not.

123
00:06:43,748 --> 00:06:46,366
I'm going to cast it. I'm going to fill that in with empty values here.

124
00:06:46,388 --> 00:06:49,166
Let's just look at the integer types after we do that. And we should see

125
00:06:49,188 --> 00:06:52,606
that cylinders pops up here as an integer type. And the lowest value for a

126
00:06:52,628 --> 00:06:56,138
cylinder is zero. That's right here. And the highest value is 16. Okay, so we've

127
00:06:56,154 --> 00:06:58,894
converted cylinders successfully from a floating point to an integer.

128
00:06:58,942 --> 00:07:02,290
Let's see what else we have. We can also, if we want to dive into

129
00:07:02,360 --> 00:07:05,426
these float types, the default float type is float 64. If we wanted to look

130
00:07:05,448 --> 00:07:08,226
at like a float 16. We can do this numpy call here to see what

131
00:07:08,248 --> 00:07:11,414
the minimum and maximum values for like a float 16 can be.

132
00:07:11,452 --> 00:07:15,186
And if we can support that in here. So this looks like the displacement.

133
00:07:15,218 --> 00:07:18,226
We should be able to use a float 16 for that. I'm going to stick

134
00:07:18,258 --> 00:07:21,846
that in there and we'll have a float 16 for that. Now let's look

135
00:07:21,868 --> 00:07:25,546
at our memory usage now. After we've done that, our memory usage is now 17.5.

136
00:07:25,568 --> 00:07:28,890
So again we've saved a little bit more memory. We started at 19 and

137
00:07:28,960 --> 00:07:31,466
we've gone down a little bit. The next type that I want to look at

138
00:07:31,488 --> 00:07:34,902
is object types. Pandas supports these native float

139
00:07:34,966 --> 00:07:38,654
and integer types and basically it's allocating a block of memory in the

140
00:07:38,692 --> 00:07:41,006
specific type. So if it's an int eight, it's going to use eight bits of

141
00:07:41,028 --> 00:07:44,126
memory for each integer rather than a whole python object. But pandas also

142
00:07:44,148 --> 00:07:47,466
allows you to stick python objects into each cell there. Now when you're

143
00:07:47,498 --> 00:07:50,178
doing that you're going to use a lot more memory because pandas objects take up

144
00:07:50,184 --> 00:07:53,198
more memory, you're also going to lose speed. And so you're sort of executing

145
00:07:53,214 --> 00:07:56,850
things at the python level rather than pandas. Optimized basically c

146
00:07:56,920 --> 00:08:00,390
speed if you stay at those low levels here. So let's look at what is

147
00:08:00,460 --> 00:08:03,654
object types and these columns we care about. We've got drive engine

148
00:08:03,692 --> 00:08:07,286
description, make, model, transmission and created on. So generally when you

149
00:08:07,308 --> 00:08:10,646
read a CSV file, an object represents string data, though it could

150
00:08:10,668 --> 00:08:14,106
also represent numeric data that had string esque values in it. You can also see

151
00:08:14,128 --> 00:08:17,446
that created on looks like a date, but by default read CSV does not convert

152
00:08:17,478 --> 00:08:21,322
that into a date. Now I like to generally treat string types as

153
00:08:21,456 --> 00:08:25,174
basically a few types. One is categorical. And what do I mean by categorical?

154
00:08:25,222 --> 00:08:27,886
Well, if you look at like make, there's only a few different makes in there,

155
00:08:27,908 --> 00:08:30,622
so that could be categorical. Model on the other hand is more free form.

156
00:08:30,676 --> 00:08:33,518
There's going to be basically for every make, there'll be a couple of models for

157
00:08:33,524 --> 00:08:36,014
every year. So it's kind of a gray area. It's a little free form.

158
00:08:36,052 --> 00:08:39,246
It could be categorical esque, but it's going to have a higher cardinality, a higher

159
00:08:39,268 --> 00:08:42,766
number of unique values. Transmission looks categorical created on. I mean, you could

160
00:08:42,788 --> 00:08:45,766
say it's categorical, but it's actually a date type. I mean, these entries all look

161
00:08:45,788 --> 00:08:48,006
the same. So because there's so many entries that look the same, you might think

162
00:08:48,028 --> 00:08:50,742
that's categorical, but I'm going to convert that to a date because then I can

163
00:08:50,796 --> 00:08:54,434
leverage pandas date operations on that. We also have this engine, DSCR,

164
00:08:54,482 --> 00:08:58,274
which is the engine description, which looks like it's got parentheses and strings

165
00:08:58,322 --> 00:09:00,646
inside of it. We'll dig into that a little bit more and then we have

166
00:09:00,668 --> 00:09:03,638
drive, which looks categorical as well. So we kind of need to figure out what

167
00:09:03,644 --> 00:09:05,314
we want to do with these if we want to keep them as strings.

168
00:09:05,362 --> 00:09:08,646
If we convert them to categorical types, we'll save memory there. If we convert them

169
00:09:08,668 --> 00:09:12,094
to dates, we can be able to do date operations on them. Let's just start

170
00:09:12,132 --> 00:09:15,018
off with drive. So what I like to do when I come across a string

171
00:09:15,034 --> 00:09:17,726
column is stick it into this value counts call. That's going to give me the

172
00:09:17,748 --> 00:09:21,086
frequency and let me evaluate whether it is indeed categorical or not. Again,

173
00:09:21,108 --> 00:09:23,038
I put in that drop Na is equal to false to see if there are

174
00:09:23,044 --> 00:09:26,650
missing values in there. So these are the entries, the unique entries for drive.

175
00:09:26,740 --> 00:09:30,226
And I would call that this is categorical. Now, it's interesting that again, there are

176
00:09:30,248 --> 00:09:33,230
some missing values in there. NAN is used to represent missing values.

177
00:09:33,310 --> 00:09:36,798
So small aside, an object type in pandas can mean string,

178
00:09:36,814 --> 00:09:39,686
but it can also mean a mixed type. So in this case, Ive is actually

179
00:09:39,708 --> 00:09:42,930
a mixed type. It's actually strings with floating points. It's using nans.

180
00:09:43,010 --> 00:09:46,546
Floating points are representing those pandas there with those missing values. So let's just dive

181
00:09:46,578 --> 00:09:49,926
into that. Like, where is the drive missing? I'm going to do this little call

182
00:09:49,948 --> 00:09:53,978
here. Query. Drive is can a. Looks like a

183
00:09:53,984 --> 00:09:57,850
lot of these are electric, but some of them are not. There's like Nissan 300.

184
00:09:57,920 --> 00:10:01,098
I'm not sure why the drive is missing from that. There's some corvettes where the

185
00:10:01,104 --> 00:10:04,426
drive is missing. So is this front wheel drive or rear wheel drive? Why is

186
00:10:04,448 --> 00:10:07,966
this missing? Again, I'm not a car expert per se, but if you talk

187
00:10:07,988 --> 00:10:11,406
to a subject matter expert, they could probably give you some insight into figuring out

188
00:10:11,428 --> 00:10:14,926
the appropriate label for those various types. What I'm going to do, given that

189
00:10:14,948 --> 00:10:17,406
I'm not really a subject matter expert here, I'm going to say, let's just fill

190
00:10:17,428 --> 00:10:20,766
in everything else with the string, empty or not empty other, and we're going to

191
00:10:20,788 --> 00:10:23,930
convert it to a category type. So I'm going to say as type category.

192
00:10:24,010 --> 00:10:26,790
Let's just look at the memory after do that and look at that just by

193
00:10:26,860 --> 00:10:30,278
changing that type there. Well, we also did change the make type down here.

194
00:10:30,284 --> 00:10:32,214
So you can see that we changed the make type. So we changed basically two

195
00:10:32,252 --> 00:10:35,734
types, drive and make. We went from, I believe we're around 17. We've now

196
00:10:35,772 --> 00:10:39,254
dropped into twelve. So categoricals are a way to save

197
00:10:39,292 --> 00:10:42,146
a lot of memory in pandas. Why do we care about saving memory in pandas?

198
00:10:42,178 --> 00:10:44,806
Again, I talked about that before, it might make things run faster. But the other

199
00:10:44,828 --> 00:10:47,986
thing is that pandas is an in memory tool. So in order to use pandas,

200
00:10:48,018 --> 00:10:50,206
you need to be able to load your data in memory. So if your sort

201
00:10:50,228 --> 00:10:53,386
of pushing that edge where you're running out of memory using the right types

202
00:10:53,418 --> 00:10:56,826
can allow you to process more data. Okay, let's jump

203
00:10:56,858 --> 00:10:59,630
into tranny again. Value counts is my go to here.

204
00:10:59,700 --> 00:11:02,766
Here's the call for value counts. And it looks like there are a bunch of

205
00:11:02,788 --> 00:11:05,646
entries in there. Does it say at the bottom here? Sometimes it'll say how many

206
00:11:05,668 --> 00:11:08,558
rows there are, but I'm going to estimate there's around 30 different entries in here.

207
00:11:08,644 --> 00:11:11,966
So is this categorical? Maybe. I mean, it looks a little free form too.

208
00:11:11,988 --> 00:11:15,254
There's these parentheticals in there. I'm not quite sure what four

209
00:11:15,292 --> 00:11:18,982
versus the s seven versus the ams seven means per se, but to me,

210
00:11:19,036 --> 00:11:21,766
as sort of a novice, one thing that sort of sticks out is this,

211
00:11:21,788 --> 00:11:24,646
is whether it's automatic or manual. The other thing is there's this number in there,

212
00:11:24,668 --> 00:11:27,062
right? So those are both things that might stick out, that I might want to

213
00:11:27,116 --> 00:11:30,306
keep track of in this data set. So what I'm going to do is I'm

214
00:11:30,338 --> 00:11:33,942
going to put a new column here called automatic,

215
00:11:34,006 --> 00:11:36,698
and that's just whether it contains automatic in there. And then I'm going to put

216
00:11:36,704 --> 00:11:40,486
another new column here called speed. And that's just using this stir extract,

217
00:11:40,518 --> 00:11:44,410
which is passing in a regular expression here and pulling out that value.

218
00:11:44,480 --> 00:11:47,678
If there are missing values, I'm sticking in the string 20 instead. It looks like

219
00:11:47,684 --> 00:11:51,098
the ones that are missing if you dive into that are probably like variable speeds.

220
00:11:51,114 --> 00:11:53,706
So I'm just putting 20 in as a large number for speeds and then I'm

221
00:11:53,738 --> 00:11:56,126
casting that as an int eight. So if we do that and we look at

222
00:11:56,148 --> 00:11:59,642
our memory, our memory now has gone down from 19 to ten weeks. So we've

223
00:11:59,706 --> 00:12:03,374
almost halved our memory by doing these little operations here. Note that

224
00:12:03,412 --> 00:12:06,918
I also now have a few more call columns that might have more interesting information.

225
00:12:07,004 --> 00:12:09,622
Is there more that you could do with this column? The tranny column? Yeah,

226
00:12:09,676 --> 00:12:12,406
there might be. Right, so again, talking to a subject matter expert might reveal more

227
00:12:12,428 --> 00:12:15,254
insight, other features that you can create for your data. Let's look at the date

228
00:12:15,292 --> 00:12:18,486
columns here. So I do have this created on column. So what

229
00:12:18,508 --> 00:12:20,614
I'm going to do is I'm going to run this two date time call.

230
00:12:20,652 --> 00:12:24,242
This is a function in pandas, and then I'm going to call this tz localize

231
00:12:24,306 --> 00:12:27,318
down here because it looked like it was in the eastern time zone. So let's

232
00:12:27,324 --> 00:12:30,318
just run that and I get a warning here. It complains about that, but if

233
00:12:30,324 --> 00:12:33,166
you look at my memory usage again, I'm going down quite a bit. I'm now

234
00:12:33,188 --> 00:12:36,766
at 7000 here. So this warning here is due to a

235
00:12:36,788 --> 00:12:40,190
python issue where python doesn't understand this EDT or ESt in there.

236
00:12:40,260 --> 00:12:44,366
So with this little change here, replacing EDT and ESt by these numbers,

237
00:12:44,468 --> 00:12:47,790
when I rerun this here, I don't have any issues. Now, in this case,

238
00:12:47,860 --> 00:12:51,466
I'm pulling off the value counts here and just looking at the engine description.

239
00:12:51,498 --> 00:12:54,982
Value counts here, but let's just drop the engine description. So that's what

240
00:12:54,996 --> 00:12:57,798
I'll do down here because this looks like free form. But before I drop it,

241
00:12:57,804 --> 00:13:00,850
I'm going to add this little thing here. Does it contain ffs?

242
00:13:00,930 --> 00:13:03,798
Again, I'm not a pro at these engine things, but looks like there's a bunch

243
00:13:03,804 --> 00:13:06,038
of ffs in there, so that might be something that I want to stick in

244
00:13:06,044 --> 00:13:09,186
there. So this is an example, just making a new column, whether it contains ffs

245
00:13:09,218 --> 00:13:13,298
or not. So let's run that. And we're now down to eight. So we're

246
00:13:13,314 --> 00:13:16,582
not quite a third of our original data size, but we're getting close. Two that,

247
00:13:16,636 --> 00:13:18,886
okay, so this is kind of nice. Now, one thing you may or may not

248
00:13:18,908 --> 00:13:20,846
have noticed is that I've just sort of built this up as we've been going

249
00:13:20,868 --> 00:13:23,166
along, and Jupyter makes it really nice to do that. What I like to do

250
00:13:23,188 --> 00:13:26,718
now is I like to make a function that will take my raw data that

251
00:13:26,724 --> 00:13:29,786
ive read directly out of a CSV file, and I generally just call it tweak,

252
00:13:29,818 --> 00:13:32,830
whatever the name is. I pass in my raw data here and I just apply

253
00:13:32,900 --> 00:13:35,998
all these chains right here, and this gives me my cleaned up data. So this

254
00:13:36,004 --> 00:13:38,814
is really nice to have a function like this in one place, in one cell.

255
00:13:38,852 --> 00:13:41,598
Generally, I'll put it right after the load up at the top. Then when I

256
00:13:41,604 --> 00:13:44,174
want to revisit my data, I know that I can load the raw data and

257
00:13:44,212 --> 00:13:47,574
I can immediately get my cleaned up data. This also allows for what's called data

258
00:13:47,612 --> 00:13:51,014
provenance, where I can track through and see what happened, where every row is coming

259
00:13:51,052 --> 00:13:54,546
from. Generally your pointy head boss, after you do an analysis or calculation,

260
00:13:54,578 --> 00:13:56,866
is going to ask you to explain it. And if you don't have this provenance

261
00:13:56,898 --> 00:13:59,826
where you can come back to the original data that might be problematic and bite

262
00:13:59,858 --> 00:14:02,706
you. So this is what I like to do, and this should read pretty clearly

263
00:14:02,738 --> 00:14:06,578
here. I'm taking autos, I'm pulling off these columns, I'm assigning a cylinders column,

264
00:14:06,594 --> 00:14:09,478
a displacement column, a drive column, an automatic column, a speeds column, a created on

265
00:14:09,484 --> 00:14:12,298
column, and an SS. I'm changing the types for various ones of these and I'm

266
00:14:12,314 --> 00:14:15,646
dropping these columns so this sort of reads like a recipe. Do these steps and

267
00:14:15,668 --> 00:14:18,894
then you'll have your cleaned up data. Okay, so that is my first thing

268
00:14:18,932 --> 00:14:21,486
to do with pandas is you probably want to look at your data and make

269
00:14:21,508 --> 00:14:24,106
sure that your types are correct. The next section here is to talk about chains.

270
00:14:24,138 --> 00:14:27,146
So chains are something that I actually just showed you. Sometimes this is called flow

271
00:14:27,178 --> 00:14:31,274
programming. But what we're doing is we're leveraging the fact that in pandas, pandas generally

272
00:14:31,322 --> 00:14:34,878
does not mutate data. Rather it returns new data frames. And so we're just going

273
00:14:34,884 --> 00:14:37,906
two, take the new data frame and do operations on that. So on that note,

274
00:14:37,938 --> 00:14:40,678
because I said pandas can in memory tool, you want to have three to ten

275
00:14:40,684 --> 00:14:43,206
times the amount of memory as the data frame you're working with, because you need

276
00:14:43,228 --> 00:14:46,294
to have some overhead for doing these intermediate operations. I know some of you who

277
00:14:46,332 --> 00:14:49,526
probably use pandas are like Nomad, that's not the way to do it. There's an

278
00:14:49,548 --> 00:14:52,406
in place parameter that you can use on various methods. And pandas will do things

279
00:14:52,428 --> 00:14:56,354
in place. Yeah, there is an in place parameter. However, the in place parameter generally

280
00:14:56,402 --> 00:14:59,846
doesn't do things in place as you might think it does. So I

281
00:14:59,868 --> 00:15:01,902
do have a hint here, if you find some code that you can't change.

282
00:15:01,956 --> 00:15:04,894
Sometimes I like to stick in a pipe. Call pipe is a nice way to

283
00:15:04,932 --> 00:15:08,334
inline any arbitrary code. Basically you can stick in a function that takes a data

284
00:15:08,372 --> 00:15:11,390
frame and returns a data frame for you. So this is my chain here.

285
00:15:11,460 --> 00:15:14,638
Now I think this actually reads very cleanly. I just walked through it in the

286
00:15:14,644 --> 00:15:17,594
last section here. Now compare this with, this is what I generally see in pandas

287
00:15:17,642 --> 00:15:21,646
code in the wild. So basically ive taken just these few lines here and ive

288
00:15:21,668 --> 00:15:24,254
split them out into something that I'll see like this. I'll see something like this,

289
00:15:24,372 --> 00:15:28,214
making all these intermediate variables, changing them and then sticking them

290
00:15:28,252 --> 00:15:30,806
back in. And I'm not even doing that for the rest of the code.

291
00:15:30,828 --> 00:15:32,918
For the rest of the code, I'm just sort of doing, not even making the

292
00:15:32,924 --> 00:15:36,338
intermediate variables, but just assigning these directly. And generally, again, I wouldn't

293
00:15:36,354 --> 00:15:38,694
see that, but just for the sake of typing this in as an example,

294
00:15:38,732 --> 00:15:41,954
you could imagine that generally what I see when I'm working with students or clients

295
00:15:42,002 --> 00:15:44,886
is that this code here is going to be like three times as long as

296
00:15:44,908 --> 00:15:47,766
it is here. So contrast that with this, which I think is really easy to

297
00:15:47,788 --> 00:15:50,894
read, it's easy to step through and is going to make your code and

298
00:15:50,932 --> 00:15:53,806
life a lot easier. Now let's just run this here. And actually when you run

299
00:15:53,828 --> 00:15:56,826
it, you'll notice all these little errors here that's sort of the bane of pandas,

300
00:15:56,858 --> 00:16:00,046
people everywhere, the setting with warning copy. You'll note that I didn't run into that

301
00:16:00,068 --> 00:16:03,406
at all because I used a sign here. So if you use a sign and

302
00:16:03,428 --> 00:16:06,986
you're chaining, you're never going to run into this setting with warning copy error.

303
00:16:07,018 --> 00:16:10,298
That's kind of confusing. And you go and read the descriptions and the stack overflows

304
00:16:10,314 --> 00:16:12,926
and you're still kind of confused on what to do. This is also easy to

305
00:16:12,948 --> 00:16:15,606
debug as well. So people sometimes say, matt, that's not easy to debug that because

306
00:16:15,628 --> 00:16:18,198
I don't have the intermediate objects. I say it is easy to debug. Let me

307
00:16:18,204 --> 00:16:20,166
just show you some things you can do. One thing I can do is I

308
00:16:20,188 --> 00:16:22,918
can just comment out everything and walk through it as I'm going through it.

309
00:16:22,924 --> 00:16:25,014
So here I've got a call to tweak autos here, let me just show you.

310
00:16:25,052 --> 00:16:27,126
So here is tweak autos. If I want to, I can just come in here

311
00:16:27,148 --> 00:16:30,086
and say, well, what did it do before this? This is what it did,

312
00:16:30,108 --> 00:16:33,046
right? And I can look at that intermediate and I can walk through from the

313
00:16:33,068 --> 00:16:36,214
start to the end. So that's one way to debug it, which I think

314
00:16:36,252 --> 00:16:39,654
is actually very useful, more so than having all these arbitrary variables sitting around.

315
00:16:39,692 --> 00:16:43,166
Another thing I can do if I want an arbitrary variable sticking around is I

316
00:16:43,188 --> 00:16:45,518
can make a function like this gitvar. It's going to take a data frame in

317
00:16:45,524 --> 00:16:47,658
a variable name and look what it does. This is sort of a dirty hack.

318
00:16:47,674 --> 00:16:51,006
It just basically injects that variable name pointing to the current data frame into

319
00:16:51,028 --> 00:16:54,922
my globals. So guess what? Right down here I've got this here pipe gitvar

320
00:16:54,986 --> 00:16:58,286
df three. So this is right after getting the columns. And if I come down

321
00:16:58,308 --> 00:17:01,438
here below this and look at df three, this is the intermediate state. So I

322
00:17:01,444 --> 00:17:04,734
can get that intermediate state very easily by piping in this little hack function here

323
00:17:04,772 --> 00:17:07,558
that updates that if I need it. So again, I claim that this is a

324
00:17:07,564 --> 00:17:10,854
lot cleaner. It's going to allow you to very easily books at your code,

325
00:17:10,892 --> 00:17:13,398
but pull out stuff that you need to. One more thing that I have in

326
00:17:13,404 --> 00:17:16,726
here is this pipe, and this is a pipe to display a data frame here.

327
00:17:16,748 --> 00:17:19,318
So I can display one as well. And I didn't really show this, but if

328
00:17:19,324 --> 00:17:22,278
you look at the output here, there's actually two data frames here. So here's the

329
00:17:22,284 --> 00:17:24,760
first one. And if I scroll down a little bit more,

330
00:17:25,450 --> 00:17:27,990
let me rerun this. This was commented out, so I'm going two, rerun this.

331
00:17:28,060 --> 00:17:30,670
Okay, so here's the first one. I commented that out when I run it.

332
00:17:30,740 --> 00:17:34,046
And there's the first one, and there's actually the second one here. So I put

333
00:17:34,068 --> 00:17:36,686
a little function in here that I piped in. That just is a lambda function.

334
00:17:36,708 --> 00:17:40,094
It says display and then it returns that data frame after it displays it.

335
00:17:40,132 --> 00:17:42,734
So the pipe takes a data frame and returns a data frame. So we're just

336
00:17:42,772 --> 00:17:45,726
saying shim in a print along the way so we can inspect it. So this

337
00:17:45,748 --> 00:17:48,254
is the way that it's going to make it very easy for you to debug

338
00:17:48,302 --> 00:17:51,666
these chain operations. Okay, the next step that I have

339
00:17:51,688 --> 00:17:54,782
is do not mutate. And if you look at my chained operations,

340
00:17:54,846 --> 00:17:58,322
because I'm using chains, chains don't mutate. And so I'm obeying this.

341
00:17:58,376 --> 00:18:02,322
And I've got a quote here from one of the core developers in pandas

342
00:18:02,386 --> 00:18:05,666
who I've got the link to the bug as well. That's talking about getting rid

343
00:18:05,698 --> 00:18:09,414
of the in place option in pandas. And the quote is Mr.

344
00:18:09,452 --> 00:18:12,726
Reebax says you're missing the point. In place rarely actually does something in

345
00:18:12,748 --> 00:18:15,546
place. You are thinking you're saving memory, but you're not. And I come across this

346
00:18:15,568 --> 00:18:18,506
all the time when I'm teaching pandas or looking at code people will use in

347
00:18:18,528 --> 00:18:21,846
place and they think, oh, it says in place. So it's just updating the memory

348
00:18:21,878 --> 00:18:25,174
in place. Generally it's not doing that. Generally when you call in place, it's making

349
00:18:25,232 --> 00:18:28,218
the new variable that you'd get anyway and then shimming it back in. So it's

350
00:18:28,234 --> 00:18:31,998
not saving you any memory that you would get by doing chaining, but basically

351
00:18:32,084 --> 00:18:35,118
by forcing yourself to chain, you're going to make that clean code and

352
00:18:35,124 --> 00:18:38,314
you're also going to get rid of the bane of python developers everywhere. That annoying

353
00:18:38,362 --> 00:18:41,386
error that's like, what does this mean by using chains and assigns?

354
00:18:41,418 --> 00:18:44,058
You get rid of that. This is a short section, but Ive sort of demoed

355
00:18:44,074 --> 00:18:47,206
it up to this point. In general, there are no performance benefits by

356
00:18:47,228 --> 00:18:50,070
doing in place. So if you think you're doing that, you're just fooling yourself.

357
00:18:50,140 --> 00:18:53,094
Pandas is not really doing an in place operation, even though it says it is.

358
00:18:53,132 --> 00:18:56,086
It also prohibits this chaining, which I think is going to really mess up your

359
00:18:56,108 --> 00:18:58,678
code, make it uglier and harder to read. And I think that's one of the

360
00:18:58,684 --> 00:19:01,606
most important things, is to have code that's easy to read. And then again you

361
00:19:01,628 --> 00:19:04,706
will get rid of that setting with warning error that is the bane of pandas

362
00:19:04,738 --> 00:19:07,974
developers everywhere. Okay, the next section is don't apply if you can.

363
00:19:08,012 --> 00:19:10,718
So let's just make our data frame here. I'm going to call auto two here,

364
00:19:10,804 --> 00:19:14,206
and this is sort of us centric, but maybe I want to move to something

365
00:19:14,228 --> 00:19:17,418
that's more like world centric. A lot of people, instead of doing miles per gallon,

366
00:19:17,434 --> 00:19:19,870
they don't even know what a mile, let alone a gallon is. They will use

367
00:19:19,940 --> 00:19:23,326
liters per 100 km. Right. As a us person, I don't really know

368
00:19:23,348 --> 00:19:26,126
what that is, but Americans are kind of weird that way. But I've got this

369
00:19:26,148 --> 00:19:29,758
function here that will convert miles per gallons to liters per 100.

370
00:19:29,764 --> 00:19:32,766
Conversion there is, you multiply it by 235. So what I can do is I

371
00:19:32,788 --> 00:19:34,950
can take like the city mileage and I call the supply method on it and

372
00:19:34,980 --> 00:19:38,294
I can do that. And there's our conversions there. This looks like that works.

373
00:19:38,412 --> 00:19:41,894
Note that I can also do this instead of doing apply here,

374
00:19:41,932 --> 00:19:45,286
I can just multiply that right here. And this gives me something that is actually

375
00:19:45,308 --> 00:19:48,838
the same value. Now the output is the same, but what's going on under the

376
00:19:48,844 --> 00:19:52,006
covers is actually very different because I'm calling apply. What apply is doing is

377
00:19:52,028 --> 00:19:55,766
going to take each individual entry in that column and run each individual entry

378
00:19:55,798 --> 00:19:59,334
into this python operation in this cell down here. Because I'm multiplying the column.

379
00:19:59,382 --> 00:20:02,182
What this is going to do is a vectorized or broadcast operation.

380
00:20:02,246 --> 00:20:04,918
Remember, Panda stores this as a block of data. It's going to take that block

381
00:20:04,934 --> 00:20:07,310
of data, multiply it by this number, and give me a new block of data

382
00:20:07,380 --> 00:20:11,290
without going back into Python. We're saving sort of the serialization deserialization

383
00:20:11,370 --> 00:20:13,966
step. In fact, we can just time this here. I'm going to run this little

384
00:20:13,988 --> 00:20:17,394
time operator and we'll see how long this takes with both of these

385
00:20:17,432 --> 00:20:21,278
options here. So it looks like the apply takes about six milliseconds.

386
00:20:21,374 --> 00:20:24,594
Let's look at the vectorized version here. It takes

387
00:20:24,632 --> 00:20:28,558
about 143. So sometimes this changes, but by my calculations

388
00:20:28,654 --> 00:20:31,574
this is from a previous run of numbers here, but it's pretty similar. It's around

389
00:20:31,612 --> 00:20:35,222
a 50 x difference in doing an apply versus doing

390
00:20:35,276 --> 00:20:38,502
the vectorized operation. So my point here is anytime you find yourself

391
00:20:38,556 --> 00:20:41,446
doing apply, check whether that is what you want to do. Now, there are some

392
00:20:41,468 --> 00:20:44,550
cases where apply sort of makes sense. Let's just sort of talk about them.

393
00:20:44,620 --> 00:20:47,830
Let's just say I want to check whether make is american or not.

394
00:20:47,900 --> 00:20:50,654
And I'm going to apply this little function here that just says is the value

395
00:20:50,692 --> 00:20:54,014
for that american? And I'll also do this other one. There's also an is in

396
00:20:54,052 --> 00:20:57,742
on make. And so if I do apply, that's 1.5

397
00:20:57,796 --> 00:21:00,926
milliseconds. You can notice that I do is in, that's about half the

398
00:21:00,948 --> 00:21:04,382
amount of time. So even with this string operation here, by doing an is,

399
00:21:04,436 --> 00:21:07,706
I am saving some operations because pandas can do it a little bit faster.

400
00:21:07,738 --> 00:21:10,482
Now, if we do something a little bit more complicated here, we might do something

401
00:21:10,536 --> 00:21:13,858
like I want to make a country column in here, and it's either going

402
00:21:13,864 --> 00:21:16,898
to be us or other. Again, I'm sort of us centric. Apologize for that if

403
00:21:16,904 --> 00:21:20,562
that offends you here, the timing for that is like 2.6 milliseconds.

404
00:21:20,706 --> 00:21:24,166
Generally, if I was doing a calculation and it had some

405
00:21:24,268 --> 00:21:27,366
conditional like this, I would use a where statement here. So this is using the

406
00:21:27,388 --> 00:21:30,114
where down here. This is how I canonically would do this with a calculation.

407
00:21:30,162 --> 00:21:32,982
However, in this case, because it's working with strings instead of numbers,

408
00:21:33,036 --> 00:21:35,366
it's going to be slower. So this is almost two times as slow to do

409
00:21:35,388 --> 00:21:38,674
this with the where. However, if you were to do a where on a numeric

410
00:21:38,722 --> 00:21:42,238
column rather than a string, it would actually see an improvement in data. Let's just

411
00:21:42,244 --> 00:21:45,486
do one more operation here. There's also this NP select, which is a nice

412
00:21:45,508 --> 00:21:48,554
little tool. And there's NPware, which again are very useful

413
00:21:48,602 --> 00:21:51,822
for basically building up conditional statements. They're a little bit more powerful than the where

414
00:21:51,876 --> 00:21:55,326
in pandas. However, in this case for strings, they are slower, but if

415
00:21:55,348 --> 00:21:59,562
you're doing numeric calculations, all of these will be faster. Than the apply operation

416
00:21:59,626 --> 00:22:03,662
there. My takeaway here with this apply, I see a lot of people using apply.

417
00:22:03,796 --> 00:22:07,058
Don't use it unless you're in a string situation. Otherwise you're

418
00:22:07,074 --> 00:22:10,134
going to want to use one of these NP selects or pandas where. NP where

419
00:22:10,172 --> 00:22:12,822
and your code is going to run a lot faster. I get it that where

420
00:22:12,876 --> 00:22:15,238
might be a little bit confusing. Your code might be a little bit harder to

421
00:22:15,244 --> 00:22:18,726
read. But if speed is of the essence, try and avoid apply if

422
00:22:18,748 --> 00:22:22,594
possible. And really with string operations, because strings are stored as python objects,

423
00:22:22,642 --> 00:22:25,858
you're going back to that python layer anyway. So in that case I'll

424
00:22:25,874 --> 00:22:29,074
say, yeah, apply is okay there. But generally I try to avoid apply for numeric

425
00:22:29,122 --> 00:22:32,954
operations. Okay, the last section here is aggregation. Let's talk about aggregation.

426
00:22:33,002 --> 00:22:36,142
Let's say that I want to compare mileage by country by year.

427
00:22:36,196 --> 00:22:39,118
And so when I started hearing buy this or buy that, basically it means to

428
00:22:39,124 --> 00:22:41,566
me that I want to do these group by aggregation. Pandas kind of has two

429
00:22:41,588 --> 00:22:43,358
ways to do this. One is with a group by, the other one is with

430
00:22:43,364 --> 00:22:46,654
a pivot table. I'm a software engineer by training and so group

431
00:22:46,692 --> 00:22:49,822
by makes sense to me. I'm not really can excel master. People who are very

432
00:22:49,876 --> 00:22:53,166
adept with Excel tend to favor the pivot table way of doing this,

433
00:22:53,188 --> 00:22:54,982
but both of them will get you to the same place. I'm just going two

434
00:22:54,996 --> 00:22:56,646
show the group by way here. So what I'm going to say is group by

435
00:22:56,668 --> 00:22:58,438
and then I'm going to take the mean here. So if you're not familiar with

436
00:22:58,444 --> 00:23:01,046
this, this is really powerful. What it's doing is it's going to take and put

437
00:23:01,068 --> 00:23:03,058
in the index the year. You can see in the left hand side, the bolded

438
00:23:03,074 --> 00:23:05,878
thing is the index. And then it's going to say, okay, for every row that

439
00:23:05,884 --> 00:23:08,886
had the year of 1984, get all the numeric columns and take the mean of

440
00:23:08,908 --> 00:23:11,830
each numeric column. Put that in the column. So this is actually pretty cool.

441
00:23:11,980 --> 00:23:14,758
I've written it out as three lines of code, but you could write it as

442
00:23:14,764 --> 00:23:18,326
a single line, a single operation to do this aggregation here, which is pretty cool.

443
00:23:18,428 --> 00:23:21,582
Now the sky's sort of the limit what you do with these. So instead of

444
00:23:21,716 --> 00:23:24,334
just doing all the columns, I can say I want to pull off combined and

445
00:23:24,372 --> 00:23:26,826
speed and just get the mean for those if I want to. So I stick

446
00:23:26,858 --> 00:23:29,806
in this little operation here. Note that I am chaining these. I'm writing these out

447
00:23:29,828 --> 00:23:32,846
each on their own line, and I can sort of walk through and debug that

448
00:23:32,868 --> 00:23:35,038
if I need to. Now, one thing I like to do is just stick on

449
00:23:35,044 --> 00:23:37,438
a plot. Humans aren't optimized for looking at tables of data. So I can look

450
00:23:37,444 --> 00:23:39,870
at this table and I can see it looks like speed is going up,

451
00:23:39,940 --> 00:23:42,238
but it's kind of hard for me to get a feel for what's going on.

452
00:23:42,244 --> 00:23:43,886
But if I just stick a plot on the end of this, what this is

453
00:23:43,908 --> 00:23:46,198
going to do, it's going to take the index by default, and each column it

454
00:23:46,204 --> 00:23:49,202
will plot as a line in there. So look at this. There's a visualization.

455
00:23:49,266 --> 00:23:52,598
This visualization tells me a lot more to me because I can visually see the

456
00:23:52,604 --> 00:23:55,638
representation, what's going on there, that our speeds are going up over time.

457
00:23:55,724 --> 00:23:58,806
Our combined mileage was sort of static until 2010,

458
00:23:58,828 --> 00:24:02,166
then it started taking up. Now, again, I'm not a car guru by any means,

459
00:24:02,188 --> 00:24:04,774
but this is kind of interesting just from looking at the visualization. So what happened?

460
00:24:04,812 --> 00:24:08,934
Why was there that inflection point around 2007 eight that made things

461
00:24:08,972 --> 00:24:11,374
start ticking up. And this could also be the data. We're also taking the mean,

462
00:24:11,412 --> 00:24:14,874
so there could be outliers in that. I'm imagining what this is, is electric cars,

463
00:24:14,922 --> 00:24:17,838
and electric cars tend to push off the mean a lot, so maybe we'd look

464
00:24:17,844 --> 00:24:21,054
at the median instead. And the median is still going up a little bit,

465
00:24:21,092 --> 00:24:24,590
but it doesn't look like it's going up quite as much as the

466
00:24:24,660 --> 00:24:27,902
mean there. And so in this case, what I'm going to do down here is

467
00:24:28,036 --> 00:24:30,606
we said by country, right? And ive just been doing this by year, but now

468
00:24:30,628 --> 00:24:32,286
what I'm going to do is I'm going to change some operations in here.

469
00:24:32,308 --> 00:24:34,478
I'm going to make a country column. So I'm going to do that, apply and

470
00:24:34,484 --> 00:24:36,486
get the country column in there. And then I'm going to group by look,

471
00:24:36,508 --> 00:24:38,566
I'm going to group by year and country. So let's look at what this gives

472
00:24:38,588 --> 00:24:40,866
me. This gives me this data frame with the index, you can see that there's

473
00:24:40,898 --> 00:24:43,446
both the year and the country in the index. So this is a hierarchical or

474
00:24:43,468 --> 00:24:47,158
a multi index, which is a little bit confusing to use. But hopefully you can

475
00:24:47,164 --> 00:24:49,942
see that there's power there. We can see for 1984. Here's the summary for us

476
00:24:49,996 --> 00:24:53,158
columns, and there's a summary for other non us as well. We can

477
00:24:53,164 --> 00:24:55,762
go a little bit deeper as well. So we can go down this rabbit hole

478
00:24:55,826 --> 00:24:58,750
here. I'm going to say, instead of just doing the mean here, I'm going to

479
00:24:58,760 --> 00:25:01,598
call AG, which allows me to call multiple aggregations. So I'm going to call the

480
00:25:01,604 --> 00:25:04,446
minimum the mean, and I'm even going to pass in my own defined function here

481
00:25:04,468 --> 00:25:07,582
that's going to take a series and aggregate that series, collapse it to a value.

482
00:25:07,636 --> 00:25:09,854
So in this case, my aggregation is a little bit weird. It takes a second

483
00:25:09,892 --> 00:25:12,574
to last value. Now this is just showing that you can pass in whatever function

484
00:25:12,612 --> 00:25:14,478
you want to. It just has to take a series and collapse it to a

485
00:25:14,484 --> 00:25:16,846
single value. And so here we go. Now, you can see that we have,

486
00:25:16,868 --> 00:25:20,238
in addition to hierarchical index, we have hierarchical columns. So for every numeric column here,

487
00:25:20,244 --> 00:25:22,878
we have the minimum value, the mean value, and the second to last value as

488
00:25:22,884 --> 00:25:25,846
well. So the sky's sort of the limit with what you want to do with

489
00:25:25,868 --> 00:25:27,926
these things. So what I'm going to do is I'm just going to go back

490
00:25:27,948 --> 00:25:30,546
to what I had, year and country, take the mean of that, and then plot

491
00:25:30,578 --> 00:25:33,334
that. And we get this plot here that's a little bit hard to grock here.

492
00:25:33,372 --> 00:25:36,438
I'm not quite sure what's going on. Issues here is that, remember, pandas is going

493
00:25:36,444 --> 00:25:39,506
to plot the index along the x axis. So now it's actually plotting these tuples

494
00:25:39,538 --> 00:25:42,430
of year and country like us or other in there, which makes it a little

495
00:25:42,440 --> 00:25:44,994
bit hard. So what you might want to do is one of these more advanced

496
00:25:45,042 --> 00:25:47,698
operations here where I'm going to use this unstack here. So if you're not familiar

497
00:25:47,714 --> 00:25:51,146
with unstack, let me just comment it out here and show you here is grouping

498
00:25:51,178 --> 00:25:53,358
by year and country. And then we're going to take the mean. If we come

499
00:25:53,364 --> 00:25:55,326
in here and say unstack, what that's going to do is it's going to take

500
00:25:55,348 --> 00:25:58,702
the innermost index, which is that country, and it's going to pop that up into

501
00:25:58,756 --> 00:26:01,338
the columns. And so you can see that we now have country in the columns.

502
00:26:01,354 --> 00:26:03,966
Now we only have a single thing in the index. We've got year. And if

503
00:26:03,988 --> 00:26:07,566
we wanted to look at, for example, just the city mileage for a year from

504
00:26:07,588 --> 00:26:09,998
that, we could pull off the city column, which is going to give us the

505
00:26:10,004 --> 00:26:12,878
sub columns there because that's hierarchical. Let me just comment that out and show you

506
00:26:12,884 --> 00:26:15,826
what that's giving us. It gives us that, again, we're going from this unstacked version,

507
00:26:15,858 --> 00:26:18,518
we're going to say city. And you can see I'm using this debug style to

508
00:26:18,524 --> 00:26:20,962
pull that out. Now we're going to plot that. Going to plot the index,

509
00:26:21,026 --> 00:26:23,526
and I'm going to plot each of those columns as a value and we get

510
00:26:23,548 --> 00:26:26,822
something that looks like this. So ive sort of used this construct of grouping by

511
00:26:26,876 --> 00:26:30,098
unstacking to illustrate some of the power. And this allows

512
00:26:30,114 --> 00:26:33,078
me to quickly visualize that and see what's going on with the US. We can

513
00:26:33,084 --> 00:26:36,662
see that the US city mileage is going up, maybe faster than the non us

514
00:26:36,716 --> 00:26:40,526
mileage, at least in this data set that I have here. Okay, so in

515
00:26:40,548 --> 00:26:43,166
summary, pandas is super powerful. You can do a lot of things with it with

516
00:26:43,188 --> 00:26:46,538
very few lines of code. In my experience, I've seen good pandas code and I've

517
00:26:46,554 --> 00:26:49,786
seen some bad pandas code. If you follow these steps, I guarantee

518
00:26:49,818 --> 00:26:52,654
that your code will be better. It will have less bugs, it will probably run

519
00:26:52,692 --> 00:26:55,859
faster, and your colleagues will probably be more happy with you because they're going to

520
00:26:56,359 --> 00:27:00,046
be able to understand your code. So correct types again. Those save time and

521
00:27:00,068 --> 00:27:03,882
make things fast and also give you functionality like date and string manipulation,

522
00:27:03,946 --> 00:27:06,914
chaining operations. That's two. Make your code more readable. It's going to remove bugs.

523
00:27:06,962 --> 00:27:09,254
It's going to be easier to debug as well. I showed some examples of that.

524
00:27:09,292 --> 00:27:12,118
Don't mutate. If you find yourself mutating, you should take a step back and think

525
00:27:12,124 --> 00:27:15,526
about how to do it without mutating. Again, apply is generally your

526
00:27:15,548 --> 00:27:18,806
enemy, so if you're trying to do math operations with apply, you're probably doing it

527
00:27:18,828 --> 00:27:21,922
the wrong way. Apply is okay for string operations, but generally should be avoided.

528
00:27:21,986 --> 00:27:24,998
And then aggregations are very powerful. I get that they can be confusing, but if

529
00:27:25,004 --> 00:27:27,734
you use this chain style and start building them up, it's going to help you

530
00:27:27,772 --> 00:27:31,246
understand what's going on and you're going to be very empowered. So thanks for watching.

531
00:27:31,428 --> 00:27:35,134
I hope you enjoyed this. If you want two learn

532
00:27:35,172 --> 00:27:38,842
more about pandas, you can follow me on Twitter. Underscore underscore Mharrison underscore underscore.

533
00:27:38,906 --> 00:27:41,694
I also have a discount code to my pandas. Course it's going to be 50%

534
00:27:41,732 --> 00:27:45,326
off for people who are watching this. So the code is learn pandas and that

535
00:27:45,348 --> 00:27:48,238
will be active for the next week. Good luck with your pandas. It's been a

536
00:27:48,244 --> 00:27:48,894
pleasure being with you.

