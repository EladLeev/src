1
00:00:27,010 --> 00:00:30,086
My name is Agita and today I will talk a bit about DevOps best

2
00:00:30,108 --> 00:00:35,062
practices and how they influence data ops mesh or

3
00:00:35,116 --> 00:00:38,278
data ops and data mesh concepts. I will provide

4
00:00:38,444 --> 00:00:41,906
an overview of each of these paradigms and explore how realistic

5
00:00:41,938 --> 00:00:45,602
it is to combine them. So it's very beginner friendly.

6
00:00:45,666 --> 00:00:49,286
And if you are data or

7
00:00:49,388 --> 00:00:53,002
DevOps engineer and you're curious about data best practices

8
00:00:53,066 --> 00:00:56,750
or data ops, this talk is for you as well as

9
00:00:56,820 --> 00:01:01,454
if you are a data engineer and you are curious about DevOps best

10
00:01:01,492 --> 00:01:04,980
practices, this also can be useful for you

11
00:01:07,270 --> 00:01:10,750
a little bit. About me at the moment I work at VMware.

12
00:01:10,910 --> 00:01:14,802
I'm part of versatile data kit team.

13
00:01:14,936 --> 00:01:18,850
It's an open source project that is automating actually data ops

14
00:01:19,430 --> 00:01:22,600
process. So that's why I'm curious to talk about it.

15
00:01:23,610 --> 00:01:27,394
I'm going to touch a little bit on promo

16
00:01:27,442 --> 00:01:30,618
versa dedicate because I'm on a mission to create a

17
00:01:30,624 --> 00:01:34,486
community around these project at the moment and I believe it's relevant

18
00:01:34,598 --> 00:01:37,500
to the topic and solves some problems that I will address.

19
00:01:39,070 --> 00:01:42,560
My personal connection and interest to the topic is that

20
00:01:42,930 --> 00:01:46,750
at the moment I'm a community manager but I have actually DevOps background.

21
00:01:47,090 --> 00:01:51,440
I worked as a DevOps engineer for around five years before

22
00:01:51,970 --> 00:01:56,180
transitioning to community management with three years

23
00:01:56,790 --> 00:02:00,946
burnout period when I didn't work. So I have some context and

24
00:02:00,968 --> 00:02:04,130
personal interest in talking about these concepts.

25
00:02:05,190 --> 00:02:08,594
And as a DevOps or ex DevOps engineer,

26
00:02:08,642 --> 00:02:12,758
I'm actually very excited to learn and use my experience to envision how

27
00:02:12,924 --> 00:02:16,610
data engineering teams can use their full potential following

28
00:02:16,690 --> 00:02:18,170
DevOps best practices.

29
00:02:20,350 --> 00:02:24,186
Learning about data ops and data mesh as part of this project that

30
00:02:24,208 --> 00:02:28,022
I'm working on now made me excited about this combination

31
00:02:28,086 --> 00:02:31,466
and actually I'm a bit of efficiency freak

32
00:02:31,498 --> 00:02:35,370
and I believe this can be very efficient

33
00:02:35,450 --> 00:02:38,814
combination. At the same time, I believe

34
00:02:38,852 --> 00:02:43,390
that there are some challenges that might arise and

35
00:02:43,460 --> 00:02:46,990
I'm going to also address these challenges

36
00:02:47,490 --> 00:02:51,026
and yeah, just final thing is that I want

37
00:02:51,048 --> 00:02:54,866
to say also kind of thanks to open source in

38
00:02:54,888 --> 00:02:58,680
general existence of open source projects because

39
00:02:59,450 --> 00:03:03,234
kind of after a little bit I mentioned the burnout.

40
00:03:03,282 --> 00:03:06,774
After the burnout I realized that I want to make this world a little bit

41
00:03:06,812 --> 00:03:10,314
a better place. And for me, working or coming back to

42
00:03:10,432 --> 00:03:14,442
data ops or DevOps world was only

43
00:03:14,496 --> 00:03:17,900
possible if I work on some open source project.

44
00:03:19,070 --> 00:03:24,846
Also you will see some graphics that I made. Actually in

45
00:03:24,868 --> 00:03:28,798
these slides you will see some graphics that I made and

46
00:03:28,964 --> 00:03:32,366
I just discovered that I have this little pen and I

47
00:03:32,388 --> 00:03:36,306
have a touch screen. My screen is working with

48
00:03:36,328 --> 00:03:39,794
this pen. And so I created several graphics here.

49
00:03:39,832 --> 00:03:43,746
They are not professional at all. This is my first time drawing but

50
00:03:43,768 --> 00:03:47,554
I just did it for my own pleasure and fun. So let's

51
00:03:47,602 --> 00:03:51,334
start. So my agenda for today is

52
00:03:51,532 --> 00:03:55,746
I will start with an introduction to DevOps

53
00:03:55,778 --> 00:03:59,046
best practices and then I will explain data Ops

54
00:03:59,078 --> 00:04:02,234
and how they are kind of correlated. Then I will

55
00:04:02,272 --> 00:04:05,862
go into Versailles data kit introduction

56
00:04:06,006 --> 00:04:09,654
very briefly because I'm going to explain data Ops

57
00:04:09,702 --> 00:04:13,278
lifecycle in the context of, or kind of with an example

58
00:04:13,364 --> 00:04:15,630
of the tool or framework.

59
00:04:16,450 --> 00:04:20,026
Then I will explain what is data mesh. And these I'm

60
00:04:20,058 --> 00:04:23,718
going to combine everything that I just talked about into this data ops

61
00:04:23,754 --> 00:04:27,634
mesh concept that well basically I

62
00:04:27,672 --> 00:04:30,962
came up with by myself, but I imagine that, and actually

63
00:04:31,016 --> 00:04:34,722
I have seen that some people also are talking or writing about

64
00:04:34,776 --> 00:04:38,006
this combination, powerful combination in my

65
00:04:38,028 --> 00:04:41,670
opinion. And then I asked or kind of the problems

66
00:04:41,740 --> 00:04:45,414
that I believe that might be there for the people who are trying

67
00:04:45,452 --> 00:04:48,614
to implementing both of these or combine

68
00:04:48,662 --> 00:04:52,778
these two concepts and

69
00:04:52,864 --> 00:04:56,890
also some solutions that I came up with under

70
00:04:56,960 --> 00:05:00,478
this research of creating this talk. Then I

71
00:05:00,484 --> 00:05:04,558
will talk a little bit about open source projects and just

72
00:05:04,724 --> 00:05:07,870
invite you to offer a little contribution.

73
00:05:09,490 --> 00:05:13,380
So for me, DevOps best practices basically

74
00:05:14,150 --> 00:05:17,714
are mainly focused, let's say, or I want to highlight two

75
00:05:17,752 --> 00:05:21,730
things. And first is automation and second is collaboration.

76
00:05:22,070 --> 00:05:25,334
Both of these are experienced, I think,

77
00:05:25,372 --> 00:05:28,402
on personally working in two projects.

78
00:05:28,466 --> 00:05:32,630
And my experience in DevOps was first I joined

79
00:05:33,370 --> 00:05:36,646
a smaller project of seven people and I

80
00:05:36,668 --> 00:05:40,538
worked there for three months. And the second project was quite large.

81
00:05:40,624 --> 00:05:44,746
I think there were several thousand people in that project when

82
00:05:44,768 --> 00:05:48,570
I joined as a DevOps. And I stayed in that project

83
00:05:48,640 --> 00:05:52,750
for four years and to explain what I was doing there,

84
00:05:52,820 --> 00:05:55,818
kind of maybe first project is more emphasized,

85
00:05:55,914 --> 00:05:59,806
I will kind of speak about it in context of automation and

86
00:05:59,828 --> 00:06:03,070
the second one in the context of collaboration.

87
00:06:04,710 --> 00:06:08,706
So when I joined the first project, basically what happened on

88
00:06:08,728 --> 00:06:12,030
my first day I arrived as a Junior DevOps engineer.

89
00:06:12,110 --> 00:06:15,574
They gave me just, I don't know, maybe seven to

90
00:06:15,612 --> 00:06:19,174
ten pages of a four printed paper with

91
00:06:19,212 --> 00:06:23,142
instructions that I have to follow every day. Basically my

92
00:06:23,196 --> 00:06:27,190
task was to support the migration

93
00:06:27,790 --> 00:06:31,926
of older sharing version to a newer spring

94
00:06:31,958 --> 00:06:36,006
version of the code or these tool that developers

95
00:06:36,038 --> 00:06:37,100
were working on.

96
00:06:40,290 --> 00:06:43,694
In order to do this, to support this

97
00:06:43,732 --> 00:06:47,722
migration I had to change or change several

98
00:06:47,786 --> 00:06:51,290
lines in some files,

99
00:06:51,450 --> 00:06:54,820
copy some folders, rename some things,

100
00:06:55,270 --> 00:06:58,580
do testing on Linux, do testing on windows and

101
00:06:59,350 --> 00:07:03,326
well yeah, kind of deployment tasks

102
00:07:03,438 --> 00:07:07,270
that were done manually, completely manually before I joined.

103
00:07:07,930 --> 00:07:11,906
And so what happened actually in my first week I managed

104
00:07:11,938 --> 00:07:17,682
to once go through the list of these instructions

105
00:07:17,826 --> 00:07:21,082
and by the end of the day, really I'm super happy.

106
00:07:21,136 --> 00:07:24,470
I'm turning to my lead and I'm saying, hey, hooray,

107
00:07:24,630 --> 00:07:28,314
I finished this. So I went

108
00:07:28,352 --> 00:07:31,774
through the instructions, so it's done. And my lead

109
00:07:31,892 --> 00:07:35,534
turns back to me and he says, you know, today actually we have

110
00:07:35,572 --> 00:07:39,040
to do this or repeat this four times more.

111
00:07:39,650 --> 00:07:43,022
And it was approximately, actually 06:00 p.m.

112
00:07:43,076 --> 00:07:46,654
So it was the end of the working day and I decided

113
00:07:46,702 --> 00:07:51,442
to prove myself and stay at work and actually follow

114
00:07:51,496 --> 00:07:55,134
the instructions and do it four more times. I think it took me 4

115
00:07:55,192 --> 00:07:58,600
hours to do it the second, third and fourth time.

116
00:07:59,770 --> 00:08:03,222
And yeah, it got me frustrated enough

117
00:08:03,276 --> 00:08:06,966
so to say, to promise myself that I will not

118
00:08:06,988 --> 00:08:10,186
repeat this. And so the next day when I arrived to the

119
00:08:10,208 --> 00:08:14,058
office, I just decided to start writing code.

120
00:08:14,224 --> 00:08:17,514
I started writing code with the simpler things like

121
00:08:17,552 --> 00:08:21,260
copying files, renaming some folders, and then

122
00:08:22,770 --> 00:08:26,622
kept going and automation them more into

123
00:08:26,756 --> 00:08:31,360
changing the lines in the code. And basically

124
00:08:32,770 --> 00:08:35,858
the idea is that by the end of those three months when I was in

125
00:08:35,864 --> 00:08:39,540
this project, I have automated absolutely everything. So I

126
00:08:40,470 --> 00:08:44,226
wrote code to automate every step of this process that was

127
00:08:44,248 --> 00:08:47,986
written across these pieces

128
00:08:48,018 --> 00:08:51,862
of paper. Well, basically I

129
00:08:51,916 --> 00:08:56,502
learned, and to me it was kind of quite

130
00:08:56,556 --> 00:08:59,420
obvious that if something can be written on paper,

131
00:09:00,430 --> 00:09:03,722
it can be definitely automated. And the team

132
00:09:03,776 --> 00:09:06,966
was actually really surprised and I think for them it wasn't

133
00:09:06,998 --> 00:09:10,670
that obvious, but they were actually super

134
00:09:10,740 --> 00:09:14,238
happy to see how I

135
00:09:14,244 --> 00:09:18,414
automated this whole process, that by the end of those three

136
00:09:18,452 --> 00:09:22,554
months this deployment could be done completely automatically,

137
00:09:22,602 --> 00:09:26,386
I think around 280 times in 2 hours or

138
00:09:26,408 --> 00:09:30,466
something. So yeah, literally I

139
00:09:30,488 --> 00:09:34,370
had nothing to do there. And even though the team wanted to keep me around,

140
00:09:34,440 --> 00:09:37,540
there was just nothing, no tasks to give me.

141
00:09:38,250 --> 00:09:41,730
So this is how I believe that DevOps is supporting automation.

142
00:09:41,810 --> 00:09:45,154
And also it goes a little bit in the second point in my slides,

143
00:09:45,202 --> 00:09:49,366
which is faster, better, cheaper, so no human error

144
00:09:49,398 --> 00:09:53,222
is there like everything is happening way faster than any manual

145
00:09:53,286 --> 00:09:56,810
work. And of course it's cheaper to have automation doing things

146
00:09:56,880 --> 00:10:00,546
instead of people. I was using CI CD

147
00:10:00,598 --> 00:10:04,606
tools and writing jobs on Jenkins at the time and

148
00:10:04,708 --> 00:10:08,046
making pipelines that are automating these process.

149
00:10:08,148 --> 00:10:12,160
And yeah, basically following the DevOps lifecycle and

150
00:10:12,950 --> 00:10:16,740
orchestration was not part of my job actually

151
00:10:18,470 --> 00:10:22,722
in this project. So I go into kind of

152
00:10:22,776 --> 00:10:26,182
these next project. When I finished with this three

153
00:10:26,236 --> 00:10:30,034
months in the first project, I got invited

154
00:10:30,082 --> 00:10:34,226
to another project that was basically infrastructure

155
00:10:34,338 --> 00:10:37,270
automation or orchestration with chef.

156
00:10:38,730 --> 00:10:42,394
But what I want to actually and what struck me the most in that

157
00:10:42,432 --> 00:10:45,594
project was that it was really focused on

158
00:10:45,632 --> 00:10:49,386
collaboration. So at the time I was living in

159
00:10:49,408 --> 00:10:53,680
Latvia and the project was

160
00:10:54,610 --> 00:10:58,222
on site, so it was in Germany. So every

161
00:10:58,276 --> 00:11:01,646
Monday, every week I was traveling to Germany, and every Thursday I was

162
00:11:01,668 --> 00:11:05,134
traveling back. And at first it didn't make

163
00:11:05,172 --> 00:11:08,978
any sense, but with time and actually with experience, I understood that

164
00:11:09,144 --> 00:11:12,434
it makes a lot and a lot of sense to be in the same room

165
00:11:12,472 --> 00:11:16,194
with other people who are working on the same thing. As I

166
00:11:16,232 --> 00:11:20,146
mentioned that the project was huge. There were many developers and apps

167
00:11:20,178 --> 00:11:23,894
and things getting built, but all

168
00:11:23,932 --> 00:11:26,902
the people who were working on it actually were in the same house,

169
00:11:26,956 --> 00:11:29,640
like in the same office, or most of them,

170
00:11:30,250 --> 00:11:33,400
but even more,

171
00:11:33,950 --> 00:11:37,114
even further. The people who were automation things,

172
00:11:37,232 --> 00:11:41,146
not just DevOps people, but everyone who was doing any type

173
00:11:41,168 --> 00:11:44,766
of automation actually were in the same room. So I got

174
00:11:44,788 --> 00:11:48,494
to meet these people that I'm working with, and actually it was great

175
00:11:48,532 --> 00:11:51,070
experience to learn the methodology,

176
00:11:51,970 --> 00:11:55,410
the mindset and this collaboration

177
00:11:56,230 --> 00:11:59,906
to actually to grow in this collaborative space

178
00:12:00,008 --> 00:12:03,714
where whenever something breaks, we all get together and

179
00:12:03,752 --> 00:12:06,310
we are solving the problem instantly on the spot,

180
00:12:07,450 --> 00:12:11,970
as soon as possible. And it just increases

181
00:12:12,050 --> 00:12:15,190
efficiency to the maximum. When I know that

182
00:12:15,260 --> 00:12:18,586
if I have a problem, I can go to these particular person or I can

183
00:12:18,608 --> 00:12:22,326
ask someone who knows someone else who might conduct me directly

184
00:12:22,358 --> 00:12:24,620
with the person and then we talk face to face.

185
00:12:25,630 --> 00:12:29,958
And of course it was pre Covid

186
00:12:30,054 --> 00:12:33,278
or before COVID times, and now the world is a

187
00:12:33,284 --> 00:12:36,942
little bit different. But I still believe that it is possible to have

188
00:12:36,996 --> 00:12:40,750
the same level of automation of collaboration between

189
00:12:40,820 --> 00:12:45,246
people if these teams are really connected.

190
00:12:45,438 --> 00:12:49,490
And this is going to be really relevant when I talk about data mesh,

191
00:12:50,870 --> 00:12:54,638
because splitting teams or putting all the relevant

192
00:12:54,734 --> 00:12:58,710
people in the same room or the same team even virtually, will make

193
00:12:58,780 --> 00:12:59,800
the big difference.

194
00:13:03,290 --> 00:13:06,854
So, data ops best

195
00:13:06,892 --> 00:13:11,002
practices basically, in a nutshell, data ops is

196
00:13:11,136 --> 00:13:15,254
DevOps, or data engineering equivalent of DevOps

197
00:13:15,302 --> 00:13:19,078
best practices. The idea is to speed up the data deployment

198
00:13:19,174 --> 00:13:21,020
while improving the quality.

199
00:13:22,990 --> 00:13:26,242
Data ops lifecycle is similar to the DevOps lifecycle.

200
00:13:26,326 --> 00:13:29,918
I'm going to dig deeper into it in the

201
00:13:30,004 --> 00:13:33,922
next slide here. I wish to mention that data

202
00:13:33,976 --> 00:13:37,934
Ops lifecycle actually hasn't been really agreed upon

203
00:13:37,982 --> 00:13:41,906
in the community. There are several options and several ways to

204
00:13:41,928 --> 00:13:44,500
see it, and I'm going to present my personal,

205
00:13:45,610 --> 00:13:49,254
let's say, subjective view on it after reading or

206
00:13:49,292 --> 00:13:51,270
researching on the topic.

207
00:13:53,210 --> 00:13:56,898
The difference between traditional data engineering and data Ops

208
00:13:56,914 --> 00:14:00,742
is that data Ops engineers work with data in an automated

209
00:14:00,806 --> 00:14:05,034
way, building their workflows or data pipelines and

210
00:14:05,072 --> 00:14:08,940
jobs that run automatically. So basically

211
00:14:09,390 --> 00:14:13,022
these data pipelines and jobs are kind of similar

212
00:14:13,156 --> 00:14:16,906
language or taken also from DevOps

213
00:14:16,938 --> 00:14:21,034
World. So if previously one data team might be dependent

214
00:14:21,082 --> 00:14:24,554
on another data team or on

215
00:14:24,692 --> 00:14:28,066
infrastructure team because of the sequence of

216
00:14:28,088 --> 00:14:32,850
data journey or some infrastructure permissions

217
00:14:33,270 --> 00:14:36,130
or accesses, now it's kind of solved.

218
00:14:36,730 --> 00:14:40,434
So data Ops is solving this, let's say decreasing

219
00:14:40,482 --> 00:14:43,954
dependencies, but I would not say that it's completely eliminating

220
00:14:44,082 --> 00:14:48,186
dependencies. I believe also that the people who

221
00:14:48,208 --> 00:14:52,022
are building data pipelines should be enabled to set up the infrastructure

222
00:14:52,086 --> 00:14:55,594
and orchestrate the entire data journey. So in

223
00:14:55,632 --> 00:14:59,594
perfect case scenario, data pipelines and infrastructure are

224
00:14:59,632 --> 00:15:02,622
built and maintained by the same people,

225
00:15:02,756 --> 00:15:06,222
which is not always the case. But still I kind of want to

226
00:15:06,356 --> 00:15:10,430
focus or have this in mind when I'm talking about data Ops.

227
00:15:12,790 --> 00:15:16,370
Still data teams are sometimes

228
00:15:16,520 --> 00:15:20,180
separated, but let's say

229
00:15:21,030 --> 00:15:25,006
I believe that it shouldn't be the case. And as I talk

230
00:15:25,048 --> 00:15:28,758
about this about data Ops, I take in consideration that

231
00:15:28,844 --> 00:15:32,690
they are doing the same thing together. So orchestration

232
00:15:32,770 --> 00:15:34,310
and building the pipelines.

233
00:15:36,890 --> 00:15:40,602
So as I'm going to jump into data

234
00:15:40,656 --> 00:15:44,074
Ops lifecycle and kind of give a practical example of

235
00:15:44,112 --> 00:15:47,610
it with versatile Datakit tool, I just

236
00:15:47,760 --> 00:15:51,420
wanted to highlight or explain a little bit what it does.

237
00:15:52,910 --> 00:15:56,430
So versatile Datakit is an open source project, as I mentioned,

238
00:15:56,500 --> 00:15:59,790
and it is found on GitHub.

239
00:16:00,130 --> 00:16:04,066
The code is there. And basically what

240
00:16:04,088 --> 00:16:07,874
it is is a framework that is created to

241
00:16:07,992 --> 00:16:11,554
build, run and manage data pipelines with basic Python or

242
00:16:11,592 --> 00:16:15,380
SQL knowledge on any cloud.

243
00:16:19,520 --> 00:16:23,100
So the emphasis kind of that is maybe

244
00:16:23,170 --> 00:16:27,324
relevant, and I believe relevant for this talk is that

245
00:16:27,522 --> 00:16:30,688
we use the word basic and it's going to solve a

246
00:16:30,694 --> 00:16:33,650
little bit later. One of the problems that I'm going to address.

247
00:16:35,700 --> 00:16:39,760
So basically I'm going to explain also what is a data pipeline.

248
00:16:40,660 --> 00:16:44,176
A data pipeline is a series of data processing steps

249
00:16:44,208 --> 00:16:47,684
scheduled and executed in a sequence, the same as in case

250
00:16:47,722 --> 00:16:52,004
of DevOps. But this

251
00:16:52,042 --> 00:16:57,492
pipeline exists in order to ingest,

252
00:16:57,556 --> 00:17:01,752
load and transform the data from its

253
00:17:01,806 --> 00:17:05,304
source to where data analysts can work

254
00:17:05,342 --> 00:17:06,830
with it.

255
00:17:09,360 --> 00:17:12,760
In some cases, the steps are also in sequence. In some cases,

256
00:17:12,840 --> 00:17:15,710
independent steps might run in parallel as well.

257
00:17:22,270 --> 00:17:25,514
Yeah, and now I will go to data

258
00:17:25,552 --> 00:17:29,340
Ops lifecycle to kind of explain also how it comes all together.

259
00:17:33,470 --> 00:17:36,526
Yeah, so basically this

260
00:17:36,548 --> 00:17:38,960
is the DevOps lifecycle as I see it.

261
00:17:40,050 --> 00:17:43,002
Plan, code, orchestrate, test, deploy, execute,

262
00:17:43,066 --> 00:17:46,750
monitor and feedback, and it goes and it cycles.

263
00:17:47,170 --> 00:17:50,286
It is similar to DevOps

264
00:17:50,318 --> 00:17:53,666
lifecycle in my opinion, but a little bit

265
00:17:53,768 --> 00:17:57,586
different. So I'm going to go through each step and then

266
00:17:57,608 --> 00:18:01,126
you can see the difference for yourself. So of course the

267
00:18:01,148 --> 00:18:04,614
most important, I believe part of

268
00:18:04,652 --> 00:18:08,166
the lifecycle is to plan. As we know, failing to

269
00:18:08,188 --> 00:18:12,002
plan is planning to fail. So this is the crucial

270
00:18:12,066 --> 00:18:16,102
step where the business value users and requirements

271
00:18:16,166 --> 00:18:19,978
are gathered and the tools are selected and everything that needs to

272
00:18:19,984 --> 00:18:23,930
be answered, needs to be answered here. So if these plan is solid

273
00:18:25,310 --> 00:18:28,846
we can proceed to the second step. The second step

274
00:18:28,868 --> 00:18:32,366
is code. Coding in data vault means writing the

275
00:18:32,388 --> 00:18:35,934
code for a pipeline to ingest, transform the data and test

276
00:18:35,972 --> 00:18:39,310
it locally. In our case,

277
00:18:39,380 --> 00:18:43,650
virtual data kit automates this part by introducing software development kit.

278
00:18:44,950 --> 00:18:48,770
The code can be written in Python SQL interchangeably as I mentioned,

279
00:18:48,840 --> 00:18:52,246
and is used to create data jobs with steps that run

280
00:18:52,268 --> 00:18:55,686
in specified sequence. Also database is

281
00:18:55,788 --> 00:18:59,794
selected and configured and data jobs are executed locally

282
00:18:59,842 --> 00:19:03,350
to test the code and make sure that data is ingested and

283
00:19:03,420 --> 00:19:07,918
transformed properly. So simple commands

284
00:19:07,954 --> 00:19:11,946
like VDK run is going to run the whole pipeline locally and give

285
00:19:11,968 --> 00:19:15,458
me the output and I can check if these desirable

286
00:19:15,654 --> 00:19:18,400
outcome is there.

287
00:19:19,890 --> 00:19:23,198
There are many tools, other tools that automate this step.

288
00:19:23,284 --> 00:19:27,810
Actually it may be using also other languages or even taking

289
00:19:27,880 --> 00:19:31,858
coding part out of the equation and

290
00:19:31,944 --> 00:19:35,794
providing can interface where data practitioners can do this by

291
00:19:35,832 --> 00:19:39,806
clicking buttons. For some it might be very helpful and

292
00:19:39,848 --> 00:19:43,926
for some it might be frustrating that actually we

293
00:19:43,948 --> 00:19:46,680
cannot debug as easily things.

294
00:19:48,010 --> 00:19:50,978
So the third step is orchestrate.

295
00:19:51,154 --> 00:19:54,850
This is a crucial operations part of the cycle. This step

296
00:19:54,940 --> 00:19:58,458
actually is independent from let's say code but

297
00:19:58,544 --> 00:20:02,694
definitely needs to be implemented or brought

298
00:20:02,742 --> 00:20:04,860
in before testing part.

299
00:20:08,290 --> 00:20:11,760
Yeah it can be also these case that

300
00:20:12,130 --> 00:20:16,638
these infrastructure team is setting up orchestration and data

301
00:20:16,804 --> 00:20:20,418
team is doing the coding part. But as I

302
00:20:20,424 --> 00:20:23,954
said previously, it's better when it's done

303
00:20:23,992 --> 00:20:28,146
by the same team to

304
00:20:28,168 --> 00:20:32,230
have a full ownership over the pipelines.

305
00:20:33,130 --> 00:20:36,854
The infrastructure is built here for the data to go through the environments like

306
00:20:36,892 --> 00:20:40,386
sharing where it gets tested and further promoted

307
00:20:40,418 --> 00:20:44,010
to production environment or preprod and

308
00:20:44,080 --> 00:20:48,826
used for analysis. Typically the code is

309
00:20:48,848 --> 00:20:53,002
pushed to git and then data is ingested into staging before

310
00:20:53,136 --> 00:20:56,366
it gets deployed to production. At this point,

311
00:20:56,468 --> 00:21:00,842
scheduling and orchestration of the pipeline is configured in the configuration

312
00:21:00,906 --> 00:21:04,586
files. In case of VDK, orchestration tools

313
00:21:04,618 --> 00:21:08,154
can schedule jobs, manage workflows and coordinate dependencies

314
00:21:08,202 --> 00:21:11,806
among tasks. VDK has implemented

315
00:21:11,838 --> 00:21:14,946
a control service component that is taking care

316
00:21:14,968 --> 00:21:19,042
of the infrastructure setup. So it is kind of also taking

317
00:21:19,096 --> 00:21:23,234
part in automating with this SDK, the code stage

318
00:21:23,282 --> 00:21:27,382
and the orchestration as well introducing this control

319
00:21:27,436 --> 00:21:32,746
service that is creating the

320
00:21:32,768 --> 00:21:33,770
infrastructure.

321
00:21:35,710 --> 00:21:39,674
Then the next step is testing so the testing once

322
00:21:39,712 --> 00:21:43,082
a data job runs on staging, the data

323
00:21:43,136 --> 00:21:46,586
can be tested. Now there are alerts for any user

324
00:21:46,618 --> 00:21:49,994
or system errors that can be tracked end to end. Testing ensures

325
00:21:50,042 --> 00:21:53,840
that no existing functionality is impacted and

326
00:21:54,210 --> 00:21:58,014
maybe some things that haven't been considered in the workflow, such as what

327
00:21:58,052 --> 00:22:00,882
would happen if and so on,

328
00:22:00,936 --> 00:22:05,090
and any type of bugs or discrepancies are fixed at this stage.

329
00:22:08,470 --> 00:22:11,686
Deploy stages after validating the data and

330
00:22:11,708 --> 00:22:14,470
resolving issues, it can be deployed to production.

331
00:22:14,890 --> 00:22:18,630
Deployments can be fully automated. So let's say if the

332
00:22:18,780 --> 00:22:22,446
data gets to staging and is tested automatically,

333
00:22:22,498 --> 00:22:26,246
it can automatically also get deployed to production

334
00:22:26,358 --> 00:22:29,370
or another way also can be introduced.

335
00:22:33,600 --> 00:22:36,928
Okay then execution execute step is the

336
00:22:36,934 --> 00:22:40,204
pipeline now is running automatically based on the configuration.

337
00:22:40,332 --> 00:22:44,556
Automated data lifecycle processing is in place which schedules

338
00:22:44,588 --> 00:22:47,360
ingestion, transformation, testing and monitoring.

339
00:22:47,780 --> 00:22:51,220
Now reports can be generated. VDK control

340
00:22:51,290 --> 00:22:54,596
service has functionality for both deployments and execution of the

341
00:22:54,618 --> 00:22:58,724
jobs. Monitoring is in place

342
00:22:58,922 --> 00:23:01,990
to track failing as quickly as possible.

343
00:23:03,100 --> 00:23:06,484
Usually it's automation set up to alert the data jobs.

344
00:23:06,532 --> 00:23:10,312
If the data jobs are failing, alerts will be sent out to

345
00:23:10,366 --> 00:23:13,876
the user specified email and provide containing

346
00:23:13,908 --> 00:23:17,420
data job name and type of error. It is also possible

347
00:23:17,490 --> 00:23:21,388
with VDK in this case to detect if

348
00:23:21,474 --> 00:23:24,716
it's a user side error or a system error, or like a

349
00:23:24,738 --> 00:23:28,240
configuration error or

350
00:23:28,310 --> 00:23:32,156
a platform error which is going to be also included

351
00:23:32,188 --> 00:23:34,210
in this alert message.

352
00:23:35,860 --> 00:23:39,720
Then monitoring is going to provide this information to help users

353
00:23:39,740 --> 00:23:42,996
to troubleshoot and fix these pipelines as soon as they get

354
00:23:43,018 --> 00:23:46,432
the alert. The final step is feedback.

355
00:23:46,496 --> 00:23:49,860
And these, these additional requirements might arise.

356
00:23:50,200 --> 00:23:53,640
Some data might be missing or something

357
00:23:53,790 --> 00:23:56,868
might be necessary to improve.

358
00:23:57,044 --> 00:24:00,276
So when the feedback is collected again, the planning

359
00:24:00,308 --> 00:24:03,880
can be done and the cycle repeats.

360
00:24:06,720 --> 00:24:10,030
So here I sketched a little bit with my little pen.

361
00:24:11,920 --> 00:24:15,676
Also some components of EdK that I was mentioning here

362
00:24:15,698 --> 00:24:19,650
in this previous slide for the visual representation. Just because

363
00:24:20,500 --> 00:24:23,840
I understood for myself that I would need something like this in order to understand

364
00:24:23,910 --> 00:24:27,200
it better. Some of the

365
00:24:27,270 --> 00:24:30,820
components, but not all of these,

366
00:24:30,970 --> 00:24:35,012
but basically, yeah. So this is, let's say data part,

367
00:24:35,066 --> 00:24:38,948
and this is like Ops part that is

368
00:24:39,034 --> 00:24:42,470
combined in the Versailleskit project.

369
00:24:43,160 --> 00:24:47,572
So data jobs are running in Python SQL. There is a command line interface

370
00:24:47,636 --> 00:24:50,810
where I can run jobs locally and test them.

371
00:24:51,420 --> 00:24:55,404
The data jobs are following these steps and actually

372
00:24:55,442 --> 00:24:59,276
it's prefixed by in alphanumerical order.

373
00:24:59,458 --> 00:25:03,324
So let's say the name of the file is going

374
00:25:03,362 --> 00:25:06,976
to also be the

375
00:25:06,998 --> 00:25:11,264
sequence defining the sequence it is doing.

376
00:25:11,462 --> 00:25:15,308
ETL or ELT. Actually automation which is extract,

377
00:25:15,404 --> 00:25:19,920
transform and load by providing plugins and templates

378
00:25:20,080 --> 00:25:23,060
and in general automating these parts.

379
00:25:25,320 --> 00:25:29,764
So not really in depth knowledge is necessary to

380
00:25:29,802 --> 00:25:33,524
do them. So basically it's for locally

381
00:25:33,572 --> 00:25:38,170
running these jobs and then the other kind of ops component is

382
00:25:39,180 --> 00:25:42,920
scheduling and execution of these jobs in kubernetes environment

383
00:25:43,420 --> 00:25:46,684
using Git. So we upload the code

384
00:25:46,722 --> 00:25:50,268
to git and then we deploy it and from there we can

385
00:25:50,354 --> 00:25:53,980
set secrets if necessary and monitor

386
00:25:54,320 --> 00:25:55,580
the pipeline.

387
00:25:59,220 --> 00:26:02,156
And now I jump into data mesh.

388
00:26:02,348 --> 00:26:06,540
And data mesh was actually quite a new concept,

389
00:26:06,700 --> 00:26:10,096
I would say young, way younger

390
00:26:10,128 --> 00:26:14,020
than DevOps and younger than data Ops as well.

391
00:26:14,170 --> 00:26:17,590
But nevertheless it's extremely popular now.

392
00:26:18,040 --> 00:26:23,316
I think people are really considering

393
00:26:23,348 --> 00:26:27,108
this as a really good practices. So it was invented by Jamaic

394
00:26:27,124 --> 00:26:30,676
Degani in 2019. And basically it's

395
00:26:30,708 --> 00:26:34,540
a type of data platform architecture that

396
00:26:34,610 --> 00:26:38,956
leverages domain oriented and self service design to

397
00:26:38,978 --> 00:26:41,900
embrace the ubiquity of data enterprises.

398
00:26:43,520 --> 00:26:46,030
In this case, the domain is a business area,

399
00:26:46,560 --> 00:26:49,790
meaning each business area owns its data,

400
00:26:50,240 --> 00:26:53,516
and data measures foster data ownership

401
00:26:53,708 --> 00:26:57,564
among data owners who are held accountable

402
00:26:57,612 --> 00:26:59,830
for delivering their data as products.

403
00:27:00,520 --> 00:27:04,070
And so data as product actually kind of comes in.

404
00:27:04,760 --> 00:27:08,720
Each domain is responsible for managing its pipelines,

405
00:27:08,800 --> 00:27:12,036
and once the data has been served and transformed by

406
00:27:12,058 --> 00:27:15,336
a given domain, the domain owners can then leverage the

407
00:27:15,358 --> 00:27:18,090
data for their analytics or operational needs.

408
00:27:18,860 --> 00:27:21,956
As the MAF argues, data architectures can be mostly

409
00:27:21,988 --> 00:27:25,616
easily scaled by being broken down into smaller domain

410
00:27:25,668 --> 00:27:28,984
oriented components. In short, data mesh

411
00:27:29,032 --> 00:27:32,300
means that the data owners or domain owners,

412
00:27:32,640 --> 00:27:36,332
people who are directly involved with particular data, are also

413
00:27:36,386 --> 00:27:39,280
building and maintaining their data pipelines.

414
00:27:40,260 --> 00:27:43,596
Users are becoming the owners, and so the data becomes

415
00:27:43,628 --> 00:27:45,600
a product and is self serviced.

416
00:27:46,580 --> 00:27:51,444
This alone also does not completely eliminate dependencies because

417
00:27:51,562 --> 00:27:55,232
let's say if we implement data mesh,

418
00:27:55,296 --> 00:27:58,564
and the orchestration and testing and CRCD are set up

419
00:27:58,602 --> 00:28:02,184
by another and managed by another team, then the

420
00:28:02,222 --> 00:28:06,136
domain owners will depend on the infrastructure engineers who will be supporting the

421
00:28:06,158 --> 00:28:09,736
orchestration. So this

422
00:28:09,758 --> 00:28:13,850
is the reason why I decided kind of to combine these two

423
00:28:14,400 --> 00:28:18,168
paradigms or concepts. So here is the data mesh

424
00:28:18,264 --> 00:28:19,550
on the left,

425
00:28:21,440 --> 00:28:25,276
like adding governance, self service data as

426
00:28:25,298 --> 00:28:31,356
a product, and domain ownership to

427
00:28:31,378 --> 00:28:34,896
the data cycle and data ops part of it

428
00:28:34,918 --> 00:28:39,200
that is providing these CI CD testing, observability and orchestration.

429
00:28:44,360 --> 00:28:47,904
Basically kind of seeing this image, this is not my original

430
00:28:47,952 --> 00:28:51,796
image. I found it in an article and

431
00:28:51,818 --> 00:28:55,112
then I kind of made it into my own

432
00:28:55,246 --> 00:28:59,064
graphics. But this is how I also inspired to

433
00:28:59,102 --> 00:29:02,244
think about the combination of data ops and data mesh,

434
00:29:02,372 --> 00:29:05,848
which I strongly believe enables true collaboration and powers

435
00:29:05,864 --> 00:29:09,820
up the data engineering process by completely eliminating dependencies.

436
00:29:10,960 --> 00:29:14,444
So if the data ops engineer owns the data pipelines and

437
00:29:14,482 --> 00:29:18,336
is enabled to have ownership over the infrastructure and orchestration of

438
00:29:18,358 --> 00:29:22,304
the data cycle and data mesh means that the

439
00:29:22,342 --> 00:29:25,536
domain owners are owning the data as well as

440
00:29:25,558 --> 00:29:29,570
the pipelines, and in this case infrastructure too.

441
00:29:30,420 --> 00:29:34,784
I believe that the result will bring the speed to the data driven projects

442
00:29:34,832 --> 00:29:37,780
to be as fast as possible with no dependencies,

443
00:29:39,240 --> 00:29:42,744
which I believe is already happening in these DevOps role. Because sometimes,

444
00:29:42,862 --> 00:29:46,376
let's say full stack developers or DevOps engineers can

445
00:29:46,398 --> 00:29:48,920
set up the infrastructure, build the pipelines,

446
00:29:50,140 --> 00:29:54,212
or be in the same room with the developers who are directly working with

447
00:29:54,286 --> 00:29:58,380
building these product. So how

448
00:29:58,450 --> 00:30:02,156
VDK supports data mesh? Well, data Mesh is

449
00:30:02,178 --> 00:30:07,736
an organizational concept. It is implemented by managing

450
00:30:07,768 --> 00:30:10,990
the teams and enabling them to work fully with their data.

451
00:30:12,160 --> 00:30:15,936
Besides these, automation of the process, versatile data kit introduces the

452
00:30:15,958 --> 00:30:19,644
functionality of creating teams. So basically, as a data

453
00:30:19,702 --> 00:30:23,060
job is created, the prerequisite is to also

454
00:30:23,210 --> 00:30:26,230
specify which team is going to be working on it.

455
00:30:27,560 --> 00:30:30,756
Besides the team functionality, VDK also has templates and

456
00:30:30,778 --> 00:30:34,024
plugins to support these teams. And in order

457
00:30:34,062 --> 00:30:37,992
not to reinvent the wheel each time, teams can

458
00:30:38,046 --> 00:30:41,332
share the work they've done with each other and collaboration

459
00:30:41,396 --> 00:30:45,324
more efficiently. And these are some of these functionalities that

460
00:30:45,442 --> 00:30:48,620
can support efficient data ops mesh implementation.

461
00:30:50,080 --> 00:30:53,676
So what can go wrong? Well, I believe that two things can

462
00:30:53,698 --> 00:30:57,772
go wrong or possibly prevent data ops

463
00:30:57,836 --> 00:31:01,600
mesh from happening, and the first one is the skills.

464
00:31:02,100 --> 00:31:09,636
So kind of while I was researching this, I was thinking that basically

465
00:31:09,738 --> 00:31:14,016
I was thinking that the required skills

466
00:31:14,048 --> 00:31:17,264
to execute data combined with data mesh

467
00:31:17,392 --> 00:31:20,656
successfully are so skills

468
00:31:20,688 --> 00:31:24,596
or knowledge. The person in these team, in each domain should have the knowledge

469
00:31:24,628 --> 00:31:27,784
about the domain. Then they have to have

470
00:31:27,822 --> 00:31:31,800
data engineering skills like Python or SQL or anything

471
00:31:31,870 --> 00:31:35,500
else, depending on the tool that they're using for writing their pipelines.

472
00:31:36,560 --> 00:31:39,836
And the third thing that they need to know is how to set up

473
00:31:39,858 --> 00:31:44,060
their infrastructure and basically DevOps skills.

474
00:31:45,920 --> 00:31:49,404
As I was creating these slides, my question was whether it's

475
00:31:49,452 --> 00:31:53,024
easy or even possible to find a person who might fit

476
00:31:53,062 --> 00:31:56,930
and have all these skills that are necessary for this,

477
00:31:57,700 --> 00:32:01,076
or actually if it's possible to train them, or even if they

478
00:32:01,098 --> 00:32:05,152
would be willing to learn. But as I presented

479
00:32:05,216 --> 00:32:08,272
this talk to my team, this presentation,

480
00:32:08,336 --> 00:32:12,372
and actually this topic in general, it became

481
00:32:12,436 --> 00:32:16,856
obvious that the

482
00:32:16,878 --> 00:32:21,112
skills issue actually is solved by VDK or

483
00:32:21,166 --> 00:32:25,080
other tools that can automate the data engineering and operations

484
00:32:25,160 --> 00:32:28,920
process. So as I said in the beginning, that basic

485
00:32:29,000 --> 00:32:32,510
Python and SQL skills are required to

486
00:32:33,040 --> 00:32:36,524
build, run and manage data pipelines with VDK and

487
00:32:36,562 --> 00:32:39,776
as far as I know, and also from my personal, let's say,

488
00:32:39,798 --> 00:32:43,810
DevOps experience, we learn every day as DevOps and

489
00:32:45,060 --> 00:32:49,548
we do something, we use new tools, new languages every day.

490
00:32:49,734 --> 00:32:53,088
So any person with understanding and kind of capability

491
00:32:53,184 --> 00:32:56,196
of learning, I believe, could be able to

492
00:32:56,218 --> 00:33:00,470
create a simple pipeline to start with, but also

493
00:33:01,740 --> 00:33:05,416
to create more difficult pipelines also as well

494
00:33:05,598 --> 00:33:09,748
with time, just by following documentation.

495
00:33:09,924 --> 00:33:14,596
So actually not just creating a pipeline, but also setting up infrastructure,

496
00:33:14,788 --> 00:33:16,860
following the documentation.

497
00:33:18,560 --> 00:33:22,268
And this could be just one of the many solutions to implement data ops and

498
00:33:22,274 --> 00:33:25,996
data mesh in real life. But this is one that I see

499
00:33:26,098 --> 00:33:29,120
that is or creates this possibility.

500
00:33:30,420 --> 00:33:33,772
And the second thing that I wanted to kind of highlight

501
00:33:33,836 --> 00:33:39,700
or that came to my mind as possible

502
00:33:39,770 --> 00:33:43,316
issue is that basically in

503
00:33:43,338 --> 00:33:44,710
my opinion, it's important.

504
00:33:46,600 --> 00:33:50,592
The trust is important because domains will now have full ownership

505
00:33:50,656 --> 00:33:54,388
over that data and infrastructure. So when it's

506
00:33:54,404 --> 00:33:58,052
possibly more reliable to have the infrastructure set up separately

507
00:33:58,116 --> 00:34:00,890
from the domain and simply give access to the data,

508
00:34:01,580 --> 00:34:05,336
some companies or some leads or some management might not have

509
00:34:05,358 --> 00:34:09,036
enough trust to allow their domain owners to own the infrastructure as

510
00:34:09,058 --> 00:34:12,092
well because of the risks that come with the full power over

511
00:34:12,146 --> 00:34:15,724
it, like accidental deletion and any

512
00:34:15,762 --> 00:34:18,924
type of kind of errors,

513
00:34:18,972 --> 00:34:22,464
human errors also. But I

514
00:34:22,502 --> 00:34:26,444
believe that this can be solved by implementing some rules and functionality

515
00:34:26,492 --> 00:34:29,828
too. But still, this question remains kind

516
00:34:29,834 --> 00:34:33,204
of unanswered to me, and I'm curious to see how data

517
00:34:33,242 --> 00:34:36,390
ops and data mesh will evolve in the future.

518
00:34:37,640 --> 00:34:41,344
So with these questions, it kind of sums

519
00:34:41,392 --> 00:34:45,640
up from my side on the data ops and data mesh.

520
00:34:46,860 --> 00:34:51,352
And yeah, to close this talk, I wanted to thank

521
00:34:51,406 --> 00:34:54,876
you and say that I'm really open to hearing some

522
00:34:54,898 --> 00:34:58,588
feedback. You can find me with my name,

523
00:34:58,754 --> 00:35:02,220
Agita Yancem on LinkedIn,

524
00:35:02,640 --> 00:35:05,416
any social media. I'm also on Twitter.

525
00:35:05,608 --> 00:35:10,336
Connect with me and I'm really happy to kind

526
00:35:10,358 --> 00:35:13,920
of explore these

527
00:35:13,990 --> 00:35:18,130
concepts even more and actually find out if they work for some people.

528
00:35:20,360 --> 00:35:24,244
And now I want to just dedicate a little moment to speak about these open

529
00:35:24,282 --> 00:35:32,184
source projects. And I believe that open

530
00:35:32,222 --> 00:35:35,636
source projects are like versatile data kit

531
00:35:35,668 --> 00:35:39,512
and actually others are creating or kind of enabling us to

532
00:35:39,566 --> 00:35:43,748
not to pay a lot of money for some functionality

533
00:35:43,844 --> 00:35:46,750
so we actually get some free tools that we can use.

534
00:35:47,120 --> 00:35:51,564
And I believe that also they deserve the visibility that

535
00:35:51,602 --> 00:35:56,136
is crucial for open source tools or projects

536
00:35:56,328 --> 00:35:59,440
to get more known and used and also to

537
00:35:59,510 --> 00:36:03,168
gain potential contributors. And what I

538
00:36:03,174 --> 00:36:07,408
want to say is that actually there is a little support

539
00:36:07,494 --> 00:36:11,308
or a little contribution, like giving a star can go a long way.

540
00:36:11,494 --> 00:36:15,296
And I suggest or invite you and I will be really grateful

541
00:36:15,408 --> 00:36:19,236
if you support me my team and actually what we

542
00:36:19,258 --> 00:36:22,616
do at the moment by creating this tool by giving us

543
00:36:22,638 --> 00:36:26,280
a star so you can just scan the QR code

544
00:36:26,350 --> 00:36:31,400
or just google versailles etiquette kit and on the top right

545
00:36:31,470 --> 00:36:35,996
corner there is a little star and if you would just spend

546
00:36:36,178 --> 00:36:39,532
a minute or so just doing this it could support me

547
00:36:39,586 --> 00:36:43,790
greatly and enable some people to reach

548
00:36:44,720 --> 00:36:48,524
and to find the project to use it or to contribute

549
00:36:48,572 --> 00:36:50,770
or at least to try it out.

550
00:36:54,500 --> 00:37:00,032
So yeah actually that concludes my

551
00:37:00,086 --> 00:37:03,824
talk. I wanted to say thank you so much for taking time to be

552
00:37:03,862 --> 00:37:07,840
with me I deeply appreciate having the opportunity to be on

553
00:37:07,910 --> 00:37:12,080
conf fourty two and also I want to thank the organizer

554
00:37:13,260 --> 00:37:16,584
these conference is organized very professionally and I feel so

555
00:37:16,622 --> 00:37:20,184
far really positive about how

556
00:37:20,222 --> 00:37:23,784
it is managed so as I said

557
00:37:23,822 --> 00:37:27,496
I welcome feedback connect with me and my

558
00:37:27,518 --> 00:37:30,744
name is agite and thank you so much and see you

559
00:37:30,782 --> 00:37:31,940
next time bye.

