1
00:00:41,090 --> 00:00:44,818
Hello I'm Rafael and I'm speaking to you from Poland.

2
00:00:44,914 --> 00:00:48,534
My talk is titled Architectural caching patterns for

3
00:00:48,572 --> 00:00:52,414
kubernetes and I will tell you what different approaches you can

4
00:00:52,452 --> 00:00:56,058
use in caching while using kubernetes. What are the applications

5
00:00:56,154 --> 00:00:59,534
for your system designs? But first a few words about myself.

6
00:00:59,652 --> 00:01:03,274
I'm cloudnative team lead lead at Hazelcast, and before Hazelcast

7
00:01:03,322 --> 00:01:07,374
I worked at Google and CERn. I'm also an author of the book continuous

8
00:01:07,422 --> 00:01:10,894
delivery with Docker and Jenkins. And from time to time I do conference

9
00:01:10,942 --> 00:01:14,782
speaking and trainings, but on a daily basis I'm an engineer.

10
00:01:14,926 --> 00:01:18,326
A few words, but Hazelcast. Hazelcast is a distributed company

11
00:01:18,428 --> 00:01:21,926
and is distributed in two meanings. First meaning is we

12
00:01:21,948 --> 00:01:25,346
are distributed company because we produce distributed software.

13
00:01:25,458 --> 00:01:28,974
Our products are hazelcast in memory, data grid,

14
00:01:29,042 --> 00:01:32,282
hazelcast jet and hazelcast cloud. But the second

15
00:01:32,336 --> 00:01:35,914
meaning is that we are distributed company because we always

16
00:01:36,032 --> 00:01:39,626
work remotely. So it was always that way. Our agenda for

17
00:01:39,648 --> 00:01:42,702
today is pretty simple. So there will be a very short

18
00:01:42,756 --> 00:01:46,874
introduction about caching on kubernetes in the microservice

19
00:01:46,922 --> 00:01:50,654
world in general. And then we will walk through all possible

20
00:01:50,772 --> 00:01:54,378
caching patterns that you can use in your system. And while I'll

21
00:01:54,394 --> 00:01:58,482
be talking, I would like you to think about two things. First thing is

22
00:01:58,616 --> 00:02:02,610
which of this pattern you use in your system because

23
00:02:02,680 --> 00:02:05,554
you must use one of them, because this list is complete.

24
00:02:05,672 --> 00:02:09,030
And the second question I would like you to ask yourself

25
00:02:09,180 --> 00:02:12,854
is youll it make sense for my system to change to any

26
00:02:12,892 --> 00:02:16,678
other pattern? Youll it be beneficial to me. And with this question

27
00:02:16,844 --> 00:02:20,614
I leave you to listen to this talk. So we are in the microservice

28
00:02:20,662 --> 00:02:24,218
world, in kubernetes we deploy microservices and that is a

29
00:02:24,224 --> 00:02:27,418
diagram of a classic microservice system. So we have a

30
00:02:27,424 --> 00:02:30,310
lot of services, they have different versions,

31
00:02:30,390 --> 00:02:33,642
they are written in different programming languages and they

32
00:02:33,696 --> 00:02:37,182
use each other. Now the question for this talk is where

33
00:02:37,236 --> 00:02:40,574
is the right place to put your cache? Is it inside of each

34
00:02:40,612 --> 00:02:44,126
microservice? Or maybe as a separate thing in

35
00:02:44,148 --> 00:02:47,682
your infrastructure? Or maybe we should put cache in front of each service.

36
00:02:47,816 --> 00:02:51,266
And that what we will discuss. So the

37
00:02:51,288 --> 00:02:54,802
first caching pattern, the first topology that you can use is

38
00:02:54,856 --> 00:02:59,042
embedded cache. Embedded cache is like the simplest possible

39
00:02:59,176 --> 00:03:02,502
thing you can think about. A diagram for this looks as follow.

40
00:03:02,556 --> 00:03:06,246
So we deploy it on Kubernetes. So as always, kubernetes request goes

41
00:03:06,268 --> 00:03:09,650
to our system, it goes to the Kubernetes service. Kubernetes service

42
00:03:09,740 --> 00:03:13,578
forwards the request to one of the Kubernetes pod in which our application

43
00:03:13,664 --> 00:03:16,634
is running and we have a cache inside our application,

44
00:03:16,752 --> 00:03:20,346
embedded as a library inside our application. So request goes

45
00:03:20,368 --> 00:03:24,142
to our application, application checks in the cache okay, did I already

46
00:03:24,196 --> 00:03:27,214
executed such a request? If yes, return the cache value.

47
00:03:27,252 --> 00:03:31,246
If no, do some business logic, put the value into the cache, return the

48
00:03:31,268 --> 00:03:35,606
risk. This is so simple that we could even think about writing

49
00:03:35,658 --> 00:03:39,278
this caching logic on our own. So if you happen to use Java,

50
00:03:39,374 --> 00:03:43,262
that is how it could look like. So we can have some collection

51
00:03:43,326 --> 00:03:46,594
like concurrent hashmap then processing the request, okay check

52
00:03:46,632 --> 00:03:49,926
if the request is into the cache. If yes, return the cached value.

53
00:03:50,028 --> 00:03:53,542
If no, do some processing, put the value into the cache and return

54
00:03:53,596 --> 00:03:57,190
the response. If you use some other language, data will be the same.

55
00:03:57,260 --> 00:04:00,906
Now you can implement it on your own. However, please never do

56
00:04:00,928 --> 00:04:04,406
it. Never do it because a collection or concurrent

57
00:04:04,438 --> 00:04:07,626
collection is not good as a cache. It's not good because it

58
00:04:07,648 --> 00:04:11,190
has no eviction policy, no max size limit, no statistics,

59
00:04:11,270 --> 00:04:14,986
no expiration time, no notification mechanism. It misses a lot

60
00:04:15,008 --> 00:04:18,334
of feature that you will need from the cache. That is why if you happen

61
00:04:18,372 --> 00:04:21,646
to use Java, there are a lot of good libraries. Guava is one of

62
00:04:21,668 --> 00:04:24,926
them, where you can define all these missing features upfront in

63
00:04:24,948 --> 00:04:28,302
your constructor. Or eh, cache is also another good solution.

64
00:04:28,366 --> 00:04:31,554
If you use some other languages for every language you will find

65
00:04:31,592 --> 00:04:35,486
a good library for caching. Now we can move this idea of caching

66
00:04:35,598 --> 00:04:39,142
one level higher and put it into our application.

67
00:04:39,276 --> 00:04:42,914
So if you again work with Java, your application framework

68
00:04:42,962 --> 00:04:46,210
may be spring. So if youll would like to cache something with spring,

69
00:04:46,290 --> 00:04:50,034
you don't need to write all this manual code, you just annotate

70
00:04:50,082 --> 00:04:53,722
your method cachable and then every call to this method will

71
00:04:53,776 --> 00:04:57,674
first check okay if the given ISBN is already in the

72
00:04:57,712 --> 00:05:00,746
cache called books. If yes, returned the cached value,

73
00:05:00,848 --> 00:05:04,350
and only if the value is not found in the cache called

74
00:05:04,420 --> 00:05:07,934
books only then execute a method find book

75
00:05:07,972 --> 00:05:11,614
in slow source. But be careful if youll use spring because for some reason

76
00:05:11,732 --> 00:05:16,286
spring uses concurrent hashmap by default. So you're

77
00:05:16,318 --> 00:05:20,018
better off changing your caching manager to something else

78
00:05:20,104 --> 00:05:23,394
to for example guava. So embedded cache is pretty simple.

79
00:05:23,512 --> 00:05:27,346
But there is one problem with embedded cache. So imagine now that request goes

80
00:05:27,368 --> 00:05:30,918
to our service, it's forwarded to the application. Let's say on the top

81
00:05:31,004 --> 00:05:34,626
we do some long lasting business logic, put the value in the cache,

82
00:05:34,658 --> 00:05:38,274
return the response all good. Now the second time the same request

83
00:05:38,322 --> 00:05:41,926
may go to the Kubernetes service, but it's load balanced

84
00:05:41,958 --> 00:05:45,802
to the application at the bottom. And now what happens? The application

85
00:05:45,936 --> 00:05:50,262
needs to do this business logic once again, because these caches

86
00:05:50,326 --> 00:05:53,518
are completely separate, they don't know about each other.

87
00:05:53,604 --> 00:05:56,926
That is why one of the improvement of the embedded cache will

88
00:05:56,948 --> 00:06:00,794
be to use embedded distributed cache. So in terms of the patterns

89
00:06:00,842 --> 00:06:03,834
or topologies, it is still the same. However,

90
00:06:03,972 --> 00:06:07,742
we just will use a different library, not caching library,

91
00:06:07,806 --> 00:06:12,414
but distributed caching library. We can use for example hazelcast

92
00:06:12,462 --> 00:06:16,294
which is a distributed caching library, so you can embed it into your application

93
00:06:16,492 --> 00:06:20,470
and now the flow is the same. But now no matter which

94
00:06:20,540 --> 00:06:23,942
embedded cache instance youll use, doesn't matter because they

95
00:06:23,996 --> 00:06:27,686
both form one consistent caching cluster. How to

96
00:06:27,708 --> 00:06:30,666
use it how will you use it in your application? If we stick to the

97
00:06:30,688 --> 00:06:33,962
spring example, the only thing you need to change in your

98
00:06:34,016 --> 00:06:38,134
application is actually to specify I would like to use hazelcast

99
00:06:38,182 --> 00:06:42,026
as my caching manager. All the rest is the same. So cache

100
00:06:42,138 --> 00:06:46,094
hazelcast instance embedded in each of your application, they will all

101
00:06:46,132 --> 00:06:49,470
form one consistent caching cluster and will work

102
00:06:49,540 --> 00:06:52,590
fine together. Now you may wonder like but how,

103
00:06:52,660 --> 00:06:55,822
I mean you deploy it somewhere like on Kubernetes

104
00:06:55,966 --> 00:06:59,854
and how they discover each other, how one instance

105
00:06:59,902 --> 00:07:03,614
of hazelcast knows that it needs to connect to another instance

106
00:07:03,662 --> 00:07:07,442
of Hazelcast. So we thought how to solve this discovery problem and

107
00:07:07,496 --> 00:07:11,282
we came up with the idea of plugins. So for each environment

108
00:07:11,346 --> 00:07:14,918
we have a plugin which is by the way auto detected. So you run on

109
00:07:14,924 --> 00:07:18,738
kubernetes, hazelcast discovers. Okay, I'm running Kubernetes,

110
00:07:18,834 --> 00:07:22,022
I should use Kubernetes plugin and it uses kubernetes

111
00:07:22,086 --> 00:07:25,466
API to discover other members. So you really don't need

112
00:07:25,488 --> 00:07:29,558
to do anything and your hazelcast cluster will form automatically.

113
00:07:29,654 --> 00:07:33,466
If you are interested in details how to configure this, then there

114
00:07:33,488 --> 00:07:36,874
are a lot of resources, we have documentation, we have a lot of blog posts

115
00:07:36,922 --> 00:07:40,446
which you can read. So we ended up with this diagram of

116
00:07:40,468 --> 00:07:43,758
our embedded distributed cache. So let's make a short

117
00:07:43,844 --> 00:07:47,134
summary about embedded caching. So from the good

118
00:07:47,172 --> 00:07:50,962
size embedded caching, it's very simple. Configuration is simple,

119
00:07:51,016 --> 00:07:54,210
the deployment is simple because it goes together with our application.

120
00:07:54,280 --> 00:07:57,870
So you don't need to do anything. Youll have very low latency data access

121
00:07:57,960 --> 00:08:01,750
and usually youll don't need any separate ops team needed.

122
00:08:01,900 --> 00:08:05,570
From the downsides, the management of your caching is not flexible

123
00:08:05,650 --> 00:08:09,146
because if you youll like to scale up the caching cluster, you need to do

124
00:08:09,168 --> 00:08:12,922
it together with your application. It's also limited to JvM based application

125
00:08:13,056 --> 00:08:16,970
like this Hazelcast example. But in general your

126
00:08:17,040 --> 00:08:20,362
embedded cache is limited to your language of choice. For every

127
00:08:20,416 --> 00:08:24,282
language you will have a different library and the data is collocated with the applications,

128
00:08:24,346 --> 00:08:27,598
which may be a problem or may not be a problem in your case.

129
00:08:27,684 --> 00:08:31,278
Okay, the next pattern, the next topology that you can use

130
00:08:31,364 --> 00:08:35,534
is client server. Client server is kind of database style,

131
00:08:35,582 --> 00:08:40,286
database style. So we will deploy our caching server separately

132
00:08:40,398 --> 00:08:43,902
and then use cache client to connect to the server.

133
00:08:44,046 --> 00:08:47,542
It looks as follows request goes to again to our Kubernetes service,

134
00:08:47,596 --> 00:08:51,714
it goes to one of the application and then the application uses

135
00:08:51,762 --> 00:08:55,650
cache client to connect to the cache server which is deployed

136
00:08:55,730 --> 00:08:59,234
separately. Usually in Kubernetes it will be deployed as a stateful

137
00:08:59,282 --> 00:09:02,506
set because cache server is a stateful thing. So it will

138
00:09:02,528 --> 00:09:05,914
be deployed as a stateful set. Now if you compare this

139
00:09:05,952 --> 00:09:09,450
solution, this pattern to embedded caching, there are

140
00:09:09,520 --> 00:09:13,054
two main differences. The first difference is that we have

141
00:09:13,092 --> 00:09:17,738
this thing on a diagram. So this cache server, it requires

142
00:09:17,914 --> 00:09:21,674
some management, some maintenance. That is why in the big enterprises

143
00:09:21,722 --> 00:09:25,154
you usually see even a separate team dedicated to

144
00:09:25,192 --> 00:09:29,166
operate like not only cache clusters but like databases.

145
00:09:29,278 --> 00:09:32,718
All this stateful thing for youll system, but also it's

146
00:09:32,734 --> 00:09:36,546
deployed separately. It means that you can separately scale it

147
00:09:36,568 --> 00:09:39,814
up or down. You can think about all this management like

148
00:09:39,852 --> 00:09:43,586
backups separately. Now if we compare this diagram

149
00:09:43,618 --> 00:09:47,158
to the embedded mode, there's also a second difference which is very important

150
00:09:47,244 --> 00:09:50,966
and that is this part. So now your application uses

151
00:09:51,078 --> 00:09:54,294
cache client to connect to the cache server. And using cache

152
00:09:54,342 --> 00:09:57,706
client means that you can actually use

153
00:09:57,808 --> 00:10:01,494
a different programming language for your cache server and different programming

154
00:10:01,542 --> 00:10:05,518
language for your applications. Because there is a well defined protocol between

155
00:10:05,604 --> 00:10:09,342
cache client and cache server. So no problem with that. That is a very common

156
00:10:09,396 --> 00:10:13,486
strategy in this microservice world where you usually deploy

157
00:10:13,598 --> 00:10:16,734
your cache server separately or multiple cache servers

158
00:10:16,782 --> 00:10:20,622
and then your applications written in different programming languages,

159
00:10:20,686 --> 00:10:24,574
they can access the server. This is such a common strategy

160
00:10:24,622 --> 00:10:28,334
that like redis, it supports only this cache client

161
00:10:28,382 --> 00:10:31,602
server, the same with memcached. So these are the only topologies

162
00:10:31,666 --> 00:10:34,854
they actually support. Now how to set it up? If youll like

163
00:10:34,892 --> 00:10:38,594
to say okay, I would like to have this client server,

164
00:10:38,642 --> 00:10:42,086
how to do it? So for the Kubernetes we provide a helm chart,

165
00:10:42,118 --> 00:10:45,770
we also provide an operator. So actually the simplest you can do is

166
00:10:45,840 --> 00:10:49,994
helm install hazelcast, you already have your cache server running.

167
00:10:50,112 --> 00:10:54,282
Now the client part, if we stick to this example from spring,

168
00:10:54,346 --> 00:10:57,486
that is how it will look like so we need to define, okay, I would

169
00:10:57,508 --> 00:11:01,050
like to use Kubernetes plugin for discovery. So please discover

170
00:11:01,130 --> 00:11:05,438
my cache server and that's it. Client will automatically

171
00:11:05,534 --> 00:11:09,362
discover the caching server, connect to this and that's actually all

172
00:11:09,416 --> 00:11:13,086
you have to do. So let's come back for a moment to this diagram.

173
00:11:13,198 --> 00:11:16,662
So we separated this, this cache server is a separate thing.

174
00:11:16,716 --> 00:11:20,118
Then your application goes separately. As I told you, in a

175
00:11:20,124 --> 00:11:23,702
big enterprise, usually this cache server is

176
00:11:23,836 --> 00:11:27,670
managed by a separate team. So we can even go one step

177
00:11:27,740 --> 00:11:31,402
further and move this managing part outside our

178
00:11:31,456 --> 00:11:35,018
organization and move it into the cloud. So cloud is kind of

179
00:11:35,104 --> 00:11:39,098
client server but it's very specific because the server part, it's not

180
00:11:39,184 --> 00:11:42,634
managed inside our organization. So it works like this.

181
00:11:42,672 --> 00:11:46,298
So again request goes to Kubernetes service, it's load balanced

182
00:11:46,314 --> 00:11:49,946
to one of the application. Now application uses cache client to connect to the cache

183
00:11:49,978 --> 00:11:53,626
server and the cache server is deployed somewhere, it's provided

184
00:11:53,658 --> 00:11:57,386
as a service so you don't need any management, you don't

185
00:11:57,418 --> 00:12:00,946
need ops team, you just need to pay the cloud provider which

186
00:12:00,968 --> 00:12:04,818
is usually cheaper than maintain it on your own, how it looks like

187
00:12:04,904 --> 00:12:08,626
in the code. So to start up the cache server caching

188
00:12:08,658 --> 00:12:12,470
cluster you just click on the console of youll ClI and then when it's started

189
00:12:12,540 --> 00:12:16,086
you can take the discovery token, put it into your

190
00:12:16,108 --> 00:12:20,246
application and it will discover your cache server automatically.

191
00:12:20,358 --> 00:12:23,898
And this is by the way like not only how hazelcast cloud works

192
00:12:23,984 --> 00:12:27,450
but how most cloud solutions, how they provide you.

193
00:12:27,520 --> 00:12:30,650
Even like databases, MongoDB or whatever you use,

194
00:12:30,720 --> 00:12:34,414
that usually is based on the Discovery token pros and cons

195
00:12:34,532 --> 00:12:38,090
of client server and cloud solution. So from the good sites

196
00:12:38,170 --> 00:12:41,866
we have our data separated from the application, we have separate management.

197
00:12:41,898 --> 00:12:45,614
So you can scale up down your cluster separately from your application.

198
00:12:45,732 --> 00:12:48,994
And it's programming language agnostic because you use cache client to connect

199
00:12:49,032 --> 00:12:52,446
to the cache server. From the downside you have a separate ops

200
00:12:52,478 --> 00:12:56,146
effort or you need to pay for the cloud solution and

201
00:12:56,248 --> 00:13:00,326
higher latency. You need to think about latency. This is

202
00:13:00,348 --> 00:13:04,310
something I didn't mention yet, but we need to cover this. So usually

203
00:13:04,380 --> 00:13:08,386
when you have your cache embedded latency is low because cache

204
00:13:08,418 --> 00:13:11,926
goes together with your application. However with the client server

205
00:13:11,958 --> 00:13:15,894
or cloud you need to think about latency. If you deploy client

206
00:13:15,942 --> 00:13:19,354
server on premises then you need

207
00:13:19,392 --> 00:13:23,050
to make sure that they are deployed in the same local network

208
00:13:23,130 --> 00:13:27,130
because remember we are in this domain of caching,

209
00:13:27,210 --> 00:13:31,162
it's a very low latency domain. So even one router hop

210
00:13:31,306 --> 00:13:35,466
is a lot. So make sure that you deploy your cache server inside the

211
00:13:35,508 --> 00:13:38,626
same local network where your application is running.

212
00:13:38,728 --> 00:13:42,018
Now what about cloud solution? So about cloud solution is the

213
00:13:42,024 --> 00:13:45,954
same. What do we do in hazelcast cloud? So when you create your caching

214
00:13:46,002 --> 00:13:49,938
server you need obviously to create this in the same geographical region.

215
00:13:50,034 --> 00:13:54,306
So we don't provide infrastructure using hazelcast

216
00:13:54,338 --> 00:13:58,054
cloud. You can deploy hazelcast on AWS, GCP or

217
00:13:58,092 --> 00:14:01,834
Azure. So you should choose the same cloud provider where your

218
00:14:01,872 --> 00:14:05,306
application is running. You should choose the same region where your application

219
00:14:05,408 --> 00:14:09,114
is running. But that's not enough. We provide also a way

220
00:14:09,152 --> 00:14:13,150
to do a VPC peering between our network and between

221
00:14:13,220 --> 00:14:16,446
the network where we deployed your cache cluster for

222
00:14:16,468 --> 00:14:19,866
you and your application. So after that VPC peering,

223
00:14:19,898 --> 00:14:23,726
you are like running in the same virtual local network basically.

224
00:14:23,828 --> 00:14:27,266
So there is not even one router hop in between. And that is very important

225
00:14:27,368 --> 00:14:31,118
to keep in mind because otherwise your latency will suffer.

226
00:14:31,214 --> 00:14:34,834
Okay, we covered like all the

227
00:14:34,872 --> 00:14:38,470
patterns so far, like embedded client server cloud. They are quite

228
00:14:38,540 --> 00:14:42,226
old in a sense that you know them, probably from databases

229
00:14:42,338 --> 00:14:46,294
or they are nothing new. So now there will be a pattern that is

230
00:14:46,412 --> 00:14:49,922
quite new, that is very popular, especially in Kubernetes,

231
00:14:49,986 --> 00:14:53,754
but it's not limited to Kubernetes. In fact you see sidecar in

232
00:14:53,792 --> 00:14:57,434
other systems as well. So cache as a sidecar, how it looks like

233
00:14:57,552 --> 00:15:00,694
request goes to our Kubernetes service, it is forwarded

234
00:15:00,742 --> 00:15:03,914
to cloud balance to one of the Kubernetes pod.

235
00:15:03,962 --> 00:15:07,550
And now inside each of the pod is not only application that is running,

236
00:15:07,620 --> 00:15:11,070
but also a cache server. So request goes to the application

237
00:15:11,220 --> 00:15:14,994
and application connects to the local host where your cache server is

238
00:15:15,032 --> 00:15:18,638
running. And all these sidecar cache servers,

239
00:15:18,734 --> 00:15:22,686
sidecar cache containers, they form one consistent caching cluster.

240
00:15:22,798 --> 00:15:26,130
So that is the idea. So this solution is somehow similar

241
00:15:26,200 --> 00:15:30,002
to embedded mode, somehow similar to the client server mode.

242
00:15:30,066 --> 00:15:33,954
It's similar to the embedded mode because kubernetes will always schedule

243
00:15:34,002 --> 00:15:37,574
your caching server on the same physical machine. So you

244
00:15:37,612 --> 00:15:41,206
have your cache close to your application, it scares up

245
00:15:41,228 --> 00:15:44,694
and down together. So it's kind of like embedded. There is no discover

246
00:15:44,742 --> 00:15:48,234
needed, your cache is always at the localhost. That is

247
00:15:48,272 --> 00:15:52,622
good. But it's also similar to client server because after all your application

248
00:15:52,756 --> 00:15:55,950
uses cache clients to connect to the cache server.

249
00:15:56,290 --> 00:15:59,914
So there is no problem like with cache

250
00:15:59,962 --> 00:16:03,406
can be written in different programming language than your application, no problem with that.

251
00:16:03,428 --> 00:16:07,010
And there is some kind of isolation between cache and application.

252
00:16:07,160 --> 00:16:10,786
It's on the container level, which may not be good enough or may be

253
00:16:10,808 --> 00:16:14,446
good enough for you, depending on your requirement. How to configure

254
00:16:14,478 --> 00:16:18,186
this. So let's stick to this spring example. So in a spring

255
00:16:18,238 --> 00:16:21,810
how we configure this, this is our client configuration.

256
00:16:21,890 --> 00:16:25,254
So we connect to the cache server with the local host because we

257
00:16:25,292 --> 00:16:28,582
just know, so it looks like a static configuration, but actually

258
00:16:28,636 --> 00:16:32,506
the whole system is dynamic. We just know that the cache server is

259
00:16:32,528 --> 00:16:36,026
running at the local host and the Kubernetes configuration. So we have two

260
00:16:36,048 --> 00:16:39,562
containerbased. One is our application with our business logic, and the second

261
00:16:39,616 --> 00:16:43,766
one is our cache server. In this case it's hazelcast.

262
00:16:43,798 --> 00:16:47,582
Short summary sitecar cache from the good sites configuration is again

263
00:16:47,636 --> 00:16:51,406
very simple, it's programming language agnostic. We have low latency and there is

264
00:16:51,428 --> 00:16:54,686
some isolation of data between application and

265
00:16:54,708 --> 00:16:58,462
the cache. From the downsides, we again do not have flexible management

266
00:16:58,526 --> 00:17:01,874
because your cache scales up and down together with your application,

267
00:17:01,992 --> 00:17:05,380
and your data is after all collocated in the same application

268
00:17:05,750 --> 00:17:09,574
with the application pop, which again may be good, maybe not good enough,

269
00:17:09,692 --> 00:17:13,282
depending on your use case. Okay, we covered sidecar.

270
00:17:13,426 --> 00:17:17,042
The last caching pattern for today, last caching topology

271
00:17:17,106 --> 00:17:20,730
will be reverse proxy, and reverse proxy will be something completely

272
00:17:20,800 --> 00:17:24,170
different. It will be completely different than what we've seen so far.

273
00:17:24,240 --> 00:17:27,914
It will be different because so far our application

274
00:17:28,112 --> 00:17:31,766
was all the time aware that such a thing as a cache

275
00:17:31,798 --> 00:17:35,418
exists. It was explicitly connecting to the cache server.

276
00:17:35,514 --> 00:17:39,294
However, now we will do something different. We will put cache in front

277
00:17:39,332 --> 00:17:42,842
of our application, so our application will be not even aware

278
00:17:42,906 --> 00:17:46,734
that such a thing as a cache exists, how it looks like. So request

279
00:17:46,782 --> 00:17:50,178
goes to our system, and now just before Kubernetes service,

280
00:17:50,264 --> 00:17:53,474
after Kubernetes service, or maybe together with like

281
00:17:53,512 --> 00:17:57,282
ingress in Nginx, we put cache and first like

282
00:17:57,336 --> 00:18:00,614
if the value is found in this cache, just return the response, it does not

283
00:18:00,652 --> 00:18:04,358
even go to the application. If the value is not found in the cache only

284
00:18:04,444 --> 00:18:08,358
then you go, the request goes to the application. Nginx is

285
00:18:08,364 --> 00:18:12,262
a very good solution because it's very mature, it's well integrated

286
00:18:12,406 --> 00:18:15,674
with Kubernetes, and it's just something you should

287
00:18:15,712 --> 00:18:19,462
use. If you go with the reverse proxy, how the configuration for caching

288
00:18:19,526 --> 00:18:23,034
looks like. So that is the simplest thing you can do with Nginx.

289
00:18:23,082 --> 00:18:26,862
So specify, okay, cache it on the HTTP level, that is

290
00:18:26,916 --> 00:18:30,554
the path for your caching. So Nginx is good, it's mature,

291
00:18:30,602 --> 00:18:33,794
it's well integrated with Kubernetes, but it has some problems.

292
00:18:33,912 --> 00:18:37,454
So one maybe not a problem, but the trait of Nginx,

293
00:18:37,502 --> 00:18:40,910
it's HTTP based, but okay, we are HTTP

294
00:18:40,990 --> 00:18:44,658
people, this is fine. But another problem which is

295
00:18:44,664 --> 00:18:48,546
a bigger thing, it's Nginx is not distributed and NginX is

296
00:18:48,568 --> 00:18:52,182
not highly available. And NginX maybe does not store

297
00:18:52,236 --> 00:18:55,366
data of the disk, but it can offload your data to

298
00:18:55,388 --> 00:18:59,218
the disk, which for example is not the case in hazelcast when you are guaranteed

299
00:18:59,314 --> 00:19:02,682
that your data is stored in memory, so that

300
00:19:02,736 --> 00:19:06,406
latency is low. That is why you have to accept

301
00:19:06,438 --> 00:19:10,566
these brings if you use Nginx. But still Nginx is a very good solution.

302
00:19:10,678 --> 00:19:14,346
Now the last, last variant of the reverse proxy

303
00:19:14,378 --> 00:19:17,870
will be reverse proxy sidecar caching. So this will be the last

304
00:19:17,940 --> 00:19:21,166
variant of the reverse proxy topology. This looks like

305
00:19:21,188 --> 00:19:24,954
that request goes to Kubernetes service. It load balances the traffic

306
00:19:25,002 --> 00:19:28,386
to one of the Kubernetes pods. But now it's not the application

307
00:19:28,488 --> 00:19:31,966
that receives the request, but it is something that we will call reverse

308
00:19:31,998 --> 00:19:35,266
proxy cache container. So like cache server, but also like

309
00:19:35,288 --> 00:19:38,190
the network interceptor which checks okay,

310
00:19:38,280 --> 00:19:41,542
what goes to the application, and only if the value

311
00:19:41,596 --> 00:19:44,962
is not found in this reverse proxy cache container, the request

312
00:19:45,026 --> 00:19:48,566
goes to the application. So the application again does not even know

313
00:19:48,668 --> 00:19:52,154
that such a thing as a cache exists. But okay, like all this

314
00:19:52,192 --> 00:19:55,514
thing, that application is not aware of the caching. It's good and bad.

315
00:19:55,552 --> 00:19:58,694
I mean it has some good sides and bad sides,

316
00:19:58,742 --> 00:20:02,506
maybe starting from the good ideas. So why is it good that application is not

317
00:20:02,528 --> 00:20:06,526
aware of caching? You remember this diagram from the beginning of this presentation, a lot

318
00:20:06,548 --> 00:20:10,430
of services in different versions, different programming languages, they use each other.

319
00:20:10,500 --> 00:20:14,414
That is by the way very small microservices in general you have way,

320
00:20:14,452 --> 00:20:17,490
way bigger. So now you can look at the system,

321
00:20:17,560 --> 00:20:21,474
you can look at the architecture design, you can look at diagram like

322
00:20:21,512 --> 00:20:25,394
this and say, I would like to introduce caching service to

323
00:20:25,432 --> 00:20:29,190
version one and service one and that's it. I mean you don't need to change

324
00:20:29,260 --> 00:20:33,266
the code of the service in order to introduce

325
00:20:33,298 --> 00:20:36,966
the caching layer. So you can do it in a declarative manner at the

326
00:20:36,988 --> 00:20:40,886
functionality of caching. And that is the whole beauty of reverse proxy

327
00:20:40,918 --> 00:20:43,994
caching. It simplifies a lot of things and

328
00:20:44,112 --> 00:20:47,866
how it looks like in practice. So in practice you

329
00:20:47,888 --> 00:20:51,546
will usually have like starting maybe from the containers at

330
00:20:51,568 --> 00:20:54,986
the bottom you will have your application, then you will have your cache server

331
00:20:55,018 --> 00:20:58,686
and the interceptor like a caching proxy. And then you need

332
00:20:58,708 --> 00:21:02,266
some init container which will basically do some iptables

333
00:21:02,298 --> 00:21:05,986
changes so that the request from outside does not go to the application, but it

334
00:21:06,008 --> 00:21:10,210
goes to this caching proxy. But okay, if we look at this diagram and

335
00:21:10,280 --> 00:21:13,650
about this idea of declarative manner of modifying your

336
00:21:13,720 --> 00:21:17,190
system, it may make you think about like

337
00:21:17,260 --> 00:21:20,886
istio and all these service meshes. And that is actually

338
00:21:21,068 --> 00:21:25,442
true because that is the same idea. And recently

339
00:21:25,506 --> 00:21:29,266
actually they added an envoy proxy. They added the support for

340
00:21:29,308 --> 00:21:32,662
HTTP caching. So this reverse proxy sidecar caching

341
00:21:32,726 --> 00:21:35,802
will become a big thing together with envoy proxy and

342
00:21:35,856 --> 00:21:39,530
all the services meshes that use envoy proxy. Like for example

343
00:21:39,600 --> 00:21:43,078
istio. I really think this will be the way a lot of

344
00:21:43,104 --> 00:21:46,766
people will do caching. But okay, like I said, okay, there is no

345
00:21:46,788 --> 00:21:50,350
such thing as a free lunch. There are some bedsides about

346
00:21:50,420 --> 00:21:53,882
all this idea that application is not aware of caching.

347
00:21:53,946 --> 00:21:57,426
And if you think about it, like if the application is not aware of

348
00:21:57,448 --> 00:22:01,298
caching, there is one thing that becomes way more difficult

349
00:22:01,464 --> 00:22:04,318
and this thing is called cache invalidation.

350
00:22:04,414 --> 00:22:08,502
And actually if you look anywhere on the Internet, what is the hardest problem

351
00:22:08,556 --> 00:22:12,162
with caching? It is the cache invalidation, meaning when to decide

352
00:22:12,226 --> 00:22:15,974
that your cached value is outdated. It's stale, you should not

353
00:22:16,012 --> 00:22:19,238
use it anymore. But go to the source of truth and

354
00:22:19,244 --> 00:22:23,194
this is not a trigger problem. And when youll application is

355
00:22:23,232 --> 00:22:26,762
aware of the cache, then you can have some business logic to

356
00:22:26,816 --> 00:22:30,314
evict the cache value or do anything, basically anything youll

357
00:22:30,352 --> 00:22:33,966
want depending on the business logic. However, if your application

358
00:22:34,068 --> 00:22:37,994
is not aware of caching, then you are left to watch what HTTP

359
00:22:38,042 --> 00:22:41,226
has, like timeouts, etags, basically timeouts,

360
00:22:41,258 --> 00:22:44,958
and that's what you can do. So that is the biggest

361
00:22:45,054 --> 00:22:49,118
issue with the reverse proxy caching. Short summary reverse

362
00:22:49,134 --> 00:22:53,138
proxy caching. So from the good sides, it's configuration based,

363
00:22:53,224 --> 00:22:57,422
so you don't need to change your application in order to introduce caching.

364
00:22:57,486 --> 00:23:00,786
That is actually the reason why youll will use reverse proxy caching.

365
00:23:00,818 --> 00:23:04,706
It's also programming language agnostic. It's everything agnostic because you don't

366
00:23:04,738 --> 00:23:08,134
even touch the code of your application and it's very consistent with the

367
00:23:08,172 --> 00:23:12,074
containers and microservice world and Kubernetes. That is why I really believe that

368
00:23:12,112 --> 00:23:15,818
will be the future of caching. However, from the downsides, it's difficult to

369
00:23:15,824 --> 00:23:19,066
do cache invalidation. There are no matrix solution yet. I mean it was

370
00:23:19,088 --> 00:23:22,218
implemented in envoy proxy, so it will be matrix soon.

371
00:23:22,304 --> 00:23:25,914
And it's protocol based, which is not such a big deal as we discussed.

372
00:23:25,962 --> 00:23:29,946
So we covered all the caching topologies, all the caching patterns.

373
00:23:30,058 --> 00:23:33,454
So now what I suggest as a short summary, I will try

374
00:23:33,492 --> 00:23:37,262
not to repeat anything I said before because it may be boring.

375
00:23:37,326 --> 00:23:40,994
So what I propose as a summary is a very simple

376
00:23:41,112 --> 00:23:45,230
is oversimplified decision tree which can help you decide

377
00:23:45,310 --> 00:23:49,030
which caching pattern is for me. So the first question I would ask is

378
00:23:49,180 --> 00:23:52,610
do my applications need to be aware of caching?

379
00:23:52,690 --> 00:23:56,402
If no, am I an area adopter? If no, use reverse proxy

380
00:23:56,466 --> 00:24:00,006
with Nginx. If yes, use reverse proxy sidecar caching like

381
00:24:00,028 --> 00:24:03,718
with the envoy proxy, istio or some other prototypes.

382
00:24:03,814 --> 00:24:07,354
If your application needs to be aware of the caching, then your next

383
00:24:07,392 --> 00:24:10,838
question is do I have a lot of data or some security restrictions?

384
00:24:10,934 --> 00:24:14,446
If no, do I need to be language agnostic? If no embedded or

385
00:24:14,468 --> 00:24:17,562
embedded distributed? If yes, use sidecar caching.

386
00:24:17,626 --> 00:24:20,986
Now if you have a lot of data, you work in big organization,

387
00:24:21,098 --> 00:24:24,642
you have some security restrictions. Then the last question you need to ask

388
00:24:24,696 --> 00:24:28,226
is is my deployment cloud and if no run your own on

389
00:24:28,248 --> 00:24:31,998
premises client server? If yes, use cloud solution.

390
00:24:32,094 --> 00:24:35,554
So as I said, it's a little maybe oversimplified, but at least

391
00:24:35,592 --> 00:24:38,978
it gives you the direction where to look for the right topology for

392
00:24:38,984 --> 00:24:42,534
youll caching. As the last slide from the presentation, I would like just to mention

393
00:24:42,572 --> 00:24:45,814
a few resources. So if you would like to play with all these

394
00:24:45,852 --> 00:24:49,590
patterns and run the code, here is the link. First link is can

395
00:24:49,660 --> 00:24:53,174
just run this code sample. Second link is a blog post of

396
00:24:53,212 --> 00:24:56,050
how to configure hazelcast as a sidecar container.

397
00:24:56,130 --> 00:24:59,526
Third one is our prototype, nothing that you can use on

398
00:24:59,548 --> 00:25:03,770
production. A prototype of this reverse proxy sidecar caching with Hazelcast

399
00:25:03,850 --> 00:25:07,882
and the last link is a very good video. Talk about Nginx

400
00:25:07,946 --> 00:25:11,198
as a reverse proxy caching and with this last slide I

401
00:25:11,204 --> 00:25:15,182
would like to thank you for listening. It was really a pleasure to speak

402
00:25:15,236 --> 00:25:15,406
to.

