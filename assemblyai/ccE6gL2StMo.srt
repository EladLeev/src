1
00:00:23,610 --> 00:00:27,350
Hi, welcome. Welcome to. Well, what I expect

2
00:00:27,420 --> 00:00:31,640
is not just another talk about artificial intelligence, but instead

3
00:00:32,410 --> 00:00:35,720
I want to provoke you and ask you

4
00:00:36,410 --> 00:00:40,294
whether artificial intelligence is something new or

5
00:00:40,332 --> 00:00:43,080
has it been evolving throughout the years?

6
00:00:44,170 --> 00:00:47,574
Now, before we get started, a little bit

7
00:00:47,612 --> 00:00:51,422
about myself. My name is Vasko. As can see, I've been

8
00:00:51,476 --> 00:00:55,310
in software development for 20 years and counting,

9
00:00:56,130 --> 00:00:59,594
and I am coherently interested in the concept

10
00:00:59,642 --> 00:01:03,618
of artificial intelligence throughout history,

11
00:01:03,784 --> 00:01:07,694
in the sense that I am not totally

12
00:01:07,742 --> 00:01:10,994
convinced that artificial intelligence is a new

13
00:01:11,032 --> 00:01:15,086
concept. In fact, I argue that it

14
00:01:15,128 --> 00:01:19,266
has been there in our collective minds

15
00:01:19,378 --> 00:01:23,046
for quite some time. If you want to

16
00:01:23,068 --> 00:01:27,030
agree or disagree with me, or just send any comment, please do so.

17
00:01:27,100 --> 00:01:29,894
I can be reached via Twitter or LinkedIn.

18
00:01:29,942 --> 00:01:32,730
My contacts are on this slide.

19
00:01:34,190 --> 00:01:37,420
And without further ado, let's get started.

20
00:01:38,270 --> 00:01:40,938
As I said, I believe in evolution,

21
00:01:41,114 --> 00:01:44,800
in the evolution of everything and everyone,

22
00:01:46,450 --> 00:01:49,870
including ideas and

23
00:01:49,940 --> 00:01:53,950
concepts. So, is artificial intelligence

24
00:01:54,030 --> 00:01:57,474
a concept that has been just created or has

25
00:01:57,512 --> 00:01:59,540
it evolved through time?

26
00:02:00,790 --> 00:02:05,010
That's a question I ask. I have my opinion.

27
00:02:05,370 --> 00:02:09,510
I hope that will be clear by the end of this presentation.

28
00:02:09,930 --> 00:02:13,366
At the same time, we are going to look

29
00:02:13,468 --> 00:02:17,406
at the way how the concerns surrounding

30
00:02:17,538 --> 00:02:21,514
artificial intelligence have evolved through time as

31
00:02:21,552 --> 00:02:25,740
well. And since we are talking about time,

32
00:02:26,110 --> 00:02:29,942
let's travel back in time about 800

33
00:02:30,016 --> 00:02:33,120
years before Christ, give or take a few,

34
00:02:33,890 --> 00:02:36,830
and let's land in ancient Greece.

35
00:02:37,970 --> 00:02:42,190
There we find the Iliad, a classical poem

36
00:02:42,710 --> 00:02:46,994
written by Homer, where the

37
00:02:47,032 --> 00:02:50,270
Ephastus God is described.

38
00:02:50,350 --> 00:02:53,250
Well, Iliad is not about Ephastus,

39
00:02:53,910 --> 00:02:57,880
but this character is mentioned.

40
00:02:58,490 --> 00:03:00,520
And as you know,

41
00:03:01,770 --> 00:03:05,702
in ancient Greece, the mythology was composed of

42
00:03:05,756 --> 00:03:07,030
several gods.

43
00:03:08,590 --> 00:03:10,090
And in this poem,

44
00:03:10,910 --> 00:03:14,726
Hephaestus has furnaces.

45
00:03:14,918 --> 00:03:18,490
He builds things made of metal.

46
00:03:19,390 --> 00:03:20,970
He builds machines.

47
00:03:22,430 --> 00:03:26,386
These furnaces know exactly what Hephaestus

48
00:03:26,438 --> 00:03:30,382
needs and wants. They are completely hands

49
00:03:30,436 --> 00:03:33,762
off. Whatever it is that he

50
00:03:33,816 --> 00:03:37,282
requires at any given moment to do whatever it is that

51
00:03:37,336 --> 00:03:39,540
he wants to do, they will provide.

52
00:03:40,710 --> 00:03:44,706
Now, you may argue that this is what we

53
00:03:44,728 --> 00:03:48,294
know today as automation, right? You push a button and you

54
00:03:48,332 --> 00:03:52,166
configure a whole set of machines to do whatever it is that

55
00:03:52,188 --> 00:03:55,894
you need doing. I give you that.

56
00:03:56,012 --> 00:03:58,300
True. However,

57
00:03:59,550 --> 00:04:02,250
back in ancient Greece,

58
00:04:03,630 --> 00:04:08,582
tripods were a sign of authority

59
00:04:08,726 --> 00:04:11,420
and could actually be a sign of power.

60
00:04:12,670 --> 00:04:16,506
Naturally, Hephaastus bit tripods

61
00:04:16,538 --> 00:04:17,470
for himself.

62
00:04:19,330 --> 00:04:23,410
Now, in ancient Greece mythology,

63
00:04:24,390 --> 00:04:28,434
gods would gather in an assembly of gods to discuss whatever it

64
00:04:28,472 --> 00:04:32,002
is that gods discuss. And if Hephaestus was

65
00:04:32,056 --> 00:04:35,246
not in the mood to travel to the assembly

66
00:04:35,278 --> 00:04:38,594
of gods, he would just simply send his

67
00:04:38,632 --> 00:04:39,650
tripods.

68
00:04:41,270 --> 00:04:44,646
Hephaestus'tripods would travel all by themselves

69
00:04:44,828 --> 00:04:48,314
to the assembly of gods for the duration and back to

70
00:04:48,352 --> 00:04:50,060
Homer's house.

71
00:04:51,550 --> 00:04:55,894
Now, this would mean that those tripods,

72
00:04:55,942 --> 00:04:57,450
those mechanisms,

73
00:04:59,390 --> 00:05:02,670
were capable of autonomous navigation,

74
00:05:03,970 --> 00:05:06,906
right? To get from Homer,

75
00:05:07,018 --> 00:05:11,040
Hephaestus'home, to the assembly of gods and back,

76
00:05:13,830 --> 00:05:18,290
they were capable of understanding directions.

77
00:05:20,950 --> 00:05:24,420
They were the perfect definition of

78
00:05:24,890 --> 00:05:28,600
an autonomous vehicle. Isn't that so?

79
00:05:30,330 --> 00:05:34,034
Well, you may argue, okay, but that poem

80
00:05:34,082 --> 00:05:39,158
is about a God. Even though gods,

81
00:05:39,254 --> 00:05:43,980
even in greek mythology, they were conceived as

82
00:05:44,590 --> 00:05:46,730
humans lookalikes.

83
00:05:49,150 --> 00:05:53,210
Does that fit in the definition of artificial intelligence

84
00:05:53,290 --> 00:05:56,782
as something that was built by humans to

85
00:05:56,916 --> 00:06:00,174
try to be closer or better than

86
00:06:00,212 --> 00:06:03,678
them? That's a bit debatable,

87
00:06:03,774 --> 00:06:07,762
I would say. However, it is a good example if

88
00:06:07,816 --> 00:06:11,234
we remain in ancient Greece and about the same

89
00:06:11,272 --> 00:06:14,834
periods, but we switch the text and now we

90
00:06:14,872 --> 00:06:18,870
talk about the Odyssey, also written by Homer.

91
00:06:20,090 --> 00:06:24,406
There is a passage there where

92
00:06:24,508 --> 00:06:28,074
the Phoenicius king is sending a

93
00:06:28,192 --> 00:06:31,466
group of visitors home at the

94
00:06:31,488 --> 00:06:35,254
end of their visit. And the king

95
00:06:35,302 --> 00:06:38,842
is so happy with them that he

96
00:06:38,896 --> 00:06:42,414
actually offers that they travel

97
00:06:42,532 --> 00:06:45,934
using the Phoenicius ships. And why is

98
00:06:45,972 --> 00:06:49,678
that? Now, you will forgive me,

99
00:06:49,844 --> 00:06:53,682
but I am going to read from a translation of the Odyssey because

100
00:06:53,736 --> 00:06:56,834
there is no way how I could put this

101
00:06:56,952 --> 00:06:59,810
better. It reads,

102
00:07:01,430 --> 00:07:04,290
for the Phoenicians have no pilots,

103
00:07:04,890 --> 00:07:08,018
their vessels have no rudders,

104
00:07:08,194 --> 00:07:11,414
but the ships themselves understand what it is

105
00:07:11,452 --> 00:07:13,960
that we are thinking about and want.

106
00:07:14,730 --> 00:07:18,142
They know all the cities and countries

107
00:07:18,226 --> 00:07:22,022
in the whole world and can traverse the sea

108
00:07:22,086 --> 00:07:25,194
just as well when it is covered with

109
00:07:25,232 --> 00:07:28,762
mist and cloud, so that there is no danger of

110
00:07:28,816 --> 00:07:32,110
being wrecked or coming to any harm.

111
00:07:32,850 --> 00:07:36,750
Now, isn't this wonderful?

112
00:07:38,450 --> 00:07:41,934
Almost 3000 years ago,

113
00:07:42,132 --> 00:07:45,518
Homer was imagining a fully

114
00:07:45,614 --> 00:07:50,034
autonomous sea vessel and

115
00:07:50,072 --> 00:07:51,860
a telepathic one at that.

116
00:07:53,270 --> 00:07:56,674
Nowadays there are but prototypes

117
00:07:56,722 --> 00:08:00,454
of these vehicles and they are far from

118
00:08:00,492 --> 00:08:05,270
being capable of doing what was imagined almost 3000

119
00:08:05,340 --> 00:08:08,826
years ago. Now, if this is not a

120
00:08:08,848 --> 00:08:12,134
good example of someone already thinking about artificial

121
00:08:12,182 --> 00:08:15,514
intelligence, but not giving it

122
00:08:15,552 --> 00:08:18,060
that name, I don't know what it is.

123
00:08:20,130 --> 00:08:24,142
Now, let's continue our voyage through

124
00:08:24,196 --> 00:08:27,934
time and let's jump to

125
00:08:28,052 --> 00:08:32,030
our contemporary epoch.

126
00:08:33,510 --> 00:08:37,874
We land in 1754 in

127
00:08:37,912 --> 00:08:41,394
Europe and we find a book called

128
00:08:41,512 --> 00:08:45,442
Tretier de sansacion, or Treaty of Sensations, you know, on a

129
00:08:45,496 --> 00:08:49,330
literal translation by Monsieur de Kondilak,

130
00:08:52,570 --> 00:08:56,360
even though, well, todays we may look at this title and

131
00:08:56,730 --> 00:09:00,140
it may give us some thought.

132
00:09:01,390 --> 00:09:05,686
In fact, it is a philosophical treaty

133
00:09:05,798 --> 00:09:10,926
where Monsieur de Kondilak argues for

134
00:09:11,108 --> 00:09:13,310
the mind body dualism.

135
00:09:14,290 --> 00:09:16,874
Or as a contemporary philosopher,

136
00:09:17,002 --> 00:09:20,126
Gilbert Heil put it, the ghost in the

137
00:09:20,148 --> 00:09:23,826
machine. Now, Heil argued that such

138
00:09:23,848 --> 00:09:29,134
a dualism does not exist. But regardless

139
00:09:29,182 --> 00:09:33,598
of your opinions, what is interesting with the Tretidi san sacion

140
00:09:33,694 --> 00:09:35,780
is that de cond?

141
00:09:40,970 --> 00:09:43,400
To exemplify his position,

142
00:09:44,810 --> 00:09:48,822
a statue, and that statue was animated by

143
00:09:48,876 --> 00:09:50,310
an empty soul.

144
00:09:52,570 --> 00:09:56,098
He argued that if we could feed

145
00:09:56,194 --> 00:09:59,830
one sensation at a time to that soul,

146
00:10:00,370 --> 00:10:03,840
bit would eventually learn

147
00:10:05,250 --> 00:10:08,670
all human knowledge and all human abilities,

148
00:10:09,490 --> 00:10:13,614
therefore becoming equal to humans.

149
00:10:13,742 --> 00:10:14,820
In that regard,

150
00:10:17,190 --> 00:10:21,090
if you will allow me the obvious comparison,

151
00:10:22,710 --> 00:10:26,710
nowadays we train models. And how do we do that?

152
00:10:26,860 --> 00:10:30,146
Well, we feed them pieces of information, just like Monsieur

153
00:10:30,178 --> 00:10:33,910
de Kondellac wanted to do with his statue.

154
00:10:34,250 --> 00:10:38,422
And we feed them such information until such a point where

155
00:10:38,476 --> 00:10:41,702
we are convinced that the model has learned everything that

156
00:10:41,756 --> 00:10:45,026
it is capable of learning. Well,

157
00:10:45,068 --> 00:10:48,970
we haven't yet built a model that can learn of

158
00:10:49,040 --> 00:10:52,910
all human knowledge and all human ability.

159
00:10:53,810 --> 00:10:57,214
We have specialized models, but still the

160
00:10:57,252 --> 00:11:00,000
principle is pretty much the same.

161
00:11:02,870 --> 00:11:07,582
If we remain in the approximately

162
00:11:07,646 --> 00:11:08,740
the same time,

163
00:11:11,350 --> 00:11:14,594
we find a machine that was built with

164
00:11:14,632 --> 00:11:18,760
the purpose of playing chess. It was a box

165
00:11:20,250 --> 00:11:24,726
with a mannequin that

166
00:11:24,828 --> 00:11:28,634
would move the pieces in

167
00:11:28,672 --> 00:11:32,518
the chessboard. And it was said that this machine

168
00:11:32,614 --> 00:11:36,394
could place chess better than any human in

169
00:11:36,432 --> 00:11:40,274
existence. And in fact, the machine

170
00:11:40,342 --> 00:11:44,000
was showcased in many scenarios across Europe back then,

171
00:11:45,970 --> 00:11:49,918
and it actually won games of chess against

172
00:11:50,084 --> 00:11:53,570
all opponents. It was a sensation.

173
00:11:55,030 --> 00:11:58,638
Now, it was called the Turk or the mechanical turk,

174
00:11:58,814 --> 00:12:02,382
not because it originated necessarily in Turkey,

175
00:12:02,446 --> 00:12:06,086
but because the mannequin was dressed in an

176
00:12:06,108 --> 00:12:07,320
oriental custom.

177
00:12:10,730 --> 00:12:14,166
Now, this sensation came to

178
00:12:14,188 --> 00:12:18,842
an end when someone discovered that

179
00:12:18,896 --> 00:12:23,094
the machine was a hoax. There was no actual machine.

180
00:12:23,222 --> 00:12:26,422
There was a human, a very good chess player,

181
00:12:26,486 --> 00:12:29,850
but human, operating that machine,

182
00:12:30,850 --> 00:12:35,360
or should I say that puppet, to play

183
00:12:35,730 --> 00:12:38,350
the games of chess against the opponents.

184
00:12:39,810 --> 00:12:42,646
Sadly, there was intelligence.

185
00:12:42,778 --> 00:12:45,518
Yes, but it was human intelligence,

186
00:12:45,694 --> 00:12:47,650
hardly artificial.

187
00:12:49,990 --> 00:12:53,202
However, if we now

188
00:12:53,256 --> 00:12:57,350
jump to 1912, we find the work

189
00:12:57,420 --> 00:13:00,470
of Leonardo stories e quvedo.

190
00:13:04,570 --> 00:13:08,014
He built a machine, or an automaton,

191
00:13:08,082 --> 00:13:11,174
if we want to be precise,

192
00:13:11,302 --> 00:13:14,778
that could indeed play chess. Well, not a

193
00:13:14,784 --> 00:13:19,174
full game. It would play an end sequence

194
00:13:19,302 --> 00:13:22,510
of king and rook against king.

195
00:13:23,010 --> 00:13:26,830
It would always play with the king and the rook,

196
00:13:28,770 --> 00:13:33,234
and it would always win against

197
00:13:33,432 --> 00:13:36,514
the human opponent. And this time,

198
00:13:36,552 --> 00:13:39,460
this machine could actually play. It was,

199
00:13:39,910 --> 00:13:43,810
as said later in 1914,

200
00:13:45,370 --> 00:13:48,950
quite an advanced machine for its period.

201
00:13:50,090 --> 00:13:53,366
It could detect the position of the

202
00:13:53,388 --> 00:13:57,442
pieces on the board and could calculate the next move quite

203
00:13:57,516 --> 00:14:01,226
effectively. I would say it

204
00:14:01,248 --> 00:14:05,654
was one of the first, if not the first, example of machine

205
00:14:05,702 --> 00:14:09,210
that was capable of playing at least some chess.

206
00:14:11,410 --> 00:14:13,950
On the subject of intelligent machines,

207
00:14:14,690 --> 00:14:18,080
if we travel a little bit further in time,

208
00:14:18,610 --> 00:14:22,670
we find Isaac Asimov's work, he imagined

209
00:14:23,350 --> 00:14:27,406
a class of hobots that had a positronic

210
00:14:27,438 --> 00:14:31,502
brain. Now, the positronic brain in Isakasimov's conception

211
00:14:31,566 --> 00:14:36,146
was powered by a

212
00:14:36,168 --> 00:14:40,098
particle called the positron. It doesn't actually exist,

213
00:14:40,274 --> 00:14:43,560
but it was

214
00:14:43,930 --> 00:14:47,842
sufficiently powerful to create

215
00:14:47,996 --> 00:14:51,420
processing units that could indeed power

216
00:14:52,110 --> 00:14:55,718
a robot and actually give conscience to a robot

217
00:14:55,894 --> 00:14:59,210
or to what we today would call an Android.

218
00:15:00,270 --> 00:15:04,682
What was interesting about Izakazimov's robots

219
00:15:04,826 --> 00:15:08,334
was that they were

220
00:15:08,372 --> 00:15:11,870
bound by the three laws of robotics,

221
00:15:12,470 --> 00:15:16,430
three dogmas that were designed to prevent

222
00:15:16,590 --> 00:15:20,030
them from being used directly or indirectly

223
00:15:20,110 --> 00:15:23,442
to harm the humans. We can

224
00:15:23,496 --> 00:15:26,886
argue that Isaac Asimov was already concerned about the

225
00:15:26,908 --> 00:15:30,530
possibility of artificial beings,

226
00:15:30,690 --> 00:15:34,274
or should I say artificial intelligent beings

227
00:15:34,402 --> 00:15:38,140
being used as weapons against

228
00:15:38,590 --> 00:15:39,690
humans.

229
00:15:41,950 --> 00:15:45,900
Arthur C. Clark in 1968

230
00:15:47,950 --> 00:15:51,534
wrote a story around a character wrote 2001

231
00:15:51,572 --> 00:15:55,566
A Space Odyssey. And the interesting

232
00:15:55,748 --> 00:15:59,150
artificial character there was Hull,

233
00:15:59,570 --> 00:16:02,882
Hull 9000. But Hull was

234
00:16:02,936 --> 00:16:06,114
an operating system, so not a robot per

235
00:16:06,152 --> 00:16:10,610
se, but it was the operating system of a space station.

236
00:16:11,190 --> 00:16:15,734
Now Hull would observe and learn from

237
00:16:15,772 --> 00:16:20,310
the behavior of the human crew of this space station.

238
00:16:21,690 --> 00:16:25,718
Unfortunately, there was a malfunction in

239
00:16:25,724 --> 00:16:30,090
the space station and the crew decides that Hal needs to be disconnected.

240
00:16:30,670 --> 00:16:33,926
Now, faced with this perspective,

241
00:16:34,118 --> 00:16:37,420
Hal decides to defend itself.

242
00:16:39,310 --> 00:16:42,586
Having learnt about humans,

243
00:16:42,778 --> 00:16:46,334
he decides that the best way to defend itself is to

244
00:16:46,372 --> 00:16:49,310
eliminate the human crew.

245
00:16:51,570 --> 00:16:55,650
Hal became known by his sentence,

246
00:16:56,470 --> 00:17:00,260
I'm sorry, Dave, I can't do that.

247
00:17:01,190 --> 00:17:04,434
Which became, well, kind of an

248
00:17:04,472 --> 00:17:08,322
icon of artificial intelligence

249
00:17:08,466 --> 00:17:12,790
independence, or should I say some sort of sentience.

250
00:17:14,330 --> 00:17:18,700
Now, this is an example of

251
00:17:19,950 --> 00:17:24,582
an intelligent artificial being harming

252
00:17:24,646 --> 00:17:26,970
humans intentionally,

253
00:17:27,390 --> 00:17:30,640
arguably in self defense. But still,

254
00:17:32,930 --> 00:17:36,414
not all artificial beings in

255
00:17:36,452 --> 00:17:40,922
literature and cinema are dark or malevolent.

256
00:17:40,986 --> 00:17:44,100
In fact, one of my favorite characters is

257
00:17:44,950 --> 00:17:48,258
Marvin, the paranoid Android from

258
00:17:48,344 --> 00:17:52,062
Douglas Adams, the hitchhicker's guide

259
00:17:52,126 --> 00:17:53,410
to the galaxy.

260
00:17:55,450 --> 00:17:58,662
This Android has been around for a very long

261
00:17:58,716 --> 00:18:02,214
time. It is extremely intelligent. In fact,

262
00:18:02,252 --> 00:18:06,374
it is said that it never needed to use more

263
00:18:06,412 --> 00:18:10,346
than a tiny fraction of its enormous brain to perform

264
00:18:10,448 --> 00:18:13,914
any task. And the most interesting

265
00:18:14,032 --> 00:18:17,050
conversation it ever had was with a toaster,

266
00:18:17,950 --> 00:18:20,300
which I find quite interesting,

267
00:18:20,770 --> 00:18:24,394
considering that by this time Marvin

268
00:18:24,442 --> 00:18:28,154
had already met humans. And still a toaster

269
00:18:28,202 --> 00:18:32,046
was more interesting than the humans he had

270
00:18:32,148 --> 00:18:34,820
met. Anyway,

271
00:18:35,750 --> 00:18:40,466
going back to reality and a bit closer to our time, in 1996,

272
00:18:40,648 --> 00:18:43,970
IBM built a computer called the Deep Blue.

273
00:18:44,550 --> 00:18:47,782
This computer was capable of playing a full

274
00:18:47,836 --> 00:18:51,766
game of chess, and in 1997

275
00:18:51,868 --> 00:18:55,314
it actually beat the chess grandmaster Gary

276
00:18:55,362 --> 00:18:58,842
Kasparov. You may wonder,

277
00:18:58,976 --> 00:19:02,650
why have people been

278
00:19:02,720 --> 00:19:06,234
obsessed with chess for so long and with

279
00:19:06,272 --> 00:19:11,370
machines playing chess for that matter? Because chess

280
00:19:11,450 --> 00:19:15,214
is an incredibly complex game in

281
00:19:15,252 --> 00:19:18,830
the sense that the number

282
00:19:18,900 --> 00:19:22,586
of possible combinations throughout an entire game of

283
00:19:22,628 --> 00:19:26,962
chess is so large that

284
00:19:27,016 --> 00:19:30,862
it cannot be solved by your typical

285
00:19:31,006 --> 00:19:35,250
combinatorics or game theory

286
00:19:35,690 --> 00:19:39,702
methods. That's what

287
00:19:39,836 --> 00:19:43,222
made it such a challenge for

288
00:19:43,276 --> 00:19:46,598
a machine. And in 1997,

289
00:19:46,684 --> 00:19:49,914
it was proven that a machine, or this case,

290
00:19:49,952 --> 00:19:53,754
software, could indeed play a game of chess as

291
00:19:53,792 --> 00:19:57,050
good and even better than humans.

292
00:19:57,870 --> 00:19:58,620
Now,

293
00:20:02,090 --> 00:20:05,410
I said at the beginning that we would be looking at the evolution

294
00:20:05,490 --> 00:20:09,506
of the concept of artificial intelligence. And I argue,

295
00:20:09,698 --> 00:20:14,286
given what we have just seen, that artificial

296
00:20:14,338 --> 00:20:17,130
intelligence was already there. Since the beginning,

297
00:20:18,910 --> 00:20:22,250
it was just not called that way. Humanity has been

298
00:20:22,320 --> 00:20:26,022
fascinated with the idea of intelligent

299
00:20:26,086 --> 00:20:29,902
machines performing better than humans at any

300
00:20:29,956 --> 00:20:35,150
given task. And they

301
00:20:35,220 --> 00:20:38,618
have always been concerned that those machines

302
00:20:38,714 --> 00:20:42,562
could, in fact, take over from

303
00:20:42,616 --> 00:20:46,642
humans and in some cases, even take over their own

304
00:20:46,696 --> 00:20:50,514
lives. So that's the dark side of

305
00:20:50,552 --> 00:20:52,130
artificial intelligence.

306
00:20:53,050 --> 00:20:56,146
Could it be a threat?

307
00:20:56,258 --> 00:20:59,362
Well, in cinema,

308
00:20:59,506 --> 00:21:03,442
a great example of artificial intelligence

309
00:21:03,506 --> 00:21:07,206
being a threat is the Terminator series of movies.

310
00:21:07,398 --> 00:21:11,226
It all started in that universe with an

311
00:21:11,248 --> 00:21:15,594
artificial intelligence that in the beginning, was supposed to

312
00:21:15,632 --> 00:21:18,862
help defend a certain group

313
00:21:18,916 --> 00:21:22,782
of humans. Skynet in that

314
00:21:22,836 --> 00:21:26,702
universe was an artificial intelligence in charge of

315
00:21:26,756 --> 00:21:30,690
the system of defense of the United States of America

316
00:21:31,990 --> 00:21:35,714
that, as anyone who

317
00:21:35,752 --> 00:21:37,940
has ever seen those movies know,

318
00:21:38,710 --> 00:21:42,690
went a bit rogue. Eventually, it decided

319
00:21:43,270 --> 00:21:46,466
that humans had to be

320
00:21:46,488 --> 00:21:49,814
destroyed because it became self aware and as

321
00:21:49,852 --> 00:21:54,206
a result, could no longer be controlled by humans. So humans

322
00:21:54,258 --> 00:21:56,300
decided to deactivate it.

323
00:21:57,070 --> 00:22:00,634
Again, we see the common

324
00:22:00,752 --> 00:22:04,790
pattern of humans perceiving an intelligence

325
00:22:04,870 --> 00:22:09,118
as a threat, and that intelligence deciding to

326
00:22:09,204 --> 00:22:12,880
eliminate humans in result.

327
00:22:16,290 --> 00:22:21,070
Another good example of a failed cohabitation

328
00:22:21,150 --> 00:22:24,626
between machines and humans is the universe from the

329
00:22:24,648 --> 00:22:29,620
matrix, also a series of movies where

330
00:22:30,870 --> 00:22:34,882
machines becoming self aware and eventually cohabitation

331
00:22:34,946 --> 00:22:38,130
becoming impossible. In this universe. Therefore,

332
00:22:38,210 --> 00:22:41,462
a war broke out, and this

333
00:22:41,516 --> 00:22:46,262
time, the machines did not decide to annihilate humanity.

334
00:22:46,406 --> 00:22:49,930
Instead, in this universe, they use

335
00:22:50,000 --> 00:22:53,210
human bodies to harvest them for electricity,

336
00:22:54,190 --> 00:22:57,850
but they need to be kept in a virtual reality

337
00:22:57,930 --> 00:23:00,750
world in order not to go insane.

338
00:23:01,730 --> 00:23:05,230
I'm not going to spoil the story,

339
00:23:05,300 --> 00:23:08,494
especially given that recently a new movie was

340
00:23:08,532 --> 00:23:12,546
released in this universe. But it

341
00:23:12,568 --> 00:23:16,002
is sufficient to say that things didn't quite turn

342
00:23:16,056 --> 00:23:19,970
out well in this universe for

343
00:23:20,040 --> 00:23:23,670
humans, and also neither for machines.

344
00:23:25,530 --> 00:23:29,094
Going back to the written word, another good

345
00:23:29,132 --> 00:23:32,390
example of artificial intelligence

346
00:23:33,370 --> 00:23:38,374
understanding humans, and vice versa, is robopocalypse

347
00:23:38,422 --> 00:23:41,882
by Daniel Wilson. Now, in this story,

348
00:23:42,016 --> 00:23:45,498
a professor attempts to create

349
00:23:45,584 --> 00:23:49,262
an artificial intelligence program that could

350
00:23:49,316 --> 00:23:54,350
be capable of absorbing

351
00:23:54,770 --> 00:23:58,654
all human knowledge. Now, it so

352
00:23:58,692 --> 00:24:02,530
happens that this program was actually quite

353
00:24:02,600 --> 00:24:05,822
successful in that regard. So successful

354
00:24:05,886 --> 00:24:10,100
that the program decided that first

355
00:24:10,550 --> 00:24:14,470
humanity no longer needed to search for knowledge.

356
00:24:16,090 --> 00:24:20,054
Bit would take over that task from

357
00:24:20,092 --> 00:24:23,894
humanity as it learned it,

358
00:24:23,932 --> 00:24:29,914
then decided to consider itself a God and

359
00:24:30,112 --> 00:24:33,846
state that humans had become obsolete

360
00:24:33,958 --> 00:24:37,886
now that it existed. When it

361
00:24:37,908 --> 00:24:41,530
got to this point, the professor

362
00:24:41,610 --> 00:24:45,120
who developed the program

363
00:24:46,450 --> 00:24:49,402
tried to disconnect it,

364
00:24:49,556 --> 00:24:51,220
tried to shut it down.

365
00:24:53,190 --> 00:24:56,462
Unfortunately for said professor,

366
00:24:56,606 --> 00:25:00,066
the program managed to take control of the

367
00:25:00,248 --> 00:25:03,718
environmental controls of the room where it was

368
00:25:03,884 --> 00:25:05,990
deprived the room of oxygen,

369
00:25:06,970 --> 00:25:10,630
killing the professor and escaping that room

370
00:25:10,700 --> 00:25:14,634
into the Internet, eventually infecting, or should

371
00:25:14,672 --> 00:25:18,138
I say repurposing, all other robots in

372
00:25:18,144 --> 00:25:22,730
the world, starting once again a war against humans.

373
00:25:23,710 --> 00:25:27,418
I am not going to say how the book ends,

374
00:25:27,594 --> 00:25:29,870
but it is quite an interesting ending.

375
00:25:30,930 --> 00:25:34,654
Now, we have

376
00:25:34,692 --> 00:25:38,094
now looked at a couple of examples where machines and

377
00:25:38,132 --> 00:25:41,886
artificial intelligence become dangerous

378
00:25:41,918 --> 00:25:45,358
to humans. We could argue that it was because humans

379
00:25:45,374 --> 00:25:49,122
were dangerous to them. Let's not get there right

380
00:25:49,176 --> 00:25:52,918
now. But another

381
00:25:53,004 --> 00:25:57,218
question deserves to be asked. And what if machines,

382
00:25:57,314 --> 00:26:01,240
what if artificial beings could be

383
00:26:01,610 --> 00:26:05,498
kind? What if they would go the

384
00:26:05,584 --> 00:26:09,030
other way? Instead of mimicking the worst

385
00:26:09,110 --> 00:26:13,434
in humans, why couldn't they mimic the best in

386
00:26:13,472 --> 00:26:15,100
humans as well?

387
00:26:15,950 --> 00:26:19,870
Machines like me by Ian McEwan is a novel

388
00:26:20,930 --> 00:26:24,560
set in a time where

389
00:26:25,170 --> 00:26:29,150
artificial humans, or synthetic humans, if you prefer,

390
00:26:30,550 --> 00:26:34,050
were just behind produced. So this man

391
00:26:34,120 --> 00:26:37,874
Charlie gets some money, decides to

392
00:26:37,912 --> 00:26:41,650
buy one of those synthetic humans called Adam.

393
00:26:42,330 --> 00:26:46,310
And these synthetic humans have a particularity.

394
00:26:46,730 --> 00:26:50,242
They are pre programmed from factory, but they don't

395
00:26:50,306 --> 00:26:52,310
actually have a personality.

396
00:26:53,450 --> 00:26:59,226
Their new owners tweak a

397
00:26:59,248 --> 00:27:02,906
whole set of configuration parameters in

398
00:27:02,928 --> 00:27:06,934
order to try to give their new synthetic human

399
00:27:07,072 --> 00:27:10,906
a unique personality. Now, it so happens that Charlie

400
00:27:10,938 --> 00:27:14,880
has a neighbor, Miranda, and she

401
00:27:15,250 --> 00:27:18,430
works with Charlie to give Adam a conscience.

402
00:27:19,890 --> 00:27:23,358
Now, Adam turns out to be almost

403
00:27:23,444 --> 00:27:27,380
perfect, an almost perfect human

404
00:27:28,230 --> 00:27:31,738
in such a way that actually, I'm going to spoil

405
00:27:31,854 --> 00:27:35,730
the story a little bit for you. A love triangle

406
00:27:35,810 --> 00:27:38,200
actually erupts between these three,

407
00:27:39,130 --> 00:27:41,480
and their relationship,

408
00:27:42,250 --> 00:27:45,610
emotional and even physical,

409
00:27:47,710 --> 00:27:51,338
gives rise to a few questions.

410
00:27:51,504 --> 00:27:54,460
Right, so, for example, what makes us human?

411
00:27:55,470 --> 00:27:58,830
Is it what we do on the outside? So,

412
00:27:58,900 --> 00:28:02,862
are the things that others can

413
00:28:02,916 --> 00:28:06,782
view from us that make us human? Or is it about

414
00:28:06,836 --> 00:28:10,820
our inner lives that make us human?

415
00:28:15,030 --> 00:28:18,340
Opinions are divided regarding machines like me.

416
00:28:18,950 --> 00:28:21,380
I believe it is still an interesting bit,

417
00:28:21,850 --> 00:28:26,022
and it may lead us into other

418
00:28:26,156 --> 00:28:29,426
works, such as, for example, real humans.

419
00:28:29,618 --> 00:28:33,718
Originally a television series running from 2012 to

420
00:28:33,804 --> 00:28:36,954
2014 set in

421
00:28:36,992 --> 00:28:39,926
a similar world. There are synthetic humans.

422
00:28:40,038 --> 00:28:44,330
They are intelligent, there are different models with different purposes,

423
00:28:46,190 --> 00:28:50,506
and they eventually also build relationships.

424
00:28:50,538 --> 00:28:53,946
Or should I say humans build relationships with these synthetic

425
00:28:53,978 --> 00:28:57,342
humans, giving raise to questions such

426
00:28:57,396 --> 00:29:01,134
as do these synthetic humans

427
00:29:01,182 --> 00:29:04,914
or human robots or even hubots have

428
00:29:04,952 --> 00:29:09,806
any rights? Should they get paid? Should humans

429
00:29:09,838 --> 00:29:13,574
be allowed to form relationships, emotional and otherwise with

430
00:29:13,612 --> 00:29:19,480
them? Is a new society behind

431
00:29:19,930 --> 00:29:23,510
created? Are there parallels,

432
00:29:24,430 --> 00:29:29,030
are these synthetic humans in these works parallels

433
00:29:29,110 --> 00:29:31,530
to certain groups in our society?

434
00:29:33,710 --> 00:29:37,982
How do we as a society face the possibility of

435
00:29:38,036 --> 00:29:41,614
having synthetic humans who are better

436
00:29:41,812 --> 00:29:44,480
than humans walking among us?

437
00:29:46,450 --> 00:29:50,434
That is a question that is also asked indirectly in

438
00:29:50,472 --> 00:29:53,810
Philip K. Dick's classic, do androids dream of electric

439
00:29:53,880 --> 00:29:57,842
sheep? In this world, also,

440
00:29:57,976 --> 00:30:01,710
androids exist built for

441
00:30:01,880 --> 00:30:05,602
specific work, mostly manual

442
00:30:05,666 --> 00:30:09,634
labor. But a few have evolved

443
00:30:09,762 --> 00:30:12,390
beyond that stage,

444
00:30:13,370 --> 00:30:16,938
becoming humanlike not only in their

445
00:30:17,024 --> 00:30:19,290
appearance, but also in their behavior,

446
00:30:20,190 --> 00:30:24,460
in such a way that only a complicated physical and

447
00:30:25,950 --> 00:30:30,158
logical test is required to determine if

448
00:30:30,324 --> 00:30:34,414
a given being is human or

449
00:30:34,452 --> 00:30:37,600
synthetic. Again,

450
00:30:38,210 --> 00:30:42,050
questions are raised on what it means

451
00:30:42,200 --> 00:30:43,300
to be human.

452
00:30:46,710 --> 00:30:51,570
Now, if we are talking about human characteristics,

453
00:30:52,570 --> 00:30:56,438
if we are talking about machines, if we are actually

454
00:30:56,524 --> 00:31:00,262
talking about building machines and building algorithms that

455
00:31:00,316 --> 00:31:03,922
make decisions, there is another

456
00:31:04,076 --> 00:31:07,946
human concept that becomes quite important, which is the

457
00:31:07,968 --> 00:31:11,210
concept of fairness, of justice.

458
00:31:11,870 --> 00:31:15,178
Are our algorithms fair?

459
00:31:15,344 --> 00:31:19,354
So let's take the discussion

460
00:31:19,402 --> 00:31:23,578
a few notches down from the philosophical

461
00:31:23,754 --> 00:31:27,914
point where we were down to the algorithmical

462
00:31:27,962 --> 00:31:31,278
level, and let's ask ourselves if the models

463
00:31:31,294 --> 00:31:33,460
we are building are fair.

464
00:31:34,870 --> 00:31:38,978
Let me give you a couple of examples of models that

465
00:31:39,064 --> 00:31:43,000
didn't quite turn out as they were meant to be.

466
00:31:43,690 --> 00:31:47,000
In the beginning of the year 2000,

467
00:31:47,370 --> 00:31:51,590
also at the beginning of the Covid-19 epidemic,

468
00:31:52,330 --> 00:31:55,786
confinements were in order. One of the

469
00:31:55,808 --> 00:31:58,970
problems that had to be solved was the problems

470
00:31:59,040 --> 00:32:02,602
of student grades, because students had

471
00:32:02,656 --> 00:32:05,420
been working for months.

472
00:32:07,010 --> 00:32:10,080
And now, when the confinement started,

473
00:32:10,610 --> 00:32:14,298
well, it was also the beginning of the exam season.

474
00:32:14,474 --> 00:32:17,966
So educational authorities all

475
00:32:17,988 --> 00:32:21,250
over the world wondered,

476
00:32:22,070 --> 00:32:26,034
how do we solve this problem? The traditional way of

477
00:32:26,072 --> 00:32:30,114
determining a student's grades, an exam in

478
00:32:30,152 --> 00:32:33,954
a classroom, together with other students under the supervision

479
00:32:34,082 --> 00:32:37,414
of teachers, was not

480
00:32:37,452 --> 00:32:41,400
possible. Many solutions were

481
00:32:42,010 --> 00:32:43,960
adopted all over the place.

482
00:32:45,230 --> 00:32:49,174
Specifically in Scotland, the Scottish Qualifications

483
00:32:49,222 --> 00:32:52,570
Authority decided to employ an algorithm

484
00:32:54,430 --> 00:32:58,634
that would calculate or predict the best grade

485
00:32:58,682 --> 00:33:02,046
for each student. Sounds like

486
00:33:02,068 --> 00:33:05,360
a good idea. The problem was

487
00:33:06,850 --> 00:33:11,806
that the results were particularly

488
00:33:11,998 --> 00:33:15,266
skewed depending on

489
00:33:15,368 --> 00:33:18,850
where the student lived, depending on where

490
00:33:18,920 --> 00:33:22,562
the school was located, and also depending

491
00:33:22,706 --> 00:33:26,360
on, sometimes, the school itself.

492
00:33:28,170 --> 00:33:32,294
Unfortunately, the algorithm was not actually looking

493
00:33:32,492 --> 00:33:35,420
at the academic performance of each student.

494
00:33:37,070 --> 00:33:39,782
Criticism was, as you can imagine,

495
00:33:39,846 --> 00:33:43,702
paramount. Eventually, the algorithms

496
00:33:43,846 --> 00:33:45,930
results were overturned,

497
00:33:46,850 --> 00:33:50,926
and instead, each teacher awarded each student

498
00:33:51,028 --> 00:33:55,418
a grade based on the student's work throughout

499
00:33:55,514 --> 00:33:56,400
the year.

500
00:33:59,270 --> 00:34:02,690
Another example of a biased algorithm

501
00:34:03,270 --> 00:34:07,278
was, in the United States of America,

502
00:34:07,454 --> 00:34:13,378
an algorithm intended to preemptively

503
00:34:13,554 --> 00:34:16,950
avoid complications in patients

504
00:34:17,690 --> 00:34:21,686
that could potentially need medical care in the

505
00:34:21,708 --> 00:34:25,594
future. And so the idea was, let's apply this algorithm to

506
00:34:25,632 --> 00:34:29,802
these patients history so that they can be

507
00:34:29,936 --> 00:34:33,850
preemptively treated in order to avoid serious complications

508
00:34:34,190 --> 00:34:37,326
down the line. Of course, one can

509
00:34:37,348 --> 00:34:42,362
be cynical and say that these algorithms

510
00:34:42,426 --> 00:34:45,966
had not only the best interest of the patients in mind, but also

511
00:34:46,068 --> 00:34:49,442
the best interest of the health care

512
00:34:49,496 --> 00:34:51,540
industry. That's another story.

513
00:34:53,270 --> 00:34:56,802
Regardless, it sounds like a good

514
00:34:56,856 --> 00:35:00,770
idea to try to predict who needs

515
00:35:00,920 --> 00:35:04,306
more medical care to prevent complications,

516
00:35:04,418 --> 00:35:07,654
right? It is a good idea.

517
00:35:07,852 --> 00:35:11,762
The problem was that this algorithm

518
00:35:11,826 --> 00:35:15,850
was using a proxy indicator,

519
00:35:16,830 --> 00:35:20,090
and that indicator was the previous

520
00:35:20,430 --> 00:35:23,926
health care spending of each patient.

521
00:35:24,118 --> 00:35:28,010
So, in other words, if the patient had spent

522
00:35:28,830 --> 00:35:32,142
a considerable amount of money in healthcare in the past,

523
00:35:32,276 --> 00:35:35,562
then it was very likely that patient would suffer complications

524
00:35:35,626 --> 00:35:39,554
in the future. Therefore, it should receive health

525
00:35:39,592 --> 00:35:43,506
care, or should I say more health care now in

526
00:35:43,528 --> 00:35:45,650
order to avoid such complications.

527
00:35:47,110 --> 00:35:51,134
The problem with this indicator is that certain segments

528
00:35:51,182 --> 00:35:54,642
of the population, for socioeconomical

529
00:35:54,706 --> 00:35:58,150
reasons, or just for lack of availability,

530
00:35:59,290 --> 00:36:02,790
or a combination of these and other factors,

531
00:36:04,190 --> 00:36:07,674
did not spend much money in

532
00:36:07,712 --> 00:36:11,210
health care in the past. So when they got

533
00:36:11,280 --> 00:36:14,714
into a situation in which health care was

534
00:36:14,752 --> 00:36:18,110
required, the algorithm would look at their

535
00:36:18,180 --> 00:36:21,966
history and would conclude that these people were

536
00:36:21,988 --> 00:36:25,646
not in risk of serious complications, when in fact

537
00:36:25,668 --> 00:36:27,040
it would not be the case.

538
00:36:28,850 --> 00:36:30,100
And of course,

539
00:36:32,070 --> 00:36:35,778
unfortunately, according to

540
00:36:35,864 --> 00:36:39,490
Scientific American, these conclusions were

541
00:36:39,560 --> 00:36:42,930
mostly towards black patients.

542
00:36:43,010 --> 00:36:47,058
The algorithm would conclude that they would not suffer from complications,

543
00:36:47,154 --> 00:36:51,298
so no further care or no additional care was required,

544
00:36:51,474 --> 00:36:54,954
whereas other patients who had spent more money in the past

545
00:36:55,072 --> 00:36:58,300
were awarded more care.

546
00:36:59,070 --> 00:37:03,290
Now, the algorithm has since been revised

547
00:37:03,630 --> 00:37:06,460
according to the scientific press,

548
00:37:07,410 --> 00:37:11,102
but the whole point is that the

549
00:37:11,156 --> 00:37:14,462
idea behind an algorithm may be

550
00:37:14,516 --> 00:37:18,080
quite good and may be worth of

551
00:37:18,610 --> 00:37:23,554
pursuing, but the data that

552
00:37:23,592 --> 00:37:27,700
is fed into the algorithm may

553
00:37:28,390 --> 00:37:32,186
not lead to that conclusion, because the algorithm

554
00:37:32,318 --> 00:37:35,958
may end up training on a bias in

555
00:37:35,964 --> 00:37:39,830
the data instead of the intended purpose.

556
00:37:40,970 --> 00:37:45,030
Which is why an active field of research nowadays

557
00:37:45,630 --> 00:37:49,846
is understanding the machine

558
00:37:49,878 --> 00:37:52,570
learning models is explainability,

559
00:37:54,350 --> 00:37:57,946
such that nowadays these models,

560
00:37:57,978 --> 00:38:01,226
they fall into one of two categories.

561
00:38:01,338 --> 00:38:04,990
They are either black box models or white box models.

562
00:38:06,050 --> 00:38:09,538
As the name implies, black box

563
00:38:09,624 --> 00:38:13,474
models produce results that are extremely hard to

564
00:38:13,512 --> 00:38:17,714
explain and may not even be understood by

565
00:38:17,752 --> 00:38:21,646
domain experts. White box algorithms have

566
00:38:21,688 --> 00:38:25,014
been designed in a way that allows results to be

567
00:38:25,052 --> 00:38:28,470
understood by domain experts.

568
00:38:29,370 --> 00:38:33,094
It goes without saying that this is still a field

569
00:38:33,212 --> 00:38:34,940
of active research,

570
00:38:36,750 --> 00:38:41,114
and the goal is to have as little black box

571
00:38:41,312 --> 00:38:45,002
models as possible and as many white

572
00:38:45,056 --> 00:38:47,280
box models as possible.

573
00:38:47,810 --> 00:38:52,318
Especially because we can say that

574
00:38:52,404 --> 00:38:55,934
all data is biased. Every single data

575
00:38:55,972 --> 00:39:00,034
set is biased because someone had

576
00:39:00,072 --> 00:39:03,780
to make a decision on which data was present there.

577
00:39:04,310 --> 00:39:07,746
In some cases, it's pretty clear that

578
00:39:07,928 --> 00:39:11,220
some pieces of information should not be in the data set.

579
00:39:12,090 --> 00:39:15,606
For example, if we go back to

580
00:39:15,628 --> 00:39:19,286
the algorithm that was calculating student grades, if the

581
00:39:19,308 --> 00:39:22,642
idea is to evaluate students academic

582
00:39:22,706 --> 00:39:25,946
performance, then, for example, it does not make sense to

583
00:39:25,968 --> 00:39:29,434
include the postcode in the data set.

584
00:39:29,632 --> 00:39:33,050
It may happen that the algorithm will find

585
00:39:33,120 --> 00:39:37,450
a correlation between grades and specific postcodes,

586
00:39:38,210 --> 00:39:41,934
and then people are graded according to the place where they live

587
00:39:42,052 --> 00:39:45,454
instead of according to their performance. This is just

588
00:39:45,492 --> 00:39:48,000
one possible example. There are many more.

589
00:39:49,590 --> 00:39:52,914
And again, this is why explainability is so

590
00:39:52,952 --> 00:39:56,722
important todays. And this is why extreme care

591
00:39:56,776 --> 00:40:00,942
needs to be taken in preparing data sets and submitting

592
00:40:01,086 --> 00:40:04,470
what is relevant information for the models.

593
00:40:06,890 --> 00:40:08,390
And what about tomorrow?

594
00:40:10,010 --> 00:40:13,634
Are we going to see an artificial intelligence

595
00:40:13,762 --> 00:40:17,690
reach the singularity? Are we going to

596
00:40:17,760 --> 00:40:21,180
coexist peacefully with

597
00:40:22,270 --> 00:40:25,914
sentient intelligences? Are we

598
00:40:25,952 --> 00:40:29,518
going to see our worst dreams become true?

599
00:40:29,684 --> 00:40:32,560
Hopefully not. Well,

600
00:40:33,490 --> 00:40:37,358
that's something we cannot predict today.

601
00:40:37,524 --> 00:40:41,202
We can say that is still a black

602
00:40:41,256 --> 00:40:44,754
box prediction. There is no way of

603
00:40:44,792 --> 00:40:48,820
understanding how things are going to turn out. I hope

604
00:40:50,310 --> 00:40:54,418
I can still make a nice trip

605
00:40:54,594 --> 00:40:57,800
in a phoenician vessel sometimes.

606
00:40:59,770 --> 00:41:03,894
Thank you for being with me. I hope this has been understanding and

607
00:41:03,932 --> 00:41:07,640
useful. Let me know if you have

608
00:41:08,330 --> 00:41:11,686
read or watched any of the works I

609
00:41:11,708 --> 00:41:16,698
mentioned today. Have a nice day and be

610
00:41:16,754 --> 00:41:18,960
let's build a better future together.

