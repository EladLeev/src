1
00:00:23,530 --> 00:00:27,170
Hi everyone, my name is Kosman or I'm Python developer. I'm from Spain.

2
00:00:27,330 --> 00:00:30,454
This tool is called tips and tricks for data

3
00:00:30,492 --> 00:00:34,520
science project with Python. Basically, Python has become the most

4
00:00:34,970 --> 00:00:38,322
widely used language for mature learning and data science

5
00:00:38,386 --> 00:00:41,190
project due to its simplicity and specialty.

6
00:00:41,770 --> 00:00:45,266
For this purpose, Python provides access to grid

7
00:00:45,298 --> 00:00:49,640
libraries and frameworks for artificial intelligence and mature learning,

8
00:00:50,130 --> 00:00:53,470
flexibility and platform independence.

9
00:00:55,970 --> 00:00:59,354
This year I have writing and published this book with title

10
00:00:59,402 --> 00:01:02,590
Big Data, Machine Learning and data science with Python.

11
00:01:02,950 --> 00:01:06,610
This book is published in Spanish. In this book,

12
00:01:06,680 --> 00:01:10,526
basically you can find practical symbols

13
00:01:10,558 --> 00:01:13,006
with panda, Pyspark, psychic learner,

14
00:01:13,038 --> 00:01:16,986
tensorflow, Hadoop, Jupiter Network

15
00:01:17,038 --> 00:01:21,254
and Apache the Pelink. This could

16
00:01:21,372 --> 00:01:25,174
be the main point, the main tackling points. I will start

17
00:01:25,212 --> 00:01:29,174
with an introducing Python as

18
00:01:29,212 --> 00:01:31,590
programming language for mature learning projects.

19
00:01:32,590 --> 00:01:35,962
I will comment the main stages for a mature learning

20
00:01:36,016 --> 00:01:39,658
project also will comment what are

21
00:01:39,664 --> 00:01:43,020
the main Python libraries for your project?

22
00:01:44,130 --> 00:01:47,786
Finally, I will commend the main Python

23
00:01:47,818 --> 00:01:50,910
tools for deep learning in data science projects.

24
00:01:51,970 --> 00:01:56,030
Well, Python simplicity allows developers to write reliable

25
00:01:56,110 --> 00:01:59,762
systems and developers get to put

26
00:01:59,816 --> 00:02:04,418
all the effort into solving a mature learning problem instead

27
00:02:04,504 --> 00:02:08,162
of focusing on the technical nuance of the

28
00:02:08,216 --> 00:02:12,022
language. Since Python is a general

29
00:02:12,156 --> 00:02:16,070
purpose language, it can do a set of complex machine learning

30
00:02:16,140 --> 00:02:20,234
tasks and enable you to build prototypes quickly

31
00:02:20,352 --> 00:02:24,490
that allow you to test your product for machine learning purpose.

32
00:02:26,510 --> 00:02:30,822
For example, there are some fields where artificial

33
00:02:30,886 --> 00:02:33,930
intelligence and mature learning techniques are applied.

34
00:02:37,630 --> 00:02:41,786
For example, Span filters, recommendation systems, certain giants,

35
00:02:41,898 --> 00:02:46,130
personal assistance and fraud detection systems.

36
00:02:47,190 --> 00:02:51,074
In this table we can see the

37
00:02:51,112 --> 00:02:54,580
main libraries, the main module we have in Python for each

38
00:02:55,130 --> 00:02:56,150
depending.

39
00:02:58,650 --> 00:03:02,550
For example for material we have kerastins for flow and central

40
00:03:03,690 --> 00:03:07,014
for high performance in scientific computing we have

41
00:03:07,052 --> 00:03:10,346
numpy and scipy. For computer

42
00:03:10,448 --> 00:03:14,426
vision, we have OpenCV. For data analysis we have numpy and

43
00:03:14,448 --> 00:03:18,054
pandas, and for natural

44
00:03:18,102 --> 00:03:29,774
language processing we have spicy and one

45
00:03:29,812 --> 00:03:33,170
way to review some of these libraries. For example,

46
00:03:33,240 --> 00:03:36,734
Numpy is the fundamental package required for high performance

47
00:03:36,862 --> 00:03:40,590
scientific computing and data analysis in the Python ecosystem.

48
00:03:40,750 --> 00:03:44,526
And the main data structure

49
00:03:44,558 --> 00:03:48,594
in Numpy is the array, which is a shorthand

50
00:03:48,642 --> 00:03:52,710
name for n dimensional array. When working with Numpy

51
00:03:53,290 --> 00:03:56,870
data and the array is simply

52
00:03:57,310 --> 00:04:00,774
referred to has an array you can create, for example, unidimensional,

53
00:04:00,822 --> 00:04:04,810
b dimensional and three dimensional arrays.

54
00:04:06,350 --> 00:04:09,814
The main advantage of Numpy is its speed,

55
00:04:09,862 --> 00:04:14,110
mainly to the fact that it is developed in C programming language

56
00:04:14,770 --> 00:04:18,430
for data science and mature learning tags.

57
00:04:18,930 --> 00:04:21,150
It provides a lot of advantages.

58
00:04:24,150 --> 00:04:28,146
Other models we have in Python is

59
00:04:28,328 --> 00:04:31,662
one of the most popular libraries for scientific

60
00:04:31,726 --> 00:04:35,540
Python is pandas that is built open

61
00:04:36,010 --> 00:04:39,718
numpy array, thereby preserving fast

62
00:04:39,884 --> 00:04:43,954
execution speed and offering many data engineering features

63
00:04:44,002 --> 00:04:47,026
including grading, writing, manifesting the format,

64
00:04:47,058 --> 00:04:50,614
selecting success of data, calculating across row

65
00:04:50,662 --> 00:04:54,282
and columns, filling and filling missing data,

66
00:04:54,416 --> 00:04:57,898
applying operations to independent groups within

67
00:04:57,984 --> 00:04:58,860
the data.

68
00:05:01,070 --> 00:05:05,386
Another task related for example, with combining multiple

69
00:05:05,418 --> 00:05:06,720
data sets together.

70
00:05:11,010 --> 00:05:14,302
One of the structures we have is

71
00:05:14,356 --> 00:05:17,954
very useful in pandas is the data frames is the most widely used data

72
00:05:17,992 --> 00:05:21,666
structure. You can imagine it at a set table in a

73
00:05:21,688 --> 00:05:25,374
database or a spreadsheet with rows and columns.

74
00:05:25,422 --> 00:05:29,282
Basically, data frames is a two dimensional data structure with

75
00:05:29,336 --> 00:05:31,720
potentially iteration data.

76
00:05:32,810 --> 00:05:35,990
The main features is that it has a size

77
00:05:36,060 --> 00:05:39,558
mutable structure that means data can be

78
00:05:39,724 --> 00:05:43,820
added or deleted from it in a simple way.

79
00:05:45,630 --> 00:05:49,018
PandAs provides another interesting project that is called

80
00:05:49,104 --> 00:05:52,640
pandas profiling. That is an open source Python model

81
00:05:53,010 --> 00:05:57,370
with which we can quickly perform an exploratory data analysis

82
00:05:57,530 --> 00:06:01,134
with just a few lines of code. In addition, it can

83
00:06:01,172 --> 00:06:05,154
generate interactive reports in web format that can be

84
00:06:05,192 --> 00:06:09,342
present to anyone. In short, what panel's profiling

85
00:06:09,406 --> 00:06:13,074
does is to save us all the work

86
00:06:13,192 --> 00:06:16,290
of visualization and understanding

87
00:06:16,450 --> 00:06:19,926
the distribution of each variable in

88
00:06:19,948 --> 00:06:21,240
our data set.

89
00:06:22,730 --> 00:06:26,774
Generating a report with all the information is

90
00:06:26,812 --> 00:06:27,750
ill visible.

91
00:06:30,270 --> 00:06:33,878
Now I'm going to commend

92
00:06:33,894 --> 00:06:37,546
the many stages of a machine learning projects. Learning is the

93
00:06:37,568 --> 00:06:40,910
study of certain algorithms and statistical techniques that allow

94
00:06:40,980 --> 00:06:45,482
computers to perform complex tasks without receiving

95
00:06:45,546 --> 00:06:51,214
instructions beforehand. Instead of using pre

96
00:06:51,252 --> 00:06:55,360
programming, directing certain behavior under a certain set of

97
00:06:56,210 --> 00:06:59,470
circumstances, machine learning relies on pattern

98
00:06:59,550 --> 00:07:02,610
recognition and associated inferences.

99
00:07:03,590 --> 00:07:07,058
In this diagram we can see the main stages of a

100
00:07:07,064 --> 00:07:11,510
material. In project we start with level observations

101
00:07:12,090 --> 00:07:16,406
and in

102
00:07:16,428 --> 00:07:19,734
the stage two with

103
00:07:19,772 --> 00:07:23,338
us is splitting these level

104
00:07:23,424 --> 00:07:26,694
observations in training and data sets.

105
00:07:26,822 --> 00:07:29,900
In step three, our model

106
00:07:30,210 --> 00:07:33,406
is built using training data and

107
00:07:33,508 --> 00:07:37,520
for validating the model we use data set.

108
00:07:37,970 --> 00:07:41,118
In the last step, basically, the model is

109
00:07:41,204 --> 00:07:45,074
then evaluated on the degree to which it arrives at the

110
00:07:45,112 --> 00:07:46,450
correct output.

111
00:07:50,230 --> 00:07:56,454
In this diagram we can see in

112
00:07:56,492 --> 00:08:00,102
a more generical way these

113
00:08:00,156 --> 00:08:04,006
stages. The machine learning lifecycle basically is the

114
00:08:04,028 --> 00:08:07,794
cyclical process that data science project follow.

115
00:08:07,932 --> 00:08:11,622
It defines each step that an organization

116
00:08:11,766 --> 00:08:15,114
should follow to take advantage of mature learning and

117
00:08:15,152 --> 00:08:19,382
artificial intelligence to derive practical

118
00:08:19,446 --> 00:08:23,740
business value. These are the five major

119
00:08:24,270 --> 00:08:28,030
steps in the mature learning lifecycle, all of which have

120
00:08:28,100 --> 00:08:31,774
equals importance and go in a specific order. We start

121
00:08:31,812 --> 00:08:35,282
with getting in data from various sources. In the step

122
00:08:35,336 --> 00:08:39,810
two, we try to clean in data to have homogeneity.

123
00:08:40,230 --> 00:08:43,714
In the step three, we try to build our model

124
00:08:43,832 --> 00:08:47,890
selecting the right material learning algorithm depending

125
00:08:48,870 --> 00:08:52,374
our data. In the step four,

126
00:08:52,492 --> 00:08:56,006
we try to grind in insights from the model threshold and

127
00:08:56,028 --> 00:09:00,186
in the step five, we have basically

128
00:09:00,288 --> 00:09:03,898
data visualization and transforming the results into

129
00:09:03,984 --> 00:09:07,466
visual graphs in

130
00:09:07,488 --> 00:09:11,374
a more detail way. In this diagram we can see a

131
00:09:11,412 --> 00:09:15,520
specific task for each stage. For example,

132
00:09:16,370 --> 00:09:20,490
in the fixed step that is related with defining

133
00:09:20,650 --> 00:09:24,066
the project objectives, the fixed step of the lifecycle is

134
00:09:24,088 --> 00:09:28,082
to define these objectives. In the second step, we try

135
00:09:28,136 --> 00:09:32,034
to acquire and explore data when we try to

136
00:09:32,072 --> 00:09:35,362
collect and prepare all of the relevant data for

137
00:09:35,416 --> 00:09:39,414
use in material learning algorithms. In the

138
00:09:39,452 --> 00:09:42,774
third step, we try to

139
00:09:42,892 --> 00:09:46,722
build our model. In order to gain insights

140
00:09:46,786 --> 00:09:50,486
from your data with machine learning, you must determine your target

141
00:09:50,598 --> 00:09:54,586
variable, which is the factor on which you wish to

142
00:09:54,688 --> 00:09:58,698
win deeper understanding. In the four step,

143
00:09:58,864 --> 00:10:02,314
we try to interpret and communicate

144
00:10:02,442 --> 00:10:05,120
the results of the model.

145
00:10:05,810 --> 00:10:09,102
Basically, the more interpretable your model

146
00:10:09,236 --> 00:10:12,614
is, the easier it will be to meet regulatory

147
00:10:12,682 --> 00:10:16,498
requirements and communicate this value to

148
00:10:16,584 --> 00:10:20,478
management and other case stakeholders.

149
00:10:20,654 --> 00:10:21,700
And finally,

150
00:10:25,050 --> 00:10:28,406
the final step is to implement, document and maintain the

151
00:10:28,428 --> 00:10:31,606
data science project so that the project can continue to

152
00:10:31,628 --> 00:10:34,790
leverage and improve upon its models.

153
00:10:36,730 --> 00:10:40,486
We are going to commend the main libraries,

154
00:10:40,518 --> 00:10:45,126
the main models we have in python. For this task,

155
00:10:45,318 --> 00:10:48,426
we start with secular. Secular is an open source tool for

156
00:10:48,448 --> 00:10:51,694
data mining and data analysis and provides a

157
00:10:51,732 --> 00:10:55,706
consistent and easy to use API for doing tasks

158
00:10:55,818 --> 00:10:59,710
related with preprocessing, training and predicting

159
00:11:00,050 --> 00:11:03,406
data. Psychilearn mesh filters include

160
00:11:03,438 --> 00:11:07,774
classification, regression, clustering, dimensionality reduction,

161
00:11:07,902 --> 00:11:10,210
model selection and preprocessing.

162
00:11:11,510 --> 00:11:15,194
It provides a range of supervised and unsupervised

163
00:11:15,262 --> 00:11:19,190
learning algorithms via consistent interface and

164
00:11:19,260 --> 00:11:23,430
delivery. Provides a lot of algorithms for

165
00:11:23,580 --> 00:11:29,674
classification, redression clustering like

166
00:11:29,712 --> 00:11:32,954
for example for clustering it's very

167
00:11:32,992 --> 00:11:36,474
useful the Cummins algorithm and the

168
00:11:36,512 --> 00:11:41,534
scan and also is

169
00:11:41,572 --> 00:11:46,042
designed to work with the Python numerical scientific

170
00:11:46,106 --> 00:11:49,070
libraries like Numpy and scipy.

171
00:11:50,530 --> 00:11:54,426
Scikilear is a grid library to master for machine learning beginners

172
00:11:54,458 --> 00:11:59,070
and professionals. Whoever have an experienced machine learning practitioners

173
00:11:59,230 --> 00:12:02,702
may not be aware of all the hiding hems

174
00:12:02,766 --> 00:12:07,490
of this package which can aid in their tasks significantly.

175
00:12:07,990 --> 00:12:11,800
I am going to commend the main features that we can

176
00:12:14,090 --> 00:12:17,394
the most relevant futures that we can find in this

177
00:12:17,452 --> 00:12:22,234
libraries. For example, pipelines are

178
00:12:22,272 --> 00:12:26,442
very useful to

179
00:12:26,496 --> 00:12:31,070
chain, for example multiple estimators. If we have multiple estimators

180
00:12:34,130 --> 00:12:38,414
in our pipeline, we can use this

181
00:12:38,532 --> 00:12:42,090
future to change these estimators.

182
00:12:42,250 --> 00:12:45,780
This is useful for

183
00:12:48,790 --> 00:12:52,686
when we need to fix a sequence of states in processing

184
00:12:52,718 --> 00:12:56,514
the data. For example, we have a feature selection,

185
00:12:56,562 --> 00:13:00,134
we have normalization classification. At this point,

186
00:13:00,172 --> 00:13:04,514
utility funtium make payline pipeline is a shorthand

187
00:13:04,562 --> 00:13:08,490
for construing pipelines. It takes a variable number

188
00:13:08,560 --> 00:13:12,518
of estimators and produce a pipeline with the steps

189
00:13:12,614 --> 00:13:16,138
that follow it. The use of pipelines to

190
00:13:16,224 --> 00:13:20,230
split, train, test and select the models and epiparameters

191
00:13:20,390 --> 00:13:23,818
made it so much easier to keep track of the outputs

192
00:13:23,914 --> 00:13:27,790
at all stage as well as reporting why you choose specific

193
00:13:27,940 --> 00:13:29,230
epiper parameters.

194
00:13:30,850 --> 00:13:35,438
Eper parameters basically are parameters that are not delivery with

195
00:13:35,524 --> 00:13:39,922
estimators, and inside they

196
00:13:39,976 --> 00:13:43,134
are passed as an argument to the constructor

197
00:13:43,182 --> 00:13:48,502
of the estimator classes. At this point, it's possible to

198
00:13:48,556 --> 00:13:51,862
search the impaired parameter space for the best

199
00:13:51,916 --> 00:13:56,386
cross valuation score. An eparameter provided

200
00:13:56,418 --> 00:14:00,598
when construing an estimator may be optimized using

201
00:14:00,684 --> 00:14:04,314
get params method. Specifically, we can

202
00:14:04,352 --> 00:14:07,846
use this method to find the names and current values

203
00:14:07,878 --> 00:14:11,518
for all parameters for a

204
00:14:11,524 --> 00:14:15,550
given estimator. Every estimator

205
00:14:15,970 --> 00:14:20,298
has its advantages and drawbacks. Its generalization

206
00:14:20,394 --> 00:14:23,742
error can be discomposed in terms of bias,

207
00:14:23,806 --> 00:14:27,326
variance and noise. The bias of an estimator

208
00:14:27,438 --> 00:14:32,078
is its average error for different training sets,

209
00:14:32,254 --> 00:14:35,762
and the variance of an estimator indicates

210
00:14:35,826 --> 00:14:39,558
how sensitive it is to vary in training

211
00:14:39,644 --> 00:14:43,510
sets. At this point, it could be helpful

212
00:14:43,850 --> 00:14:47,838
to plot the influence of a single epiper parameter

213
00:14:47,954 --> 00:14:51,610
on the training score and evaluation score

214
00:14:52,750 --> 00:14:57,098
to find out whether the estimator is overfitting or

215
00:14:57,184 --> 00:15:00,730
underfitting for some parameter values.

216
00:15:02,850 --> 00:15:08,538
At this point, the fonteon

217
00:15:08,554 --> 00:15:11,774
validation curve can help us in this case

218
00:15:11,812 --> 00:15:15,586
and return and validate scores. If the training

219
00:15:15,688 --> 00:15:18,786
score and the validation score are both

220
00:15:18,888 --> 00:15:22,126
low, the estimator will be underfitting.

221
00:15:22,318 --> 00:15:25,746
If the training score is high and the valuation score

222
00:15:25,778 --> 00:15:29,430
is low, the estimator is of overfitting and otherwise

223
00:15:30,250 --> 00:15:34,760
we suppose that estimator is working well.

224
00:15:36,330 --> 00:15:40,282
Another interesting feature is one hotel encoding that is a very common data

225
00:15:40,336 --> 00:15:44,134
processing tax to transfer input

226
00:15:44,182 --> 00:15:48,134
categorical features in one binary

227
00:15:48,182 --> 00:15:51,558
encodings for using in classification or prediction tax.

228
00:15:51,664 --> 00:15:55,562
For example, let us assume that we have two categorical

229
00:15:55,626 --> 00:15:58,846
values and in this table we can

230
00:15:58,868 --> 00:16:02,502
see that we have one column

231
00:16:02,586 --> 00:16:06,482
with js and no values and this column it

232
00:16:06,536 --> 00:16:09,662
transferred into new columns,

233
00:16:09,726 --> 00:16:13,202
one for each category. For example, with the JS value

234
00:16:13,336 --> 00:16:16,822
we have values one and

235
00:16:16,876 --> 00:16:20,578
zero, and for the no values

236
00:16:20,594 --> 00:16:24,294
we have zero and one. In these

237
00:16:24,412 --> 00:16:28,394
two new columns created and

238
00:16:28,432 --> 00:16:31,706
in IC way we

239
00:16:31,728 --> 00:16:35,994
can use the one hot encoder for

240
00:16:36,032 --> 00:16:38,250
applying these distress formations.

241
00:16:40,510 --> 00:16:44,018
Cycular also includes random sapling

242
00:16:44,054 --> 00:16:47,502
generators that can be used to boil artificial data

243
00:16:47,556 --> 00:16:51,754
sets of control size and complexity. It has functions

244
00:16:51,802 --> 00:16:56,094
for classification, clustering, regression, matrix decomposition,

245
00:16:56,142 --> 00:16:57,970
and manifold tense testing.

246
00:16:59,190 --> 00:17:02,914
Other techniques that can be useful when we have a large

247
00:17:02,952 --> 00:17:06,850
data set and we need to reduce

248
00:17:07,590 --> 00:17:13,846
the dimensionality of the data is in

249
00:17:13,868 --> 00:17:17,670
these cases, we can apply the principal component analysis PCA.

250
00:17:19,070 --> 00:17:23,030
Basically, PC functions by finding the directions

251
00:17:23,110 --> 00:17:27,100
of maximum variance in the data and provides the data

252
00:17:28,110 --> 00:17:29,770
in those directions.

253
00:17:31,410 --> 00:17:35,326
The amount of variance explained by each direction is called the

254
00:17:35,348 --> 00:17:38,974
splain variance. Explained variance can be used to choose the

255
00:17:39,012 --> 00:17:43,022
number of dimensions to kept in a reduced data

256
00:17:43,076 --> 00:17:46,686
set. It can also be used to assess the quality of a

257
00:17:46,708 --> 00:17:50,034
mature learning model. In general, a model with high

258
00:17:50,152 --> 00:17:53,540
splain variance will have good predictive power,

259
00:17:53,910 --> 00:17:57,746
while a model with low explained variance

260
00:17:57,938 --> 00:18:00,150
may not be as accurate.

261
00:18:01,770 --> 00:18:05,986
In this diagram we can see that we have two independent principal components,

262
00:18:06,018 --> 00:18:09,594
PC one and PC two. The PC one represents the

263
00:18:09,632 --> 00:18:13,670
vector which explain most of the information variance and PCE

264
00:18:13,750 --> 00:18:16,220
two represents the lesson information.

265
00:18:18,430 --> 00:18:21,982
In this example, we are following the classical machine learning

266
00:18:22,036 --> 00:18:25,982
pipeline where we will first import libraries and data set,

267
00:18:26,036 --> 00:18:29,610
perform exploratory data analysis

268
00:18:29,690 --> 00:18:34,094
and preprocessing, and finally train our models predictions

269
00:18:34,222 --> 00:18:37,586
and evaluate accuracy. At this point we

270
00:18:37,608 --> 00:18:41,214
can use PCA to find optimal

271
00:18:41,342 --> 00:18:44,670
number of features before we train our models.

272
00:18:44,830 --> 00:18:49,330
Performing PCA is as easy as following

273
00:18:50,410 --> 00:18:54,450
these two tips. Process first, we initialize

274
00:18:54,530 --> 00:18:58,758
the PCA class by passing the number of components to the constructor

275
00:18:58,934 --> 00:19:02,502
and in the second step we call the fit and transform

276
00:19:02,566 --> 00:19:05,990
methods. By passing

277
00:19:06,070 --> 00:19:09,782
the future set to these methods and the transform

278
00:19:09,926 --> 00:19:14,010
method returns the specified number of principal

279
00:19:14,170 --> 00:19:17,982
components that we have in this data

280
00:19:18,036 --> 00:19:21,614
set. Another interesting library we can find in

281
00:19:21,652 --> 00:19:25,200
Python for task related with the

282
00:19:27,110 --> 00:19:31,902
obtaining statistical

283
00:19:31,966 --> 00:19:35,522
data for data exploration is

284
00:19:35,576 --> 00:19:39,510
ESTAS model. EstAs model is another grid library which focus on

285
00:19:39,580 --> 00:19:43,414
statistical models and can

286
00:19:43,452 --> 00:19:47,282
be used for predictive and exploratory analysis.

287
00:19:47,426 --> 00:19:51,270
If you want for example to fit linear models,

288
00:19:51,430 --> 00:19:54,762
do statistical analysis, maybe a bit

289
00:19:54,816 --> 00:19:58,154
of pre modeling, then start models is

290
00:19:58,192 --> 00:19:58,780
great.

291
00:20:03,950 --> 00:20:08,250
We continue with commenting libraries

292
00:20:08,330 --> 00:20:12,666
we have in Python for deep learning. We start with tensorflow,

293
00:20:12,698 --> 00:20:16,050
that is an open source library that is based on a

294
00:20:16,200 --> 00:20:20,020
neural network system. This means that it can

295
00:20:20,390 --> 00:20:23,746
relate several network data simultaneously in

296
00:20:23,768 --> 00:20:26,950
the same way that the human brain does.

297
00:20:27,100 --> 00:20:30,706
For example, it can recognize several words of the Alphabet

298
00:20:30,738 --> 00:20:34,002
because it relates letters and phonemes.

299
00:20:34,146 --> 00:20:38,282
Another case is that of image and text that

300
00:20:38,336 --> 00:20:41,654
can be related to each other thanks to the association

301
00:20:41,702 --> 00:20:44,460
capacity of the neural network system.

302
00:20:46,270 --> 00:20:49,900
Internally, what is used in terms of law is

303
00:20:50,370 --> 00:20:54,350
use the tensors for building the neural network. A tensor basically

304
00:20:54,500 --> 00:20:58,510
is a mathematical object represent as a rise

305
00:20:59,490 --> 00:21:03,358
of higher dimensions and this rise

306
00:21:03,374 --> 00:21:06,894
of data with different sizes

307
00:21:06,942 --> 00:21:10,926
and runs get fit as input to the neural

308
00:21:10,958 --> 00:21:14,958
network. Tensorflow has become an intermachent

309
00:21:14,974 --> 00:21:18,542
learning ecosystem for all kinds of artificial

310
00:21:18,606 --> 00:21:22,534
intelligence technology. For example, here are the features the community has add

311
00:21:22,572 --> 00:21:26,246
to the original tensorflow package. For example, we have the

312
00:21:26,268 --> 00:21:29,494
Tensorflow a little for working with a smartphone

313
00:21:29,622 --> 00:21:32,330
operating system and IoT devices.

314
00:21:33,870 --> 00:21:39,066
Since tertial flow 2.0

315
00:21:39,168 --> 00:21:43,006
version, keras has been

316
00:21:43,028 --> 00:21:46,378
adopted as the main API to interact with Tensorflow.

317
00:21:46,554 --> 00:21:50,874
Keras the main difference is that Tensorflow

318
00:21:50,922 --> 00:21:54,722
was at low level and keras it was a high

319
00:21:54,776 --> 00:21:59,714
level for

320
00:21:59,752 --> 00:22:03,106
building neural networks and interact with

321
00:22:03,128 --> 00:22:06,466
the Tensorflow with the API that Tensorflow

322
00:22:06,498 --> 00:22:07,350
provides.

323
00:22:09,850 --> 00:22:13,206
This is maybe the best choice for any beginner in

324
00:22:13,228 --> 00:22:16,642
mature learning. It offers an easy way to express

325
00:22:16,706 --> 00:22:19,930
neural networks compared to other libraries.

326
00:22:21,550 --> 00:22:25,914
Basically, it provides an interface for

327
00:22:25,952 --> 00:22:29,340
interacting with tensorflow in an easy way.

328
00:22:30,610 --> 00:22:34,286
In this code, we can see how a

329
00:22:34,308 --> 00:22:38,142
little code for Keras Keras has I

330
00:22:38,196 --> 00:22:42,362
comment before is the real model for rapid

331
00:22:42,506 --> 00:22:46,514
experimentation and the most common way

332
00:22:46,552 --> 00:22:49,806
to define your model is by building a graph

333
00:22:49,838 --> 00:22:53,422
of layers which correspond to the mental model we normally

334
00:22:53,486 --> 00:22:57,362
use when we think about leap learning. The simplest

335
00:22:57,506 --> 00:23:00,774
type of model is a stack of layers and

336
00:23:00,812 --> 00:23:04,278
you can define such a model using the

337
00:23:04,364 --> 00:23:07,814
sequential API like we can

338
00:23:07,852 --> 00:23:11,814
see in this code. The advantages of this process

339
00:23:11,932 --> 00:23:16,854
are that it's easy to visualize the

340
00:23:16,892 --> 00:23:20,720
book and building

341
00:23:23,250 --> 00:23:30,974
a deep learning model using the

342
00:23:31,012 --> 00:23:34,914
different methods and

343
00:23:34,952 --> 00:23:39,330
different classes that provides the grass API.

344
00:23:40,470 --> 00:23:44,094
Another interesting library

345
00:23:44,142 --> 00:23:48,150
is Pytorch. Pytorch is similar to Tensorflow

346
00:23:51,850 --> 00:23:55,910
and we can use, for example, Tensorflow Pytorch

347
00:23:57,930 --> 00:24:01,020
in all the stages of a material learning project.

348
00:24:05,230 --> 00:24:09,260
We can use Pytorch for getting

349
00:24:09,710 --> 00:24:13,566
the data ready, building or pick up a

350
00:24:13,588 --> 00:24:17,546
training model, fit the model to the data and make a prediction,

351
00:24:17,658 --> 00:24:21,614
evaluate the model, improve, throw experimentation and

352
00:24:21,652 --> 00:24:25,330
save and reload your training model. All these stages

353
00:24:30,390 --> 00:24:35,634
can be executed with Pytorch if

354
00:24:35,672 --> 00:24:40,166
we compare the three libraries. Has I commented we

355
00:24:40,188 --> 00:24:43,126
can see that Keras works at high level,

356
00:24:43,228 --> 00:24:47,586
normally in conjunction with TensorFlow and

357
00:24:47,708 --> 00:24:51,306
Pytorch however works at low level.

358
00:24:51,488 --> 00:24:55,270
At architecture level, Pytorch and Tensorflow are more complex

359
00:24:55,350 --> 00:24:58,714
to use and keras is more

360
00:24:58,752 --> 00:25:00,170
simpler and readable.

361
00:25:01,070 --> 00:25:04,670
Regarding the speed, keras offers low

362
00:25:04,820 --> 00:25:08,320
performance comparing with the others and

363
00:25:08,690 --> 00:25:12,326
Tesoflow and Pytos offers fast and high end performance.

364
00:25:12,458 --> 00:25:16,350
And regarding training models, the three libraries

365
00:25:16,430 --> 00:25:20,994
provides offers this feature and

366
00:25:21,032 --> 00:25:25,186
finally commenting the Theano that

367
00:25:25,208 --> 00:25:27,746
is a Python libraries that allows you to define,

368
00:25:27,858 --> 00:25:31,202
optimize and evaluate mathematical expressions involving

369
00:25:31,266 --> 00:25:33,830
multi dimensional arise efficiently.

370
00:25:34,570 --> 00:25:38,262
Thanos main filters include tight integration

371
00:25:38,326 --> 00:25:42,278
with numpy transparent use of gpus,

372
00:25:42,454 --> 00:25:45,030
efficient symbolic differentiation,

373
00:25:45,190 --> 00:25:49,290
speed and stability optimizations, dynamic C code generation,

374
00:25:51,550 --> 00:25:54,750
and extensive unit testing and self verification.

375
00:25:55,090 --> 00:25:58,906
It provides many tools to define, optimize and evaluate mathematical

376
00:25:58,938 --> 00:26:02,478
expressions and numerous other libraries can be built

377
00:26:02,564 --> 00:26:06,690
upon Theano that explore its data structures.

378
00:26:10,470 --> 00:26:13,918
Theano is one of the most mature of machine

379
00:26:13,934 --> 00:26:17,270
learning libraries since it provides nice data

380
00:26:17,340 --> 00:26:21,286
structures like tensors, like the

381
00:26:21,308 --> 00:26:25,410
structure that we have in terms of flow to represent lies

382
00:26:25,490 --> 00:26:28,650
of neural networks, and they are efficient in terms

383
00:26:28,720 --> 00:26:32,598
of linear algebra. Similar to numpy arise.

384
00:26:32,774 --> 00:26:36,406
There are a lot of libraries which will on top of Thanos

385
00:26:36,438 --> 00:26:39,890
exploiting its data structure,

386
00:26:40,070 --> 00:26:43,514
and as I commented before, it has support for GPU

387
00:26:43,562 --> 00:26:46,080
programming out of the wall of as well.

388
00:26:48,290 --> 00:26:52,414
And that's all. Thank you very much. Thank you for

389
00:26:52,452 --> 00:26:53,790
doing this presentation.

390
00:26:56,210 --> 00:26:59,678
In this slide we can see my

391
00:26:59,764 --> 00:27:03,214
contact if you want to

392
00:27:03,252 --> 00:27:07,094
contact me, in social networks like

393
00:27:07,132 --> 00:27:10,950
Twitter and LinkedIn, and if you have any question

394
00:27:11,020 --> 00:27:19,654
or have any doubt, you can use this channel for

395
00:27:19,692 --> 00:27:22,100
resolving your questions. Thank you very much.

