1
00:00:20,170 --> 00:00:24,282
Hello and welcome to using the flipn pattern for edge AI.

2
00:00:24,426 --> 00:00:27,714
That's when you use Flink, Nifi and Pulsar

3
00:00:27,762 --> 00:00:32,200
together to solve really complex problems.

4
00:00:33,290 --> 00:00:36,502
David and I are using to go through a couple of

5
00:00:36,556 --> 00:00:40,506
slides, little demo, just to give you a feel for how

6
00:00:40,528 --> 00:00:44,650
you would develop IoT edge applications

7
00:00:44,990 --> 00:00:49,158
using this pattern and these open source technologies

8
00:00:49,334 --> 00:00:53,102
that work together really well and scale out

9
00:00:53,156 --> 00:00:56,026
tremendously. I'm Tim span.

10
00:00:56,218 --> 00:00:59,582
I'm a principal developer advocate working

11
00:00:59,636 --> 00:01:03,394
with all the streaming stack. I run some meetups, do some

12
00:01:03,432 --> 00:01:07,154
events, do some blogging, and we'll go

13
00:01:07,192 --> 00:01:10,734
into some cool tech. And I'm David Kermgard,

14
00:01:10,782 --> 00:01:13,330
I'm a developer advocate at Stream native.

15
00:01:14,070 --> 00:01:17,566
I publish some books there, Pulsar in action. I also contribute

16
00:01:17,598 --> 00:01:20,166
a lot of code and give a lot of talks around the world. And I

17
00:01:20,188 --> 00:01:23,042
focus primarily on Ni five, Flink, and Pulsar.

18
00:01:23,106 --> 00:01:25,640
So thanks Tim, for having me.

19
00:01:26,010 --> 00:01:29,514
Yeah, it's the dream team. If you're interested in these

20
00:01:29,552 --> 00:01:33,258
three technologies, these are probably the two people

21
00:01:33,344 --> 00:01:36,906
who like them the most. Every week I put out a

22
00:01:36,928 --> 00:01:40,778
newsletter, covers all kinds of streaming stuff.

23
00:01:40,864 --> 00:01:44,846
All the fun tech out there, very easy to check all

24
00:01:44,868 --> 00:01:48,494
the time. We do meetups all over the place, just look for

25
00:01:48,532 --> 00:01:52,254
them and they cover all the

26
00:01:52,292 --> 00:01:57,230
technology that you like. Today we're going to cover introducing

27
00:01:57,310 --> 00:02:00,210
to these technologies a little overview,

28
00:02:01,030 --> 00:02:04,210
some examples, and then a little bit about each

29
00:02:04,280 --> 00:02:08,210
of the streaming tech there. That's important for this pattern

30
00:02:08,370 --> 00:02:11,554
pulsar to be able to get anything in the stream.

31
00:02:11,682 --> 00:02:15,394
Flink to do some processing and joining Nifi

32
00:02:15,442 --> 00:02:19,334
to scoop up the data and shove it into pulsar

33
00:02:19,382 --> 00:02:22,506
and get it working. And that's really the

34
00:02:22,528 --> 00:02:26,650
flipn stack or flipn pattern or

35
00:02:26,720 --> 00:02:30,666
flipping federation. I've been talking with some

36
00:02:30,688 --> 00:02:34,154
of our friends, you know who you are, Peter, online about

37
00:02:34,192 --> 00:02:38,042
should you use the word stack going back all the way to the lamp stack?

38
00:02:38,106 --> 00:02:41,678
Probably not, but it's a bunch of things together.

39
00:02:41,844 --> 00:02:44,290
Pattern module,

40
00:02:46,950 --> 00:02:50,542
just a guide, some examples, best practices,

41
00:02:50,686 --> 00:02:54,386
things that work together really well and make things easy for you to

42
00:02:54,408 --> 00:02:58,454
build. Streaming apps, everything's Apache. They all work

43
00:02:58,492 --> 00:03:01,926
together really nice. David wrote the connector from Nifi to

44
00:03:01,948 --> 00:03:05,474
Pulsar. There's a really solid connector

45
00:03:05,522 --> 00:03:08,978
from Flink to Pulsar and also things like Pinot

46
00:03:08,994 --> 00:03:13,366
to pulsar and a lot of other projects. Once you get into Apache,

47
00:03:13,558 --> 00:03:16,346
things just really fit together really well.

48
00:03:16,528 --> 00:03:20,310
Again, that is the reason why we like to use that together

49
00:03:20,480 --> 00:03:24,522
with this pattern. It could really accelerate

50
00:03:24,666 --> 00:03:28,174
what you can do as an edge data engineer. Whether this

51
00:03:28,212 --> 00:03:31,614
is just grabbing that raw sensor data,

52
00:03:31,732 --> 00:03:34,878
log data and get that into a stream.

53
00:03:34,974 --> 00:03:39,090
Or it's even more advanced where you've got different

54
00:03:39,160 --> 00:03:42,526
models running at the edge, cameras connected

55
00:03:42,558 --> 00:03:46,994
to gpus, different accelerators

56
00:03:47,042 --> 00:03:50,966
there, and you'll see there's a lot of different people involved in

57
00:03:50,988 --> 00:03:54,774
these projects, lots of framework language options

58
00:03:54,972 --> 00:03:58,746
connecting to different clouds. Most of you out

59
00:03:58,768 --> 00:04:02,602
there probably some kind of engineer. To be a cloud

60
00:04:02,656 --> 00:04:06,380
data engineer involved in these edge things, you're going to need

61
00:04:07,710 --> 00:04:11,342
python comes up a Iot, or Java. Both of those work really

62
00:04:11,396 --> 00:04:15,658
well within this pattern. And you should know little SQL.

63
00:04:15,834 --> 00:04:19,310
My cat is always messing with the camera.

64
00:04:19,650 --> 00:04:22,906
He likes the tools that we use. He thinks I'm spending

65
00:04:22,938 --> 00:04:25,906
too much money on all these devices, but what can you do?

66
00:04:26,088 --> 00:04:30,190
AI, obviously we can run that within Nifi, within pulsar

67
00:04:30,270 --> 00:04:33,522
functions, flink at the edge in these different

68
00:04:33,576 --> 00:04:36,934
agents, and at some point it's going to run this whole

69
00:04:36,972 --> 00:04:40,434
talk for me and fix any typos. So hopefully

70
00:04:40,482 --> 00:04:43,666
we'll see that happen. A lot of different projects.

71
00:04:43,698 --> 00:04:47,254
If I listed all the Apache projects, we could probably spend the

72
00:04:47,292 --> 00:04:50,774
whole 30 minutes. There's so many beneath the covers,

73
00:04:50,822 --> 00:04:54,646
so many that work together. Calcites used everywhere.

74
00:04:54,838 --> 00:04:58,282
You Iot bookkeeper, you got zookeeper, you got

75
00:04:58,336 --> 00:05:02,074
different registries, Tika, whole bunch of different projects.

76
00:05:02,122 --> 00:05:05,566
OpenNLP, I've been playing with some more. Some good things in

77
00:05:05,588 --> 00:05:08,974
there. Iceberg, all them work together.

78
00:05:09,172 --> 00:05:13,486
But the main things we need to build these streaming

79
00:05:13,678 --> 00:05:17,026
applications that connect from the edge all

80
00:05:17,048 --> 00:05:20,526
the way to the cloud or wherever your enterprise is centered,

81
00:05:20,558 --> 00:05:24,686
even if it's federated between multiple availability zones

82
00:05:24,878 --> 00:05:28,454
on premise, wherever this is running, those all

83
00:05:28,492 --> 00:05:32,626
connect together. You're probably not running Flink on the edge.

84
00:05:32,818 --> 00:05:36,326
You certainly could, especially on one of these Jetson boxes or

85
00:05:36,348 --> 00:05:39,690
even one of the beefier ones. But Flink is usually

86
00:05:39,760 --> 00:05:43,434
for downstream. Again, IoT minify, probably running

87
00:05:43,472 --> 00:05:47,094
at the edge. Maybe pulsar or pulsar

88
00:05:47,142 --> 00:05:51,466
might be up a level. How you decide where

89
00:05:51,568 --> 00:05:54,942
you put those connections is a talk

90
00:05:54,996 --> 00:05:57,678
for later. And David has a couple of those out there if you want to

91
00:05:57,684 --> 00:06:01,566
take a look. But Flink is really nice for taking

92
00:06:01,668 --> 00:06:05,630
billions of data points, streaming them together, and run

93
00:06:05,700 --> 00:06:09,566
real time things on them. You can also support batch

94
00:06:09,758 --> 00:06:13,650
if you need to do some batch things. Sometimes people will join gets

95
00:06:13,720 --> 00:06:17,670
things out of a batch source like say

96
00:06:17,740 --> 00:06:21,206
kudu or a relational database. You could

97
00:06:21,228 --> 00:06:24,806
use that to augment what you're doing within Flink. And I'll show you

98
00:06:24,828 --> 00:06:28,466
a little bit of Flink SQl. And what's

99
00:06:28,498 --> 00:06:31,766
nice is scales as big as you need to go scales

100
00:06:31,798 --> 00:06:35,494
out really solid. That's the part of the second part of Flink

101
00:06:35,542 --> 00:06:39,466
I like. First part is they all work together well.

102
00:06:39,648 --> 00:06:43,694
Then you've got this scale out and then third is

103
00:06:43,812 --> 00:06:47,374
with the SQL. It is easy to start. Even if this is

104
00:06:47,412 --> 00:06:51,386
all running in one docker where I've got Flink,

105
00:06:51,498 --> 00:06:54,750
Nifi and Pulsar in one docker on a laptop.

106
00:06:54,910 --> 00:06:58,642
This still can handle all the data engineering I need

107
00:06:58,696 --> 00:07:02,562
from a bunch of devices into final use

108
00:07:02,616 --> 00:07:06,086
case of having real time analytics. Important piece of

109
00:07:06,108 --> 00:07:09,894
the puzzle there. And like I said with that

110
00:07:09,932 --> 00:07:13,746
Flink connector makes it very easy for me to stream

111
00:07:13,778 --> 00:07:17,538
the data in from pulsar regardless of the mode

112
00:07:17,554 --> 00:07:21,846
it's in. Could be in generic pulsar mode, could be in Kafka

113
00:07:21,878 --> 00:07:25,462
mode, and then stream it down, stream into a sync

114
00:07:25,526 --> 00:07:29,002
when you're done. So do my analytics, which could be as easy

115
00:07:29,056 --> 00:07:32,430
as possible in SQL, and then insert the results later,

116
00:07:32,500 --> 00:07:36,154
which could be aggregates, which could be summaries,

117
00:07:36,202 --> 00:07:39,514
could be windows of data, and do this at whatever scale

118
00:07:39,562 --> 00:07:43,278
you want with simple SQL. I'll show you a little bit of

119
00:07:43,284 --> 00:07:46,720
that running in our short talk here.

120
00:07:47,250 --> 00:07:51,474
Then next up, I'll let the expert go into pulsar for you.

121
00:07:51,672 --> 00:07:55,266
Thanks, Tim. So yeah, Apache Pulsar sort of fits nicely in

122
00:07:55,288 --> 00:07:58,958
that stack. As Tim mentioned, Nifi is the edge device or

123
00:07:58,984 --> 00:08:02,214
the edge technology that brings in all the information. But you have

124
00:08:02,252 --> 00:08:06,246
a nice place to buffer this information before it gets processed by a flink sort

125
00:08:06,268 --> 00:08:09,554
of processing engine. And pulsar serves that exact purpose.

126
00:08:09,602 --> 00:08:13,274
It can scale up to just one particular use case. Ten petabytes of data per

127
00:08:13,312 --> 00:08:16,666
day coming in. So all the data you could ever need coming in in a

128
00:08:16,688 --> 00:08:20,026
truly elastically scalable platform that

129
00:08:20,048 --> 00:08:23,982
integrates nicely with all the open source tools, as Tim mentioned. Spark, flink, everything else,

130
00:08:24,036 --> 00:08:27,146
Nifi. So it's a nice, think of it as an infinite buffer,

131
00:08:27,178 --> 00:08:30,766
an infinite state streaming storage that has multiple layers of

132
00:08:30,788 --> 00:08:34,250
different protocols on top. So it speaks its own native

133
00:08:34,330 --> 00:08:38,494
pulsar protocol. It also speaks Kafka, it also speaks MQTT,

134
00:08:38,622 --> 00:08:42,114
it also speaks RabbitMQ. So it's a great way to bring

135
00:08:42,152 --> 00:08:45,166
data in from different sources and put it in a single place and then expose

136
00:08:45,198 --> 00:08:48,920
it up to something like flink for your processing as well.

137
00:08:49,290 --> 00:08:53,078
Yeah, it's hard to underestimate something

138
00:08:53,164 --> 00:08:57,218
that can run more than a million topics,

139
00:08:57,394 --> 00:09:01,100
which if you're starting to do this, you'll see why that's a big deal.

140
00:09:01,870 --> 00:09:04,954
It's actually 10 million now, Tim, I've updated that 10 million.

141
00:09:05,072 --> 00:09:09,114
Yeah, we've done 10 million now. Yeah, it's great. So everybody,

142
00:09:09,232 --> 00:09:12,926
why Apache Pulsar? Everything Kafka can do but

143
00:09:12,948 --> 00:09:16,154
better and more is how I describe it. It came with unified messaging

144
00:09:16,202 --> 00:09:19,662
and streaming on day one. So we actually

145
00:09:19,716 --> 00:09:22,886
support queuing semantics, streaming semantics,

146
00:09:23,018 --> 00:09:25,166
infinite message retention, tiered storage,

147
00:09:25,198 --> 00:09:28,786
offloading things, capabilities that you still don't have in

148
00:09:28,808 --> 00:09:32,478
Kafka like dead letter queues and scheduling, scheduled message delivery

149
00:09:32,574 --> 00:09:35,838
and multi protocol support. Easily scalable,

150
00:09:35,934 --> 00:09:39,214
no data movement. So when you add, as you add up capacity

151
00:09:39,262 --> 00:09:42,854
to your cluster, you don't have to move the data rebalance free and 10

152
00:09:42,892 --> 00:09:46,786
million topics and soon do ten x that again once we get Oxia

153
00:09:46,898 --> 00:09:49,766
fully up and running, that's our zookeeper replacement. There's been a lot of buzz in

154
00:09:49,788 --> 00:09:53,450
the Kafka community around. We finally got rid of zookeeper and that's great.

155
00:09:53,600 --> 00:09:56,726
We've done a similar process, but we rearchitected it entirely

156
00:09:56,838 --> 00:10:00,618
to make it more scalable as well. And then georeplication was the

157
00:10:00,624 --> 00:10:03,834
first use case. It was built at Yahoo in 2012 to

158
00:10:03,872 --> 00:10:07,246
replicate data across multiple directions, a bi

159
00:10:07,268 --> 00:10:11,022
directional mesh, multitenancies built in on day one, and then

160
00:10:11,076 --> 00:10:14,654
encryption all the cool goodies you'd need for a full featured enterprise system.

161
00:10:14,692 --> 00:10:18,018
So it's not just a toy, but it's truly scalable. And you can scan that

162
00:10:18,024 --> 00:10:21,906
core, decode more and find out more information about why all the different capabilities of

163
00:10:21,928 --> 00:10:25,860
Pulsar. 100 million coming.

164
00:10:26,710 --> 00:10:29,540
I hope I don't have 100 million on my next project.

165
00:10:30,090 --> 00:10:33,400
I don't even know how many Kafka clusters you need.

166
00:10:34,810 --> 00:10:38,680
I wouldn't want to run that many. But it allows you to do things like

167
00:10:39,290 --> 00:10:42,922
a topic per device. So if you have large number of devices in IFU space,

168
00:10:42,976 --> 00:10:47,094
you want to have just their information commingle it. You're no longer limited

169
00:10:47,142 --> 00:10:50,058
because of the platform to decide how you structure your data. I think that's the

170
00:10:50,064 --> 00:10:53,822
big win there. Yeah, that's pretty cool. And not just

171
00:10:53,876 --> 00:10:57,454
one per device, maybe one per device sensor. Like even

172
00:10:57,492 --> 00:11:01,582
my little device I'm running for the demo here has two

173
00:11:01,636 --> 00:11:05,266
sensors on it, so maybe each one gets its own topic. And then

174
00:11:05,288 --> 00:11:07,650
I could join them together with flink.

175
00:11:07,990 --> 00:11:11,566
Yes, absolutely. I don't

176
00:11:11,598 --> 00:11:15,886
know if I want to write a sequel that has a 10 million topics

177
00:11:15,918 --> 00:11:19,046
in it and then join them all. I don't know.

178
00:11:19,148 --> 00:11:22,120
That might be a little much. That might be a bit much.

179
00:11:22,890 --> 00:11:26,854
There might be a nice way to view it up though. There are ways

180
00:11:26,892 --> 00:11:30,214
to aggregate. I mean, that's always the thing. If I put everything

181
00:11:30,252 --> 00:11:33,814
in one topic, then I don't have to join.

182
00:11:33,942 --> 00:11:37,258
But then there's a whole lot in that one topic, and that could be a

183
00:11:37,264 --> 00:11:40,666
bottleneck. If I have a million topics, like if

184
00:11:40,688 --> 00:11:43,946
I want to aggregate all those sensors, I could

185
00:11:43,968 --> 00:11:47,040
do it in I five. I don't know. There you go.

186
00:11:47,490 --> 00:11:50,558
That's when you got to balance it. Like, how many topics do I want?

187
00:11:50,644 --> 00:11:54,242
I don't know. No limitations, though.

188
00:11:54,296 --> 00:11:58,174
That's the key. You're not. Whatever makes sense for your architecture.

189
00:11:58,222 --> 00:12:01,986
You pick the best. And Flink plus, pulsar is so

190
00:12:02,008 --> 00:12:05,522
mature at this point, we're talking three or four years

191
00:12:05,576 --> 00:12:08,926
in, so it's pretty solid.

192
00:12:08,958 --> 00:12:12,562
And the flink versions and the Pulsar versions, always incrementing.

193
00:12:12,626 --> 00:12:16,086
Everything's getting better. So pretty nice way

194
00:12:16,108 --> 00:12:19,446
to connect there. And how do

195
00:12:19,468 --> 00:12:22,918
we get data in there now? Most commonly

196
00:12:23,094 --> 00:12:26,662
you could do it with any kind of code because there's support for Pulsar,

197
00:12:26,726 --> 00:12:30,314
support for a lot of different languages. So sometimes I'll just

198
00:12:30,352 --> 00:12:34,970
have a Python app at the edge, push right into pulsar,

199
00:12:35,130 --> 00:12:39,294
and that could be using the native pulsar library that

200
00:12:39,332 --> 00:12:42,880
installs on all these devices. And I've done IoT or

201
00:12:43,650 --> 00:12:47,070
some of these devices are hard coded to push out MqtT.

202
00:12:47,230 --> 00:12:50,498
We can just have pulsar listen that way. So you got some options.

203
00:12:50,584 --> 00:12:53,490
But often I'll have something like minify,

204
00:12:54,390 --> 00:12:58,274
which is a small Nifi agent, running on that device,

205
00:12:58,322 --> 00:13:01,334
just to make it easier for me to manage what's going on.

206
00:13:01,532 --> 00:13:05,430
But one of the reasons why I want Nifi or minify

207
00:13:05,770 --> 00:13:09,458
is besides it's easy to work with, and we'll show you that in the demo.

208
00:13:09,644 --> 00:13:13,446
It has some features that are really nice for picking

209
00:13:13,478 --> 00:13:16,490
up data. If you've ever used any kind of logging agent,

210
00:13:16,640 --> 00:13:20,714
they're usually pretty simple. Maybe you're setting up some YAML or

211
00:13:20,752 --> 00:13:23,470
XML or JSOn, some kind of configuration,

212
00:13:23,970 --> 00:13:27,418
but they are designed to run maybe just at the edge.

213
00:13:27,514 --> 00:13:31,214
You're not going to have a scalable cluster, not going to have something that

214
00:13:31,252 --> 00:13:34,766
guarantees delivery, has built in back pressure,

215
00:13:34,958 --> 00:13:39,598
prioritize queuing, allow you to change the qos

216
00:13:39,694 --> 00:13:43,282
on them. Built in data, providence. Hundreds of different

217
00:13:43,336 --> 00:13:46,680
controls, lots of different sources, version control,

218
00:13:47,530 --> 00:13:51,206
DevOps, all those things you might want with a

219
00:13:51,228 --> 00:13:54,790
scalable architecture. Maybe the last people still using

220
00:13:54,860 --> 00:13:58,634
zookeeper, though, depends what environment you're in.

221
00:13:58,672 --> 00:14:02,630
That could also be done otherwise in Kubernetes,

222
00:14:02,710 --> 00:14:06,806
but pretty straightforward. But it's

223
00:14:06,838 --> 00:14:10,618
just designed so you don't lose data. They keep adding new ones. I'll show you

224
00:14:10,624 --> 00:14:14,586
a little bit of NiFi 20 and the additional records

225
00:14:14,618 --> 00:14:18,560
we could read so I can read all these type of data.

226
00:14:18,930 --> 00:14:23,274
Convert that into a format that's easier to use within pulsar

227
00:14:23,322 --> 00:14:26,866
and flink like Avro, and then use that to

228
00:14:26,888 --> 00:14:30,494
join together data. Got a number of articles out there if you're

229
00:14:30,542 --> 00:14:33,714
interested in using the different data.

230
00:14:33,832 --> 00:14:37,646
This is the one from the demo today, which is the raspberry

231
00:14:37,678 --> 00:14:40,786
PI 400, which is cool because it's got the keyboard.

232
00:14:40,898 --> 00:14:44,566
I don't know why they didn't put a screen with it, but I added a

233
00:14:44,588 --> 00:14:47,602
very tiny screen that has my ip on it.

234
00:14:47,756 --> 00:14:51,226
You're not too valuable there, but data from

235
00:14:51,248 --> 00:14:55,830
the edge, we get that into pulsar

236
00:14:55,910 --> 00:14:59,978
so we can start doing things with IoT and a lot of different options there.

237
00:15:00,144 --> 00:15:04,374
Mine for this particular example is minify agent HTTP

238
00:15:04,422 --> 00:15:08,302
into niFi. Nifi does the cleanup and just gets that into

239
00:15:08,356 --> 00:15:11,518
a topic. Flink does the simplest possible,

240
00:15:11,604 --> 00:15:14,870
SQL gets it, and it can push it anywhere.

241
00:15:14,970 --> 00:15:18,562
I mean, it could go into another topic and then someone else

242
00:15:18,616 --> 00:15:22,018
can consume it. You've Iot a lot of options there,

243
00:15:22,184 --> 00:15:26,294
and I'll show you some inside the demo. Just wanted

244
00:15:26,332 --> 00:15:29,974
to show you different examples. And there's lots of different sources of data that

245
00:15:30,012 --> 00:15:33,686
Nifi, Flink Pulsar can read,

246
00:15:33,868 --> 00:15:37,366
not super hard. And then once we get it out

247
00:15:37,388 --> 00:15:40,682
there, very easy to distribute the data for people to write

248
00:15:40,736 --> 00:15:44,006
apps or whatever you want there. But that's

249
00:15:44,038 --> 00:15:46,986
all the slides. Let's see if we've got things running here.

250
00:15:47,088 --> 00:15:51,486
Hopefully things haven't timed out. Seem to be okay here.

251
00:15:51,668 --> 00:15:56,074
So this is Nifi. This is Nifi

252
00:15:56,202 --> 00:15:59,534
one two four, which is a newer version, but not

253
00:15:59,572 --> 00:16:04,034
the 20 ones. So I've got a

254
00:16:04,072 --> 00:16:08,222
controller here that is receiving Nifi

255
00:16:08,286 --> 00:16:13,250
calls. And Nifi is the ability to listen to rest

256
00:16:13,320 --> 00:16:16,786
endpoints on demand, and take any data you don't have to have it fixed

257
00:16:16,818 --> 00:16:20,520
to some schema or some specific class of data.

258
00:16:21,130 --> 00:16:24,726
Anybody who wants to call it, just post data on

259
00:16:24,748 --> 00:16:28,582
this port and I will consume it pretty

260
00:16:28,636 --> 00:16:32,666
easy. And then I have a provenance and all the data that's come in,

261
00:16:32,848 --> 00:16:36,538
and I know how long it was,

262
00:16:36,624 --> 00:16:40,438
what type of data, what the data was, what the

263
00:16:40,464 --> 00:16:43,326
user agent was, plus the data itself,

264
00:16:43,508 --> 00:16:47,322
which in this case is JSON, pretty straightforward.

265
00:16:47,386 --> 00:16:50,894
And then I could process it, route it, and in this

266
00:16:50,932 --> 00:16:54,574
case, I'm consuming it in here. I paused

267
00:16:54,622 --> 00:16:58,082
the live data. So we've got a bunch of data coming in.

268
00:16:58,136 --> 00:17:01,266
So it's not limited amount and

269
00:17:01,288 --> 00:17:04,610
then I'm using IoT to pulsar and elsewhere.

270
00:17:05,050 --> 00:17:09,074
But how did I get that data? Well, on that device,

271
00:17:09,202 --> 00:17:13,720
that raspberry PI keyboard, I have a minify agent

272
00:17:14,170 --> 00:17:17,818
and it's running a shell script to run some python to

273
00:17:17,824 --> 00:17:21,098
grab some sensor data. I set that agent

274
00:17:21,184 --> 00:17:24,762
that you saw there, and then if it's not

275
00:17:24,816 --> 00:17:29,210
empty, I'm calling Ni five via HTTP

276
00:17:29,950 --> 00:17:34,014
and just sending it into that port. So the data is just streaming in.

277
00:17:34,212 --> 00:17:37,546
If that agent is not running, I'll see what's

278
00:17:37,578 --> 00:17:41,102
going on. I could see all the metadata about that particular

279
00:17:41,236 --> 00:17:44,826
agent. I can also change Iot and debug

280
00:17:44,858 --> 00:17:48,146
it on the fly if I need to. I'll see all

281
00:17:48,168 --> 00:17:51,746
the alerts going on, sort by that. If I have

282
00:17:51,768 --> 00:17:55,694
a ton of different things going on at once, I could see that.

283
00:17:55,832 --> 00:17:59,320
I could see if something's offline and I can

284
00:18:00,090 --> 00:18:03,266
delete new things if need be. I could see if there's

285
00:18:03,298 --> 00:18:07,106
new things on that device. If someone's done an upgrade,

286
00:18:07,298 --> 00:18:10,554
lots of different things you could do. Pretty easy way to do that.

287
00:18:10,672 --> 00:18:14,810
So we got the data from Nifi streaming into pulsar,

288
00:18:15,150 --> 00:18:18,826
and this is an easy way to run flink behind the

289
00:18:18,848 --> 00:18:22,702
scenes, it's just regular apache flink and

290
00:18:22,756 --> 00:18:25,946
there's no jobs running yet. And I'll

291
00:18:25,978 --> 00:18:29,210
just start my query here in this UI,

292
00:18:29,370 --> 00:18:32,894
and as you can see, it deployed the job already. I only have

293
00:18:32,932 --> 00:18:36,538
one node because this is running on a laptop. If I had

294
00:18:36,564 --> 00:18:40,338
a massive cluster, you know it's going to look different. This UI is going

295
00:18:40,344 --> 00:18:44,174
to say there's more resources. You don't have to code differently,

296
00:18:44,222 --> 00:18:47,830
you don't have to write SQL differently. It's just going to take that data

297
00:18:47,900 --> 00:18:51,880
in from the table, filter it out wherever it makes sense.

298
00:18:52,330 --> 00:18:56,070
And here we're showing a sample of that data as it comes in.

299
00:18:56,220 --> 00:19:00,310
And what I do with that is I've got a little materialized

300
00:19:00,390 --> 00:19:04,234
view that takes a cache of that data and

301
00:19:04,272 --> 00:19:07,510
presents it as a rest endpoint.

302
00:19:07,670 --> 00:19:11,134
And then I could just put it in a dashboard. But I could have

303
00:19:11,172 --> 00:19:14,730
also written this dashboard directly

304
00:19:14,810 --> 00:19:18,586
against Pulsar using the pulsar's websocket

305
00:19:18,618 --> 00:19:21,470
API. Depends what you're doing, really.

306
00:19:21,540 --> 00:19:24,930
I should be joining this to another data source,

307
00:19:25,510 --> 00:19:28,834
and there's lots of different things I could join that

308
00:19:28,872 --> 00:19:32,802
to. Depending on what

309
00:19:32,856 --> 00:19:36,806
tables I have out there, like this one, I'd probably join this

310
00:19:36,828 --> 00:19:40,294
to multiple devices that are either the same or similar.

311
00:19:40,492 --> 00:19:43,926
Maybe looking at ones that have similar

312
00:19:44,028 --> 00:19:47,962
regions or maybe from the same area or have

313
00:19:48,016 --> 00:19:52,566
similar sensors, like here, I'm looking at carbon dioxide

314
00:19:52,758 --> 00:19:56,506
and volatile chemicals in this area.

315
00:19:56,688 --> 00:20:00,220
So I might use that to pinpoint things going on

316
00:20:01,470 --> 00:20:05,034
based on what's going on there and maybe join them based

317
00:20:05,072 --> 00:20:08,734
on lat long. I should probably add lat long based on where it is.

318
00:20:08,852 --> 00:20:12,522
I can add a gps sensor there or just manually

319
00:20:12,586 --> 00:20:16,494
do it if I know that sensor is not moving again. When you're

320
00:20:16,542 --> 00:20:20,020
setting up these things, you put them where it makes sense.

321
00:20:20,790 --> 00:20:24,500
And as you can see, more data shows up. This is because

322
00:20:24,970 --> 00:20:28,934
another record came off. The device got pushed to

323
00:20:28,972 --> 00:20:32,594
Nifi. Nifi did a little bit of cleanup

324
00:20:32,642 --> 00:20:36,390
on it, and then we pushed it into pulsar

325
00:20:36,730 --> 00:20:40,554
and then Flink got that event and

326
00:20:40,592 --> 00:20:44,262
it shows up here. And then it'll update this materialized

327
00:20:44,326 --> 00:20:47,642
view with any updates. And then we could push that

328
00:20:47,696 --> 00:20:51,770
to a dashboard Jupyter notebook,

329
00:20:51,850 --> 00:20:55,198
a regular application, or maybe another flink app,

330
00:20:55,284 --> 00:20:58,654
or maybe a spark app. I mean, you have a lot of

331
00:20:58,692 --> 00:21:02,218
options here. Or it can be pushed into

332
00:21:02,404 --> 00:21:06,254
data sync like Pino or Hive or kudu

333
00:21:06,302 --> 00:21:10,450
or hbase or mongo or relational database.

334
00:21:10,870 --> 00:21:14,162
Lots of options there. Depends on what makes

335
00:21:14,216 --> 00:21:17,494
sense for you. I want to show you another thing

336
00:21:17,532 --> 00:21:21,330
we have here, and this is the latest

337
00:21:21,410 --> 00:21:25,910
candidate release for Apache 9520

338
00:21:26,060 --> 00:21:29,690
that runs on JDK 21, which is super

339
00:21:29,760 --> 00:21:33,462
fast and has the ability to run Python.

340
00:21:33,606 --> 00:21:37,082
And there's a couple of python processors built in

341
00:21:37,216 --> 00:21:40,990
for doing some cool stuff like chunking documents,

342
00:21:41,330 --> 00:21:45,274
parsing them, interacting with chat

343
00:21:45,322 --> 00:21:48,446
GBT. You got to put your keys in

344
00:21:48,468 --> 00:21:54,190
there. Pretty fun pushing to different vector

345
00:21:54,270 --> 00:21:57,250
stores like Chroma and Pine cone.

346
00:21:57,750 --> 00:22:01,794
Also there's a couple of new processors I like, one that'll listen

347
00:22:01,832 --> 00:22:05,290
to slack. So if I post a message to slack,

348
00:22:05,390 --> 00:22:09,206
it'll get pushed into, Nifi will

349
00:22:09,228 --> 00:22:13,574
grab that and we'll be able to process data coming from

350
00:22:13,772 --> 00:22:17,090
Slack, which is really cool. Another new feature

351
00:22:17,170 --> 00:22:21,206
is I can now take groups of processors

352
00:22:21,318 --> 00:22:24,826
and run them as stateless. So it runs in

353
00:22:24,848 --> 00:22:28,538
its own clean environment, isolated from anything

354
00:22:28,624 --> 00:22:32,320
else, and it runs from start to finish

355
00:22:32,850 --> 00:22:37,086
as sort of a job or function as a service

356
00:22:37,268 --> 00:22:41,114
runs that completes and you get the logs of results

357
00:22:41,242 --> 00:22:44,546
and then it'll stop depending on how you want to

358
00:22:44,568 --> 00:22:48,706
schedule that. Here we've got it to run in 1

359
00:22:48,888 --> 00:22:52,306
minute segments. It depends on how you want it to

360
00:22:52,328 --> 00:22:56,514
go. Typically with these you'll do something that's triggered

361
00:22:56,562 --> 00:23:01,574
by a time, say maybe

362
00:23:01,612 --> 00:23:05,702
I'm listening for s three changes

363
00:23:05,836 --> 00:23:09,720
or assist log. Something lands in a file system.

364
00:23:11,390 --> 00:23:15,066
This will get anytime a new object shows up, and then

365
00:23:15,088 --> 00:23:18,778
maybe we run some processing against it. And then we're done.

366
00:23:18,944 --> 00:23:22,506
Just gives you an example. There lots

367
00:23:22,538 --> 00:23:26,670
of different things you could do. Just wanted to show you. The new Nifi

368
00:23:27,090 --> 00:23:30,810
20 also has the ability to read parameters

369
00:23:30,890 --> 00:23:34,702
dynamically from different servers, like one password,

370
00:23:34,846 --> 00:23:39,570
like a database, like Azure key vault AWS secrets.

371
00:23:39,910 --> 00:23:43,442
Nice way to do that. A couple of new features in the new

372
00:23:43,496 --> 00:23:47,554
system are pretty cool flow

373
00:23:47,602 --> 00:23:51,302
analysis and

374
00:23:51,356 --> 00:23:54,360
this is an area where we need a lot of work.

375
00:23:54,890 --> 00:23:58,794
So new people who are learning Nifi, you could add

376
00:23:58,832 --> 00:24:02,122
stuff to their server to dissuade them

377
00:24:02,176 --> 00:24:05,882
from doing things that may be problematic. With this

378
00:24:05,936 --> 00:24:09,690
one you could tell them not to use certain component types.

379
00:24:10,830 --> 00:24:15,118
Knifey still has a bunch of them that haven't been deprecated yet,

380
00:24:15,284 --> 00:24:18,558
and any of the ones without records. If you're using

381
00:24:18,644 --> 00:24:22,142
structured data, you might want to not

382
00:24:22,196 --> 00:24:25,730
use them because we've added new ones for Excel,

383
00:24:26,870 --> 00:24:31,010
for window events, for YaML,

384
00:24:31,910 --> 00:24:35,300
for grok. So you could do a lot there.

385
00:24:35,610 --> 00:24:39,942
Makes it pretty easy. Let's get back to this

386
00:24:39,996 --> 00:24:43,478
here. I think we're pretty close to the time here. I want

387
00:24:43,484 --> 00:24:46,838
to thank David. Hopefully everyone has

388
00:24:47,004 --> 00:24:51,326
learned some deep decent stuff on using flipn

389
00:24:51,378 --> 00:24:55,690
pattern and definitely reach out. If you're interested in

390
00:24:55,840 --> 00:24:58,854
seeing more on Pulsar, on Nifi,

391
00:24:58,902 --> 00:25:02,890
on Flink, all these are very cool ways to write apps

392
00:25:02,970 --> 00:25:06,782
and as you can see, there was no magic there. Setting these

393
00:25:06,836 --> 00:25:10,286
things up is pretty easy, especially if you use the cloud

394
00:25:10,468 --> 00:25:13,842
managed services for them, even if not within

395
00:25:13,896 --> 00:25:17,346
a docker container or kubernetes. It's a couple of

396
00:25:17,368 --> 00:25:20,050
clicks. Things are running as you saw,

397
00:25:20,120 --> 00:25:23,794
drag and drop some simple SQl and

398
00:25:23,832 --> 00:25:27,554
things are just running for you and you get your data, you do with

399
00:25:27,592 --> 00:25:31,606
what you want with uh, we can

400
00:25:31,708 --> 00:25:35,794
join things together like Flink has got a pretty rich SQl

401
00:25:35,842 --> 00:25:40,762
here. Here I'm joining two different topics based

402
00:25:40,816 --> 00:25:44,282
on a similar, you know,

403
00:25:44,336 --> 00:25:48,122
lots of different things you could do. You could also use debesium here

404
00:25:48,256 --> 00:25:51,934
so I can load from relational tables, join them

405
00:25:51,972 --> 00:25:56,046
together. Pretty powerful way to do that.

406
00:25:56,228 --> 00:26:00,474
And with this I can join things like Pulsar

407
00:26:00,522 --> 00:26:04,526
and Kafka together. I can join a

408
00:26:04,548 --> 00:26:08,010
database table with a topic.

409
00:26:08,170 --> 00:26:12,206
So a lot of options there. Thanks for coming to the

410
00:26:12,228 --> 00:26:15,474
talk David. Do you want to send them out? Yep.

411
00:26:15,522 --> 00:26:18,454
Thank you everyone for attending the talk. Hopefully you have a lot of fun with

412
00:26:18,492 --> 00:26:21,570
the flipn stack. Thanks Tim. Thanks everyone for attending.

