1
00:02:11,170 --> 00:02:15,278
You hi,

2
00:02:15,364 --> 00:02:19,754
my name is Christian Elsen and I'm a specialist solutions architect for networking.

3
00:02:19,882 --> 00:02:23,354
I've been with AWS for about six years and have previously

4
00:02:23,402 --> 00:02:27,306
worked in other networking roles for about 15 years, spanning areas

5
00:02:27,338 --> 00:02:30,346
of data center switching, network virtualization,

6
00:02:30,458 --> 00:02:33,898
global continuity, distribution networks, and DNS providers,

7
00:02:33,994 --> 00:02:37,206
as well as BGP routing for service providers. My name

8
00:02:37,228 --> 00:02:40,674
is Lornak McJolo. I'm a senior solution architect in AWS.

9
00:02:40,802 --> 00:02:44,962
Prior to this, for about 17 years I also worked on web infrastructure,

10
00:02:45,026 --> 00:02:48,774
centralized authentication systems, distributed caching, multiregion cloud

11
00:02:48,812 --> 00:02:52,506
native deployments, using infrastructure as code and pipelines to

12
00:02:52,528 --> 00:02:55,894
name a few. But today we are going to talk about optimizing

13
00:02:55,942 --> 00:02:59,302
end user connectivity for multiregion architectures.

14
00:02:59,446 --> 00:03:03,466
So first, why are we talking about this? There are two aspects

15
00:03:03,498 --> 00:03:07,374
that we are focused in today's talk. First is

16
00:03:07,412 --> 00:03:11,034
performant to connect end users to application endpoints in multiple

17
00:03:11,082 --> 00:03:14,720
regions in the most performant and reliable way possible.

18
00:03:15,270 --> 00:03:18,594
Second is maximizing availability in case of

19
00:03:18,632 --> 00:03:23,262
disaster recovery. So how can we ensure that we can perform instantaneous failovers

20
00:03:23,326 --> 00:03:27,054
even in face of gray features? If you're running our application in

21
00:03:27,112 --> 00:03:30,754
multiple regions, let's first take a look at how to achieve

22
00:03:30,802 --> 00:03:34,518
performance for end user connectivity. For this,

23
00:03:34,684 --> 00:03:38,742
we are going to take a deeper dive into this networking service called

24
00:03:38,796 --> 00:03:42,106
AWS Global Accelerator. So here we

25
00:03:42,128 --> 00:03:45,594
have an application that is deployed in multi regions, in this

26
00:03:45,632 --> 00:03:49,066
case North Virginia US, east one and Ireland US one,

27
00:03:49,168 --> 00:03:53,294
and the user of our application. The users are

28
00:03:53,332 --> 00:03:56,762
accessing one of these stacks from around the globe.

29
00:03:56,906 --> 00:04:00,430
However, as they're accessing the application over the public

30
00:04:00,500 --> 00:04:03,726
Internet, each hop from the end user to the

31
00:04:03,748 --> 00:04:07,694
application endpoint can incur additional latency,

32
00:04:07,822 --> 00:04:11,698
and this is going to result in a nonoptimal experience for

33
00:04:11,784 --> 00:04:15,234
the end user. Now, is there a better way to provide them

34
00:04:15,272 --> 00:04:18,680
with more reliable performance connectivity to the application?

35
00:04:19,210 --> 00:04:23,106
Here we are looking at a map of the global

36
00:04:23,138 --> 00:04:26,758
network of 96 points of presence across

37
00:04:26,844 --> 00:04:30,374
46 countries in 84 cities that AWS

38
00:04:30,422 --> 00:04:33,658
global accelerator uses. So for example,

39
00:04:33,824 --> 00:04:37,900
in Asia, here are the edge locations, to name a few of them.

40
00:04:38,670 --> 00:04:41,658
So there is edge locations in Bangalore,

41
00:04:41,754 --> 00:04:44,122
Bangkok, Chennai, Hyderabad,

42
00:04:44,186 --> 00:04:47,594
Jakarta to name a few. And global

43
00:04:47,642 --> 00:04:51,466
accelerator provides you with two static IP addresses

44
00:04:51,578 --> 00:04:55,090
that serve as a fixed entry point to the application

45
00:04:55,240 --> 00:04:58,834
hosted in one or more AWS regions. And underneath the

46
00:04:58,872 --> 00:05:02,686
covers, these IP addresses are any cast from those edge

47
00:05:02,718 --> 00:05:06,898
locations. So they're announced from multiple AWS edge locations

48
00:05:06,994 --> 00:05:10,754
at those same time. And this enables traffic

49
00:05:10,802 --> 00:05:13,910
to enter the AWS global network as close

50
00:05:13,980 --> 00:05:17,266
to the user as possible. So if you have a user

51
00:05:17,298 --> 00:05:21,046
in Jakarta, then if you're using global

52
00:05:21,078 --> 00:05:24,170
accelerator for your application deployment, in that case,

53
00:05:24,240 --> 00:05:27,946
that user will be entering the AWS global network through the

54
00:05:27,968 --> 00:05:31,846
Jakarta Edge. Similarly for the Chennai user,

55
00:05:31,878 --> 00:05:35,802
for example, this way the end users of your apps are benefiting

56
00:05:35,866 --> 00:05:39,306
from the reliable, consistent performance of the AWS

57
00:05:39,338 --> 00:05:42,734
global network. So you can associate these ip addresses to

58
00:05:42,772 --> 00:05:46,898
regional AWS resources or endpoints in this case such as

59
00:05:46,984 --> 00:05:50,686
application load balancer, network load balancer, easy to instances elastic

60
00:05:50,718 --> 00:05:54,542
IP addresses and global accelerator's ip addresses serve

61
00:05:54,606 --> 00:05:58,226
as the front end interface to the app. You can think of

62
00:05:58,248 --> 00:06:01,894
it AWS a door close to your end users wherever they

63
00:06:01,932 --> 00:06:05,762
may be located across those globe and that door is at these edge applications

64
00:06:05,826 --> 00:06:09,850
that will be looked at on the global network map.

65
00:06:10,430 --> 00:06:13,420
Next, Chris is going to show us a demo.

66
00:06:13,870 --> 00:06:17,354
AWS Global Accelerator improves the availability and

67
00:06:17,392 --> 00:06:21,126
performance of your applications through AWS edge locations

68
00:06:21,238 --> 00:06:24,458
in the AWS backbone. This test tool compares

69
00:06:24,474 --> 00:06:28,302
those performance of global accelerator with the public Internet from

70
00:06:28,356 --> 00:06:31,902
your location. It does so by comparing the time it takes

71
00:06:31,956 --> 00:06:35,710
to download a file of a certain size via the public Internet

72
00:06:35,790 --> 00:06:39,198
as well as the optimized path via global accelerator.

73
00:06:39,374 --> 00:06:43,474
In my case, the end user location is San Francisco, California and

74
00:06:43,512 --> 00:06:47,206
I selected 100 kilobyte as the file size to speed up

75
00:06:47,228 --> 00:06:50,818
this particular test run. Let's have a look at the results.

76
00:06:50,994 --> 00:06:54,614
We can see the performance gain via global accelerator from

77
00:06:54,652 --> 00:06:57,990
San Francisco to these five selected AWS regions.

78
00:06:58,410 --> 00:07:01,130
Next, let's take a look at traffic dials.

79
00:07:01,550 --> 00:07:05,318
So in global accelerator we have ability to set traffic

80
00:07:05,334 --> 00:07:08,774
dials for fine grained traffic control. We can dial

81
00:07:08,822 --> 00:07:12,614
up or dial down traffic directed to a specific endpoint

82
00:07:12,662 --> 00:07:16,286
group. Now we do this by setting a traffic dial to

83
00:07:16,308 --> 00:07:19,610
control the percentage of traffic that is already directed

84
00:07:19,690 --> 00:07:23,086
to that endpoint group to that region. Now here I

85
00:07:23,108 --> 00:07:26,626
have two endpoint groups. One is in us east one, one is

86
00:07:26,648 --> 00:07:30,290
in us west one and in each endpoint group I have two

87
00:07:30,440 --> 00:07:34,606
endpoints, the two elastic load balancers.

88
00:07:34,798 --> 00:07:38,326
The percentage is applied only to traffic that is already directed to the

89
00:07:38,348 --> 00:07:41,606
endpoint group based on proximity and health of the

90
00:07:41,628 --> 00:07:45,574
endpoints. So if we have 100 requests directed to

91
00:07:45,612 --> 00:07:48,934
northern Virginia, then 100% of those requests will be

92
00:07:48,972 --> 00:07:52,586
directed to US east one northern Virginia endpoint group if we

93
00:07:52,608 --> 00:07:55,580
set the traffic dial for it at 100%.

94
00:07:56,030 --> 00:07:59,194
Similarly for requests that are directed to US west one

95
00:07:59,232 --> 00:08:02,538
endpoint group. So you can think of traffic dials as giant valves that

96
00:08:02,544 --> 00:08:05,546
are controlling the traffic to the endpoint groups.

97
00:08:05,738 --> 00:08:09,486
Later on we may decide to switch all traffic flow to

98
00:08:09,588 --> 00:08:12,862
only go to us west one endpoint group and for this,

99
00:08:12,916 --> 00:08:17,138
we can set the traffic dial for us east one to 0%.

100
00:08:17,224 --> 00:08:22,078
So we close, we shut off that giant valve that controls

101
00:08:22,174 --> 00:08:24,980
the traffic that is sent to us east one.

102
00:08:25,430 --> 00:08:29,282
Now we have a finer grain control, a smaller nub

103
00:08:29,346 --> 00:08:32,918
if you will. We can use to set

104
00:08:33,004 --> 00:08:36,390
weights on each endpoint inside an endpoint group,

105
00:08:36,460 --> 00:08:40,282
such that we can adjust the amount of traffic each endpoint gets.

106
00:08:40,416 --> 00:08:44,006
Now, endpoints can be network load balancers, application load balancers,

107
00:08:44,038 --> 00:08:47,286
EC two instances or elastic ip addresses. Here I'm showing the elastic

108
00:08:47,318 --> 00:08:50,630
load balancers as endpoints, and global accelerator

109
00:08:50,710 --> 00:08:54,106
calculates the sum of those weights for the endpoints in an endpoint group,

110
00:08:54,208 --> 00:08:57,722
and then directs traffic to the endpoints based on the ratio

111
00:08:57,786 --> 00:09:01,454
of each endpoint's weight to the total. So you can go as

112
00:09:01,492 --> 00:09:04,914
fine grained as one over 256 for the percentage of

113
00:09:04,952 --> 00:09:08,642
traffic that is directed to an endpoint inside an endpoint group.

114
00:09:08,776 --> 00:09:12,638
Now, how do these features help us with bluegreen deployments?

115
00:09:12,814 --> 00:09:16,734
First, a quick recap of bluegreen deployments. The goal

116
00:09:16,782 --> 00:09:20,294
of bluegreen deployments is to deploy and roll back new

117
00:09:20,332 --> 00:09:24,006
versions of an application with minimal to no downtime for

118
00:09:24,028 --> 00:09:28,342
our app consumers. The way we achieve this is by having two

119
00:09:28,476 --> 00:09:32,186
environments in production that are identical to each other.

120
00:09:32,288 --> 00:09:36,362
And at any given point in time, only one of these environments is

121
00:09:36,416 --> 00:09:40,074
alive in terms of taking in production traffic, and those

122
00:09:40,112 --> 00:09:43,674
other one is idle. So the one that's taking

123
00:09:43,712 --> 00:09:47,294
in production traffic, we can call this those blue environment. For example,

124
00:09:47,492 --> 00:09:50,654
if you want to perform a new release, then we deploy the new

125
00:09:50,692 --> 00:09:54,414
version of the application in the green environment. That is not taking any

126
00:09:54,452 --> 00:09:58,254
production traffic. We test it those, we verify it in the green environment,

127
00:09:58,302 --> 00:10:02,462
and then we cut over the production traffic to the green environment.

128
00:10:02,606 --> 00:10:05,806
Now the blue environment becomes the new idle environment.

129
00:10:05,918 --> 00:10:09,670
And in case of issues in the newly deployed version of the application,

130
00:10:09,820 --> 00:10:14,230
we can always cut the traffic back over to the blue environment.

131
00:10:14,570 --> 00:10:18,534
Now, one way to achieve blue green deployments in a single region is

132
00:10:18,572 --> 00:10:22,422
by using the little nubs that we just talked about using endpoint

133
00:10:22,486 --> 00:10:26,422
weights in global accelerator. So we stand up two identical

134
00:10:26,486 --> 00:10:30,214
stacks of our application, for example, behind can ALB

135
00:10:30,262 --> 00:10:33,850
endpoint for the blue environment and another ALB endpoint

136
00:10:33,930 --> 00:10:37,550
for the green environment. And these two endpoints are inside

137
00:10:37,700 --> 00:10:40,926
one endpoint group. Think of that as a

138
00:10:40,948 --> 00:10:45,194
region, and we use the endpoint weights to adjust the prod traffic flow

139
00:10:45,242 --> 00:10:48,786
as part of the deployment that we just discussed. Next, we are going to

140
00:10:48,808 --> 00:10:52,882
look at a slight modification of blue green deployments. But for

141
00:10:52,936 --> 00:10:56,786
multiregion applications and those goal is always the same

142
00:10:56,888 --> 00:11:00,406
to have minimal to no downtime as we

143
00:11:00,428 --> 00:11:04,850
are deploying and rolling back new version of an app for our app consumers.

144
00:11:05,010 --> 00:11:07,974
So here we have version one of our application.

145
00:11:08,092 --> 00:11:11,766
It's deployed in two regions, US west two and US east

146
00:11:11,798 --> 00:11:15,386
one. We are using global accelerator for the application.

147
00:11:15,568 --> 00:11:18,886
So our clients in Japan are accessing the application via

148
00:11:18,918 --> 00:11:22,686
the global accelerator point of presence that is closest to them,

149
00:11:22,788 --> 00:11:26,046
that is either in Tokyo or Osaka for global accelerator points of

150
00:11:26,068 --> 00:11:29,326
presence. And the

151
00:11:29,348 --> 00:11:33,494
global accelerator is then intelligently routing their requests

152
00:11:33,562 --> 00:11:36,926
now inside the global network, AWS global

153
00:11:36,958 --> 00:11:40,654
network into the nearest app stack. That's in US west.

154
00:11:40,702 --> 00:11:44,820
Two clients in Europe are also accessing this application

155
00:11:45,850 --> 00:11:49,602
also through the global accelerator point of presence

156
00:11:49,746 --> 00:11:53,430
closest to them. So that'll be in Europe.

157
00:11:54,090 --> 00:11:58,226
And global accelerator again intelligently routes their request

158
00:11:58,418 --> 00:12:02,198
know after taking them in through the point of presence

159
00:12:02,294 --> 00:12:06,150
through that door in Europe, it then routes the requests

160
00:12:06,230 --> 00:12:09,434
inside the AWS global network into the nearest app

161
00:12:09,472 --> 00:12:12,986
stack, and that is Us east one. So one thing to note

162
00:12:13,018 --> 00:12:16,794
is that both of these app stacks are actually serving live production

163
00:12:16,842 --> 00:12:20,478
traffic. And now we decided to upgrade our application

164
00:12:20,564 --> 00:12:24,330
from version one to version two without incurring any downtime to our

165
00:12:24,340 --> 00:12:27,682
app consumers. So remember that

166
00:12:27,736 --> 00:12:31,294
we have the traffic dials to control traffic for endpoint

167
00:12:31,342 --> 00:12:34,386
groups in global accelerator. Those are the giant valves that we

168
00:12:34,408 --> 00:12:37,522
can use to control to dial up and down

169
00:12:37,656 --> 00:12:41,622
the traffic for endpoint groups. So we are going to use these.

170
00:12:41,756 --> 00:12:45,206
We first set the traffic dial in US west two to 0%,

171
00:12:45,308 --> 00:12:48,822
and then all production traffic now flows to us

172
00:12:48,876 --> 00:12:51,850
east one. For our clients in Japan,

173
00:12:53,150 --> 00:12:57,558
they still enter using the same point of presence closest to them in Japan.

174
00:12:57,654 --> 00:13:01,010
So that will be either Tokyo or Osaka, depending on where they're located

175
00:13:01,110 --> 00:13:04,366
in Japan, whichever is closer to them.

176
00:13:04,548 --> 00:13:08,090
But then global accelerator is going to intelligently

177
00:13:08,170 --> 00:13:12,026
route their request this time to US espawn application stack.

178
00:13:12,218 --> 00:13:15,602
And now that us best two has no production traffic flowing into

179
00:13:15,656 --> 00:13:19,086
it, we can upgrade our application in Us west

180
00:13:19,118 --> 00:13:22,754
two to version two without incurring any downtime to

181
00:13:22,792 --> 00:13:25,906
our app consumers. Next, we are going to

182
00:13:25,928 --> 00:13:29,734
repeat this process for us east one. So we first turn down

183
00:13:29,772 --> 00:13:32,966
the traffic dial to 0%. In US east one.

184
00:13:33,068 --> 00:13:36,914
All traffic now goes to users two, including for our clients

185
00:13:37,042 --> 00:13:40,558
in Europe. And they still enter the AWS

186
00:13:40,594 --> 00:13:44,374
global network through the door through that point of presence that's closest

187
00:13:44,422 --> 00:13:48,554
to them. So they'll be in Europe, and global accelerator is

188
00:13:48,592 --> 00:13:52,014
going to intelligently route their request to the application app

189
00:13:52,052 --> 00:13:54,878
stack in USS two.

190
00:13:55,044 --> 00:13:58,554
So this way we can upgrade the app in USC

191
00:13:58,602 --> 00:14:01,962
SWAN to version two without incurring any downtime

192
00:14:02,026 --> 00:14:07,186
for app consumers. This time. Clients in Europe finally,

193
00:14:07,288 --> 00:14:10,578
we now have both regions with version two of the app,

194
00:14:10,744 --> 00:14:14,514
and we turn the traffic dial in US east one up to

195
00:14:14,632 --> 00:14:17,858
100%. And now the clients in Japan go

196
00:14:17,864 --> 00:14:21,122
to us best two and clients in Europe to USC swan through

197
00:14:21,176 --> 00:14:25,142
their global senator point of presence doors that

198
00:14:25,196 --> 00:14:28,786
are closest to them that will take them in take their requests

199
00:14:28,818 --> 00:14:32,602
into the AWS global network. Next, Chris is going to

200
00:14:32,656 --> 00:14:36,422
show a demo on bluegreen deployments for multi region

201
00:14:36,486 --> 00:14:40,134
applications. In this demo, we will look at AWS

202
00:14:40,182 --> 00:14:44,426
global accelerator for a multi region blue green software deployment

203
00:14:44,458 --> 00:14:48,394
scenario. As depicted in the presentation, this setup

204
00:14:48,442 --> 00:14:52,234
uses a single accelerator with endpoints in two AWS

205
00:14:52,282 --> 00:14:56,526
regions. The US west one region represents our plue

206
00:14:56,558 --> 00:15:00,062
deployment and the US east two region represents

207
00:15:00,126 --> 00:15:02,994
our cream deployment. Right now,

208
00:15:03,112 --> 00:15:06,420
traffic dials for both regions are at 50%

209
00:15:06,810 --> 00:15:10,802
as the percentage is applied to only the traffic already directed

210
00:15:10,866 --> 00:15:14,914
to the endpoint group. Not all listener traffic.

211
00:15:15,042 --> 00:15:18,466
Only by explicitly specifying 50% as a traffic

212
00:15:18,498 --> 00:15:21,846
die for both will we see each region receiving

213
00:15:21,878 --> 00:15:24,890
about the same amount of incoming end user traffic.

214
00:15:25,870 --> 00:15:29,962
This cloud watch dashboard shows the incoming traffic ratios across

215
00:15:30,016 --> 00:15:33,694
the two application load balancer that front each of the two

216
00:15:33,732 --> 00:15:37,582
regions. On the left we have a traffic gorge that shows

217
00:15:37,636 --> 00:15:41,050
the most recent distribution with a historical distribution

218
00:15:41,130 --> 00:15:44,862
over the last half hour. On the right. As expected,

219
00:15:44,926 --> 00:15:48,580
the traffic ratio across the two regions is about 50 50.

220
00:15:49,190 --> 00:15:52,610
Now let's train traffic from our flu region us

221
00:15:52,680 --> 00:15:56,580
west one so we can perform a software update there.

222
00:15:57,910 --> 00:16:01,526
For this, we will set the traffic dial for us west one to

223
00:16:01,548 --> 00:16:04,914
zero while leaving the traffic dial for us east

224
00:16:04,962 --> 00:16:08,826
two where it is AWS. Us East two will be the

225
00:16:08,848 --> 00:16:12,010
only remaining region. It will receive all traffic.

226
00:16:23,710 --> 00:16:27,334
Let's have a look at the cloud watch dashboard and see how traffic

227
00:16:27,382 --> 00:16:30,846
shifts. We will speed up the recording a bit so we don't have

228
00:16:30,868 --> 00:16:33,200
to wait the two to three minutes it takes.

229
00:16:49,790 --> 00:16:53,574
Great. Now we can see that 100% of our, including traffic

230
00:16:53,622 --> 00:16:57,226
is headed to the cream deployment, allowing us to upgrade

231
00:16:57,258 --> 00:17:00,814
the application in the plue deployment. After we finish

232
00:17:00,852 --> 00:17:04,986
this upgrade, let's switch all traffic to the plue deployment so we can upgrade

233
00:17:05,018 --> 00:17:08,786
the cream deployment. This time, we will set the

234
00:17:08,808 --> 00:17:12,354
traffic dial for us west one to 50% and

235
00:17:12,392 --> 00:17:15,380
the one for us east two to 0%.

236
00:17:35,520 --> 00:17:39,816
Let's have a look at the Cloudwatch dashboard and see how traffic shifts

237
00:17:39,928 --> 00:17:42,976
again. We will speed up the recording a bit so we don't have

238
00:17:42,998 --> 00:17:45,250
to wait the two to three minutes it takes.

239
00:18:00,770 --> 00:18:04,206
Now 100% of our incoming traffic is headed to

240
00:18:04,228 --> 00:18:08,242
the blue deployment, allowing us to upgrade the application in the green

241
00:18:08,296 --> 00:18:11,454
deployment. Once we finish this upgrade, let's switch

242
00:18:11,502 --> 00:18:14,850
all the traffic back to the original 50 50 split.

243
00:18:15,830 --> 00:18:18,994
This time we will set the traffic dial for us east

244
00:18:19,042 --> 00:18:20,760
two back to 50%.

245
00:18:33,390 --> 00:18:36,814
We'll return to the Cloudwatch dashboard the last time and see

246
00:18:36,852 --> 00:18:40,350
how traffic shifts again. We will speed up the recording a bit

247
00:18:40,420 --> 00:18:43,200
so we don't have to wait the two to three minutes it takes,

248
00:19:01,000 --> 00:19:04,440
and we're back to a 50 50 traffic split.

249
00:19:05,980 --> 00:19:10,120
So let's take a look at disaster recovery in multiregion architectures.

250
00:19:11,820 --> 00:19:15,336
The concepts of data plane and control plane date all

251
00:19:15,358 --> 00:19:19,400
the way back to networking terminology. So these are not new concepts.

252
00:19:19,560 --> 00:19:23,848
And for a given AWS service, there is typically a control plane.

253
00:19:24,024 --> 00:19:28,004
That is what allows us to create resources, to modify resources,

254
00:19:28,072 --> 00:19:30,290
and to destroy resources. For example,

255
00:19:30,980 --> 00:19:34,156
if you think of EC two, control plane operations

256
00:19:34,268 --> 00:19:38,032
are launching an EC two instance, changing a security group

257
00:19:38,086 --> 00:19:41,684
on an existing EC two instance, or terminating an EC two

258
00:19:41,722 --> 00:19:45,172
instance, among others. At the same time, there are data

259
00:19:45,226 --> 00:19:48,836
plane operations that allow for resources that are already

260
00:19:49,018 --> 00:19:52,596
up and running to continue to operate. So in the

261
00:19:52,618 --> 00:19:56,144
case of EC two, you may have already instantiated EC two servers

262
00:19:56,192 --> 00:20:00,196
that are already serving requests, that are up and running and serving requests.

263
00:20:00,308 --> 00:20:03,672
So all operations that are performed while these instances are running

264
00:20:03,726 --> 00:20:07,244
are part of the data plane. So for example, reading and

265
00:20:07,282 --> 00:20:11,532
writing to existing elastic block storage volumes or

266
00:20:11,586 --> 00:20:15,740
routing packets according to the existing VPC route tables.

267
00:20:16,160 --> 00:20:20,204
Now, in case of impairments to the control plane, the EC two instances

268
00:20:20,252 --> 00:20:23,360
have all of those information that they need available to them

269
00:20:23,430 --> 00:20:27,296
locally in order to continue to run. Here I have

270
00:20:27,318 --> 00:20:30,560
an analogy for you. So let's think about lifecycle

271
00:20:30,640 --> 00:20:33,908
of flights. So you can think that

272
00:20:34,074 --> 00:20:37,780
for any given flight there is a takeoff,

273
00:20:38,120 --> 00:20:41,844
there is landing, and there is the part in between where

274
00:20:41,882 --> 00:20:45,176
the plane is up and running, flying in

275
00:20:45,198 --> 00:20:48,888
the sky. And for the parts of this

276
00:20:49,054 --> 00:20:52,644
that has to do with creating a flight, which is a takeoff,

277
00:20:52,692 --> 00:20:56,556
and terminating a flight, which is the lending, you need a

278
00:20:56,578 --> 00:21:00,360
certain set of steps. That includes the control tower,

279
00:21:00,440 --> 00:21:04,252
getting clearance from. It also includes running through a rum book,

280
00:21:04,306 --> 00:21:08,236
ensuring that the plane is ready, et cetera. So those are strict procedures around

281
00:21:08,338 --> 00:21:12,028
what we need to create a flight and to terminate a flight,

282
00:21:12,124 --> 00:21:15,536
and those are part of the control plane operations. But once the

283
00:21:15,558 --> 00:21:18,412
plane is up and running and it's in the sky,

284
00:21:18,556 --> 00:21:22,224
it no longer needs the control tower. It has zero dependencies

285
00:21:22,272 --> 00:21:25,876
on that control tower. For example, to continue to be up and

286
00:21:25,898 --> 00:21:29,700
running to continue to fly. So if the control tower, for whatever reason,

287
00:21:29,770 --> 00:21:34,308
goes away, the plane has everything that it needs locally,

288
00:21:34,484 --> 00:21:38,360
old instruments. It has the fuel that is needed

289
00:21:38,430 --> 00:21:40,200
for it to continue to fly.

290
00:21:42,140 --> 00:21:46,208
So data plane operations ensure to keep what's already in flight to operate.

291
00:21:46,324 --> 00:21:49,580
Now, both data plane and control plane operations are important,

292
00:21:49,730 --> 00:21:53,500
but data plane operations favor availability.

293
00:21:53,920 --> 00:21:57,212
We want to make sure that if we have easy to instances that are already

294
00:21:57,266 --> 00:22:00,336
serving requests, they should continue to be serving requests in

295
00:22:00,358 --> 00:22:03,040
case our control plane is having some impairment.

296
00:22:04,260 --> 00:22:07,516
And that has its roots also in the cap theorem

297
00:22:07,548 --> 00:22:10,852
if you think about it. So the idea is to rely on

298
00:22:10,906 --> 00:22:14,260
the data plane that is designed with a higher

299
00:22:14,330 --> 00:22:17,524
availability target and not on the control plane that

300
00:22:17,562 --> 00:22:20,980
favors consistency during recovery.

301
00:22:24,220 --> 00:22:28,084
Let's have a look at Amazon Route 53 application recovery

302
00:22:28,132 --> 00:22:31,972
controller, which provides a mechanism to simplify and automate

303
00:22:32,036 --> 00:22:34,840
recovery for highly available applications.

304
00:22:35,680 --> 00:22:39,196
Some industry and workloads have very high requirements in

305
00:22:39,218 --> 00:22:43,064
terms of decide availability cases and recovery time objectives.

306
00:22:43,192 --> 00:22:47,016
As can example, think about how real time payment processing or

307
00:22:47,058 --> 00:22:51,180
trading engines can affect entire economies if disrupted.

308
00:22:51,340 --> 00:22:55,340
To address these requirements, you typically deploy multiple replicas,

309
00:22:55,420 --> 00:22:58,812
called cells, across a variety of AWS

310
00:22:58,876 --> 00:23:02,512
availability zones, AWS regions, and on premises

311
00:23:02,576 --> 00:23:06,592
environments. Route 53 application recovery controller

312
00:23:06,656 --> 00:23:10,096
provides those highly reliable mechanism to aid Route

313
00:23:10,128 --> 00:23:13,624
53 in reliably routing end users to the

314
00:23:13,662 --> 00:23:16,680
appropriate cell in an active, active setup.

315
00:23:18,060 --> 00:23:22,548
Or in a nutshell, Amazon Route 53 application recovery controller

316
00:23:22,644 --> 00:23:25,816
gives you a big red emergency stop button, which acts

317
00:23:25,848 --> 00:23:29,710
like a circuit breaker to take a problematic cell out of service.

318
00:23:31,520 --> 00:23:35,656
What are the key capabilities of application recovery controller?

319
00:23:35,848 --> 00:23:40,396
First, readiness checks continually monitor AWS resources

320
00:23:40,508 --> 00:23:44,304
across your application replicas. Checks can monitor a number

321
00:23:44,342 --> 00:23:47,712
of areas that can affect recovery readiness, such as

322
00:23:47,766 --> 00:23:51,440
updates to configurations, also called configuration drift

323
00:23:51,600 --> 00:23:54,340
capacity or network routing policies.

324
00:23:54,920 --> 00:23:59,184
Second, routing controls gives you a way to manually and reliably

325
00:23:59,232 --> 00:24:03,252
fail over the entire application stack. Such a failover

326
00:24:03,316 --> 00:24:07,336
decision is often a conscious manual choice based on application

327
00:24:07,438 --> 00:24:10,872
metrics or partial features. You can also use

328
00:24:10,926 --> 00:24:14,836
them to shift traffic for maintenance purposes or to recover from

329
00:24:14,878 --> 00:24:18,140
failures when your monitors itself fail.

330
00:24:19,120 --> 00:24:23,176
Third, safety rules as safeguards for application recovery

331
00:24:23,208 --> 00:24:26,712
controller itself to determine the side combinations

332
00:24:26,776 --> 00:24:30,668
of routing control and to avoid unintended consequences.

333
00:24:30,844 --> 00:24:34,236
For example, you might want to prevent inadvertently turning

334
00:24:34,268 --> 00:24:37,724
off all routing controls, which would stop all traffic

335
00:24:37,772 --> 00:24:40,900
flow and thereby result in a failopen scenario.

336
00:24:42,920 --> 00:24:46,656
Let's look closer at the architectures of the application recovery

337
00:24:46,688 --> 00:24:50,150
controller. We start by looking at routing control.

338
00:24:51,240 --> 00:24:54,872
Routing controls allow us to create control panels and

339
00:24:54,926 --> 00:24:58,552
model the desired cell structure of our applications and

340
00:24:58,606 --> 00:25:02,004
how our big red emergency buttons or circuit breakers

341
00:25:02,132 --> 00:25:05,432
should look like in this example here we have

342
00:25:05,486 --> 00:25:09,388
two cells with one circuit breaker switch each.

343
00:25:09,554 --> 00:25:13,260
The cell in the active region is currently in the on position,

344
00:25:13,410 --> 00:25:16,830
while those cell in the standby region is in the off position.

345
00:25:18,180 --> 00:25:22,044
To actually influence traffic between active and standby region,

346
00:25:22,172 --> 00:25:27,040
Amazon route 53 is needed. In addition to the application recovery controller,

347
00:25:27,380 --> 00:25:31,004
our circuit breaker buttons are mapped to route 53 health

348
00:25:31,062 --> 00:25:34,644
checks that can be used for various record type such as

349
00:25:34,682 --> 00:25:38,004
failover record type. At this point, it is very

350
00:25:38,042 --> 00:25:42,176
important to point out the key capability of this integration.

351
00:25:42,368 --> 00:25:46,024
Route 53 healthcare checks are part of the data plane which

352
00:25:46,062 --> 00:25:50,024
has a 100% uptime slA. Therefore, even if

353
00:25:50,062 --> 00:25:53,716
the route 53 control plane is affected during a large scale

354
00:25:53,748 --> 00:25:57,052
event, forcing a route 53 health check

355
00:25:57,186 --> 00:26:00,424
to unhealthy via the application recovery controller

356
00:26:00,552 --> 00:26:04,296
still allows us to perform a failover between the active

357
00:26:04,328 --> 00:26:07,648
and stem by region. But why is the applications

358
00:26:07,734 --> 00:26:11,680
recovery controller's control plane superior in this scenario?

359
00:26:12,020 --> 00:26:15,692
As depicted in the diagram, you can see that each controller

360
00:26:15,836 --> 00:26:19,660
consistency of a cluster across five different AWS regions

361
00:26:19,820 --> 00:26:23,636
with API endpoints in each of them. As long

362
00:26:23,658 --> 00:26:27,476
as one of these five endpoints is still available, changes to

363
00:26:27,498 --> 00:26:31,028
the routing control state can be made and these changes

364
00:26:31,114 --> 00:26:34,570
can still factor in safety rules that you previously saw.

365
00:26:36,220 --> 00:26:40,116
Now let's look in detail at how the application recovery controller

366
00:26:40,228 --> 00:26:44,244
interfaces with route 53. A DNS request

367
00:26:44,292 --> 00:26:47,804
for our application MyApp AWS would

368
00:26:47,842 --> 00:26:50,860
reach route 50 three's distributed data plane.

369
00:26:51,360 --> 00:26:55,864
At the same time, route 50 three's global health checkers integrate

370
00:26:55,912 --> 00:26:58,930
with application recovery controller's routing control.

371
00:26:59,700 --> 00:27:03,344
The application recovery controller provides a virtual health check

372
00:27:03,462 --> 00:27:07,372
that is mapped to manual on off switch which can be controlled

373
00:27:07,436 --> 00:27:11,364
via a highly available API. If we flip this

374
00:27:11,402 --> 00:27:15,296
switch via the routing control, the information is provided

375
00:27:15,328 --> 00:27:19,104
by the route 53 global health checkers to the distributed

376
00:27:19,152 --> 00:27:23,064
data plane. This updated health check information

377
00:27:23,262 --> 00:27:26,792
within the distributed data plane now allows a change

378
00:27:26,846 --> 00:27:31,012
DNS response and this change DNS response

379
00:27:31,076 --> 00:27:35,012
reroutes incoming traffic to our secondary or standby

380
00:27:35,076 --> 00:27:38,796
region. Now let's look at a brief demo and

381
00:27:38,818 --> 00:27:42,030
how all this can be used with an example application.

382
00:27:42,400 --> 00:27:46,220
In those demo, we will look at the Amazon Route 83 application

383
00:27:46,370 --> 00:27:50,176
recovery controller. For this we have deployed a

384
00:27:50,198 --> 00:27:54,076
simple demo architecture with a tictactoe game deployed

385
00:27:54,108 --> 00:27:57,472
across two regions. US east one acts as

386
00:27:57,526 --> 00:28:00,844
active region while US best two acts as a hot

387
00:28:00,902 --> 00:28:04,384
standby region. Initially, both AWS regions

388
00:28:04,432 --> 00:28:08,336
are healthy and therefore the game should be served out of the active US east

389
00:28:08,368 --> 00:28:12,548
one region. Those is accomplished by searing inbound network traffic

390
00:28:12,644 --> 00:28:16,644
via a Route 53 failover record using Amazon

391
00:28:16,692 --> 00:28:19,784
Route 53 application recovery controller. We also

392
00:28:19,822 --> 00:28:23,496
have a circuit breaker in the form of routing controls in

393
00:28:23,518 --> 00:28:27,116
place. Let's have a look. Here we can see

394
00:28:27,218 --> 00:28:31,080
the route 53 failover record with a primary entry

395
00:28:31,160 --> 00:28:35,064
for the US east one and a secondary entry for us west

396
00:28:35,112 --> 00:28:38,904
two. Both records have a distinct health check associated

397
00:28:38,952 --> 00:28:42,544
with them. Looking at these health checks, we can see

398
00:28:42,582 --> 00:28:46,124
that each of them is currently healthy and therefore the primary

399
00:28:46,172 --> 00:28:49,730
failover record entry for us east one is being used.

400
00:28:50,100 --> 00:28:54,084
Each of the two health checks corresponds to route 53 application

401
00:28:54,202 --> 00:28:57,616
recovery controller routing control. We can imagine

402
00:28:57,648 --> 00:29:00,960
each of these routing controls to be like a circuit breaker.

403
00:29:01,120 --> 00:29:05,288
Here we can see that at this point, each routing control state is on

404
00:29:05,454 --> 00:29:09,032
let's play a game of tictactoe we can see

405
00:29:09,086 --> 00:29:12,328
that the tictactoe games are currently served route of the US

406
00:29:12,414 --> 00:29:13,880
east one region,

407
00:29:16,640 --> 00:29:20,312
creating a new game and choosing a worthy opponent. We can validate

408
00:29:20,376 --> 00:29:22,110
that the application is working.

409
00:29:34,430 --> 00:29:38,554
So far so good. But what if we are faced with

410
00:29:38,592 --> 00:29:42,134
a disastrous event in our active region and need to fail

411
00:29:42,182 --> 00:29:45,998
over to the Sampai region? As part of this disastrous event,

412
00:29:46,084 --> 00:29:49,786
we're also no longer able to make changes to the DNS public hosted

413
00:29:49,818 --> 00:29:53,466
zone. But thanks to the Route 53 application recovery

414
00:29:53,498 --> 00:29:57,166
controller, we have our circuit breakers in the form of route

415
00:29:57,198 --> 00:30:00,994
controls in place. With this,

416
00:30:01,032 --> 00:30:04,322
we can open the circuit breaker for the US east one

417
00:30:04,376 --> 00:30:07,522
region and thereby initiate a failover to the US

418
00:30:07,576 --> 00:30:10,420
west two region. Let's have a look.

419
00:30:11,030 --> 00:30:14,754
First, we will open the circuit breaker for the US east one

420
00:30:14,792 --> 00:30:19,338
region by making the route control state as off it.

421
00:30:19,384 --> 00:30:25,290
It's

422
00:30:37,600 --> 00:30:40,864
looking at the associated health checks. We can see that

423
00:30:40,902 --> 00:30:44,924
the health check for us east one will change to unhealthy,

424
00:30:45,052 --> 00:30:48,696
which was triggered by the route 53 application recovery controller

425
00:30:48,748 --> 00:30:52,384
routing control state change. If we reload

426
00:30:52,432 --> 00:30:56,036
our tictactoe game, we can see that it will now be served out of the

427
00:30:56,058 --> 00:30:57,620
Usos two region.

428
00:31:04,740 --> 00:31:07,600
Time to play another round of tictactoe.

429
00:31:14,540 --> 00:31:17,290
Let's look at those key takeaways from this talk.

430
00:31:17,660 --> 00:31:21,584
Improve those application performance resiliency by minimizing the

431
00:31:21,622 --> 00:31:24,240
number of network hops through the AWS backbone.

432
00:31:24,740 --> 00:31:28,556
Eliminate control plane dependencies of your application to improve

433
00:31:28,588 --> 00:31:32,060
disaster recovery and consider manual failover

434
00:31:32,140 --> 00:31:35,980
mechanisms by using route 53 applications recovery controller

435
00:31:36,060 --> 00:31:38,130
as a big red emergency button.

436
00:31:39,220 --> 00:31:39,710
Thank you.

