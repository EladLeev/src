1
00:00:23,290 --> 00:00:26,946
Hello everyone and welcome to the session on data to discovery,

2
00:00:27,058 --> 00:00:30,290
unveiling, clustering in birth topic topic modeling.

3
00:00:30,370 --> 00:00:34,006
This session will be presented by me, Abhiram and as well as my

4
00:00:34,028 --> 00:00:37,858
co presenter Jaspal. So a little bit about ourselves.

5
00:00:38,034 --> 00:00:41,206
I work as a cloud machine learning engineer,

6
00:00:41,228 --> 00:00:44,534
Collinson and hold a master's in data science from

7
00:00:44,572 --> 00:00:47,926
King's College London. Previously I was working as

8
00:00:47,948 --> 00:00:50,954
a research fellow at SAP Labs in Bangalore.

9
00:00:51,082 --> 00:00:54,654
Additionally, I also have published courses on

10
00:00:54,692 --> 00:00:58,542
LinkedIn learning as an instructor on rust programming and it has

11
00:00:58,596 --> 00:01:02,422
over 40,000 participants. I love to volunteer

12
00:01:02,506 --> 00:01:06,894
at Data Kind Bangalore on nonprofit products and projects

13
00:01:07,022 --> 00:01:10,706
and also play badminton and love to listen 80s

14
00:01:10,728 --> 00:01:14,890
rock music in my free time. Along with me is Jaspal

15
00:01:14,990 --> 00:01:18,582
who works as a lead data scientist with me at

16
00:01:18,716 --> 00:01:21,974
Collinson. His expertise lies in the areas of

17
00:01:22,012 --> 00:01:25,766
AI, Python, AWS and building data products

18
00:01:25,868 --> 00:01:29,654
from scratch. He holds a CBA in advanced

19
00:01:29,702 --> 00:01:33,338
business analytics from Indian School of Business.

20
00:01:33,504 --> 00:01:37,274
He loves to play football and his favorite football club is

21
00:01:37,312 --> 00:01:41,098
Arsenal. Both of us are reachable at Twitter

22
00:01:41,194 --> 00:01:44,734
and our handles are mentioned below. So let's look at

23
00:01:44,772 --> 00:01:48,430
today's agenda. First we will present the problem

24
00:01:48,500 --> 00:01:52,266
statement and the topic modeling use case following

25
00:01:52,298 --> 00:01:55,842
which we present why build topic is best

26
00:01:55,896 --> 00:01:59,506
suited to solve this problem and the

27
00:01:59,528 --> 00:02:02,594
end to end flow or the different processes that are

28
00:02:02,632 --> 00:02:06,482
involved in build topic. One such process is

29
00:02:06,536 --> 00:02:10,370
clustering, which is an integral part of the build topic technique

30
00:02:10,450 --> 00:02:13,874
and here we will explain about hdb scan.

31
00:02:14,002 --> 00:02:17,206
Then we will go into a hands on session where we look

32
00:02:17,228 --> 00:02:20,506
at Amazon Alexa reviews and how we can get topics of

33
00:02:20,528 --> 00:02:24,502
interest from that data set published on Kaggle.

34
00:02:24,646 --> 00:02:27,962
Then we close it off by discussing about the future

35
00:02:28,016 --> 00:02:31,786
scope of topic modeling. So let's

36
00:02:31,818 --> 00:02:35,422
get started. Let's imagine that you have

37
00:02:35,476 --> 00:02:38,894
a product like Alexa Echo Dot, which is basically a

38
00:02:38,932 --> 00:02:42,618
Bluetooth enabled voice assistant. People buy it mostly online

39
00:02:42,724 --> 00:02:46,014
and also in offline stores and leave reviews on websites

40
00:02:46,062 --> 00:02:49,620
like Amazon Reviews and also the Twitter website.

41
00:02:50,630 --> 00:02:54,482
So both e commerce as well as

42
00:02:54,536 --> 00:02:57,602
social media sites. So in this scenario,

43
00:02:57,666 --> 00:03:01,910
the first review on the Amazon platform is a positive review

44
00:03:02,060 --> 00:03:05,954
which says about the usability

45
00:03:06,082 --> 00:03:09,858
of the echo dot and how it's helping the customers.

46
00:03:09,964 --> 00:03:13,270
Kids learning current affairs general knowledge

47
00:03:13,430 --> 00:03:17,034
also improve their english skills. So this is definitely a pleasure point

48
00:03:17,072 --> 00:03:20,746
for Amazon. Whereas in the tweet below a

49
00:03:20,768 --> 00:03:24,414
customer is complaining saying that the echo dot is not

50
00:03:24,452 --> 00:03:27,918
working on his phone and the customer care

51
00:03:28,004 --> 00:03:31,982
is also not picking up his calls, both of which are crucial pain

52
00:03:32,036 --> 00:03:34,420
points that Amazon has to address.

53
00:03:34,950 --> 00:03:39,140
It is inhumanly possible that

54
00:03:40,150 --> 00:03:43,426
it's not possible by humans to sift through

55
00:03:43,608 --> 00:03:47,250
hundreds and thousands of reviews like these and pick up

56
00:03:47,320 --> 00:03:51,094
pain points. So that is where a topic model link comes

57
00:03:51,132 --> 00:03:54,738
into play, where it can process millions of reviews

58
00:03:54,834 --> 00:03:58,282
and intelligently pick out topics of interest

59
00:03:58,416 --> 00:04:02,314
for organization like Amazon to process and

60
00:04:02,352 --> 00:04:06,202
take action. So now that we've established a use case,

61
00:04:06,256 --> 00:04:10,022
or the problem statement of why topic modeling is

62
00:04:10,096 --> 00:04:14,298
necessary, we look at why build topic

63
00:04:14,394 --> 00:04:18,206
is the best suitable one, or one of the best suitable ones

64
00:04:18,308 --> 00:04:21,854
in this area right now. So the first

65
00:04:21,892 --> 00:04:25,666
thing about such topic modeling is that it has to

66
00:04:25,688 --> 00:04:29,026
work on unstructured data. So as you saw in

67
00:04:29,048 --> 00:04:32,238
the reviews samples before, the reviews

68
00:04:32,254 --> 00:04:35,522
are unstructured text and they might have

69
00:04:35,576 --> 00:04:39,390
emojis, they might have hashtags, all of these need to be processed.

70
00:04:39,470 --> 00:04:42,630
And bertopic is capable of doing that.

71
00:04:42,780 --> 00:04:46,306
Also, it is capable of taking advantage of transformer

72
00:04:46,338 --> 00:04:49,990
models like Bert and other embedding

73
00:04:50,070 --> 00:04:53,910
related models, and efficiently convert

74
00:04:54,070 --> 00:04:57,690
the textual words into

75
00:04:57,840 --> 00:05:01,674
contextual embeddings, which is a vector format or a numeric

76
00:05:01,802 --> 00:05:05,306
format, right? It offers modularity.

77
00:05:05,418 --> 00:05:09,194
So for example, on the image, right, there are different processes

78
00:05:09,242 --> 00:05:12,170
listed that happen as part of the bird topic technique,

79
00:05:12,250 --> 00:05:15,666
and embeddings is one such process. Dimensionality reduction is

80
00:05:15,688 --> 00:05:18,914
another process followed by clustering. All these three

81
00:05:18,952 --> 00:05:22,846
processes can easily be interchanged by the most advanced

82
00:05:22,878 --> 00:05:26,402
or the state of the art algorithm, which is there out there.

83
00:05:26,536 --> 00:05:30,214
For example, if we take the example of UMAP for

84
00:05:30,252 --> 00:05:33,894
dimensionality reduction, if you're not satisfied with UMAP and

85
00:05:33,932 --> 00:05:37,314
you want to try out something like PCA, which is principal component

86
00:05:37,362 --> 00:05:40,706
analysis for dimensionality reduction, that can be used as

87
00:05:40,748 --> 00:05:44,602
well. And for now, the state of the art for clustering seems

88
00:05:44,656 --> 00:05:47,674
to be HDB scan. But a year from now or

89
00:05:47,712 --> 00:05:51,846
even two months from now, you never know. A clustering

90
00:05:51,878 --> 00:05:55,306
algorithm, which is far more advanced, comes up and you just have to plug

91
00:05:55,338 --> 00:05:58,970
and play the HDB scan, like replace HDB scan

92
00:05:59,050 --> 00:06:02,586
with the latest algorithm, and the whole build

93
00:06:02,618 --> 00:06:06,330
technique will work seamlessly as expected,

94
00:06:06,490 --> 00:06:10,130
right? And that is why one of the main crucial aspects of

95
00:06:10,200 --> 00:06:14,034
this technique is that new advancements in clustering can be adapted very

96
00:06:14,072 --> 00:06:17,774
easily. And the improved version of TFIDF,

97
00:06:17,822 --> 00:06:21,234
which is the CTFIDF extraction of topic

98
00:06:21,282 --> 00:06:24,770
representation, has been used for the weighting scheme

99
00:06:24,930 --> 00:06:28,674
in this particular technique. And that is also one of the motivational

100
00:06:28,722 --> 00:06:33,286
factors to use per topic. The CTF IDF

101
00:06:33,398 --> 00:06:36,934
works quite well in extracting topic representations

102
00:06:37,062 --> 00:06:40,486
from clusters of documents without focusing

103
00:06:40,518 --> 00:06:44,366
on centroid based extraction, which again, as you may

104
00:06:44,388 --> 00:06:48,734
be aware, has its own share of problems moving

105
00:06:48,772 --> 00:06:52,714
to the end to end flow. What are the different parts of this whole picture

106
00:06:52,762 --> 00:06:57,154
of bird topic. So we pick up undokenized reviews and

107
00:06:57,192 --> 00:07:00,478
then we convert them into a numerical format in the vectorization

108
00:07:00,574 --> 00:07:04,302
process using sentence transformers or count vectorizer

109
00:07:04,366 --> 00:07:08,274
or such technique. And then we do dimensionality

110
00:07:08,322 --> 00:07:12,258
reduction which reduces this high dimensional

111
00:07:12,354 --> 00:07:15,766
vectors into smaller dimensions so that

112
00:07:15,788 --> 00:07:19,858
it can be processed by the clustering algorithms. We will be looking at clustering

113
00:07:19,874 --> 00:07:23,402
and hdb scan in detail and Jaspar will be presenting that

114
00:07:23,456 --> 00:07:26,662
in a short while. And the last part is the importance

115
00:07:26,726 --> 00:07:31,162
of words or the representation with term frequency and inverse document frequency

116
00:07:31,226 --> 00:07:35,630
that is also taken care. And finally we get the required topics.

117
00:07:36,370 --> 00:07:39,982
So a quick preview on clustering over

118
00:07:40,036 --> 00:07:43,760
here. So, clustering, you may

119
00:07:44,130 --> 00:07:47,810
have heard of a famous clustering algorithm called k means, right?

120
00:07:47,880 --> 00:07:51,842
It allows you to select how many clusters you would like and forces every

121
00:07:51,896 --> 00:07:55,374
single point to be in a clustering so that there are no outliers

122
00:07:55,422 --> 00:07:59,622
at all. But in real life situations that is not possible. Also, K means

123
00:07:59,676 --> 00:08:03,154
assumes that all the data points form a neat gaussian

124
00:08:03,202 --> 00:08:06,646
sphere or a gaussian circle, right? That is

125
00:08:06,668 --> 00:08:10,650
also not the case. And the shape of these clusters may be different in

126
00:08:10,720 --> 00:08:14,780
real world. And that is where httpscan is quite

127
00:08:15,230 --> 00:08:19,420
popular and quite effective. There are other algorithms like

128
00:08:19,950 --> 00:08:23,606
agglomerative clustering as well, beg your pardon,

129
00:08:23,798 --> 00:08:27,354
which can take care of these things. But yeah, it depends

130
00:08:27,402 --> 00:08:31,262
on how you plug and play these algorithms and see what works

131
00:08:31,316 --> 00:08:34,370
best in your use case.

132
00:08:34,520 --> 00:08:37,826
So let's jump into the hands on part

133
00:08:38,008 --> 00:08:41,010
where we have the data set description.

134
00:08:41,430 --> 00:08:44,834
So here we pick up the Amazon Alexa review data

135
00:08:44,872 --> 00:08:49,090
set, which is available on Kaggle. And this data set has five columns.

136
00:08:49,170 --> 00:08:52,546
And for us, the verified reviews contains the textual

137
00:08:52,578 --> 00:08:55,622
reviews. And that is the column that we will be working

138
00:08:55,676 --> 00:08:59,254
with today. So without further ado,

139
00:08:59,302 --> 00:09:03,690
let's jump to the Jupyter notebook on Google Collab.

140
00:09:04,430 --> 00:09:08,314
All right, so this is the Kaggle website where

141
00:09:08,352 --> 00:09:11,830
the data source data set is available. And this

142
00:09:11,840 --> 00:09:15,582
is how I have retrieved the data. Now don't worry, this notebook is available

143
00:09:15,636 --> 00:09:19,262
on GitHub for your perusal later

144
00:09:19,316 --> 00:09:22,606
on after discussion. So first I get the data, and this is

145
00:09:22,628 --> 00:09:26,562
how the data looks after I feed it into a pandas data

146
00:09:26,616 --> 00:09:30,494
frame. And we are focused on this verified reviews

147
00:09:30,622 --> 00:09:34,878
column. And then we install build topic and again check

148
00:09:35,064 --> 00:09:39,154
our reviews data frame. And this is where the magic

149
00:09:39,202 --> 00:09:43,766
happens. So first we instantiate a vectorizing model,

150
00:09:43,948 --> 00:09:47,862
vectorizer model, which basically allows us to define certain

151
00:09:47,916 --> 00:09:51,154
parameters on what are the ngrams

152
00:09:51,202 --> 00:09:55,146
and certain configurations we can declare over here. And this

153
00:09:55,168 --> 00:09:58,474
is where we are declaring the build topic model

154
00:09:58,672 --> 00:10:02,382
with what is the language and certain config options

155
00:10:02,436 --> 00:10:05,742
that are available in the documentation. And yeah,

156
00:10:05,796 --> 00:10:09,194
this is where on line 19 we do the fit function fit transform

157
00:10:09,322 --> 00:10:12,058
gets us basically the topics and the probabilities.

158
00:10:12,234 --> 00:10:16,162
So moving ahead, this is how our topics look

159
00:10:16,216 --> 00:10:19,870
like. So basically the first most topic

160
00:10:19,950 --> 00:10:23,266
which is popular is 284 occurrences of

161
00:10:23,288 --> 00:10:26,978
this topic is Alexa and love and then echo

162
00:10:26,994 --> 00:10:30,354
dot and music and smart hub. All these are the topics

163
00:10:30,402 --> 00:10:34,360
that have been generated organically from our data set.

164
00:10:35,690 --> 00:10:39,494
And then over here we have the bertopic distance

165
00:10:39,542 --> 00:10:42,954
map. So all these circles actually are representation of

166
00:10:42,992 --> 00:10:46,726
topics. So for example, this circle refers to topic

167
00:10:46,758 --> 00:10:50,154
30 where the keywords are hub and hue and plus and

168
00:10:50,192 --> 00:10:53,566
here if you go here, this is topic 15 which talks about hulu and

169
00:10:53,588 --> 00:10:57,418
streaming. And the one next to it which is topic ten talks

170
00:10:57,434 --> 00:11:01,360
about stick and fire stick. So we can also zoom in

171
00:11:01,970 --> 00:11:05,394
to see why these topics are so close to each

172
00:11:05,432 --> 00:11:08,434
other. So as we can see,

173
00:11:08,472 --> 00:11:11,986
there are two different topics over here. The first one is

174
00:11:12,168 --> 00:11:15,506
about fastic and the second one is about streaming which talks

175
00:11:15,538 --> 00:11:18,998
about similar topics of interest. And also

176
00:11:19,164 --> 00:11:22,950
to wrap this up, basically the distance between

177
00:11:23,020 --> 00:11:26,262
these circles actually indicate how close or how far

178
00:11:26,316 --> 00:11:29,834
or how similar or how dissimilar one topic is

179
00:11:29,952 --> 00:11:34,742
to the other. There is also a hierarchical representation

180
00:11:34,806 --> 00:11:38,586
of all the topics that have been generated. And here if you

181
00:11:38,608 --> 00:11:42,094
can see so the easy setup or easy

182
00:11:42,292 --> 00:11:45,566
set is one of the topics that got

183
00:11:45,588 --> 00:11:49,562
picked up. And if you see over here, all these related

184
00:11:49,626 --> 00:11:52,962
topics are listed together to form the

185
00:11:53,016 --> 00:11:56,242
bigger hierarchy. So moving

186
00:11:56,296 --> 00:12:00,894
ahead, we have this final initialization of the topic word scores

187
00:12:01,022 --> 00:12:04,990
where we actually display what are the most frequently occurring

188
00:12:05,070 --> 00:12:08,834
topics and within those topics, what are the different keywords

189
00:12:08,882 --> 00:12:12,198
and what is the frequency of their occurrence. So over here

190
00:12:12,284 --> 00:12:16,130
you can see Alexa without doubt is the most popular topic.

191
00:12:16,210 --> 00:12:20,070
And one of the interesting thing that comes out of this is this gift topic,

192
00:12:20,150 --> 00:12:23,722
topic four which talks about love, gift and bot which says

193
00:12:23,776 --> 00:12:27,514
that this product has been purchased as a gift by

194
00:12:27,552 --> 00:12:31,550
customers to their friends and family and that is also being captured in

195
00:12:31,620 --> 00:12:34,862
topic number four. So that's about

196
00:12:34,916 --> 00:12:38,650
it on the hands on of bird topic.

197
00:12:38,810 --> 00:12:42,206
And these code snippets and notebooks are available on

198
00:12:42,228 --> 00:12:46,370
GitHub for your reference. I will now pass it on to Jaspal for

199
00:12:46,440 --> 00:12:50,610
a detailed overview on httpscan over to you Jaspal.

200
00:12:51,510 --> 00:12:55,166
Thank you abhinam for your explanation on build

201
00:12:55,198 --> 00:12:59,574
topics. Now get ready with your swimsuits everyone

202
00:12:59,772 --> 00:13:03,314
because we are going to go and take a deep

203
00:13:03,362 --> 00:13:05,590
dive into HDB scan.

204
00:13:06,090 --> 00:13:09,574
So what exactly is HDB

205
00:13:09,622 --> 00:13:12,794
scan? If you look at the full form,

206
00:13:12,992 --> 00:13:17,222
it says hierarchical density based spatial clustering

207
00:13:17,286 --> 00:13:20,862
of applications with noise. Quite a mouthful, right?

208
00:13:20,996 --> 00:13:24,446
Basically what I'm saying is HDB scan is

209
00:13:24,468 --> 00:13:27,998
a long acronym, and in addition to that,

210
00:13:28,084 --> 00:13:32,094
it's a clustering algorithm. To be exact, it's a density based

211
00:13:32,132 --> 00:13:35,618
clustering algorithm. If we want to know what exactly

212
00:13:35,704 --> 00:13:39,298
is HDB scan, we would want to know what is

213
00:13:39,384 --> 00:13:42,882
DB scan. So on the right hand side of your screen,

214
00:13:43,016 --> 00:13:46,582
you find a list of data points scattered around

215
00:13:46,636 --> 00:13:50,242
the graph. You also see a small circles

216
00:13:50,386 --> 00:13:53,480
encircling the data points.

217
00:13:54,250 --> 00:13:58,118
Now that's what DBScan does. It creates

218
00:13:58,214 --> 00:14:01,260
small circle around every data point.

219
00:14:01,630 --> 00:14:05,210
And if there is another data

220
00:14:05,280 --> 00:14:09,530
point within that small circle, it traverses

221
00:14:09,970 --> 00:14:13,934
and it traverses as much as possible

222
00:14:14,132 --> 00:14:17,758
until it finds like next data point,

223
00:14:17,844 --> 00:14:20,446
as long as it finds next data point.

224
00:14:20,628 --> 00:14:25,154
Now what it does is basically it

225
00:14:25,192 --> 00:14:28,386
helps us to find high density regions inside

226
00:14:28,488 --> 00:14:31,774
a data set like you see on the graph

227
00:14:31,822 --> 00:14:35,506
here, where there are a lot of data points close to

228
00:14:35,528 --> 00:14:38,946
each other. The circles, there are a lot of circles,

229
00:14:38,978 --> 00:14:42,950
you see, and that is the result of DB scan algorithm.

230
00:14:43,450 --> 00:14:47,058
It's a good algorithm and it is quite robust,

231
00:14:47,154 --> 00:14:50,374
flexible, and it's quite like outlier

232
00:14:50,422 --> 00:14:53,990
resistant as well. We don't need to define

233
00:14:54,150 --> 00:14:57,210
any predefined number of clusters in this,

234
00:14:57,280 --> 00:15:00,766
which is a really good advantage. But it has got a couple

235
00:15:00,788 --> 00:15:04,186
of problems. Like you need to define

236
00:15:04,218 --> 00:15:08,046
the radius of the circle. Now it is okay

237
00:15:08,148 --> 00:15:13,506
if you have already seen the data set, but what

238
00:15:13,528 --> 00:15:17,342
if you have not reduced the dimensionality

239
00:15:17,406 --> 00:15:20,994
and then not seen the data sets? How do you know what should

240
00:15:21,032 --> 00:15:23,730
be the radius of the circle?

241
00:15:24,230 --> 00:15:28,550
So that's one of the problem. And the other is it's a bit slow.

242
00:15:29,130 --> 00:15:32,230
So what if there was a mechanism?

243
00:15:34,010 --> 00:15:38,198
If we don't need to define

244
00:15:38,214 --> 00:15:41,178
the radius of the circle like this,

245
00:15:41,344 --> 00:15:45,990
no fixed radius, but at the same time we could identify

246
00:15:46,150 --> 00:15:50,300
dense regions. This would help us to be fast.

247
00:15:50,690 --> 00:15:53,920
And that's what hdb scan does

248
00:15:55,010 --> 00:15:58,414
here. Now we have

249
00:15:58,452 --> 00:16:02,014
got an old friend in KNN algorithm. We take

250
00:16:02,132 --> 00:16:04,770
help from this old friend of ours.

251
00:16:05,270 --> 00:16:08,734
Now, KNN algorithm is basically identifying

252
00:16:08,782 --> 00:16:12,114
its nearest neighbors. So we define a

253
00:16:12,152 --> 00:16:16,482
k, and using that KNN algorithm,

254
00:16:16,626 --> 00:16:20,550
you draw a circle around it, and the last data

255
00:16:20,620 --> 00:16:23,762
point in that circle becomes the radius

256
00:16:23,826 --> 00:16:27,240
from the center of that circle, which is the actual data point.

257
00:16:27,630 --> 00:16:32,794
Now that is the core distance. And when

258
00:16:32,832 --> 00:16:36,490
we define this minimum number of point in a cluster,

259
00:16:37,150 --> 00:16:40,794
we will find out that at some

260
00:16:40,832 --> 00:16:44,222
places your cluster size is huge,

261
00:16:44,356 --> 00:16:47,822
like you see on the right hand side, the cluster size for the green

262
00:16:47,876 --> 00:16:50,974
circle is really, really high. On the left hand side,

263
00:16:51,012 --> 00:16:54,626
the blue circle is small because the data points are

264
00:16:54,648 --> 00:16:57,140
closer to each other. Now,

265
00:16:58,710 --> 00:17:03,022
this distance between two clusters is defined

266
00:17:03,086 --> 00:17:07,586
by another distance called mutual reachability distance.

267
00:17:07,778 --> 00:17:11,714
To explain this, we need to imagine a scenario.

268
00:17:11,842 --> 00:17:15,158
Imagine yourself with your friend inside a

269
00:17:15,164 --> 00:17:18,200
park with a lot of other people. Now,

270
00:17:18,570 --> 00:17:22,106
one thing we know that some human beings are

271
00:17:22,128 --> 00:17:25,530
short, some human beings are tall, and as and when their

272
00:17:25,600 --> 00:17:29,210
arm size are different as well. Now imagine the arm size

273
00:17:29,280 --> 00:17:33,680
being the core distance of a human being. Now, another human being

274
00:17:34,770 --> 00:17:38,394
within that arm's distance is your mutual

275
00:17:38,442 --> 00:17:41,946
relatability distance. Reachability distance. And that's

276
00:17:41,978 --> 00:17:45,682
the concept, basically that how many clusters are

277
00:17:45,736 --> 00:17:49,054
within your radius, and that's where dense

278
00:17:49,102 --> 00:17:51,998
regions are formed. If they are outside of your radius,

279
00:17:52,094 --> 00:17:55,890
then they are away from you. And this mutual reachability

280
00:17:55,970 --> 00:18:00,390
distance helps us to identify the dense regions.

281
00:18:01,130 --> 00:18:05,270
Now, once we have this mutual reachability distance,

282
00:18:06,410 --> 00:18:09,926
we find out, like how many items

283
00:18:09,958 --> 00:18:13,990
are closer to each other and by what meaning,

284
00:18:14,070 --> 00:18:17,706
by what relation. Now, when we

285
00:18:17,728 --> 00:18:19,850
are at a park,

286
00:18:21,090 --> 00:18:25,342
you have your arm's length to identify who is

287
00:18:25,396 --> 00:18:28,974
closer to you or who is not. Now let's take that

288
00:18:29,012 --> 00:18:31,680
park example to a different place.

289
00:18:32,130 --> 00:18:35,970
Let's imagine a room full of objects

290
00:18:36,470 --> 00:18:39,250
and they are scattered around. Now,

291
00:18:39,320 --> 00:18:43,406
you have a string and you want to tie

292
00:18:43,438 --> 00:18:47,158
these objects to each other, but using shortest possible

293
00:18:47,244 --> 00:18:49,960
paths. So how would you do that?

294
00:18:50,810 --> 00:18:54,534
Okay, you'll try to do it by

295
00:18:54,572 --> 00:18:57,914
calculating the distance between these object. And now you

296
00:18:57,952 --> 00:19:01,098
have this mutual reachability distance defined against

297
00:19:01,184 --> 00:19:05,306
each object and each cluster. So there

298
00:19:05,328 --> 00:19:08,774
is an algorithm called minimum spanning tree

299
00:19:08,822 --> 00:19:11,998
algorithm, which finds the shortest possible

300
00:19:12,084 --> 00:19:17,194
path for you. And that minimum spanning tree algorithm

301
00:19:17,322 --> 00:19:20,878
is really useful to identify the density and the hierarchy of the

302
00:19:20,884 --> 00:19:25,118
cluster. Now, you might ask the density. Okay, you have the mutual

303
00:19:25,214 --> 00:19:28,718
reachability distance as the minimum, but how the hierarchy?

304
00:19:28,894 --> 00:19:32,386
For that, I need you to imagine another set.

305
00:19:32,488 --> 00:19:36,614
Now imagine that you have tied all the object with

306
00:19:36,812 --> 00:19:40,578
the minimum span using the minimum spanning tree algorithm.

307
00:19:40,754 --> 00:19:44,390
Now, what you do next is pick up these objects,

308
00:19:44,890 --> 00:19:48,518
and here you see a hierarchy is formed,

309
00:19:48,614 --> 00:19:49,980
something like this.

310
00:19:52,350 --> 00:19:55,802
The objects that are closer to each other come closer and form

311
00:19:55,856 --> 00:19:59,354
a cluster. The objects that are farther from each other

312
00:19:59,472 --> 00:20:02,350
are lying below in the hierarchy,

313
00:20:02,690 --> 00:20:06,366
and that's the way the

314
00:20:06,388 --> 00:20:10,160
spatial clustering happens. Now, let's say

315
00:20:12,370 --> 00:20:16,066
you got the hierarchy, but you still are not really happy with the

316
00:20:16,088 --> 00:20:19,250
number of clusters because there are a lot of hierarchy there.

317
00:20:19,400 --> 00:20:23,534
So how do you define what should be your optimum

318
00:20:23,582 --> 00:20:27,046
length of the clusters? So for that basically you need to

319
00:20:27,068 --> 00:20:30,374
define how many number of objects do you need in

320
00:20:30,412 --> 00:20:33,954
one cluster? And once you define that, you get a pruned

321
00:20:34,002 --> 00:20:37,754
tree, something like this. So this tree basically is

322
00:20:37,792 --> 00:20:42,106
the hierarchy of the objects where you

323
00:20:42,128 --> 00:20:45,706
have removed the objects where the clusters are

324
00:20:45,728 --> 00:20:50,358
not really big, and I

325
00:20:50,384 --> 00:20:53,278
have put it across a lambda value.

326
00:20:53,444 --> 00:20:57,326
Now you'll ask what is the lambda value? Now once

327
00:20:57,348 --> 00:21:01,210
you have pruned those objects, you would want to capture

328
00:21:01,290 --> 00:21:04,654
the clusters which are most meaningful

329
00:21:04,702 --> 00:21:08,958
or reliable. So how do you do that is by using the stability

330
00:21:09,054 --> 00:21:12,670
score. Now, stability score is calculated

331
00:21:12,750 --> 00:21:16,070
for each cluster in the hierarchy based on two factors.

332
00:21:16,650 --> 00:21:20,454
One is the density of the points and the other

333
00:21:20,492 --> 00:21:24,386
is the persistence, basically how long it lasts

334
00:21:24,418 --> 00:21:28,074
in the hierarchy. Now here when you see the y

335
00:21:28,112 --> 00:21:32,170
axis, it's the stability score, which is the highest

336
00:21:33,550 --> 00:21:37,178
for which cluster. So as you can see,

337
00:21:37,344 --> 00:21:41,034
the first two are clearly highest stability scores

338
00:21:41,162 --> 00:21:44,734
and the width of the cluster and

339
00:21:44,772 --> 00:21:48,174
the color gives you the number

340
00:21:48,212 --> 00:21:51,354
of points in that cluster. So the third cluster

341
00:21:51,402 --> 00:21:55,570
which we choose is the green one on the right hand side, which is quite

342
00:21:55,720 --> 00:21:59,426
stable as well as it has a lot of points in it.

343
00:21:59,608 --> 00:22:03,106
Now these are the clusters that are being chosen based on the

344
00:22:03,128 --> 00:22:06,810
stability score. Now once we have chosen these clustering,

345
00:22:06,910 --> 00:22:11,014
this is how the final clustering look like. There would be a few points which

346
00:22:11,052 --> 00:22:13,800
you can see are grayish in color.

347
00:22:15,130 --> 00:22:20,330
It might be a little difficult, but yeah, they are there. So somewhere

348
00:22:20,830 --> 00:22:24,182
near the greens, on the edges of the greens, those gray

349
00:22:24,246 --> 00:22:27,674
points are the outliers. But we

350
00:22:27,712 --> 00:22:31,702
are able to find highly well defined clusters

351
00:22:31,766 --> 00:22:35,820
here. Now, just a recap of what we did.

352
00:22:36,530 --> 00:22:40,014
We transformed the spaces as per the density we

353
00:22:40,052 --> 00:22:43,822
identified, which is the highest density region.

354
00:22:43,966 --> 00:22:47,118
Then we built a minimum spanning tree

355
00:22:47,294 --> 00:22:50,850
and then we constructed a cluster hierarchy.

356
00:22:51,510 --> 00:22:55,086
Then we condensed this hierarchy based on minimum cluster size,

357
00:22:55,208 --> 00:22:59,282
and then we extracted stable clustering from the condensed

358
00:22:59,346 --> 00:23:03,218
tree. So that's what HDB scan

359
00:23:03,314 --> 00:23:06,754
does. Now let's

360
00:23:06,802 --> 00:23:09,926
see what is the performance of the HDB can.

361
00:23:10,118 --> 00:23:13,914
As you can see in this

362
00:23:13,952 --> 00:23:17,926
graph, HDB can performs better than DB

363
00:23:17,958 --> 00:23:21,606
scan. As you see, on the blue

364
00:23:21,638 --> 00:23:25,182
line is the DB scan and the green line

365
00:23:25,236 --> 00:23:28,894
is the HDB scan. On the x axis you see

366
00:23:28,932 --> 00:23:32,990
the number of data points, and on the y axis, the time taken

367
00:23:33,140 --> 00:23:35,870
to perform clustering activity.

368
00:23:36,210 --> 00:23:39,890
Now, as you see, past cluster is not really

369
00:23:39,960 --> 00:23:43,682
that good with higher

370
00:23:43,816 --> 00:23:47,602
set of data points, but DB can is better than that

371
00:23:47,656 --> 00:23:51,030
HDB scan, even better. And then the last two are the K means.

372
00:23:51,100 --> 00:23:55,080
K means is the fastest, no doubt about it. But it's not really

373
00:23:56,090 --> 00:23:58,810
that great in giving you better cluster.

374
00:23:59,390 --> 00:24:03,290
Now let's see what are the strengths and weaknesses of

375
00:24:03,360 --> 00:24:06,780
HDB scan? HDB scan basically

376
00:24:07,310 --> 00:24:11,470
focuses on high density clustering and as a result

377
00:24:11,540 --> 00:24:15,710
it reduces noise problem and

378
00:24:15,780 --> 00:24:19,406
the minimum cluster size parameter can be set and as a

379
00:24:19,428 --> 00:24:22,962
result it's relatively fast than

380
00:24:23,016 --> 00:24:26,194
others as we see in the performance. And it

381
00:24:26,232 --> 00:24:29,474
does have problems in handling large

382
00:24:29,512 --> 00:24:33,198
amounts of data. But you can use cumL

383
00:24:33,294 --> 00:24:37,654
HDB scan which uses GPU acceleration to

384
00:24:37,692 --> 00:24:41,042
perform this, and I've given the link in the description

385
00:24:41,106 --> 00:24:45,046
as well, so you can click on it and look

386
00:24:45,068 --> 00:24:50,342
at it in your free time. Now knitting

387
00:24:50,406 --> 00:24:54,170
it back to what we discussed previously with bird topic.

388
00:24:54,830 --> 00:25:00,134
So basically we know that every component

389
00:25:00,182 --> 00:25:03,406
of build topic is modular and it's scalable and

390
00:25:03,428 --> 00:25:06,842
it's also flexible. If we are not using HTTPscan,

391
00:25:06,906 --> 00:25:10,686
probably we might have used some other algorithm and

392
00:25:10,708 --> 00:25:14,478
as a result clustering is modular, which gives you a

393
00:25:14,484 --> 00:25:16,480
really good advantage over it.

394
00:25:17,010 --> 00:25:21,102
And the assumption here is basically every

395
00:25:21,156 --> 00:25:24,446
document contains one topic. So when we are

396
00:25:24,468 --> 00:25:28,694
giving each topic to

397
00:25:28,732 --> 00:25:32,630
analyze, we have only one topic for every document.

398
00:25:33,130 --> 00:25:36,774
And also there can

399
00:25:36,812 --> 00:25:41,050
be chat CPT might

400
00:25:41,120 --> 00:25:44,774
be impacting like topic modeling,

401
00:25:44,822 --> 00:25:47,770
but yeah, I mean it does the job, Chat GPT,

402
00:25:48,510 --> 00:25:52,234
there are a bit of biases,

403
00:25:52,362 --> 00:25:56,346
ethical issues and so on and so forth. But topic

404
00:25:56,378 --> 00:25:59,870
modeling is just topic modeling. You identify which is the important

405
00:25:59,940 --> 00:26:03,154
topic and you get it. So yeah, it's really

406
00:26:03,192 --> 00:26:07,006
useful for that and it's

407
00:26:07,038 --> 00:26:10,894
easier to operationalize as well. And there are a lot of cloud vendors

408
00:26:11,022 --> 00:26:14,770
that provides this build topic

409
00:26:15,770 --> 00:26:19,494
and you can do it using really

410
00:26:19,612 --> 00:26:23,990
simple architecture,

411
00:26:24,410 --> 00:26:28,600
using sort of lambda functions and stuff. And it's really quick.

412
00:26:28,970 --> 00:26:32,854
So I have given some of the references and the

413
00:26:32,892 --> 00:26:36,390
resources as well. So yeah, that's about

414
00:26:36,460 --> 00:26:40,254
it. And thank you so much for joining my

415
00:26:40,292 --> 00:26:44,480
session, hope you enjoyed it. Hope you took some of

416
00:26:44,930 --> 00:26:47,930
the material with you, which is useful,

417
00:26:48,010 --> 00:26:51,246
which might be useful for you in the future for your

418
00:26:51,268 --> 00:26:54,810
further research. And thank you so much for joining,

419
00:26:54,890 --> 00:26:55,518
see you later,

