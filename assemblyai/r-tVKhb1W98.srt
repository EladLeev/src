1
00:00:23,690 --> 00:00:27,638
Hi, welcome to Comp Kube native Con. This is

2
00:00:27,724 --> 00:00:31,046
really fun. I get to share with you horizontal auto scaling with

3
00:00:31,068 --> 00:00:34,854
Kubernetes. Let's dive in. Here's the part where I tell you I am

4
00:00:34,892 --> 00:00:37,430
definitely going to post the slides on my site tonight.

5
00:00:37,930 --> 00:00:41,734
I've changed similar speakers and it's never worked out very well for me either.

6
00:00:41,852 --> 00:00:45,782
So let's go to robridge.org where we can find the slides online right

7
00:00:45,836 --> 00:00:49,734
now. We'll go to robridge.org and

8
00:00:49,772 --> 00:00:53,418
click click here on presentations and we can see horizontal autoscaling

9
00:00:53,434 --> 00:00:56,720
with Kubernetes. The slides are online right now.

10
00:00:57,170 --> 00:01:00,366
While we're here on robrich.org, let's click on about me and see some

11
00:01:00,388 --> 00:01:04,254
of the things that I've done recently. I'm a developer advocate for

12
00:01:04,292 --> 00:01:08,146
Jetpackio. If you're struggling with Kubernetes, I would love to learn with you. I'm also

13
00:01:08,168 --> 00:01:12,034
a Microsoft MVP and MCT, a Docker captain and

14
00:01:12,072 --> 00:01:16,286
a friend of Redgate. AZ Givecamp is really fun. AZ Givecamp brings

15
00:01:16,318 --> 00:01:19,986
volunteer developers together with charities to build free software.

16
00:01:20,098 --> 00:01:23,874
We start building software Friday after work. Sunday afternoon, we deliver

17
00:01:23,922 --> 00:01:27,686
completed software to charities. Sleep is optional, caffeine provided. If you're in

18
00:01:27,708 --> 00:01:31,158
Phoenix, come join us for the next AZ give camp. Or if you'd like a

19
00:01:31,164 --> 00:01:34,666
give camp close to where you live, hit me up here at the conference or

20
00:01:34,768 --> 00:01:38,330
on email or Twitter. And let's get a gift camp in your neighborhood too.

21
00:01:38,480 --> 00:01:41,466
Some of the other things that I've done I do a lot with Kubernetes and

22
00:01:41,488 --> 00:01:44,926
Docker and one of the things I'm the most proud of I

23
00:01:44,948 --> 00:01:48,382
replied to a Net Rocks podcast episode they read my comments on here.

24
00:01:48,436 --> 00:01:49,710
They sent me a mug.

25
00:01:51,730 --> 00:01:55,250
So let's dig into horizontal auto scaling with Kubernetes.

26
00:01:55,590 --> 00:01:59,554
We talked about this guy first. Let's talk about autoscaling now.

27
00:01:59,592 --> 00:02:03,294
As we talk about scaling, part of what we're trying to accomplish

28
00:02:03,422 --> 00:02:06,902
is to meet the capacity when we need it and to save money

29
00:02:06,956 --> 00:02:09,670
when we don't. That's the nature of scaling.

30
00:02:10,170 --> 00:02:14,054
When we talk about scaling, we can talk about both horizontal autoscaling and

31
00:02:14,092 --> 00:02:17,714
vertical scaling. With horizontal scaling, we're adding

32
00:02:17,762 --> 00:02:20,646
more items to be able to reach that capacity.

33
00:02:20,758 --> 00:02:24,790
With vertical scaling, we're increasing the size of each item

34
00:02:24,870 --> 00:02:27,050
to be able to reach that capacity.

35
00:02:27,710 --> 00:02:30,970
So dialing in some more. With vertical scaling,

36
00:02:31,050 --> 00:02:33,978
we're increasing the size of each item.

37
00:02:34,154 --> 00:02:38,014
Now this might be great for things that need state where we

38
00:02:38,052 --> 00:02:41,854
don't want to manage synchronization, maybe a database by

39
00:02:41,892 --> 00:02:46,138
comparison with horizontal scaling, we are increasing the number of items.

40
00:02:46,234 --> 00:02:49,646
Now we need to coordinate between them. We may need to populate

41
00:02:49,678 --> 00:02:52,866
data into a new node. We may need to ensure that

42
00:02:52,888 --> 00:02:55,960
there's one main node and that they coordinate together.

43
00:02:56,570 --> 00:02:59,734
This synchronization isn't necessary if we're using a

44
00:02:59,772 --> 00:03:02,550
stateless service, perhaps a web server.

45
00:03:03,370 --> 00:03:07,250
So horizontal and vertical scaling.

46
00:03:07,410 --> 00:03:11,302
Now how did we get here? What are we building on top of? Whose shoulders

47
00:03:11,366 --> 00:03:14,346
are we standing on top of? Well, back in the old day,

48
00:03:14,448 --> 00:03:17,834
scaling was hard, it was slow, so we would

49
00:03:17,872 --> 00:03:21,594
generally over provision. We're provisioning for the traffic

50
00:03:21,642 --> 00:03:25,534
on our peak day. Maybe that's Black Friday, maybe that's Super

51
00:03:25,572 --> 00:03:29,310
Bowl Sunday. Maybe that's when we go viral. But because

52
00:03:29,380 --> 00:03:33,674
we're over provisioning for those worst case scenarios,

53
00:03:33,802 --> 00:03:37,166
on a normal day our machines may sit completely idle.

54
00:03:37,278 --> 00:03:41,006
Now why did we do this? Well, we did this because provisioning

55
00:03:41,118 --> 00:03:44,706
was hard. It might take days or weeks or months

56
00:03:44,888 --> 00:03:48,886
to get approval, buy the hardware, install the content

57
00:03:49,068 --> 00:03:52,182
and install the operating system, then install

58
00:03:52,236 --> 00:03:55,138
our application, plug this into the load balancer.

59
00:03:55,234 --> 00:03:59,058
That's definitely not something that we can do. If we have additional load yesterday

60
00:03:59,154 --> 00:04:02,506
that we need to handle tomorrow, this process may take weeks or

61
00:04:02,528 --> 00:04:05,834
months. So we need to have it all the way done by the time we

62
00:04:05,872 --> 00:04:09,514
reach that peak load. So we're over provisioning to

63
00:04:09,552 --> 00:04:13,070
be able to support the load on those extreme circumstances.

64
00:04:13,490 --> 00:04:16,894
Today we don't need to do that. Today we

65
00:04:16,932 --> 00:04:20,666
buy just what we need. Utility billing with clouds allows

66
00:04:20,698 --> 00:04:24,034
us to scale easily and quickly to meet

67
00:04:24,072 --> 00:04:27,586
the demand. And then when the demand eases, then we

68
00:04:27,608 --> 00:04:30,580
can give those resources back and stop paying them.

69
00:04:31,830 --> 00:04:35,218
Previously we would run our machines mostly idle so that

70
00:04:35,224 --> 00:04:38,962
we had additional capacity available. Today we run our machines

71
00:04:39,026 --> 00:04:43,154
mostly at capacity. 80 or 90% is not uncommon

72
00:04:43,202 --> 00:04:46,310
because we really want to use our hardware most effectively.

73
00:04:46,970 --> 00:04:50,620
So utility billing makes this possible.

74
00:04:51,150 --> 00:04:54,726
Now, all of that scaling applies to any scaling scenario.

75
00:04:54,838 --> 00:04:58,026
Let's apply this specifically to kubernetes. Now in

76
00:04:58,048 --> 00:05:01,306
kubernetes we could talk about scaling a cluster. We're not going to

77
00:05:01,328 --> 00:05:04,538
do that today. But as you grab the slides from robrich.org,

78
00:05:04,634 --> 00:05:08,138
dig into scaling the cluster and that can be a really fun topic.

79
00:05:08,234 --> 00:05:12,078
Today we're going to talk about scaling the workload, talking about

80
00:05:12,164 --> 00:05:15,406
pods. Now that presumes that your cluster is big enough to handle

81
00:05:15,438 --> 00:05:19,006
this. Next up we could talk about vertical scaling

82
00:05:19,038 --> 00:05:22,482
or horizontal scaling. Vertical scaling is definitely interesting.

83
00:05:22,616 --> 00:05:25,990
It's about changing resource limits on our pods to match

84
00:05:26,060 --> 00:05:30,210
increased demand. But today we're going to talk about horizontal

85
00:05:30,290 --> 00:05:34,210
scaling. We're going to talk about increasing the count of pods

86
00:05:34,290 --> 00:05:38,294
to match the demand that we have. So let's dig

87
00:05:38,332 --> 00:05:40,700
into pod scaling. Well,

88
00:05:42,190 --> 00:05:45,738
why isn't this automatic? Why isn't there just a push the button

89
00:05:45,824 --> 00:05:49,386
and now we have scaling in our cluster. Well, it depends a lot on

90
00:05:49,408 --> 00:05:53,022
the workload, in particular, how your workload works and what

91
00:05:53,076 --> 00:05:56,938
metric you're using to measure it. What metric?

92
00:05:57,114 --> 00:06:00,554
Now, we might have a cpu bound workload, in which case we want to scale

93
00:06:00,602 --> 00:06:04,154
based on our cpu. Or we might have an IL bound workload,

94
00:06:04,202 --> 00:06:07,810
in which case we want to scale based on the request current

95
00:06:07,880 --> 00:06:11,486
concurrent request. Or maybe the request queue length.

96
00:06:11,678 --> 00:06:14,898
We may scale based on external factors. Maybe we're looking at

97
00:06:14,904 --> 00:06:18,446
our message bus queue length. Or maybe we're looking at our

98
00:06:18,488 --> 00:06:21,586
load balancer for details of how we should scale.

99
00:06:21,698 --> 00:06:24,694
Or maybe we're looking at latency in a critical function.

100
00:06:24,892 --> 00:06:28,294
Is it taking a long time to log in? Maybe we need to up the

101
00:06:28,332 --> 00:06:32,234
servers associated with our authentication process. Now this

102
00:06:32,272 --> 00:06:35,446
is definitely not an exhaustive list of metrics, but why isn't

103
00:06:35,478 --> 00:06:39,386
this built in? Because it really depends on how our workload works.

104
00:06:39,568 --> 00:06:43,434
If it works based on a cpu scaling metric, and we're scaling based

105
00:06:43,472 --> 00:06:47,098
on I o metrics, then of course we're not going to scale correctly.

106
00:06:47,194 --> 00:06:50,666
Let's take a look at a few use cases. In this case, we chose

107
00:06:50,698 --> 00:06:54,330
to scale based on cpu, but it's an I O bound workload.

108
00:06:54,490 --> 00:06:58,366
Now, because it's an I O bound workload, our system sits

109
00:06:58,398 --> 00:07:01,378
mostly idle as we're waiting for our external data store.

110
00:07:01,544 --> 00:07:04,706
Now in this case, because we're waiting for our external data store,

111
00:07:04,808 --> 00:07:08,726
and well, our machine is idle, the cpu is low,

112
00:07:08,828 --> 00:07:12,546
and our system never discovers that our system is under load. So we're

113
00:07:12,578 --> 00:07:15,430
never going to scale up beyond the minimum number of pods.

114
00:07:16,010 --> 00:07:19,862
Similarly, maybe we're autoscaling based on an I O bound workload,

115
00:07:19,926 --> 00:07:22,730
so we're scaling based on concurrent requests.

116
00:07:23,070 --> 00:07:26,426
And perhaps our application framework limits the number of

117
00:07:26,448 --> 00:07:29,986
concurrent requests, putting back pressure on our load balancer to queue

118
00:07:30,038 --> 00:07:33,774
the incoming requests. And so we only have a certain number

119
00:07:33,812 --> 00:07:37,294
of concurrent requests. So we'll never scale beyond our

120
00:07:37,332 --> 00:07:40,906
normal thing. We'll never scale beyond

121
00:07:40,938 --> 00:07:43,998
our minimum because Kubernetes doesn't

122
00:07:44,014 --> 00:07:47,490
know that our system is under load, we've chosen the wrong metric.

123
00:07:48,550 --> 00:07:52,686
Let's look at a third use case here. We've chosen

124
00:07:52,718 --> 00:07:56,514
to scale based on our service bus

125
00:07:56,632 --> 00:07:59,926
queue length. So if there are messages in our

126
00:07:59,948 --> 00:08:03,890
queue, we're going to scale up additional pods to be able to handle those messages.

127
00:08:03,970 --> 00:08:07,414
Perhaps each message sends an email and then when the

128
00:08:07,452 --> 00:08:10,842
queue length is short, then we'll scale back down

129
00:08:10,896 --> 00:08:14,774
so that we're not using extra resources. In this case we matched

130
00:08:14,822 --> 00:08:17,866
our metric with our business concerns and so

131
00:08:17,888 --> 00:08:19,770
we're able to scale appropriately.

132
00:08:21,070 --> 00:08:24,606
Now in each of these scenarios we looked at mechanisms where we

133
00:08:24,628 --> 00:08:28,346
could choose a metric to scale and in many of the instances we chose

134
00:08:28,378 --> 00:08:32,010
the wrong metric. Not to say that those metrics aren't good for scaling,

135
00:08:32,090 --> 00:08:35,758
but that in those scenarios that's not how that application works.

136
00:08:35,924 --> 00:08:39,742
Now this is definitely not an exhaustive list, but as you look at scaling

137
00:08:39,806 --> 00:08:43,186
you might look to these and other metrics to understand the health of

138
00:08:43,208 --> 00:08:46,390
your system and what your system looks like under load.

139
00:08:47,770 --> 00:08:51,222
So let's take a look at the Kubernetes autoscaler and in particular

140
00:08:51,356 --> 00:08:53,510
let's look at built in metrics.

141
00:08:54,650 --> 00:08:57,874
Our first step in enabling the Kubernetes

142
00:08:57,922 --> 00:09:01,942
autoscale is to enable the metric server. The metric server captures

143
00:09:02,006 --> 00:09:05,274
cpu and memory on all of our pods and

144
00:09:05,312 --> 00:09:08,666
presents that to horizontal autoscale. So let's turn on the

145
00:09:08,688 --> 00:09:11,978
metric server first. Is it on? Let's do a

146
00:09:11,984 --> 00:09:15,342
kubectl top for our pods and in this case across

147
00:09:15,396 --> 00:09:19,086
all namespaces and see if it errors. If it errors, we need to turn it

148
00:09:19,108 --> 00:09:22,282
on. So let's head off to the metric server part of the Kubernetes

149
00:09:22,346 --> 00:09:25,642
project. Go grab the latest release and apply components.

150
00:09:25,706 --> 00:09:29,394
Yaml in our case we're using minicube, so I just enabled the add

151
00:09:29,432 --> 00:09:33,534
on to enable the metric server. Now that we've got the metric server

152
00:09:33,582 --> 00:09:37,234
enabled, let's deploy our workload. Now we do need to customize

153
00:09:37,282 --> 00:09:41,074
our workload ever so slightly to ensure that the autoscale will behave

154
00:09:41,122 --> 00:09:44,754
as expected. We need to have a resource that knows

155
00:09:44,802 --> 00:09:47,586
how to build pods. So a deployment,

156
00:09:47,698 --> 00:09:51,058
a stateful set or a daemon set. In this

157
00:09:51,084 --> 00:09:54,586
case we now have a recipe, a template of how to build pods and

158
00:09:54,608 --> 00:09:57,946
we can scale out as we need to in

159
00:09:57,968 --> 00:10:01,254
our pod definition. We also need to set resource limits.

160
00:10:01,382 --> 00:10:05,022
Kubernetes is going to look to these resource limits to know how much

161
00:10:05,076 --> 00:10:08,734
capacity we have left in that pod before it needs to create another

162
00:10:08,932 --> 00:10:12,046
similar. We need to remove the replica count in

163
00:10:12,068 --> 00:10:15,666
our deployment. The replica count is going to be controlled by the

164
00:10:15,688 --> 00:10:18,290
autoscaler, not by the deployment anymore.

165
00:10:18,870 --> 00:10:22,466
So here's our deployment. Or maybe it's a stateful set or daemon set.

166
00:10:22,568 --> 00:10:26,194
You notice that we've commented out the replicas. This shouldn't be here because

167
00:10:26,232 --> 00:10:29,846
the autoscaler will do it. We've also set limits so that we know how

168
00:10:29,868 --> 00:10:33,398
much it is. So if we're using 100%

169
00:10:33,484 --> 00:10:37,282
of the capacity of this pod, that means we're using half a cpu.

170
00:10:37,426 --> 00:10:41,434
For using more than half a cpu, we're using more than 100% of this

171
00:10:41,472 --> 00:10:45,414
pod. And if we're using a quarter of the cpu, we're using half the capacity

172
00:10:45,462 --> 00:10:49,430
for this pod. Next up, let's build the autoscaler.

173
00:10:49,510 --> 00:10:53,222
Now the autoscaler is going to go grab that metric and

174
00:10:53,376 --> 00:10:56,990
understand across all of the pods if we need to,

175
00:10:57,060 --> 00:11:00,554
then increase the pods to get that average down, or decrease

176
00:11:00,602 --> 00:11:02,720
the count of pods to get that average up.

177
00:11:04,290 --> 00:11:07,854
A horizontal autoscaler now it's going to go check these metrics

178
00:11:07,902 --> 00:11:11,666
every 15 seconds. And once it notices that it needs to make a

179
00:11:11,688 --> 00:11:15,458
change, then it'll make that change, but then it won't make additional changes

180
00:11:15,544 --> 00:11:19,330
for five minutes. Now we can definitely customize these defaults,

181
00:11:19,410 --> 00:11:23,014
but these defaults help us to not overly burden our

182
00:11:23,052 --> 00:11:26,866
system and to reach consensus once we've made a scaling

183
00:11:26,898 --> 00:11:30,950
change. Maybe we need to populate the cache or adjust

184
00:11:31,030 --> 00:11:34,566
our content across all of our available nodes

185
00:11:34,598 --> 00:11:38,710
now. And that may take some time. So once we've reached stasis

186
00:11:38,790 --> 00:11:41,690
now, we can make additional scaling decisions.

187
00:11:42,370 --> 00:11:46,314
So let's create a horizontal autoscaler. Now here with this Kubectl

188
00:11:46,362 --> 00:11:50,314
command, we can just quickly identify the deployment that it's cpu

189
00:11:50,362 --> 00:11:54,094
bound, the target percentage, in this case 50%. And the minimum and

190
00:11:54,132 --> 00:11:57,954
maximum number of pods we should create. Now 50% probably

191
00:11:57,992 --> 00:12:01,426
isn't a good metric. We probably should run more like 80 or 90%,

192
00:12:01,528 --> 00:12:05,246
but this is a good demo. We could also deploy

193
00:12:05,278 --> 00:12:08,630
this as a Kubernetes YamL file.

194
00:12:09,050 --> 00:12:12,562
Now we've identified the deployment that we want to target

195
00:12:12,626 --> 00:12:16,290
and it is a deployment. We've specified the min and max replicas,

196
00:12:16,370 --> 00:12:19,974
so we'll have between one and four pods. And we've identified the

197
00:12:20,012 --> 00:12:22,854
type of thing that we want to do. In this case, it's a resource.

198
00:12:22,902 --> 00:12:26,074
It's cpu or memory. We've chosen cpu and we

199
00:12:26,112 --> 00:12:30,282
say that we have a utilization an average of 50%. Now 50%

200
00:12:30,336 --> 00:12:34,718
is probably low, maybe we want to go 80%. But this is the percent

201
00:12:34,884 --> 00:12:38,286
of the details in our deployment. So here in

202
00:12:38,308 --> 00:12:42,110
our deployment we have 0.5 cpu. So 50%

203
00:12:42,180 --> 00:12:45,290
of our 0.5 cpu would be a quarter of a

204
00:12:45,300 --> 00:12:48,754
cpu. If we go over a quarter of a cpu, it sounds

205
00:12:48,792 --> 00:12:51,758
like we need more pods. We go under a quarter of a cpu,

206
00:12:51,854 --> 00:12:53,490
we'll need less pods.

207
00:12:55,130 --> 00:12:58,262
So let's do this demo. We have

208
00:12:58,316 --> 00:13:01,702
here a deployment that we will

209
00:13:01,756 --> 00:13:05,350
build out. Now this references a pod that is

210
00:13:05,420 --> 00:13:08,662
just a basic exprs server. It has a single

211
00:13:08,716 --> 00:13:12,234
route that will allow us to just grab text. Notice that we've set

212
00:13:12,272 --> 00:13:15,686
our resource limits so that we can auto scale based on those limits.

213
00:13:15,798 --> 00:13:19,034
And we've chosen just the cpu. Now we

214
00:13:19,072 --> 00:13:22,278
could specify other limits if we want, but we want to make sure that we

215
00:13:22,304 --> 00:13:25,530
horizontally autoscale based only on a single metric.

216
00:13:25,690 --> 00:13:29,274
We've also commented out the replicas. We don't want our deployment

217
00:13:29,322 --> 00:13:32,890
to specify the count, we want our autoscale to specify it.

218
00:13:33,060 --> 00:13:36,740
So here's our autoscale and we can see that it

219
00:13:37,190 --> 00:13:40,882
scales based on a deployment. This is the deployment name

220
00:13:41,016 --> 00:13:44,830
and we scaled based on our cpu.

221
00:13:44,910 --> 00:13:47,718
And in this case, because it's a very small application,

222
00:13:47,884 --> 00:13:51,750
we're going to only scale on 5% of that half a cpu.

223
00:13:52,250 --> 00:13:56,066
So first stop is to make sure that our metric server is enabled.

224
00:13:56,178 --> 00:14:00,322
Let's do cubectl top pod

225
00:14:00,386 --> 00:14:03,882
and let's look in all namespaces. And it looks like yes, in this case

226
00:14:03,936 --> 00:14:07,722
our metric server is enabled. We get the cpu and memory for

227
00:14:07,776 --> 00:14:10,838
all of our pods whether we're using a horizontal autoscale or

228
00:14:10,864 --> 00:14:15,280
not. Great. Cubectl apply F

229
00:14:15,890 --> 00:14:20,158
cpu deploy. We've got our deployment in place

230
00:14:20,324 --> 00:14:24,450
and let's also grab our horizontal autoscaler.

231
00:14:25,510 --> 00:14:29,566
Now when we first spin

232
00:14:29,598 --> 00:14:33,278
up our horizontal autoscaler our value will be unknown.

233
00:14:33,374 --> 00:14:36,994
It takes 15 seconds for us to do a lap to go ask the

234
00:14:37,032 --> 00:14:41,126
pods what their resources are. And so for those 15 seconds we

235
00:14:41,148 --> 00:14:44,902
don't know if we need to do anything. So let's take a look.

236
00:14:44,956 --> 00:14:48,458
Oh it looks like we have 31%. So it's going to go create a

237
00:14:48,464 --> 00:14:51,382
whole bunch of pods to match that capacity.

238
00:14:51,526 --> 00:14:55,898
Now I'm surprised that it got to 31% straight away. Let's change

239
00:14:55,984 --> 00:15:00,910
this metric then to be 25%

240
00:15:01,060 --> 00:15:03,120
and apply this again.

241
00:15:07,490 --> 00:15:11,326
Now we probably have to deploy our

242
00:15:11,508 --> 00:15:13,620
horizontal auto scaler again.

243
00:15:15,670 --> 00:15:19,234
Yes. So now we should only need two pods. Now we

244
00:15:19,272 --> 00:15:22,590
were in that spot where we were automatically checking.

245
00:15:22,750 --> 00:15:26,034
We're in that spot where we've made an adjustment. And so now

246
00:15:26,072 --> 00:15:30,114
it'll take a while for this to calm down, but let's

247
00:15:30,162 --> 00:15:33,846
generate some load on our system and see if that changes things. So in

248
00:15:33,868 --> 00:15:38,022
this case I have a busy box thing where I'm just curling into

249
00:15:38,156 --> 00:15:41,230
that app and so it's

250
00:15:41,250 --> 00:15:45,100
returning. Hello world. And we'll do that a whole bunch of times.

251
00:15:45,710 --> 00:15:48,826
So now that we've got some load on the system, let's take a look at

252
00:15:48,848 --> 00:15:52,206
our autoscale. And it looks like we're down to zero. So now

253
00:15:52,228 --> 00:15:56,046
we've scaled down a whole lot. Now let's take a

254
00:15:56,068 --> 00:15:59,614
look at our Autoscale and

255
00:15:59,652 --> 00:16:03,262
we'll watch it and we'll see if we capture some change in

256
00:16:03,316 --> 00:16:06,500
metrics now that we've added some load to it.

257
00:16:07,670 --> 00:16:11,522
Now, it does run every 15 seconds, so we will have to wait

258
00:16:11,576 --> 00:16:14,100
a little bit to see if this metric changes.

259
00:16:14,630 --> 00:16:18,946
But once it changes, then we can recalculate the number of pods

260
00:16:18,978 --> 00:16:23,106
that we need. Now we can adjust the 15 seconds.

261
00:16:23,138 --> 00:16:26,934
We can make it more aggressive, so maybe 10

262
00:16:26,972 --> 00:16:31,270
seconds or sooner to be able to look at our pods more aggressively.

263
00:16:31,350 --> 00:16:34,380
Or we can specify it much more,

264
00:16:35,070 --> 00:16:38,406
much larger so that we put less load on our pods

265
00:16:38,438 --> 00:16:41,866
because, well, it needs to go look. Okay, so now we've gotten up

266
00:16:41,888 --> 00:16:45,246
to 24%. So now based on our 24%,

267
00:16:45,348 --> 00:16:48,778
let's see if we need additional pods. No, it looks like we don't.

268
00:16:48,874 --> 00:16:52,858
So I'm going to specify this back at 10%

269
00:16:53,044 --> 00:16:58,290
and let's deploy our autoscale.

270
00:16:59,190 --> 00:17:03,330
And now based on 24%,

271
00:17:03,400 --> 00:17:07,206
then it will spin up a whole lot of pods. Great. We saw how we

272
00:17:07,228 --> 00:17:10,582
could spin up pods and spin down pods based

273
00:17:10,636 --> 00:17:13,570
on the needs of our system. Cubectl,

274
00:17:13,650 --> 00:17:18,230
delete F cpu.

275
00:17:19,390 --> 00:17:23,130
Let's delete our deployment. Let's also delete our

276
00:17:23,280 --> 00:17:24,650
autoscale.

277
00:17:28,030 --> 00:17:31,534
And now we can see that those are done. Oh, we're still

278
00:17:31,572 --> 00:17:34,320
generating a load. Let's undo that.

279
00:17:36,530 --> 00:17:40,910
Now. If we do kubectl top pod,

280
00:17:42,050 --> 00:17:45,966
we can see that we're still collecting metrics even though there's no horizontal

281
00:17:45,998 --> 00:17:50,002
autoscale using those metrics. That's fine, it's nice data

282
00:17:50,056 --> 00:17:55,250
to have, but the metric server is still running great.

283
00:17:55,400 --> 00:17:58,702
So we got to see the horizontal autoscale,

284
00:17:58,766 --> 00:18:02,262
we got to scale based on a cpu metric. And that was great.

285
00:18:02,316 --> 00:18:05,846
Let's take a look at other metrics that we might want to choose. Now,

286
00:18:05,868 --> 00:18:09,366
as we looked at this, here's kind of our mental model. Our horizontal

287
00:18:09,398 --> 00:18:12,922
pod autoscaler goes and checks the application

288
00:18:13,056 --> 00:18:16,982
every 15 seconds to see if data needs adjusting.

289
00:18:17,126 --> 00:18:20,438
Now, that's a little bit of a naive interpretation

290
00:18:20,534 --> 00:18:24,094
because, well, we have our metric server here. I love these

291
00:18:24,132 --> 00:18:27,662
graphics. Grab these slides from robridge.org and click through

292
00:18:27,716 --> 00:18:30,766
to the learnks IO page.

293
00:18:30,868 --> 00:18:34,242
It's a great tutorial. Now, this too is

294
00:18:34,296 --> 00:18:38,466
a little bit naive because inside the metrics server we actually have

295
00:18:38,568 --> 00:18:42,286
three different APIs we have resource metrics,

296
00:18:42,318 --> 00:18:45,454
APIs, we have custom metrics and we have external

297
00:18:45,502 --> 00:18:49,746
metrics. Now, zooming in on each of those, the resource metrics

298
00:18:49,778 --> 00:18:53,910
is about cpu and memory. Those are the two built in custom

299
00:18:53,980 --> 00:18:57,142
metrics. We might look to our pods to be able to find other

300
00:18:57,196 --> 00:19:00,970
details, any details that we can get out of those pods would

301
00:19:01,040 --> 00:19:04,486
work nicely here. In the custom metrics API, maybe we'll harvest

302
00:19:04,518 --> 00:19:08,310
Prometheus metrics for example, and then external metrics.

303
00:19:08,390 --> 00:19:12,030
Maybe we're looking at a requests queue length or a

304
00:19:12,100 --> 00:19:15,582
message queue length and taking actions based on

305
00:19:15,636 --> 00:19:19,402
resources that are external to our pods, maybe other Kubernetes

306
00:19:19,466 --> 00:19:23,578
resources, maybe other cloud resources, other hardware within our environment.

307
00:19:23,674 --> 00:19:27,406
We can look to that to make external decisions about how many pods

308
00:19:27,438 --> 00:19:31,470
we need. So we have these three APIs

309
00:19:31,630 --> 00:19:35,394
and each of them will reach out to an adapter that will reach out

310
00:19:35,432 --> 00:19:38,374
to a service to get data. So for example,

311
00:19:38,492 --> 00:19:41,778
in the metrics API we reach out to the metrics server.

312
00:19:41,874 --> 00:19:45,334
The metrics server in turn looks to see Advisor and C

313
00:19:45,372 --> 00:19:48,642
advisor will harvest those metrics from our pods.

314
00:19:48,786 --> 00:19:52,518
In our custom metrics API. Perhaps we're using the Prometheus adapter.

315
00:19:52,614 --> 00:19:56,022
The Prometheus adapter looks to Prometheus and Prometheus

316
00:19:56,086 --> 00:19:59,690
looks to all of our pods. When we're looking at external things,

317
00:19:59,760 --> 00:20:02,942
maybe we're using Prometheus to monitor those as well. Or maybe

318
00:20:02,996 --> 00:20:07,054
we're using another adapter. So we

319
00:20:07,092 --> 00:20:10,382
have the autoscaling in really interesting ways.

320
00:20:10,436 --> 00:20:13,630
Let's take a look at how we might use that with Prometheus.

321
00:20:14,050 --> 00:20:17,394
So our first stop is to install the metrics server. We did

322
00:20:17,432 --> 00:20:21,278
this previously and even though we're not using the metrics coming out of the metrics

323
00:20:21,294 --> 00:20:24,340
server, this gets us the metrics APIs as well.

324
00:20:24,970 --> 00:20:28,914
Then we need to install Prometheus. Now I'm grabbing Prometheus

325
00:20:28,962 --> 00:20:30,950
from the Prometheus helm chart.

326
00:20:33,210 --> 00:20:36,946
The Prometheus community Prometheus helm chart installs

327
00:20:36,978 --> 00:20:40,486
just Prometheus. But if I chose the Prometheus stack, I would also

328
00:20:40,508 --> 00:20:44,106
get Grafana. Now I could choose to put this in a

329
00:20:44,128 --> 00:20:47,386
different namespace. That would probably be a good best practice, but for the

330
00:20:47,408 --> 00:20:51,478
sake of today's demo, I'll just put it in the default namespace.

331
00:20:51,654 --> 00:20:55,358
Next up, I'm going to install the Metrics adapter. Now I'm going to pull this

332
00:20:55,444 --> 00:20:59,102
from Prometheus community helm charts as well. And so

333
00:20:59,156 --> 00:21:02,542
then I'll install this Prometheus adapter. I am going to

334
00:21:02,596 --> 00:21:06,610
set some properties here. I could do this with a yaml file or

335
00:21:06,680 --> 00:21:10,254
here, I'll just set them straight away. I'm going to point it at the prometheus

336
00:21:10,302 --> 00:21:14,322
URL. The Prometheus helm chart creates a prometheus service

337
00:21:14,376 --> 00:21:17,782
called Prometheus service and it's on port 80 instead

338
00:21:17,836 --> 00:21:21,046
of 90 90 as it typically is.

339
00:21:21,148 --> 00:21:24,246
So I'll configure it to point to my Prometheus service.

340
00:21:24,348 --> 00:21:26,920
And now I've got the Prometheus adapter running.

341
00:21:27,930 --> 00:21:31,254
Now I'm going to configure my workload. Now I could configure

342
00:21:31,302 --> 00:21:35,206
Prometheus to point it at where my workload is and configure

343
00:21:35,238 --> 00:21:38,682
it that way, but because it's all running in my cluster, I can create

344
00:21:38,736 --> 00:21:42,554
an annotation on that deployment that will allow Prometheus

345
00:21:42,602 --> 00:21:46,346
to automatically discover it. So here in my deployment,

346
00:21:46,458 --> 00:21:50,410
I'm going to create an annotation on my pod

347
00:21:50,490 --> 00:21:53,958
that identifies that. I want Prometheus to scrape

348
00:21:53,994 --> 00:21:57,490
the metrics and what's the URL and port that it should scrape?

349
00:21:57,910 --> 00:22:01,890
Now this is perfect. I've identified that Prometheus should

350
00:22:01,960 --> 00:22:05,198
scrape the metrics out of all of these pods.

351
00:22:05,294 --> 00:22:08,782
I turn it on, I give it the URL, and I give it the port.

352
00:22:08,936 --> 00:22:12,566
Now it is important that these are quoted because if they aren't, this would

353
00:22:12,588 --> 00:22:16,214
be a boolean and an integer respectively, and they need to be

354
00:22:16,252 --> 00:22:19,526
strings to be annotations. So I put quotes around it and

355
00:22:19,548 --> 00:22:23,406
it works just fine. Now I'll deploy my workload kubectl,

356
00:22:23,458 --> 00:22:27,526
apply that deployment, and now I've got my content. I know that Prometheus

357
00:22:27,558 --> 00:22:31,514
is going to monitor that content, make those metrics available to the Prometheus

358
00:22:31,562 --> 00:22:35,662
adapter, and now our horizontal autoscale can

359
00:22:35,716 --> 00:22:39,502
harvest those metrics to make scaling decisions. So next up,

360
00:22:39,556 --> 00:22:43,114
let's put in the horizontal autoscale. Now I've got this horizontal

361
00:22:43,162 --> 00:22:46,706
autoscale and in this case it's going to target a deployment. Here's the

362
00:22:46,728 --> 00:22:50,306
name of my deployment. And rather than resources as

363
00:22:50,328 --> 00:22:53,886
we saw previously, this one's going to scale based on pods. I wish

364
00:22:53,918 --> 00:22:57,938
this said prometheus, but it doesn't.

365
00:22:58,114 --> 00:23:01,746
We'll give it some interesting metric from prometheus that the Prometheus

366
00:23:01,778 --> 00:23:05,254
adapter knows how to get and we can give it a value that we

367
00:23:05,292 --> 00:23:09,314
want to specify. So if it's lower than this, then we'll

368
00:23:09,362 --> 00:23:12,474
reduce the number of pods and if it's above this, then we'll increase the number

369
00:23:12,512 --> 00:23:16,026
of pods. Now, how might we look

370
00:23:16,048 --> 00:23:19,386
to other metrics? There are lots of different adapters that we can

371
00:23:19,408 --> 00:23:23,246
look at and let's tour. Come. Here's a really great project

372
00:23:23,348 --> 00:23:26,794
that allows us to create adapters from all kinds of sources.

373
00:23:26,922 --> 00:23:30,442
So we could call HTTP into our pods

374
00:23:30,506 --> 00:23:33,626
and harvest JSON querying into that JSON

375
00:23:33,658 --> 00:23:37,534
to be able to grab a particular metric. There's also Prometheus collector,

376
00:23:37,582 --> 00:23:40,862
although the Prometheus adapter is usually a better choice.

377
00:23:41,006 --> 00:23:44,242
We could look at the influxDB collector, which is a great example

378
00:23:44,296 --> 00:23:47,754
of how we might query other data stores the AWS collector.

379
00:23:47,822 --> 00:23:51,094
So we could look at, for example, sqs queue length. We could use

380
00:23:51,132 --> 00:23:54,850
this as a template to grab things from blob storage

381
00:23:54,930 --> 00:23:58,594
or GCP. And we can also scale

382
00:23:58,642 --> 00:24:01,894
based on a schedule. If we know that our work starts in the morning and

383
00:24:01,932 --> 00:24:05,354
ends in the evening, and that we won't use the cluster overnight, we can create

384
00:24:05,392 --> 00:24:08,778
a schedule to automatically scale that up or down. Now, unlike the

385
00:24:08,784 --> 00:24:12,206
other metrics that look to behaviors in our cluster, this will just

386
00:24:12,228 --> 00:24:15,470
do it on a timer and maybe that's sufficient.

387
00:24:16,210 --> 00:24:19,726
We could also look to istio and grab metrics from

388
00:24:19,748 --> 00:24:23,022
istio. There are many other adapters that will allow

389
00:24:23,076 --> 00:24:26,654
us to query other systems to be able to get metrics

390
00:24:26,702 --> 00:24:30,402
both for pods and for external resources where we can make

391
00:24:30,456 --> 00:24:34,322
choices about how to scale. We took a look

392
00:24:34,376 --> 00:24:37,986
at built in sources for cpu and memory. Now those are

393
00:24:38,008 --> 00:24:41,266
definitely the easiest, but if our workload isn't based on cpu

394
00:24:41,298 --> 00:24:44,374
and memory, if it's based on another metric, perhaps we can look to

395
00:24:44,412 --> 00:24:48,440
Prometheus to be able to find metrics specific for our application.

396
00:24:48,810 --> 00:24:52,106
Anything that we can expose as a Prometheus metric we can

397
00:24:52,128 --> 00:24:54,380
then use to auto scale our service.

398
00:24:54,830 --> 00:24:57,878
Similarly, we could look to external metrics.

399
00:24:57,974 --> 00:25:01,942
Perhaps we're looking at our load balancer or our queue length

400
00:25:02,006 --> 00:25:05,360
and making decisions on how many pods we need. Based on that,

401
00:25:07,010 --> 00:25:10,686
let's take a look at some best practices. Now, these best practices will

402
00:25:10,708 --> 00:25:13,710
help us to make good scaling decisions.

403
00:25:14,290 --> 00:25:17,780
What if we scale up too high? Maybe we're getting

404
00:25:18,470 --> 00:25:21,598
attacked. Maybe we have a bug in our software,

405
00:25:21,774 --> 00:25:25,234
or maybe we've just gone viral and we need to be able to scale up

406
00:25:25,272 --> 00:25:29,586
to impossible scale. We probably don't want to scale infinitely

407
00:25:29,698 --> 00:25:33,542
because, well, especially in the case of an attack, we don't want that big

408
00:25:33,596 --> 00:25:37,174
cloud bill. It would be really easy for us to stay

409
00:25:37,292 --> 00:25:41,242
scaled for the duration of the attack and end up paying a lot.

410
00:25:41,376 --> 00:25:44,666
So let's create a max value that matches the

411
00:25:44,688 --> 00:25:48,026
budget of our organization and lets us determine that.

412
00:25:48,128 --> 00:25:51,546
Well, we're going to understand that our system may not

413
00:25:51,568 --> 00:25:55,322
be able to reach that capacity, but it'll reach our budgetary goals.

414
00:25:55,466 --> 00:25:58,798
If it's an attack, we're not paying extra to handle that attack.

415
00:25:58,964 --> 00:26:03,230
Or if we've gone viral, maybe we need to reconsider that upper bound.

416
00:26:03,650 --> 00:26:07,006
The other reason we might scale up too high is maybe we're

417
00:26:07,038 --> 00:26:10,258
monitoring more than one metric. Now, Kubernetes is going

418
00:26:10,264 --> 00:26:13,586
to do the right thing here. If we're monitoring two metrics and one

419
00:26:13,608 --> 00:26:17,122
is high and one is low, Kubernetes will use the high metric

420
00:26:17,186 --> 00:26:20,566
to be able to reach the capacity to make sure that all the

421
00:26:20,588 --> 00:26:24,546
metrics are within bounds. But for example, if we're monitoring

422
00:26:24,578 --> 00:26:28,380
both cpu and memory, and the cpu is low,

423
00:26:28,750 --> 00:26:31,930
we might say, well, you shouldn't have scaled up here,

424
00:26:32,080 --> 00:26:35,914
but maybe the memory is high and so we did scale up by

425
00:26:35,952 --> 00:26:39,674
comparison. So here we need

426
00:26:39,712 --> 00:26:43,006
to define the max replicas in addition to

427
00:26:43,028 --> 00:26:46,414
the min replicas. Don't just assume that infinitely high

428
00:26:46,452 --> 00:26:50,766
is sufficient. Pick a budget that matches the needs of our organization and

429
00:26:50,868 --> 00:26:54,234
understand that our system may be less available if our load

430
00:26:54,282 --> 00:26:57,986
exceeds that. But we're reaching our budget goals, which is the

431
00:26:58,008 --> 00:27:01,262
needs of the business. Next up, we may scale too slow.

432
00:27:01,326 --> 00:27:05,090
Now, we noted how the horizontal autoscale only checked every 15

433
00:27:05,160 --> 00:27:08,646
seconds. That's definitely a configurable value. And in

434
00:27:08,668 --> 00:27:12,582
the case of missing metrics, then Kubernetes will assume the best

435
00:27:12,636 --> 00:27:16,086
case scenario. So if we're scaling out, Kubernetes will

436
00:27:16,108 --> 00:27:20,142
assume that that pod is using 0%, therefore it won't

437
00:27:20,226 --> 00:27:24,026
scale out unnecessarily. If we're scaling in, Kubernetes will

438
00:27:24,048 --> 00:27:27,654
assume that that metric was at 100%. Therefore it probably won't scale

439
00:27:27,702 --> 00:27:30,794
in until that metric is available. So if it takes

440
00:27:30,832 --> 00:27:34,330
a long time for our pods to be able to surface these metrics,

441
00:27:34,410 --> 00:27:37,962
then we may notice that the Kubernetes autoscaler won't take action.

442
00:27:38,106 --> 00:27:41,994
What if when we deploy a new version, it always resets to one pod

443
00:27:42,042 --> 00:27:45,742
and then scales back up? Yeah, if we put our

444
00:27:45,796 --> 00:27:49,682
HUD coded pod limit, then we might hit a scenario. Well, what happened

445
00:27:49,736 --> 00:27:53,362
here? In our deployment or our other service,

446
00:27:53,496 --> 00:27:57,806
we had a hard coded replicas count as we deployed

447
00:27:57,838 --> 00:28:01,346
the new version of our deployment. Or stateful set or replica

448
00:28:01,378 --> 00:28:05,094
set, Kubernetes followed our instructions and killed off all the other

449
00:28:05,132 --> 00:28:08,498
pods. So we only ended up with one or maybe a few. Yeah,

450
00:28:08,524 --> 00:28:12,666
we really want our autoscale to handle this. So we

451
00:28:12,688 --> 00:28:17,078
need to comment out the replicas line in our deployment.

452
00:28:17,174 --> 00:28:20,726
We want our horizontal autoscaler to hit this. Now. This does create an edge

453
00:28:20,758 --> 00:28:24,078
case where when we deploy it the very, very first time, we're only going to

454
00:28:24,084 --> 00:28:27,342
get one until the autoscaler notices. Well, I'd rather

455
00:28:27,396 --> 00:28:31,040
have 15 seconds of only one pod than

456
00:28:31,730 --> 00:28:35,398
have it reset to one on every deployment. If you grab these slides

457
00:28:35,434 --> 00:28:38,914
from roberts.org and click through to this post, you can find more about

458
00:28:38,952 --> 00:28:41,662
that edge case and how to handle it gracefully.

459
00:28:41,806 --> 00:28:45,486
Flapping or sloshing? The documentation talks about flapping

460
00:28:45,518 --> 00:28:49,846
as if the door keeps opening and closing. I like to talk about sloshing like

461
00:28:49,948 --> 00:28:53,590
we're just pushing the water up against the beach and so the water

462
00:28:53,660 --> 00:28:56,934
just keeps going. Let's assume that we have a Java app that

463
00:28:56,972 --> 00:29:00,210
takes a while to spin up. We notice that we don't have enough capacity,

464
00:29:00,290 --> 00:29:04,154
so we spin up some pods. 15 seconds later we notice that those

465
00:29:04,192 --> 00:29:07,786
pods aren't live yet and we choose to decide to scale up again.

466
00:29:07,888 --> 00:29:11,754
And so now in a minute, once our Java app is booted and everything is

467
00:29:11,792 --> 00:29:15,066
working, we now have too much capacity. So we'll scale

468
00:29:15,098 --> 00:29:18,446
back down. Then we'll notice we don't have up scale back up and down and

469
00:29:18,468 --> 00:29:22,330
up and on we go, sloshing back and Forth. Well, what happened

470
00:29:22,420 --> 00:29:27,966
here? Well, Kubernetes will automatically block additional scaling

471
00:29:27,998 --> 00:29:31,502
operations until our system has reached stability.

472
00:29:31,646 --> 00:29:35,650
So by default, it's five minutes. In this case, I added some content

473
00:29:35,720 --> 00:29:39,074
to my horizontal autoscaler to set that stabilization

474
00:29:39,122 --> 00:29:41,110
window to 60 seconds,

475
00:29:42,010 --> 00:29:45,414
600 seconds or ten minutes. So I want to give

476
00:29:45,532 --> 00:29:48,954
my time, a little bit of my application, a little bit extra time to get

477
00:29:48,992 --> 00:29:52,774
stable. Alternatively, in our demos, we chose to scale

478
00:29:52,822 --> 00:29:56,234
it, to set that stabilization window to 1

479
00:29:56,272 --> 00:29:59,994
second so that it would automatically make additional scaling things. And that

480
00:30:00,032 --> 00:30:04,254
worked out really well for our demo. Scale to zero. Now in

481
00:30:04,292 --> 00:30:07,566
our horizontal autoscale, it won't scale to zero, it'll only scale to

482
00:30:07,588 --> 00:30:10,686
one. And now we're paying for our application to

483
00:30:10,708 --> 00:30:14,286
run even if there's no load at all. How do we overcome this?

484
00:30:14,388 --> 00:30:17,954
Well, the problem is that, well, how do we start the application back

485
00:30:17,992 --> 00:30:21,598
up? If a request comes in and there are no deployment,

486
00:30:21,694 --> 00:30:24,814
no pods running to handle it, then that request

487
00:30:24,862 --> 00:30:28,306
will just fail. So to make this happen, we need a reverse proxy in

488
00:30:28,328 --> 00:30:31,506
front of that to be able to handle the load. Notice there's

489
00:30:31,538 --> 00:30:34,902
no pods and kick it back up. Now, it may take a minute for

490
00:30:34,956 --> 00:30:38,882
a pod to start on a completely cold start, so maybe that initial request

491
00:30:38,946 --> 00:30:42,506
also still fails. But that's our scale to zero problem. We need

492
00:30:42,528 --> 00:30:45,962
a reverse proxy in front of it to be able to notice that our

493
00:30:46,016 --> 00:30:49,386
cluster needs turning back on. Now that's not built in. That's a

494
00:30:49,408 --> 00:30:52,746
problem larger than just horizontal autoscaling between one and

495
00:30:52,768 --> 00:30:56,666
a set number. So we might need to reach for external tools.

496
00:30:56,778 --> 00:31:00,094
Knative and keta both offer this solution, and so

497
00:31:00,132 --> 00:31:03,546
we can reach for one of these products to be able to get to scale

498
00:31:03,578 --> 00:31:07,246
to zero. Now, it does mean that we're going to have some pods running associated

499
00:31:07,278 --> 00:31:10,766
with knative and Keda. So if we only have one microservice,

500
00:31:10,878 --> 00:31:14,786
maybe just leaving that one microservice running, that might

501
00:31:14,808 --> 00:31:18,354
be a simpler solution than a scale to zero solution.

502
00:31:18,482 --> 00:31:22,342
We took a look at horizontal autoscaling in Kubernetes and it was really

503
00:31:22,396 --> 00:31:25,666
cool. Horizontal autoscaling is about scaling

504
00:31:25,698 --> 00:31:29,654
up when we have additional demand and scaling down, paying less

505
00:31:29,772 --> 00:31:33,330
when we don't have demand. This is made possible by utility billing.

506
00:31:33,410 --> 00:31:36,774
We can go to a cloud, we can get some resources. When we're done,

507
00:31:36,812 --> 00:31:40,126
we can hand them back and pay. Nothing and the scaling operation can

508
00:31:40,148 --> 00:31:43,902
handle can happen in real time. So we don't need to pre

509
00:31:43,956 --> 00:31:47,706
provision our hardware. We don't need to over buy our systems

510
00:31:47,738 --> 00:31:51,006
to reach maximum capacity. We can scale up and down as

511
00:31:51,028 --> 00:31:51,820
our needs are.

