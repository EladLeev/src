1
00:02:14,760 --> 00:02:17,876
Good day and thanks for joining me today for

2
00:02:17,898 --> 00:02:21,396
this talk about open telemetry and some of the

3
00:02:21,578 --> 00:02:25,216
things you may run into with working in an open telemetry

4
00:02:25,248 --> 00:02:29,412
environment. And first, my thanks to comp 42 for letting me

5
00:02:29,546 --> 00:02:33,028
talk about this today. Open telemetry is near and dear to my

6
00:02:33,034 --> 00:02:37,228
heart. Having spent the last four years working in the observability space,

7
00:02:37,394 --> 00:02:41,020
opentelemetry is definitely taking the world by storm.

8
00:02:41,360 --> 00:02:45,276
But we're going to talk about how you can add opentelemetry to

9
00:02:45,298 --> 00:02:49,200
a modern application. But to do that, we kind of need to start

10
00:02:49,270 --> 00:02:52,370
by talking about what we mean by modern application.

11
00:02:52,740 --> 00:02:56,028
Well, every company is on a cloud journey,

12
00:02:56,124 --> 00:02:58,972
and they're moving from their old style,

13
00:02:59,036 --> 00:03:02,256
monolithic, tightly coupled environments all

14
00:03:02,278 --> 00:03:05,648
the way out to a cloud native or rearchitected environment.

15
00:03:05,744 --> 00:03:09,092
And we're seeing this more and more driven by combination of

16
00:03:09,146 --> 00:03:12,496
cloud economics as well as the ability and ease

17
00:03:12,528 --> 00:03:15,430
of scaling, management and support.

18
00:03:15,800 --> 00:03:18,900
But a modern application actually

19
00:03:18,970 --> 00:03:22,564
is a little bit more than that. It's not defined by its

20
00:03:22,602 --> 00:03:26,460
implementation, it's defined by its capabilities here.

21
00:03:26,570 --> 00:03:30,184
And we think of these when we talk about modern apps in terms of portability,

22
00:03:30,312 --> 00:03:33,848
scalability, observability, reproducibility,

23
00:03:33,944 --> 00:03:37,436
as well as debugability. Last word is,

24
00:03:37,458 --> 00:03:41,264
by the way, quite a mouthful to say so. When we look at this,

25
00:03:41,382 --> 00:03:45,516
modern applications really have certain feature sets

26
00:03:45,548 --> 00:03:49,280
to them, but they are not implementation details.

27
00:03:49,780 --> 00:03:52,892
And so I don't care whether you write this in cobalt,

28
00:03:53,036 --> 00:03:57,092
heaven forbid, Fortran eight and 95, it doesn't matter. What really

29
00:03:57,146 --> 00:04:00,708
matters is that it works. And at this, you can look at

30
00:04:00,714 --> 00:04:04,344
this list and see that certain things make up

31
00:04:04,382 --> 00:04:07,800
characteristics that define this modern application space.

32
00:04:07,870 --> 00:04:11,876
Here in this that we're talking about, this observability open telemetry

33
00:04:11,908 --> 00:04:15,512
space, it really comes down to three quickly find

34
00:04:15,566 --> 00:04:19,084
performance bottlenecks. So where are things not working the way we expect

35
00:04:19,122 --> 00:04:23,036
them to do? After all, user happiness is heavily dependent on

36
00:04:23,138 --> 00:04:26,696
performance as well as results, provide answers

37
00:04:26,728 --> 00:04:30,236
to platform engineering questions. So how the people

38
00:04:30,258 --> 00:04:33,920
are working in the platform aspects, as well as how the application itself

39
00:04:33,990 --> 00:04:37,088
is behaving, and what can I find that I need to

40
00:04:37,094 --> 00:04:41,392
find out about that? And then finally provides context on errors and

41
00:04:41,446 --> 00:04:44,464
crashes. This doesn't mean simply reporting an error,

42
00:04:44,512 --> 00:04:48,016
but it means finding the context, what's going on around the error,

43
00:04:48,048 --> 00:04:51,668
what was leading up to this error. And so these things actually

44
00:04:51,754 --> 00:04:54,804
lead us into this larger category

45
00:04:54,932 --> 00:04:58,804
of things that are important for modern apps in this observability

46
00:04:58,932 --> 00:05:02,804
open telemetry space. And so what I've

47
00:05:02,852 --> 00:05:05,880
done is taken with my engineering team,

48
00:05:05,950 --> 00:05:09,608
this thing called Mara modern application reference architecture,

49
00:05:09,704 --> 00:05:13,144
and it's a microservices architecture. It uses Kubernetes

50
00:05:13,272 --> 00:05:16,760
leader in orchestration, of course, here, and is designed

51
00:05:16,840 --> 00:05:20,176
to be production ready. As you can see, the application

52
00:05:20,278 --> 00:05:23,484
actually only makes up part of this. And the part that's

53
00:05:23,532 --> 00:05:27,724
kind of hidden below the surface is monitoring observability management

54
00:05:27,772 --> 00:05:31,452
aspects, infrastructure aspects, and Kubernetes itself.

55
00:05:31,606 --> 00:05:35,268
The application is really only one part of this question.

56
00:05:35,434 --> 00:05:40,036
The larger amount is the stuff that

57
00:05:40,058 --> 00:05:43,664
the application is dependent on without being hidden

58
00:05:43,712 --> 00:05:47,336
below the service. And so this

59
00:05:47,358 --> 00:05:50,890
is a quick look at the application itself. This is

60
00:05:51,900 --> 00:05:55,604
a banking application. It's written in Python

61
00:05:55,652 --> 00:05:58,952
at the front end and uses Python for certain

62
00:05:59,006 --> 00:06:02,104
pieces in the back end, as well as Java for other pieces in the back

63
00:06:02,142 --> 00:06:06,556
end. It has two postgres SQL databases that

64
00:06:06,578 --> 00:06:10,860
it makes use of. And so each of these pieces is an independent microservices

65
00:06:10,940 --> 00:06:14,128
on its own right. But this does lead us to some of

66
00:06:14,134 --> 00:06:17,740
the interesting things. We need a solutions

67
00:06:17,900 --> 00:06:21,964
that has the ability to support more than one language,

68
00:06:22,092 --> 00:06:25,924
and we need this solution that understands the context of to and

69
00:06:25,962 --> 00:06:29,572
fro, where things are coming from, where things are going to,

70
00:06:29,626 --> 00:06:32,070
and what the results in each piece are.

71
00:06:32,600 --> 00:06:35,936
And when we started looking at this,

72
00:06:36,058 --> 00:06:40,040
we built this so that it was portable. Remember that portability aspect here

73
00:06:40,110 --> 00:06:43,896
so it can run on lots of different infrastructures built in

74
00:06:43,998 --> 00:06:46,936
at this point with tracing metrics, logging,

75
00:06:46,968 --> 00:06:50,492
visualization, management skills. We also built a

76
00:06:50,546 --> 00:06:54,364
testing structure for load generation as well as

77
00:06:54,402 --> 00:06:58,012
a continuous integration model. This mimics what goes

78
00:06:58,066 --> 00:07:01,228
on in production applications every single day.

79
00:07:01,394 --> 00:07:04,828
Maybe not completely complete or as complex as youll,

80
00:07:04,924 --> 00:07:07,968
but this is what a reference architecture is designed to do.

81
00:07:08,054 --> 00:07:11,532
In our case, we wanted the reference architecture to be more than just a reference

82
00:07:11,596 --> 00:07:15,940
drawing. We wanted this to actually work and be able to test what this meant

83
00:07:16,920 --> 00:07:19,956
and so quickly drawing it out. You can see the

84
00:07:19,978 --> 00:07:23,600
various pieces inside of here, kubernetes driving our

85
00:07:23,770 --> 00:07:27,624
node pod arrangements. Here we're using application

86
00:07:27,742 --> 00:07:30,952
networking pieces. In this case we're actually

87
00:07:31,006 --> 00:07:34,376
using the NgInx open source project to do

88
00:07:34,398 --> 00:07:38,440
that. And then we have other pieces around it for automation, these pipeline

89
00:07:38,520 --> 00:07:42,492
code repos which are on GitHub or GitLab, and then

90
00:07:42,546 --> 00:07:46,252
automation code to drive all of this. And so there's lots of

91
00:07:46,306 --> 00:07:49,970
moving parts across this entire application space.

92
00:07:51,220 --> 00:07:54,796
Now, when we start looking at this, all those lots of moving parts

93
00:07:54,828 --> 00:07:58,512
means that things have become complicated. Microservices mean

94
00:07:58,566 --> 00:08:02,736
things have come complicated. Failures don't always repeat here.

95
00:08:02,838 --> 00:08:06,708
Debugging can be really painful because those failures don't repeat here.

96
00:08:06,794 --> 00:08:10,708
And the scale of data that we're dealing with is massive here.

97
00:08:10,874 --> 00:08:14,084
So when we started looking at this, we sort of used this

98
00:08:14,122 --> 00:08:17,736
kniven framework to say, what do we need to do to drive

99
00:08:17,838 --> 00:08:22,004
to where we need to be able to probe, sense and respond the emergent

100
00:08:22,052 --> 00:08:25,784
technologies that are being driven by cloud native or modern application

101
00:08:25,902 --> 00:08:29,672
spaces. And so observability

102
00:08:29,736 --> 00:08:33,596
is a data problem. More observable the data the

103
00:08:33,618 --> 00:08:36,924
system is, the faster we can understand why it's acting up and we can fix

104
00:08:36,962 --> 00:08:40,208
it. But in general, we look at this from

105
00:08:40,294 --> 00:08:43,872
having a visibility across the entire stack, but based

106
00:08:43,926 --> 00:08:47,472
on three classes of data metrics, traces and

107
00:08:47,526 --> 00:08:51,136
locks, or do I have a problem, where is the

108
00:08:51,158 --> 00:08:54,592
problem, what's causing the problem? And each of these pieces

109
00:08:54,656 --> 00:08:58,704
plays a role in helping us look at that application stack.

110
00:08:58,832 --> 00:09:03,460
But when we started looking at the needs for real observability

111
00:09:03,800 --> 00:09:07,764
in a production application, we found that logs,

112
00:09:07,812 --> 00:09:10,810
metrics and traces were a great place to start.

113
00:09:11,180 --> 00:09:14,424
But we also needed things like error aggregation. We needed

114
00:09:14,462 --> 00:09:17,348
to be able to look at the runtime state introspections,

115
00:09:17,444 --> 00:09:20,408
we needed health checks, we need to be able to look at core dumps.

116
00:09:20,504 --> 00:09:24,092
All these pieces play a role across this in

117
00:09:24,146 --> 00:09:26,350
actually pulling the data out.

118
00:09:27,360 --> 00:09:30,376
Today I'm probably going to spend most of my time talking into logs,

119
00:09:30,408 --> 00:09:33,952
metrics and traces, but if you want to, this is an open

120
00:09:34,006 --> 00:09:37,488
source effort. You can go through, play with it and take a

121
00:09:37,494 --> 00:09:41,056
look at what the underlying structures are for all of these

122
00:09:41,078 --> 00:09:43,410
different pieces and how we built them together.

123
00:09:44,900 --> 00:09:48,836
And so that is a starting wish list. And as you can see,

124
00:09:48,938 --> 00:09:52,340
technology with logging all the way over to heap dumps, and then we

125
00:09:52,410 --> 00:09:56,664
listed some that came off the top of our head around where things

126
00:09:56,782 --> 00:10:00,676
might actually make sense. So, elastic, the elastic

127
00:10:00,708 --> 00:10:03,160
APM model, elastic cloud,

128
00:10:03,310 --> 00:10:07,156
elasticsearch, whatever you call it. We also looked at Grafana,

129
00:10:07,268 --> 00:10:09,968
Greylog, Jaeger, Opensenses,

130
00:10:10,084 --> 00:10:12,776
opentelemetry, production apps,

131
00:10:12,968 --> 00:10:16,664
statsD, Zipkin. There are a few others that crept

132
00:10:16,712 --> 00:10:19,936
in and out during this discussion, but this is where we started.

133
00:10:20,038 --> 00:10:23,680
And then we mapped across this and said, oh, does that offer

134
00:10:23,750 --> 00:10:27,408
this capability? What we've quickly found

135
00:10:27,574 --> 00:10:31,212
is that this chart is very

136
00:10:31,286 --> 00:10:34,820
pretty and honestly kind of meaningless.

137
00:10:36,200 --> 00:10:40,052
If you look at this, you can't easily compare something

138
00:10:40,106 --> 00:10:43,120
like Zipkin to elastic APM,

139
00:10:43,200 --> 00:10:47,780
even though they do have a crossover. They are very different structures

140
00:10:47,860 --> 00:10:50,584
and very solving very different problems.

141
00:10:50,782 --> 00:10:55,432
And so we decided that we were going to sort of stop

142
00:10:55,486 --> 00:10:59,368
doing this and start looking at the qualitative views

143
00:10:59,464 --> 00:11:02,700
of looking at different projects and see

144
00:11:02,770 --> 00:11:06,520
what they could meet, not to this checklist items,

145
00:11:06,600 --> 00:11:10,380
but to our underlying needs for these functionality.

146
00:11:10,800 --> 00:11:14,060
So we actually started by looking at Opencensus.

147
00:11:14,420 --> 00:11:16,690
In Opencensus at the top of the page,

148
00:11:17,620 --> 00:11:20,956
interestingly enough, goes opensensus and open tracing have merged

149
00:11:20,988 --> 00:11:24,844
into opentelemetry and we. Hmm, so open tracing,

150
00:11:24,972 --> 00:11:28,736
open tracing project is archived. Learn more migrate to Opentelemetry

151
00:11:28,768 --> 00:11:32,276
today. This kind of gave us the feeling that we should

152
00:11:32,298 --> 00:11:35,764
probably be looking at open telemetry.

153
00:11:35,892 --> 00:11:38,570
And so that was where we went to.

154
00:11:39,340 --> 00:11:42,904
So what is open telemetry? It's a standards based

155
00:11:43,022 --> 00:11:46,840
agents and cloud integration structure offering

156
00:11:47,260 --> 00:11:51,020
the capabilities of observability. It has automated code

157
00:11:51,090 --> 00:11:55,032
instrumentation, it supports multiple mini frameworks

158
00:11:55,096 --> 00:11:58,332
and pretty much any code at any time. Remember that we are using

159
00:11:58,386 --> 00:12:02,380
multiple languages and we're using multiple frameworks inside of here.

160
00:12:02,530 --> 00:12:06,028
Open tracing and opensenses merged to form open telemetry,

161
00:12:06,124 --> 00:12:09,884
so that we did have some backward capabilities for older

162
00:12:09,932 --> 00:12:13,156
implementations. And as we'll find out

163
00:12:13,178 --> 00:12:16,816
when we get into open telemetry. Open telemetry has also considered

164
00:12:16,928 --> 00:12:20,464
how we work with existing applications in feeding

165
00:12:20,512 --> 00:12:23,824
any of those classes of data into the back end

166
00:12:23,882 --> 00:12:27,000
where we can do the aggregation and analysis.

167
00:12:27,900 --> 00:12:31,208
And so the nice thing when we really looked at this,

168
00:12:31,294 --> 00:12:35,272
is that opentelemetry provided the classes of data

169
00:12:35,326 --> 00:12:39,308
we want, provided the conduit that we could use here,

170
00:12:39,474 --> 00:12:43,404
and we could start trying to solve the specific

171
00:12:43,522 --> 00:12:47,404
problems that we were looking for of how to reach an

172
00:12:47,442 --> 00:12:50,864
observable state in a modern application made up of

173
00:12:50,902 --> 00:12:56,464
microservices under Kubernetes control. And so when

174
00:12:56,502 --> 00:12:59,808
we looked at this, we found a couple of interesting things.

175
00:12:59,894 --> 00:13:03,276
First of all, tracing metrics, logs,

176
00:13:03,388 --> 00:13:06,532
all of them are important for this observability space.

177
00:13:06,666 --> 00:13:09,584
But we also had to worry about instrumentation.

178
00:13:09,712 --> 00:13:13,460
How do we get the data out here? How do we see the

179
00:13:13,530 --> 00:13:17,240
sdks, the canonical representations coming across

180
00:13:17,310 --> 00:13:20,756
here? How standardized is the data structure?

181
00:13:20,868 --> 00:13:24,248
And then can we use it any place we want to?

182
00:13:24,334 --> 00:13:27,576
Because we didn't want to solve a single problem that

183
00:13:27,598 --> 00:13:30,764
was going to lock us in to a solution, that we

184
00:13:30,802 --> 00:13:34,508
couldn't change it to meet our ever changing needs.

185
00:13:34,674 --> 00:13:38,772
And so opentelemetry actually did provide all of those aspects,

186
00:13:38,936 --> 00:13:42,336
and this made it a lot easier for us to start looking at

187
00:13:42,358 --> 00:13:43,890
the open telemetry space.

188
00:13:45,140 --> 00:13:49,152
So when we started working with this,

189
00:13:49,286 --> 00:13:53,090
some interesting things showed up.

190
00:13:53,480 --> 00:13:56,980
First, let me note that this activity was

191
00:13:57,050 --> 00:14:00,900
a point in time snapshot, and it is based on work that we did

192
00:14:00,970 --> 00:14:04,244
roughly five months ago, maybe six months ago.

193
00:14:04,362 --> 00:14:07,204
And so the rules may have changed.

194
00:14:07,332 --> 00:14:10,712
We have not gone back and done the exercise to

195
00:14:10,766 --> 00:14:14,504
constantly keep this up to date. We will be changing and looking

196
00:14:14,542 --> 00:14:18,344
at future implementations around these development

197
00:14:18,392 --> 00:14:22,124
of open telemetry, both the standardization of the specs for

198
00:14:22,162 --> 00:14:25,832
data classes, as well as its evolving nature

199
00:14:25,896 --> 00:14:29,390
of being able to transmit that data.

200
00:14:30,160 --> 00:14:33,648
But let's start with logs, so everybody kind of

201
00:14:33,654 --> 00:14:37,264
knows what a log is, something the system decides to write off

202
00:14:37,302 --> 00:14:40,624
someplace and let us know what it thinks is going on for here.

203
00:14:40,742 --> 00:14:44,228
But that very simplicity leads to some really

204
00:14:44,314 --> 00:14:47,476
complex decisions inside of here. The simple side,

205
00:14:47,578 --> 00:14:50,564
grab the log output. Sounds simple,

206
00:14:50,762 --> 00:14:55,044
what the log output looks like, what format it is, what's included

207
00:14:55,092 --> 00:14:58,392
in there, and how can I scraped the data to make

208
00:14:58,446 --> 00:15:01,624
sense of it becomes incredibly important,

209
00:15:01,742 --> 00:15:04,696
especially when we reach production level scales here.

210
00:15:04,798 --> 00:15:07,944
So the complicated side starts to become how do we get

211
00:15:07,982 --> 00:15:11,432
the data from where it is? And remember, in a Kubernetes

212
00:15:11,496 --> 00:15:14,888
space, we don't necessarily know where it is. It could be lots

213
00:15:14,904 --> 00:15:18,936
of places, it could be ephemeral, it could be elastic, shrink and expand

214
00:15:18,968 --> 00:15:22,384
on things. Where do we store this data? We could have a lot of data

215
00:15:22,422 --> 00:15:25,568
coming in here. Do we need to index it so that we

216
00:15:25,574 --> 00:15:28,912
can actually search it easily for things

217
00:15:28,966 --> 00:15:32,116
that we expect to have happen here, and then how long do

218
00:15:32,138 --> 00:15:35,616
we retain it? Just those four questions alone

219
00:15:35,728 --> 00:15:38,836
become a really challenging point

220
00:15:38,938 --> 00:15:42,980
when we start looking at logs and production class applications.

221
00:15:43,640 --> 00:15:47,300
To be really useful, though, those log files need to be easily searchable,

222
00:15:47,380 --> 00:15:51,092
and they should be based on varying criteria. We shouldn't be limited

223
00:15:51,156 --> 00:15:54,664
to just the indexing structures. We should be able to search

224
00:15:54,782 --> 00:15:59,052
easily and efficiently on anything that we

225
00:15:59,106 --> 00:16:03,036
may want to look at it. Developer SRE platform

226
00:16:03,138 --> 00:16:06,972
operations we need to be able to find the data that we need,

227
00:16:07,106 --> 00:16:10,504
when we need it, without a huge amount of overhead.

228
00:16:10,632 --> 00:16:14,044
And so we can start looking at the various players,

229
00:16:14,092 --> 00:16:17,424
the indexing and things like this. We're also, honestly, a little

230
00:16:17,462 --> 00:16:20,496
bit, not to say it this way, but we

231
00:16:20,518 --> 00:16:23,844
like to take advantage of things that people have

232
00:16:23,882 --> 00:16:27,936
built that are efficient and work correctly. We also tend

233
00:16:27,968 --> 00:16:31,328
to favor open source, and so those things led

234
00:16:31,344 --> 00:16:35,124
us to our first viewpoint, and we did look at a number of

235
00:16:35,162 --> 00:16:38,016
them, but we started with Elasticstack.

236
00:16:38,208 --> 00:16:41,368
So Filebeat became our data transport from the

237
00:16:41,374 --> 00:16:46,148
kubernetes demon set. So it's easy to put it into kubernetes.

238
00:16:46,244 --> 00:16:49,724
It manages to put the right pieces where we need to have them go

239
00:16:49,762 --> 00:16:53,560
into, and then we transmitted that from beats

240
00:16:53,720 --> 00:16:57,420
into elasticsearch and then visualized in Cabana.

241
00:16:57,760 --> 00:17:00,832
We used bitnami as the chart to

242
00:17:00,886 --> 00:17:04,320
split that deployment. So we had an ingest engine,

243
00:17:04,390 --> 00:17:08,284
we had a coordinating engine, we had a master and data node aspects.

244
00:17:08,412 --> 00:17:12,096
The nice thing about Cabana was it let us do the

245
00:17:12,118 --> 00:17:15,888
search as well as a bunch of preloaded decisions

246
00:17:15,904 --> 00:17:18,628
that were made for us. The nice thing,

247
00:17:18,714 --> 00:17:22,656
it works. But it turned out to be extremely resource hungry.

248
00:17:22,768 --> 00:17:26,836
And the way that we had to deal with queries varied.

249
00:17:26,948 --> 00:17:30,248
It was okay, but it did vary. If you

250
00:17:30,254 --> 00:17:33,796
were doing something that was in that preloaded indexing strategy,

251
00:17:33,908 --> 00:17:37,476
it was not bad. If you were trying to do something that

252
00:17:37,518 --> 00:17:41,580
was a little bit out of their index limitations,

253
00:17:42,240 --> 00:17:46,060
life became a little bit more challenging. And so,

254
00:17:46,130 --> 00:17:49,292
as you can see here, this is one of the areas that we

255
00:17:49,346 --> 00:17:53,292
can tell you it works, but it needs to be improvement.

256
00:17:53,436 --> 00:17:57,292
One of the driving pieces behind this is that opentelemetry,

257
00:17:57,356 --> 00:18:00,896
even though it has logged as one of the classes of

258
00:18:00,918 --> 00:18:05,236
data, is in an early beta stage, and we

259
00:18:05,258 --> 00:18:08,612
didn't quite feel comfortable at this point in time.

260
00:18:08,746 --> 00:18:12,384
Depending on that emerging functionality, we wouldn't

261
00:18:12,432 --> 00:18:15,856
recommend putting it into production very honestly,

262
00:18:15,968 --> 00:18:19,560
simply because it's still in a state of change, in a state of flux.

263
00:18:20,300 --> 00:18:24,228
Great specification, probably going to be finalized very shortly.

264
00:18:24,324 --> 00:18:27,976
But then we have implementation details, SDK details,

265
00:18:28,088 --> 00:18:31,372
languages themselves. All those pieces have to come into

266
00:18:31,506 --> 00:18:33,950
consideration across that.

267
00:18:35,520 --> 00:18:38,868
Now, when we looked at the distributed tracing, so distributed

268
00:18:38,904 --> 00:18:42,496
tracing is where Opentelemetry started out of open tracing and

269
00:18:42,518 --> 00:18:46,272
opensensus here it is complex. It can also be

270
00:18:46,326 --> 00:18:49,728
semi chaotic, and we have some very

271
00:18:49,894 --> 00:18:53,604
definitive requirements. First one,

272
00:18:53,722 --> 00:18:57,670
it must not impact the quality of service for the application.

273
00:18:58,280 --> 00:19:02,036
We've all seen it where log files get written off

274
00:19:02,058 --> 00:19:05,252
in a batch format. Every hour I'm going to take

275
00:19:05,306 --> 00:19:08,872
my accumulated log files and write these off to disk and

276
00:19:08,926 --> 00:19:12,776
watched as we get a sawtooth effect of performance. Because the

277
00:19:12,798 --> 00:19:16,552
system is now busy trying to dump its buffers for here.

278
00:19:16,686 --> 00:19:20,300
We also needed to support all the languages of interest.

279
00:19:20,450 --> 00:19:24,216
In our case it was two we had Java and we had Python.

280
00:19:24,328 --> 00:19:28,296
We also had two frameworks, sprint boot, spring boot,

281
00:19:28,328 --> 00:19:32,096
sorry, misprint for that, as well as class that

282
00:19:32,118 --> 00:19:34,496
was going on for these things.

283
00:19:34,678 --> 00:19:38,224
So we wanted to make sure that we accomplished all those

284
00:19:38,262 --> 00:19:41,376
things. And what we found was something really nice. When we

285
00:19:41,398 --> 00:19:45,360
looked at Opentelemetry, Opentelemetry had this collector,

286
00:19:45,520 --> 00:19:48,996
this agent, if you will, that could pull the data,

287
00:19:49,098 --> 00:19:52,308
and it had the ability to pull that data from any of

288
00:19:52,314 --> 00:19:56,152
the independent services, but it could also in turn act

289
00:19:56,206 --> 00:19:59,764
as an aggregator. So we could feed the open telemetry

290
00:19:59,812 --> 00:20:04,090
collector from its point into an

291
00:20:05,100 --> 00:20:08,776
aggregation model, which we could then hand off in

292
00:20:08,798 --> 00:20:12,764
this case to Jaeger. So we had the ability to look at all

293
00:20:12,802 --> 00:20:15,756
the different things that we needed to look at. It also let us look at

294
00:20:15,778 --> 00:20:19,868
some unique capabilities. It was very simple to set this up.

295
00:20:20,034 --> 00:20:23,376
We could roll data through very quickly and take

296
00:20:23,398 --> 00:20:27,008
a look at what it looked like while we were in the development process

297
00:20:27,174 --> 00:20:29,570
and be able to test what was going on.

298
00:20:30,820 --> 00:20:34,224
And so here's an output of a trace. So starting from

299
00:20:34,262 --> 00:20:37,412
Nginx we can see what the front end access logs like,

300
00:20:37,546 --> 00:20:41,652
what the various fans took like. We can also look at what was happening

301
00:20:41,786 --> 00:20:45,472
once we moved through this. So each of these pieces became

302
00:20:45,536 --> 00:20:48,740
very easy to look at. This is a Jaeger chart,

303
00:20:48,900 --> 00:20:52,324
if you're not familiar with it, but most of the distributed traces.

304
00:20:52,372 --> 00:20:56,324
This acyclic chart model is something youll find very common in

305
00:20:56,382 --> 00:20:58,060
looking at distributed tracing.

306
00:21:00,880 --> 00:21:04,636
Honestly though, when we get into this space, it is really all about the

307
00:21:04,658 --> 00:21:08,124
language. And remember we have two here. Well, we started

308
00:21:08,162 --> 00:21:11,788
with Python and Python was actually really pretty straightforward.

309
00:21:11,884 --> 00:21:15,516
We added a couple of files, we updated our requirements file.

310
00:21:15,708 --> 00:21:19,376
We actually made use of Bunyan for JSON logs so

311
00:21:19,398 --> 00:21:23,156
that we could make sure that our log formats were also

312
00:21:23,258 --> 00:21:27,076
tied to it. And as youll see, one of

313
00:21:27,098 --> 00:21:30,512
the things was also we took the logging

314
00:21:30,656 --> 00:21:33,932
pieces and attached the tracing ids

315
00:21:34,016 --> 00:21:38,820
to the logs where it was useful.

316
00:21:38,980 --> 00:21:42,728
And that gave us the ability to do easy coordination across

317
00:21:42,814 --> 00:21:46,570
those, to be able to backtrack the context from a problem space.

318
00:21:48,000 --> 00:21:51,996
So Java on the other hand was a

319
00:21:52,018 --> 00:21:55,740
little bit more challenging. First of all, let me point out that

320
00:21:55,810 --> 00:21:59,436
simple Java Greenfield Java wasn't too bad. Pull the

321
00:21:59,458 --> 00:22:02,784
libraries in, use the APIs, you're pretty much done.

322
00:22:02,902 --> 00:22:06,032
And interestingly enough, when we

323
00:22:06,086 --> 00:22:09,552
started looking at this, that was these kind of the model we were using

324
00:22:09,606 --> 00:22:14,532
an existing application. And that existing application made

325
00:22:14,586 --> 00:22:18,768
come life some challenges here. So with spring

326
00:22:18,944 --> 00:22:22,116
framework it looked pretty easy. So we could

327
00:22:22,138 --> 00:22:25,284
use spring cloud. Sleuth adds the traces and span

328
00:22:25,332 --> 00:22:29,364
ids instruments common to ingress and nextpress points adds

329
00:22:29,412 --> 00:22:32,932
traces to scheduled tasks and it can directly generate

330
00:22:32,996 --> 00:22:37,288
Zipkin, another open source project like Jaeger that traces.

331
00:22:37,464 --> 00:22:40,190
But at that point in time,

332
00:22:40,960 --> 00:22:44,424
Autoconfig was a milestone release and it supported

333
00:22:44,472 --> 00:22:48,936
some really old out of date open telemetry versions.

334
00:22:49,128 --> 00:22:52,428
And we also had to pull from spring snapshot due

335
00:22:52,444 --> 00:22:55,632
to coded dependency references. And so this made it

336
00:22:55,686 --> 00:23:00,000
a little bit more problematic using looking at an old product set

337
00:23:00,150 --> 00:23:03,972
using losing functionality. Remember this, open telemetry is under

338
00:23:04,026 --> 00:23:07,924
constant development for this, as well as not being able

339
00:23:07,962 --> 00:23:11,316
to control the environments, the code

340
00:23:11,418 --> 00:23:15,156
repositories quite as easily as we wanted to for

341
00:23:15,178 --> 00:23:18,872
this. Some of these things have challenges, but when I went out right before

342
00:23:18,926 --> 00:23:22,760
this talk and still looked at this, it looks like they still

343
00:23:22,830 --> 00:23:27,220
are listing older versions of opentelemetry tracing

344
00:23:27,380 --> 00:23:31,244
and instrumentation at 111 and

345
00:23:31,282 --> 00:23:34,744
112, I believe, whereas I think the current versions are about 14

346
00:23:34,792 --> 00:23:38,316
or 15. And so need to be careful. When we

347
00:23:38,338 --> 00:23:42,332
looked at doing this, how we could make use of existing

348
00:23:42,396 --> 00:23:46,300
structures and what they're tied to became

349
00:23:46,380 --> 00:23:49,730
a dependency that we needed to constantly check across.

350
00:23:52,180 --> 00:23:55,972
So we needed to be able to work around

351
00:23:56,026 --> 00:23:59,604
some of these limitations here. And so what we decided to do was we

352
00:23:59,642 --> 00:24:03,008
built a common opentelemetry model telemetry

353
00:24:03,104 --> 00:24:06,824
way. We're passing data across these, and this provided us the ability to

354
00:24:06,862 --> 00:24:10,232
extend our tracing functionality and so we could build

355
00:24:10,286 --> 00:24:13,332
auto confusion classes and add additional

356
00:24:13,396 --> 00:24:16,872
trace attributes. There were certain pieces of information that we

357
00:24:16,926 --> 00:24:19,790
really wanted to make sure were carried forward,

358
00:24:20,640 --> 00:24:24,728
as well as certain things that we wanted to make sure we're constantly

359
00:24:24,824 --> 00:24:28,236
up to date and shared. We also wanted to

360
00:24:28,258 --> 00:24:31,536
be able to clearly see what the impact was going to be.

361
00:24:31,638 --> 00:24:35,152
So we built an implementation of no

362
00:24:35,206 --> 00:24:38,668
ops, so a no op implementation, production durage tracing

363
00:24:38,764 --> 00:24:42,336
that we can simply flip on and off and so we can see what the

364
00:24:42,358 --> 00:24:45,492
impact of tracing versus no tracing looks like here.

365
00:24:45,626 --> 00:24:48,756
We decided to standardize our trace names. They're across

366
00:24:48,858 --> 00:24:51,984
different languages, crossing different structures.

367
00:24:52,112 --> 00:24:55,284
So we needed to be able to look and see how we did this.

368
00:24:55,322 --> 00:24:59,348
And that allowed us to do this. We added an error handler

369
00:24:59,444 --> 00:25:03,204
so that we could output errors to both logs and tracings.

370
00:25:03,332 --> 00:25:07,268
This gives us the ability to coordinate back across those pieces here.

371
00:25:07,374 --> 00:25:11,400
And then we actually added some additional tracing attributes,

372
00:25:11,480 --> 00:25:15,324
a service name, the instance id, machine id. There's a few others

373
00:25:15,362 --> 00:25:18,844
that are there here, but we also made sure

374
00:25:19,042 --> 00:25:22,988
we're in a database world that we put the trace id into the comments

375
00:25:23,084 --> 00:25:26,880
that preceded our SQL statements. It's really important

376
00:25:26,950 --> 00:25:30,624
to know where your SQL statements are coming from,

377
00:25:30,742 --> 00:25:34,932
what request generated them, all the way back to what the user started them

378
00:25:34,986 --> 00:25:38,900
looking like here. All of those things were built

379
00:25:38,970 --> 00:25:42,544
into an open source module, Opentelemetry Nginx

380
00:25:42,592 --> 00:25:46,276
module, which you can find on our Nginx Inc.

381
00:25:46,378 --> 00:25:47,590
GitHub site.

382
00:25:51,020 --> 00:25:54,264
So the last part was the metrics piece.

383
00:25:54,382 --> 00:25:57,260
And honestly we kind of skipped python.

384
00:25:58,160 --> 00:26:01,640
It's not difficult to put metrics into python,

385
00:26:01,720 --> 00:26:05,612
but the type and class of metrics we were getting from those front

386
00:26:05,666 --> 00:26:09,116
end pieces was not really meaningful for what we

387
00:26:09,138 --> 00:26:12,896
were trying to accomplish as part of this. And so we

388
00:26:12,918 --> 00:26:16,060
then broke it down and said, okay, so what are we doing in Java?

389
00:26:16,220 --> 00:26:19,600
So Java, the original code bank of Anthros

390
00:26:20,020 --> 00:26:24,064
used something called micrometer and used connected to GCP

391
00:26:24,112 --> 00:26:27,620
stackdriver. And when we looked at that,

392
00:26:27,690 --> 00:26:31,056
we found that micrometer was actually a very mature layer for Java

393
00:26:31,088 --> 00:26:34,964
virtual machines and the default metrics API in spring.

394
00:26:35,092 --> 00:26:38,196
And so micrometer became near and dear to our hearts

395
00:26:38,228 --> 00:26:41,544
very quickly. We also looked at the current

396
00:26:41,662 --> 00:26:45,850
opentelemetry work for metrics. And at that point in time,

397
00:26:46,620 --> 00:26:50,220
and actually I will admit still today, there are still

398
00:26:50,290 --> 00:26:53,660
some discussions going on around what limits there are to

399
00:26:53,730 --> 00:26:57,852
otel metrics, in particular, what kind of metrics are not

400
00:26:57,986 --> 00:27:01,916
being covered, what kind of metrics are missing inside of these picture?

401
00:27:02,028 --> 00:27:05,980
Open telemetry metrics are stable, specification is stable, the languages

402
00:27:06,060 --> 00:27:09,216
vary. But is it complete enough of

403
00:27:09,238 --> 00:27:12,964
what you want? Well, the nice thing

404
00:27:13,162 --> 00:27:17,204
was that opentelemetry and micrometer together

405
00:27:17,402 --> 00:27:20,000
were not a blocking factor.

406
00:27:20,160 --> 00:27:23,836
So if you remember back that opentelemetry collector, the opentelemetry

407
00:27:23,888 --> 00:27:27,576
collector can use metrics from anything. It can pull

408
00:27:27,598 --> 00:27:31,860
them from Prometheus stats id, it can pull them from opentelemetry

409
00:27:31,940 --> 00:27:35,672
line protocol OTLP, it can pull pretty much

410
00:27:35,726 --> 00:27:40,030
anything. And that's the strength of the collector is the ability to

411
00:27:41,600 --> 00:27:45,804
export things and collect the various data and

412
00:27:45,842 --> 00:27:49,096
then process it in the middle. Micrometer supported

413
00:27:49,128 --> 00:27:52,524
lots of actions. It already supported Prometheus, it already supported

414
00:27:52,572 --> 00:27:55,884
statsd. And so these was obviously a natural

415
00:27:55,932 --> 00:27:59,424
fit. We could have micrometer produce Prometheus and these

416
00:27:59,462 --> 00:28:03,076
use the collector to coordinate and aggregate our

417
00:28:03,098 --> 00:28:06,480
data together. The opentelemetry metrics SDK

418
00:28:06,640 --> 00:28:09,952
allowed us to then send those metrics via

419
00:28:10,016 --> 00:28:13,604
OTLP. And so we could use the micrometer model,

420
00:28:13,722 --> 00:28:17,460
but use OTLP as the collection agent

421
00:28:17,530 --> 00:28:20,904
of choice. From these we could send them to anything. We could send them to

422
00:28:20,942 --> 00:28:24,532
lightstep, we could send these to Grafana, we could send them anywhere

423
00:28:24,596 --> 00:28:28,024
we want to. Again, because we have that

424
00:28:28,062 --> 00:28:31,576
receiver concept and that export concept.

425
00:28:31,768 --> 00:28:35,660
The thing that we did ended up was that this is not a streaming model,

426
00:28:35,730 --> 00:28:39,076
this was actually a tools model for the collector, and the collector

427
00:28:39,128 --> 00:28:42,256
would pull the various metrics to be able

428
00:28:42,278 --> 00:28:43,650
to pass them forward.

429
00:28:46,100 --> 00:28:49,424
So without too much more, a quick

430
00:28:49,462 --> 00:28:51,892
summary for what we're going on here,

431
00:28:52,026 --> 00:28:55,270
and this is a more complete list.

432
00:28:55,640 --> 00:28:59,056
So when we did this, opentelemetry was clearly

433
00:28:59,088 --> 00:29:02,820
the right choice for dealing and production categories for

434
00:29:02,890 --> 00:29:06,920
observability here for distributed tracing, we did

435
00:29:06,990 --> 00:29:10,948
have a number of interesting pieces.

436
00:29:11,044 --> 00:29:15,460
So Java used spring cloud sleuth with the Otel exporter,

437
00:29:15,540 --> 00:29:20,168
leading to the Otel collector which then led to a pluggable store python

438
00:29:20,264 --> 00:29:24,700
used the Python libraries, the opentelemetry collector to the pluggable store,

439
00:29:24,850 --> 00:29:28,188
and Nginx, which was the driving heart

440
00:29:28,274 --> 00:29:32,272
for this application space. The ingress controller for these

441
00:29:32,326 --> 00:29:35,648
is not currently otelized, not currently

442
00:29:35,814 --> 00:29:39,244
traceable for here, but we had an Otel module

443
00:29:39,372 --> 00:29:42,964
that we can pull data in from NginX itself to the

444
00:29:43,002 --> 00:29:46,292
collector. The metrics piece was

445
00:29:46,346 --> 00:29:49,492
one of those that we thought was going to be easier, but actually

446
00:29:49,546 --> 00:29:53,600
turned out to be easy with some caveats.

447
00:29:53,680 --> 00:29:57,176
For this we used micrometer via spring to a

448
00:29:57,198 --> 00:29:59,668
Prometheus exporter to the collector.

449
00:29:59,844 --> 00:30:03,844
Likewise with Python we use unicorn

450
00:30:03,972 --> 00:30:08,328
with statsd to Prometheus via the service monitor

451
00:30:08,504 --> 00:30:11,784
out to hotel collector Python. We did not implement

452
00:30:11,832 --> 00:30:15,932
metrics for and in NgInx we have a Prometheus endpoint that we

453
00:30:15,986 --> 00:30:19,244
also passed to Prometheus and then logs.

454
00:30:19,292 --> 00:30:22,512
We collected all the container log files through

455
00:30:22,566 --> 00:30:26,092
filebeat, which went into elasticsearch and were exposable

456
00:30:26,156 --> 00:30:29,296
via cabana. Is this perfect?

457
00:30:29,478 --> 00:30:33,180
No, we would like to have a more centralized

458
00:30:33,260 --> 00:30:36,484
source. We would like to be able to pull all these things together and have

459
00:30:36,522 --> 00:30:40,148
them easily correlated without necessarily having to chose lots of

460
00:30:40,154 --> 00:30:44,344
different tools with lots of different query structures to this.

461
00:30:44,462 --> 00:30:48,292
But this was a demonstration of the fact that Otel

462
00:30:48,356 --> 00:30:51,784
can become the underlying structure for any

463
00:30:51,822 --> 00:30:55,540
of these pieces. So our summary

464
00:30:55,620 --> 00:30:59,300
that continues out that I didn't cover here error aggregation

465
00:30:59,380 --> 00:31:02,968
we use Otel to distribute traces for health checks.

466
00:31:02,984 --> 00:31:06,712
We have a spring boot actuator through Kubernetes and Python

467
00:31:06,776 --> 00:31:10,224
flask management endpoints fed into kubernetes for

468
00:31:10,262 --> 00:31:13,724
this introspection. Again bring in Python

469
00:31:13,772 --> 00:31:17,536
had the ability to allow us do introspection and then with the

470
00:31:17,558 --> 00:31:21,844
heap core dumps, Java spring boot actuator let

471
00:31:21,882 --> 00:31:25,140
us have thread dumps. And right now we do not have support

472
00:31:25,210 --> 00:31:28,996
for this in Python. So the

473
00:31:29,018 --> 00:31:32,356
TLDR for this three classes of

474
00:31:32,378 --> 00:31:36,016
data metrics, traces and logs. All of these took different approaches

475
00:31:36,048 --> 00:31:40,024
to get what we wanted, but we managed to get what we wanted out

476
00:31:40,062 --> 00:31:43,864
of all of those. And the open telemetry collector is

477
00:31:43,902 --> 00:31:47,396
the thing that made this actually work. The open television

478
00:31:47,508 --> 00:31:50,712
collector is our friend, and it made it incredibly

479
00:31:50,776 --> 00:31:54,364
possible to youll off what six months ago looked like a

480
00:31:54,402 --> 00:31:58,236
very challenge project, by the way. It was

481
00:31:58,258 --> 00:32:01,448
a very challenging project. The people that were

482
00:32:01,474 --> 00:32:05,264
helping code this used to scream bloody murder at

483
00:32:05,302 --> 00:32:08,416
various times. It's probably easier now, and we

484
00:32:08,438 --> 00:32:11,990
will be revisiting this in the next few months just to see what's happening.

485
00:32:12,440 --> 00:32:16,192
Metrics and traces had some interesting gotchas based on the languages

486
00:32:16,256 --> 00:32:20,500
that we were looking at. Also, metrics and traces in

487
00:32:20,570 --> 00:32:23,712
conjunction with logs needed to be correlated

488
00:32:23,776 --> 00:32:27,376
and so we needed to make sure that we could have those trace ids

489
00:32:27,488 --> 00:32:31,352
show up crossing into our log files so that we could trap them.

490
00:32:31,486 --> 00:32:35,256
And as I said before, this was a snapshot in time. This was as the

491
00:32:35,278 --> 00:32:39,244
state of the industry was about six months ago for

492
00:32:39,282 --> 00:32:43,176
this. And things have changed. We have flat out will admit

493
00:32:43,208 --> 00:32:46,796
that things have changed and we will be looking at updating this to

494
00:32:46,818 --> 00:32:50,656
see how the changes impact our decisions. We're also going

495
00:32:50,678 --> 00:32:54,016
to be looking at other solutions. Should we go to

496
00:32:54,038 --> 00:32:57,836
Loki for log files or should we look at Greylog?

497
00:32:57,868 --> 00:33:01,760
Or can we start making use of the nascent

498
00:33:02,260 --> 00:33:06,176
Otel logging information? And by the way, there are auto

499
00:33:06,208 --> 00:33:09,264
configuration instrumentation files and they are amazingly

500
00:33:09,312 --> 00:33:12,804
cool. Drop it into your Java file and you get all these great

501
00:33:12,842 --> 00:33:15,992
traces out. But they don't necessarily give you what you

502
00:33:16,046 --> 00:33:18,650
want. And so when it works,

503
00:33:19,260 --> 00:33:22,730
it's great and when it gives you what you want,

504
00:33:23,500 --> 00:33:27,828
you can also take this for yourself. We built this into an

505
00:33:27,934 --> 00:33:31,244
open source category. Take a look at

506
00:33:31,282 --> 00:33:34,572
our repositories, play with the application itself

507
00:33:34,706 --> 00:33:38,616
and give us some feedback on what you're interested in hearing

508
00:33:38,648 --> 00:33:42,216
from or where you'd like to see us next. For that, feel free to join

509
00:33:42,328 --> 00:33:45,784
our community and look for the Mara

510
00:33:45,832 --> 00:33:49,800
modern applications reference architecture and give us feedback.

511
00:33:49,880 --> 00:33:53,492
Tell us what you're thinking about this and tell us what your

512
00:33:53,546 --> 00:33:57,508
world looks like with that. Let me thank you for

513
00:33:57,674 --> 00:34:00,470
listening to me today, and again,

514
00:34:01,080 --> 00:34:04,964
I hope you enjoy the rest of the conference. Thanks and have a good

515
00:34:05,002 --> 00:34:05,636
day.

