1
00:00:25,570 --> 00:00:28,534
You. Thanks to be here for this.

2
00:00:28,572 --> 00:00:32,070
Talk about an introduction to opentelemetry tracing.

3
00:00:33,530 --> 00:00:37,250
I'm Nicola Frankel, I've been a developer

4
00:00:37,330 --> 00:00:40,614
or an architect or whatever for a long time,

5
00:00:40,652 --> 00:00:44,614
like more than 20 years nowadays, and since a

6
00:00:44,652 --> 00:00:48,070
couple of years I'm also a developer advocate.

7
00:00:49,970 --> 00:00:51,840
So given my history,

8
00:00:53,010 --> 00:00:56,462
I can tell you about what happened a long time

9
00:00:56,516 --> 00:00:59,870
ago, perhaps even

10
00:00:59,940 --> 00:01:03,220
before some of you knew about it.

11
00:01:03,990 --> 00:01:07,060
So in the good old days, or not so good,

12
00:01:07,430 --> 00:01:10,686
we had monitoring. And monitoring

13
00:01:10,878 --> 00:01:14,722
meant that you has a bunch of people like

14
00:01:14,776 --> 00:01:18,870
synthing in front of a huge screen full

15
00:01:18,940 --> 00:01:22,514
of widgets, with dashboards,

16
00:01:22,642 --> 00:01:26,486
with graphs, with whatever. Sometimes they even add

17
00:01:26,588 --> 00:01:30,346
an additional screen in front of them and they were actually

18
00:01:30,528 --> 00:01:33,782
monitoring, like visually monitoring

19
00:01:33,926 --> 00:01:37,594
the situation. Sometimes you has

20
00:01:37,632 --> 00:01:40,842
even alerting or alerting came afterwards

21
00:01:40,986 --> 00:01:45,280
built. Basically monitoring meant you had low level

22
00:01:45,650 --> 00:01:49,534
information into your information system and

23
00:01:49,572 --> 00:01:53,462
you had to have people who could realize

24
00:01:53,546 --> 00:01:57,294
that hey, this slight anomaly

25
00:01:57,342 --> 00:02:00,866
here and this slight anomaly here meant that

26
00:02:00,968 --> 00:02:03,460
actually something bad was happening.

27
00:02:05,450 --> 00:02:09,560
It was working more or less, but actually

28
00:02:10,010 --> 00:02:12,550
system became more distributed.

29
00:02:13,050 --> 00:02:16,422
You can do this kind of stuff on

30
00:02:16,476 --> 00:02:20,442
a huge monolith built when you have

31
00:02:20,576 --> 00:02:24,790
components starting to interact across the network

32
00:02:24,950 --> 00:02:28,182
with different machines, with different nodes,

33
00:02:28,326 --> 00:02:31,962
then having this kind of insight

34
00:02:32,026 --> 00:02:35,470
into the system through experience becomes very very

35
00:02:35,540 --> 00:02:39,018
hard. And so monitoring

36
00:02:39,194 --> 00:02:42,430
gave way to something called observability.

37
00:02:42,870 --> 00:02:46,354
So you can think about observability as the

38
00:02:46,392 --> 00:02:49,870
next level, as to provide insight

39
00:02:49,950 --> 00:02:53,860
into not only one single component, but about

40
00:02:54,230 --> 00:02:58,614
distributed system. And there actually are

41
00:02:58,812 --> 00:03:01,986
three main pillars

42
00:03:02,178 --> 00:03:05,890
of observability. The first is metric,

43
00:03:06,050 --> 00:03:09,170
the second is logging and the third is tracing.

44
00:03:09,250 --> 00:03:12,198
And though I will focus this talk on tracing,

45
00:03:12,294 --> 00:03:15,850
I still need to introduce the others two because

46
00:03:16,000 --> 00:03:19,878
they make a lot of sense and probably you need to be aware

47
00:03:19,894 --> 00:03:23,486
of them anyway. So as I mentioned,

48
00:03:23,668 --> 00:03:27,838
the first step to monitoring was to have metrics. And it was easy

49
00:03:27,924 --> 00:03:31,598
because nearly all operating system give

50
00:03:31,684 --> 00:03:35,154
some way to release the data of how

51
00:03:35,192 --> 00:03:39,182
they operate. So you can have the cpu usage,

52
00:03:39,326 --> 00:03:43,022
the memory, the swap memory, the number of thread

53
00:03:43,086 --> 00:03:47,560
or whatever. And this is very easy to get.

54
00:03:47,930 --> 00:03:51,954
But as I mentioned, if your system become more complex

55
00:03:52,082 --> 00:03:55,734
and evolves more machines, it's not possible to

56
00:03:55,772 --> 00:03:59,654
have them. So nowadays we still rely

57
00:03:59,702 --> 00:04:03,414
on those metrics, but we want to have higher level metrics.

58
00:04:03,462 --> 00:04:06,570
They can be like still technical metrics, such as

59
00:04:06,640 --> 00:04:10,614
the number of requests per second, or the HTTP

60
00:04:10,662 --> 00:04:14,298
status code, or it can be like business

61
00:04:14,384 --> 00:04:17,566
metrics as the number of orders and this

62
00:04:17,588 --> 00:04:21,182
kind of stuff now

63
00:04:21,236 --> 00:04:24,914
comes logging. Logging is another level of

64
00:04:24,952 --> 00:04:29,122
complexity, logging. There are

65
00:04:29,176 --> 00:04:32,898
different questions you need to solve. The first question is

66
00:04:33,064 --> 00:04:36,582
what to log. For example, in the past

67
00:04:36,716 --> 00:04:40,502
it happened that I used like a Java agent

68
00:04:40,636 --> 00:04:44,402
to log whenever I entered a method and whenever

69
00:04:44,466 --> 00:04:47,794
I exited a method. So when I read the log

70
00:04:47,852 --> 00:04:51,210
files I could see which steps were

71
00:04:51,280 --> 00:04:55,174
used so I could do some kind of debugging in production

72
00:04:55,222 --> 00:04:59,146
just by looking at the log. This is good,

73
00:04:59,248 --> 00:05:02,910
this gives me come insight. Actually it's more like tracing,

74
00:05:04,210 --> 00:05:07,966
but it doesn't tell much about

75
00:05:08,148 --> 00:05:11,070
the business side of things. So again,

76
00:05:11,140 --> 00:05:14,782
perhaps we want to log more interesting stuff such

77
00:05:14,836 --> 00:05:18,706
as who is connected, what their session id,

78
00:05:18,808 --> 00:05:22,354
how much items do they have

79
00:05:22,392 --> 00:05:26,338
in their come this kind of stuff? This is much harder

80
00:05:26,434 --> 00:05:29,378
and it requests manual introversion.

81
00:05:29,554 --> 00:05:33,490
So you need to have a requirement and the developer

82
00:05:33,570 --> 00:05:37,830
like actually writing the codes to log this. Of course

83
00:05:37,980 --> 00:05:41,530
with auto instrumentation you cannot log every

84
00:05:41,600 --> 00:05:44,746
parameter of a method by default because some of them might

85
00:05:44,768 --> 00:05:47,690
be a password and you don't want to log the password.

86
00:05:48,430 --> 00:05:52,266
So auto instrumentation in this case of logging

87
00:05:52,378 --> 00:05:55,950
is easy built, doesn't provide much value

88
00:05:56,020 --> 00:06:00,270
and can be actually harmful to the operation.

89
00:06:01,170 --> 00:06:04,626
Then the logging formats. For a

90
00:06:04,648 --> 00:06:08,180
long time we use whatever logging formats the

91
00:06:09,110 --> 00:06:12,674
framework provided us. Whether you are using sLf four

92
00:06:12,712 --> 00:06:16,040
j log four j, log four j whatever.

93
00:06:16,410 --> 00:06:20,514
But nowadays it's perhaps better to use directly

94
00:06:20,562 --> 00:06:24,086
JSoN, so that when you send it to another

95
00:06:24,188 --> 00:06:27,438
system which expect JSON format, you don't

96
00:06:27,474 --> 00:06:30,746
need to have an intermediate operation that actually

97
00:06:30,848 --> 00:06:34,006
transform the human readable log into JSON

98
00:06:34,038 --> 00:06:37,910
format, like directly print it in JSON formats

99
00:06:37,990 --> 00:06:42,266
and you avoid one additional potentially

100
00:06:42,378 --> 00:06:46,094
expensive operation. Then where to

101
00:06:46,132 --> 00:06:50,442
log is also important. When I started coding,

102
00:06:50,586 --> 00:06:54,434
I was explicitly forbidden to write to

103
00:06:54,472 --> 00:06:58,642
console like if you use like

104
00:06:58,696 --> 00:07:02,754
sonar or any other quality tool,

105
00:07:02,872 --> 00:07:06,946
they will tell you hey, it's forbidden to use system out

106
00:07:07,048 --> 00:07:10,338
print ln or system er

107
00:07:10,504 --> 00:07:16,130
print ln. You don't write console nowadays.

108
00:07:16,210 --> 00:07:19,994
However with containers you probably will want

109
00:07:20,032 --> 00:07:24,490
to write to the console so that it can be scrapped and again

110
00:07:24,640 --> 00:07:26,490
like sent further.

111
00:07:28,190 --> 00:07:31,786
Logging on a system is good when you've

112
00:07:31,818 --> 00:07:35,070
got a single components, or perhaps two components.

113
00:07:36,210 --> 00:07:40,394
As soon as you've got a distributed system, you need to aggregate

114
00:07:40,442 --> 00:07:44,100
the logs together so as to understand again

115
00:07:44,710 --> 00:07:47,970
what happens across all those components.

116
00:07:48,550 --> 00:07:52,354
Therefore you need to get all

117
00:07:52,392 --> 00:07:55,974
the logs from all the components into

118
00:07:56,092 --> 00:07:59,734
a single place, into a centralized logging system.

119
00:07:59,932 --> 00:08:03,254
And again, that's more question basically you

120
00:08:03,292 --> 00:08:06,806
need to ask yourself should I send the

121
00:08:06,828 --> 00:08:10,058
logs? And basically meaning that

122
00:08:10,144 --> 00:08:14,166
my application will lose some performance

123
00:08:14,358 --> 00:08:18,380
to send it and actually might crash the app,

124
00:08:18,750 --> 00:08:22,846
or do I expose only the log so

125
00:08:22,868 --> 00:08:25,854
that another component can scrap them.

126
00:08:26,052 --> 00:08:29,150
This is how Prometheus works, for example,

127
00:08:29,220 --> 00:08:32,846
you add some endpoint and Prometheus will scrap the

128
00:08:32,868 --> 00:08:35,540
metrics. Loki will do the same.

129
00:08:37,910 --> 00:08:41,362
I mentioned about parsing the logs again,

130
00:08:41,416 --> 00:08:44,946
it's better to actually write them in the format that

131
00:08:44,968 --> 00:08:49,166
the centralized logging system can directly exploit than

132
00:08:49,208 --> 00:08:52,802
to add an additional operation

133
00:08:52,866 --> 00:08:56,854
in the middle. Then you need to store the log, of course.

134
00:08:57,052 --> 00:09:00,642
Then you need to somehow search for the log because,

135
00:09:00,716 --> 00:09:04,026
well, just having the logs one by one is not interesting.

136
00:09:04,208 --> 00:09:08,106
So you need to search for them according to some filters, for example a

137
00:09:08,128 --> 00:09:11,840
timestamp, for example a transaction id or anything.

138
00:09:12,530 --> 00:09:16,734
And then you need to display them in a certain way to

139
00:09:16,772 --> 00:09:20,270
look for the interesting bits, for example

140
00:09:20,420 --> 00:09:24,254
the components that produce them. And I didn't write it,

141
00:09:24,292 --> 00:09:28,078
but of course you need to somehow afterwards get rid of the logs.

142
00:09:28,174 --> 00:09:32,018
Otherwise your logging system will grow and grow and grow and

143
00:09:32,104 --> 00:09:35,800
well, you probably will have your disk full in no time.

144
00:09:37,850 --> 00:09:42,198
You probably used one of those systems in

145
00:09:42,204 --> 00:09:46,354
the past. I've been using elasticsearch a lot, it's quite useful,

146
00:09:46,482 --> 00:09:49,642
but any other system will

147
00:09:49,696 --> 00:09:53,770
do. Then comes the third

148
00:09:53,840 --> 00:09:57,542
components, which is tracing the Wikipedia

149
00:09:57,606 --> 00:10:02,938
definition. I love Wikipedia definition. In my opinion,

150
00:10:03,034 --> 00:10:05,886
this one is not a great one.

151
00:10:06,068 --> 00:10:09,898
So I've come up with my own, you will excuse me, probably it's

152
00:10:09,914 --> 00:10:13,194
not my own I've got inspired by lots of people that

153
00:10:13,332 --> 00:10:17,186
I don't remember but well, credits to them.

154
00:10:17,288 --> 00:10:20,770
So basically, tracing is a set of techniques and tools

155
00:10:21,350 --> 00:10:25,134
that help follow a business request across

156
00:10:25,192 --> 00:10:27,750
the network through multiple components.

157
00:10:28,090 --> 00:10:31,078
This is really really important. Again,

158
00:10:31,164 --> 00:10:34,754
in distributed system you've got multiple components.

159
00:10:34,802 --> 00:10:38,550
Your transaction, your business transaction will require

160
00:10:38,630 --> 00:10:41,926
all those components to work together to achieve

161
00:10:41,958 --> 00:10:45,766
the business goal. If one of them fails,

162
00:10:45,878 --> 00:10:49,942
you're in trouble. And if repeatedly the same

163
00:10:50,016 --> 00:10:53,962
component that says an issue, you need to be aware of it. So tracing

164
00:10:54,026 --> 00:10:57,520
the request across all those components is important.

165
00:11:00,530 --> 00:11:03,822
I believe you probably are aware about some

166
00:11:03,876 --> 00:11:06,994
of the tracing pioneers existing already.

167
00:11:07,112 --> 00:11:09,614
So there is zipkin, Yeager,

168
00:11:09,742 --> 00:11:13,138
Openfresync. Those are the three

169
00:11:13,224 --> 00:11:16,630
most like, widespread and famous.

170
00:11:19,770 --> 00:11:23,190
Every one of them has proprietary

171
00:11:24,410 --> 00:11:28,026
interfaces and implementation. But as you

172
00:11:28,048 --> 00:11:31,434
know, we want something to be standardized so that the

173
00:11:31,472 --> 00:11:34,826
whole software that

174
00:11:34,848 --> 00:11:38,570
you write, we can write libraries that have

175
00:11:38,640 --> 00:11:41,850
one standard. And for that there is something called

176
00:11:42,000 --> 00:11:46,190
the double width three C trace context configuration.

177
00:11:46,610 --> 00:11:50,734
It's quite easy to read, so you can already do

178
00:11:50,772 --> 00:11:55,026
that at home. And basically if

179
00:11:55,048 --> 00:11:57,826
you don't have time, let me refresh it for you.

180
00:11:57,928 --> 00:12:01,838
Basically you've got two important concepts. The first concept

181
00:12:01,934 --> 00:12:05,462
is the idea of a trace, and trace is

182
00:12:05,516 --> 00:12:09,046
actually like the abstraction of the business

183
00:12:09,148 --> 00:12:13,042
transaction. So the trace will go from entry

184
00:12:13,106 --> 00:12:16,562
points to the ute, most deeply

185
00:12:16,626 --> 00:12:19,386
nested components and back again.

186
00:12:19,488 --> 00:12:23,782
That's the trace. And then you've got a span. The span

187
00:12:23,846 --> 00:12:27,190
is actually the execution

188
00:12:27,350 --> 00:12:31,310
of one port of the trace in a single

189
00:12:31,380 --> 00:12:34,894
component. And you can have, of course for each

190
00:12:34,932 --> 00:12:38,720
component you can have multiple spans. If you are interested

191
00:12:39,250 --> 00:12:42,802
where inside this components where

192
00:12:42,856 --> 00:12:46,770
the flow of the request goes and each

193
00:12:46,840 --> 00:12:50,594
of those spans are bounded together in a

194
00:12:50,632 --> 00:12:54,466
parent could relationship. So the first one is

195
00:12:54,488 --> 00:12:57,830
the important one, the first component is the important one. It gives

196
00:12:57,980 --> 00:13:01,270
the first span id and

197
00:13:01,340 --> 00:13:04,994
then the next component will check the span

198
00:13:05,042 --> 00:13:07,894
id of the parent and will say hey,

199
00:13:08,092 --> 00:13:11,494
I am this span id. So it also generates a new span

200
00:13:11,542 --> 00:13:15,018
id and say hey, I'm bound with this one. And through

201
00:13:15,104 --> 00:13:18,474
all those parential relationship you

202
00:13:18,512 --> 00:13:22,622
can actually make sense of the flow and check

203
00:13:22,756 --> 00:13:27,040
the flow overview in one set.

204
00:13:32,170 --> 00:13:35,642
Opentelemetry is a set of tools that

205
00:13:35,696 --> 00:13:39,574
implements observerability

206
00:13:39,702 --> 00:13:43,178
and that relies actually on

207
00:13:43,344 --> 00:13:46,694
double three C trace context.

208
00:13:46,742 --> 00:13:50,174
So it's an implementation and more so

209
00:13:50,212 --> 00:13:54,490
it's compatible. And it also go beyond

210
00:13:54,570 --> 00:13:58,750
because w

211
00:13:58,900 --> 00:14:02,786
three c is good for web stuff, but perhaps you want

212
00:14:02,808 --> 00:14:06,482
to trace a request through, I don't know,

213
00:14:06,536 --> 00:14:10,306
Kafka. So somewhere where you can store and

214
00:14:10,408 --> 00:14:13,734
in this case the specification does not

215
00:14:13,772 --> 00:14:18,390
handle it. Opentelemetry allows you to trace this

216
00:14:18,540 --> 00:14:21,874
business transaction across many different components,

217
00:14:21,922 --> 00:14:23,560
not only web ones.

218
00:14:25,290 --> 00:14:29,238
It's a merge of the open tracing and open census

219
00:14:29,334 --> 00:14:32,954
project. So it's one of those few merges that

220
00:14:32,992 --> 00:14:36,646
were successful where people decided to joined their efforts

221
00:14:36,758 --> 00:14:40,798
to create something better. It has become a CNCF project,

222
00:14:40,884 --> 00:14:44,526
which is good, meaning that it has support. It will be supported for

223
00:14:44,548 --> 00:14:48,474
a long time. It's licensed under the Apache

224
00:14:48,522 --> 00:14:52,100
license, so it's also good if you want to use it right away.

225
00:14:52,630 --> 00:14:56,178
You don't need to acquire a license or whatever,

226
00:14:56,344 --> 00:15:00,190
and it's popular,

227
00:15:00,350 --> 00:15:04,194
especially on GitHub. The architecture

228
00:15:04,242 --> 00:15:08,054
in itself is quite easy. Basically you've got

229
00:15:08,172 --> 00:15:10,920
components, whatever they are,

230
00:15:11,290 --> 00:15:15,266
and then you've got what they call an opentelemetry

231
00:15:15,298 --> 00:15:19,910
collector. And this opentelemetry collector

232
00:15:21,230 --> 00:15:25,446
accepts data in a specific format,

233
00:15:25,638 --> 00:15:29,402
and this specific format allows to have these light parential

234
00:15:29,466 --> 00:15:32,750
relationship between spans. So the idea

235
00:15:32,820 --> 00:15:36,622
is on the client side you've got

236
00:15:36,676 --> 00:15:40,690
stuff that dumps data into the hotel collector

237
00:15:41,030 --> 00:15:44,754
and then you've got something that is able

238
00:15:44,872 --> 00:15:48,446
to search and display data from the hotel

239
00:15:48,558 --> 00:15:52,894
collector. The Opentelemetry collector

240
00:15:53,022 --> 00:15:56,294
in itself doesn't provide anything, it's just the

241
00:15:56,332 --> 00:15:59,110
storage stuff in a certain format.

242
00:15:59,610 --> 00:16:04,082
So we need something afterwards Opentelemetry provides

243
00:16:04,146 --> 00:16:07,400
a dedicated collector, but actually

244
00:16:08,010 --> 00:16:11,340
Jaeger and Zipkin, which are also

245
00:16:12,110 --> 00:16:16,010
like tracing providers, they are able to provide

246
00:16:16,080 --> 00:16:19,374
the same collector, or let's put it

247
00:16:19,412 --> 00:16:24,026
that way, they also provide a collector which accepts

248
00:16:24,218 --> 00:16:27,710
opentelemetry data. So basically what they did,

249
00:16:27,780 --> 00:16:31,842
they kept the storage engine. They added a new interface where

250
00:16:31,896 --> 00:16:35,198
you can state your data in open telemetry format.

251
00:16:35,294 --> 00:16:39,230
If you already have your architecture, your tracing

252
00:16:39,310 --> 00:16:43,650
architecture, you can easily move to Opentelemetry because,

253
00:16:43,800 --> 00:16:47,274
well, the collectors of Zipkin and Jaeger,

254
00:16:47,422 --> 00:16:51,062
they have this additional interface. You just need

255
00:16:51,196 --> 00:16:54,366
to change the formats and the ports

256
00:16:54,418 --> 00:16:58,170
because I think every one of them has different ports.

257
00:17:00,510 --> 00:17:03,062
On the client side it's,

258
00:17:03,206 --> 00:17:06,474
well, I wouldn't say easy, but the first

259
00:17:06,512 --> 00:17:10,730
step is straightforward. First step is auto instrumentation.

260
00:17:10,890 --> 00:17:13,930
This is only available when you've got a runtime.

261
00:17:14,010 --> 00:17:17,422
For example on the Java side

262
00:17:17,476 --> 00:17:21,202
you have the GM, that is a runtime. On the Python side,

263
00:17:21,256 --> 00:17:25,250
well, Python is a runtime node, JS is a runtime.

264
00:17:26,630 --> 00:17:30,146
In this case you will delegate to the runtime to

265
00:17:30,168 --> 00:17:34,280
do like auto instrumentation. I told you about

266
00:17:35,530 --> 00:17:38,854
automatically logging, entering and

267
00:17:38,892 --> 00:17:42,406
exiting a method. It's exactly the same here.

268
00:17:42,508 --> 00:17:45,290
You will do that automatically.

269
00:17:46,670 --> 00:17:50,586
It gives you already a lot of insight. Now if you want to

270
00:17:50,608 --> 00:17:54,170
go further, you can actually get

271
00:17:54,240 --> 00:17:56,982
the library depending on your tech stack.

272
00:17:57,046 --> 00:18:01,130
Again, you can check the Opentelemetry websites. You will notice

273
00:18:01,290 --> 00:18:04,766
there are lots and lots of stacks that are supported out of

274
00:18:04,788 --> 00:18:08,586
the box. And for Java there is one. For Python,

275
00:18:08,618 --> 00:18:11,778
there is one. For rust there is one.

276
00:18:11,944 --> 00:18:15,442
Whatever rocks your boat, probably you will find there.

277
00:18:15,576 --> 00:18:19,266
And then you can either call an

278
00:18:19,288 --> 00:18:21,330
API or use some annotation.

279
00:18:22,890 --> 00:18:26,070
As I mentioned, auto instrumentation,

280
00:18:26,570 --> 00:18:30,342
very easy to do. You don't need to couple

281
00:18:30,476 --> 00:18:34,006
your application to opentelemetry. It's a law of

282
00:18:34,028 --> 00:18:37,866
being fruit, so you should probably do it right away.

283
00:18:37,968 --> 00:18:41,098
If you are using a distributed system,

284
00:18:41,184 --> 00:18:44,540
it will give you a lot of insight into your application.

285
00:18:47,230 --> 00:18:51,006
I mentioned it's practical introduction, so here

286
00:18:51,108 --> 00:18:55,006
let's try to do some practical stuff finally that we have

287
00:18:55,108 --> 00:18:58,334
delved into the theory. So here is my use case.

288
00:18:58,452 --> 00:19:03,010
My use case is well, simple for

289
00:19:03,080 --> 00:19:06,702
a real application, but a bit more involved

290
00:19:06,846 --> 00:19:10,942
for a demo. So at the beginning, an API gateway.

291
00:19:11,006 --> 00:19:13,858
I'm using the Apache API six gateway.

292
00:19:14,034 --> 00:19:17,874
It forwards the request

293
00:19:18,002 --> 00:19:21,430
to my main application, which is

294
00:19:21,580 --> 00:19:24,890
a bring boot Kotlin application. Actually it

295
00:19:24,960 --> 00:19:28,474
gives like products API and

296
00:19:28,512 --> 00:19:32,218
then it has the detail for the product

297
00:19:32,304 --> 00:19:36,154
itself, but it relies on two other components, the one

298
00:19:36,272 --> 00:19:40,154
for the tracing. The pricing is implemented in Python through

299
00:19:40,192 --> 00:19:45,086
a sask framework application and

300
00:19:45,188 --> 00:19:49,398
for the stocks. So how many items

301
00:19:49,434 --> 00:19:53,762
do I have in which warehouse? I have created a rust application

302
00:19:53,896 --> 00:19:56,530
using the XM framework.

303
00:19:59,510 --> 00:20:03,550
So the entry point is actually the reverse proxy

304
00:20:03,630 --> 00:20:06,834
API gateway. Most information

305
00:20:06,952 --> 00:20:10,726
system have such an entry point. You probably never expose your

306
00:20:10,748 --> 00:20:14,418
application directly over the Internet. You have something

307
00:20:14,524 --> 00:20:18,074
in between because well you want to

308
00:20:18,112 --> 00:20:21,980
protect your information system from illegal access.

309
00:20:22,590 --> 00:20:26,758
So that's the most important part. As I mentioned, I'm using

310
00:20:26,864 --> 00:20:29,978
Abeshi API six. Perhaps you don't know about Apeshi

311
00:20:29,994 --> 00:20:34,062
API six. It's a Nabashi project. So basically

312
00:20:34,196 --> 00:20:38,046
again like good for maintenance, everything will

313
00:20:38,068 --> 00:20:41,602
be there with a license that will never change.

314
00:20:41,736 --> 00:20:45,646
It's based on the very successful Nginx reverse proxy. Then you've

315
00:20:45,678 --> 00:20:48,530
got like Lua jits,

316
00:20:49,190 --> 00:20:53,062
additional openrest layer on top which allow you to do

317
00:20:53,196 --> 00:20:56,626
scripting in Lua over the Nginx,

318
00:20:56,738 --> 00:20:59,910
and then you've got out of the box plugins.

319
00:21:01,690 --> 00:21:05,430
So to configure it there is this general configuration,

320
00:21:05,850 --> 00:21:09,738
as I mentioned, like Apache API Six has a

321
00:21:09,744 --> 00:21:13,926
plugin architecture. So here I say, hey, I will be using opentelemetry,

322
00:21:14,038 --> 00:21:17,846
it's an out of the box plugin, you don't need to write any come

323
00:21:17,958 --> 00:21:21,134
and then you can tell, hey, this is the

324
00:21:21,172 --> 00:21:24,894
name by which I want to be known in the

325
00:21:24,932 --> 00:21:28,414
data of opentelemetry, and this is where I will send

326
00:21:28,452 --> 00:21:32,574
the data. So I will be using docker compose,

327
00:21:32,702 --> 00:21:36,210
and so I have a dedicated Jaeger component

328
00:21:38,230 --> 00:21:41,746
then for each route. So here I have

329
00:21:41,768 --> 00:21:46,066
a single one built. You can have different configuration depending

330
00:21:46,098 --> 00:21:50,118
on which route. You will say okay, how much

331
00:21:50,204 --> 00:21:53,458
sample do I want? Normally, and depending

332
00:21:53,474 --> 00:21:57,386
on your volume, you probably don't want 100% because it will

333
00:21:57,488 --> 00:22:01,110
overflow your stuff. You want a sample

334
00:22:01,270 --> 00:22:04,938
here, this is a demo. I will say I want to sample everything.

335
00:22:05,104 --> 00:22:08,382
Again, probably not what you want to do, and then

336
00:22:08,436 --> 00:22:12,218
you can log additional attributes.

337
00:22:12,394 --> 00:22:16,318
So here, for example, I decided for no reason built for demo

338
00:22:16,404 --> 00:22:20,250
purpose, to have the root id, the request method

339
00:22:20,410 --> 00:22:24,034
and an additional hair. So if I can pass through

340
00:22:24,072 --> 00:22:28,718
the clients, some has and they will be traced

341
00:22:28,894 --> 00:22:30,850
along the span.

342
00:22:37,830 --> 00:22:42,034
The next step is the GVM level. As I mentioned,

343
00:22:42,232 --> 00:22:45,902
GVM is runtime, so I can easily

344
00:22:45,966 --> 00:22:48,782
use auto instrumentation.

345
00:22:48,926 --> 00:22:52,446
And on the GVM, auto instrumentation is for Java

346
00:22:52,478 --> 00:22:56,374
agents, so this is quite easy actually. I just need to

347
00:22:56,412 --> 00:22:59,862
pass the Java agent when I start

348
00:22:59,996 --> 00:23:03,094
the application and I don't need to write

349
00:23:03,212 --> 00:23:07,210
anything. So your developers, they are completely

350
00:23:07,360 --> 00:23:10,758
isolated from this tracing concern.

351
00:23:10,854 --> 00:23:14,314
They can write their code and everything will work as

352
00:23:14,352 --> 00:23:17,566
expected. This is regardless of the language and the

353
00:23:17,588 --> 00:23:19,710
framework because it's cheaper.

354
00:23:21,890 --> 00:23:25,466
So here, this is how it works. Here is my docker

355
00:23:25,498 --> 00:23:29,118
file to build my docker container.

356
00:23:29,214 --> 00:23:32,638
This is a multi stage docker file.

357
00:23:32,734 --> 00:23:36,702
First I will compile everything through a GDK, and afterwards

358
00:23:36,766 --> 00:23:40,406
I will run it through a GrE because I don't need a GDK and

359
00:23:40,428 --> 00:23:44,086
it's actually bigger and less secure. So the

360
00:23:44,108 --> 00:23:48,280
first thing I do is like normal standard built,

361
00:23:48,730 --> 00:23:52,714
and then afterwards I get the

362
00:23:52,752 --> 00:23:56,522
jar that I just built and I add the

363
00:23:56,656 --> 00:24:00,234
Java agent through GitHub. And when

364
00:24:00,272 --> 00:24:04,630
I run it, actually I run it through the Java agent.

365
00:24:04,800 --> 00:24:08,654
And this is as simple as it gets. You cannot be

366
00:24:08,692 --> 00:24:12,606
simpler. Afterwards you

367
00:24:12,628 --> 00:24:17,254
can do more precise, more fine grained calls

368
00:24:17,402 --> 00:24:21,860
through manual instrumentation. It needs

369
00:24:22,630 --> 00:24:25,460
an explicit dependency in the application.

370
00:24:25,830 --> 00:24:29,678
This time your developers need to be aware of it.

371
00:24:29,864 --> 00:24:33,970
And then there are two ways to do that, either through a regular

372
00:24:34,050 --> 00:24:37,190
explicit API call or through annotation.

373
00:24:37,610 --> 00:24:40,914
I'm benefiting from bring boot.

374
00:24:40,962 --> 00:24:45,014
So basically I will use annotation. I will issue the codes

375
00:24:45,062 --> 00:24:46,250
just afterwards.

376
00:24:47,950 --> 00:24:51,530
Okay, now it's time to delve into the codes. I will just

377
00:24:51,600 --> 00:24:55,638
focus on the Kotlin spring boot ports.

378
00:24:55,814 --> 00:24:59,566
Everything is on GitHub. So in case you need to check, you can check the

379
00:24:59,588 --> 00:25:03,930
python ports, you can check the rust ports.

380
00:25:04,090 --> 00:25:06,670
Here I will focus on the Javascript.

381
00:25:07,090 --> 00:25:10,286
I've created my application on

382
00:25:10,468 --> 00:25:14,434
Springboot staller start spring IO. So here

383
00:25:14,552 --> 00:25:17,470
I'm using the latest version of tools.

384
00:25:17,630 --> 00:25:21,654
I'm using the latest LTS version of

385
00:25:21,692 --> 00:25:24,854
Java, which is required by the latest version of

386
00:25:24,892 --> 00:25:28,726
Springboot. I'm using also the latest version of Scotland and

387
00:25:28,748 --> 00:25:32,194
then it's a reactive application. So I will be fetching

388
00:25:32,242 --> 00:25:36,006
data, bring r two dbc, otherwise I'm using

389
00:25:36,108 --> 00:25:39,802
webflox. So again to be reactive and the rest

390
00:25:39,856 --> 00:25:43,606
is just like standard kotlin stuff. I didn't

391
00:25:43,638 --> 00:25:47,614
want to bother myself with a regular database, so I'm using h

392
00:25:47,652 --> 00:25:51,514
two, using the r two dbc, h two reactive

393
00:25:51,562 --> 00:25:54,590
driver on the code side itself.

394
00:25:54,740 --> 00:25:58,574
Here I'm using coffin. So I want to use coroutines

395
00:25:58,702 --> 00:26:02,910
because well that's how you can do easily reactive

396
00:26:02,990 --> 00:26:07,294
code stuff. So I'm using the coroutine cred repository.

397
00:26:07,422 --> 00:26:11,202
This is my r two Dbc repository.

398
00:26:11,346 --> 00:26:14,726
Here I have a handler and you can see that

399
00:26:14,748 --> 00:26:18,070
I have suspend function. Suspend function

400
00:26:18,220 --> 00:26:21,110
are like four coroutines in kotlin.

401
00:26:21,950 --> 00:26:25,738
Then I have like one endpoint and

402
00:26:25,824 --> 00:26:29,626
the other endpoint. This one endpoint is for all products.

403
00:26:29,808 --> 00:26:33,260
This one endpoint is for a single product.

404
00:26:35,710 --> 00:26:39,214
Let's see the first one. Okay, and then the rest will be

405
00:26:39,252 --> 00:26:42,814
exactly the same. So I will fetch all product, I will

406
00:26:42,852 --> 00:26:46,366
find all of them into the repository, which in

407
00:26:46,388 --> 00:26:50,002
turn will look into the H two database, and for every

408
00:26:50,056 --> 00:26:53,922
one of them, which probably what you shouldn't do in real life,

409
00:26:54,056 --> 00:26:58,402
I will fetch the product details. So whether not

410
00:26:58,456 --> 00:27:01,846
whether, but their price and their availability in

411
00:27:01,868 --> 00:27:05,526
the stock. So here I can see how it works.

412
00:27:05,708 --> 00:27:09,126
Again, I will have two different

413
00:27:09,228 --> 00:27:12,834
calls, like protected under a nesting block,

414
00:27:12,962 --> 00:27:16,822
which means that here, because I'm using this picture IO,

415
00:27:16,886 --> 00:27:21,194
they will make the calls in parallel and

416
00:27:21,312 --> 00:27:24,486
we can check that it works like this in the traces.

417
00:27:24,678 --> 00:27:28,766
So I will get the price, I will get the stocks, then I

418
00:27:28,788 --> 00:27:32,014
will merge everything. And here is how I

419
00:27:32,052 --> 00:27:35,950
merge everything. So I transform data into

420
00:27:36,020 --> 00:27:39,234
the expected data. And at the end I

421
00:27:39,272 --> 00:27:42,834
create a product with details, including the products from

422
00:27:42,872 --> 00:27:46,562
the database in the catalog, plus the price,

423
00:27:46,696 --> 00:27:50,162
plus the stocks, that I have changed a bit.

424
00:27:50,216 --> 00:27:53,926
For example, I don't want to return to the client any

425
00:27:54,028 --> 00:27:57,878
warehouse where the quantity is like zero

426
00:27:57,964 --> 00:28:01,666
or less, because, well, not zero or less, just zero doesn't

427
00:28:01,698 --> 00:28:04,280
make sense. So I just filter them out.

428
00:28:05,130 --> 00:28:08,262
And at the end I'm using the pins DSL

429
00:28:08,326 --> 00:28:12,486
from Kotlin and the router DSL, or here the come

430
00:28:12,518 --> 00:28:16,330
router DSL to assemble everything. And I start

431
00:28:16,400 --> 00:28:20,254
my application with this bins method. So even if

432
00:28:20,292 --> 00:28:23,758
you are not familiar with Kotlin, if you are a Java developer, I think

433
00:28:23,844 --> 00:28:27,270
it could talk to you. And here you can see my two endpoints.

434
00:28:27,370 --> 00:28:31,330
Products for products and products id

435
00:28:31,400 --> 00:28:35,458
for my product. Now I assemble everything

436
00:28:35,544 --> 00:28:39,390
through Docker compose. So this is the Docker compose file.

437
00:28:39,470 --> 00:28:41,010
I'm using Yeager.

438
00:28:42,230 --> 00:28:46,038
Yeager is available in multiple triggers. You can have

439
00:28:46,204 --> 00:28:49,734
different containers. For example, here I'm using the all

440
00:28:49,772 --> 00:28:53,622
in one. So I'm using the batteries

441
00:28:53,686 --> 00:28:56,666
included package docker image in this case.

442
00:28:56,768 --> 00:29:00,854
So I can already have the opentelemetry collector

443
00:29:00,982 --> 00:29:04,482
provided by Jaeger. So it's

444
00:29:04,646 --> 00:29:07,690
not the open telemetry collector,

445
00:29:07,770 --> 00:29:11,866
it's the Jaeger collector that allows an open telemetry

446
00:29:11,898 --> 00:29:15,886
interface. So here I don't need to think about the architecture of

447
00:29:15,908 --> 00:29:19,298
Jaeger, I'm just using the docker image that does everything.

448
00:29:19,464 --> 00:29:23,298
Then I'm using API six because I want to protect my

449
00:29:23,384 --> 00:29:27,474
services. Then I have the catalog, which is the

450
00:29:27,592 --> 00:29:31,640
spring boot Kotlin application that I've just shown. And here

451
00:29:32,090 --> 00:29:35,922
I need to tell several configuration

452
00:29:35,986 --> 00:29:39,960
parameters. The first one is where does the

453
00:29:40,410 --> 00:29:44,390
Jada agent need to send the data? Well, to Jaeger

454
00:29:44,470 --> 00:29:48,234
on this port. How will

455
00:29:48,272 --> 00:29:52,330
it flag this component here it will be called orders,

456
00:29:52,990 --> 00:29:56,858
which is bring, it should be catalog. Then here

457
00:29:57,024 --> 00:30:00,814
does I want to export metrics here I said no,

458
00:30:00,852 --> 00:30:04,542
I don't want. Of course, depending what you want to do, you can

459
00:30:04,596 --> 00:30:08,660
also export metrics and logs, and logs, the same.

460
00:30:09,990 --> 00:30:13,922
And now pricing, I do the same for the python application,

461
00:30:14,056 --> 00:30:18,322
but again, this is not relevant for this talk. So here,

462
00:30:18,456 --> 00:30:21,414
pricing, same stuff for the python application,

463
00:30:21,532 --> 00:30:25,094
not relevant for this talk. Stock, same thing

464
00:30:25,132 --> 00:30:29,480
for the rust application, not relevant for this talk. Let's start

465
00:30:30,090 --> 00:30:33,260
this architecture. So it might take

466
00:30:33,710 --> 00:30:37,238
a bit of time, especially with the GVM

467
00:30:37,254 --> 00:30:41,146
to start. I will just speed the time

468
00:30:41,248 --> 00:30:44,140
and let's go very fast.

469
00:30:46,850 --> 00:30:50,734
Okay, the logs tell us that it has

470
00:30:50,772 --> 00:30:54,106
started. We can check with Docker

471
00:30:54,138 --> 00:30:58,130
Ps, Docker Ps.

472
00:30:59,430 --> 00:31:03,314
So here it seems that everything has started. We've got the

473
00:31:03,352 --> 00:31:06,846
catalog, the tracing, the stock and Jaeger.

474
00:31:07,038 --> 00:31:11,362
Now we can issue our first curl.

475
00:31:11,426 --> 00:31:14,646
So curl, I will be using the header that

476
00:31:14,668 --> 00:31:17,846
I have configured, Apache API 64. So if

477
00:31:17,868 --> 00:31:21,560
I remember it's hotel key,

478
00:31:22,350 --> 00:31:24,940
then I can say, let's say hello world,

479
00:31:25,790 --> 00:31:29,414
because I have no imagination and I'm

480
00:31:29,462 --> 00:31:32,694
on localhost 90 80,

481
00:31:32,742 --> 00:31:35,360
which is Apache API six,

482
00:31:35,730 --> 00:31:38,320
default port, and we'd say products.

483
00:31:40,130 --> 00:31:43,374
So it takes a bit of time because it will go

484
00:31:43,412 --> 00:31:47,534
through all our systems. So here you can already see that the catalog

485
00:31:47,582 --> 00:31:50,750
has taken, then none of the stocks, the pricing

486
00:31:50,910 --> 00:31:53,300
and Apache API six as well.

487
00:31:54,390 --> 00:31:57,762
So we've got the response, which is not very

488
00:31:57,816 --> 00:32:00,886
interesting, has it is, but it still gives you the

489
00:32:00,908 --> 00:32:04,950
data, you ask. Now the idea is to check the traces,

490
00:32:05,850 --> 00:32:10,162
so I'm now on like the Jaeger web Ui,

491
00:32:10,306 --> 00:32:14,394
and I can check and I will go exactly here.

492
00:32:14,512 --> 00:32:17,610
And here we can see all the microservices,

493
00:32:17,760 --> 00:32:21,594
so there are some traces here I need to

494
00:32:21,632 --> 00:32:25,470
refresh, because here I need to find the traces, and here

495
00:32:25,540 --> 00:32:29,054
we have single requests, because we sample everything.

496
00:32:29,172 --> 00:32:32,080
Here we have it.

497
00:32:32,850 --> 00:32:36,858
And we can see already with only auto instrumentation,

498
00:32:36,954 --> 00:32:40,274
a lot of interesting data. So we can see

499
00:32:40,312 --> 00:32:44,434
that we have our API six, which is the entry point,

500
00:32:44,632 --> 00:32:48,082
and here we have the orders, which I misnamed, it should be called

501
00:32:48,136 --> 00:32:51,394
catalog, but here it's orders. And here

502
00:32:51,432 --> 00:32:55,266
we have the product, here we have the first auto instrumentation

503
00:32:55,378 --> 00:32:58,498
inside of product, because we are using bring boot.

504
00:32:58,594 --> 00:33:02,194
We have lots of proxies inside, you know how bring boot works?

505
00:33:02,332 --> 00:33:05,786
And here decided, hey, here I will make a call for

506
00:33:05,808 --> 00:33:09,194
a proxy, I will trace it. Here we have the

507
00:33:09,232 --> 00:33:13,734
final why? Because it's an interface provided

508
00:33:13,782 --> 00:33:17,182
by Springboot. We didn't provide the implementation. So basically again,

509
00:33:17,236 --> 00:33:21,326
it's a proxy, so it's automatically traced. We can

510
00:33:21,348 --> 00:33:24,926
see here that there is a call to the

511
00:33:25,028 --> 00:33:29,310
other components called stock, so it's

512
00:33:29,390 --> 00:33:33,406
traced as well, which is good. And here we've

513
00:33:33,438 --> 00:33:37,234
got the second one. So here there is one for stock and

514
00:33:37,272 --> 00:33:40,854
one for pricing. And here we see that in

515
00:33:40,892 --> 00:33:44,834
one case I went directly to the component

516
00:33:44,882 --> 00:33:48,290
and the other one went through the API gateway.

517
00:33:48,370 --> 00:33:51,654
Both are completely possible. This is how I

518
00:33:51,692 --> 00:33:54,300
configured it inside,

519
00:33:54,670 --> 00:33:59,194
sorry, my architecture, basically in

520
00:33:59,232 --> 00:34:02,378
one case you can say I want to protect everything that I

521
00:34:02,464 --> 00:34:06,790
always need to get back to the API gateway to do some authentication,

522
00:34:06,870 --> 00:34:10,230
authorization, whatever you want. And the other side you

523
00:34:10,240 --> 00:34:13,326
say oh, I'm pretty secure, I can directly go through it,

524
00:34:13,428 --> 00:34:17,194
but it gives you insight into your architecture as well. In case

525
00:34:17,252 --> 00:34:21,410
you misconfigure something, you can check it through the traces,

526
00:34:22,070 --> 00:34:26,482
something interesting as well. We can see that the

527
00:34:26,536 --> 00:34:30,158
get calls to the stock and the pricing,

528
00:34:30,334 --> 00:34:33,926
they are made in parallel because we use coroutine. So this

529
00:34:33,948 --> 00:34:37,574
is also a good way to check that you actually coded your

530
00:34:37,612 --> 00:34:41,174
stuff correctly. If you see one going after the

531
00:34:41,212 --> 00:34:44,886
other, then probably your code was not right. Though tracing

532
00:34:44,998 --> 00:34:48,474
is not made to do that. You can also validate some

533
00:34:48,512 --> 00:34:51,370
of the come. And then as I mentioned,

534
00:34:51,520 --> 00:34:55,246
it's not good to do that built here. For each of

535
00:34:55,268 --> 00:34:58,810
them I go to the stock and the tracing stock and pricing,

536
00:34:58,890 --> 00:35:02,174
stock and pricing, and we can check

537
00:35:02,212 --> 00:35:05,600
for example that on Apeshi API six

538
00:35:06,950 --> 00:35:10,210
I actually add the additional stuff that I sent.

539
00:35:10,280 --> 00:35:14,020
So basically the routes and the get

540
00:35:14,550 --> 00:35:17,858
and here I'm missing the hotel stuff.

541
00:35:17,944 --> 00:35:20,470
So probably I didn't use the right one,

542
00:35:20,620 --> 00:35:22,840
but believe me, it should work.

543
00:35:25,290 --> 00:35:29,046
Now that also that already give us come information

544
00:35:29,228 --> 00:35:32,982
about our flow. But we might

545
00:35:33,036 --> 00:35:37,066
want to better, we might want for example on the

546
00:35:37,088 --> 00:35:40,438
get to say like which internal,

547
00:35:40,534 --> 00:35:44,186
before which internal method did we call

548
00:35:44,288 --> 00:35:47,440
which parameter? So let's do it.

549
00:35:49,330 --> 00:35:52,474
So now I want manual instrumentation.

550
00:35:52,602 --> 00:35:56,638
It means I need to explicitly couple

551
00:35:56,724 --> 00:36:00,002
myself to the library. So here,

552
00:36:00,136 --> 00:36:03,086
because I'm using spring, as I said, I want annotations.

553
00:36:03,198 --> 00:36:06,110
I don't want to have API calls.

554
00:36:06,190 --> 00:36:09,578
Actually if you check the documentation of the opentelemetry

555
00:36:09,614 --> 00:36:13,510
in Java to get an exporter is not that

556
00:36:13,580 --> 00:36:17,010
fun, requires a lot of API calls.

557
00:36:17,170 --> 00:36:21,126
And well, I have a notation, spring boot is compatible with

558
00:36:21,228 --> 00:36:25,500
opentelemetry. So let's use it. So I've added this

559
00:36:25,950 --> 00:36:29,434
additional dependency in my code and now we can

560
00:36:29,472 --> 00:36:34,090
check the application itself, code itself and

561
00:36:34,160 --> 00:36:38,074
we can go here and here we can see that I've

562
00:36:38,122 --> 00:36:43,518
added like here this

563
00:36:43,604 --> 00:36:47,182
with span, so this with span means that

564
00:36:47,236 --> 00:36:51,022
it will be instrumented and you will find it in the trace.

565
00:36:51,086 --> 00:36:54,580
So I should have these product handler products.

566
00:36:55,110 --> 00:36:58,580
If I'm calling one single product, I will have this one,

567
00:36:58,950 --> 00:37:02,338
but it's also possible to use additional

568
00:37:02,514 --> 00:37:05,862
details. So for example here I will have this

569
00:37:05,916 --> 00:37:09,398
product handle fetch, but I also say hey,

570
00:37:09,484 --> 00:37:12,050
here not only capture this call,

571
00:37:12,140 --> 00:37:15,306
capture this id.

572
00:37:15,408 --> 00:37:18,570
So which product id will I fetch?

573
00:37:19,310 --> 00:37:22,742
Which means that here it's interesting because normally

574
00:37:22,806 --> 00:37:25,934
I shouldn't need it. Here you see that

575
00:37:26,052 --> 00:37:29,902
the id parameter is not used because I already

576
00:37:29,956 --> 00:37:33,706
have the product, but because I want to capture

577
00:37:33,898 --> 00:37:37,762
the id I need to separate this

578
00:37:37,816 --> 00:37:41,534
parameter so I wouldn't be able to capture

579
00:37:41,582 --> 00:37:45,586
the span attribute product because then I could have

580
00:37:45,768 --> 00:37:49,930
not the id but the whole memory reference,

581
00:37:50,030 --> 00:37:53,126
unless I create a two string whatever,

582
00:37:53,308 --> 00:37:57,142
which is not a great idea. So here I change my

583
00:37:57,196 --> 00:38:00,982
method signature a bit to explicitly pass the

584
00:38:01,036 --> 00:38:04,266
id and then, well,

585
00:38:04,448 --> 00:38:07,546
I don't use it, but then it means that this will

586
00:38:07,568 --> 00:38:11,126
be captured by the tracing,

587
00:38:11,318 --> 00:38:14,958
by bring tools and I will find it, which might be super

588
00:38:15,044 --> 00:38:18,974
useful, especially if it fails. So normally now

589
00:38:19,012 --> 00:38:22,622
everything could have started and we can try

590
00:38:22,676 --> 00:38:24,830
again with this configuration.

591
00:38:26,210 --> 00:38:29,422
So here it's the same request. I've just changed

592
00:38:29,566 --> 00:38:33,342
the header because I missed the previous header.

593
00:38:33,406 --> 00:38:37,006
It was not hotel, it was ot. So let's

594
00:38:37,038 --> 00:38:41,294
run this again. We can check that everything works

595
00:38:41,432 --> 00:38:44,562
on the logging side. So here I'm in the catalog,

596
00:38:44,626 --> 00:38:48,280
then I'm in the stock, the pricing, whatever. I've got

597
00:38:48,890 --> 00:38:51,982
the response and we can check back on the Yeager

598
00:38:52,066 --> 00:38:55,610
UI how it looks like we expect more details.

599
00:38:58,190 --> 00:39:02,220
So first we can already see that we have more spans than before.

600
00:39:03,310 --> 00:39:06,942
Just to check on the Apache API six side,

601
00:39:07,076 --> 00:39:10,686
we can see that now my ot key,

602
00:39:10,788 --> 00:39:14,078
this header has been logged, which is good.

603
00:39:14,244 --> 00:39:18,400
And then we can see that I have the product here,

604
00:39:18,850 --> 00:39:22,194
I have the fetch here. So basically

605
00:39:22,312 --> 00:39:26,034
we added additional data. So inside the

606
00:39:26,072 --> 00:39:29,506
components we added a couple more spans to understand

607
00:39:29,608 --> 00:39:33,054
how the flow of the code went inside the components,

608
00:39:33,102 --> 00:39:36,498
not only through the outs, across components.

609
00:39:36,594 --> 00:39:40,326
You can also see that I did the same in Python, so if you are

610
00:39:40,348 --> 00:39:44,358
interested you can check the code. So here I'm logging

611
00:39:44,454 --> 00:39:48,118
the query like manually unfortunately,

612
00:39:48,214 --> 00:39:52,874
but I'm logging the query so you can have additional information what

613
00:39:52,992 --> 00:39:56,670
you are doing. So thanks for your attention.

614
00:39:57,090 --> 00:40:00,526
I hope you learned something. I showed you how you

615
00:40:00,548 --> 00:40:04,266
could use open telemetry, how you could use auto

616
00:40:04,298 --> 00:40:08,790
instrumentation, how you could use manual instrumentation,

617
00:40:08,970 --> 00:40:12,370
and I believe now you can start your journey.

618
00:40:12,790 --> 00:40:16,766
You can follow me on Twitter, you can follow me on Mastodon

619
00:40:16,798 --> 00:40:21,066
if you are on Mastodon. I previously wrote

620
00:40:21,118 --> 00:40:23,778
a blog post about opentelemetry.

621
00:40:23,954 --> 00:40:28,150
It's much more narrow focused. I've improved the demo

622
00:40:28,300 --> 00:40:31,954
code a lot, but perhaps you can read the blog

623
00:40:32,002 --> 00:40:35,046
post. It might give you some insight if

624
00:40:35,068 --> 00:40:38,530
you are interested about everything. So the python,

625
00:40:38,610 --> 00:40:42,154
the rust stuff, everything is on GitHub. I will be very

626
00:40:42,192 --> 00:40:45,834
happy if you check it and if you store it just

627
00:40:45,872 --> 00:40:49,798
to check there is a bitly URL.

628
00:40:49,894 --> 00:40:53,658
So basically I can see how many people were interested in the code.

629
00:40:53,824 --> 00:40:57,514
And though the talk was not about Apache API six,

630
00:40:57,632 --> 00:41:00,610
if got you interested in Apache API six,

631
00:41:00,760 --> 00:41:04,338
then you're welcome to greet us and have a

632
00:41:04,344 --> 00:41:07,586
look. So thanks again for attention and I wish you

633
00:41:07,608 --> 00:41:09,074
a great end of the day.

