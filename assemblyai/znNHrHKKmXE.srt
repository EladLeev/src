1
00:00:34,530 --> 00:00:38,034
Hi, my name is Satish Mane. I will talk about data lake table

2
00:00:38,082 --> 00:00:41,734
formats and their integration with AWS Analytics Services to

3
00:00:41,772 --> 00:00:44,838
build cloud native data lake house on AWS Cloud.

4
00:00:45,004 --> 00:00:48,854
Hi, my name is Rajeev Jaiiswal. In this session I'll take

5
00:00:48,892 --> 00:00:52,750
you on a journey to Data Lake. Thanks for joining in.

6
00:00:52,900 --> 00:00:56,846
Before we dive further, let's first understand couple of trends we

7
00:00:56,868 --> 00:01:00,926
see in businesses. The first one is expectation of the customers.

8
00:01:01,028 --> 00:01:04,306
In the digital era, customers expect the kind of

9
00:01:04,328 --> 00:01:07,634
experience they get from Airbnb, Uber and all

10
00:01:07,672 --> 00:01:11,666
these technologies. Apart from these experiences, the BDI is

11
00:01:11,688 --> 00:01:14,814
personalized, demonstrating a true understanding

12
00:01:14,862 --> 00:01:18,746
of a customer and their context. People's expectations

13
00:01:18,798 --> 00:01:22,310
vary from industry to industry as we offer contextually.

14
00:01:23,290 --> 00:01:25,910
The second trend is new data volume.

15
00:01:26,730 --> 00:01:29,818
Data is growing at an unprecedented rate,

16
00:01:29,904 --> 00:01:34,650
exploiting from terabyte to petabyte and sometimes exabytes.

17
00:01:35,470 --> 00:01:38,854
Traditional on premise data analytics approaches do not scale

18
00:01:38,902 --> 00:01:42,240
well and are too expensive to handle these volumes of data.

19
00:01:43,490 --> 00:01:46,574
We often hear from businesses that they are trying to

20
00:01:46,612 --> 00:01:50,202
extract more value from their data, but are struggling to capture,

21
00:01:50,266 --> 00:01:53,854
store and analyze all the data generated by today's modern

22
00:01:53,902 --> 00:01:57,758
digital business. Data grow exponentially.

23
00:01:57,934 --> 00:02:01,538
They come from new sources, are becoming more diverse, and need to

24
00:02:01,544 --> 00:02:05,890
be securedly accessed and analyzed by any number of applications

25
00:02:05,970 --> 00:02:09,494
and people. All this brings us

26
00:02:09,532 --> 00:02:13,794
to the subject of technology. Before diving into the broader analytics

27
00:02:13,842 --> 00:02:17,286
architecture, let's first understand how legacy or traditional on

28
00:02:17,308 --> 00:02:21,174
premise data analytics stacks up. There is typically

29
00:02:21,222 --> 00:02:24,822
an operating system and database for storing customer records

30
00:02:24,886 --> 00:02:28,470
and transactions, followed by a reporting

31
00:02:28,550 --> 00:02:33,166
database for data Mart and data lakehouse. Type use cases there

32
00:02:33,188 --> 00:02:35,790
are four main problems with the type of architecture.

33
00:02:36,210 --> 00:02:39,790
First, the analytics implementation cycle is too long,

34
00:02:39,940 --> 00:02:43,266
as moving data sets and building dashboard can take weeks or

35
00:02:43,288 --> 00:02:47,170
even months. The second issue is scalability,

36
00:02:47,750 --> 00:02:51,026
higher cost because you always have to plan ahead to

37
00:02:51,048 --> 00:02:53,966
buy more hardware and pay for more licenses.

38
00:02:54,158 --> 00:02:57,986
Third, this architecture is not suitable for modern analytics use cases

39
00:02:58,018 --> 00:03:01,910
such as machine learning adapt queries for data sciences use cases.

40
00:03:02,410 --> 00:03:05,558
Finally, organizations struggle to keep up with the

41
00:03:05,564 --> 00:03:07,720
pace of changing business needs.

42
00:03:09,710 --> 00:03:11,820
Now, how you can solve all these problems?

43
00:03:13,470 --> 00:03:17,034
The answer is data lake. A data lake makes it easier to

44
00:03:17,072 --> 00:03:20,974
derive insights from all your data by providing a single place

45
00:03:21,092 --> 00:03:24,206
to access structured data, semi structured data

46
00:03:24,308 --> 00:03:27,706
and unstructured data. Customers need highly

47
00:03:27,738 --> 00:03:30,942
scalable, highly available, secure and flexible data

48
00:03:30,996 --> 00:03:35,134
store that can handle very large data sets at a reasonable

49
00:03:35,182 --> 00:03:39,330
cost. Therefore, three key points are important for data lakes

50
00:03:39,830 --> 00:03:43,698
data in its original form and format, no matter how much,

51
00:03:43,784 --> 00:03:46,310
what kind, and how fast it is generated.

52
00:03:47,130 --> 00:03:50,454
Structures and processing rules should be defined only when

53
00:03:50,492 --> 00:03:54,342
necessary. Also, known as reading schema. As data

54
00:03:54,396 --> 00:03:58,520
is used by large community, we need to democratize data.

55
00:04:01,610 --> 00:04:05,500
What you are seeing is a high level architecture of data lake in the cloud

56
00:04:06,670 --> 00:04:10,266
as a low cost, durable and scalable storage. Amazon S

57
00:04:10,288 --> 00:04:13,934
three provides storage layer that is completely decoupled from

58
00:04:13,972 --> 00:04:17,822
data processing and various big data tools and has a zero

59
00:04:17,876 --> 00:04:21,438
operation over it. Customer can choose a data lake file format such

60
00:04:21,444 --> 00:04:25,298
as Apache, Parquet, spark powered AWS services

61
00:04:25,384 --> 00:04:29,566
such as AWS, Glow, Amazon, EMR and Athena enables

62
00:04:29,598 --> 00:04:34,286
access and compute at scale. The meta level stores metadata

63
00:04:34,398 --> 00:04:38,310
about tables, columns and partitions in the AWS glue catalog.

64
00:04:40,490 --> 00:04:44,086
To keep the data in its original form and format, you need the ability to

65
00:04:44,108 --> 00:04:47,554
handle various file formats such as JSON, CSV, Parquet,

66
00:04:47,602 --> 00:04:51,146
Avro and more. Each format is suitable for

67
00:04:51,168 --> 00:04:52,940
different use cases. For example,

68
00:04:53,710 --> 00:04:57,478
CSV is popular for its low volume and human readable format.

69
00:04:57,654 --> 00:05:02,058
CSV is what we call the line storage parquet

70
00:05:02,074 --> 00:05:05,754
file. Organize data into column column store files

71
00:05:05,802 --> 00:05:09,134
are more optimized because you can perform better compression on

72
00:05:09,172 --> 00:05:13,062
each column. Parquet is well suited for bulk processing

73
00:05:13,226 --> 00:05:15,300
of complex data.

74
00:05:16,710 --> 00:05:19,730
Now you understand the data lake component.

75
00:05:20,310 --> 00:05:24,526
As such, if you're creating your own data lake, there are some companies

76
00:05:24,638 --> 00:05:29,026
that you will most likely need to create and maintain for your data lake.

77
00:05:29,218 --> 00:05:32,530
Now data need to be collected and must be scalable

78
00:05:32,610 --> 00:05:35,030
storage. Without ETL transformation,

79
00:05:36,970 --> 00:05:40,374
all data must be cataloged because without a catalog you cannot

80
00:05:40,422 --> 00:05:43,900
manage data, find data and organize access control.

81
00:05:45,070 --> 00:05:48,294
All kind of analytics are needed including patch

82
00:05:48,342 --> 00:05:51,674
analytics, stream analytics and advanced analytics

83
00:05:51,722 --> 00:05:55,834
like machine learning. Therefore endtoend data ingestion and analysis.

84
00:05:55,882 --> 00:05:58,110
Processes need to be coordinated.

85
00:05:59,410 --> 00:06:02,362
Data should be available to all kind of people,

86
00:06:02,516 --> 00:06:06,142
users and roles. Most importantly,

87
00:06:06,206 --> 00:06:09,300
you need a framework for managing analytics and data.

88
00:06:10,230 --> 00:06:13,330
Without governance, finding good solution is impossible.

89
00:06:13,830 --> 00:06:17,926
Data analysts can query data lakes directly using fast

90
00:06:18,028 --> 00:06:21,910
compute engines such as redshift and their preferred language SQL.

91
00:06:22,330 --> 00:06:25,446
Data scientists then have all the data they need to

92
00:06:25,468 --> 00:06:28,794
build robust models. Data engineers can also

93
00:06:28,832 --> 00:06:32,330
easily simplify data and focus on infrastructure.

94
00:06:33,230 --> 00:06:37,414
So let's understand benefit of serverless data lakes services

95
00:06:37,462 --> 00:06:41,482
is a native architecture of the cloud, allowing you to offload more operational

96
00:06:41,546 --> 00:06:45,114
responsibilities to AWS. Increase agility

97
00:06:45,162 --> 00:06:48,670
and innovation by allowing you to focus on writing the business

98
00:06:48,740 --> 00:06:52,346
logic and services. Your customer services

99
00:06:52,378 --> 00:06:55,534
technology offers automatic scaling, built in, high availability,

100
00:06:55,662 --> 00:06:58,770
and a consumption based billing model for cost optimization.

101
00:06:59,510 --> 00:07:03,042
Serverless allow you to build and run applications and services without

102
00:07:03,096 --> 00:07:06,658
worrying about infrastructure. Eliminate infrastructure management tasks

103
00:07:06,674 --> 00:07:10,178
such as server or cluster provisioning, patching,

104
00:07:10,274 --> 00:07:13,110
operating system maintenance and capacity provisioning.

105
00:07:13,530 --> 00:07:17,126
AWS offers many other serverless services which I won't cover here

106
00:07:17,228 --> 00:07:20,354
such as DynamoDB and Redshift, serverless etc.

107
00:07:20,482 --> 00:07:23,286
Now I'm going to hand over the call to Satish who is going to deep

108
00:07:23,318 --> 00:07:27,178
dive Lake house architecture. Thank you Rajiv. Now that you understand

109
00:07:27,264 --> 00:07:31,610
regular data lake, let me explain the building blocks of lake House architecture

110
00:07:32,190 --> 00:07:35,898
data lakes have become default repository for all kinds of data.

111
00:07:36,064 --> 00:07:39,534
A data lake services as a single source of truth for a large number of

112
00:07:39,572 --> 00:07:43,310
users querying from a variety of analytics and machine learning tools.

113
00:07:43,650 --> 00:07:47,074
Is your data lake getting unmanageable? Do you want to build

114
00:07:47,112 --> 00:07:51,246
a highly scalable, cost effective data lakes with transactional capabilities?

115
00:07:51,438 --> 00:07:54,990
Are you struggling to comply with data regulations as to how customer

116
00:07:55,080 --> 00:07:59,106
data in data lakes can be used? If you are facing these challenges

117
00:07:59,218 --> 00:08:03,074
then this session talks about how lake house architecture solves

118
00:08:03,122 --> 00:08:04,070
those challenges.

119
00:08:08,510 --> 00:08:11,260
What challenges do typical data lake face?

120
00:08:12,110 --> 00:08:15,654
Regular data lakes provide scalable and cost effective storage.

121
00:08:15,782 --> 00:08:19,594
However, this is not possible with regular data lakes when

122
00:08:19,632 --> 00:08:23,290
continuously ingesting data and using transactional capabilities

123
00:08:23,370 --> 00:08:26,800
to query from many analytics tools. At same time,

124
00:08:27,170 --> 00:08:29,978
under CCPA and GDPR regulations,

125
00:08:30,154 --> 00:08:33,786
businesses must change or delete all of customers'data

126
00:08:33,898 --> 00:08:37,426
upon request to comply with customer's right to be

127
00:08:37,448 --> 00:08:41,154
forgotten or change of data. Change of consent to the use

128
00:08:41,192 --> 00:08:45,134
of data it is difficult to make these kind of record level

129
00:08:45,192 --> 00:08:48,822
changes in regular data lakes. Some customers find

130
00:08:48,956 --> 00:08:52,002
change data capture pipeline difficult to handle.

131
00:08:52,146 --> 00:08:56,086
This is especially true for recent data or erroneous data

132
00:08:56,188 --> 00:08:59,706
that needs to be rewritten. A typical data lake would

133
00:08:59,728 --> 00:09:03,402
have to reprocess missing or corrupted data due to job

134
00:09:03,456 --> 00:09:06,140
failures, which can be a big problem.

135
00:09:06,670 --> 00:09:10,322
Regular data lake do not enforce schema when writing

136
00:09:10,486 --> 00:09:13,280
so you cannot avoid ingesting low quality data.

137
00:09:13,810 --> 00:09:17,994
Also, one should know the partition or table structure

138
00:09:18,122 --> 00:09:21,294
to avoid full table scan and listing files from

139
00:09:21,332 --> 00:09:22,590
all partitions.

140
00:09:24,790 --> 00:09:28,130
So let's see how a open table format can be used

141
00:09:28,200 --> 00:09:31,986
to address these challenges mentioned on previous slide one of

142
00:09:32,008 --> 00:09:35,746
the key characteristics expected of lake house architecture

143
00:09:35,858 --> 00:09:38,786
is transactional or acid properties.

144
00:09:38,978 --> 00:09:42,246
You do not have to write any code in the transactional or

145
00:09:42,268 --> 00:09:45,654
data lake format which I will cover in next

146
00:09:45,692 --> 00:09:49,686
few slides. Transactions are automatically written to the log presenting

147
00:09:49,718 --> 00:09:53,178
a single source of truth. Advanced features such as

148
00:09:53,264 --> 00:09:57,126
time travel, data transformation with DML, and concurrent

149
00:09:57,158 --> 00:10:00,574
read and writes are also expected in data lake to handle use

150
00:10:00,612 --> 00:10:04,394
cases such as change data capture and late arriving streaming

151
00:10:04,442 --> 00:10:08,334
data over time. You can also expect data lake to have

152
00:10:08,372 --> 00:10:12,750
features such AWS, schema evolution and schema enforcement.

153
00:10:13,110 --> 00:10:16,946
These features allow you to update your schema over time to

154
00:10:16,968 --> 00:10:19,970
ensure data quality during ingestion.

155
00:10:20,390 --> 00:10:23,746
Engine neutrality is also expected in future of

156
00:10:23,768 --> 00:10:27,942
data architecture. Today you use a compute engine to process data,

157
00:10:28,076 --> 00:10:32,200
but tomorrow you can use different engine for new needs

158
00:10:32,730 --> 00:10:36,418
for time travel data lake table format

159
00:10:36,594 --> 00:10:40,038
versions, the big data that you store in the data lake.

160
00:10:40,134 --> 00:10:43,354
You can access any historical versions of the data,

161
00:10:43,472 --> 00:10:46,966
simplifying data management with easy to audit rollback

162
00:10:46,998 --> 00:10:51,610
data in case of accidental bad writes or deletes and reproduce

163
00:10:51,690 --> 00:10:54,190
experiments and reports.

164
00:10:54,690 --> 00:10:58,286
Time travel enables reproducible queries by allowing two

165
00:10:58,308 --> 00:11:01,120
different versions to be queried at same time.

166
00:11:01,490 --> 00:11:05,294
Opentable format work at scale by automatically checkpointing

167
00:11:05,342 --> 00:11:09,154
and summarizing large amounts of data, many files and their

168
00:11:09,192 --> 00:11:10,210
metadata.

169
00:11:12,550 --> 00:11:16,166
So what are your options for creating a data lake house

170
00:11:16,268 --> 00:11:20,070
architecture to solve those regular data lake challenges?

171
00:11:20,490 --> 00:11:24,294
Customers often face a dilemma when it covers to choosing right

172
00:11:24,332 --> 00:11:27,958
data architecture for building data lake house. As such,

173
00:11:28,124 --> 00:11:31,674
some customers use data warehouse to

174
00:11:31,712 --> 00:11:35,430
eliminate the need of data lakes and the complexity

175
00:11:35,510 --> 00:11:38,778
that comes with the data lake. However, a new pattern that

176
00:11:38,784 --> 00:11:42,426
is emerging as a popular pattern for implementing

177
00:11:42,458 --> 00:11:46,078
data lake house on AWS is to combine both

178
00:11:46,164 --> 00:11:49,258
data lake and data warehousing capabilities.

179
00:11:49,434 --> 00:11:53,982
This pattern is known as lake House architectural pattern.

180
00:11:54,126 --> 00:11:58,318
There are two options for creating lakes House on AWS,

181
00:11:58,414 --> 00:12:01,010
which I will talk about in next few slides.

182
00:12:03,510 --> 00:12:07,366
So before diving into each data lake table format and

183
00:12:07,468 --> 00:12:11,250
the lake house architectural options on AWS,

184
00:12:11,410 --> 00:12:14,626
let me quickly compare the building blocks

185
00:12:14,658 --> 00:12:18,330
that we discussed on or I discussed on previous slide.

186
00:12:19,550 --> 00:12:23,002
So depending on your needs, a typical organization will

187
00:12:23,056 --> 00:12:26,762
need both a data warehouse and

188
00:12:26,816 --> 00:12:30,250
data lake that serve different needs and use cases.

189
00:12:30,770 --> 00:12:34,554
Data lakes store both structured and unstructured

190
00:12:34,602 --> 00:12:37,694
data from various other data sources such as

191
00:12:37,732 --> 00:12:41,246
mobile apps, IoT devices, and social media.

192
00:12:41,428 --> 00:12:44,786
The structure of the data or schema is not defined at

193
00:12:44,808 --> 00:12:48,450
the time of data collection. This means you can store

194
00:12:48,520 --> 00:12:52,590
all your data without having to plan carefully

195
00:12:52,670 --> 00:12:56,120
or know what questions you will need to answer in the future.

196
00:12:57,210 --> 00:13:00,722
A data warehouse is a database optimized for analyzing

197
00:13:00,786 --> 00:13:04,662
relational data from transactional systems. Data structures and

198
00:13:04,716 --> 00:13:08,790
schemas are predefined to optimize fast SQL queries,

199
00:13:08,950 --> 00:13:12,902
the result of which are typically used for operational reporting

200
00:13:12,966 --> 00:13:17,238
and analysis. Data is cleaned, enriched, and transformed

201
00:13:17,334 --> 00:13:20,778
so that it can serve as a single source of truth that users

202
00:13:20,794 --> 00:13:24,362
can rely on. However, once organizations

203
00:13:24,426 --> 00:13:28,234
with data warehouse recognize the benefits of data lakes

204
00:13:28,282 --> 00:13:32,058
house that provide the functionality of both data lake

205
00:13:32,154 --> 00:13:35,502
and data warehouse, they can evolve their data warehouse

206
00:13:35,566 --> 00:13:39,230
to include data lake house and enable various

207
00:13:39,310 --> 00:13:40,610
query capabilities.

208
00:13:43,430 --> 00:13:47,046
So the first lake house architecture option is ready to

209
00:13:47,068 --> 00:13:51,042
use platform on AWS. This approach

210
00:13:51,106 --> 00:13:54,978
allows for separate data warehouse with transactional capabilities

211
00:13:55,074 --> 00:13:58,326
such as Amazon, Redshift, and a cost effective

212
00:13:58,358 --> 00:14:02,518
scalable data lake on Amazon s three technologies

213
00:14:02,614 --> 00:14:05,914
such as Amazon Redshift spectrum can then be used to

214
00:14:05,952 --> 00:14:09,354
integrate strategically distributed data in both data

215
00:14:09,392 --> 00:14:12,638
warehouse and data lake. This approach definitely

216
00:14:12,724 --> 00:14:16,698
simplifies the engineering effort free developers

217
00:14:16,794 --> 00:14:20,842
to focus on feature development and leave the infrastructure to the cloud to harness

218
00:14:20,906 --> 00:14:24,734
the power of serverless technology from storage to processing

219
00:14:24,862 --> 00:14:28,350
and to presentation layer. In this pattern,

220
00:14:28,510 --> 00:14:31,822
data from various data sources is aggregated

221
00:14:31,886 --> 00:14:35,726
into Amazon s three before transformation or loading

222
00:14:35,758 --> 00:14:39,046
into data warehouse. This pattern is useful if

223
00:14:39,068 --> 00:14:42,726
you want to keep the raw data in the data lake and process data in

224
00:14:42,748 --> 00:14:47,080
the data warehouse to avoid scaling cost. With this option,

225
00:14:47,390 --> 00:14:51,238
you can take advantage of Amazon Redshift's transactional

226
00:14:51,254 --> 00:14:55,770
capabilities and also run low latency analytical queries.

227
00:14:58,270 --> 00:15:01,466
The second option is do it yourself option for creating

228
00:15:01,498 --> 00:15:04,846
a larger data lakes house. Why do it

229
00:15:04,868 --> 00:15:08,654
yourself? This pattern is growing in popularity because

230
00:15:08,692 --> 00:15:12,694
of three table formats, Apache hoodie, Delta Lake,

231
00:15:12,762 --> 00:15:15,922
and Iceberg, that have emerged over the past few

232
00:15:15,976 --> 00:15:19,394
years to power data lake house that

233
00:15:19,432 --> 00:15:22,242
support acid transactions, time travel,

234
00:15:22,376 --> 00:15:25,874
granular access control, and deliver a very good

235
00:15:25,912 --> 00:15:29,754
performance compared to regular data lake. These open table

236
00:15:29,822 --> 00:15:33,398
data lake formats combines the scalability and

237
00:15:33,484 --> 00:15:37,046
cost effectiveness of data lake on Amazon s

238
00:15:37,068 --> 00:15:40,362
three and transactional capabilities, reliability and

239
00:15:40,416 --> 00:15:44,550
performance of data warehouse to ensure greater scale.

240
00:15:44,710 --> 00:15:48,474
Table formats or data lake table formats are instrumental for

241
00:15:48,512 --> 00:15:52,458
getting the scalability benefits of data lake and the underlying

242
00:15:52,634 --> 00:15:56,574
Amazon s three object store, while at the same time getting the data

243
00:15:56,612 --> 00:16:00,270
quality and governance associated with data warehouses.

244
00:16:00,610 --> 00:16:04,050
These data lake table format framework also

245
00:16:04,120 --> 00:16:08,142
add additional governance compared to regular data lake.

246
00:16:08,286 --> 00:16:12,020
Optionally, you can connect Amazon redshift for

247
00:16:12,390 --> 00:16:16,280
low latency OLAP access to business ready data.

248
00:16:19,130 --> 00:16:22,738
Now I will quickly walk through three popular table formats.

249
00:16:22,834 --> 00:16:27,030
First one is Apache hoodie. Apache Hoodie follows

250
00:16:27,370 --> 00:16:30,742
timeline based transaction model. A timeline

251
00:16:30,806 --> 00:16:34,534
contains all actions performed on the table at different instance

252
00:16:34,582 --> 00:16:38,006
of time. The time could provide instantaneous views

253
00:16:38,038 --> 00:16:42,010
of table and support to get data in the order of arrival.

254
00:16:42,170 --> 00:16:46,106
Apache Hoodie offers both multi

255
00:16:46,138 --> 00:16:49,870
version concurrency control and optimistic concurrency control.

256
00:16:50,020 --> 00:16:53,454
Using multi version concurrency control, Hoodie provides

257
00:16:53,502 --> 00:16:57,122
snapshot isolation between an ingestion writer and

258
00:16:57,176 --> 00:17:01,106
multiple concurrent readers. It also apply

259
00:17:01,208 --> 00:17:04,254
optimistic concurrency control for a reader and writer.

260
00:17:04,382 --> 00:17:07,874
Hoodie supports file level optimistic

261
00:17:07,922 --> 00:17:11,698
concurrency control, that is, for any two commits or writers

262
00:17:11,794 --> 00:17:15,202
happening to the same table. If they do not have rights

263
00:17:15,346 --> 00:17:19,110
to overlapping files being changed, both writers

264
00:17:19,190 --> 00:17:22,794
are allowed to succeed. The next one is time

265
00:17:22,832 --> 00:17:26,406
travel. You could also do a time travel. According to hoodie,

266
00:17:26,438 --> 00:17:30,006
commit time hoodie supports schema evolution to

267
00:17:30,048 --> 00:17:33,374
add, delete, modify, and move columns, but it does not

268
00:17:33,412 --> 00:17:36,842
support partition evolution. You cannot change partition

269
00:17:36,906 --> 00:17:40,170
column when it comes to storage optimization.

270
00:17:40,330 --> 00:17:44,158
Auto file sizing and auto companies is great for ensuring

271
00:17:44,254 --> 00:17:48,034
storage optimization by avoiding small files in

272
00:17:48,152 --> 00:17:52,574
Apache hoodie, and the last one is indexing. By default.

273
00:17:52,702 --> 00:17:56,118
Hoodie uses index that stores mapping between record

274
00:17:56,204 --> 00:18:00,242
key and file group id it belongs to. When modeling,

275
00:18:00,386 --> 00:18:03,570
use record key that is monotonically increasing.

276
00:18:03,650 --> 00:18:07,710
For example timestamp prefix for best index performance

277
00:18:07,810 --> 00:18:11,450
by range pruning to filter out the files.

278
00:18:15,070 --> 00:18:18,422
The second option is a table format is Apache iceberg.

279
00:18:18,486 --> 00:18:22,266
Apache Iceberg follows snapshot based transaction

280
00:18:22,298 --> 00:18:25,758
model. A snapshot is a complete list of files in

281
00:18:25,764 --> 00:18:28,986
the table. The table state is maintained in metadata

282
00:18:29,018 --> 00:18:33,310
files. All changes to table state create a new metadata file

283
00:18:33,390 --> 00:18:37,010
and they replace old metadata file. With atomic swap,

284
00:18:37,590 --> 00:18:41,634
iceberg follows optimistic concurrency control.

285
00:18:41,832 --> 00:18:45,966
The writers create a table metadata files optimistically assuming

286
00:18:45,998 --> 00:18:50,162
that current version will not be changed before the writers commit.

287
00:18:50,306 --> 00:18:54,194
Once writer has created can update, it commits

288
00:18:54,242 --> 00:18:58,318
by swapping the table's metadata file pointer from the base version

289
00:18:58,354 --> 00:19:01,834
to the new version. If the snapshot on which update is

290
00:19:01,872 --> 00:19:05,814
based is no longer current, the writer must retry

291
00:19:05,862 --> 00:19:09,242
the update based on the new version current the

292
00:19:09,296 --> 00:19:12,734
new version time

293
00:19:12,772 --> 00:19:16,586
travel user could also do time travel using according

294
00:19:16,618 --> 00:19:20,558
to snapshot id and timestamp. When it

295
00:19:20,564 --> 00:19:24,286
comes to storage optimization, you can clean up unused older

296
00:19:24,318 --> 00:19:28,846
snapshots by marking them as expired based on certain time period

297
00:19:28,958 --> 00:19:33,490
and then manually run spark job to delete them.

298
00:19:33,640 --> 00:19:37,046
To optimize files into larger files, you need

299
00:19:37,068 --> 00:19:40,610
to run spark job in background manually

300
00:19:40,770 --> 00:19:44,162
and the last one is indexing. Apache iceberg uses

301
00:19:44,226 --> 00:19:47,842
value range for columns to skip data files

302
00:19:47,986 --> 00:19:52,406
and partition fields to skip manifest files

303
00:19:52,518 --> 00:19:54,330
when executing query.

304
00:19:58,190 --> 00:20:01,790
Delta Lake Delta Lake has a transaction model

305
00:20:01,860 --> 00:20:06,058
based on transaction log. It logs the file operations

306
00:20:06,154 --> 00:20:10,494
in JSON file and then commit to the table using

307
00:20:10,612 --> 00:20:14,746
atomic operations. Delta Lake automatically generates checkpoint

308
00:20:14,778 --> 00:20:17,570
files every ten commits into parquet file.

309
00:20:18,390 --> 00:20:22,206
Delta Lake employs optimistic concurrency control optimistic

310
00:20:22,238 --> 00:20:26,182
concurrency control is a method of dealing with concurrent transactions that

311
00:20:26,236 --> 00:20:29,974
assume that transactions or changes made to

312
00:20:30,012 --> 00:20:34,280
table by different users can complete without conflicting with one another.

313
00:20:35,210 --> 00:20:39,314
User cloud also do time travel query according to the timestamp

314
00:20:39,362 --> 00:20:43,346
or version number deltax supports

315
00:20:43,378 --> 00:20:46,938
or lets you update schema by a schema of

316
00:20:46,944 --> 00:20:50,682
a table by adding new column or reordering existing column.

317
00:20:50,826 --> 00:20:54,394
And when it comes to storage optimization, delta Lake

318
00:20:54,442 --> 00:20:58,554
does not have companies as it follows copy

319
00:20:58,602 --> 00:21:02,350
and write. Hence file sizing is manual.

320
00:21:02,710 --> 00:21:05,922
You need to run vacuum and optimize file size

321
00:21:05,976 --> 00:21:10,050
command to convert small files into large files.

322
00:21:10,710 --> 00:21:14,820
Delta Lake collects column sets for

323
00:21:15,130 --> 00:21:18,822
data skipping index during query so it takes

324
00:21:18,876 --> 00:21:22,258
advantage of this information, the minimum maximum values

325
00:21:22,274 --> 00:21:26,450
of each column to add query time to provide faster queries.

326
00:21:26,610 --> 00:21:30,054
The zorder index technique it uses

327
00:21:30,102 --> 00:21:33,846
to colocate the data skipping information in same file

328
00:21:33,958 --> 00:21:37,180
for a particular column to be used in Z order.

329
00:21:40,530 --> 00:21:44,250
So this is a quick snapshot of how these table

330
00:21:44,330 --> 00:21:48,394
formats are integrated with AWS analytics

331
00:21:48,442 --> 00:21:51,774
services. Amazon Athena has better integration with

332
00:21:51,812 --> 00:21:55,474
Apache Iceberg in terms of read write operations, whereas it

333
00:21:55,512 --> 00:21:58,606
supports only read operations on Apache hoodie

334
00:21:58,638 --> 00:22:02,334
and Delta Lake. Amazon redshift spectrum supports

335
00:22:02,382 --> 00:22:06,280
both Apache hoodie and Delta Lake for reading data.

336
00:22:06,810 --> 00:22:10,438
EMR and glue supports both read write

337
00:22:10,524 --> 00:22:14,630
against these table formats. These three table formats

338
00:22:15,370 --> 00:22:18,534
you can also manage permissions in Amazon Athena

339
00:22:18,582 --> 00:22:22,758
using AWS Lake formation for Apache hoodie and Apache iceberg

340
00:22:22,774 --> 00:22:26,474
table format. Similarly, you can manage permissions in

341
00:22:26,512 --> 00:22:30,462
Amazon redshift spectrum using AWS Lake formation for

342
00:22:30,516 --> 00:22:32,990
Delta Lake and Apache Hoodie.

343
00:22:35,650 --> 00:22:39,214
To conclude, here are final thoughts on choosing data

344
00:22:39,252 --> 00:22:42,938
Lake table format for building lake House architecture on AWS

345
00:22:43,034 --> 00:22:46,734
based on your use case as well as integration with AWS

346
00:22:46,782 --> 00:22:50,286
analytics services. Apache hoodie is considered

347
00:22:50,398 --> 00:22:53,794
suitable for streaming use cases whether it's IoT data or

348
00:22:53,832 --> 00:22:57,346
change data capture from database. Hoodie provides highly

349
00:22:57,378 --> 00:23:00,386
flexible three types of indexes for optimizing

350
00:23:00,418 --> 00:23:03,810
query performance and also optimizing data storage.

351
00:23:03,970 --> 00:23:07,958
Because of autofile sizing and clustering optimization feature

352
00:23:08,054 --> 00:23:12,134
backed by index lookup, it is great for streaming use cases.

353
00:23:12,262 --> 00:23:16,006
It comes with managed data ingestion tool called delta

354
00:23:16,038 --> 00:23:18,570
Streamer unlike other two table formats.

355
00:23:19,470 --> 00:23:22,798
Second option is Apache Iceberg. If you're looking for

356
00:23:22,884 --> 00:23:25,818
easy management of schema and partition evolution,

357
00:23:25,914 --> 00:23:29,118
then Apache Iceberg is suitable table format. One of

358
00:23:29,124 --> 00:23:33,062
the advantage of Apache iceberg is how it handles partitions.

359
00:23:33,226 --> 00:23:36,930
Basically IT services partition value from data fields used

360
00:23:37,000 --> 00:23:39,678
in a SQL where condition.

361
00:23:39,854 --> 00:23:43,218
One does not need to specify exact partition key in

362
00:23:43,224 --> 00:23:47,186
the SQL query. Unlike Hoodie

363
00:23:47,218 --> 00:23:50,326
and Delta Lake, Iceberg allows easily to change

364
00:23:50,428 --> 00:23:54,438
partition column on table. It simply starts writing to

365
00:23:54,604 --> 00:23:58,338
new partition. Third option is Delta

366
00:23:58,354 --> 00:24:02,134
Lake. This table format is suitable if your data platform is

367
00:24:02,172 --> 00:24:05,506
built around spark framework with deep integration of spark features.

368
00:24:05,618 --> 00:24:09,254
AWS Delta Lake stores all metadata state information in

369
00:24:09,292 --> 00:24:12,602
transaction log and checkpoint files instead of metastore.

370
00:24:12,746 --> 00:24:16,634
You can use separate spark cluster to build table

371
00:24:16,682 --> 00:24:20,218
state independently without relying on central Metastore

372
00:24:20,314 --> 00:24:24,170
and this really helps to scale and meet the performance requirements

373
00:24:24,250 --> 00:24:28,046
depending on your spark cluster size. Thank you for listening. Me and my

374
00:24:28,068 --> 00:24:30,090
colleague hope you all enjoyed this session.

